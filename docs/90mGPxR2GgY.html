<html><head><title>BERT explained: Training, Inference,  BERT vs GPT/LLamA, Fine tuning, [CLS] token</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>BERT explained: Training, Inference,  BERT vs GPT/LLamA, Fine tuning, [CLS] token</h2><a href="https://www.youtube.com/watch?v=90mGPxR2GgY"><img src="https://i.ytimg.com/vi_webp/90mGPxR2GgY/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=120">2:0</a> Language Models<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=190">3:10</a> Training (Language Models)<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=443">7:23</a> Inference (Language Models)<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=555">9:15</a> Transformer architecture (Encoder)<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=628">10:28</a> Input Embeddings<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=857">14:17</a> Positional Encoding<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1034">17:14</a> Self-Attention and causal mask<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1754">29:14</a> BERT (overview)<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1928">32:8</a> BERT vs GPT/LLaMA<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2065">34:25</a> Left context and right context<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2196">36:36</a> BERT pre-training<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2225">37:5</a> Masked Language Model<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2701">45:1</a> [CLS] token<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2906">48:26</a> BERT fine-tuning<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2940">49:0</a> Text classification<br><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3050">50:50</a> Question answering<br><br><div style="text-align: left;"><a href="./90mGPxR2GgY.html">Whisper Transcript</a> | <a href="./transcript_90mGPxR2GgY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my new video on BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2" target="_blank">00:00:02.520</a></span> | <span class="t">In this video I will be explaining BERT from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=5" target="_blank">00:00:05.860</a></span> | <span class="t">What do I mean by this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=7" target="_blank">00:00:07.500</a></span> | <span class="t">I will explain actually all the building blocks that make up BERT along with all the background</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=13" target="_blank">00:00:13.280</a></span> | <span class="t">knowledge that you need to understand this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=15" target="_blank">00:00:15.780</a></span> | <span class="t">So we will start with a little review of what are language models, how they are trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=20" target="_blank">00:00:20.360</a></span> | <span class="t">how we inference language models, then we will see the transformer architecture, at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=24" target="_blank">00:00:24.400</a></span> | <span class="t">least the one used by language models, so the encoder part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=27" target="_blank">00:00:27.680</a></span> | <span class="t">We will review embedding vectors, positional encoding, self-attention and causal mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=32" target="_blank">00:00:32.560</a></span> | <span class="t">And then I will introduce BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=34" target="_blank">00:00:34.640</a></span> | <span class="t">So I will first give you a big background knowledge before you can start learning BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=42" target="_blank">00:00:42.600</a></span> | <span class="t">so you understand each building blocks of BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=45" target="_blank">00:00:45.640</a></span> | <span class="t">And then we will see concepts like what is the left context, what is the right context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=49" target="_blank">00:00:49.960</a></span> | <span class="t">and how it's used in BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=51" target="_blank">00:00:51.960</a></span> | <span class="t">We will see the two tasks on which BERT has been pre-trained, so the masked language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=55" target="_blank">00:00:55.760</a></span> | <span class="t">and the next sentence prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=58" target="_blank">00:00:58.640</a></span> | <span class="t">And finally, we will also see what is fine tuning and how do we fine tune BERT with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=65" target="_blank">00:01:05.280</a></span> | <span class="t">text classification task and the question answering task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=69" target="_blank">00:01:09.200</a></span> | <span class="t">What do I expect you to know before watching this video?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=72" target="_blank">00:01:12.240</a></span> | <span class="t">Well, for sure, I hope that you are familiar with the transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=77" target="_blank">00:01:17.100</a></span> | <span class="t">For example, if you have watched my previous video on the transformer model, that would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=80" target="_blank">00:01:20.400</a></span> | <span class="t">be great because even if I will review some of the concepts of the transformer, I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=87" target="_blank">00:01:27.480</a></span> | <span class="t">not review all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=88" target="_blank">00:01:28.480</a></span> | <span class="t">So for example, I will not touch concepts like the cross attention or the multi-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=92" target="_blank">00:01:32.680</a></span> | <span class="t">attention or the normalization, the feed-forward layer, et cetera, et cetera, because I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=98" target="_blank">00:01:38.320</a></span> | <span class="t">also only give you enough background to understand BERT, so it's not a video on the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=103" target="_blank">00:01:43.160</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=104" target="_blank">00:01:44.160</a></span> | <span class="t">So please, if you are not familiar with the transformer, please go watch my previous video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=107" target="_blank">00:01:47.680</a></span> | <span class="t">on the transformer model and then you can watch this video if you want to fully understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=112" target="_blank">00:01:52.240</a></span> | <span class="t">BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=113" target="_blank">00:01:53.440</a></span> | <span class="t">So let's start our journey with language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=117" target="_blank">00:01:57.560</a></span> | <span class="t">What is a language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=119" target="_blank">00:01:59.000</a></span> | <span class="t">Well, a language model is a probabilistic model that assigns probabilities to sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=124" target="_blank">00:02:04.360</a></span> | <span class="t">of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=125" target="_blank">00:02:05.360</a></span> | <span class="t">In practice, a language model allows us to compute the following probability, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=130" target="_blank">00:02:10.280</a></span> | <span class="t">the probability of the word, for example, China, following the sentence, "Shanghai is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=137" target="_blank">00:02:17.120</a></span> | <span class="t">a city in".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=138" target="_blank">00:02:18.780</a></span> | <span class="t">So what is the probability that the word China comes next in the sentence, "Shanghai is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=144" target="_blank">00:02:24.520</a></span> | <span class="t">city in"?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=145" target="_blank">00:02:25.520</a></span> | <span class="t">This is the kind of probability that we model using language models, which is a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=150" target="_blank">00:02:30.880</a></span> | <span class="t">trained on a very large corpora of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=154" target="_blank">00:02:34.160</a></span> | <span class="t">When this large corpora of text is very, very, very large, we also call them large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=159" target="_blank">00:02:39.120</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=160" target="_blank">00:02:40.120</a></span> | <span class="t">There are many examples of large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=161" target="_blank">00:02:41.920</a></span> | <span class="t">For example, we have Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=163" target="_blank">00:02:43.440</a></span> | <span class="t">For example, we have GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=165" target="_blank">00:02:45.200</a></span> | <span class="t">They are also called foundation models because they have been pre-trained on a very big corpora</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=171" target="_blank">00:02:51.120</a></span> | <span class="t">of text, for example, the entire Wikipedia or billions of pages from the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=176" target="_blank">00:02:56.620</a></span> | <span class="t">And then we can use them with prompting or with fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=181" target="_blank">00:03:01.360</a></span> | <span class="t">Later we will see how do we do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=184" target="_blank">00:03:04.240</a></span> | <span class="t">Okay, let's review how we train a large language model or a language model in general, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=191" target="_blank">00:03:11.160</a></span> | <span class="t">So the training of a large language model involves that we have some corpora, so a piece</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=196" target="_blank">00:03:16.800</a></span> | <span class="t">of text, which could be the entire Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=199" target="_blank">00:03:19.920</a></span> | <span class="t">It would be web pages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=201" target="_blank">00:03:21.720</a></span> | <span class="t">It could be just one book or it could be anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=204" target="_blank">00:03:24.240</a></span> | <span class="t">In my example, imagine we want to train a large language model or a large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=209" target="_blank">00:03:29.240</a></span> | <span class="t">on Chinese poems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=211" target="_blank">00:03:31.040</a></span> | <span class="t">And suppose we only have one poem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=214" target="_blank">00:03:34.320</a></span> | <span class="t">This one, this is a very famous poem from Li Bai.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=216" target="_blank">00:03:36.920</a></span> | <span class="t">It's one of the first poems that you learn if you want to study Chinese literature or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=221" target="_blank">00:03:41.920</a></span> | <span class="t">the Chinese language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=223" target="_blank">00:03:43.320</a></span> | <span class="t">So we will concentrate only on the following line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=227" target="_blank">00:03:47.080</a></span> | <span class="t">So before my bed lies a pool of moon bright.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=231" target="_blank">00:03:51.160</a></span> | <span class="t">Let's see how to train the large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=233" target="_blank">00:03:53.400</a></span> | <span class="t">Well, the first thing we do is we create a sequence of the line that we want to teach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=239" target="_blank">00:03:59.200</a></span> | <span class="t">to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=240" target="_blank">00:04:00.200</a></span> | <span class="t">So let's start with the first line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=242" target="_blank">00:04:02.680</a></span> | <span class="t">We create a sentence to which we prepend one token called start of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=249" target="_blank">00:04:09.000</a></span> | <span class="t">This sentence or input, which is made up of tokens, in this case, in our very simple case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=256" target="_blank">00:04:16.360</a></span> | <span class="t">we can consider that each word is a token, but this is not always the case because depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=260" target="_blank">00:04:20.600</a></span> | <span class="t">on the tokenizer, each word may be split into multiple tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=265" target="_blank">00:04:25.840</a></span> | <span class="t">But suppose for simplicity that our tokenizer always takes one word as one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=271" target="_blank">00:04:31.520</a></span> | <span class="t">So we feed this input sequence to our neural network, which is modeling the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=277" target="_blank">00:04:37.000</a></span> | <span class="t">Usually it's the trans, the encoder part of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=279" target="_blank">00:04:39.840</a></span> | <span class="t">So only this part here, along with the linear and the softmax, for example, like llama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=287" target="_blank">00:04:47.160</a></span> | <span class="t">Then we, this transformer encoder will output a sequence of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=291" target="_blank">00:04:51.600</a></span> | <span class="t">So transformer is a sequence to sequence model, means that if you give it a sequence of 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=296" target="_blank">00:04:56.960</a></span> | <span class="t">tokens, it will output a sequence of 10 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=302" target="_blank">00:05:02.280</a></span> | <span class="t">When we train a model, we all, we have an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=304" target="_blank">00:05:04.880</a></span> | <span class="t">We have also a target.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=306" target="_blank">00:05:06.560</a></span> | <span class="t">What is, that is what we want the model to output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=311" target="_blank">00:05:11.340</a></span> | <span class="t">So we want the model to output the same sentence, but without prepending anything, but by appending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=317" target="_blank">00:05:17.280</a></span> | <span class="t">one last token called end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=320" target="_blank">00:05:20.080</a></span> | <span class="t">So the total length is still 10 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=322" target="_blank">00:05:22.960</a></span> | <span class="t">So it's the same sentence as before, but instead of having a token at the beginning, it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=327" target="_blank">00:05:27.640</a></span> | <span class="t">a token at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=329" target="_blank">00:05:29.200</a></span> | <span class="t">This token is called end of sentence token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=332" target="_blank">00:05:32.400</a></span> | <span class="t">Now let's review why do we need this start of sentence token and end of sentence token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=337" target="_blank">00:05:37.360</a></span> | <span class="t">Because as I said before, the neural network that we are using, which is a transform is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=341" target="_blank">00:05:41.520</a></span> | <span class="t">a sequence to sequence model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=343" target="_blank">00:05:43.280</a></span> | <span class="t">It means that if we have an input of N tokens as input, it will produce N tokens as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=349" target="_blank">00:05:49.600</a></span> | <span class="t">So if, for example, we give this neural network only the first token, so the start of sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=355" target="_blank">00:05:55.800</a></span> | <span class="t">it will only output one token as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=358" target="_blank">00:05:58.520</a></span> | <span class="t">So in case it has already been trained, it should output the first token of the target.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=363" target="_blank">00:06:03.320</a></span> | <span class="t">So before, if we, let me switch to the pen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=368" target="_blank">00:06:08.920</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=369" target="_blank">00:06:09.960</a></span> | <span class="t">If we input the first two tokens, for example, start of sentence before it should output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=375" target="_blank">00:06:15.680</a></span> | <span class="t">the first two tokens of the target before my, if we input the first three tokens, start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=383" target="_blank">00:06:23.880</a></span> | <span class="t">of sentence before my, it should output the first three tokens of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=388" target="_blank">00:06:28.560</a></span> | <span class="t">So before my bed, as you can see, every time we give it an input, the last token of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=393" target="_blank">00:06:33.800</a></span> | <span class="t">output, in case it has already been trained and it's matching the target is the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=400" target="_blank">00:06:40.080</a></span> | <span class="t">that we need to complete the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=402" target="_blank">00:06:42.160</a></span> | <span class="t">So for example, if we give only the first two tokens, start of sentence before the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=407" target="_blank">00:06:47.840</a></span> | <span class="t">outputs before my, so it will give us the next token after before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=412" target="_blank">00:06:52.840</a></span> | <span class="t">If we give it start of sentence before my, it will output before my bed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=418" target="_blank">00:06:58.220</a></span> | <span class="t">So the next token after the word my, so bed, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=422" target="_blank">00:07:02.520</a></span> | <span class="t">So every time we give the model a token and the model will return the next token as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=427" target="_blank">00:07:07.920</a></span> | <span class="t">last token in the output sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=430" target="_blank">00:07:10.440</a></span> | <span class="t">And this is how we train a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=433" target="_blank">00:07:13.400</a></span> | <span class="t">So once we have our target token and the output, we compute the loss, which is the cross entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=438" target="_blank">00:07:18.840</a></span> | <span class="t">loss and we run back propagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=442" target="_blank">00:07:22.800</a></span> | <span class="t">Now let's review how we inference from a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=447" target="_blank">00:07:27.080</a></span> | <span class="t">So imagine you are a student who had to memorize Leiby's poem, but you only remember the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">two words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=454" target="_blank">00:07:34.000</a></span> | <span class="t">How do you survive an exam?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=456" target="_blank">00:07:36.360</a></span> | <span class="t">So you only remember the first two words of the first line of the poem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=462" target="_blank">00:07:42.020</a></span> | <span class="t">What you could do, imagine you already have a language model that has been trained on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=465" target="_blank">00:07:45.500</a></span> | <span class="t">Chinese poem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=466" target="_blank">00:07:46.500</a></span> | <span class="t">You could ask the language model to write the rest of the poem for you, but of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=470" target="_blank">00:07:50.580</a></span> | <span class="t">you need to give the language model some input on what you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=474" target="_blank">00:07:54.060</a></span> | <span class="t">That input is called the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=475" target="_blank">00:07:55.920</a></span> | <span class="t">So you tell the language model the first two tokens and the model will come up with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=480" target="_blank">00:08:00.760</a></span> | <span class="t">following tokens that make up the poem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=485" target="_blank">00:08:05.580</a></span> | <span class="t">Let's review the poem again, which is before my bed lies a pool of moon bright.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=489" target="_blank">00:08:09.880</a></span> | <span class="t">So let's start our inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=492" target="_blank">00:08:12.600</a></span> | <span class="t">We give the model of our first two tokens by prepending the start of sentence token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=498" target="_blank">00:08:18.520</a></span> | <span class="t">and we feed it to the neural network, which has been already pre-trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=503" target="_blank">00:08:23.680</a></span> | <span class="t">Then the model should output before my bed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=507" target="_blank">00:08:27.040</a></span> | <span class="t">We take this last token, we append it to the input and we give it back to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=513" target="_blank">00:08:33.200</a></span> | <span class="t">So now we give it before my bed and we feed it again to the model and the model will output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=519" target="_blank">00:08:39.000</a></span> | <span class="t">the next token, which is lies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=521" target="_blank">00:08:41.320</a></span> | <span class="t">We take this last token lies, we append it again to the input and we feed it again to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=526" target="_blank">00:08:46.780</a></span> | <span class="t">the transformer model and the model will output the next token of the line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=532" target="_blank">00:08:52.280</a></span> | <span class="t">And then we keep doing like this until we arrive to the end of the line or the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=537" target="_blank">00:08:57.760</a></span> | <span class="t">the poem, which is indicated by the end of sentence token, depending on how we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=544" target="_blank">00:09:04.000</a></span> | <span class="t">the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=545" target="_blank">00:09:05.000</a></span> | <span class="t">In our case, suppose we only trained it on the first line of this poem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=549" target="_blank">00:09:09.480</a></span> | <span class="t">And this is how we inference from a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=552" target="_blank">00:09:12.280</a></span> | <span class="t">In a language model like Lama, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=555" target="_blank">00:09:15.240</a></span> | <span class="t">Now let's understand the architecture of the transformer model, at least the part that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=560" target="_blank">00:09:20.000</a></span> | <span class="t">we are interested in, which is the encoder, because this is the architecture that is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=565" target="_blank">00:09:25.200</a></span> | <span class="t">used in BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=566" target="_blank">00:09:26.400</a></span> | <span class="t">So we need to understand the building blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=568" target="_blank">00:09:28.500</a></span> | <span class="t">Even if you have already watched the transformer model, my video on the transformer model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=572" target="_blank">00:09:32.480</a></span> | <span class="t">let's review all this concept again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=575" target="_blank">00:09:35.200</a></span> | <span class="t">So as I said before, this is the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=580" target="_blank">00:09:40.680</a></span> | <span class="t">So this is the vanilla transformer, the transformer model as presented in the original paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=585" target="_blank">00:09:45.880</a></span> | <span class="t">Attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=587" target="_blank">00:09:47.400</a></span> | <span class="t">In most language models are actually modeled just using the encoder side or the decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=592" target="_blank">00:09:52.520</a></span> | <span class="t">side, depending on the application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=595" target="_blank">00:09:55.080</a></span> | <span class="t">And they have the last linear layer to project back the tokens in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=600" target="_blank">00:10:00.880</a></span> | <span class="t">Let's review all these building blocks of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=604" target="_blank">00:10:04.000</a></span> | <span class="t">I will not review actually all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=605" target="_blank">00:10:05.720</a></span> | <span class="t">So I will not review the normalization, the linear layer, or the feed forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=609" target="_blank">00:10:09.760</a></span> | <span class="t">Because this, I hope you're already familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=612" target="_blank">00:10:12.120</a></span> | <span class="t">What I am actually interested in is the input embeddings, the positional encodings, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=615" target="_blank">00:10:15.640</a></span> | <span class="t">the multi-head attention, for which I will only actually do the case of the single head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=621" target="_blank">00:10:21.360</a></span> | <span class="t">because it's easier to visualize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=623" target="_blank">00:10:23.360</a></span> | <span class="t">So if you want to have more information, please watch my previous video on the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=626" target="_blank">00:10:26.600</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=627" target="_blank">00:10:27.600</a></span> | <span class="t">Now, let's review the embedding vectors, what they are and how do we use them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=633" target="_blank">00:10:33.160</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=634" target="_blank">00:10:34.160</a></span> | <span class="t">Usually when we train a language model or we inference a language model, we use a prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=638" target="_blank">00:10:38.920</a></span> | <span class="t">or some input for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=641" target="_blank">00:10:41.560</a></span> | <span class="t">This input is a text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=643" target="_blank">00:10:43.160</a></span> | <span class="t">What the first thing we do is we split this text into tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=647" target="_blank">00:10:47.400</a></span> | <span class="t">In our simple case, we will do a very simple tokenization in which we split each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=653" target="_blank">00:10:53.680</a></span> | <span class="t">Each word becomes a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=655" target="_blank">00:10:55.560</a></span> | <span class="t">This is actually not always the case with language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=658" target="_blank">00:10:58.660</a></span> | <span class="t">For example, in Lama or in other language models, we use the BPE tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=663" target="_blank">00:11:03.940</a></span> | <span class="t">In BERT, we will see, we use the word piece tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=666" target="_blank">00:11:06.920</a></span> | <span class="t">So each word can become multiple tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=670" target="_blank">00:11:10.060</a></span> | <span class="t">In our simple case, we just pretend that each word is actually a token with some tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=676" target="_blank">00:11:16.000</a></span> | <span class="t">actually not mapping to words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=677" target="_blank">00:11:17.520</a></span> | <span class="t">For example, the start of sentence token is a special token that exists only virtually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=682" target="_blank">00:11:22.040</a></span> | <span class="t">It's not actually part of the training text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=684" target="_blank">00:11:24.840</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=685" target="_blank">00:11:25.920</a></span> | <span class="t">The first thing we do is we do this tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=688" target="_blank">00:11:28.120</a></span> | <span class="t">So each word becomes a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=690" target="_blank">00:11:30.320</a></span> | <span class="t">We map each token into its position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=694" target="_blank">00:11:34.120</a></span> | <span class="t">So imagine you have very big corpora of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=697" target="_blank">00:11:37.160</a></span> | <span class="t">This text is made up of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=699" target="_blank">00:11:39.440</a></span> | <span class="t">Each word will occupy a position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=702" target="_blank">00:11:42.960</a></span> | <span class="t">So we map each word to its position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=706" target="_blank">00:11:46.440</a></span> | <span class="t">In this case, each token into its position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=711" target="_blank">00:11:51.000</a></span> | <span class="t">Then we map each of these numbers, which are the position of the token in the vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=717" target="_blank">00:11:57.040</a></span> | <span class="t">to an embedding vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=721" target="_blank">00:12:01.120</a></span> | <span class="t">Now this size of size 512 is the one used in the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=727" target="_blank">00:12:07.400</a></span> | <span class="t">We will see that in BERT the size is 768, if I'm not mistaken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=733" target="_blank">00:12:13.000</a></span> | <span class="t">But for now, I will only always refer to the configuration of the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=738" target="_blank">00:12:18.360</a></span> | <span class="t">So the transformer as presented in the attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=742" target="_blank">00:12:22.640</a></span> | <span class="t">So each of these input IDs, which is the position of the token of each word is projected into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=748" target="_blank">00:12:28.560</a></span> | <span class="t">an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=749" target="_blank">00:12:29.560</a></span> | <span class="t">This embedding is a vector of size 512 that captures the meaning of each token, in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=756" target="_blank">00:12:36.360</a></span> | <span class="t">case of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=758" target="_blank">00:12:38.560</a></span> | <span class="t">But why do we use embedding vectors to capture the meaning of each token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">Let's review.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=767" target="_blank">00:12:47.000</a></span> | <span class="t">For example, given the word "cherry", "digital" and "information", the idea is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=773" target="_blank">00:12:53.560</a></span> | <span class="t">Imagine we live in a very simple world in which the embedding vector is not made up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=777" target="_blank">00:12:57.240</a></span> | <span class="t">of 512 dimensions, but only two dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=781" target="_blank">00:13:01.080</a></span> | <span class="t">So we can project these vectors on the XY plane.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=786" target="_blank">00:13:06.120</a></span> | <span class="t">If we project them, and if the embedding vectors have been trained correctly, we will see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=791" target="_blank">00:13:11.040</a></span> | <span class="t">the words with similar meaning will point to the same direction in space, while words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=796" target="_blank">00:13:16.720</a></span> | <span class="t">with different meaning will point to different directions in space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=800" target="_blank">00:13:20.160</a></span> | <span class="t">For example, the word "digital" and "information", because they capture the same kind of semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=807" target="_blank">00:13:27.040</a></span> | <span class="t">meaning "information", they will point to similar directions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=812" target="_blank">00:13:32.040</a></span> | <span class="t">And we can measure this similarity by measuring the angle between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=816" target="_blank">00:13:36.080</a></span> | <span class="t">So for example, the angle between "digital" and "information" is very small, you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=819" target="_blank">00:13:39.840</a></span> | <span class="t">here, while the angle between "cherry" and "digital" is quite big, because they represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=824" target="_blank">00:13:44.880</a></span> | <span class="t">different semantic groups, so they have different meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=828" target="_blank">00:13:48.640</a></span> | <span class="t">Imagine there is also another word called "tomato".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=830" target="_blank">00:13:50.720</a></span> | <span class="t">We expect the word "tomato" to point to the vertical direction, very similar to the "cherry",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=835" target="_blank">00:13:55.120</a></span> | <span class="t">for example, here it may be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=837" target="_blank">00:13:57.560</a></span> | <span class="t">So that the angle between "cherry" and "tomato" is very small, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=842" target="_blank">00:14:02.280</a></span> | <span class="t">And we measure this angle between vectors using the cosine similarity, which is based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=847" target="_blank">00:14:07.360</a></span> | <span class="t">on the dot product between two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=849" target="_blank">00:14:09.880</a></span> | <span class="t">And we will see that this dot product is very important, because we will use it in the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=854" target="_blank">00:14:14.360</a></span> | <span class="t">mechanism that we will see later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=857" target="_blank">00:14:17.060</a></span> | <span class="t">Ok, now let's review the positional encodings as presented in the original paper of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=864" target="_blank">00:14:24.920</a></span> | <span class="t">transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=866" target="_blank">00:14:26.920</a></span> | <span class="t">We need to give some positional information to our model, because now we only gave some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=872" target="_blank">00:14:32.120</a></span> | <span class="t">vectors that represent the meaning of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=876" target="_blank">00:14:36.520</a></span> | <span class="t">But we also need to tell the model that this particular word is in the position 1 in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=881" target="_blank">00:14:41.160</a></span> | <span class="t">sentence, and this particular word is in position 2 in the sentence, etc. etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=885" target="_blank">00:14:45.120</a></span> | <span class="t">And this is the job of the positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=887" target="_blank">00:14:47.400</a></span> | <span class="t">Let's see how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=889" target="_blank">00:14:49.360</a></span> | <span class="t">So we start with our original sentence, which is the first line of the Chinese poem we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=893" target="_blank">00:14:53.800</a></span> | <span class="t">before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=895" target="_blank">00:14:55.020</a></span> | <span class="t">We convert it into embedding vectors of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=899" target="_blank">00:14:59.200</a></span> | <span class="t">Then each of these embedding vectors, we add another vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=903" target="_blank">00:15:03.240</a></span> | <span class="t">This is called the positional encoding or positional embedding, I saw both names used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=909" target="_blank">00:15:09.160</a></span> | <span class="t">And this position embedding actually indicates the position of this particular token inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=915" target="_blank">00:15:15.520</a></span> | <span class="t">the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=916" target="_blank">00:15:16.520</a></span> | <span class="t">And this vector here indicates the position 1 of this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=920" target="_blank">00:15:20.800</a></span> | <span class="t">And this indicates the position 2 and the position 3 and position 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=924" target="_blank">00:15:24.360</a></span> | <span class="t">Now, actually, this position embedding, at least in the vanilla transformer, they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=929" target="_blank">00:15:29.000</a></span> | <span class="t">computed once and they are reused for every sentence during training and inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=934" target="_blank">00:15:34.000</a></span> | <span class="t">So they are not specific for this particular token, but they are only specific for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=939" target="_blank">00:15:39.000</a></span> | <span class="t">particular position, which means that every token in the position 0 or every token in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=944" target="_blank">00:15:44.920</a></span> | <span class="t">position 1 will receive this particular vector added to it that represents the position number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=950" target="_blank">00:15:50.400</a></span> | <span class="t">1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=951" target="_blank">00:15:51.960</a></span> | <span class="t">So the result of this addition is these vectors here, which will become the input of the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=958" target="_blank">00:15:58.500</a></span> | <span class="t">and that we will see it later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=961" target="_blank">00:16:01.600</a></span> | <span class="t">How do we compute these positional encodings, at least as presented in the original transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=966" target="_blank">00:16:06.200</a></span> | <span class="t">paper?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=967" target="_blank">00:16:07.200</a></span> | <span class="t">Well, suppose we have a sentence made up of three words or three tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=972" target="_blank">00:16:12.800</a></span> | <span class="t">We have seen these formulas before from the paper "Attention is all you need".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=977" target="_blank">00:16:17.280</a></span> | <span class="t">We create a vector of size 512 and for the even dimensions of this vector, we use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=983" target="_blank">00:16:23.080</a></span> | <span class="t">first formula and for the odd dimensions, we use the second formula, in which the arguments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=988" target="_blank">00:16:28.640</a></span> | <span class="t">of these two formulas is, the first one is a pause, which indicates the position of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=994" target="_blank">00:16:34.200</a></span> | <span class="t">word inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=995" target="_blank">00:16:35.560</a></span> | <span class="t">So for the first token, it's 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=998" target="_blank">00:16:38.200</a></span> | <span class="t">And 2i indicates the dimension of this vector to which we are applying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1003" target="_blank">00:16:43.920</a></span> | <span class="t">And we can compute it also for the second vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1006" target="_blank">00:16:46.280</a></span> | <span class="t">For the third position, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1008" target="_blank">00:16:48.040</a></span> | <span class="t">If we have another sentence that is different, for example, I love you, which is also made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1012" target="_blank">00:16:52.080</a></span> | <span class="t">up of three tokens, we will reuse the same vectors as the other sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1018" target="_blank">00:16:58.200</a></span> | <span class="t">So this particular vector here is associated with the position 0, not with the token before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1024" target="_blank">00:17:04.400</a></span> | <span class="t">So if we have another token, we will reuse the same vector for the position 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1031" target="_blank">00:17:11.920</a></span> | <span class="t">Ok, now let's review what is the self-attention that we use in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1039" target="_blank">00:17:19.480</a></span> | <span class="t">Because language models need to find a way to relate tokens with each other so that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1044" target="_blank">00:17:24.360</a></span> | <span class="t">can compute some kind of interactions between tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1049" target="_blank">00:17:29.120</a></span> | <span class="t">So for example, tokens have a meaning not by themselves, but by the way they are present</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1055" target="_blank">00:17:35.240</a></span> | <span class="t">inside of the sentence and their relationship with other tokens inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1060" target="_blank">00:17:40.840</a></span> | <span class="t">And this is the job of the self-attention, which is done here in the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1065" target="_blank">00:17:45.420</a></span> | <span class="t">We will not see the multi-head attention, we will see the single head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1069" target="_blank">00:17:49.160</a></span> | <span class="t">So let's start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1070" target="_blank">00:17:50.920</a></span> | <span class="t">Now let's build the input for this self-attention because for now we have worked with independent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1078" target="_blank">00:17:58.320</a></span> | <span class="t">tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1079" target="_blank">00:17:59.320</a></span> | <span class="t">So we took this token and we converted it into a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1083" target="_blank">00:18:03.040</a></span> | <span class="t">Then we added the positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1086" target="_blank">00:18:06.280</a></span> | <span class="t">We first converted it into embedding, then we added the positional encoding to capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1090" target="_blank">00:18:10.720</a></span> | <span class="t">the position information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1092" target="_blank">00:18:12.920</a></span> | <span class="t">But actually I lied to you by telling you that we work independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1097" target="_blank">00:18:17.520</a></span> | <span class="t">Actually when we code the transformer, we always work with the matrix form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1101" target="_blank">00:18:21.720</a></span> | <span class="t">So all these tokens, all these vectors are never alone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1106" target="_blank">00:18:26.000</a></span> | <span class="t">They are always in a big matrix that contains all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1109" target="_blank">00:18:29.480</a></span> | <span class="t">So now we create this big matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1111" target="_blank">00:18:31.240</a></span> | <span class="t">But before it was not easy to work with a big matrix directly because it's not easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1116" target="_blank">00:18:36.040</a></span> | <span class="t">to visualize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1117" target="_blank">00:18:37.040</a></span> | <span class="t">So now we create a big matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1118" target="_blank">00:18:38.360</a></span> | <span class="t">So we combine all these vectors in a one big matrix here in which each row is one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1126" target="_blank">00:18:46.080</a></span> | <span class="t">vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1127" target="_blank">00:18:47.080</a></span> | <span class="t">So for example, the first vector here becomes the first row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1130" target="_blank">00:18:50.560</a></span> | <span class="t">The second vector here becomes the second row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1134" target="_blank">00:18:54.060</a></span> | <span class="t">The third vector here becomes the third row, et cetera, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1138" target="_blank">00:18:58.600</a></span> | <span class="t">The shape of this matrix is 10 by 512 because we have 10 tokens and each token is represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1145" target="_blank">00:19:05.680</a></span> | <span class="t">by a vector of 512 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1149" target="_blank">00:19:09.560</a></span> | <span class="t">We take this matrix and we make three copies of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1153" target="_blank">00:19:13.040</a></span> | <span class="t">So three identical copies of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1155" target="_blank">00:19:15.920</a></span> | <span class="t">The first one, we will call it query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1158" target="_blank">00:19:18.040</a></span> | <span class="t">The second one, we will call it key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1160" target="_blank">00:19:20.120</a></span> | <span class="t">And the third one, we will call it value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1162" target="_blank">00:19:22.040</a></span> | <span class="t">As you can see, the values in this matrix are all the same because there are three identical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1165" target="_blank">00:19:25.920</a></span> | <span class="t">copies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1167" target="_blank">00:19:27.680</a></span> | <span class="t">Why we use three identical copies?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1169" target="_blank">00:19:29.480</a></span> | <span class="t">Because this is the self-attention mechanism, which means that we relate tokens to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1174" target="_blank">00:19:34.720</a></span> | <span class="t">other, tokens that belong to the same sentence with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1179" target="_blank">00:19:39.600</a></span> | <span class="t">If we relate tokens of two different sentences or from two different languages, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1184" target="_blank">00:19:44.840</a></span> | <span class="t">when we are doing a language translation, in that case, we will talk about cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1189" target="_blank">00:19:49.840</a></span> | <span class="t">In this case, we talk about self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1191" target="_blank">00:19:51.880</a></span> | <span class="t">And this is the kind of attention that is used in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1195" target="_blank">00:19:55.960</a></span> | <span class="t">OK, the self-attention mechanism, as you have seen, probably in the paper, works with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1202" target="_blank">00:20:02.320</a></span> | <span class="t">formula here, which is the attention is calculated as the softmax of the query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1207" target="_blank">00:20:07.680</a></span> | <span class="t">the transpose of the keys divided by the square root of dk, then multiplied by b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1214" target="_blank">00:20:14.160</a></span> | <span class="t">What is this dk here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1216" target="_blank">00:20:16.080</a></span> | <span class="t">The dk here actually represents the dimension of the vector of each head in case of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1222" target="_blank">00:20:22.000</a></span> | <span class="t">multi-head attention, because we are actually simplifying our scenario and working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1226" target="_blank">00:20:26.760</a></span> | <span class="t">only one head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1227" target="_blank">00:20:27.760</a></span> | <span class="t">In our case, dk corresponds to d model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1230" target="_blank">00:20:30.840</a></span> | <span class="t">So that is the size of the embedding vector that we created before, which is 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1238" target="_blank">00:20:38.200</a></span> | <span class="t">So we take our matrix, the one we built before, so the query, so 10 by 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1243" target="_blank">00:20:43.600</a></span> | <span class="t">We multiply it by the transpose of the keys, which becomes a 512 by 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1248" target="_blank">00:20:48.960</a></span> | <span class="t">So they are basically the identical matrix, but one is transposed and one is not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1254" target="_blank">00:20:54.760</a></span> | <span class="t">We divide it by the square root of 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1258" target="_blank">00:20:58.280</a></span> | <span class="t">We apply the softmax and this will produce the following matrix we can see here, in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1264" target="_blank">00:21:04.040</a></span> | <span class="t">each value is the softmax of the dot product of one token with another token of the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1273" target="_blank">00:21:13.280</a></span> | <span class="t">of one token with another token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1275" target="_blank">00:21:15.480</a></span> | <span class="t">For example, let's visualize with some vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1280" target="_blank">00:21:20.220</a></span> | <span class="t">So we built the matrix before, which is made up of 10 rows because each row is a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1289" target="_blank">00:21:29.460</a></span> | <span class="t">and each row contains 512 numbers because it's a vector of 512 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1294" target="_blank">00:21:34.380</a></span> | <span class="t">So the dimension one up to 512, then the dimension one up to 512 and we have 10 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1304" target="_blank">00:21:44.320</a></span> | <span class="t">This is the transpose of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1306" target="_blank">00:21:46.120</a></span> | <span class="t">So we will have not 10 rows, but we will have 10 column of vectors with the dimension one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1313" target="_blank">00:21:53.140</a></span> | <span class="t">up to 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1315" target="_blank">00:21:55.720</a></span> | <span class="t">Then we have another column vector here with the dimension one and then 512, etc, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1323" target="_blank">00:22:03.860</a></span> | <span class="t">We have another one, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1326" target="_blank">00:22:06.080</a></span> | <span class="t">So this value here is the dot product of the first row of the first matrix with the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1333" target="_blank">00:22:13.440</a></span> | <span class="t">column of the first matrix, which is the embedding of the first token, which if you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1339" target="_blank">00:22:19.040</a></span> | <span class="t">is the start of sentence with the embedding of the token start of sentence and it's this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1346" target="_blank">00:22:26.480</a></span> | <span class="t">value here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1348" target="_blank">00:22:28.360</a></span> | <span class="t">Then this value here, it's the dot product of the embedding of the start of sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1353" target="_blank">00:22:33.960</a></span> | <span class="t">with the second token, which is this one here and it's the token before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1358" target="_blank">00:22:38.980</a></span> | <span class="t">So this is before, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1362" target="_blank">00:22:42.280</a></span> | <span class="t">Then we apply the Softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1363" target="_blank">00:22:43.840</a></span> | <span class="t">The Softmax basically changes the values in this matrix in such a way that they sum up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1368" target="_blank">00:22:48.560</a></span> | <span class="t">to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1369" target="_blank">00:22:49.560</a></span> | <span class="t">So each row in this matrix, this row for example here, sums up to one and also this row here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1376" target="_blank">00:22:56.440</a></span> | <span class="t">sums up to one, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1380" target="_blank">00:23:00.560</a></span> | <span class="t">As you can see here, this word start of sentence is able to relate to the word before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1391" target="_blank">00:23:11.000</a></span> | <span class="t">And this is not what we want, because as I said before, our goal is to model a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1399" target="_blank">00:23:19.260</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1400" target="_blank">00:23:20.260</a></span> | <span class="t">That is, a language model is a probabilistic model that assigns probability to sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1404" target="_blank">00:23:24.600</a></span> | <span class="t">of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1405" target="_blank">00:23:25.760</a></span> | <span class="t">That is, we want to calculate the probability of the word China being the next word in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1411" target="_blank">00:23:31.200</a></span> | <span class="t">sentence Shanghai is a city in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1414" target="_blank">00:23:34.760</a></span> | <span class="t">That is, we want to condition the word China only on the words that come before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1420" target="_blank">00:23:40.680</a></span> | <span class="t">That is, Shanghai is a city in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1422" target="_blank">00:23:42.960</a></span> | <span class="t">So our model should only be able to watch this part of the sentence to predict the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1428" target="_blank">00:23:48.520</a></span> | <span class="t">token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1429" target="_blank">00:23:49.760</a></span> | <span class="t">This is also called the left context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1432" target="_blank">00:23:52.520</a></span> | <span class="t">But this is not what is happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1435" target="_blank">00:23:55.360</a></span> | <span class="t">Because we are able to relate tokens that also come in the future with tokens that come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1440" target="_blank">00:24:00.920</a></span> | <span class="t">in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1441" target="_blank">00:24:01.920</a></span> | <span class="t">So for example, the word SOS is being related with the token before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1446" target="_blank">00:24:06.600</a></span> | <span class="t">And the token SOS is also being related with the token my, even if the token my comes after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1451" target="_blank">00:24:11.640</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1453" target="_blank">00:24:13.040</a></span> | <span class="t">So what we do, we basically need to introduce the causal mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1458" target="_blank">00:24:18.120</a></span> | <span class="t">Let's see how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1460" target="_blank">00:24:20.960</a></span> | <span class="t">The causal mask works like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1463" target="_blank">00:24:23.720</a></span> | <span class="t">We take the metrics that we saw before with all the attention scores and all the interactions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1469" target="_blank">00:24:29.240</a></span> | <span class="t">that are not causal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1470" target="_blank">00:24:30.660</a></span> | <span class="t">So all the interaction of words with the words that come on its right are replaced with minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1477" target="_blank">00:24:37.120</a></span> | <span class="t">infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1478" target="_blank">00:24:38.120</a></span> | <span class="t">For example, start of sentence should not be able to relate to the word before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1484" target="_blank">00:24:44.160</a></span> | <span class="t">So we replace the interaction with minus infinity before we apply the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1490" target="_blank">00:24:50.120</a></span> | <span class="t">And then also, for example, the word before should not be able to watch the word my bed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1495" target="_blank">00:24:55.000</a></span> | <span class="t">lies pool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1496" target="_blank">00:24:56.140</a></span> | <span class="t">So all these interactions are also replaced with minus infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1498" target="_blank">00:24:58.900</a></span> | <span class="t">So basically all the values above the principal diagonal that you can see here are replaced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1504" target="_blank">00:25:04.100</a></span> | <span class="t">with minus infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1506" target="_blank">00:25:06.620</a></span> | <span class="t">Then we apply the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1508" target="_blank">00:25:08.760</a></span> | <span class="t">And if you remember the formula for the softmax, you can see here it's on the numerator is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1513" target="_blank">00:25:13.240</a></span> | <span class="t">e to the power of z i, which is the item to which you are applying the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1520" target="_blank">00:25:20.140</a></span> | <span class="t">And e to the power of minus infinity will become zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1523" target="_blank">00:25:23.080</a></span> | <span class="t">So basically we replace them with minus infinity so that when we apply the softmax, they will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1527" target="_blank">00:25:27.680</a></span> | <span class="t">be replaced with zero by the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1532" target="_blank">00:25:32.060</a></span> | <span class="t">This way the model will not have access to any information of interactions between the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1538" target="_blank">00:25:38.480</a></span> | <span class="t">word start of sentence and all the tokens that come after it because we replace them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1544" target="_blank">00:25:44.260</a></span> | <span class="t">with zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1545" target="_blank">00:25:45.260</a></span> | <span class="t">So even there is some kind of connection between this token, the model will not be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1549" target="_blank">00:25:49.140</a></span> | <span class="t">learn it because we never give this information to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1553" target="_blank">00:25:53.020</a></span> | <span class="t">And this is how the model becomes causal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1555" target="_blank">00:25:55.040</a></span> | <span class="t">The only token that is able to watch all the previous token is this bright token here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1559" target="_blank">00:25:59.580</a></span> | <span class="t">So the last token, because this token can see all the tokens that come before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1564" target="_blank">00:26:04.140</a></span> | <span class="t">That's why this line has no zero here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1568" target="_blank">00:26:08.220</a></span> | <span class="t">Okay, in the formula of the attention, we also have a multiplication with v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1573" target="_blank">00:26:13.900</a></span> | <span class="t">So we take the output of the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1576" target="_blank">00:26:16.420</a></span> | <span class="t">So the matrix that we saw before, we apply the causal mask, and then we multiply with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1580" target="_blank">00:26:20.860</a></span> | <span class="t">v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1581" target="_blank">00:26:21.860</a></span> | <span class="t">And this way you will understand why we apply the causal mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1586" target="_blank">00:26:26.180</a></span> | <span class="t">So let's review the shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1588" target="_blank">00:26:28.140</a></span> | <span class="t">This matrix here is a 10 by 10 matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1591" target="_blank">00:26:31.300</a></span> | <span class="t">And this matrix v is the initial matrix that we built with our vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1596" target="_blank">00:26:36.740</a></span> | <span class="t">So it's 10 by 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1598" target="_blank">00:26:38.620</a></span> | <span class="t">So it's the value matrix, one of the three identical copies that we made before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1603" target="_blank">00:26:43.260</a></span> | <span class="t">The multiplication between these two metrics will produce an output matrix that is 10 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1607" target="_blank">00:26:47.300</a></span> | <span class="t">512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1609" target="_blank">00:26:49.620</a></span> | <span class="t">Let's see how the output works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1612" target="_blank">00:26:52.260</a></span> | <span class="t">Okay, so this is a matrix made of rows of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1618" target="_blank">00:26:58.380</a></span> | <span class="t">So the first row is a vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1621" target="_blank">00:27:01.620</a></span> | <span class="t">The second row is a vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1624" target="_blank">00:27:04.860</a></span> | <span class="t">The third is also a vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1627" target="_blank">00:27:07.800</a></span> | <span class="t">So with 512 dimensions here, this one also have dimension one dimension two dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1634" target="_blank">00:27:14.180</a></span> | <span class="t">three, 512 dimension one dimension two dimension three, 512, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1641" target="_blank">00:27:21.420</a></span> | <span class="t">The output token will also have the same shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1644" target="_blank">00:27:24.220</a></span> | <span class="t">So it will be 10 by 512, which means we have 10 vectors of dimension 512 dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1649" target="_blank">00:27:29.860</a></span> | <span class="t">So the dimension one dimension two dimension three, up to 512 dimension one, two, three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1659" target="_blank">00:27:39.160</a></span> | <span class="t">up to 512, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1662" target="_blank">00:27:42.620</a></span> | <span class="t">Now let's do this product by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1665" target="_blank">00:27:45.220</a></span> | <span class="t">To get this first value here of this matrix, so the dimension one of the first vector of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1670" target="_blank">00:27:50.940</a></span> | <span class="t">the attention output matrix here, this value here is the dot product of the first row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1678" target="_blank">00:27:58.380</a></span> | <span class="t">So this row here, all this row with the first column of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1686" target="_blank">00:28:06.020</a></span> | <span class="t">So the first dimension of the embedding of each token in our input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1692" target="_blank">00:28:12.220</a></span> | <span class="t">But as you can see, because of the causal mask, most of the values in this matrix are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1697" target="_blank">00:28:17.100</a></span> | <span class="t">zero, as you can see here, this means that the output value here will be only be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1703" target="_blank">00:28:23.800</a></span> | <span class="t">to watch the first dimension of the first token, which also means that in the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1708" target="_blank">00:28:28.860</a></span> | <span class="t">of the attention, the first token will only be able to attend only itself, as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1715" target="_blank">00:28:35.420</a></span> | <span class="t">see, not the values that come after it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1719" target="_blank">00:28:39.140</a></span> | <span class="t">Let's look at the second, for example, this output here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1722" target="_blank">00:28:42.280</a></span> | <span class="t">So the first dimension of the second row of this matrix attention output here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1728" target="_blank">00:28:48.940</a></span> | <span class="t">So this value here comes from the second row of the initial matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1733" target="_blank">00:28:53.200</a></span> | <span class="t">So this one here multiplied by the first column of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1738" target="_blank">00:28:58.540</a></span> | <span class="t">Now we have two values that are non-zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1742" target="_blank">00:29:02.100</a></span> | <span class="t">So this means that this output here will depend only on the first two tokens because all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1749" target="_blank">00:29:09.020</a></span> | <span class="t">other are zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1750" target="_blank">00:29:10.900</a></span> | <span class="t">And this is how we make the model causal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1754" target="_blank">00:29:14.340</a></span> | <span class="t">OK, now we are ready to explore BERT and the architecture behind BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1760" target="_blank">00:29:20.620</a></span> | <span class="t">So BERT's architecture is also using the encoder of the transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1766" target="_blank">00:29:26.180</a></span> | <span class="t">So we have input embeddings, we have positional encodings, we will see that the positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1770" target="_blank">00:29:30.140</a></span> | <span class="t">encodings are actually different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1772" target="_blank">00:29:32.500</a></span> | <span class="t">Then we have self-attention, we have normalization, we have feedforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1776" target="_blank">00:29:36.540</a></span> | <span class="t">And then we have this head, this linear head we can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1780" target="_blank">00:29:40.100</a></span> | <span class="t">And this we will see later that it changes according to the specific task for which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1784" target="_blank">00:29:44.620</a></span> | <span class="t">are using BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1786" target="_blank">00:29:46.740</a></span> | <span class="t">BERT was introduced with the two pre-trained models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1789" target="_blank">00:29:49.460</a></span> | <span class="t">One is BERT-Base and one is BERT-Large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1792" target="_blank">00:29:52.340</a></span> | <span class="t">The BERT-Base, for example, has 12 encoder layers, which means that this block here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1797" target="_blank">00:29:57.180</a></span> | <span class="t">so the gray block you can see here, is repeated 12 times, one after another, and the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1802" target="_blank">00:30:02.620</a></span> | <span class="t">of the last layer is fed to this linear layer and then to the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1808" target="_blank">00:30:08.100</a></span> | <span class="t">The size of the hidden size of the feedforward layer is 3072, so the feedforward layer you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1813" target="_blank">00:30:13.580</a></span> | <span class="t">can see here, which is basically just two linear layers, the size of the features is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1819" target="_blank">00:30:19.180</a></span> | <span class="t">3072.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1820" target="_blank">00:30:20.660</a></span> | <span class="t">And then we have 12 attention heads in the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1824" target="_blank">00:30:24.380</a></span> | <span class="t">Then BERT-Large have these numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1826" target="_blank">00:30:26.420</a></span> | <span class="t">Now what are the differences between BERT and the vanilla transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1830" target="_blank">00:30:30.320</a></span> | <span class="t">The first difference is that the embedding vector is not 512 anymore, but it's 768 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1837" target="_blank">00:30:37.260</a></span> | <span class="t">BERT-Base and 1024 for BERT-Large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1841" target="_blank">00:30:41.540</a></span> | <span class="t">From now on, I will always refer to the number 768, so that the embedding of the vector in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1847" target="_blank">00:30:47.860</a></span> | <span class="t">BERT-Base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1850" target="_blank">00:30:50.420</a></span> | <span class="t">Another difference is that the positional encoding in the vanilla transformer were computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1854" target="_blank">00:30:54.900</a></span> | <span class="t">using the sine and the cosine function we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1858" target="_blank">00:30:58.560</a></span> | <span class="t">But in BERT, these positional embeddings are not fixed and pre-computed using fixed functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1865" target="_blank">00:31:05.500</a></span> | <span class="t">but they are actually embeddings that are learned during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1869" target="_blank">00:31:09.480</a></span> | <span class="t">And they are of the same size of the embedding vector, of course, because they are summed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1873" target="_blank">00:31:13.900</a></span> | <span class="t">together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1874" target="_blank">00:31:14.900</a></span> | <span class="t">So they have 768 dimensions in BERT-Base and 1024 in BERT-Large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1881" target="_blank">00:31:21.600</a></span> | <span class="t">But these positional embeddings are limited to 512 positions, which means that BERT cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1889" target="_blank">00:31:29.920</a></span> | <span class="t">handle sentences longer than 512 tokens, because we only have 512 vectors to represent positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1900" target="_blank">00:31:40.960</a></span> | <span class="t">And the linear layer head changes according to the application, so this linear layer here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1907" target="_blank">00:31:47.200</a></span> | <span class="t">So we saw before that BERT uses not the tokenizer that we have used, the simple one, which only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1913" target="_blank">00:31:53.100</a></span> | <span class="t">treats each word as a token, but it uses what's called the word piece tokenizer, which also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1917" target="_blank">00:31:57.860</a></span> | <span class="t">allows sub-word tokens, so each word can become multiple tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1922" target="_blank">00:32:02.440</a></span> | <span class="t">The vocabulary size is roughly 30,000 tokens in the BERT-Base and BERT-Large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1928" target="_blank">00:32:08.000</a></span> | <span class="t">Ok, let's see the differences between BERT and the language models like GPT and LLAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1934" target="_blank">00:32:14.140</a></span> | <span class="t">In my slide I call these kind of models the common language models, so they are commonly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1939" target="_blank">00:32:19.200</a></span> | <span class="t">known as language models, so like GPT and LLAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1942" target="_blank">00:32:22.200</a></span> | <span class="t">So unlike them, BERT does not handle special tasks with prompts, but rather it can be specialized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1949" target="_blank">00:32:29.040</a></span> | <span class="t">on a particular task by means of fine-tuning, and we will see what I mean by this in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1953" target="_blank">00:32:33.680</a></span> | <span class="t">next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1955" target="_blank">00:32:35.920</a></span> | <span class="t">The second difference is that BERT has been trained using the left context and the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1960" target="_blank">00:32:40.040</a></span> | <span class="t">context, so we will see also what we mean by this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1965" target="_blank">00:32:45.040</a></span> | <span class="t">BERT is not built specifically for text generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1968" target="_blank">00:32:48.400</a></span> | <span class="t">So for example, you can use LLAMA to generate a big article given a prompt, but you cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1972" target="_blank">00:32:52.840</a></span> | <span class="t">use BERT for this purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1974" target="_blank">00:32:54.960</a></span> | <span class="t">BERT is useful for other kind of tasks, and we will see which ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1979" target="_blank">00:32:59.160</a></span> | <span class="t">And BERT has not been trained on the next token prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1983" target="_blank">00:33:03.320</a></span> | <span class="t">So the model that we trained initially on the Chinese poems was trained on the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1988" target="_blank">00:33:08.080</a></span> | <span class="t">token prediction task, but BERT has not been trained on this particular task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1992" target="_blank">00:33:12.440</a></span> | <span class="t">BERT has been trained on the musket language model and the next sentence prediction task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1997" target="_blank">00:33:17.200</a></span> | <span class="t">and we will see both of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=1999" target="_blank">00:33:19.200</a></span> | <span class="t">Ok, so let's see how we handle different tasks in GPT or in LLAMA, and how we handle them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2006" target="_blank">00:33:26.240</a></span> | <span class="t">in BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2007" target="_blank">00:33:27.600</a></span> | <span class="t">Suppose we want to do question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2010" target="_blank">00:33:30.320</a></span> | <span class="t">If we want to do it with GPT or with LLAMA, what we do is we build a particular prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2015" target="_blank">00:33:35.640</a></span> | <span class="t">and this is called a few shot prompting, in which we teach the model how to handle a task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2023" target="_blank">00:33:43.480</a></span> | <span class="t">inside of the prompt, and then we let the model tell us the answer without the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2030" target="_blank">00:33:50.760</a></span> | <span class="t">one in the last part of the prompt, we let the model answer using the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2035" target="_blank">00:33:55.760</a></span> | <span class="t">So for example, if we tell the model that given the context and the question how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2040" target="_blank">00:34:00.280</a></span> | <span class="t">build the answer, then given the context and the question the model should be able to come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2046" target="_blank">00:34:06.520</a></span> | <span class="t">up with an answer that makes sense given the previous example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2051" target="_blank">00:34:11.280</a></span> | <span class="t">While in BERT we do not work with the prompt, like we do with chat GPT or with LLAMA or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2057" target="_blank">00:34:17.200</a></span> | <span class="t">with GPT, but we fine tune BERT on the specific task we want to work on, and we will see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2065" target="_blank">00:34:25.880</a></span> | <span class="t">As I said before, language models are models that predict the next token using only the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2072" target="_blank">00:34:32.420</a></span> | <span class="t">left context of each word, that is the tokens that come to the left side of each word for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2078" target="_blank">00:34:38.000</a></span> | <span class="t">predicting the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2079" target="_blank">00:34:39.920</a></span> | <span class="t">This is not the case with BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2081" target="_blank">00:34:41.880</a></span> | <span class="t">BERT uses the left context and the right context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2085" target="_blank">00:34:45.320</a></span> | <span class="t">So I want to give you some intuition in why also we humans may be using the left and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2090" target="_blank">00:34:50.800</a></span> | <span class="t">right context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2091" target="_blank">00:34:51.800</a></span> | <span class="t">Now, I am not really a linguist, so my intuition will not be maybe technically valid, but it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2098" target="_blank">00:34:58.000</a></span> | <span class="t">will help you understand the importance of left and right context also in human conversations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2104" target="_blank">00:35:04.040</a></span> | <span class="t">So let's start with the left context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2105" target="_blank">00:35:05.920</a></span> | <span class="t">The left context in human conversation is used every time we have a phone conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2109" target="_blank">00:35:09.920</a></span> | <span class="t">For example, the operator's answer is based on the user's input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2115" target="_blank">00:35:15.720</a></span> | <span class="t">So the user says something, then the operator will say something, then the user will reply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2119" target="_blank">00:35:19.960</a></span> | <span class="t">with something based on what the operator said, and then the operator will continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2124" target="_blank">00:35:24.360</a></span> | <span class="t">based on the context given by the previous conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2128" target="_blank">00:35:28.280</a></span> | <span class="t">This is called using the left context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2131" target="_blank">00:35:31.120</a></span> | <span class="t">For right context, it's more difficult to visualize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2134" target="_blank">00:35:34.320</a></span> | <span class="t">For example, imagine there is a kid who just broke his mom's favorite necklace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2138" target="_blank">00:35:38.680</a></span> | <span class="t">The kid doesn't want to tell the truth to his mom, so he decides to make up a lie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2143" target="_blank">00:35:43.320</a></span> | <span class="t">So instead of saying directly to his mom, your favorite necklace has broken, the kid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2148" target="_blank">00:35:48.640</a></span> | <span class="t">may say, "Mom, I just saw the cat playing in your room and your favorite necklace has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2153" target="_blank">00:35:53.640</a></span> | <span class="t">broken."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2154" target="_blank">00:35:54.640</a></span> | <span class="t">Or it may say, "Mom, alien came through the window with laser guns and your favorite necklace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2160" target="_blank">00:36:00.880</a></span> | <span class="t">has broken."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2161" target="_blank">00:36:01.880</a></span> | <span class="t">As you can see, the kid conditioned the lie on what he want to say next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2168" target="_blank">00:36:08.400</a></span> | <span class="t">So it doesn't even matter which lie the kid says, because he want to create some context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2176" target="_blank">00:36:16.200</a></span> | <span class="t">before arriving to the conclusion, which is, "Your favorite necklace has broken."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2180" target="_blank">00:36:20.760</a></span> | <span class="t">So he conditions the word he chooses initially on what he want to say next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2187" target="_blank">00:36:27.040</a></span> | <span class="t">That is the definition of making up a lie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2189" target="_blank">00:36:29.840</a></span> | <span class="t">And this could be an intuition in how we humans use the right context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2195" target="_blank">00:36:35.960</a></span> | <span class="t">And I hope you also see it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2198" target="_blank">00:36:38.680</a></span> | <span class="t">Now let's talk about the pre-training of model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2201" target="_blank">00:36:41.520</a></span> | <span class="t">Because for example LAMA or GPT, they have been pre-trained on a large corpora of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2207" target="_blank">00:36:47.560</a></span> | <span class="t">And then we can use them with prompting or with fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2211" target="_blank">00:36:51.360</a></span> | <span class="t">BERT has not been pre-trained like with LAMA or GPT using the next token prediction task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2218" target="_blank">00:36:58.440</a></span> | <span class="t">but on two specific tasks called the Masked Language Model and the Next Sentence Prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2223" target="_blank">00:37:03.440</a></span> | <span class="t">Task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2224" target="_blank">00:37:04.440</a></span> | <span class="t">Let's review them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2227" target="_blank">00:37:07.560</a></span> | <span class="t">The Masked Language Model is also known as the Claude's Task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2230" target="_blank">00:37:10.880</a></span> | <span class="t">And you may know it from some papers or tests that you have done at university.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2236" target="_blank">00:37:16.300</a></span> | <span class="t">For example, the teacher gives you a sentence in which one word is missing and you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2240" target="_blank">00:37:20.780</a></span> | <span class="t">to fill the empty space with the missing word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2244" target="_blank">00:37:24.400</a></span> | <span class="t">And this is how BERT has been trained with the Masked Language Model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2248" target="_blank">00:37:28.080</a></span> | <span class="t">Basically what they did is they took some sentence from the corpora from which they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2252" target="_blank">00:37:32.960</a></span> | <span class="t">were training BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2254" target="_blank">00:37:34.700</a></span> | <span class="t">They choose some tokens randomly and they replace this random token with a special token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2263" target="_blank">00:37:43.620</a></span> | <span class="t">called Mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2265" target="_blank">00:37:45.300</a></span> | <span class="t">Then they feed this masked input to BERT and BERT has to come up with the word that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2273" target="_blank">00:37:53.940</a></span> | <span class="t">removed initially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2276" target="_blank">00:37:56.420</a></span> | <span class="t">One or more words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2278" target="_blank">00:37:58.060</a></span> | <span class="t">Let's see how it was done technically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2280" target="_blank">00:38:00.820</a></span> | <span class="t">So first we need to understand how BERT uses the left and the right context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2287" target="_blank">00:38:07.820</a></span> | <span class="t">So as you saw before, when we compute the attention we use the formula softmax of query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2292" target="_blank">00:38:12.480</a></span> | <span class="t">multiplied by the transpose of the keys divided by the square root of dk and then we multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2296" target="_blank">00:38:16.460</a></span> | <span class="t">it by v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2297" target="_blank">00:38:17.460</a></span> | <span class="t">This means that we take the query matrix, we multiply it by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2304" target="_blank">00:38:24.000</a></span> | <span class="t">we do the softmax and this will produce this matrix here that we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2308" target="_blank">00:38:28.640</a></span> | <span class="t">But unlike before, like we did for Language Models, in this case we will not use any mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2315" target="_blank">00:38:35.940</a></span> | <span class="t">to cancel out the interactions of words with words that come after it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2321" target="_blank">00:38:41.920</a></span> | <span class="t">So for example before we replaced the value here, for example, with minus infinity and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2326" target="_blank">00:38:46.640</a></span> | <span class="t">also this value with minus infinity and actually all the values above this diagonal with minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2331" target="_blank">00:38:51.440</a></span> | <span class="t">infinity because we didn't want the model to learn these interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2335" target="_blank">00:38:55.160</a></span> | <span class="t">But with BERT we do not use any mask, which means that each token attends tokens to its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2341" target="_blank">00:39:01.960</a></span> | <span class="t">left and tokens to its right in a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2345" target="_blank">00:39:05.600</a></span> | <span class="t">Ok, let's review the details of the Masked Language Model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2350" target="_blank">00:39:10.300</a></span> | <span class="t">So imagine we want to mask this sentence, so Rome is the capital of Italy, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2355" target="_blank">00:39:15.160</a></span> | <span class="t">why it hosts many government buildings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2358" target="_blank">00:39:18.880</a></span> | <span class="t">The pre-training procedure selects 15% of the tokens from this sentence to be masked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2364" target="_blank">00:39:24.980</a></span> | <span class="t">If a token is selected, for example the word capital is selected, then 80% of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2370" target="_blank">00:39:30.720</a></span> | <span class="t">it is replaced with the masked token, becoming this input for BERT, 10% of the time it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2376" target="_blank">00:39:36.880</a></span> | <span class="t">replaced with a random token, Zebra, and 10% of the time it's not replaced at all, so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2382" target="_blank">00:39:42.560</a></span> | <span class="t">remains as original.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2384" target="_blank">00:39:44.640</a></span> | <span class="t">In any of these three cases, BERT has to predict the word that was masked out, so BERT should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2389" target="_blank">00:39:49.920</a></span> | <span class="t">output capital for each of these three inputs based on the probability they happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2398" target="_blank">00:39:58.880</a></span> | <span class="t">So during training, what we do, for example, imagine this is our initial sentence in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2404" target="_blank">00:40:04.840</a></span> | <span class="t">we masked out the word capital like we saw before, this will result in an input to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2409" target="_blank">00:40:09.760</a></span> | <span class="t">fed to BERT of 14 tokens, if we count the tokens here, we feed it to BERT, BERT will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2417" target="_blank">00:40:17.160</a></span> | <span class="t">produce an output because BERT is a transformer model so 14 tokens of input result in 14 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2423" target="_blank">00:40:23.800</a></span> | <span class="t">in the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2425" target="_blank">00:40:25.480</a></span> | <span class="t">What we do is we only check the BERT, the position that was masked out, so this token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2430" target="_blank">00:40:30.640</a></span> | <span class="t">here, we compare it with our capital, with our target, and we compute the loss, and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2438" target="_blank">00:40:38.560</a></span> | <span class="t">run back propagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2439" target="_blank">00:40:39.760</a></span> | <span class="t">So basically what we want is this token here to be capital.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2447" target="_blank">00:40:47.560</a></span> | <span class="t">Okay now let's review the next sentence prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2451" target="_blank">00:40:51.920</a></span> | <span class="t">Next sentence prediction task means that we have a text, we choose two sentences randomly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2460" target="_blank">00:41:00.040</a></span> | <span class="t">actually we select one sentence randomly, so the sentence A, and then 50% of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2465" target="_blank">00:41:05.920</a></span> | <span class="t">we select its immediately next sentence, so the second line, or 50% of the time we select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2472" target="_blank">00:41:12.400</a></span> | <span class="t">a random sentence from the rest of the text, in this case this one, the third line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2477" target="_blank">00:41:17.480</a></span> | <span class="t">In our case we selected the first line as sentence A, and the sentence B is the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2482" target="_blank">00:41:22.800</a></span> | <span class="t">line, so it's not the immediately following sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2486" target="_blank">00:41:26.880</a></span> | <span class="t">We feed both of these sentences to BERT, and BERT has to predict if the sentence B comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2492" target="_blank">00:41:32.380</a></span> | <span class="t">immediately after sentence A or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2495" target="_blank">00:41:35.320</a></span> | <span class="t">In this case, because sentence B is the third line, and sentence A is the first line, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2501" target="_blank">00:41:41.880</a></span> | <span class="t">not immediately following, so BERT should reply with "not next".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2506" target="_blank">00:41:46.480</a></span> | <span class="t">In the case we had selected the second line as sentence B, BERT should reply with "is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2511" target="_blank">00:41:51.840</a></span> | <span class="t">next".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2512" target="_blank">00:41:52.840</a></span> | <span class="t">We have two problems here, how can we encode two sentences to become the input for BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2520" target="_blank">00:42:00.360</a></span> | <span class="t">and second problem is how can BERT tell us that it's the next sentence or it's not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2528" target="_blank">00:42:08.000</a></span> | <span class="t">next sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2529" target="_blank">00:42:09.800</a></span> | <span class="t">Well the idea is this, we take the two sentences and we encode it as only one sentence, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2537" target="_blank">00:42:17.600</a></span> | <span class="t">becomes one input, so the tokens of the two sentences become one input concatenated together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2543" target="_blank">00:42:23.920</a></span> | <span class="t">in which we prepend one special token called CLS, then the tokens of the first sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2550" target="_blank">00:42:30.160</a></span> | <span class="t">so suppose the sentence is "my dog is cute", then we add the token called separator, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2555" target="_blank">00:42:35.200</a></span> | <span class="t">the tokens of the second sentence, so this "he likes playing" and then another token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2560" target="_blank">00:42:40.800</a></span> | <span class="t">sep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2563" target="_blank">00:42:43.580</a></span> | <span class="t">The problem is, if we feed only this input to BERT, BERT will not be able to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2568" target="_blank">00:42:48.560</a></span> | <span class="t">that this "my" belongs to sentence A and this "likes" belongs to sentence B, so what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2574" target="_blank">00:42:54.640</a></span> | <span class="t">did before initially, as I told you before, when we feed the input to any language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2580" target="_blank">00:43:00.560</a></span> | <span class="t">that uses transformer, we first tokenize it, then we convert it into embedding vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2586" target="_blank">00:43:06.080</a></span> | <span class="t">of size 512 in case the vanilla transformer or 768 in case of BERT, then we append another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2593" target="_blank">00:43:13.720</a></span> | <span class="t">vector that represents the position of this token, so the position embeddings, in BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2598" target="_blank">00:43:18.720</a></span> | <span class="t">we have another embedding called segment embedding, so it's another vector that we add to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2604" target="_blank">00:43:24.240</a></span> | <span class="t">position embedding and to the token embedding and represents the fact that this token belongs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2609" target="_blank">00:43:29.920</a></span> | <span class="t">to sentence A or to sentence B, and this is how we encode the input for BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2616" target="_blank">00:43:36.760</a></span> | <span class="t">Let's see how we train it, so we create our input which is the first line of this poem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2624" target="_blank">00:43:44.240</a></span> | <span class="t">and the third line of this poem together with the separator token we can see here and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2630" target="_blank">00:43:50.280</a></span> | <span class="t">special token called CLS here, and we also encode the information that all these tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2637" target="_blank">00:43:57.360</a></span> | <span class="t">belong to sentence A and all these tokens belong to sentence B by using this special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2643" target="_blank">00:44:03.920</a></span> | <span class="t">this one here, segment embedding we saw here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2647" target="_blank">00:44:07.480</a></span> | <span class="t">Now we feed this input to BERT, BERT will come out with an output because it's a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2655" target="_blank">00:44:15.760</a></span> | <span class="t">model so an input of 20 tokens corresponds to an output of 20 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2661" target="_blank">00:44:21.480</a></span> | <span class="t">We take only the first token of the output, the one that corresponds to the token CLS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2668" target="_blank">00:44:28.040</a></span> | <span class="t">which stands for classifier, we feed it to a linear layer with only two output features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2675" target="_blank">00:44:35.120</a></span> | <span class="t">one feature indicating next and one indicating not next, we apply the softmax, we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2681" target="_blank">00:44:41.880</a></span> | <span class="t">it with our target so we expect BERT to say not next because we fed it the third line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2688" target="_blank">00:44:48.680</a></span> | <span class="t">as sentence B and not the second line, and then we compute the loss which is the cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2694" target="_blank">00:44:54.120</a></span> | <span class="t">entropy loss and we run backpropagation to update the weights, and this is how we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2698" target="_blank">00:44:58.400</a></span> | <span class="t">BERT on the next sentence prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2702" target="_blank">00:45:02.600</a></span> | <span class="t">Now this CLS token, let's review how this works, so as we saw before the formula for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2708" target="_blank">00:45:08.800</a></span> | <span class="t">the attention is a query multiplied by the keys, the transpose of the keys divided by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2712" target="_blank">00:45:12.760</a></span> | <span class="t">the square root of 768, we apply the softmax and this will produce this attention score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2721" target="_blank">00:45:21.080</a></span> | <span class="t">matrix we can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2723" target="_blank">00:45:23.160</a></span> | <span class="t">Now as we can see the CLS token always interacts with all the other tokens because we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2729" target="_blank">00:45:29.000</a></span> | <span class="t">apply any causal mask, so we can consider that the CLS token acts as a token that captures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2736" target="_blank">00:45:36.680</a></span> | <span class="t">the information from all the other tokens because the attention matrix here, we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2744" target="_blank">00:45:44.280</a></span> | <span class="t">apply any mask before applying the softmax, so all of these attention values will be actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2750" target="_blank">00:45:50.760</a></span> | <span class="t">learned by the model, and this is the idea behind this CLS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2756" target="_blank">00:45:56.840</a></span> | <span class="t">So if we do for example the matrix multiplication that we did before, so let's compute the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2762" target="_blank">00:46:02.400</a></span> | <span class="t">row of the output matrix, let's see, so this matrix here is the input matrix which is 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2772" target="_blank">00:46:12.240</a></span> | <span class="t">by 768, because suppose the input is very simple, so before my bed lies a pool of moon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2777" target="_blank">00:46:17.040</a></span> | <span class="t">bright, so 10 tokens of input, 1, 2, 3, etc, etc, so 10 tokens, each of them has 768 embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2787" target="_blank">00:46:27.840</a></span> | <span class="t">because we are talking about birth, 8, 768, so the first dimension, the first dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2796" target="_blank">00:46:36.980</a></span> | <span class="t">the first dimension up to 768, this will result in an output matrix of 10 by 768, so the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2805" target="_blank">00:46:45.480</a></span> | <span class="t">dimension up to 768, 1, 2, 768, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2812" target="_blank">00:46:52.960</a></span> | <span class="t">We are only interested in this output here, which corresponds to the position of the CLS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2819" target="_blank">00:46:59.280</a></span> | <span class="t">token, let's see, the first dimension, so the dimension number 1 of this output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2825" target="_blank">00:47:05.840</a></span> | <span class="t">will be the dot product of this vector here, which is made of 10 dimensions, with the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2834" target="_blank">00:47:14.440</a></span> | <span class="t">column of this matrix here, which is also made of 10 dimensions because we have 10 tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2840" target="_blank">00:47:20.660</a></span> | <span class="t">but because none of the value here is 0, actually here is 0 because I chose random numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2846" target="_blank">00:47:26.000</a></span> | <span class="t">but suppose this is 0.03 and 0.04 let's say, because none of the values in this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2856" target="_blank">00:47:36.560</a></span> | <span class="t">is 0, the output, the CLS will be able to access the attention scores of all the tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2863" target="_blank">00:47:43.520</a></span> | <span class="t">so basically this token here will aggregate the attention scores, so the relationship</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2869" target="_blank">00:47:49.440</a></span> | <span class="t">with all the tokens, the CLS can also be thought of as the CEO in a company and you are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2876" target="_blank">00:47:56.800</a></span> | <span class="t">shareholder, when you are the shareholder, you don't ask the information to the employees,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2880" target="_blank">00:48:00.640</a></span> | <span class="t">you ask to the CEO, and the CEO's job is to talk to every guy in the company, to every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2887" target="_blank">00:48:07.720</a></span> | <span class="t">person in the company to get the necessary information to reach the goal, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2893" target="_blank">00:48:13.920</a></span> | <span class="t">the goal of the CLS, the CLS can be thought of as the aggregator of all the information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2899" target="_blank">00:48:19.020</a></span> | <span class="t">of all the information present inside of the sentence, and we use it to classify, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2904" target="_blank">00:48:24.120</a></span> | <span class="t">why it's called the CLS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2908" target="_blank">00:48:28.040</a></span> | <span class="t">Okay now let's talk about fine-tuning, as we saw before, BERT does not work with prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2914" target="_blank">00:48:34.720</a></span> | <span class="t">like LLAMA or GPT, so we cannot use zero-shot prompting or few-shot prompting or chain of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2922" target="_blank">00:48:42.240</a></span> | <span class="t">thoughts or any other prompting technique, with BERT we work with fine-tuning, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2927" target="_blank">00:48:47.200</a></span> | <span class="t">take the pre-trained model, and if we want to do text classification, we fine-tune BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2931" target="_blank">00:48:51.960</a></span> | <span class="t">on our dataset for text classification, or question answering, let's see how these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2936" target="_blank">00:48:56.640</a></span> | <span class="t">tasks work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2939" target="_blank">00:48:59.440</a></span> | <span class="t">Suppose we want to do text classification, so text classification is the task of assigning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2945" target="_blank">00:49:05.320</a></span> | <span class="t">a label to a piece of text, for example, imagine you are running an internet provider and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2949" target="_blank">00:49:09.960</a></span> | <span class="t">receive complaints from our customers, we may want to classify requests coming from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2955" target="_blank">00:49:15.060</a></span> | <span class="t">users as hardware problems, software problems or billing problems, for example, this complaint</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2961" target="_blank">00:49:21.360</a></span> | <span class="t">here is definitely a hardware problem, this complaint here is definitely a software problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2967" target="_blank">00:49:27.160</a></span> | <span class="t">and this one is definitely a billing problem, we want to classify automatically this request</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2973" target="_blank">00:49:33.360</a></span> | <span class="t">that we keep receiving from customers, how do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2977" target="_blank">00:49:37.440</a></span> | <span class="t">Well we take our request, we feed it to BERT and BERT should tell us which one of the three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2983" target="_blank">00:49:43.280</a></span> | <span class="t">options it best represents this particular request, how can BERT tell us one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2991" target="_blank">00:49:51.960</a></span> | <span class="t">three options and how can we feed our request to BERT?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2995" target="_blank">00:49:55.600</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=2997" target="_blank">00:49:57.000</a></span> | <span class="t">So when we train BERT for text classification, we create our input, we prepend to the request</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3003" target="_blank">00:50:03.440</a></span> | <span class="t">text the classifier token, so the CLS token, we feed it to BERT, BERT will come up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3010" target="_blank">00:50:10.840</a></span> | <span class="t">an output, so 16 input tokens correspond to 16 output tokens, we only care about the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3016" target="_blank">00:50:16.200</a></span> | <span class="t">one, which is the one corresponding to the CLS token, we send the output to a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3022" target="_blank">00:50:22.640</a></span> | <span class="t">layer with three output features, because we have three possible classes, one is software,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3028" target="_blank">00:50:28.280</a></span> | <span class="t">one is hardware, one is billing and then we apply Softmax, we compare it with what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3033" target="_blank">00:50:33.160</a></span> | <span class="t">expect BERT to learn about this particular request, that is that this request is hardware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3040" target="_blank">00:50:40.440</a></span> | <span class="t">then we calculate the loss, which is the cross entropy loss and finally we run backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3044" target="_blank">00:50:44.460</a></span> | <span class="t">to update the weights and this is how we fine-tune BERT on text classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3051" target="_blank">00:50:51.040</a></span> | <span class="t">Our next task is question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3054" target="_blank">00:50:54.920</a></span> | <span class="t">Question answering basically means this, we have a context from which we need to extract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3060" target="_blank">00:51:00.360</a></span> | <span class="t">the answer to a particular question, for example the context is Shanghai is a city in China,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3067" target="_blank">00:51:07.520</a></span> | <span class="t">it is also a financial center, it's fashion capital and industrial city, the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3072" target="_blank">00:51:12.200</a></span> | <span class="t">is what is the fashion capital of China, well the model should highlight the word or tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3077" target="_blank">00:51:17.920</a></span> | <span class="t">us which part of the context we can find the answer, so BERT should be able to tell us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3084" target="_blank">00:51:24.720</a></span> | <span class="t">where the answer starts and where the answer ends in the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3091" target="_blank">00:51:31.640</a></span> | <span class="t">But we have two problems, first we need to find a way for BERT to understand which part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3096" target="_blank">00:51:36.120</a></span> | <span class="t">of the input is the context and which one is the question, second we need to find a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3101" target="_blank">00:51:41.040</a></span> | <span class="t">way for BERT to tell us where the answer starts and where the answer ends, let's see both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3107" target="_blank">00:51:47.040</a></span> | <span class="t">of these problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3108" target="_blank">00:51:48.640</a></span> | <span class="t">The first problem can be solved easily using the segment embedding we saw before, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3114" target="_blank">00:51:54.120</a></span> | <span class="t">concatenate the question and the context together as a single input with the separator token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3120" target="_blank">00:52:00.220</a></span> | <span class="t">in the middle like we saw before for the next sentence prediction task, the question will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3129" target="_blank">00:52:09.920</a></span> | <span class="t">be encoded as a sentence A while the context will be encoded as a sentence B, so this problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3137" target="_blank">00:52:17.840</a></span> | <span class="t">is solved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3140" target="_blank">00:52:20.100</a></span> | <span class="t">How do we get the answer from BERT, well let's see, so first we prepare the input for BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3147" target="_blank">00:52:27.920</a></span> | <span class="t">so we prepend the CLS token, what is the fashion capital of China, separator, Shanghai is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3154" target="_blank">00:52:34.200</a></span> | <span class="t">city in China etc, so this is the context which has been encoded as sentence B and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3158" target="_blank">00:52:38.720</a></span> | <span class="t">first part has been encoded as sentence A, we feed it to BERT, BERT will come up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3165" target="_blank">00:52:45.040</a></span> | <span class="t">the output which is 27 tokens because the input is made up of 27 tokens, we also know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3171" target="_blank">00:52:51.960</a></span> | <span class="t">which tokens correspond to which sentence, so which correspond to the sentence A, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3177" target="_blank">00:52:57.520</a></span> | <span class="t">correspond to the sentence B because we give it as input, we apply a linear layer with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3183" target="_blank">00:53:03.900</a></span> | <span class="t">two output features, one that indicates if one particular token is the start token and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3191" target="_blank">00:53:11.040</a></span> | <span class="t">another feature that indicates if the token is an end token, we know where is the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3198" target="_blank">00:53:18.720</a></span> | <span class="t">because we know the answer is the word Shanghai which is the start should be the token 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3204" target="_blank">00:53:24.160</a></span> | <span class="t">and the end should be the token 10, then we calculate the loss based on our target and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3209" target="_blank">00:53:29.600</a></span> | <span class="t">the output of this linear layer, we run backpropagation and this is how we fine tune BERT for question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3215" target="_blank">00:53:35.480</a></span> | <span class="t">answering and this is it guys, I hope that you liked my video, I used a very unconventional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3223" target="_blank">00:53:43.900</a></span> | <span class="t">way of describing BERT, that is I started from language models, I introduced the concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3229" target="_blank">00:53:49.200</a></span> | <span class="t">of how language models work and then I introduced BERT because I wanted to create a comparison</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3234" target="_blank">00:53:54.280</a></span> | <span class="t">of how BERT works versus how other language models work so that you can appreciate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3240" target="_blank">00:54:00.440</a></span> | <span class="t">qualities and the weaknesses of both, BERT is actually not so recent model, it was introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3246" target="_blank">00:54:06.240</a></span> | <span class="t">in 2018 if I remember correctly, so it is quite aged but still very relevant for a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3251" target="_blank">00:54:11.880</a></span> | <span class="t">of tasks and I hope that you will be coming again to my channel for more content so please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3258" target="_blank">00:54:18.520</a></span> | <span class="t">subscribe and share this video if you like it, if you have any questions please write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3263" target="_blank">00:54:23.140</a></span> | <span class="t">it in the comment, I am also very active on LinkedIn if you want to add me, if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3269" target="_blank">00:54:29.840</a></span> | <span class="t">to have some particular video review, model review, write it in the comment and I hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3276" target="_blank">00:54:36.640</a></span> | <span class="t">that in my next video I will be able to code BERT from scratch, so using PyTorch so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3282" target="_blank">00:54:42.000</a></span> | <span class="t">can also learn, put to practice all the knowledge that we acquired in today's video, thank you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=90mGPxR2GgY&t=3288" target="_blank">00:54:48.000</a></span> | <span class="t">again for coming to my channel guys and have a nice day!</span></div></div></body></html>