<html><head><title>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 3 - Backprop and Neural Networks</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 3 - Backprop and Neural Networks</h2><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg"><img src="https://i.ytimg.com/vi/X0Jw4kgaFlg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=40">0:40</a> Assignments<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=291">4:51</a> Named Entity Recognition<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=496">8:16</a> Neural Network<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=825">13:45</a> Gradient Computing<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1289">21:29</a> Nonlinearities<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1434">23:54</a> Jacobians<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1594">26:34</a> Neural Net<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2132">35:32</a> Jacobian<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2226">37:6</a> Shape Convention<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2535">42:15</a> Jacobian vs Shape Convention<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2896">48:16</a> Computation Graph<br><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3252">54:12</a> Example<br><br><div style="text-align: left;"><a href="./X0Jw4kgaFlg.html">Whisper Transcript</a> | <a href="./transcript_X0Jw4kgaFlg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, everyone. I'll get started. Okay. So we're now back to the second week of CS224N on natural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=14" target="_blank">00:00:14.600</a></span> | <span class="t">language processing with deep learning. Okay. So for today's lecture, what we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=20" target="_blank">00:00:20.040</a></span> | <span class="t">be looking at is all the math details of doing neural net learning. First of all, looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=27" target="_blank">00:00:27.480</a></span> | <span class="t">at how we can work out by hand gradients for training neural networks and then looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=34" target="_blank">00:00:34.320</a></span> | <span class="t">at how it's done more algorithmically, which is known as the back propagation algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=40" target="_blank">00:00:40.600</a></span> | <span class="t">And correspondingly for you guys, well, I hope you've remembered that, you know, one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=46" target="_blank">00:00:46.000</a></span> | <span class="t">minute ago was when assignment one was due and everyone has handed that in. If by some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=51" target="_blank">00:00:51.640</a></span> | <span class="t">chance you haven't handed it in, really should hand it in as soon as possible. Best to preserve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=57" target="_blank">00:00:57.400</a></span> | <span class="t">those late days for the harder assignments. So, I mean, I actually forgot to mention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=62" target="_blank">00:01:02.200</a></span> | <span class="t">we actually did make one change for this year to make it a bit easier when occasionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=67" target="_blank">00:01:07.940</a></span> | <span class="t">people join the class a week late. If you want to this year in the grading assignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=73" target="_blank">00:01:13.960</a></span> | <span class="t">one can be discounted and we'll just use your other four assignments. But if you've been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=79" target="_blank">00:01:19.120</a></span> | <span class="t">in the class so far for that 98% of people, well, since assignment one is the easiest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=84" target="_blank">00:01:24.840</a></span> | <span class="t">assignment, again, it's silly not to do it and have it as part of your grade. Okay. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=92" target="_blank">00:01:32.000</a></span> | <span class="t">starting today, we've put out assignment two and assignment two is all about making sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=97" target="_blank">00:01:37.640</a></span> | <span class="t">you really understand the math of neural networks and then the software that we use to do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=104" target="_blank">00:01:44.040</a></span> | <span class="t">math. So, this is going to be a bit of a tough week for some. So, for some people who are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=110" target="_blank">00:01:50.440</a></span> | <span class="t">great on all their math and backgrounds, they'll feel like this is stuff they know well, nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=117" target="_blank">00:01:57.380</a></span> | <span class="t">very difficult. But I know there are quite a few of you who this lecture and week is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=123" target="_blank">00:02:03.600</a></span> | <span class="t">the biggest struggle of the course. We really do want people to actually have an understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=129" target="_blank">00:02:09.900</a></span> | <span class="t">of what goes on in neural network learning rather than viewing it as some kind of deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=135" target="_blank">00:02:15.440</a></span> | <span class="t">magic. And I hope that some of the material we give today and that you read up on and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=141" target="_blank">00:02:21.040</a></span> | <span class="t">use in the assignment will really give you more of a sense of what these neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=146" target="_blank">00:02:26.360</a></span> | <span class="t">are doing and how it is just math that's applied in a systematic large scale that works out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=153" target="_blank">00:02:33.160</a></span> | <span class="t">the answers and that this will be valuable and give you a deeper sense of what's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=158" target="_blank">00:02:38.080</a></span> | <span class="t">on. But if this material seems very scary and difficult, you can take some refuge in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=166" target="_blank">00:02:46.120</a></span> | <span class="t">the fact that there's fast light at the end of the tunnel, since this is really the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=171" target="_blank">00:02:51.120</a></span> | <span class="t">lecture that's heavily going through the math details of neural networks. After that, we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=177" target="_blank">00:02:57.240</a></span> | <span class="t">be kind of popping back up to a higher level. And by and large, after this week, we'll be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=183" target="_blank">00:03:03.640</a></span> | <span class="t">making use of software to do a lot of the complicated math for us. But nevertheless,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=190" target="_blank">00:03:10.440</a></span> | <span class="t">I hope this is valuable. I'll go through everything quickly today. But if this isn't stuff that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=196" target="_blank">00:03:16.840</a></span> | <span class="t">you know backwards, I really do encourage you to, you know, work through it and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=202" target="_blank">00:03:22.640</a></span> | <span class="t">help as you need it. So do come along to our office hours. There are also a number of pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=208" target="_blank">00:03:28.200</a></span> | <span class="t">of tutorial material given in the syllabus. So there's both the lecture notes, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=214" target="_blank">00:03:34.080</a></span> | <span class="t">some materials from CS 231. In the list of readings, the very top reading is some material</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=221" target="_blank">00:03:41.800</a></span> | <span class="t">put together by Kevin Clark a couple of years ago. And actually, that one's my favorite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=227" target="_blank">00:03:47.880</a></span> | <span class="t">The presentation there fairly closely follows the presentation in this lecture of going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=233" target="_blank">00:03:53.840</a></span> | <span class="t">through matrix calculus. So, you know, personally, I'd recommend starting with that one. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=238" target="_blank">00:03:58.560</a></span> | <span class="t">there are four different ones you can choose from if one of them seems more helpful to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=243" target="_blank">00:04:03.000</a></span> | <span class="t">you. Two other things on what's coming up. Actually, for Thursday's lecture, we make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=251" target="_blank">00:04:11.360</a></span> | <span class="t">a big change. And Thursday's lecture is probably the most linguistic lecture of the whole class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=256" target="_blank">00:04:16.640</a></span> | <span class="t">where we go through the details of dependency grammar and dependency parsing. Some people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=261" target="_blank">00:04:21.000</a></span> | <span class="t">find that tough as well, but at least it'll be tough in a different way. And then one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=265" target="_blank">00:04:25.720</a></span> | <span class="t">other really good opportunity is this Friday, we have our second tutorial at 10am, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=272" target="_blank">00:04:32.240</a></span> | <span class="t">is an introduction to PyTorch, which is the deep learning framework that we'll be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=277" target="_blank">00:04:37.360</a></span> | <span class="t">for the rest of the class, once we've gone through these first two assignments where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=281" target="_blank">00:04:41.640</a></span> | <span class="t">you do things by yourself. So this is a great chance to get an intro to PyTorch. It'll be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=287" target="_blank">00:04:47.320</a></span> | <span class="t">really useful for later in the class. Okay. Today's material is really all about sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=295" target="_blank">00:04:55.920</a></span> | <span class="t">of the math of neural networks. But just to sort of introduce a setting where we can work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=302" target="_blank">00:05:02.040</a></span> | <span class="t">through this, I'm going to introduce a simple NLP task and a simple form of classifier that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=308" target="_blank">00:05:08.880</a></span> | <span class="t">we can use for it. So the task of named entity recognition is a very common basic NLP task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=315" target="_blank">00:05:15.840</a></span> | <span class="t">And the goal of this is you're looking through pieces of text and you're wanting to label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=321" target="_blank">00:05:21.300</a></span> | <span class="t">by labeling the words, which words belong to entity categories like persons, locations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=327" target="_blank">00:05:27.680</a></span> | <span class="t">products, dates, times, etc. So for this piece of text, last night, Paris Hilton wowed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=335" target="_blank">00:05:35.120</a></span> | <span class="t">the sequined gown. Samuel Quinn was arrested in the Hilton Hotel in Paris in April 1989.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=342" target="_blank">00:05:42.320</a></span> | <span class="t">Some words are being labeled as named entities as shown. These two sentences don't actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=348" target="_blank">00:05:48.160</a></span> | <span class="t">belong together in the same article, but I chose those two sentences to illustrate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=354" target="_blank">00:05:54.080</a></span> | <span class="t">basic point that it's not that you can just do this task by using a dictionary. Yes, a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=359" target="_blank">00:05:59.760</a></span> | <span class="t">dictionary is helpful to know that Paris can possibly be a location, but Paris can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=366" target="_blank">00:06:06.120</a></span> | <span class="t">be a person name. So you have to use context to get named entity recognition right. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=374" target="_blank">00:06:14.840</a></span> | <span class="t">well how might we do that with a neural network? There are much more advanced ways of doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=381" target="_blank">00:06:21.960</a></span> | <span class="t">this, but a simple yet already pretty good way of doing named entity recognition with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=389" target="_blank">00:06:29.600</a></span> | <span class="t">a simple neural net is to say, well, what we're going to do is use the word vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=396" target="_blank">00:06:36.560</a></span> | <span class="t">that we've learned about, and we're going to build up a context window of word vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=403" target="_blank">00:06:43.920</a></span> | <span class="t">And then we're going to put those through a neural network layer and then feed it through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=409" target="_blank">00:06:49.520</a></span> | <span class="t">a softmax classifier of the kind that we, sorry, I said that wrong. And then we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=415" target="_blank">00:06:55.760</a></span> | <span class="t">to feed it through a logistic classifier of the kind that we saw when looking at negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=420" target="_blank">00:07:00.800</a></span> | <span class="t">sampling, which is going to say for a particular entity type, such as location, is it high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=428" target="_blank">00:07:08.280</a></span> | <span class="t">probability location or is it not a high probability location? So for a sentence like the museums</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=435" target="_blank">00:07:15.740</a></span> | <span class="t">in Paris are amazing to see, what we're going to do is for each word, say we're doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=440" target="_blank">00:07:20.880</a></span> | <span class="t">word Paris, we're going to form a window around it, say a plus or minus two word window. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=447" target="_blank">00:07:27.080</a></span> | <span class="t">so for those five words, we're going to get word vectors for them from the kind of word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=453" target="_blank">00:07:33.720</a></span> | <span class="t">to vector or glove word vectors we've learned. And we're going to make a long vector out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=458" target="_blank">00:07:38.880</a></span> | <span class="t">of the concatenation of those five word vectors. So the word of interest is in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=464" target="_blank">00:07:44.060</a></span> | <span class="t">And then we're going to feed this vector to a classifier, which is at the end going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=470" target="_blank">00:07:50.900</a></span> | <span class="t">have a probability of the word being a location. And then we could have another classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=477" target="_blank">00:07:57.300</a></span> | <span class="t">that says the probability of the word being a person name. And so once we've done that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=482" target="_blank">00:08:02.020</a></span> | <span class="t">we're then going to run it at the next position. So we then say, well, is the word are a location?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=487" target="_blank">00:08:07.660</a></span> | <span class="t">And we'd feed a window of five words as then in Paris are amazing to and put it through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=493" target="_blank">00:08:13.100</a></span> | <span class="t">the same kind of classifier. And so this is the classifier that we'll use. So it's input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=500" target="_blank">00:08:20.440</a></span> | <span class="t">will be this word window. So if we have D dimensional word vectors, this will be a five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=506" target="_blank">00:08:26.420</a></span> | <span class="t">D vector. And then we're going to put it through a layer of a neural network. So the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=512" target="_blank">00:08:32.500</a></span> | <span class="t">of the neural network is going to multiply this vector by a matrix, add on a bias spectra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=521" target="_blank">00:08:41.540</a></span> | <span class="t">and then put that through a nonlinearity such as the softmax transformation that we've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=529" target="_blank">00:08:49.980</a></span> | <span class="t">before. And that will give us a hidden vector, which might be of a smaller dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=536" target="_blank">00:08:56.340</a></span> | <span class="t">such as this one here. And so then with that hidden vector, we're then going to take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=544" target="_blank">00:09:04.380</a></span> | <span class="t">dot product of it with an extra vector here. Here's a U. So we take U dot product H. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=553" target="_blank">00:09:13.100</a></span> | <span class="t">so when we do that, we're getting out a single number and that number can be any real number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=560" target="_blank">00:09:20.100</a></span> | <span class="t">And so then finally, we're going to put that number through a logistic transform of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=567" target="_blank">00:09:27.020</a></span> | <span class="t">same kind that we saw when doing negative sampling. The logistic transform will take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=573" target="_blank">00:09:33.600</a></span> | <span class="t">any real number and it will transform it into a probability that that word is a location.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=580" target="_blank">00:09:40.620</a></span> | <span class="t">So its output is the predicted probability of the word belonging to a particular class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=587" target="_blank">00:09:47.260</a></span> | <span class="t">And so this could be our location classifier, which could classify each word in a window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=593" target="_blank">00:09:53.500</a></span> | <span class="t">as to what the probability is that it's a location word. And so this little neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=599" target="_blank">00:09:59.900</a></span> | <span class="t">here is the neural network I'm going to use today when going through some of the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=606" target="_blank">00:10:06.580</a></span> | <span class="t">But actually I'm going to make it even easier on myself. I'm going to throw away the logistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=613" target="_blank">00:10:13.620</a></span> | <span class="t">function at the top, and I'm really just going to work through the math of the bottom three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=618" target="_blank">00:10:18.640</a></span> | <span class="t">quarters of this. If you look at Kevin Clark's handout that I just mentioned, he includes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=625" target="_blank">00:10:25.620</a></span> | <span class="t">when he works through it, also working through the logistic function. And we also saw working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=632" target="_blank">00:10:32.120</a></span> | <span class="t">through a softmax in the first lecture when I was working through some of the word to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">deck model. Okay. So the overall question we want to be able to answer is, so here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=645" target="_blank">00:10:45.600</a></span> | <span class="t">our stochastic gradient descent equation that we have existing parameters of our model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=653" target="_blank">00:10:53.940</a></span> | <span class="t">and we want to update them based on our current loss, which is at the J of theta. So for getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=663" target="_blank">00:11:03.700</a></span> | <span class="t">our loss here, that the true answer as to whether a word is a location or not will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=669" target="_blank">00:11:09.680</a></span> | <span class="t">either, you know, one if it is a location or zero if it isn't. Our logistic classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=676" target="_blank">00:11:16.140</a></span> | <span class="t">will return some number like .9, and we'll use the distance away from what it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=681" target="_blank">00:11:21.620</a></span> | <span class="t">have been squared as our loss. So we work out a loss, and then we're moving a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=688" target="_blank">00:11:28.260</a></span> | <span class="t">distance in the negative of the gradient, which will be changing our parameter estimates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=695" target="_blank">00:11:35.140</a></span> | <span class="t">in such a way that they reduce the loss. And so this is already being written in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=701" target="_blank">00:11:41.640</a></span> | <span class="t">of a whole vector of parameters, which is being updated as to a new vector of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=708" target="_blank">00:11:48.220</a></span> | <span class="t">But you can also think about it that for each individual parameter, theta J, that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=714" target="_blank">00:11:54.160</a></span> | <span class="t">working out the partial derivative of the loss with respect to that parameter, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=720" target="_blank">00:12:00.860</a></span> | <span class="t">we're moving a little bit in the negative direction of that. That's going to give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=727" target="_blank">00:12:07.420</a></span> | <span class="t">a new value for parameter theta J. And we're going to update all of the parameters of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=735" target="_blank">00:12:15.580</a></span> | <span class="t">model as we learn. I mean, in particular, in contrast to what commonly happens in statistics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=743" target="_blank">00:12:23.100</a></span> | <span class="t">we also we update not only the sort of parameters of our model that are sort of weights in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=750" target="_blank">00:12:30.260</a></span> | <span class="t">classifier, but we also will update our data representation. So we'll also be changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=756" target="_blank">00:12:36.180</a></span> | <span class="t">our word vectors as we learn. Okay. So to build neural nets, i.e. to train neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=762" target="_blank">00:12:42.740</a></span> | <span class="t">based on data, what we need is to be able to compute this gradient of the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=769" target="_blank">00:12:49.860</a></span> | <span class="t">so that we can then iteratively update the weights of the model and efficiently train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=775" target="_blank">00:12:55.460</a></span> | <span class="t">a model that has good weights, i.e. that has high accuracy. And so how can we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=783" target="_blank">00:13:03.860</a></span> | <span class="t">Well, what I'm going to talk about today is first of all, how you can do it by hand. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=791" target="_blank">00:13:11.820</a></span> | <span class="t">so for doing it by hand, this is basically a review of matrix calculus. And that'll take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=799" target="_blank">00:13:19.860</a></span> | <span class="t">quite a bit of the lecture. And then after we've talked about that for a while, I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=806" target="_blank">00:13:26.940</a></span> | <span class="t">then shift gears and introduce the back propagation algorithm, which is the central technology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=813" target="_blank">00:13:33.540</a></span> | <span class="t">for neural networks. And that technology is essentially the efficient application of calculus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=821" target="_blank">00:13:41.220</a></span> | <span class="t">on a large scale, as we'll come to talking about soon. So for computing gradients by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=827" target="_blank">00:13:47.300</a></span> | <span class="t">hand, what we're doing is matrix calculus. So we're working with vectors and matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=835" target="_blank">00:13:55.900</a></span> | <span class="t">and working out gradients. And this can seem like pretty scary stuff. And well, to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=845" target="_blank">00:14:05.620</a></span> | <span class="t">extent that you're kind of scared and don't know what's going on, one choice is to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=853" target="_blank">00:14:13.260</a></span> | <span class="t">out a non-vectorized gradient by just working out what the partial derivative is for one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=860" target="_blank">00:14:20.620</a></span> | <span class="t">parameter at a time. And I showed a little example of that in the first lecture. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=867" target="_blank">00:14:27.100</a></span> | <span class="t">it's much, much faster and more useful to actually be able to work with vectorized gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=877" target="_blank">00:14:37.580</a></span> | <span class="t">And in some sense, if you're not very confident, this is kind of almost a leap of faith. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=883" target="_blank">00:14:43.740</a></span> | <span class="t">it really is the case that multivariable calculus is just like single variable calculus, except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=890" target="_blank">00:14:50.620</a></span> | <span class="t">you're using vectors and matrices. So providing you remember some basics of single variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=896" target="_blank">00:14:56.740</a></span> | <span class="t">calculus, you really should be able to do this stuff and get it to work out. Lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=902" target="_blank">00:15:02.900</a></span> | <span class="t">other sources. I've mentioned the notes. You can also look at the textbook for Math 51,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=910" target="_blank">00:15:10.940</a></span> | <span class="t">which also has quite a lot of material on this. I know some of you have bad memories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=915" target="_blank">00:15:15.860</a></span> | <span class="t">of Math 51. Okay. So let's go through this and see how it works, ramping up from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=921" target="_blank">00:15:21.940</a></span> | <span class="t">beginning. So the beginning of calculus is, you know, we have a function with one input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=927" target="_blank">00:15:27.900</a></span> | <span class="t">and one output, f of x equals x cubed. And so then its gradient is its slope, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=934" target="_blank">00:15:34.940</a></span> | <span class="t">So that's its derivative. So its derivative is 3x squared. And the way to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=941" target="_blank">00:15:41.700</a></span> | <span class="t">this is how much will the output change if we change the input a little bit, right? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=948" target="_blank">00:15:48.620</a></span> | <span class="t">what we're wanting to do in our neural net models is change what they output so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=954" target="_blank">00:15:54.140</a></span> | <span class="t">they do a better job of predicting the correct answers when we're doing supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=960" target="_blank">00:16:00.620</a></span> | <span class="t">And so what we want to know is if we fiddle different parameters of the model, how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=965" target="_blank">00:16:05.500</a></span> | <span class="t">of an effect will that have on the output? Because then we can choose how to fiddle them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=969" target="_blank">00:16:09.900</a></span> | <span class="t">in the right way to move things down, right? So, you know, when we're saying that the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=975" target="_blank">00:16:15.780</a></span> | <span class="t">here is 3x squared, well, what we're saying is that if you're at x equals 1, if you fiddle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=984" target="_blank">00:16:24.740</a></span> | <span class="t">the input a little bit, the output will change three times as much, 3 times 1 squared. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=991" target="_blank">00:16:31.180</a></span> | <span class="t">it does. So if I say what's the value at 1.01, it's about 1.03. It's changed three times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=997" target="_blank">00:16:37.700</a></span> | <span class="t">as much, and that's its slope. But at x equals 4, the derivative is 16 times 3, 48. So if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1007" target="_blank">00:16:47.740</a></span> | <span class="t">we fiddle the input a little, it'll change 48 times as much. And that's roughly what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1012" target="_blank">00:16:52.580</a></span> | <span class="t">happens. 4.01 cubed is 64.48. Now, of course, you know, this is just sort of showing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1020" target="_blank">00:17:00.020</a></span> | <span class="t">for a small fiddle, but, you know, that's an approximation to the actual truth. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1026" target="_blank">00:17:06.820</a></span> | <span class="t">So then we sort of ramp up to the more complex cases, which are more reflective of what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1032" target="_blank">00:17:12.460</a></span> | <span class="t">do with neural networks. So if we have a function with one output and n inputs, then we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1039" target="_blank">00:17:19.740</a></span> | <span class="t">a gradient. So a gradient is a vector of partial derivatives with respect to each input. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1046" target="_blank">00:17:26.180</a></span> | <span class="t">we've got n inputs, x1 to xn, and we're working out the partial derivative f with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1051" target="_blank">00:17:31.660</a></span> | <span class="t">to x1, the partial derivative f with respect to x2, et cetera. And we then get a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1059" target="_blank">00:17:39.460</a></span> | <span class="t">of partial derivatives, where each element of this vector is just like a simple derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1066" target="_blank">00:17:46.380</a></span> | <span class="t">with respect to one variable. Okay. So from that point, we just keep on ramping up for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1073" target="_blank">00:17:53.380</a></span> | <span class="t">what we do with neural networks. So commonly when we have something like a layer in a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1079" target="_blank">00:17:59.860</a></span> | <span class="t">network, we'll have a function with n inputs that will be like our word vectors. Then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1087" target="_blank">00:18:07.140</a></span> | <span class="t">do something like multiply by a matrix, and then we'll have m outputs. So we have a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1093" target="_blank">00:18:13.700</a></span> | <span class="t">now which is taking n inputs and is producing m outputs. So at this point, what we're calculating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1103" target="_blank">00:18:23.100</a></span> | <span class="t">for the gradient is what's called a Jacobian matrix. So for m inputs and n outputs, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1110" target="_blank">00:18:30.540</a></span> | <span class="t">Jacobian is an m by n matrix of every combination of partial derivatives. So function f splits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1120" target="_blank">00:18:40.280</a></span> | <span class="t">up into these different sub functions, f1 through m, fm, which generate each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1127" target="_blank">00:18:47.660</a></span> | <span class="t">m outputs. And so then we're taking the partial derivative f1 with respect to x1 through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1134" target="_blank">00:18:54.380</a></span> | <span class="t">partial derivative of f1 with respect to xn. Then heading down, you know, we make it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1139" target="_blank">00:18:59.700</a></span> | <span class="t">to the partial derivative of fm with respect to x1, et cetera. So we have every possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1145" target="_blank">00:19:05.580</a></span> | <span class="t">partial derivative of an output variable with respect to one of the input variables. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1154" target="_blank">00:19:14.940</a></span> | <span class="t">So in simple calculus, when you have a composition of one variable functions, so that if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1163" target="_blank">00:19:23.100</a></span> | <span class="t">have y equals x squared and then z equals 3y, that's then z is a composition of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1173" target="_blank">00:19:33.260</a></span> | <span class="t">functions of - or you're composing two functions to get z as a function of x. Then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1180" target="_blank">00:19:40.020</a></span> | <span class="t">work out the derivative of z with respect to x. And the way you do that is with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1185" target="_blank">00:19:45.220</a></span> | <span class="t">chain rule. And so in the chain rule, you multiply derivatives. So dz dx equals dz dy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1193" target="_blank">00:19:53.180</a></span> | <span class="t">times dy dx. So dz dy is just 3 and dy dx is 2x. So we get 3 times 2x. So that overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1207" target="_blank">00:20:07.180</a></span> | <span class="t">derivative here is 6x. And since if we multiply this together, we're really saying that z</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1214" target="_blank">00:20:14.940</a></span> | <span class="t">equals 3x squared, you should trivially be able to see again, aha, its derivative is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1221" target="_blank">00:20:21.940</a></span> | <span class="t">6x. So that works. Okay. So once we move into vectors and matrices and Jacobians, it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1230" target="_blank">00:20:30.780</a></span> | <span class="t">the same game. So when we're working with those, we can compose functions and work out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1236" target="_blank">00:20:36.380</a></span> | <span class="t">the derivatives by simply multiplying Jacobians. So if we have start with an input x and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1244" target="_blank">00:20:44.060</a></span> | <span class="t">put it through the simplest form of neural network layer and say that z equals wx plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1251" target="_blank">00:20:51.100</a></span> | <span class="t">b. So we multiply the x vector by matrix w and then add on a bias vector b. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1257" target="_blank">00:20:57.620</a></span> | <span class="t">typically we'd put things through a nonlinearity f. So f could be a sigmoid function. We'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1263" target="_blank">00:21:03.900</a></span> | <span class="t">then say h equals f of z. So this is the composition of two functions in terms of vectors and matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1273" target="_blank">00:21:13.300</a></span> | <span class="t">So we can use Jacobians and we can say the partial of h with respect to x is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1278" target="_blank">00:21:18.980</a></span> | <span class="t">be the product of the partial of h with respect to z and the partial of z with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1285" target="_blank">00:21:25.940</a></span> | <span class="t">x. And this all does work out. So let's start going through some examples of how these things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1295" target="_blank">00:21:35.220</a></span> | <span class="t">work slightly more concretely. First, just particular Jacobians and then composing them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1303" target="_blank">00:21:43.220</a></span> | <span class="t">together. So one case we look at is the nonlinearities that we put a vector through. So this is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1311" target="_blank">00:21:51.620</a></span> | <span class="t">like putting a vector through the sigmoid function f. And so if we have an intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1319" target="_blank">00:21:59.740</a></span> | <span class="t">vector z and we're turning into a vector h by putting it through a logistic function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1325" target="_blank">00:22:05.820</a></span> | <span class="t">we can say what is dh dz? Well, for this, formally this is a function that has n inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1340" target="_blank">00:22:20.140</a></span> | <span class="t">and n outputs. So at the end of the day, we're computing an n by n Jacobian. And so what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1348" target="_blank">00:22:28.860</a></span> | <span class="t">that's meaning is the elements of this n by n Jacobian are going to take the partial derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1356" target="_blank">00:22:36.460</a></span> | <span class="t">of each output with respect to each input. And well, what is that going to be in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1364" target="_blank">00:22:44.780</a></span> | <span class="t">case? Well, in this case, because we're actually just computing element-wise a transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1374" target="_blank">00:22:54.340</a></span> | <span class="t">such as a logistic transform of each element zi, like the second equation here, if i equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1383" target="_blank">00:23:03.420</a></span> | <span class="t">j, we've got something to compute. Whereas if i doesn't equal j, there's just the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1390" target="_blank">00:23:10.820</a></span> | <span class="t">has no influence on the output. And so the derivative is zero. So if i doesn't equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1396" target="_blank">00:23:16.460</a></span> | <span class="t">j, we're going to get a zero. And if i does equal j, then we're going to get the regular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1402" target="_blank">00:23:22.820</a></span> | <span class="t">one variable derivative of the logistic function, which if I remember correctly, you were asked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1411" target="_blank">00:23:31.660</a></span> | <span class="t">to compute. Now I can't remember what's assignment one or assignment two, but one of the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1416" target="_blank">00:23:36.660</a></span> | <span class="t">asks you to compute it. So our Jacobian for this case looks like this. We have a diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1423" target="_blank">00:23:43.380</a></span> | <span class="t">matrix with the derivatives of each element along the diagonal and everything else is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1432" target="_blank">00:23:52.540</a></span> | <span class="t">zero. Okay, so let's look at a couple of other Jacobians. So if we are asking, if we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1442" target="_blank">00:24:02.180</a></span> | <span class="t">this WX plus B basic neural network layer, and we're asking for the gradient with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1448" target="_blank">00:24:08.900</a></span> | <span class="t">to X, then what we're going to have coming out is that that's actually going to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1455" target="_blank">00:24:15.420</a></span> | <span class="t">matrix W. So this is where, what I hope you can do is look at the notes at home and work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1464" target="_blank">00:24:24.740</a></span> | <span class="t">through this exactly and see that this is actually the right answer. But this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1472" target="_blank">00:24:32.820</a></span> | <span class="t">way in which if you just have faith and think this is just like single variable calculus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1480" target="_blank">00:24:40.900</a></span> | <span class="t">except I've now got vectors and matrices, the answer you get is actually what you expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1485" target="_blank">00:24:45.540</a></span> | <span class="t">to get, because this is just like the derivative of AX plus B with respect to X where it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1492" target="_blank">00:24:52.780</a></span> | <span class="t">A. So similarly, if we take the partial derivative with respect to B of WX plus B, we get out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1501" target="_blank">00:25:01.740</a></span> | <span class="t">the identity matrix. Okay, then one other Jacobian that we mentioned while in the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1510" target="_blank">00:25:10.260</a></span> | <span class="t">lecture while working through Word2Vec is if you have the dot product of two vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1518" target="_blank">00:25:18.340</a></span> | <span class="t">i.e. that's a number, that what you get coming out of that, so the partial derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1528" target="_blank">00:25:28.900</a></span> | <span class="t">UTH with respect to U is H transpose. And at this point, there's some fine print that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1536" target="_blank">00:25:36.660</a></span> | <span class="t">I'm going to come back to in a minute. So this is the correct Jacobian, right? Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1543" target="_blank">00:25:43.580</a></span> | <span class="t">in this case, we have the dimension of H inputs and we have one output. And so we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1553" target="_blank">00:25:53.660</a></span> | <span class="t">have a row vector. But there's a little bit more to say on that that I'll come back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1560" target="_blank">00:26:00.740</a></span> | <span class="t">in about 20 slides. But this is the correct Jacobian. Okay, so if you are not familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1568" target="_blank">00:26:08.900</a></span> | <span class="t">with these kind of Jacobians, do please look at some of the notes that are available and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1576" target="_blank">00:26:16.540</a></span> | <span class="t">try and compute these in more detail element-wise and convince yourself that they really are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1581" target="_blank">00:26:21.740</a></span> | <span class="t">right. But I'm going to assume these now and show you what happens when we actually then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1587" target="_blank">00:26:27.740</a></span> | <span class="t">work out gradients for at least a mini little neural net. Okay. So here is most of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1601" target="_blank">00:26:41.660</a></span> | <span class="t">neural net. I mean, as I commented, that, you know, really we'd be working out the partial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1609" target="_blank">00:26:49.140</a></span> | <span class="t">derivative of the loss J with respect to these variables. But for the example I'm doing here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1616" target="_blank">00:26:56.140</a></span> | <span class="t">I've locked that off to keep it a little simpler and more manageable for the lecture. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1620" target="_blank">00:27:00.980</a></span> | <span class="t">we're going to just work out the partial derivative of the score S, which is a real number with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1627" target="_blank">00:27:07.100</a></span> | <span class="t">respect to the different parameters of this model, where the parameters of this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1633" target="_blank">00:27:13.060</a></span> | <span class="t">are going to be the W and the B and the U and also the input, because we can update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1642" target="_blank">00:27:22.500</a></span> | <span class="t">the weight vectors of the word vectors of different words based on tuning them to better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1650" target="_blank">00:27:30.900</a></span> | <span class="t">predict the classification outputs that we desire. So let's start off with a fairly easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1656" target="_blank">00:27:36.980</a></span> | <span class="t">one where we want to update the bias vector B to have our system classify better. So to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1665" target="_blank">00:27:45.980</a></span> | <span class="t">be able to do that, what we want to work out is the partial derivatives of S with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1671" target="_blank">00:27:51.380</a></span> | <span class="t">to B. So we know how to put that into our stochastic gradient update for the B parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1680" target="_blank">00:28:00.020</a></span> | <span class="t">Okay. So how do we go about doing these things? So the first step is we want to sort of break</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1687" target="_blank">00:28:07.100</a></span> | <span class="t">things up into different functions of minimal complexity that compose together. So in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1695" target="_blank">00:28:15.900</a></span> | <span class="t">this neural net layer, H equals F of WX plus B, it's still a little bit complex. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1701" target="_blank">00:28:21.780</a></span> | <span class="t">decompose that one further step. So we have the input X, we then calculate the linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1710" target="_blank">00:28:30.980</a></span> | <span class="t">transformation Z equals WX plus B. And then we put things through the sort of element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1720" target="_blank">00:28:40.780</a></span> | <span class="t">wise nonlinearity, H equals F of Z, and then we do the dot product with U. And, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1730" target="_blank">00:28:50.380</a></span> | <span class="t">it's useful for working these things out to, you know, split into pieces like this, have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1737" target="_blank">00:28:57.660</a></span> | <span class="t">straight what your different variables are, and to know what the dimensionality of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1742" target="_blank">00:29:02.740</a></span> | <span class="t">of these variables is. It's well worth just writing out the dimensionality of every variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1748" target="_blank">00:29:08.500</a></span> | <span class="t">and making sure that the answers that you're computing are of the right dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1754" target="_blank">00:29:14.140</a></span> | <span class="t">So at this point, though, what we can see is that calculating S is the product of three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1762" target="_blank">00:29:22.020</a></span> | <span class="t">- sorry, is the composition of three functions around X. So for working out the partials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1771" target="_blank">00:29:31.020</a></span> | <span class="t">of S with respect to B, it's the composition of the three functions shown on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1779" target="_blank">00:29:39.500</a></span> | <span class="t">And so therefore, the gradient of S with respect to B, we're going to take the product of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1789" target="_blank">00:29:49.500</a></span> | <span class="t">three partial derivatives. Okay, so how do - what do we - so, you know, we've got the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1799" target="_blank">00:29:59.860</a></span> | <span class="t">S equals UTH, so that's the sort of the top corresponding partial derivative, partial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1806" target="_blank">00:30:06.740</a></span> | <span class="t">derivative of H with respect to Z, partial derivative of Z with respect to B, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1813" target="_blank">00:30:13.380</a></span> | <span class="t">the first one that we're working out. Okay, so we want to work this out, and if we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1819" target="_blank">00:30:19.340</a></span> | <span class="t">lucky, we remember those Jacobians I showed previously about the Jacobian for a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1826" target="_blank">00:30:26.420</a></span> | <span class="t">dot product, the Jacobian for the non-linearity, and the Jacobian for the simple linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1836" target="_blank">00:30:36.460</a></span> | <span class="t">And so we can use those. So for the partials of S with respect to H, well, that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1845" target="_blank">00:30:45.940</a></span> | <span class="t">to be UT using the first one. The partials of H with respect to Z, okay, so that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1853" target="_blank">00:30:53.540</a></span> | <span class="t">non-linearity, and so that's going to be the matrix, it's the diagonal matrix with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1860" target="_blank">00:31:00.580</a></span> | <span class="t">element wise derivative F prime of Z and zero elsewhere. And then for the WX plus B, when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1869" target="_blank">00:31:09.780</a></span> | <span class="t">we're taking the partials with respect to B, that's just the identity matrix. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1876" target="_blank">00:31:16.140</a></span> | <span class="t">can simplify that down a little, the identity matrix disappears, and since UT is a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1887" target="_blank">00:31:27.820</a></span> | <span class="t">and this is a diagonal matrix, we can rewrite this as UT Hadamard product of F prime of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1895" target="_blank">00:31:35.260</a></span> | <span class="t">Z. I think this is the first time I've used this little circle for a Hadamard product,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1901" target="_blank">00:31:41.380</a></span> | <span class="t">but it's something that you'll see quite a bit in your network work, since it's often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1907" target="_blank">00:31:47.740</a></span> | <span class="t">used. So when we have two vectors, UT and this vector here, sometimes you want to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1916" target="_blank">00:31:56.860</a></span> | <span class="t">an element wise product. So the output of this will be a vector where you've taken the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1922" target="_blank">00:32:02.140</a></span> | <span class="t">first element of each and multiplied them, the second element of each and multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1926" target="_blank">00:32:06.180</a></span> | <span class="t">them, et cetera, downwards. And so that's called the Hadamard product, and it's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1931" target="_blank">00:32:11.100</a></span> | <span class="t">we're calculating as to calculate a vector, which is the gradient of S with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1940" target="_blank">00:32:20.140</a></span> | <span class="t">B. Okay. So that's good. So we now have a gradient of S with respect to B, and we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1950" target="_blank">00:32:30.260</a></span> | <span class="t">use that in our stochastic gradient, but we don't stop there. We also want to work out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1957" target="_blank">00:32:37.940</a></span> | <span class="t">the gradient with respect to others of our parameters. So we might want to next go on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1965" target="_blank">00:32:45.540</a></span> | <span class="t">and work out the gradient of S with respect to W. Well, we can use the chain rule just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1974" target="_blank">00:32:54.660</a></span> | <span class="t">like we did before, right? So we've got the same product of functions and everything is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1981" target="_blank">00:33:01.040</a></span> | <span class="t">going to be the same apart from now taking the derivatives with respect to W rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1988" target="_blank">00:33:08.220</a></span> | <span class="t">B. So it's now going to be the partial of S with respect to H, H with respect to Z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=1996" target="_blank">00:33:16.620</a></span> | <span class="t">and Z with respect to W. And the important thing to notice here, and this leads into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2004" target="_blank">00:33:24.220</a></span> | <span class="t">what we do with the backpropagation algorithm, is wait a minute, this is very similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2012" target="_blank">00:33:32.420</a></span> | <span class="t">what we've already done. So when we were working out the gradients of S with respect to B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2018" target="_blank">00:33:38.620</a></span> | <span class="t">the first two terms were exactly the same. It's only the last one that differs. So to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2026" target="_blank">00:33:46.620</a></span> | <span class="t">be able to build or to train neural networks efficiently, this is what happens all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2034" target="_blank">00:33:54.460</a></span> | <span class="t">time. And it's absolutely essential that we use an algorithm that avoids repeated computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2042" target="_blank">00:34:02.980</a></span> | <span class="t">And so the idea we're going to develop is when we have this equation stack, that there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2049" target="_blank">00:34:09.220</a></span> | <span class="t">sort of stuff that's above where we compute Z, and we're going to be sort of, that'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2055" target="_blank">00:34:15.420</a></span> | <span class="t">be the same each time. And we want to compute something from that, that we can then sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2061" target="_blank">00:34:21.700</a></span> | <span class="t">of feed downwards when working out the gradients with respect to W, X, or B. And so we do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2072" target="_blank">00:34:32.540</a></span> | <span class="t">by defining delta, which is delta is the partials composed that are above the linear transform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2082" target="_blank">00:34:42.660</a></span> | <span class="t">And that's referred to as the local error signal. It's what's being passed in from above</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2088" target="_blank">00:34:48.620</a></span> | <span class="t">the linear transform. And we've already computed the gradient of that in the preceding slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2097" target="_blank">00:34:57.200</a></span> | <span class="t">And so the final form of the partial of S with respect to B will be delta times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2107" target="_blank">00:35:07.700</a></span> | <span class="t">remaining part. And well, we'd seen that, you know, for partial S with respect to B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2115" target="_blank">00:35:15.300</a></span> | <span class="t">the partial of Z with respect to B is just the identity. So the end result was delta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2120" target="_blank">00:35:20.820</a></span> | <span class="t">But in this time, we then going to have to work out the partial of Z with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2125" target="_blank">00:35:25.540</a></span> | <span class="t">W and multiply that by delta. So that's the part that we still haven't yet done. So, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2135" target="_blank">00:35:35.260</a></span> | <span class="t">this is where things get, in some sense, a little bit hairier. And so there's something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2144" target="_blank">00:35:44.060</a></span> | <span class="t">that's important to explain. So, you know, what should we have for the Jacobian of DSDW?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2153" target="_blank">00:35:53.580</a></span> | <span class="t">Well, that's a function that has one output. The output is just a score, a real number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2164" target="_blank">00:36:04.240</a></span> | <span class="t">And then it has N by M inputs. So the Jacobian is a 1 by N by M matrix, i.e. a very long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2176" target="_blank">00:36:16.620</a></span> | <span class="t">row vector. But that's correct math. But it turns out that that's kind of bad for our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2184" target="_blank">00:36:24.940</a></span> | <span class="t">neural networks. Because remember, what we want to do with our neural networks is do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2190" target="_blank">00:36:30.420</a></span> | <span class="t">stochastic gradient descent. And we want to say theta new equals theta old minus a small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2198" target="_blank">00:36:38.220</a></span> | <span class="t">multiplier times the gradient. And well, actually, the W matrix is an N by M matrix. And so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2213" target="_blank">00:36:53.620</a></span> | <span class="t">couldn't actually do this subtraction if this gradient we calculate is just a huge row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2221" target="_blank">00:37:01.060</a></span> | <span class="t">We'd like to have it as the same shape as the W matrix. In neural network land, when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2227" target="_blank">00:37:07.900</a></span> | <span class="t">we do this, we depart from pure math at this point. And we use what we call the shape convention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2235" target="_blank">00:37:15.900</a></span> | <span class="t">So what we're going to say is, and you're meant to use this for answers in the assignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2242" target="_blank">00:37:22.400</a></span> | <span class="t">that the shape of the gradient we're always going to make to be the shape of the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2248" target="_blank">00:37:28.900</a></span> | <span class="t">And so therefore, DSDW, we are also going to represent as an N by M matrix just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2257" target="_blank">00:37:37.640</a></span> | <span class="t">W. And we're going to reshape the Jacobian to place it into this matrix shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2266" target="_blank">00:37:46.100</a></span> | <span class="t">Okay, so if we want to place it into this matrix shape, what do we, what are we going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2275" target="_blank">00:37:55.700</a></span> | <span class="t">to want to get for DSDW? Well, we know that it's going to involve delta, our local error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2287" target="_blank">00:38:07.100</a></span> | <span class="t">signal. And then we have to work out something for DZDW. Well, since C equals WX plus B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2303" target="_blank">00:38:23.420</a></span> | <span class="t">you'd kind of expect that the answer should be X. And that's right. So the answer to DSDW</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2313" target="_blank">00:38:33.940</a></span> | <span class="t">is going to be delta transpose times X transpose. And so the form that we're getting for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2322" target="_blank">00:38:42.420</a></span> | <span class="t">derivative is going to be the product of the local error signal that comes from above versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2332" target="_blank">00:38:52.900</a></span> | <span class="t">what we calculate from the local input X. So that shouldn't yet be obvious why that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2340" target="_blank">00:39:00.300</a></span> | <span class="t">is true. So let me just go through in a bit more detail why that's true. So when we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2347" target="_blank">00:39:07.140</a></span> | <span class="t">to work out DSDW, right, it's sort of delta times DZDW, where what that's computing for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2357" target="_blank">00:39:17.780</a></span> | <span class="t">Z is WX plus B. So let's just consider for a moment what the derivative is with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2365" target="_blank">00:39:25.860</a></span> | <span class="t">to a single weight WIJ. So WIJ might be W2 3 that's shown in my little neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2375" target="_blank">00:39:35.460</a></span> | <span class="t">here. And so the first thing to notice is that WIJ only contributes to ZI. So it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2386" target="_blank">00:39:46.380</a></span> | <span class="t">into Z2, which then computes H2. And it has no effect whatsoever on H1. OK, so when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2399" target="_blank">00:39:59.020</a></span> | <span class="t">working out DZI, DWIJ, it's going to be DWIX, that sort of row of-- that row of the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2411" target="_blank">00:40:11.820</a></span> | <span class="t">plus BI, which means that for-- we've got a kind of a sum of WIK times XK. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2422" target="_blank">00:40:22.140</a></span> | <span class="t">for this sum, this is like one variable calculus that when we're taking the derivative of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2428" target="_blank">00:40:28.220</a></span> | <span class="t">with respect to WIJ, every term in this sum is going to be 0. The derivative is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2436" target="_blank">00:40:36.220</a></span> | <span class="t">to be 0 except for the one that involves WIJ. And then the derivative of that is just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2442" target="_blank">00:40:42.740</a></span> | <span class="t">AX with respect to A. It's going to be X. So you get XJ out as the answer. And so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2452" target="_blank">00:40:52.180</a></span> | <span class="t">end result of that is that when we're working out, what we want as the answer is that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2459" target="_blank">00:40:59.620</a></span> | <span class="t">going to get that these columns where X1 is all that's left, X2 is all that's left, through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2470" target="_blank">00:41:10.180</a></span> | <span class="t">XM is all that's left. And then that's multiplied by the vectors of the local error signal from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2478" target="_blank">00:41:18.620</a></span> | <span class="t">above. And what we want to compute is this outer product matrix where we're getting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2484" target="_blank">00:41:24.740</a></span> | <span class="t">different combinations of the delta and the X. And so we can get the N by M matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2493" target="_blank">00:41:33.620</a></span> | <span class="t">we'd like to have via our shape convention by taking delta transpose, which is N by 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2501" target="_blank">00:41:41.620</a></span> | <span class="t">times X transpose, which is N1 by M. And then we get this outer product matrix. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2509" target="_blank">00:41:49.500</a></span> | <span class="t">a kind of a hacky argument that I've made. It's certainly a way of doing things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2514" target="_blank">00:41:54.540</a></span> | <span class="t">the dimensions work out and sort of make sense. There's a more detailed run through this that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2521" target="_blank">00:42:01.300</a></span> | <span class="t">appears in the lecture notes. And I encourage you to sort of also look at the more mathy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2528" target="_blank">00:42:08.180</a></span> | <span class="t">version of that. Here's a little bit more information about the shape convention. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2534" target="_blank">00:42:14.820</a></span> | <span class="t">well, first of all, one more example of this. So when you're working out the SDB, that comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2549" target="_blank">00:42:29.580</a></span> | <span class="t">out as it's Jacobian is a row vector. But similarly, you know, according to the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2557" target="_blank">00:42:37.820</a></span> | <span class="t">convention, we want our gradient to be the same shape as B and B is a column vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2566" target="_blank">00:42:46.380</a></span> | <span class="t">So that's sort of, again, they're different shapes and you have to transpose one to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2571" target="_blank">00:42:51.340</a></span> | <span class="t">the other. And so effectively what we have is a disagreement between the Jacobian form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2578" target="_blank">00:42:58.180</a></span> | <span class="t">So the Jacobian form makes sense for, you know, calculus and math, because if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2583" target="_blank">00:43:03.620</a></span> | <span class="t">to have it like I claimed that matrix calculus is just like single variable calculus apart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2591" target="_blank">00:43:11.020</a></span> | <span class="t">from using vectors and matrices, you can just multiply together the partials. That only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2596" target="_blank">00:43:16.180</a></span> | <span class="t">works out if you're using Jacobians. But on the other hand, if you want to do stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2603" target="_blank">00:43:23.860</a></span> | <span class="t">gradient descent and be able to sort of subtract off a piece of the gradient, that only works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2612" target="_blank">00:43:32.820</a></span> | <span class="t">if you have the same shape matrix for the gradient as you do for the original matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2621" target="_blank">00:43:41.400</a></span> | <span class="t">And so this is a bit confusing, but that's just the reality. There are both of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2627" target="_blank">00:43:47.340</a></span> | <span class="t">two things. So the Jacobian form is useful in doing the calculus. But for the answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2636" target="_blank">00:43:56.860</a></span> | <span class="t">in the assignment, we want the answers to be presented using the shape convention so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2643" target="_blank">00:44:03.700</a></span> | <span class="t">that the gradient is shown in the same shape as the parameters. And therefore, you'll be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2652" target="_blank">00:44:12.380</a></span> | <span class="t">able to it's the right shape for doing a gradient update by just subtracting a small amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2659" target="_blank">00:44:19.100</a></span> | <span class="t">of the gradient. So for working through things, there are then basically two choices. One</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2668" target="_blank">00:44:28.500</a></span> | <span class="t">choice is to work through all the math using Jacobians and then right at the end to reshape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2677" target="_blank">00:44:37.780</a></span> | <span class="t">following the shape convention to give the answer. So that's what I did when I worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2682" target="_blank">00:44:42.740</a></span> | <span class="t">out DSDB. We worked through it using Jacobians. We got an answer, but it turned out to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2693" target="_blank">00:44:53.620</a></span> | <span class="t">a row vector. And so, well, then we have to transpose it at the end to get it into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2699" target="_blank">00:44:59.100</a></span> | <span class="t">right shape for the shape convention. The alternative is to always follow the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2708" target="_blank">00:45:08.740</a></span> | <span class="t">convention. And that's kind of what I did when I was then working out DSDW. I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2716" target="_blank">00:45:16.540</a></span> | <span class="t">fully use Jacobians. I said, oh, well, when we work out whatever it was, DZDW, let's work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2726" target="_blank">00:45:26.260</a></span> | <span class="t">out what shape we want it to be and what to fill in the cells with. And if you're sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2732" target="_blank">00:45:32.940</a></span> | <span class="t">of trying to do it immediately with the shape convention, it's a little bit more hacky in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2741" target="_blank">00:45:41.460</a></span> | <span class="t">a way since you have to look at the dimensions for what you want and figure out when to transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2747" target="_blank">00:45:47.740</a></span> | <span class="t">or to reshape the matrix to be at the right shape. But the kind of informal reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2754" target="_blank">00:45:54.100</a></span> | <span class="t">that I gave is what you do and what works. And one way of - and there are sort of hints</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2761" target="_blank">00:46:01.940</a></span> | <span class="t">that you can use, right, that you know that your gradient should always be the same shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2767" target="_blank">00:46:07.300</a></span> | <span class="t">as your parameters, and you know that the error message coming in will always have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2773" target="_blank">00:46:13.420</a></span> | <span class="t">same dimensionality as that hidden layer, and you can sort of work it out always following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2779" target="_blank">00:46:19.060</a></span> | <span class="t">the shape convention. Okay. So that is, hey, doing this is all matrix calculus. So after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2795" target="_blank">00:46:35.180</a></span> | <span class="t">pausing for breath for a second, the rest of the lecture is then, okay, let's look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2806" target="_blank">00:46:46.580</a></span> | <span class="t">how our software trains neural networks using what's referred to as the back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2815" target="_blank">00:46:55.380</a></span> | <span class="t">algorithm. So the short answer is, you know, basically we've already done it. The rest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2831" target="_blank">00:47:11.580</a></span> | <span class="t">of the lecture is easy. So, you know, essentially I've just shown you what the back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2838" target="_blank">00:47:18.660</a></span> | <span class="t">algorithm does. So the back propagation algorithm is judiciously taking and propagating derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2854" target="_blank">00:47:34.060</a></span> | <span class="t">using the matrix chain rule. The rest of the back propagation algorithm is to say, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2863" target="_blank">00:47:43.620</a></span> | <span class="t">when we have these neural networks, we have a lot of shared structure and shared derivatives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2872" target="_blank">00:47:52.360</a></span> | <span class="t">So what we want to do is maximally efficiently reuse derivatives of higher layers when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2882" target="_blank">00:48:02.180</a></span> | <span class="t">computing derivatives for lower layers so that we minimize computation. And I already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2888" target="_blank">00:48:08.380</a></span> | <span class="t">pointed that out in the first half, but we want to systematically exploit that. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2895" target="_blank">00:48:15.300</a></span> | <span class="t">the way we do that in our computational systems is they construct computation graphs. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2904" target="_blank">00:48:24.420</a></span> | <span class="t">maybe looks a little bit like what you saw in a compiler's class if you did one, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2911" target="_blank">00:48:31.060</a></span> | <span class="t">that you're creating, I call it here a computation graph, but it's really a tree, right? So you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2917" target="_blank">00:48:37.300</a></span> | <span class="t">creating here this tree of computations in this case, but in more general case, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2924" target="_blank">00:48:44.380</a></span> | <span class="t">some kind of directed graph of computations, which has source nodes, which are inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2934" target="_blank">00:48:54.340</a></span> | <span class="t">either inputs like X or input parameters like W and B, and it's interior nodes operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2943" target="_blank">00:49:03.920</a></span> | <span class="t">And so then once we've constructed a graph, and so this graph corresponds to exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2949" target="_blank">00:49:09.380</a></span> | <span class="t">example I did before, right? This was our little neural net that's in the top right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2954" target="_blank">00:49:14.100</a></span> | <span class="t">And here's the corresponding computation graph of computing WX plus B, put it through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2961" target="_blank">00:49:21.460</a></span> | <span class="t">sigmoid non-linearity F, multiply the resulting dot product of the resulting vector with U</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2968" target="_blank">00:49:28.620</a></span> | <span class="t">gives us our output score S. Okay, so what we do to compute this is we pass along the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2978" target="_blank">00:49:38.180</a></span> | <span class="t">edges the results of operations. So this is WX, then Z, then H, and then our output is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2984" target="_blank">00:49:44.420</a></span> | <span class="t">S. And so the first thing we want to be able to do to compute with neural networks is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2990" target="_blank">00:49:50.980</a></span> | <span class="t">be able to compute for different inputs what the output is. And so that's referred to as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=2997" target="_blank">00:49:57.780</a></span> | <span class="t">forward propagation. And so we simply run this expression much like you'd standardly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3007" target="_blank">00:50:07.740</a></span> | <span class="t">do in a compiler to compute the value of S, and that's the forward propagation phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3014" target="_blank">00:50:14.460</a></span> | <span class="t">But the essential additional element of neural networks is that we then also want to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3021" target="_blank">00:50:21.460</a></span> | <span class="t">to send back gradients, which will tell us how to update the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3028" target="_blank">00:50:28.140</a></span> | <span class="t">And so it's this ability to send back gradients, which gives us the ability for these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3035" target="_blank">00:50:35.180</a></span> | <span class="t">to learn. Once we have a loss function at the end, we can work out how to change the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3041" target="_blank">00:50:41.140</a></span> | <span class="t">parameters of the model so that they more accurately produce the desired output, i.e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3048" target="_blank">00:50:48.580</a></span> | <span class="t">they minimize the loss. And so it's doing that part that then is called back propagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3056" target="_blank">00:50:56.740</a></span> | <span class="t">So we then, once we forward propagated a value with our current parameters, we then head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3065" target="_blank">00:51:05.540</a></span> | <span class="t">backwards reversing the direction of the arrows and pass along gradients down to the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3073" target="_blank">00:51:13.460</a></span> | <span class="t">parameters like B and W and U that we can use to change using stochastic gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3081" target="_blank">00:51:21.260</a></span> | <span class="t">what the value of B is and what the value of W is. So we start off with DSDS, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3087" target="_blank">00:51:27.620</a></span> | <span class="t">is just one, and then we run our back propagation, and we're using the sort of same kind of composition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3095" target="_blank">00:51:35.500</a></span> | <span class="t">of Jacobian. So we have DSDH here and DSDZ, and we progressively pass back those gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3106" target="_blank">00:51:46.260</a></span> | <span class="t">So we just need to work out how to efficiently and cleanly do this in a computational system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3113" target="_blank">00:51:53.940</a></span> | <span class="t">And so let's sort of work through again a few of these cases. So the general situation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3120" target="_blank">00:52:00.060</a></span> | <span class="t">is we have a particular node. So a node is where some kind of operation like multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3130" target="_blank">00:52:10.340</a></span> | <span class="t">or a non-linearity happens. And so the simplest case is that we've got one output and one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3139" target="_blank">00:52:19.100</a></span> | <span class="t">input. So we'll do that first. So that's like H equals F of Z. So what we have is an upstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3147" target="_blank">00:52:27.540</a></span> | <span class="t">gradient, DSDH, and what we want to do is compute the downstream gradient of DSDZ. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3159" target="_blank">00:52:39.220</a></span> | <span class="t">the way we're going to do that is say, well, for this function F, it's a function, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3166" target="_blank">00:52:46.780</a></span> | <span class="t">got a derivative, a gradient. So what we want to do is work out that local gradient, DHDZ,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3176" target="_blank">00:52:56.020</a></span> | <span class="t">and then that gives us everything that we need to work out DSDZ, because that's precisely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3183" target="_blank">00:53:03.340</a></span> | <span class="t">we're going to use the chain rule. We're going to say that DSDZ equals the product of DSDH</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3189" target="_blank">00:53:09.700</a></span> | <span class="t">times DHDZ, where this is again using Jacobians. Okay. So the general principle that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3197" target="_blank">00:53:17.140</a></span> | <span class="t">going to use is the downstream gradient equals the upstream gradient times the local gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3203" target="_blank">00:53:23.740</a></span> | <span class="t">Okay. Sometimes it gets a little bit more complicated. So we might have multiple inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3209" target="_blank">00:53:29.980</a></span> | <span class="t">to a function. So this is the matrix vector multiply. So Z equals WX. Okay. When there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3217" target="_blank">00:53:37.820</a></span> | <span class="t">are multiple inputs, we still have an upstream gradient, DSDZ. But what we're going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3227" target="_blank">00:53:47.220</a></span> | <span class="t">is work out a local gradient with respect to each input. So we have DZDW and DZDX. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3236" target="_blank">00:53:56.260</a></span> | <span class="t">so then at that point, it's exactly the same for each piece of it. We're going to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3241" target="_blank">00:54:01.980</a></span> | <span class="t">out the downstream gradients, DSDW and DSDX by using the chain rule with respect to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3251" target="_blank">00:54:11.180</a></span> | <span class="t">local gradient. So let's go through an example of this. I mean, this is kind of a silly example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3260" target="_blank">00:54:20.900</a></span> | <span class="t">It's not really an example that looks like a typical neural net, but it's sort of a simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3265" target="_blank">00:54:25.740</a></span> | <span class="t">example where we can show some of the components of what we do. So what we're going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3271" target="_blank">00:54:31.620</a></span> | <span class="t">want to calculate F of XYZ, which is being calculated as X plus Y times the max of Y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3280" target="_blank">00:54:40.740</a></span> | <span class="t">and Z. And we've got, you know, particular values that we're starting off with. X equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3287" target="_blank">00:54:47.900</a></span> | <span class="t">1, Y equals 2, and Z equals 0. So these are the current values of our parameters. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3294" target="_blank">00:54:54.780</a></span> | <span class="t">so we can say, okay, well, we want to build an expression tree for that. Here's our expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3301" target="_blank">00:55:01.860</a></span> | <span class="t">tree. We're taking X plus Y, we're taking the max of Y and Z, and then we're multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3308" target="_blank">00:55:08.740</a></span> | <span class="t">them. And so our forward propagation phase is just to run this. So we take the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3316" target="_blank">00:55:16.620</a></span> | <span class="t">of our parameters and we simply start to compute with them, right? So we have 1, 2, 2, 0, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3324" target="_blank">00:55:24.580</a></span> | <span class="t">we add them as 3, the max is 2, we multiply them, and that gives us 6. Okay. So then at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3334" target="_blank">00:55:34.180</a></span> | <span class="t">that point, we then want to go and work out how to do things for back propagation and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3343" target="_blank">00:55:43.860</a></span> | <span class="t">how these back propagation steps work. And so the first part of that is sort of working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3350" target="_blank">00:55:50.460</a></span> | <span class="t">out what our local gradients are going to be. So this is A here, and this is X and Y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3360" target="_blank">00:56:00.260</a></span> | <span class="t">So dA/dX, since A equals X plus Y is just going to be 1, and dA/dy is also going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3368" target="_blank">00:56:08.140</a></span> | <span class="t">be 1. Then for B equals the max of YZ, so this is this max node. So the local gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3380" target="_blank">00:56:20.420</a></span> | <span class="t">for that is, it's going to depend on whether Y is greater than Z. So dB/dy is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3389" target="_blank">00:56:29.500</a></span> | <span class="t">be 1 if and only if Y is greater than Z, which it is at our particular point here, so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3397" target="_blank">00:56:37.740</a></span> | <span class="t">1. And dB/dz is going to be 1 only if Z is greater than Y. So for our particular values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3407" target="_blank">00:56:47.460</a></span> | <span class="t">here, that one is going to be 0. And then finally, here, we're calculating the product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3416" target="_blank">00:56:56.700</a></span> | <span class="t">F equals AB. So for that, we're going to - sorry, that slide is a little imperfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3430" target="_blank">00:57:10.100</a></span> | <span class="t">So for the product, the derivative F with respect to A is equal to B, which is 2, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3436" target="_blank">00:57:16.740</a></span> | <span class="t">the derivative F with respect to B is A equals 3. So that gives us all of the local gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3443" target="_blank">00:57:23.900</a></span> | <span class="t">at each node. And so then to run backpropagation, we start with dF/df, which is just 1, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3452" target="_blank">00:57:32.420</a></span> | <span class="t">then we're going to work out the downstream equals the upstream times the local. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3461" target="_blank">00:57:41.380</a></span> | <span class="t">so the local - so when you have a product like this, note that sort of the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3468" target="_blank">00:57:48.260</a></span> | <span class="t">flip. So we take upstream times the local, which is 2. So the downstream is 2. On this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3483" target="_blank">00:58:03.820</a></span> | <span class="t">side, dF/dB is 3. So we're taking upstream times local, that gives us 3. And so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3494" target="_blank">00:58:14.740</a></span> | <span class="t">gives us backpropagates values to the plus and max nodes. And so then we continue along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3502" target="_blank">00:58:22.700</a></span> | <span class="t">So for the max node, the local gradient dB/dy equals 1. So we're going to take upstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3512" target="_blank">00:58:32.180</a></span> | <span class="t">is 3. So we're going to take 3 times 1, and that gives us 3. dB/dz is 0, because of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3522" target="_blank">00:58:42.220</a></span> | <span class="t">fact that z's value is not the max. So we're taking 3 times 0, and saying the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3528" target="_blank">00:58:48.260</a></span> | <span class="t">there is 0. So finally, doing the plus node, the local gradients for both x and y there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3536" target="_blank">00:58:56.700</a></span> | <span class="t">are 1. So we're just getting 2 times 1 in both cases, and we're saying the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3542" target="_blank">00:59:02.540</a></span> | <span class="t">there are 2. Okay, and so again, at the end of the day, the interpretation here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3551" target="_blank">00:59:11.180</a></span> | <span class="t">this is giving us information as to if we wiggle the values of x, y, and z, how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3558" target="_blank">00:59:18.700</a></span> | <span class="t">of a difference does it make to the output? What is the slope, the gradient, with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3565" target="_blank">00:59:25.300</a></span> | <span class="t">to the variable? So what we've seen is that since z isn't the max of y and z, if I change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3575" target="_blank">00:59:35.860</a></span> | <span class="t">the value of z a little, like if I make z 0.1 or minus 0.1, it makes no difference at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3582" target="_blank">00:59:42.980</a></span> | <span class="t">all to what I compute as the output. So therefore, the gradient there is 0. If I change the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3592" target="_blank">00:59:52.100</a></span> | <span class="t">of x a little, then that is going to have an effect, and it's going to affect the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3602" target="_blank">01:00:02.340</a></span> | <span class="t">by twice as much as the amount I change it. Right, so, and that's because the df/dz equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3617" target="_blank">01:00:17.540</a></span> | <span class="t">2. So interestingly, so I mean, we can basically work that out. So if we imagine making sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3629" target="_blank">01:00:29.580</a></span> | <span class="t">of x 2.1, well, then what we'd calculate the max is 2. Oh, sorry, sorry, if we make x 1.1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3641" target="_blank">01:00:41.740</a></span> | <span class="t">we then get the max here is 2, and we get 1.1 plus 2 is 3.1. So we get 3.1 times 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3651" target="_blank">01:00:51.780</a></span> | <span class="t">So that'd be about 6.2. So changing x by 0.1 has added 0.2 to the value of f. Conversely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3662" target="_blank">01:01:02.100</a></span> | <span class="t">for the value of y, we find the df/dy equals 5. So what we do when we've got two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3669" target="_blank">01:01:09.640</a></span> | <span class="t">coming out here, as I'll go through again in a moment, is we're summing the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3674" target="_blank">01:01:14.840</a></span> | <span class="t">So again, 3 plus 2 equals 5. And empirically, that's what happens. So if we consider fiddling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3681" target="_blank">01:01:21.400</a></span> | <span class="t">the value of y a little, let's say we make it a value of 2.1, then the prediction is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3688" target="_blank">01:01:28.840</a></span> | <span class="t">they'll have five times as big an effect on the output value we compute. And well, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3695" target="_blank">01:01:35.060</a></span> | <span class="t">do we compute? So we compute 1 plus 2.1. So that's 3.1. And we compute the max of 2.1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3705" target="_blank">01:01:45.640</a></span> | <span class="t">and 0 is 2.1. So we'll take the product of 2.1 and 3.1. And I calculate that in advance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3713" target="_blank">01:01:53.480</a></span> | <span class="t">as I can't really do this arithmetic in my head. And the product of those two is 6.51.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3719" target="_blank">01:01:59.400</a></span> | <span class="t">So it has gone up about by 0.5. So we've multiplied my fiddling it by 0.1 by five times to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3728" target="_blank">01:02:08.560</a></span> | <span class="t">out the magnitude of the effect on the output. OK. So for this, before I did the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3740" target="_blank">01:02:20.160</a></span> | <span class="t">when we had one in and one out here and multiple ins and one out here, the case that I hadn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3752" target="_blank">01:02:32.480</a></span> | <span class="t">actually dealt with is the case of when you have multiple outward branches. But that then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3760" target="_blank">01:02:40.240</a></span> | <span class="t">turned up in the computation of y. So once you have multiple outward branches, what you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3767" target="_blank">01:02:47.560</a></span> | <span class="t">doing is you're summing. So that when you want to work out the df/dy, you've got a local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3780" target="_blank">01:03:00.000</a></span> | <span class="t">gradient. You've got two upstream gradients. And you're working it out with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3789" target="_blank">01:03:09.080</a></span> | <span class="t">each of them as in the chain rule. And then you're summing them together to work out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3795" target="_blank">01:03:15.240</a></span> | <span class="t">impact at the end. Right. So we also saw some of the other node intuitions, which is useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3805" target="_blank">01:03:25.840</a></span> | <span class="t">to have doing this. So when you have an addition, that distributes the upstream gradient to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3815" target="_blank">01:03:35.480</a></span> | <span class="t">each of the things below it. When you have max, it's like a routing node. So when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3823" target="_blank">01:03:43.000</a></span> | <span class="t">have max, you have the upstream gradient and it goes to one of the branches below it and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3828" target="_blank">01:03:48.960</a></span> | <span class="t">the rest of them get no gradient. When you then have a multiplication, it has this effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3838" target="_blank">01:03:58.640</a></span> | <span class="t">of switching the gradient. So if you're taking 3 by 2, the gradient on the 2 side is 3 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3847" target="_blank">01:04:07.800</a></span> | <span class="t">on the 3 side is 2. And if you think about in terms of how much effect you get from when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3854" target="_blank">01:04:14.160</a></span> | <span class="t">you're doing this sort of wiggling, that totally makes sense. Right. Because if you're multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3859" target="_blank">01:04:19.400</a></span> | <span class="t">another number by 3, then any change here is going to be multiplied by 3 and vice versa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3868" target="_blank">01:04:28.920</a></span> | <span class="t">Okay. So this is the kind of computation graph that we want to use to work out derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3879" target="_blank">01:04:39.880</a></span> | <span class="t">in an automated computational fashion, which is the basis of the backpropagation algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3887" target="_blank">01:04:47.360</a></span> | <span class="t">But at that point, this is what we're doing, but there's still one mistake that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3893" target="_blank">01:04:53.640</a></span> | <span class="t">make. It would be wrong for us to sort of say, okay, well, first of all, we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3898" target="_blank">01:04:58.360</a></span> | <span class="t">work out the SDB. So look, we can start up here. We can propagate our upstream errors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3907" target="_blank">01:05:07.760</a></span> | <span class="t">work out local gradients, upstream error, local gradient, and keep all the way down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3913" target="_blank">01:05:13.880</a></span> | <span class="t">and get the DSDB down here. Okay. Next we want to do it for DSDW. Let's just run it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3925" target="_blank">01:05:25.020</a></span> | <span class="t">all over again. Because if we did that, we'd be doing repeated computation, as I showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3931" target="_blank">01:05:31.360</a></span> | <span class="t">in the first half, that this term is the same both times, this term is the same both times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3938" target="_blank">01:05:38.680</a></span> | <span class="t">this term is the same both times, that only the bits at the end differ. So what we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3944" target="_blank">01:05:44.680</a></span> | <span class="t">to do is avoid duplicated computation and compute all the gradients that we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3953" target="_blank">01:05:53.680</a></span> | <span class="t">to need successively so that we only do them once. And so that was analogous to when I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3960" target="_blank">01:06:00.500</a></span> | <span class="t">introduced this delta variable when we computed gradients by hand. So starting off here from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3968" target="_blank">01:06:08.160</a></span> | <span class="t">D, we're starting off here with DSDS is one. We then want to one time compute gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3981" target="_blank">01:06:21.080</a></span> | <span class="t">in the green here. One time compute the gradient in green here. That's all common work. Then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3988" target="_blank">01:06:28.360</a></span> | <span class="t">we're going to take the local gradient for DZDB and multiply that by the upstream gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=3998" target="_blank">01:06:38.480</a></span> | <span class="t">to have worked out DSDB. And then we're going to take the same upstream gradient and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4007" target="_blank">01:06:47.080</a></span> | <span class="t">work out the local gradient here. And then sort of propagate that down to give us DSDW.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4016" target="_blank">01:06:56.000</a></span> | <span class="t">So the end result is we want to sort of systematically work to forward computation forward in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4023" target="_blank">01:07:03.480</a></span> | <span class="t">graph and backward computation back propagation backward in the graph in a way that we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4030" target="_blank">01:07:10.380</a></span> | <span class="t">things efficiently. So this is the general form of the algorithm which works for an arbitrary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4042" target="_blank">01:07:22.680</a></span> | <span class="t">computation graph. So at the end of the day, we've got a single scalar output Z. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4051" target="_blank">01:07:31.720</a></span> | <span class="t">we have inputs and parameters which compute Z. And so once we have this computation graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4061" target="_blank">01:07:41.680</a></span> | <span class="t">and I added in this funky extra arrow here to make it a more general computation graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4068" target="_blank">01:07:48.320</a></span> | <span class="t">well, we can always say that we can work out a starting point, something that doesn't depend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4075" target="_blank">01:07:55.320</a></span> | <span class="t">on anything. So in this case, both of these bottom two nodes don't depend on anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4081" target="_blank">01:08:01.040</a></span> | <span class="t">else. So we can start with them and we can start to compute forward. We can compute values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4087" target="_blank">01:08:07.920</a></span> | <span class="t">for all of these sort of second row from the bottom nodes. And then we're able to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4095" target="_blank">01:08:15.200</a></span> | <span class="t">the third ones up. So we can have a topological sort of the nodes based on the dependencies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4102" target="_blank">01:08:22.840</a></span> | <span class="t">in this directed graph. And we can compute the value of each node given some subset of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4110" target="_blank">01:08:30.240</a></span> | <span class="t">its predecessors, which it depends on. And so doing that is referred to as the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4115" target="_blank">01:08:35.560</a></span> | <span class="t">propagation phase and gives us a computation of the scalar output Z using our current parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4123" target="_blank">01:08:43.880</a></span> | <span class="t">and our current inputs. And so then after that, we run back propagation. So for back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4130" target="_blank">01:08:50.920</a></span> | <span class="t">propagation, we initialize the output gradient, dz dz as one. And then we visit nodes in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4140" target="_blank">01:09:00.760</a></span> | <span class="t">reverse order of the topological sort and we compute the gradients downward. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4148" target="_blank">01:09:08.280</a></span> | <span class="t">our recipe is that for each node as we head down, we're going to compute the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4156" target="_blank">01:09:16.360</a></span> | <span class="t">of the node with respect to its successors, the things that it feeds into and how we compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4165" target="_blank">01:09:25.000</a></span> | <span class="t">that gradient is using this chain rule that we've looked at. So this is sort of the generalized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4171" target="_blank">01:09:31.400</a></span> | <span class="t">form of the chain rule where we have multiple outputs. And so we're summing over the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4177" target="_blank">01:09:37.120</a></span> | <span class="t">outputs and then for each output, we're computing the product of the upstream gradient and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4183" target="_blank">01:09:43.360</a></span> | <span class="t">local gradient with respect to that node. And so we head downwards and we continue down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4191" target="_blank">01:09:51.240</a></span> | <span class="t">the reverse topological sort order and we work out the gradient with respect to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4197" target="_blank">01:09:57.880</a></span> | <span class="t">variable in this graph. And so it hopefully looks kind of intuitive looking at this picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4209" target="_blank">01:10:09.960</a></span> | <span class="t">that if you think of it like this, the big O complexity of forward propagation and backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4218" target="_blank">01:10:18.140</a></span> | <span class="t">propagation is the same, right? In both cases, you're doing a linear pass through all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4224" target="_blank">01:10:24.920</a></span> | <span class="t">these nodes and calculating values given predecessors and then values given successors. I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4233" target="_blank">01:10:33.080</a></span> | <span class="t">you have to do a little bit more work for working out the gradients sort of as shown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4238" target="_blank">01:10:38.960</a></span> | <span class="t">by this chain rule, but it's the same big O complexity. So if somehow you're implementing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4244" target="_blank">01:10:44.320</a></span> | <span class="t">stuff for yourself rather than relying on the software and you're calculating the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4249" target="_blank">01:10:49.720</a></span> | <span class="t">as sort of a different order of complexity of forward propagation, it means that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4254" target="_blank">01:10:54.840</a></span> | <span class="t">doing something wrong. You're doing repeated work that you shouldn't have to do. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4260" target="_blank">01:11:00.120</a></span> | <span class="t">So this algorithm works for a completely arbitrary computation graph, any directed acyclic graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4270" target="_blank">01:11:10.040</a></span> | <span class="t">you can apply this algorithm. In general, what we find is that we build neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4277" target="_blank">01:11:17.120</a></span> | <span class="t">that have a regular layer structure. So we have things like a vector of inputs, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4282" target="_blank">01:11:22.280</a></span> | <span class="t">that's multiplied by a matrix. It's transformed into another vector, which might be multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4288" target="_blank">01:11:28.680</a></span> | <span class="t">by another matrix or summed with another matrix or something, right? So once we're using that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4293" target="_blank">01:11:33.760</a></span> | <span class="t">kind of regular layer structure, we can then parallelize the computation by working out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4300" target="_blank">01:11:40.360</a></span> | <span class="t">the gradients in terms of Jacobians of vectors and matrices and do things in parallel much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4309" target="_blank">01:11:49.360</a></span> | <span class="t">more efficiently. Okay. So doing this is then referred to as automatic differentiation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4318" target="_blank">01:11:58.140</a></span> | <span class="t">And so essentially, if you know the computation graph, you should be able to have your clever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4326" target="_blank">01:12:06.680</a></span> | <span class="t">computer system work out what the derivatives of everything is, and then apply back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4339" target="_blank">01:12:19.000</a></span> | <span class="t">to work out how to update the parameters and learn. And there's actually a sort of an interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4346" target="_blank">01:12:26.840</a></span> | <span class="t">sort of thing of how history has gone backwards here, which I'll just note. So some of you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4355" target="_blank">01:12:35.960</a></span> | <span class="t">might be familiar with symbolic computation packages. So those are things like Mathematica.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4365" target="_blank">01:12:45.160</a></span> | <span class="t">So Mathematica, you can give it a symbolic form of a computation, and then it can work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4372" target="_blank">01:12:52.480</a></span> | <span class="t">out derivatives for you. So it should be the case that if you give a complete symbolic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4378" target="_blank">01:12:58.160</a></span> | <span class="t">form of a computation graph, then it should be able to work out all the derivatives for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4384" target="_blank">01:13:04.520</a></span> | <span class="t">you and you never have to work out a derivative by hand whatsoever. And that was actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4390" target="_blank">01:13:10.680</a></span> | <span class="t">attempted in a famous deep learning library called Fiano, which came out of Yoshua Bengio's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4397" target="_blank">01:13:17.400</a></span> | <span class="t">group at the University of Montreal, that it had a compiler that did that kind of symbolic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4404" target="_blank">01:13:24.240</a></span> | <span class="t">manipulation. But somehow that sort of proved a little bit too hard a road to follow. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4415" target="_blank">01:13:35.640</a></span> | <span class="t">imagine it actually might come back again in the future. And so for modern deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4421" target="_blank">01:13:41.880</a></span> | <span class="t">frameworks, which includes both TensorFlow or PyTorch, they do 90% of this computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4431" target="_blank">01:13:51.680</a></span> | <span class="t">of automatic differentiation for you, but they don't actually symbolically compute derivatives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4439" target="_blank">01:13:59.240</a></span> | <span class="t">So for each particular node or layer of your deep learning system, somebody, either you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4448" target="_blank">01:14:08.400</a></span> | <span class="t">or the person who wrote that layer, has handwritten the local derivatives. But then everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4458" target="_blank">01:14:18.020</a></span> | <span class="t">from that point on, the sort of the taking, doing the chain rule of combining upstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4464" target="_blank">01:14:24.820</a></span> | <span class="t">gradients with local gradients to work out downstream gradients, that's then all being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4470" target="_blank">01:14:30.240</a></span> | <span class="t">done automatically for back propagation on the computation graph. And so what that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4477" target="_blank">01:14:37.680</a></span> | <span class="t">is for a whole neural network, you have a computation graph and it's going to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4483" target="_blank">01:14:43.680</a></span> | <span class="t">forward pass and a backward pass. And so for the forward pass, you're topologically sorting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4492" target="_blank">01:14:52.120</a></span> | <span class="t">the nodes based on their dependencies in the computation graph. And then for each node,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4500" target="_blank">01:15:00.200</a></span> | <span class="t">you're running forward, the forward computation on that node. And then for backward propagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4506" target="_blank">01:15:06.740</a></span> | <span class="t">you're reversing the topological sort of the graph. And then for each node in the graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4512" target="_blank">01:15:12.520</a></span> | <span class="t">you're running the backward propagation, which is the little bit of backdrop, the chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4517" target="_blank">01:15:17.840</a></span> | <span class="t">at that node. And then the result of doing that is you have gradients for your inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4525" target="_blank">01:15:25.620</a></span> | <span class="t">and parameters. And so this is the overall software runs this for you. And so what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4536" target="_blank">01:15:36.120</a></span> | <span class="t">want to do is then actually have stuff for particular nodes or layers in the graph. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4543" target="_blank">01:15:43.880</a></span> | <span class="t">if I have a multiply gate, it's going to have a forward algorithm, which just computes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4550" target="_blank">01:15:50.680</a></span> | <span class="t">the output is X times Y in terms of the two inputs. And then I'm going to want to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4557" target="_blank">01:15:57.680</a></span> | <span class="t">to tell it also how to calculate the local derivative. So I want to say, what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4563" target="_blank">01:16:03.440</a></span> | <span class="t">local derivative? So DLDX and DLDY in terms of the upstream gradient, DLDZ. And so I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4575" target="_blank">01:16:15.400</a></span> | <span class="t">then manually work out how to calculate that. And normally what I have to do is I assume</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4583" target="_blank">01:16:23.240</a></span> | <span class="t">the forward pass is being run first. And I'm going to shove into some local variables for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4590" target="_blank">01:16:30.720</a></span> | <span class="t">my class, the values that were used in the forward computation. So as well as computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4596" target="_blank">01:16:36.720</a></span> | <span class="t">Z equals X times Y, I'm going to sort of remember what X and Y were. So that then when I'm asked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4604" target="_blank">01:16:44.720</a></span> | <span class="t">to compute the backward pass, I'm then going to have implemented here what we saw earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4612" target="_blank">01:16:52.760</a></span> | <span class="t">of that when it's X, Y, you're going to sort of swap the Y and the X to work out the local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4621" target="_blank">01:17:01.240</a></span> | <span class="t">gradients. And so then I'm going to multiply those by the upstream gradient. And I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4626" target="_blank">01:17:06.640</a></span> | <span class="t">to return-- I've just written it here as a sort of a little list, but really it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4631" target="_blank">01:17:11.800</a></span> | <span class="t">to be a NumPy vector of the gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4637" target="_blank">01:17:17.040</a></span> | <span class="t">OK, so that's 98% of what I wanted to cover today. Just a couple of quick comments left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4648" target="_blank">01:17:28.280</a></span> | <span class="t">So that can and should all be automated. Sometimes you want to just check if you're computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4656" target="_blank">01:17:36.120</a></span> | <span class="t">the right gradients. And so the standard way of checking that you're computing the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4661" target="_blank">01:17:41.560</a></span> | <span class="t">gradients is to manually work out the gradient by doing a numeric calculation of the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4669" target="_blank">01:17:49.120</a></span> | <span class="t">And so you can do that. So you can work out what the derivative of X-- of f with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4676" target="_blank">01:17:56.720</a></span> | <span class="t">to X should be by choosing some sort of small number like 10 to the minus 4, adding it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4684" target="_blank">01:18:04.760</a></span> | <span class="t">X, subtracting it from X. And then so the difference between these numbers is 2h, dividing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4690" target="_blank">01:18:10.520</a></span> | <span class="t">it through by 2h. And you're simply working out the rise over the run, which is the slope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4696" target="_blank">01:18:16.120</a></span> | <span class="t">of that point with respect to X. And that's an approximation of the gradient of f with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4702" target="_blank">01:18:22.600</a></span> | <span class="t">respect to X at that value of X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4707" target="_blank">01:18:27.000</a></span> | <span class="t">So this is so simple you can't make a mistake implementing it. And so therefore you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4712" target="_blank">01:18:32.200</a></span> | <span class="t">use this to check whether your gradient values are correct or not. This isn't something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4719" target="_blank">01:18:39.820</a></span> | <span class="t">you'd want to use much because not only is it approximate, but it's extremely slow. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4725" target="_blank">01:18:45.800</a></span> | <span class="t">to work this out, you have to run the forward computation for every parameter of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4732" target="_blank">01:18:52.120</a></span> | <span class="t">So if you have a model with a million parameters, you're now doing a million times as much work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4736" target="_blank">01:18:56.720</a></span> | <span class="t">to run backprop as you would do if you were actually using calculus. So calculus is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4742" target="_blank">01:19:02.760</a></span> | <span class="t">good thing to know. But it can be really useful to check that the right values are being calculated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4750" target="_blank">01:19:10.340</a></span> | <span class="t">In the old days when we hand wrote everything, this was kind of the key unit test that people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4755" target="_blank">01:19:15.580</a></span> | <span class="t">used everywhere. These days, most of the time you're reusing layers that are built into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4761" target="_blank">01:19:21.340</a></span> | <span class="t">PyTorch or some other deep learning framework. So it's much less needed. But sometimes you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4766" target="_blank">01:19:26.740</a></span> | <span class="t">implementing your own layer and you really do want to check that things are implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4770" target="_blank">01:19:30.700</a></span> | <span class="t">correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4771" target="_blank">01:19:31.700</a></span> | <span class="t">There's a fine point in the way this is written. If you saw this in sort of high school calculus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4779" target="_blank">01:19:39.180</a></span> | <span class="t">class, you will have seen rise over run of f of x plus h minus f of x divided by h. It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4790" target="_blank">01:19:50.460</a></span> | <span class="t">turns out that doing this two-sided estimate like this is much, much more accurate than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4796" target="_blank">01:19:56.600</a></span> | <span class="t">doing a one-sided estimate. And so you're really much encouraged to use this approximation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4802" target="_blank">01:20:02.220</a></span> | <span class="t">Okay. So at that point, we've mastered the core technology of neural nets. Backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4810" target="_blank">01:20:10.060</a></span> | <span class="t">is recursively and hence efficiently applying the chain rule along the computation graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4816" target="_blank">01:20:16.620</a></span> | <span class="t">with this sort of key step that downstream gradient equals upstream gradient times local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4824" target="_blank">01:20:24.580</a></span> | <span class="t">gradient. And so for calculating with neural nets, we do the forward pass to work out values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4831" target="_blank">01:20:31.300</a></span> | <span class="t">with current parameters, then run backpropagation to work out the gradient of the loss, currently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4839" target="_blank">01:20:39.860</a></span> | <span class="t">computed loss with respect to those parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4845" target="_blank">01:20:45.180</a></span> | <span class="t">Now to some extent, you know, with modern deep learning frameworks, you don't actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4850" target="_blank">01:20:50.620</a></span> | <span class="t">have to know how to do any of this, right? It's the same as you don't have to know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4855" target="_blank">01:20:55.480</a></span> | <span class="t">to implement a C compiler. You can just write C code and say GCC and it will compile it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4863" target="_blank">01:21:03.740</a></span> | <span class="t">and it will run the right stuff for you. And that's the kind of functionality you get from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4870" target="_blank">01:21:10.220</a></span> | <span class="t">the PyTorch framework. So do come along to the PyTorch tutorial this Friday and get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4876" target="_blank">01:21:16.100</a></span> | <span class="t">sense about how easy it is to write neural networks using a framework like PyTorch or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4882" target="_blank">01:21:22.020</a></span> | <span class="t">TensorFlow. And you know, it's so easy. That's why, you know, high school students across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4887" target="_blank">01:21:27.540</a></span> | <span class="t">the nation are now doing their science projects, training deep learning systems, because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4893" target="_blank">01:21:33.340</a></span> | <span class="t">don't actually have to understand very much, debunk a few neural network layers together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4899" target="_blank">01:21:39.380</a></span> | <span class="t">and set it computing on some data. But, you know, we hope in this class that you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4905" target="_blank">01:21:45.220</a></span> | <span class="t">are also learning how these things are implemented. So you have a deeper understanding than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4913" target="_blank">01:21:53.420</a></span> | <span class="t">And you know, it turns out that sometimes you need to have a deeper understanding. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4917" target="_blank">01:21:57.980</a></span> | <span class="t">back propagation doesn't always work perfectly. And so understanding what it's really doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4923" target="_blank">01:22:03.780</a></span> | <span class="t">can be crucial to debugging things. And so we'll actually see an example of that fairly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4928" target="_blank">01:22:08.940</a></span> | <span class="t">soon when we start looking at recurrent models and some of the problems that they have, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4934" target="_blank">01:22:14.580</a></span> | <span class="t">will require us to think a bit more deeply about what's happening in our gradient computations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4940" target="_blank">01:22:20.460</a></span> | <span class="t">Okay. That's it for today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4943" target="_blank">01:22:23.260</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4944" target="_blank">01:22:24.260</a></span> | <span class="t">[ Applause ]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=X0Jw4kgaFlg&t=4944" target="_blank">01:22:24.260</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>