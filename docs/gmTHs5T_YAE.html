<html><head><title>Optimizing inference for voice models in production - Philip Kiely, Baseten</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Optimizing inference for voice models in production - Philip Kiely, Baseten</h2><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE"><img src="https://i.ytimg.com/vi_webp/gmTHs5T_YAE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=89">1:29</a> Architecture<br><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=197">3:17</a> Performance metrics<br><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=500">8:20</a> Performance<br><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=627">10:27</a> What to avoid<br><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=753">12:33</a> Conclusion<br><br><div style="text-align: left;"><a href="./gmTHs5T_YAE.html">Whisper Transcript</a> | <a href="./transcript_gmTHs5T_YAE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=15" target="_blank">00:00:15.000</a></span> | <span class="t">Hello, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=16" target="_blank">00:00:16.000</a></span> | <span class="t">Thank you so much for being here, for sticking around for this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=19" target="_blank">00:00:19.000</a></span> | <span class="t">I'm going to be talking about optimizing influence for voice models in production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=23" target="_blank">00:00:23.000</a></span> | <span class="t">I'm going to be talking mostly about the runtime component, but also just a little bit on the infrastructure side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=30" target="_blank">00:00:30.000</a></span> | <span class="t">Just a quick introduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=32" target="_blank">00:00:32.000</a></span> | <span class="t">I'm Philip from Base10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=33" target="_blank">00:00:33.000</a></span> | <span class="t">Base10 is a model influence platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=36" target="_blank">00:00:36.000</a></span> | <span class="t">We run production workloads for a wide variety of AI-native startups and enterprises.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=41" target="_blank">00:00:41.000</a></span> | <span class="t">I'm based here in SF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=43" target="_blank">00:00:43.000</a></span> | <span class="t">I actually just moved here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=44" target="_blank">00:00:44.000</a></span> | <span class="t">It's really awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=45" target="_blank">00:00:45.000</a></span> | <span class="t">My favorite part about being in SF is much better sports teams than I had in Chicago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">And one of my favorite voice models is Orpheus TTS, which we're going to be talking about a whole bunch today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=57" target="_blank">00:00:57.000</a></span> | <span class="t">Quick agenda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=60" target="_blank">00:01:00.000</a></span> | <span class="t">So we're going to talk about TTS model architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=62" target="_blank">00:01:02.000</a></span> | <span class="t">Like, what is a text-to-speech model actually when you look on the config in Hugging Face?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=67" target="_blank">00:01:07.000</a></span> | <span class="t">What sort of performance metrics are we looking at?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=70" target="_blank">00:01:10.000</a></span> | <span class="t">What sort of optimization techniques can we do to make the model better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=73" target="_blank">00:01:13.000</a></span> | <span class="t">How do we measure whether or not we succeeded?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=76" target="_blank">00:01:16.000</a></span> | <span class="t">And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=89" target="_blank">00:01:29.000</a></span> | <span class="t">So, architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=91" target="_blank">00:01:31.000</a></span> | <span class="t">This is one of the things I've been learning this year, which has been pretty great to realize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=96" target="_blank">00:01:36.000</a></span> | <span class="t">It's made life a lot simpler at the runtime level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=99" target="_blank">00:01:39.000</a></span> | <span class="t">Now, this is wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=100" target="_blank">00:01:40.000</a></span> | <span class="t">Like, the thing up here that I'm going to say is that, like, everything is an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=106" target="_blank">00:01:46.000</a></span> | <span class="t">That is wrong, but it's useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=108" target="_blank">00:01:48.000</a></span> | <span class="t">There's kind of like two types of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=110" target="_blank">00:01:50.000</a></span> | <span class="t">There's auto-aggressive transformers models that are LLM or very LLM-adjacent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=115" target="_blank">00:01:55.000</a></span> | <span class="t">You see this in embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=116" target="_blank">00:01:56.000</a></span> | <span class="t">You see this in transcription with stuff like whisper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=119" target="_blank">00:01:59.000</a></span> | <span class="t">TTS is another example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=121" target="_blank">00:02:01.000</a></span> | <span class="t">You also have the more like diffuser image type models, which is like a very different optimization problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=127" target="_blank">00:02:07.000</a></span> | <span class="t">But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=142" target="_blank">00:02:22.000</a></span> | <span class="t">So, the TTS model that we're going to be using and as an example all day is Orpheus TTS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=147" target="_blank">00:02:27.000</a></span> | <span class="t">We're using it for two reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=149" target="_blank">00:02:29.000</a></span> | <span class="t">Okay, three reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=150" target="_blank">00:02:30.000</a></span> | <span class="t">The two reasons are because it's open source and it's really good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=153" target="_blank">00:02:33.000</a></span> | <span class="t">And also, I think Elias and Amu and everyone at Canopy Labs is really awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=157" target="_blank">00:02:37.000</a></span> | <span class="t">So, that's the third reason we're talking about their model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=161" target="_blank">00:02:41.000</a></span> | <span class="t">But it's a Llama 3.2 3B backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=165" target="_blank">00:02:45.000</a></span> | <span class="t">So, like, if you look at this is the literal like config from hugging face copy and pasted onto the screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=171" target="_blank">00:02:51.000</a></span> | <span class="t">It's a Llama for causal LM architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=174" target="_blank">00:02:54.000</a></span> | <span class="t">And so, because of that we can do like all of our normal Llama stuff to this model and make it faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=179" target="_blank">00:02:59.000</a></span> | <span class="t">They did a couple things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">I mean, they did a bunch of things to make it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=183" target="_blank">00:03:03.000</a></span> | <span class="t">But a couple things that are relevant here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=185" target="_blank">00:03:05.000</a></span> | <span class="t">There is a larger vocab size because you need all the speech specific tokens like laugh and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=190" target="_blank">00:03:10.000</a></span> | <span class="t">And then they also extended the context links with rope scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=194" target="_blank">00:03:14.000</a></span> | <span class="t">So, we've got to make sure everything we do supports that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=197" target="_blank">00:03:17.000</a></span> | <span class="t">So, performance metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=199" target="_blank">00:03:19.000</a></span> | <span class="t">Like, what do we want to actually do here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=201" target="_blank">00:03:21.000</a></span> | <span class="t">We think about LLM metrics a little bit here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=205" target="_blank">00:03:25.000</a></span> | <span class="t">We just look at them a little bit differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=208" target="_blank">00:03:28.000</a></span> | <span class="t">So, in LLMs you talk about time to first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=211" target="_blank">00:03:31.000</a></span> | <span class="t">Now we're talking about time to first byte or sometimes even time to first sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=215" target="_blank">00:03:35.000</a></span> | <span class="t">We need a little bit more of a useful output from the model before we really start feeling good about our response time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=221" target="_blank">00:03:41.000</a></span> | <span class="t">We do think about tokens per second, although we're going to think about it differently, which I'll explain later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=226" target="_blank">00:03:46.000</a></span> | <span class="t">And we mostly think about throughput, which is, you know, how many requests are we able to serve at a given time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">So, on that, you know, goals perspective, if you ask me, like, hey, Philip, how do you want to optimize Llama in general?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=239" target="_blank">00:03:59.000</a></span> | <span class="t">I'll say, well, we want a lot of TPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=241" target="_blank">00:04:01.000</a></span> | <span class="t">We want 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=242" target="_blank">00:04:02.000</a></span> | <span class="t">We want 500 TPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=243" target="_blank">00:04:03.000</a></span> | <span class="t">We want 1,000 tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=245" target="_blank">00:04:05.000</a></span> | <span class="t">We want as many tokens per second as we can get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=247" target="_blank">00:04:07.000</a></span> | <span class="t">With voice models, you actually don't necessarily need that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=250" target="_blank">00:04:10.000</a></span> | <span class="t">In many cases, you only want as many tokens per second as you need for a real-time stream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=256" target="_blank">00:04:16.000</a></span> | <span class="t">For Orpheus, that's like 83 tokens per second, which for like a 3 billion per LLM is nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=262" target="_blank">00:04:22.000</a></span> | <span class="t">But what we actually want to do instead is we want to, once we hit that mark, start optimizing for time to first byte so that our latency is really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=272" target="_blank">00:04:32.000</a></span> | <span class="t">and start optimizing for concurrency so that we can get more connections and spend less on GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=279" target="_blank">00:04:39.000</a></span> | <span class="t">So, our goal in general, if all of these very nice and definitely not AI-generated people, all the different, like, voices that our model is capable of creating,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=288" target="_blank">00:04:48.000</a></span> | <span class="t">these are all the voice agents that we're running, how can we make all of these people fit on one or even less than one GPU?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=295" target="_blank">00:04:55.000</a></span> | <span class="t">That's the goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=297" target="_blank">00:04:57.000</a></span> | <span class="t">So, how do we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=298" target="_blank">00:04:58.000</a></span> | <span class="t">So, how do we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=300" target="_blank">00:05:00.000</a></span> | <span class="t">Bunch of ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">So, first off, it's an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=303" target="_blank">00:05:03.000</a></span> | <span class="t">If you are running an LLM with like VLLM, for example, you can generally, in many cases, get better performance with TensorRT LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=312" target="_blank">00:05:12.000</a></span> | <span class="t">TensorRT is something that we've been using at Base10 a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=315" target="_blank">00:05:15.000</a></span> | <span class="t">I like to joke that I'm the unofficial marketing department for TensorRT LLM because of how much I talk about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=321" target="_blank">00:05:21.000</a></span> | <span class="t">But it really is fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=322" target="_blank">00:05:22.000</a></span> | <span class="t">It can be a little bit complicated from a developer experience perspective to get up and running with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=327" target="_blank">00:05:27.000</a></span> | <span class="t">But once you are up and running, it works really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=330" target="_blank">00:05:30.000</a></span> | <span class="t">We can also just like quantize the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=333" target="_blank">00:05:33.000</a></span> | <span class="t">Even though it's small, you can always make it faster by making it smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">With Hopper architecture, we quantize this model to FP8 pretty successfully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=341" target="_blank">00:05:41.000</a></span> | <span class="t">I know usually quantizing really small models like this can lead to performance degradation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=346" target="_blank">00:05:46.000</a></span> | <span class="t">But for this model, it's working pretty well in FP8, even when we quantize the KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=351" target="_blank">00:05:51.000</a></span> | <span class="t">And then a lot of the other runtime stuff is actually more like audio specific than it is LLM specific.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=357" target="_blank">00:05:57.000</a></span> | <span class="t">So one of the big challenges that we don't have with LLMs, which are just parsing nice convenient bits of text back and forth,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=363" target="_blank">00:06:03.000</a></span> | <span class="t">is you have your audio, you have your audio codec, you have your decoding, all that kind of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=367" target="_blank">00:06:07.000</a></span> | <span class="t">So we use snack, which I was very disappointed to learn is not an actual tasty snack, but an audio decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=374" target="_blank">00:06:14.000</a></span> | <span class="t">And we actually use torch compile.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=376" target="_blank">00:06:16.000</a></span> | <span class="t">And torch compile, you might be used to running on, you know, a model, compiling your model weights to make your runtime faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=385" target="_blank">00:06:25.000</a></span> | <span class="t">We're actually using the same kind of system with torch compile and with PyTorch inference mode on the audio decoder and running that on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=395" target="_blank">00:06:35.000</a></span> | <span class="t">We make sure that all the token batching, token level batching works well throughout the entire pipeline and support multiple streaming protocols.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=403" target="_blank">00:06:43.000</a></span> | <span class="t">Yeah, so these are the engine settings that you would need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=407" target="_blank">00:06:47.000</a></span> | <span class="t">You've got the, you know, quantization type of FP8KV, the FP8Context FMHA in order to, you know, support the, support the hopper architecture and the quantization there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=423" target="_blank">00:07:03.000</a></span> | <span class="t">And here's a quick code sample of, I got a little ahead of my slides, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=429" target="_blank">00:07:09.000</a></span> | <span class="t">Here's a little quick code sample of the audio decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">So, we are basically, you know, batching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=436" target="_blank">00:07:16.000</a></span> | <span class="t">Usually, we would talk about continuous batching when we're talking about LLM optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=442" target="_blank">00:07:22.000</a></span> | <span class="t">We want to package all those tokens together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=444" target="_blank">00:07:24.000</a></span> | <span class="t">In this case, we are doing dynamic batching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=446" target="_blank">00:07:26.000</a></span> | <span class="t">So, we're trying to pack as much into a batch as we can, but every 15 milliseconds, we're going to shoot it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">You've got that timeout set up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=455" target="_blank">00:07:35.000</a></span> | <span class="t">If you want to trade off for a little bit of latency for more throughput, you can make that batch bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=461" target="_blank">00:07:41.000</a></span> | <span class="t">So, yeah, we don't have token level continuous batching yet here, but we do have dynamic batching, which is going to get you pretty close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=467" target="_blank">00:07:47.000</a></span> | <span class="t">And because of this, actually, something that I was surprised about when we profiled this is that our TTS implementation with Orpheus is actually, in many cases, CPU bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=479" target="_blank">00:07:59.000</a></span> | <span class="t">Which is kind of where you want to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=481" target="_blank">00:08:01.000</a></span> | <span class="t">You can throw more CPUs at a resource pretty efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=485" target="_blank">00:08:05.000</a></span> | <span class="t">Even though the next token prediction and the audio decoding are both on the GPU, both of those loops hit the CPU at different points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=493" target="_blank">00:08:13.000</a></span> | <span class="t">And that can actually be the bottleneck in the number of simultaneous streams that we're able to create.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=498" target="_blank">00:08:18.000</a></span> | <span class="t">So, how'd we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=501" target="_blank">00:08:21.000</a></span> | <span class="t">Like, I just showed you a lot of code and talked through it really quickly without really getting into depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=506" target="_blank">00:08:26.000</a></span> | <span class="t">That could all just be smoke and mirrors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=508" target="_blank">00:08:28.000</a></span> | <span class="t">Let's see if it's actually any faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=511" target="_blank">00:08:31.000</a></span> | <span class="t">So, again, the number one thing is going to be simultaneous streams because you want to be able to be very cost efficient and use fewer GPU resources to serve a, you know, larger stream,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=523" target="_blank">00:08:43.000</a></span> | <span class="t">large amount of traffic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=524" target="_blank">00:08:44.000</a></span> | <span class="t">And in this case, a base implementation, I don't necessarily want to, like, call anyone out because there's a lot of really good ways to run this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=531" target="_blank">00:08:51.000</a></span> | <span class="t">You can get really good performance with VLLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=534" target="_blank">00:08:54.000</a></span> | <span class="t">But this is just kind of like the off the shelf.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=537" target="_blank">00:08:57.000</a></span> | <span class="t">Just take it, run it completely standard implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=541" target="_blank">00:09:01.000</a></span> | <span class="t">So, with variable traffic, we're able to support 16 simultaneous streams and with constant traffic, 24 simultaneous streams on an H100 MIG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=551" target="_blank">00:09:11.000</a></span> | <span class="t">So, this is actually half an H100 GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=553" target="_blank">00:09:13.000</a></span> | <span class="t">It's a skew that we do a lot because it's really good for these small models where you want the hopper performance, the hopper architecture, uplift, and tensor RT LLM, the FP8 support.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=563" target="_blank">00:09:23.000</a></span> | <span class="t">But you don't want to pay for, like, an entire 80 gigabyte GPU for just a 3 billion parameter model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=569" target="_blank">00:09:29.000</a></span> | <span class="t">So, you know, we're seeing much better concurrency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=574" target="_blank">00:09:34.000</a></span> | <span class="t">So, if you kind of, like, price that out with, like, our list prices and stuff, you can get, you know, a few cents per hour of conversation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=582" target="_blank">00:09:42.000</a></span> | <span class="t">which is going to be, you know, substantially better than if you're -- if you have the volume for it, it's going to be substantially better than paying for a sort of, like, per token type API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=592" target="_blank">00:09:52.000</a></span> | <span class="t">But, okay, sure, maybe it's cheap at scale, but is it fast?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=597" target="_blank">00:09:57.000</a></span> | <span class="t">Yes, it's fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=598" target="_blank">00:09:58.000</a></span> | <span class="t">So, with the, you know, with the TRT implementation on the MIGs and on the H100s, we can actually get all the way down to 150 millisecond time to first bytes in, like, real-world testing that we've done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=611" target="_blank">00:10:11.000</a></span> | <span class="t">Now, that -- we'll talk in a minute, like, that doesn't mean your whole pipeline is that fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=616" target="_blank">00:10:16.000</a></span> | <span class="t">That's just, like, one part of the pipeline, but it's important because, you know, you definitely don't want to be spending a lot of time waiting around for that first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=627" target="_blank">00:10:27.000</a></span> | <span class="t">So, to kind of transition into that discussion of, like, what can go wrong here, like, you have this graph and you have this, you know, nice config that I had up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=639" target="_blank">00:10:39.000</a></span> | <span class="t">And you're, like, all right, cool, I'm going to take this, I'm going to put it in production, and I'm going to see the results that he put up on screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=646" target="_blank">00:10:46.000</a></span> | <span class="t">And it's going to work great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=647" target="_blank">00:10:47.000</a></span> | <span class="t">And the answer is no, it's not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=649" target="_blank">00:10:49.000</a></span> | <span class="t">It's a little bit harder than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=650" target="_blank">00:10:50.000</a></span> | <span class="t">So, the thing is, like, non-run-time factors when we get -- especially with these small models and with these multimodal systems --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=659" target="_blank">00:10:59.000</a></span> | <span class="t">can actually be, like, way more important than your runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=663" target="_blank">00:11:03.000</a></span> | <span class="t">And that's your infrastructure and your client code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=666" target="_blank">00:11:06.000</a></span> | <span class="t">Because, you know, I showed here -- all right, maybe I got it, you know, I cut the runtime in half from the base implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=676" target="_blank">00:11:16.000</a></span> | <span class="t">I saved a couple hundred milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=678" target="_blank">00:11:18.000</a></span> | <span class="t">Very easy to add those couple hundred milliseconds back and well beyond that by, you know, sending my query to New York instead of California.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=686" target="_blank">00:11:26.000</a></span> | <span class="t">Or by having to establish a session every time I, you know, run my client code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=692" target="_blank">00:11:32.000</a></span> | <span class="t">So, a few, like, pitfalls to avoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=696" target="_blank">00:11:36.000</a></span> | <span class="t">Number one, like, if you go in, you know, our model library or something, and we're just trying to get you started very quickly with this kind of inference sample,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=706" target="_blank">00:11:46.000</a></span> | <span class="t">it's basically going to be, hey, use requests, make a stream, stream it to your local computer, and start, you know, playing it on FFMPG or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=715" target="_blank">00:11:55.000</a></span> | <span class="t">The issue is that here, like, the requests are going to be sent sequentially, and you need to create a new session every time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=721" target="_blank">00:12:01.000</a></span> | <span class="t">That takes time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=722" target="_blank">00:12:02.000</a></span> | <span class="t">So, if you're using this in production, you want a code sample -- by the way, this is all up on our GitHub --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=729" target="_blank">00:12:09.000</a></span> | <span class="t">you'll want a code sample that looks a lot more like a benchmarking script, where you're using a multiprocess pool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=737" target="_blank">00:12:17.000</a></span> | <span class="t">You're sharing the session between all of these different requests, and you're actually sending traffic with the concurrency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=745" target="_blank">00:12:25.000</a></span> | <span class="t">that allows you to, you know, saturate this benchmark with, you know, the multiple concurrent requests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=751" target="_blank">00:12:31.000</a></span> | <span class="t">Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=760" target="_blank">00:12:40.000</a></span> | <span class="t">In many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or PipeCat or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=765" target="_blank">00:12:45.000</a></span> | <span class="t">and you're also potentially going to be using a different protocol.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=768" target="_blank">00:12:48.000</a></span> | <span class="t">You're going to be using something like WebSockets or GRPC, which we do have support for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=773" target="_blank">00:12:53.000</a></span> | <span class="t">And finally, I wanted to leave you on the thought that these, you know, these models are only one part of a voice agent pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=782" target="_blank">00:13:02.000</a></span> | <span class="t">So, like, we can spend a lot more than 15 minutes actually talking about, like, the very detailed implementation mechanics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=790" target="_blank">00:13:10.000</a></span> | <span class="t">of making your voice model faster, of, you know, we haven't even touched on stuff like fine-tuning the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=796" target="_blank">00:13:16.000</a></span> | <span class="t">you know, custom voices, zero-shot voice cloning, being able to, you know, remove static and popping at the end of messages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=804" target="_blank">00:13:24.000</a></span> | <span class="t">There's a lot of work to do just on the voice part, but it really only is one-third of the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=810" target="_blank">00:13:30.000</a></span> | <span class="t">When I think about voice agents, I think about three parts: listening, thinking, talking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=814" target="_blank">00:13:34.000</a></span> | <span class="t">And the most important thing here is, again, while you can have great runtimes, the infrastructure to connect these three together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=822" target="_blank">00:13:42.000</a></span> | <span class="t">is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=831" target="_blank">00:13:51.000</a></span> | <span class="t">with, you know, minimal, like, minimal network overhead in between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=837" target="_blank">00:13:57.000</a></span> | <span class="t">Even things as simple as not having to go off and do a hairpin at the DNS level and come back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=843" target="_blank">00:14:03.000</a></span> | <span class="t">If that saves you 10 milliseconds on every step and your voice pipeline has this and, you know, a chunking algorithm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=849" target="_blank">00:14:09.000</a></span> | <span class="t">it's got an interruption model, and so you end up having four or five steps, well, they're just hairpinning alone is costing you 40 or 50 milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=858" target="_blank">00:14:18.000</a></span> | <span class="t">And that can be 10% of your SLA for a voice model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=862" target="_blank">00:14:22.000</a></span> | <span class="t">So, yeah, that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=870" target="_blank">00:14:30.000</a></span> | <span class="t">the infrastructure and the client implementation is equally important, if not more so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=876" target="_blank">00:14:36.000</a></span> | <span class="t">Anyway, thank -- so, yeah, that's the review.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=879" target="_blank">00:14:39.000</a></span> | <span class="t">Thank you all for coming through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=881" target="_blank">00:14:41.000</a></span> | <span class="t">I have -- we're doing an event next week at Fogo de Chao, which is going to be pretty fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=886" target="_blank">00:14:46.000</a></span> | <span class="t">I'm going to be talking in more detail about building some systems with open source models, and there's also going to be a lot of steak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=893" target="_blank">00:14:53.000</a></span> | <span class="t">So, definitely come on through if you're interested, and I'm on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=898" target="_blank">00:14:58.000</a></span> | <span class="t">I'm on LinkedIn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=899" target="_blank">00:14:59.000</a></span> | <span class="t">So, it's base 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=900" target="_blank">00:15:00.000</a></span> | <span class="t">Hit me up if you have any questions about this or anything else model performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=903" target="_blank">00:15:03.000</a></span> | <span class="t">Thank you so much, and I'll let you go eight seconds early.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=906" target="_blank">00:15:06.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=907" target="_blank">00:15:07.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=907" target="_blank">00:15:07.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=908" target="_blank">00:15:08.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=gmTHs5T_YAE&t=909" target="_blank">00:15:09.000</a></span> | <span class="t">I'll see you next time.</span></div></div></body></html>