<html><head><title>‘Speaking Dolphin’ to AI Data Dominance, 4.1 + Kling 2.0: 7 Updates Critically Analysed</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>‘Speaking Dolphin’ to AI Data Dominance, 4.1 + Kling 2.0: 7 Updates Critically Analysed</h2><a href="https://www.youtube.com/watch?v=yPxavsb2rgk"><img src="https://i.ytimg.com/vi/yPxavsb2rgk/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=30">0:30</a> Kling 2.0<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=95">1:35</a> GPT 4.1<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=325">5:25</a> o3 Build-up<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=457">7:37</a> ‘Product Company’<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=571">9:31</a> Safe Superintelligence<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=654">10:54</a> DolphinGemma<br><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=796">13:16</a> Data Dominance?<br><br><div style="text-align: left;"><a href="./yPxavsb2rgk.html">Whisper Transcript</a> | <a href="./transcript_yPxavsb2rgk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Progress in AI can feel incremental until you step back and think in terms of weeks and months</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=6" target="_blank">00:00:06.140</a></span> | <span class="t">rather than just days. So this video won't just be about the release then of GPT 4.1 in the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=13" target="_blank">00:00:13.680</a></span> | <span class="t">48 hours, or Cling 2.0, or a sneak peek at the next OpenAI model O3, or even about Dolphin Gemma,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=21" target="_blank">00:00:21.020</a></span> | <span class="t">the hopeful new language model from Google. It will be about all of this in the wider context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=27" target="_blank">00:00:27.140</a></span> | <span class="t">and how seven such stories contextualize where we are in AI and what's happening. I want to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=33" target="_blank">00:00:33.160</a></span> | <span class="t">with something super practical just for those who don't care much about the incremental advance in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=37" target="_blank">00:00:37.900</a></span> | <span class="t">intelligence and just want cool tools to use. For those people in the last few days, Cling have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=43" target="_blank">00:00:43.440</a></span> | <span class="t">released Cling 2.0. And here is a workflow that I recommend. Generate an image with ChatGPT because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=50" target="_blank">00:00:50.360</a></span> | <span class="t">it has incredible text fidelity. Not perfect, but really good. Now you shouldn't really explain a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=55" target="_blank">00:00:55.920</a></span> | <span class="t">joke, but as you can tell, the background image is somewhat taking the piss out of OpenAI's model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=60" target="_blank">00:01:00.840</a></span> | <span class="t">names. Pro tip, if you have any curse words, ChatGPT will generate that, but Cling won't work with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=66" target="_blank">00:01:06.740</a></span> | <span class="t">within the image. So I had to use the version without GPT WTF. Anyway, the only point I wanted to make is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=72" target="_blank">00:01:12.940</a></span> | <span class="t">that Cling 2.0 for me is the state of the art at generating smooth, realistic scenes. Of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=78" target="_blank">00:01:18.780</a></span> | <span class="t">not perfect with regard to physics. And yes, I have compared it directly to VO2 and also to Sora video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=86" target="_blank">00:01:26.420</a></span> | <span class="t">generation. And no, I'm not going to belabor the point because it's not perfect, but sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=90" target="_blank">00:01:30.760</a></span> | <span class="t">incremental progress when you step back can add up to something quite significant. Speaking of which,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=96" target="_blank">00:01:36.160</a></span> | <span class="t">of course, in the last 48 hours, we got GPT 4.1 from OpenAI. That's their first model that can process up to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=103" target="_blank">00:01:43.300</a></span> | <span class="t">million tokens, or think of that as around 750,000 words. Aside from being a bit less verbose than the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=110" target="_blank">00:01:50.300</a></span> | <span class="t">notoriously talkative Claude 3.7 Sonnet or the famously garrulous Gemini 2.5 Pro, I don't actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=117" target="_blank">00:01:57.620</a></span> | <span class="t">think GPT 4.1 is that much of a step forward. So I'm not going to spend a huge amount of time on it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=123" target="_blank">00:02:03.140</a></span> | <span class="t">but just a few words on its background. GPT 4.1, like GPT 4.5, is not a reasoning model. It doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=129" target="_blank">00:02:09.220</a></span> | <span class="t">output those long chains of thought before giving you an answer. That then begs the question of why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=134" target="_blank">00:02:14.360</a></span> | <span class="t">release 4.1 when we already have GPT 4.0 and GPT 4.5, which are also non-reasoning models. It seems like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=141" target="_blank">00:02:21.940</a></span> | <span class="t">demand for GPT 4.5 wasn't as great as OpenAI hoped, and that could be because of how expensive GPT 4.5 was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=149" target="_blank">00:02:29.100</a></span> | <span class="t">or how great Gemini 2.5 Pro is. So OpenAI wanted to release a non-reasoning model that answers more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=156" target="_blank">00:02:36.140</a></span> | <span class="t">quickly that was better than GPT 4.0, but not as expensive as GPT 4.5. And for those of you at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=163" target="_blank">00:02:43.140</a></span> | <span class="t">back with your hands up, why do we even need non-reasoning models when the reasoning models do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=167" target="_blank">00:02:47.240</a></span> | <span class="t">so well? If you can improve these, quote, base models on software engineering, then when reasoning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=173" target="_blank">00:02:53.360</a></span> | <span class="t">applied to those better base models, you will get a better end result. But there is a slight marketing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=178" target="_blank">00:02:58.360</a></span> | <span class="t">problem for OpenAI. If Google can somehow serve reasoning models that perform better for a lower price</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=185" target="_blank">00:03:05.320</a></span> | <span class="t">than OpenAI can serve non-reasoning models. Take Ada's Polyglot Coding Benchmark, which is a popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=190" target="_blank">00:03:10.900</a></span> | <span class="t">and well-regarded benchmark. You don't have to remember any of these numbers, just focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=194" target="_blank">00:03:14.640</a></span> | <span class="t">relative performance, with GPT 4.1 getting 52% at a cost of around $10. If you bear in mind those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=200" target="_blank">00:03:20.460</a></span> | <span class="t">numbers, we can then scroll up and see Gemini 2.5 Pro getting 73% correct at a cost of $6. On my own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=208" target="_blank">00:03:28.720</a></span> | <span class="t">benchmark, SimpleBench, you can see a clustering effect now for those base models, those non-reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=214" target="_blank">00:03:34.700</a></span> | <span class="t">models. GPT 4.1 gets 27%, which is very similar to Llama 4 Maverick, Claude 3.5 Sonnet and the new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=221" target="_blank">00:03:41.760</a></span> | <span class="t">DeepSeek V3. Worth noting though that we could finally benchmark Grok 3 because the API was released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=227" target="_blank">00:03:47.780</a></span> | <span class="t">and that scored 36.1% comparing directly to the original GPT 4.5 at around 34%. Now, I know you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=235" target="_blank">00:03:55.880</a></span> | <span class="t">guys are noticing Gemini 2.5 Pro way out in the lead, but more on that in a moment. What about that 1 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=241" target="_blank">00:04:01.480</a></span> | <span class="t">token context window? Doesn't that really stand out? Well, yes, except Gemini 2.5 Pro also has a 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=247" target="_blank">00:04:07.500</a></span> | <span class="t">million token context window. And when you sprinkle in a bunch of clues across a long fiction story,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=253" target="_blank">00:04:13.620</a></span> | <span class="t">as for this amazing benchmark, you can see which models actually pick up on those clues and utilize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=259" target="_blank">00:04:19.480</a></span> | <span class="t">that long context the best, piecing together plot narratives across many diverse chapters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=264" target="_blank">00:04:24.540</a></span> | <span class="t">Well, as you can possibly see if you zoom in, Gemini 2.5 Pro can do this even across novel length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=271" target="_blank">00:04:31.080</a></span> | <span class="t">100,000 word length pieces of fiction. On this benchmark, GPT 4.1 falls far behind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=276" target="_blank">00:04:36.860</a></span> | <span class="t">as does pretty much every other model than Gemini 2.5 Pro. So when you see OpenAI tout needle in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=282" target="_blank">00:04:42.780</a></span> | <span class="t">haystack charts like this one, remember they're selectively picking the benchmarks that make their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=287" target="_blank">00:04:47.640</a></span> | <span class="t">models look the best. Llama 4, you may remember, did a very similar thing. And of course, don't get any of us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=293" target="_blank">00:04:53.920</a></span> | <span class="t">on LM arena, which can be gamed heavily and was by meta. Now you might say that's a little harsh given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=299" target="_blank">00:04:59.900</a></span> | <span class="t">that OpenAI just open sourced a brand new benchmark, an eval on long context called OpenAI MRCR. The only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=306" target="_blank">00:05:06.860</a></span> | <span class="t">problem is that we've already had a benchmark like that for over a year from Google. And with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=312" target="_blank">00:05:12.280</a></span> | <span class="t">benchmark, we could compare across model families. It turns out that if you're in the lead, you're much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=316" target="_blank">00:05:16.920</a></span> | <span class="t">more inclined to compare your model to other model families. Now, of course, I am aware that by tonight, or at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=323" target="_blank">00:05:23.300</a></span> | <span class="t">within the next week, we are likely getting O3 from OpenAI. According to the information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=329" target="_blank">00:05:29.020</a></span> | <span class="t">that's the model that can really help with science. It can connect the dots between concepts from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=334" target="_blank">00:05:34.660</a></span> | <span class="t">different fields to suggest new types of experiments involving anything from nuclear fusion to pathogen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=339" target="_blank">00:05:39.880</a></span> | <span class="t">detection. This is according to people who have tested the model. And apparently, we're not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=344" target="_blank">00:05:44.180</a></span> | <span class="t">going to get O3, but O4 mini. Now, as the title promised, I do have to critically analyze this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=349" target="_blank">00:05:49.920</a></span> | <span class="t">announcement, though, even before it actually happens. First, obviously, it would have to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=354" target="_blank">00:05:54.180</a></span> | <span class="t">extremely good to justify a $20,000 per month price, as the information reports. Second, models can perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=361" target="_blank">00:06:01.360</a></span> | <span class="t">well in benchmarks, but not actually understand the real world or perform effectively when conducting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=367" target="_blank">00:06:07.220</a></span> | <span class="t">science. And yes, that includes Gemini 2.5, as one researcher recently found. Any of you following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=372" target="_blank">00:06:12.780</a></span> | <span class="t">simple bench or doing your own tests probably saw this coming, but look what happened when he created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=378" target="_blank">00:06:18.020</a></span> | <span class="t">a benchmark on manufacturing this simple brass part. All models except Gemini 2.5 failed at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=385" target="_blank">00:06:25.020</a></span> | <span class="t">first hurdle just because they had horrible visual abilities. But even Gemini 2.5 had terrible physical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=391" target="_blank">00:06:31.640</a></span> | <span class="t">reasoning. Its machining plans had multiple critical errors that a beginner machinist would spot. It could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=398" target="_blank">00:06:38.440</a></span> | <span class="t">parrot textbook terms but lacked practical understanding. And that's Gemini 2.5, remember that AI co-scientist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=406" target="_blank">00:06:46.080</a></span> | <span class="t">powered by Gemini 2 that Google touted in February. I'm not saying that Gemini 2.5 or O3 can't suggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=413" target="_blank">00:06:53.260</a></span> | <span class="t">interesting new research directions. It's just, they don't have this mystical understanding of science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=418" target="_blank">00:06:58.260</a></span> | <span class="t">that no other human has. Not yet at least. And I say not yet for two reasons. One I'll give at the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=423" target="_blank">00:07:03.500</a></span> | <span class="t">the video and one here. Models are incrementally getting better even at physical reasoning, spatial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=430" target="_blank">00:07:10.100</a></span> | <span class="t">reasoning type of questions. I haven't personally tested O3, but I have analysed a bunch of its answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=436" target="_blank">00:07:16.500</a></span> | <span class="t">through someone I know. No, they don't work at OpenAI. The model does still make basic errors, but it's the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=443" target="_blank">00:07:23.780</a></span> | <span class="t">one to get certain questions right that I have never seen any other model get right even once. That's pretty much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=449" target="_blank">00:07:29.780</a></span> | <span class="t">all I can say at this point without a risk of getting sued, but it does back up my incremental improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=455" target="_blank">00:07:35.220</a></span> | <span class="t">point. Of course, O3, especially on any high setting, is likely to be slower and more expensive than Gemini 2.5 Pro, but that might not matter as much as you think. Especially if, in the words of Satya Nadella,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=466" target="_blank">00:07:46.140</a></span> | <span class="t">Satya Nadella, and now Sam Altman from this week, that OpenAI is moving from being a model company to being a product company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=473" target="_blank">00:07:53.420</a></span> | <span class="t">ChatGPT is like a standard user. The model capability is very smart, but we have to build a great product, not just a great model. And so there will be a lot of people with great models, and we will try to build the best product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=485" target="_blank">00:08:05.740</a></span> | <span class="t">The focus on this channel is much more on the state of the art in model intelligence rather than in product features, but I have noticed a trend. More and more product features are now being copied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=495" target="_blank">00:08:15.780</a></span> | <span class="t">shared across all the different model providers. Anthropic with the Claude series now has web search and is soon going to have a voice assistant just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=503" target="_blank">00:08:23.060</a></span> | <span class="t">like OpenAI. And now Anthropic have joined the deep research party with their own research mode. That comes after Gemini updating their own deep research tool with Gemini 2.5 Pro, meaning that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=515" target="_blank">00:08:35.060</a></span> | <span class="t">that, and it's hardly surprising, the Gemini tool is now arguably the best one. I recently switched over from defaulting to OpenAI's deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=522" target="_blank">00:08:42.340</a></span> | <span class="t">research to Gemini's one simply because it's faster and, on average, slightly better. I'm not as keen on how, even for a simple query, it outputs this massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=532" target="_blank">00:08:52.340</a></span> | <span class="t">volume of text, but still its accuracy is slightly higher for me. Of course, that means it's getting slightly harder to justify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=539" target="_blank">00:08:59.620</a></span> | <span class="t">paying for the $200 pro tier for OpenAI, but let's see what they release in the next week. Speaking of deep research, if you want to see how all LLMs lie when they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=549" target="_blank">00:09:09.620</a></span> | <span class="t">trying to justify their reasons for an answer, do check out the latest video on my Patreon. One somewhat amusing test I gave was to see which deep research would make up a report on an African author that I entirely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=561" target="_blank">00:09:21.980</a></span> | <span class="t">made up from my imagination. One deep research tool does spectacularly well, the other not so much. If you are a little overwhelmed with all the product offerings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=570" target="_blank">00:09:30.860</a></span> | <span class="t">Safe Super Intelligence from Ilya Sutskova has got your back. They offer precisely zero products. That has not stopped them being now valued at $32 billion, apparently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=582" target="_blank">00:09:42.820</a></span> | <span class="t">This is not a made up figure, people are giving them billions of dollars, this time $2 billion, at that valuation. That's what they value the company at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=591" target="_blank">00:09:51.620</a></span> | <span class="t">I have precisely zero extra details to give you. It's just the obvious question, what on earth are they up to?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=598" target="_blank">00:09:58.500</a></span> | <span class="t">One product that is here right now is the sponsors of today's video, Emergent Mind, where you can see which AI papers have caught fire online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=608" target="_blank">00:10:08.180</a></span> | <span class="t">Now while I might have the time to go off and then read those papers, what you can do if you wish is use Gemini 2.5 Pro to summarize those papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=617" target="_blank">00:10:17.060</a></span> | <span class="t">Or say you're just interested in a topic like reward hacking, you can search for it and get a summary of all the relevant papers from Gemini 2.5 Pro.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=625" target="_blank">00:10:25.060</a></span> | <span class="t">And I actually know the creator of Emergent Mind. And I asked for this feature where you could directly then click on the PDF and boom, here it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=631" target="_blank">00:10:31.940</a></span> | <span class="t">As I've said before, I also love the social section at the bottom where you can see the social media reaction to a certain paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=639" target="_blank">00:10:39.940</a></span> | <span class="t">And I'm on the pro tier, which is free, by the way, for students who are currently enrolled in college or university.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=646" target="_blank">00:10:46.820</a></span> | <span class="t">The links as ever will be in the description for emergentmind.com.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=650" target="_blank">00:10:50.820</a></span> | <span class="t">Now as with many of you, my attention was caught by Dolphin Gemma from Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=655" target="_blank">00:10:55.860</a></span> | <span class="t">How Google AI is "helping decode dolphin communication".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=660" target="_blank">00:11:00.340</a></span> | <span class="t">That is a grand title and it got millions of views on social media.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=664" target="_blank">00:11:04.100</a></span> | <span class="t">But when you dig in, I don't know, there's just a bit less than meets the eye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=668" target="_blank">00:11:08.660</a></span> | <span class="t">Don't get me wrong, I think it's incredible that we are trying to do this and I am so enthusiastic about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=674" target="_blank">00:11:14.580</a></span> | <span class="t">I absolutely love animals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=676" target="_blank">00:11:16.260</a></span> | <span class="t">It's just that if you analyse any of the hype headlines you've seen across YouTube and Twitter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=681" target="_blank">00:11:21.540</a></span> | <span class="t">it would make it sound like we already have a model that can do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=685" target="_blank">00:11:25.060</a></span> | <span class="t">The announcement though was more about progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=688" target="_blank">00:11:28.020</a></span> | <span class="t">How they had accumulated an incredible dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=691" target="_blank">00:11:31.460</a></span> | <span class="t">And how they had an ultimate goal of doing certain things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=695" target="_blank">00:11:35.860</a></span> | <span class="t">See down here, the ultimate goal of this research is to understand the structure and potential meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=701" target="_blank">00:11:41.700</a></span> | <span class="t">within the natural sounds of dolphins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=703" target="_blank">00:11:43.780</a></span> | <span class="t">Seeking patterns and rules that might indicate language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=706" target="_blank">00:11:46.740</a></span> | <span class="t">If you watch the accompanying video, one of the researchers says "We don't know if they have words".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=712" target="_blank">00:11:52.900</a></span> | <span class="t">Obviously I, and pretty much everyone watching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=715" target="_blank">00:11:55.860</a></span> | <span class="t">HOPES they have a language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=717" target="_blank">00:11:57.220</a></span> | <span class="t">Because that would be insane to be able to decode it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=719" target="_blank">00:11:59.940</a></span> | <span class="t">It's just don't be fooled by any hype headlines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=722" target="_blank">00:12:02.340</a></span> | <span class="t">We don't actually know if they have a coherent language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=725" target="_blank">00:12:05.860</a></span> | <span class="t">We know certain sound types correlate with certain behaviours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=729" target="_blank">00:12:09.540</a></span> | <span class="t">Like whistles that seem like they are unique names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=732" target="_blank">00:12:12.740</a></span> | <span class="t">Special sounds, as many animals have, that they release during fights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=737" target="_blank">00:12:17.140</a></span> | <span class="t">Or buzzing during courtship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=739" target="_blank">00:12:19.060</a></span> | <span class="t">But that's different from more abstract rules, as they say, that might indicate language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=743" target="_blank">00:12:23.620</a></span> | <span class="t">Again, they're looking for potential meanings in the sounds coming from dolphins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=747" target="_blank">00:12:27.860</a></span> | <span class="t">Using a 400 million parameter model that can fit on a Pixel 9 phone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=752" target="_blank">00:12:32.260</a></span> | <span class="t">They then go on to tout one obvious benefit of fitting on a phone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=756" target="_blank">00:12:36.820</a></span> | <span class="t">Which is their goal to "speak dolphin".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=759" target="_blank">00:12:39.860</a></span> | <span class="t">Of course, once you've decoded certain sounds, you can get your phone to emit those sounds and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=764" target="_blank">00:12:44.340</a></span> | <span class="t">essentially communicate with a dolphin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=766" target="_blank">00:12:46.340</a></span> | <span class="t">That would be incredible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=767" target="_blank">00:12:47.620</a></span> | <span class="t">It would, of course, establish a simpler shared vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=771" target="_blank">00:12:51.380</a></span> | <span class="t">That is the hope of researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=775" target="_blank">00:12:55.220</a></span> | <span class="t">That naturally curious dolphins will learn to mimic the whistles to request certain items, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=780" target="_blank">00:13:00.100</a></span> | <span class="t">Again, absolutely incredible research and I really do hope they succeed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=783" target="_blank">00:13:03.780</a></span> | <span class="t">I would be their number one fan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=785" target="_blank">00:13:05.620</a></span> | <span class="t">But I just wanted to give you a sense of where we actually are currently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=789" target="_blank">00:13:09.860</a></span> | <span class="t">By the way, I suspect dolphins do have a proto-language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=793" target="_blank">00:13:13.060</a></span> | <span class="t">So I'm fingers crossed for this mission.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=795" target="_blank">00:13:15.300</a></span> | <span class="t">Now, I could have ended the video there, but as I outlined at the beginning, I wanted to step back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=800" target="_blank">00:13:20.340</a></span> | <span class="t">and give you guys a sense of context of where we are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=802" target="_blank">00:13:22.820</a></span> | <span class="t">You may have gathered from various media reports over the last few years that we are compute constrained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=807" target="_blank">00:13:27.620</a></span> | <span class="t">That the only thing limiting progress is a lack of, for example, NVIDIA GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=812" target="_blank">00:13:32.420</a></span> | <span class="t">Even that, of course, would be a simplification because Google announced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=815" target="_blank">00:13:35.940</a></span> | <span class="t">a 7th generation TPU not reliant on NVIDIA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=819" target="_blank">00:13:39.540</a></span> | <span class="t">But even if you've just imbibed that general narrative that it's all about compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=823" target="_blank">00:13:43.620</a></span> | <span class="t">Well, this video from OpenAI, pre-training GPT 4.5, might have a few answers for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=829" target="_blank">00:13:49.460</a></span> | <span class="t">The truth is, it's actually much more about data constraints now rather than compute constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=834" target="_blank">00:13:54.820</a></span> | <span class="t">It's very interesting because I think up until this rough point in time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=838" target="_blank">00:13:58.580</a></span> | <span class="t">like if you look even through GPT-4, we were largely just in a compute constrained environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=842" target="_blank">00:14:02.900</a></span> | <span class="t">So that was kind of where all the research was going into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=846" target="_blank">00:14:06.020</a></span> | <span class="t">But now we're in a very different kind of regime, starting with 4.5 for some aspects of the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=852" target="_blank">00:14:12.580</a></span> | <span class="t">where we are much more data bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=854" target="_blank">00:14:14.100</a></span> | <span class="t">So there's not a lot more excitement about this research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=857" target="_blank">00:14:17.300</a></span> | <span class="t">It is a crazy update that I don't think the world has really grokked yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=860" target="_blank">00:14:20.340</a></span> | <span class="t">I should pick a different one that the world has understood yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=863" target="_blank">00:14:23.300</a></span> | <span class="t">That we're no longer compute constrained on the best models we can produce.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=867" target="_blank">00:14:27.380</a></span> | <span class="t">That's just like, that was so the world we lived in for so long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=870" target="_blank">00:14:30.420</a></span> | <span class="t">And what's the most useful kind of data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=872" target="_blank">00:14:32.420</a></span> | <span class="t">Evaluations or benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=874" target="_blank">00:14:34.980</a></span> | <span class="t">The Chief Product Officer at OpenAI explained it well this week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=878" target="_blank">00:14:38.900</a></span> | <span class="t">You made a kind of a comment along these same lines around eBells that AI is almost like capped in how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=885" target="_blank">00:14:45.060</a></span> | <span class="t">amazing it can be by how good we are at eBells.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=888" target="_blank">00:14:48.420</a></span> | <span class="t">Does that resonate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=889" target="_blank">00:14:49.780</a></span> | <span class="t">Any more thoughts along those lines?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=891" target="_blank">00:14:51.220</a></span> | <span class="t">These models are intelligences and intelligence is so fundamentally multidimensional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=899" target="_blank">00:14:59.380</a></span> | <span class="t">So you can talk about a model being amazing at competitive coding, which may not be the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=905" target="_blank">00:15:05.060</a></span> | <span class="t">as that model being great at front end coding or back end coding or taking a whole bunch of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=911" target="_blank">00:15:11.540</a></span> | <span class="t">that's written in COBOL and turning it into Python, you know, like and that's just within the software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=916" target="_blank">00:15:16.100</a></span> | <span class="t">engineering world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=917" target="_blank">00:15:17.380</a></span> | <span class="t">Still, most of the world's data knowledge process is is not public.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=924" target="_blank">00:15:24.100</a></span> | <span class="t">It's behind the walls of companies or governments or other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=927" target="_blank">00:15:27.220</a></span> | <span class="t">And same way, if you were going to join a company, you would spend your first two weeks onboarding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=932" target="_blank">00:15:32.100</a></span> | <span class="t">You'd be learning the company specific processes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=934" target="_blank">00:15:34.180</a></span> | <span class="t">You'd get access to company specific data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=936" target="_blank">00:15:36.100</a></span> | <span class="t">It's you can teach these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=939" target="_blank">00:15:39.300</a></span> | <span class="t">The models are smart enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=940" target="_blank">00:15:40.180</a></span> | <span class="t">You can teach them anything, but they need to have the sort of the raw data to to learn from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=948" target="_blank">00:15:48.580</a></span> | <span class="t">And so there's a there's a sense in which I think the future is really going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=953" target="_blank">00:15:53.060</a></span> | <span class="t">incredibly smart, broad based models tailored with company specific or use case specific data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=962" target="_blank">00:16:02.420</a></span> | <span class="t">so that they perform really well on company specific or use case specific things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=967" target="_blank">00:16:07.940</a></span> | <span class="t">You're going to measure that with custom evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=970" target="_blank">00:16:10.180</a></span> | <span class="t">And so, you know, what I what I was referring to is just like these models are really smart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=974" target="_blank">00:16:14.500</a></span> | <span class="t">You need to still teach them things if the data is not in their training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=978" target="_blank">00:16:18.100</a></span> | <span class="t">And there's a huge amount of use cases that are not going to be in their training set because they're relevant to one industry or one company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=983" target="_blank">00:16:23.700</a></span> | <span class="t">That's why OpenAI want to work with anyone they can with their OpenAI pioneer program to get domain specific evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=991" target="_blank">00:16:31.460</a></span> | <span class="t">Having niche evaluations for your models doesn't just help you extract the good data from the bad and improve the data efficiency of your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=999" target="_blank">00:16:39.220</a></span> | <span class="t">It also helps you identify the best new data to improve your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1003" target="_blank">00:16:43.620</a></span> | <span class="t">If that new data contains information or you can think of it as functions or programs that help models perform better during reinforcement learning, then it will be prioritized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1013" target="_blank">00:16:53.300</a></span> | <span class="t">And that is why, to sum up, among many other reasons, I think Google has taken the lead and may even have an enduring lead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1021" target="_blank">00:17:01.220</a></span> | <span class="t">I'm not saying the new O3 won't pip it on a few benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1024" target="_blank">00:17:04.340</a></span> | <span class="t">I'm talking about a long term trend over the next year or two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1027" target="_blank">00:17:07.540</a></span> | <span class="t">Google can source almost unlimited data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1030" target="_blank">00:17:10.500</a></span> | <span class="t">Think Google Search, Android, Chrome, Gmail, Google Maps, YouTube, Waymo Self-Driving, even Calico Life Extension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1038" target="_blank">00:17:18.180</a></span> | <span class="t">And to wrap things up where we started, remember the lack of performance sometimes in SimpleBench or on that brass manufacturing benchmark?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1045" target="_blank">00:17:25.780</a></span> | <span class="t">Well, just a week ago or so, Google announced Geospatial Reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1049" target="_blank">00:17:29.860</a></span> | <span class="t">One of their first attempts to integrate Gemini with a bunch of these spatial reasoning tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1054" target="_blank">00:17:34.820</a></span> | <span class="t">I'll let their one minute promo video speak for itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1057" target="_blank">00:17:37.380</a></span> | <span class="t">From maps and trends to weather, floods and wildfires, Google has studied the geospatial world for decades.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1066" target="_blank">00:17:46.820</a></span> | <span class="t">And we've made that information accessible through AI models and real-time services.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1072" target="_blank">00:17:52.340</a></span> | <span class="t">But synthesizing across these models and combining your data with ours can be challenging and expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1078" target="_blank">00:17:58.820</a></span> | <span class="t">That's why we're introducing Geospatial Reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1083" target="_blank">00:18:03.780</a></span> | <span class="t">We now bring your data and models together with Google's geospatial tools for easier analysis using Gemini's reasoning ability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1092" target="_blank">00:18:12.740</a></span> | <span class="t">"Gemini plans and enacts a custom program, searching over data and gathering inferences from multiple models to unlock powerful insights all through a simple conversational interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1109" target="_blank">00:18:29.700</a></span> | <span class="t">Geospatial reasoning can be a critical tool for advancing public health.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1118" target="_blank">00:18:38.660</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1123" target="_blank">00:18:43.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1125" target="_blank">00:18:45.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1127" target="_blank">00:18:47.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1129" target="_blank">00:18:49.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1131" target="_blank">00:18:51.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1133" target="_blank">00:18:53.620</a></span> | <span class="t">Geospatial reasoning can be a critical tool for emerging technologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1138" target="_blank">00:18:58.580</a></span> | <span class="t">Google taking what could be a permanent lead must be a bitter decade-long sting for Musk and Altman in particular.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1146" target="_blank">00:19:06.580</a></span> | <span class="t">I'm going to end with a 45 second extract from a recent documentary I put on Patreon about how OpenAI was founded a decade ago, almost to the month, to stop Google creating AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1159" target="_blank">00:19:19.540</a></span> | <span class="t">Leaked emails from a later lawsuit between Musk and Altman revealed a May email exchange about stopping Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1165" target="_blank">00:19:25.540</a></span> | <span class="t">And here it is, been thinking a lot about whether it's possible to stop humanity from developing AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1171" target="_blank">00:19:31.540</a></span> | <span class="t">To stop humanity from developing AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1173" target="_blank">00:19:33.540</a></span> | <span class="t">This is Sam Altman in an email to Musk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1175" target="_blank">00:19:35.540</a></span> | <span class="t">I think the answer is almost definitely not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1178" target="_blank">00:19:38.900</a></span> | <span class="t">If it's going to happen anyway, it seems like it would be good for someone other than Google to do it first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1184" target="_blank">00:19:44.980</a></span> | <span class="t">Any thoughts on whether it would be good for Y Combinator to start a Manhattan project for AI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1191" target="_blank">00:19:51.140</a></span> | <span class="t">My sense is we could get many of the top 50 to work on it and we could structure it so that the tech belongs to the world via some sort of non-profit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1200" target="_blank">00:20:00.020</a></span> | <span class="t">Thank you so much for watching to the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yPxavsb2rgk&t=1201" target="_blank">00:20:01.860</a></span> | <span class="t">I can't wait for this OpenAI researcher to add O4mini to this long whiteboard list and have an absolutely wonderful day.</span></div></div></body></html>