<html><head><title>Stanford CS25: V5 I Transformers in Diffusion Models for Image Generation and Beyond</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Stanford CS25: V5 I Transformers in Diffusion Models for Image Generation and Beyond</h2><a href="https://www.youtube.com/watch?v=vXtapCFctTI" target="_blank"><img src="https://i.ytimg.com/vi/vXtapCFctTI/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>All right, thank you so much for joining us today. Today, I'm very honored to welcome Syek Paul from Hugging Face, who works a lot on diffusion models, image generation, and so forth. So his day-to-day includes contributing to diffusers, diffusers library, training and babysitting diffusion models, and working on applied ideas.</p><p>He's interested in subject-driven generation, preference alignment, and evaluation of diffusion models. And when he's not working, he can be found playing the guitar and binge-watching ICML tutorials and suits. So without further ado, I'll hand it off to Syek. I guess I'm here to depart and deviate from the usual theme that's followed at CS25.</p><p>It's not a lot about visual modality, especially the generative aspects of them. So I guess I'm happy in a way that I'm here to depart and deviate from that theme. So, yeah. I wanted to start with a couple of disclaimers. And this talk is definitely not going to be an exhaustive overview of all the possible methods.</p><p>So it might be the case that I didn't cover your work, which you thought to be very seminal. So I apologize in advance. And then I'm not going to cover what is diffusion or flow matching in details, but I'll give a very quick overview just to set the context and tone.</p><p>And then the architectures I'll discuss today will be fairly agnostic to diffusion or flow matching. And since I work on image and videos primarily, I will take my examples with images. But just know that these architectures are fairly well known to generalize to other continuous modalities such as audios.</p><p>And then, of course, I'll share my slide after the talk. And here's how I want to approach this talk. This is the rough overview of all the things that I want to cover. As I mentioned, I'll give you a brief introduction to diffusion models or flow matching as well.</p><p>And then I'll try to set the context by discussing the early architectures for diffusion, the early architectures for image generation in this field. And then we'll head straight to DITS and, as I like to call, their friends. And I'll conclude with some thoughts. And in those thoughts, I'll discuss some of the promising directions that I've become lately interested in.</p><p>And then I'm sure there will be time for Q&A. I really wanted to kind of fascinate you with all the cool examples in the text-to-image arena. But I guess at this point in time, we all know these examples. Because I think, like, these days we are becoming more and more interested in native multimodal generation, not just images.</p><p>But nonetheless, I just wanted to start off by giving you a couple of examples from the text-to-image arena. My favorite is the last one, "tiny astronaut hatching from an egg on the moon." I think human imagination really took off there. But yeah, you can see, apart from DALI-3, all these are open models.</p><p>And the photorealism aspects of these models are quite impressive. So, yeah. And then I want to also give you an infographic of how I like to think about diffusion models in general. So, I like to think about diffusion models as the following. So, when you take a random noise vector and what happens as you denoise it over a period of time so that it becomes a photorealistic image.</p><p>And if you take a look closely, you will notice that it's an iterative process, unlike GANs. GANs are one-shot in nature, but diffusion models are iterative. It's sequential in nature. So, in a way, we are essentially denoising the random noise that we had started off with until and unless we are done with the kind of image that we are looking for.</p><p>So, just know that it's iterative in nature. That's the main takeaway from this slide. And when you condition, when you start conditioning the denoising process with text, for example, you can condition in lots of ways. But let's say, I think, text is one of the more liberating conditions. As you start conditioning the denoising process with text, you get abstract creatures like this.</p><p>But yeah, you will start feeling very liberated with text images, I like to believe. And then, let's take a step back and start developing a mental model of what it takes to have a fairly state-of-the-art text-to-image model. Like, let's say I want to get from this text prompt a cat looking like a tiger and from all the way up to the image on the top.</p><p>What does it take? What are the components that we are looking for? How should they be connected? So, I want to kind of give you a connected graph of how the different components should be connected so that we can backtrack them and start developing more intuition. So, of course, when you have text, you need to have some sort of embedding so that you can sort of work with those embeddings and we have text encoders for them.</p><p>And like state-of-the-art diffusion models, they usually rely on more than one text encoders. For example, stable diffusion three, it relies on three text encoders, not one, not two, but three. So, you have your text prompt, you pass it off to text encoders, and you then have the embeddings. And as we saw in the earlier slide, we are starting off with some random noise drawn from a Gaussian distribution.</p><p>So, we are starting with some noisy latents and then you have your text embeddings. And then you have some time step which you do the math with the scheduler. I'm going to come to the scheduler component in a minute, but you have your conditions at this point in time.</p><p>You have your text embeddings, you have your noisy latents, and you have some time step. And then you have your core diffusion network, and you pass all these inputs to your core diffusion network. And it's a sequential process. You run the diffusion network over a period of time as we saw in the earlier slide.</p><p>And then it's going to give you some refined latent. And then you give it to some decoder model, and you have your image out. Now, I would like to call out two broad classes of diffusion models. This overview resembles the pixels, the latent space diffusion models. But there's another class of diffusion models, which is called the pixel space diffusion model.</p><p>But the recent or the state of the diffusion models, they are all latent space based. Because pixel space is quite prohibitive and computationally intensive in nature. That's why it's more common to see latent space diffusion models. So that's why you see noisy latents there, and not raw pixels. And if you were to sort of generalize this to other continuous modalities, the basic idea here is how do you represent the raw modality data points, and how do you compute intermediate representations of them?</p><p>So if you were to do this on the audio space, you can think of some similar analogies. But long story cut short, for a fairly well-performing text-to-image system, these are roughly the components that you need. So yeah. And then, let's now start developing some notations, so that we can start discussing about how these models are trained, and finally how you should perform inference with these models.</p><p>So again, I'm giving these examples with images, but these are fairly agnostic, modality agnostic. Apart from text, of course, these won't work on discrete tokens. So let's say I have some original image, and I'm drawing some noise from a standard Gaussian distribution. And then, let's say I'm also drawing some time step from a uniform distribution, and then I have a particular noise schedule, and also some terms that are controlling the noise schedule.</p><p>And then, let's say I have some conditioning vector, and by conditioning vector, I essentially mean some text embeddings or some other form of structural controls, like let's say depth maps, segmentation maps, and so on. And then, you have your diffusion model that you will learn. And during training, what we basically do is, we compute some intermediate representations of the images, or let's say if you are working on the pixel space, you add small amount of noise to your clean images, and you make the model predict what was the amount of noise that was added.</p><p>This is one very popular and widely adopted parameterization of the diffusion network training. It's called the epsilon objective, if you will. And that's basically it. So, we basically make the model learn what was the amount of noise that was added. So, that's training. And during sampling, we repeat the noise prediction part in a sequential manner until and unless we arrive at an image that we feel good about.</p><p>So, we start from a random noise, and we denoise it over a period of time with some condition, which can be text embeddings, for example. And then we sort of repeat the denoising process over a period of time until and unless we arrive at a data point that we feel good about.</p><p>So, yeah. And for flow matching, which is becoming more and more common these days, for example, flux, if you have heard of the model flux or stable diffusion three, these are all flow matching based. In flow matching, the paths become more straight. We try to connect noise and clean data through a straight path.</p><p>That's why you see the linear interpolation equation in the first point. And we try to predict the target velocity field with a neural network. So, the key takeaway here is we try to connect noise and clean data through a straight line. But in diffusion models, the path is not assumed to be a straight line.</p><p>So, that's why a lot of simplification you will see in flow matching. But long story cut short, in this talk, we are more interested in the network component of things. We are not interested in how you should add the noise, what particular noise schedules you should follow, and so on.</p><p>We are more focused on the parameterization side of things in this talk. So, yeah. And let's now start discussing the components we might need. The components that are kind of expected in a diffusion model. What are the core requirements that we want to see in a diffusion model to have so that it performs within some bound of expectations.</p><p>So, we have to figure a way out to deal with the noisy inputs. As we saw noise, we can't really get rid of the noise because that kind of gives us the foundation. And usually, in pixel space, your shapes will look something like this if your channels first. And if you are working on the latent space, it will, of course, be compressed a bit.</p><p>And we usually always deal with the latent space diffusion model. So, it helps to have the kind of shapes that you would expect to see in your models that will flow through. And then you will have to deal with the conditions, right? Because all the text image models that you see, text is your condition.</p><p>So, you'll have to deal with your conditions. And, of course, time step. So, the amount of noise that gets added to your latents, it should depend on the time step. Like, let's say, if your time step is 10, the amount of noise that will get added to your data points, it will be different from the time step if the time step were to be somewhere like 1,000 or 100.</p><p>So, that's why time step is also a very crucial condition. Because it basically tells the model that which point in the trajectory it's in. Should it denoise less? Should it denoise more aggressively? And so on. And then you have got other forms of conditions such as class, maybe tiger, bird, or just like natural language.</p><p>And then we have to also model the dependencies quite a bit. How should the noisy inputs interact with your conditions? Of course, you might want to think about these things through the lens of cross-attention and so on. But we'll get to that in a moment. And then how should the final outputs be produced?</p><p>Should we just flat out decode everything if we were to deal with transformer-based architectures? Should we be up-sampling if we were to deal with pure convolutional architectures? And so on. And usually, for the diffusion network, the input shape exactly equals the output shape until and unless you are dealing with a separate parameterization of diffusion.</p><p>There are other parameterizations, but it's way more common to see the inputs that are flowing through the diffusion model will equal the outputs that the diffusion model is supposed to produce. And then, yes, here's a bit of a history. So, DDPM, all the early work in this area, DDPM latent diffusion models, they all used unit-based architectures.</p><p>And I think, historically speaking, units have kind of dominated this area for quite a bit, like StyleGAN, they all had a unit-based architecture, right? And then, until SDXL, which is a fairly recent, I would like to say recent because it's like from 2023. And if you are trying to develop a chronology of all the architectures that have come around, I would like to argue that SDXL is fairly decent.</p><p>And also, based on its usage, I think it's decent. So, until SDXL, all the works relied on unit. So, I think it makes sense to sort of discuss the unit-based architecture in this paradigm and then start seeing why we really need to transition to transformers. So, yeah. And, of course, the unit for diffusion, it's giant.</p><p>It's also one of the reasons why you would want to probably get rid of it, but that's not the end of the story. But let's try to see what are the different components that are involved in our giant unit. So, you, of course, need to have an input convolutional stem that directly operates at the inputs that are coming at it.</p><p>And then, you have a bunch of down blocks, which is basically comprised of custom residual blocks made of normalization layers and convolutional layers. And then, you have got custom transformer blocks. Again, normalization projections and regular transformer blocks. And then, you have got convolutional layers for upsampling. So, basically, you are… So, when you are operating on the latent space, you would want to upsample to a higher resolution.</p><p>And then, you would have some middle block like you would have in a standard unit architecture where you would not have any resolution changes. And then, you would have a series of up blocks, which will basically downsample the upsampled, you know, outputs that you had in your down blocks.</p><p>And finally, you will produce the output that will have the same shape as your input. And it basically resembles the same kind of blocks that you would have in your down block counterpart. But instead of doing upsampling, it will have downsampling layers. So, yeah. And there are some miscellaneous things that you should worry about.</p><p>For example, time step embeddings, how you should embed your time steps, and additional embeddings. For example, if it's a class conditional model, the way to embed them would be different. If it's a text image model, the way to embed the text embeddings would be different. Basically, the way you modulate all your conditional embeddings, that will change depending on the kind of conditions that you are dealing with.</p><p>And here's basically how a down block of the unit architecture looks like. I mean, it's so, so giant that I had to transpose it. So, it's kind of very painful to even think in the head. So, if you are someone that works with architectures quite a bit, I think we'll have to solemnly agree that this is bad.</p><p>This is bad news already. I mean, I can't even imagine it in my head. So, and here's a bit of a zoomed-in look of what goes into that down block component of the unit. You have got a bunch of custom ResNet blocks, as I was discussing. I've got a bunch of custom transformer blocks, as I was discussing, but nothing too brutal.</p><p>Nothing that you haven't seen already. So, it's just a bunch of composition of those blocks. And I have also tried mentioning the resolution changes in each of the stages. So, yeah. And putting it all together, it looks something like this. I mean, I have tried shortening it quite a bit.</p><p>But I mean, if I had to imagine it in my head, it's going to be extremely painful. And the blocks are just, oh, damn. It's very prohibitive in nature, just for the task of image generation. So, yeah. That's like the complete perspective. I know it's not as complete as you might expect.</p><p>But it's also hard to kind of fit all the unit in a single screen. So, yeah. And then of course, the natural next thing that one would have tried would be to try to replace the convolutional layers with MLP blocks to try to simplify quite a bit. And yeah, folks from Google tried it in the UWIT architecture.</p><p>So, that's there. That was the precursor to the pure transformer-based architectures, but not quite. It still had its fair share of complicacies and the architectural painfulness, as you saw in the unit-based design. So, yeah. Now, I'm going to now try to motivate why we really need a pure transformer-based architecture.</p><p>Now, the first point is probably very obvious. We would want to benefit from the advancements that are happening in the transformer-based architectures, like all the divine benevolence, as Noam Shazir likes to call it. You would want to have Swigloo. You would want to have QK normalization. You would want to have parallel MLP layers and so on.</p><p>So, that's one. Of course, good scaling properties and so on. And then, let's say you want to connect the pure transformer-based diffusion architecture or the backbone with some other backbones, let's say a pure LLM-based backbone, the integration becomes very easy. And then, it allows you to get rid of the giant unit, which I guess is my main motivation.</p><p>But I hope I was able to convince you fairly strongly that why we need a change, like a paradigm shift in the architectures that are primarily inspired from the unit design. And also, this is side to the sore eyes already. I mean, this is not uncommon. We all know that this is the standard forward pass in a vision transformer network.</p><p>They should feel very familiar at this point in time. Now, the point I am trying to make here is this doesn't have to change a whole lot. If we were to sort of extrapolate this to image generation. And I think you will agree with me that this is not changing a lot.</p><p>Like all the core components are there. The patchification is there. The positional embeddings are there. Of course, the way to class embed things, that's different. The Y embedder bit, you would still need to have a component to embed your time steps. That's different. But the rest of the components, it's still there.</p><p>Like you are still iterating through the blocks. You have your final layer to finally decode your outputs and so on. And then non-patchification layer. So like this is still very similar to how you would do it in a standard bit. But of course, you have to, you know, account for the generation head also at the same time.</p><p>So my point is, you are not changing a whole lot in the standard bit forward pass. Now taking a closer look, I think this should also kind of feel very familiar apart from a few mods here and there. Like we have got some scale and shift parameters, which I'm going to discuss in a bit.</p><p>But rest of the other blocks, like you have got the same layer norm, you have got the patchification layer, you have got some embeddings and so on. And then you have got a linear and reshape operation. So most of it should feel familiar, but the ones that are apparently a little foreign, I'm going to discuss them now.</p><p>So yeah. Let's start with the time step bit. Like how do we actually embed time steps? I'm, I've been talking about time steps for quite a bit now. But let's now see how do we actually embed the time steps. And time steps are really important. And I'm also going to show you the expected shapes, the output shapes.</p><p>And in this case, this is batch size comma the hidden dimension of the transform blocks that you are expecting. So how do we embed time steps? And time steps can range in between zero to thousand, where zero meaning like no noise and thousand should mean it's fully noised. So each T is embedded into sinusoidal frequency to make them phase aware.</p><p>Like at any point in time, the network is seeing extremely low frequencies and extremely higher frequencies. And it must be aware of the kind of phase it should be, it should be modulating into. So that's why sinusoidal frequencies are really helpful. And then after that, how the network should weigh these different frequencies.</p><p>And in order to model those weights, we basically pass it through a very shallow MLP. And then how do you embed class labels? You just take an nn.embedding layer as simple as that. And it's the standard patchification. You do it with a convolutional stem. And for positional encodings, they use the standard sine cosine scheme.</p><p>And the final conditioning is you first embed the time steps and then you embed the class labels and you basically sum them up. And then you have your final condition that goes into the transformer blocks. And this is very important to note that C remains fixed across all the blocks.</p><p>So that's very important to note. And you would probably think in order to model the conditioning with the actual inputs, the noisy latency, you would probably want to use cross-attention, but that's not the case, as we will see in a few slides later. So let's take a step back and try to think of how we can inject the conditioning bit into the transformer blocks.</p><p>So we have something called adaptive layer norm, which is very important in order to be able to model the stylistic aspects that you are getting out of your images. And it's basically this. So you have your standard layer norm and then you have an additional set of parameters, which we call modulation parameters, which basically operate on the condition space.</p><p>And remember, the condition is basically a summation of the time step embeddings as well as the class embeddings. So that's your condition right there. And skipping the regular transformer bits, like the QKV, the multi-head self-attention and the MLP layers, we know the equations that govern the computations that take place within a standard transformer encoder block, right?</p><p>So this is fairly well known. Now the part that's not known at this point in time, hopefully, is how do we modulate the conditioning bit? How do we actually inject the conditioning in the transformer blocks? So this is how we do it. Instead of doing any cross-attention, which would have been a fairly natural choice, I guess, we actually do not do cross-attention.</p><p>We are still doing self-attention and then we are basically modulating the conditioning along with self-attention, as you can see in the bottom half of your equations. And these modulation parameters are learned from the modality that you are training these things on. And then in order to get your final outputs, you basically have a single-layer decoder, and then you basically unpatchify it to get the same shape as your inputs.</p><p>Now, a note on init, because the initialization is fairly important. It's all standard vision transformer init, but with two key modifications. Each transformer block is initialized as identity block, taking inspiration from the early works in ImageNet training, wherein if you have a bunch of ResNet blocks, you usually initialize the beta parameter in the batch normalization layer as zero.</p><p>It helps with training stability and stabilization, and turns out that's the case here as well. Now, coming to the adaptive layer normalization thing, it's very important and it's also more compute efficient. And as I was mentioning, cross-attention would have been the natural choice in order to kind of model the dependency between the conditions and the noise inputs, but that's not at all the case, if you take a look at the graph.</p><p>In fact, adaptive layer norm performs the best and it beats cross-attention big time. And it's also not because the conditions are fairly simple to model, but it's because when you are operating with continuous modalities like images, it's not that trivial to model dependencies with cross-attention. And when you are conditions are simple like class embeddings, it doesn't make sense to model them with cross-attention.</p><p>It's also because it's a waste of compute, and the graph kind of confirms it. And later works have also explored like a more compute efficient variant of adaptive layer norm, which we are going to get to in a couple slides later. And how you are modulating these conditions, like in this case, we are basically operating on a summation of the time step embeddings and the class embeddings.</p><p>Now, how you are modulating it across and throughout your different transformer blocks, that becomes very important, as we will see in a couple blocks later. And as expected, it scales fairly graciously with more compute and turns out that you can basically apply all the unit-based training techniques to a diffusion transformer.</p><p>So, that's pretty cool. And it scales pretty graciously. It performs also well when compared to other equi-sized unit counterparts. And of course, at this point in time, you must be thinking no one really does class conditional in the space of image generation. So, yeah, that's where we are headed next.</p><p>Now, I want to try to motivate what it would take to enable text to image generation in the standard diffusion transformer architecture, because I think it makes sense to approach the problem in that sense. like, what are the components that are missing in a standard diffusion transformer so that it becomes a tool for text to image generation as well.</p><p>And I think PixArt Alpha is one of the early works that explored it. So, yeah, we are going to definitely see it in details. Now, one natural question would be how to embed the input natural language text prompts. And the answer would be simple. You would need a text encoder.</p><p>And that's exactly what the PixArt Alpha work does. And then how to learn your contextual dependencies. Now we have, instead of classes, we have got natural language text on top of time steps. We could do self-attention on noisy latents. And then we could do cross-attention in between your noisy latents and text.</p><p>And mind you, this text is not just class. This text is some natural language description, like a baby astronaut hatching out of an egg on the moon. So this is natural language description we are talking about. This is not simply class labels. So that could make sense. It could make sense to have self-attention on noisy latents, to model the local dependencies within the patches.</p><p>And then also to cross-attention in between the noisy patches and the text embeds. And then we'll have to figure a way out to modulate the time steps as well. And as we saw in the DIT work, that it's important to modulate the time step embeddings throughout your transformer blocks.</p><p>And then, if you have got access to a class conditional diffusion transformer, and if it's compatible, it might also make sense to kind of initialize some blocks from it. Because why waste compute, right? It might help with training stabilization, and so on. Now, that's exactly what Pixart Alpha does.</p><p>So if you were paying attention at this point in time, you would have realized that implementing these things is not extremely challenging, but it helps to know that that's exactly what Pixart Alpha does. So it uses a text encoder to embed the prompts. It uses self-attention on noisy latents.</p><p>It uses cross-attention to model the dependency between the noisy latents and the text embeddings. And it also initializes from a class conditional DIT model in order to accelerate training. So it kind of helps to know that you can still think about these things and see it getting implemented in practice.</p><p>So yeah. And here's some discussion around the use of text encoder. So Pixart Alpha used Flan T5XXL in order to really get that text rendering ability, and some of the concurrent works like Imagine, they showed that if you scale the text encoder, and if you ever wanted to render text in your images, having a better text encoder actually helps.</p><p>And also then there's this problem of long prompt following ability, because models like SDXL, they rely on clip for embedding text, and clip has a very short context length. I think it's 77, but with the T5XXL, you get a way longer context length. So you basically get to have longer prompts, you get to describe your prompt in a bit more detail.</p><p>So that's there. And also exploring the space of text encoder, it's still a kind of good research problem to take a look at. And if you were wondering, why not just use a standard language model in place of T5? Well, it's not that difficult to actually use it. At many works like Lumina, they explore it quite a bit.</p><p>And if you were wondering, diffusion models are already compute bound. So why add the baggage of adding another heavy model? Well, I think it's okay to use that because computing prompt embedding, it's a one-step process. So it's okay to have a memory bound model like a large language model in the mix.</p><p>Cool. And here we will again see the return of adaptive layer norm. And remember, if you forgot, remember that for each diffusion transformer block, we were operating on a summation of timestep embeddings and class embeddings. Right? And for Pixart Alpha, we basically, we already have a way to compute our text embeddings, which we do not want to touch.</p><p>Because those embeddings are already computed with a dedicated rich text encoder. So we maybe do not want to touch those embeddings and modulate them, unlike diffusion transformers, where we modulated the class embeddings as well. And instead of having adaptive layer normalization blocks in every diffusion transformer encoder block, we basically maintain tables.</p><p>We basically maintain embedding tables and we sum them up. So instead of doing another matmul for each and every transformer encoder block, we basically get away with addition. Addition with another embedding table. And it helps us reduce the compute quite a bit. It helps us reduce 27% of the original diffusion transformer computation.</p><p>And I think the idea is fairly elegant. It also gives another perspective to think about how you can reduce parameters and still sort of maintain performance. This was good. Cool. And I must say, this is impressive performance for a fairly compact model. Like it's only 0.6 billion parameter model and it's already like breaking all the charts.</p><p>Like when it came around in 2023, it was fairly, fairly good, fairly good. I think I have the general scores. No, we don't. But the last chart here, it basically shows you the human preference rating. Quality wise, it's overall image quality and alignment wise. It's the alignment between the text and the generated images.</p><p>And in both of those aspects, PixArt Alpha performed fairly well. Now, before I jump to the time and memory complexity of these models, because we are still using the quadratic vanilla attention thing, I want to see if there are any questions at this point in time. So, I'm going to open the floor for questions, if that's okay, Steven.</p><p>That's okay. Yeah, go ahead. You mentioned that the state-of-the-art diffusion models require more than one text encoder. Why is that? What's the benefit of having multiple different types of encodings for the same text? Oh, yeah, for sure. So, the question is, why would you want to have multiple text encoders?</p><p>And why does it help improve the image generation performance? So, you saw in class conditional dates, you modulate not just the time step embeddings, but also the class embeddings. So, the conditional embeddings that we were modulating along with self-attention were a summation of class embeddings and time step embeddings.</p><p>Now, for text image models, that is not as trivial. So, apart from your text embeddings, you also kind of have your time step embeddings. But what about modulating the other condition that you computed from your text input? So, turns out that the richer the representations are, the diverse the representations are, the better it is for the generation backbone.</p><p>Now, when I said many models do use more than one text encoders, they usually have a combination of Clip and T5. Now, Clip is an entirely different model and T5 is an entirely different model. With Clip, you embed some kind of contrastive nature in the text embeddings. And with T5, you have a completely different nature in your text embeddings.</p><p>So, the more the diverse are these embeddings, the better it is for the generative performance. But there's no systematic study of this. That's what the general belief is. Yeah, but there are works that get away just by using a single language model. So, maybe the language model, maybe the language models inherit both as a virtue of their good pre-training.</p><p>I think most of them are using diffusion transformers because of their efficiencies and also they are easy to adapt. Like, if you wanted to embed another form of control, like let's say you wanted to additionally prompt the model with some stylistic reference from images, it's easier to do that on a diffusion transformer as we will see in a couple slides later.</p><p>So, in the original diffusion transformer block, you initialize all your modulation parameters as basically an affine layer. So, you can do that, right? I mean, you can chunk it and then you have your different modulation parameters. And this is where your matrix multiplication lies. And notice how I am computing the modulation parameters, right?</p><p>But here I am only computing it through an embedding table. There's no affine transformation happening. And I end up adding it to my timestep embeddings, that's it. So, that's how I avoid the matmul. And that's largely why I am able to sort of reduce the computation by 27%. I'll take one last question before I jump to the next section.</p><p>If there's any. I've got some over Zoom as well as online. I'll ask some, let me see. Someone's wondering, is there still any point of GANs for image and video generation? Or have they been fully replaced basically with diffusion models? I won't say they have been fully replaced because you, as we saw, diffusion models are still sequential in nature.</p><p>So, if you're looking for really ultra real-time generation, I think GANs are still the way to go. And many companies are, in fact, using them. So, if you're looking for really cool one-shot generation, I think GANs is the way. And even if, so there's this literature around timestep distillation that basically looks at reducing the number of timesteps or inference steps that you need in order to produce a good quality image.</p><p>So, there you need a GAN loss, actually. So, GANs are not going to be completely replaced yet. Okay, because I thought, I thought GANs had a lot of issues like mode collapse. Oh yeah, they have, but if you have got specific use cases and if you have got a fairly good data set, you can still train a good GAN.</p><p>I can ask one more if there's time from online. Sure, go ahead. Image generation generally requires a lot of data to train, especially for diffusion models. Are there techniques or architectural choices for low data regimes? That's a good one. The short answer is no. You need, I mean, I think it's kind of correlated with your use case a bit.</p><p>Like for medical imaging, probably you do not need a whole lot of diversity, but if you need a fairly well and generative model, I think you need to train it on a lot of data to inherit all the biases that you are looking for. Like if you want to train on the distribution of natural images and you want the model to always produce realistic images, I think you need to have a lot of data.</p><p>At least diverse data. Because nowadays for like LLMs, you know, there's like zero shot, few shot learning after you preach. Oh, I'm going to talk a bit about in context learning for diffusion models. Okay. But, but you still, you are assuming you have access to a pre-trained model, right?</p><p>Right, right. So there you go. Alright, I'm going to, I'm going to start this leg of the talk by discussing the quadratic time and memory complexity argument a bit, because now we are in the image generation territory and when you are trying to generate like really high resolution images, like 4K images, So, vanilla attention becomes extremely prohibitive even if you were to operate on the latent space.</p><p>Like take a look at the dimensions of the latents. Uh, let's say your, uh, number of latent channels is 16 and your latent dimensions are 512, 512. Like it's, it's still way too large, way larger than what we are used to seeing in the VLM space. Right. And I have some dummy computations here.</p><p>This is of course not using flash attention. If I were to give you some dummy estimates, it would be like 190 GBs in floating point 16. And this is all like reasonable defaults. I mean, I have 24 attention heads, I've got batch size one, the sequence length is flattened out.</p><p>I would need 190 GB. This is of course not using flash attention. But you kind of get an idea of the prohibitive computation space of the quadratic attention that we usually use in diffusion worlds. If we were to deal with really, uh, high quality and high resolution, uh, images.</p><p>So what could we do? Like two simple things that we could do is operate on an even more compressed space. Like this, this 1, 16, 512, 512, this space is already compressed enough. But could we even increase the compression ratio further? Right. And then the second, second obvious thing could be to use some form of linear attention.</p><p>that doesn't do the n cross n, uh, multiplication. So that brings me to my next architecture, which is the SANA architecture. And it uses both. It uses an, an even more compressed latent space. And it also uses a linear variant, uh, of the, of the, of the attention mechanism.</p><p>So let's see. So a linear attention is on the other hand. And of course you might expect some performance loss with, with the linear complexity of the attention mechanism. And to compensate for the performance lost, we use mix FF. And I'm gonna, I'm gonna get into the details in a bit, but just wanted to give you a quick overview.</p><p>So SANA does self, self-attention, self-linear attention as you will. But it still does cross-attention to model the dependencies between the noisy latents, as well as, uh, the text prompts that you are providing it to. And then there's no n-square computation happening in the self-attention. And the equation makes that clear.</p><p>So we have got shared terms, shared terms computed from the KV projections. And these are reused for all the queries. And this way we are basically and effectively, uh, not doing the n cross n multiplications. And all of the multiplications are upper bound to n. That's why we are able to reduce from order of n-squared to order of n.</p><p>And then, as I was mentioning, there has to be some form of accountability as we are not using the quadratic attention mechanism, right? So we use mix FFN blocks. It's basically a bunch of inverted residual blocks and point-first convolutions in order to model the local dependencies. Because it turns out that when you take the softmax out of, out of the picture, you lose all the notion of locality.</p><p>Let's not say all, majority of the locality notions are taken out also. So that's why you, you need some, some components to also model the locality aspect. And mix FFN blocks are used for that. And for the first time, Sana got rid of the positional embedding bit. So, I mean, it's very, it's very funny to say it nope.</p><p>So no positional embeddings. And mix FFN blocks actually helps because you have a bunch of convolutional blocks in there. So we are still returning to the convolutional argument, but not so much. Like it's, it's not fully convolutional. You just have a few convolutional layers thrown in there. So it's largely did based, but instead of using like a linear MLP, you have two convolutional layers to, to, to account for the local interactions.</p><p>And it performs fairly well. For its compact size, it performs fairly well. I must say that we shouldn't take, take into account the kernel fusion and the fancy, fancy DPM problem solver that they're using. But up until the fourth row, I think it's still performing fairly well. Like it's giving very decent general performance.</p><p>It's giving very decent DPG performance. And these are all metrics that assess, that assess a given image in terms of compositionality, in terms of their fidelity, in terms of their overall quality, and so on. And these metrics are fairly well grounded in terms of reality. So yeah. And, and this is, this is probably going to be the final flavor of attention that I'm going to discuss before I move on to other topics.</p><p>And this is, I think, a bit new. And also new in the sense that no one in the VLM space really does it. So what happens if we were to kind of, you know, model dependencies of the different modalities in separate spaces? So let's see. So, and the, and, and one motivation behind this could be text embeddings.</p><p>Let's say you are computing text embeddings with a large language model. You will end up inheriting a lot of bias from it, right? Like the unidirectionality bias will be there. If you're using a standard autoregressive large language model, you will end up inheriting the unidirectionality bias. So, so you will have bias.</p><p>And they might, you know, creep into your, your generative model in all sorts of different ways. So, so one idea could be to do QKV projections, but, but separately. Like you do QKV projections on the text embeddings and you also, you also maintain another set of QKV projection parameters for your noisy latins.</p><p>And then you concatenate them before you compute the attention. So you operate on a concatenate, concatenated representation before you actually do the attention. So you project your QKV stuff on your image latins separately from the text embeddings. You concatenate them before you actually compute attention. So this is basically it.</p><p>So this is MMDIT that was introduced in the Stable Diffusion 3 paper. And their motivation was to get rid of the different biases that might be there in the text embeddings. And they also showed how qualitatively different text embeddings can be from the image embedding. So I guess that kind of gives you a hint about why we might need different text embeddings to not end up inherit each other's biases and have more diversity in the mix.</p><p>So it basically looks like this. So it might feel a little complicated, but the reason why it's so big is because we have separate projection matrices for the different modalities that we are modeling. So on the left-hand side, we have got the captions. And on the right-hand side, we have got the noisy latents.</p><p>And we have got separate QKV projections and adaptive layer non-matrices for the separate modalities. So that's why it feels big. But conceptually, it's basically this. So you have got separate adaptive layer non-matrices for the separate modalities. You have got separate QKV projection matrices. You have got separate output projection matrices.</p><p>And then you have got separate, everything separate. Everything is separate from the two modalities that we are interested in. So in a way, it kind of gives a way to co-evolve the two embeddings from the two different modalities that we are working with for the given task, which is image generation in this case.</p><p>And also, if we are very used to cross-attention, you might want to ask, if we were to compute attention in this way, how do we do masking in the first place? And that's actually an active area of research. We do not know how to do masks holistically if we were to do mmdit variant of attention.</p><p>And then modulation happens with both time steps and the conditional embedding that you are operating with. So Stable Diffusion 3 uses a different set of pulled text embeddings that are usually computed from CLIP and not the T5-based text encoder. So again, you need some form of diversity to not end up inheriting the bias from the other text encoder that you have.</p><p>And Stable Diffusion 3 uses three, but they can be mixed and matched during inference. They showed that you need the T5 if you need to have really good text rendering capabilities. But other than that, you can still do a lot with the two clips that they use. And you can drop the T5 if you are not solely focused on text rendering tasks.</p><p>And of course, it matters quite a bit. You might want to ask, does mmdit matter at all? And it turns out that it does matter. Like they tried all forms of different attention variants. They tried cross-dit, they tried uvid. And mmdit seems to be the variant that gives you the lowest validation loss.</p><p>And they also show that validation loss is fairly well correlated with the kind of image generation matrix that we care about, such as GenieVal and so on. And it scales fairly well, but it needs QKNORM relation. And thanks to the concurrent set of works, they didn't have to reinvent QKNORM from the scratch.</p><p>So thanks to the developments in the regular transformer literature, they were able to just use QKNORM to solve the training and stability issues. And I think Stability Fusion 3 is basically incomplete without this picture. And I'm not showing this picture just for fun, but also because it shows how complex is this prompt, and how well Stability Fusion 3 was able to interpret this prompt and get us a creature like this.</p><p>So I must give credits to the authors of Stability Fusion 3 who came up with these kinds of prompts. But it does fairly well. So it was among the first kinds of models that shows impressive prompt following ability, and also while also preserving the details that we care about.</p><p>So that was quick. And mmdit didn't stop here. There are different flavors of mmdits that I wanted to discuss. Like in Stability Fusion 3, all the transformer blocks followed the mmdit flavor of attention. And it's computationally demanding. I mean, if you have worked with transformer encoders, you would appreciate the computational intensity of mmdits, because you are having to kind of maintain separate projection matrices for the different modalities that you are working with.</p><p>So it's computationally extremely demanding. So maybe we could combine mmdit blocks and regular dit blocks to be able to better utilize the flops. So conceptually, this becomes this. So you have some number of mmdit blocks, you concatenate the final representation, and then you operate on the concatenate space and basically do vanilla dit.</p><p>You do not do mmdit. And you end up utilizing the flops a little bit. And that's exactly what recent models like Flux from Black Forest Labs, they do. So another twisty one could be you have modality A, you compute output A by passing it through a bunch of transformer blocks.</p><p>And then you have got modality B, let's say text embeddings. You pass it through another set of transformer blocks. You concatenate these outputs, and then you pass them through another set of transformer blocks. And then you basically have your final output. And in this way, you can configure all the different blocks in their own manner.</p><p>like the transformer blocks for modality A could be different from the transformer blocks that you would use for modality B. So this way, you have a greater level of flexibility and control. And that's what the Lumina 2 work did. And as we can see, for conditions, they pass it through a separate set of transformer blocks.</p><p>For noisy latents, they pass it through a separate set of transformer blocks. And they end up concatenating them and then again, they have another set of transformer blocks. So transformer blocks on the left-hand side, in the RHS big block, can be different from the transformer blocks used to model the noisy latents.</p><p>And I want to give you a sense of how we can think of simplifying all of this design. Because it might feel complicated at this point in time, but I don't know. Maybe we can simplify it quite a bit. But I want to quickly see if there are any questions at this point.</p><p>Do we have any questions over Zoom? Yeah, we've got some online questions. Let me see. Somebody just asked, is GPT-4.0 a completely different architecture? How does that compare with the diffusion transformer? That's a good one. I think it uses a hybrid architecture. It definitely has an LLM component to it, which was evolved to generate images.</p><p>I'm going to come to this sort of hybrid architectures in a moment. Okay. And then someone's wondering for evaluation, are there good automatic metrics for image and video generation? Or is it mainly human-based subjective evaluation, similar to creative writing? I think it's an ensemble of different metrics. Like you can't evaluate an image on a single metric.</p><p>And also it depends on what you are exactly looking for. Like if you are more interested in compositionality, the metrics will change. You are more, if you are more interested in aesthetics, the metrics will change. So that depends. Let me see. What are your thoughts on the vision models?</p><p>Someone noted, there's a common perspective about like image generation models failing to generate things like fingers. I also see that, you know, do they still suffer from things like counting and like spatial consistency? Or are those basic things pretty much like solved? I think models like flags definitely, they do a whole lot better.</p><p>And there's a question of like, what are the major additional challenges of video generation compared to image generation? And how ways to overcome them? Well, the first problem is the time. Because image generation, diffusion-based image generation models are already compute intensive. And then with video generation models, you have got another dimension of temporality.</p><p>So they just become more compute intensive. And then you have got more cars of dimensionality to basically address. And then if you are generating more and more frames, they just become more and more compute intensive. So how do you, how do you kind of make them more efficient is definitely the need of the art.</p><p>It also adds things like temporal dependencies. Yeah, yeah, exactly. Yeah, another access of dependency to model. Right, right. And you are basically doing full 3D attention, which is extremely prohibitive to even think about. In the video, you don't have to generate frame by frame from scratch. You already have something more like figuring out and then just do a small change.</p><p>Well, I mean, if you, if you take, if you take a 2D image generation model, and if you try to conflate it such that it can also do spatio-temporal generation, well, it doesn't turn out. It will work well, at least in the realistic setting. Like if you wanted to generate cinematic frames, it won't work.</p><p>Because the spatio-temporal consistency, it just gets lost. Oh, but, but, but, but there has to be a limit, right? Like if you are operating with that many frames as your previous input, you're, you're like, the queue also becomes extremely prohibitive. So you have to figure out how, how, how you can compress the temporal dimension effectively.</p><p>Like, let's say you have a variable number of frames. Now, how do you map it on the latent temporal level so that it still has some meaning, uh, to it while being efficient at the same time? So when you are doing videos, you no longer just have the spatial compression, but also you have temporal compression.</p><p>And how, how do you model the two as you, as you, as you make progress? So, are there any other questions? I'll just take one maybe? Come on, Zoom asks, can you give some intuition about adaptive layer normalization and why it works so well? Oh yeah. So, so for, for layer norm, so you, you are basically using layer norm for stabilizing training.</p><p>I would say like you have more stable representations across the, across the, across the blocks. But for images, there are certain kind of characteristics that you would want to model beyond the standard representations that you are computing. And in, and you will have to let those kind of, let those features flow freely into your transform blocks.</p><p>And that's why you need the modulation parameters. Otherwise, the interaction in between the normalization parameters and your regular attention features or whatever MLP features that you are computing, they don't get to interact, uh, in a way that will benefit the generation performance. If you were to deal with just understanding or maybe just discriminative performance, it wouldn't have mattered that much, much.</p><p>But if you care about stylistic aspects and fidelity, you need to kind of modulate, uh, the additional features that you are getting from the, uh, visual, uh, cues. Yeah. Any, any, any, any other questions? Yeah. So you mentioned, we don't know yet how to do masking. For, for mmdit.</p><p>Yeah, mmdit. Yeah. What's the next best thing? Is there an alternative to masking? Maybe that's a hunch, yeah. Yeah. So may, why would you need masking? That's a good reason to ask for mmdit. Because if you, if you are, so mmdit, mmdit was done in order to benefit, like, in order to get away with the unidirectionality bias that you may have in your text embedding representations.</p><p>So that was one. Second, if you have really long prompts, you do not need to compute masks in the first place, right? Yes. And it also turns out that it's always better to have long prompts, like descriptive prompts, rather than having to have short prompts. So that is kind of a hand wavy way to answer this question.</p><p>But long story cut short, it's really non-trivial to add masks when you are doing mmdit attention. So, uh, there you go. So that, go ahead. It was about doing separately QKV on both text and image, how about on the image side? Yeah, you, you, so when you are doing the image, image interaction, you could in theory do masks.</p><p>But again, how would you frame the problem? where would you end up adding the masks if you are just doing text to image? Maybe, maybe if you have another form of condition, conditional control, maybe you would want to mask some of the interactions there. But if you are just restricted to text to image, where would you add the masks in the first place?</p><p>Yeah, but what I'm asking to avoid masking, you know? Oh, yeah, yeah, yeah, yeah. I mean, probably that's why they didn't do masking in the first place. Maybe their motivations, yeah, yeah, exactly, exactly. I'll maybe take one final question if there's any. Okay, cool. So I, I also want to leave you with some ideas for simplification, simplifying the design a bit, if these things felt a little complicated, because they do not have to be complicated at all.</p><p>We can simplify it quite a bit, and let's see how. So how much is parameter sharing is useful? Like things like Adiln that we saw and the, and the, and the work on Pixar Alpha that reduce the computation by 27%. How much is that kind of parameter sharing is useful?</p><p>And we saw Adiln already. Can we also share QKVO and the MLP, like Albert? This is from back in the days, Albert, where we shared all the projection matrices for, for a couple of layers. We also shared the MLP matrices and so on. And do we really need, like self-attention and then cross-attention?</p><p>Or do we really need MMD? I think this will probably answer your question. So look forward, I look forward to that. So do, do we really need these things? Because this kinds, this in a sense complicates the design space quite a bit. Can we simplify it? So can we basically do self-attention on this concatenated space?</p><p>We do not have to do self-attention on noisy latency. noisy latency. We do not have to do cross-attention on noisy latency and text. We do not have to do MMDit. Can we basically concatenate all the image tokens and the text tokens and compute self-attention on it? Can we do it?</p><p>And text encoders, like what's the secret sauce? We are using three. We are using two. What helps? What helps the cause of text to image generation at the end of the day? It turns out you can simplify things a lot. So one thing that you can do is you can parameter share quite a bit.</p><p>You can parameter share QKVO if you are looking for efficiency. You can definitely parameter share the adaptive layer norm parameters. And you can basically do self-attention on a concatenated representation space of image tokens and text tokens. So this is, I think, good news, but not so much because Apple didn't open source this work.</p><p>So we'll have to do our own. But this is good to know that you can simplify the design quite a bit. So the extreme right hand side is the simplified design that I was talking about. So you are basically operating on a concatenated space. You are reducing the adaptive layer norm parameters quite a bit.</p><p>And then your parameter sharing the QKV projections as well as the MLP layers. So yeah. And it turns out to be working well, fairly well, I must say, in practice. So the green one is the simplified design, and it always gets the lowest amount of loss. So yeah. And as I was mentioning, other than sharing the adaptive layer norm parameters, you can also share the QKV parameters as well as the MLP.</p><p>if you can compromise some of the quality a bit, and if you are only targeting efficiency. And for text encoders, the Apple folks found out that bi-directional clip and text-only LLM is better. So instead of using a combination of clip and T5, if you use clip and a regular large language model, it's better for text presentations.</p><p>So the performance is pretty good. The final row is Apple's work. It's called DIT year. So it turns out fairly well. All the bold numbers from DIT. So it turns out pretty well. So long story cut short, you can simplify the design quite a bit. So you do not need mmDIT as it turns out, but we will see, I guess.</p><p>Now, one burning question you must be having at this point in time is, text-to-image is liberating, but how do I inject more control, more sources of control? Like if I were to do more structural inputs to my text-to-image model, how do I do that? Maybe I want the model to follow a particular pose.</p><p>Maybe I want the model to follow a particular segmentation map and so on. And how do I do that? And can I combine multiple structural signals at the same time? So one could be to learn maybe an auxiliary network that gives you a way to compute salient representations from your structural image signals, and maybe you then figure a way out to inject those structural signals into your base diffusion model.</p><p>So that could be one. And control net, these lines of work, they basically follow this philosophy. And then maybe you could also change the base diffusion transformer model. You basically increase your input channels to accept, you know, more controls. And that's what the flux control framework does. And maybe you could also learn a small adapter network to model the dependencies between all your conditions and your noisy latent tokens.</p><p>And also one important call-out here is structural control will always have spatial correspondence, but other tasks like subject-driven generation or image edits, they may not have direct spatial correspondence. So what do we do in those cases? And this is, again, an active area of research. And for videos, well, rope works fairly well for positional encoding.</p><p>And as I was mentioning, the attention computation space becomes way more prohibitive because it's full 3D. And if you were thinking about some form of factorized attention, it doesn't work. I have been very explicit about it. And for efficiency, if you are particularly interested in efficiency-related literature around video models, I highly suggest checking out the LTX video work.</p><p>And for performance, like for fidelity, photorealism, and so on, I think one has to be my favorite. It's pretty good. I have a little demo here. It's not that bad. So it's still short-form video, but the realism aspect and the fidelity aspect of these videos, I think they have improved quite a bit.</p><p>And some next-generation architectures. Now I'm coming towards the end of my talk. I wanted to also give you a flavor and a sense of next-generation architectures. You probably have this question, how do we enable in-context learning in diffusion models? Like in language model, it's very common, zero-shot learning and so on.</p><p>How do we enable in-context learning in diffusion models? Current architectures are clearly not sufficient to enable it in a reasonable time. So the basic idea is you take an LLM model, you add some components so that it also becomes adept at generating images. Like recent works like Bagel, then there are works like Lada, Mada, they basically follow this area of work.</p><p>They are not necessarily diffusion-based, but they share a similar philosophy. So you basically start from a pre-trained LLM, you add components to it so that it also becomes adept at generating images. Like you could do autoregression on discrete tokens, for example, and you could also do at the same time diffusion on the continuous tokens.</p><p>So that's one line of work that's called Transfusion. So as I was mentioning, this Playground v3, this Fusedit, this Transfusion. Fusedit is from our group. It's the only open source work that you will probably find that tackles this problem of in-context learning and how do you sort of explore these architectures in a holistic manner.</p><p>And if you are already feeling inspired, I hope you are feeling inspired to explore these architectures. I highly welcome you to check out the library that I work on at Hugging Face. It's called Diffusers. We have got reasonably clean implementations of all the models that I discussed today, which should probably inspire you to hack into these things and tweak them.</p><p>And there's a bunch of things I didn't get to cover, of course. I am a little bit over time. I think one minute over time. I'm probably going to finish it in the next one minute. So I apologize. I didn't cover MOEs, hot topic again. But MOEs are making their ways in the diffusion community.</p><p>It's called HiDream. I didn't cover training at all. There are all kinds of shenanigans there. How do you do data? How do you do alignment? How do you do post-training? How do you do safety mitigation? How do you do memorization mitigation? And so on. And of course, these architectures go well beyond image and video generation.</p><p>They find their application in all sorts of stuff, like robotics, gene synthesis, and so on. And if you are into mechanistic interpretability, interpretation of these beasts are not trivial at all. So they interest me quite well. And yeah, that's about it. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you.</p></div></div></body></html>