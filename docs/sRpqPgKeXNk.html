<html><head><title>Trends Across the AI Frontier — George Cameron, ArtificialAnalysis.ai</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Trends Across the AI Frontier — George Cameron, ArtificialAnalysis.ai</h2><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk"><img src="https://i.ytimg.com/vi_webp/sRpqPgKeXNk/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=0">0:0</a> Introduction to Artificial Analysis: An overview of the company's work in benchmarking AI models across various modalities and metrics.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=114">1:54</a> The State of AI Progress: A look at the rapid advancements in AI since the launch of ChatGPT, with a focus on the current leaders in AI intelligence.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=246">4:6</a> The Reasoning Models Frontier: An exploration of the trade-offs between the enhanced intelligence of reasoning models and their increased latency and cost.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=505">8:25</a> The Open Weights Frontier: A discussion on the closing intelligence gap between open-weights and proprietary models, with a nod to the significant contributions from China-based AI labs.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=626">10:26</a> The Cost Frontier: An analysis of the dramatic decrease in the cost of accessing high-level AI intelligence and the implications for application development.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=849">14:9</a> The Speed Frontier: A look at the remarkable increase in the output speed of AI models and the technological advancements driving this trend.<br><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=994">16:34</a> The Future of Compute Demand: A concluding perspective on why the demand for compute will likely continue to rise despite efficiency gains, driven by larger models, the quest for greater intelligence, and the rise of AI agents.<br><br><div style="text-align: left;"><a href="./sRpqPgKeXNk.html">Whisper Transcript</a> | <a href="./transcript_sRpqPgKeXNk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone, I'm George, co-founder of artificial analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=26" target="_blank">00:00:26.000</a></span> | <span class="t">A quick background to who we are before we dive into things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=29" target="_blank">00:00:29.980</a></span> | <span class="t">A quick background to who we are: we're a leading independent AI benchmarking company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=52" target="_blank">00:00:52.580</a></span> | <span class="t">We benchmark a broad spectrum across AI, so we benchmark models for their intelligence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=58" target="_blank">00:00:58.320</a></span> | <span class="t">we benchmark API endpoints for their speed, their cost, we also benchmark hardware and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=65" target="_blank">00:01:05.220</a></span> | <span class="t">all the AI accelerators out there, and we also benchmark a range of modalities, not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=70" target="_blank">00:01:10.480</a></span> | <span class="t">language but also vision, speech, image generation, video generation, and we publish essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=79" target="_blank">00:01:19.300</a></span> | <span class="t">nearly all of it for free on our website, artificialanalysis.ai, whereby we benchmark over 150 different models across a range of metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=91" target="_blank">00:01:31.460</a></span> | <span class="t">We also publish reports, many of which are publicly accessible, and we also have a subscription for enterprises looking to enter or bring AI to production in their environments in an efficient and effective way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=114" target="_blank">00:01:54.360</a></span> | <span class="t">Let's start off with AI progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=117" target="_blank">00:01:57.300</a></span> | <span class="t">Let's set the scene.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=118" target="_blank">00:01:58.260</a></span> | <span class="t">So, it's been a crazy two years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=121" target="_blank">00:02:01.260</a></span> | <span class="t">I think that we've all felt it in this room, whereby OpenAI kicked off the race with the ChatGPT and GPT 3.5 launch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=132" target="_blank">00:02:12.260</a></span> | <span class="t">And since then, it's only gotten more hectic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=135" target="_blank">00:02:15.660</a></span> | <span class="t">There's been more and more model releases by more and more labs pushing the AI frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=142" target="_blank">00:02:22.160</a></span> | <span class="t">So, the current state now of frontier AI intelligence, I think this order of models will be familiar to a lot in this room.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=153" target="_blank">00:02:33.160</a></span> | <span class="t">O3 is the leader, but followed closely by O4 Mini with Reasoning Mode High, DeepSeq R1, the release in the last week or two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=164" target="_blank">00:02:44.160</a></span> | <span class="t">GROC 3 Mini, Reasoning High, Gemini 2.5 Pro, Claude 4 Opus Thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=171" target="_blank">00:02:51.620</a></span> | <span class="t">This benchmark is our Artificial Analysis Intelligence Index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=178" target="_blank">00:02:58.160</a></span> | <span class="t">It's made up of a composite index of seven evaluations, which we then wait to develop our Artificial Analysis Intelligence Index,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=189" target="_blank">00:03:09.620</a></span> | <span class="t">which just provides a generalist perspective on the intelligence of these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=197" target="_blank">00:03:17.240</a></span> | <span class="t">We all have an understanding of what frontier AI intelligence is, but what I want to explore with you today is that there's more than one frontier in AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=208" target="_blank">00:03:28.140</a></span> | <span class="t">There's trade-offs to accessing this intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=210" target="_blank">00:03:30.780</a></span> | <span class="t">You shouldn't always use the leading most intelligent model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=215" target="_blank">00:03:35.100</a></span> | <span class="t">And so, what we want to do is we want to explore the different frontiers out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=219" target="_blank">00:03:39.480</a></span> | <span class="t">And as an AI benchmarking company, we're going to bring some numbers to the fore to help you reason about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=226" target="_blank">00:03:46.180</a></span> | <span class="t">First, we'll be looking at reasoning models, next, we'll be looking at the open weights frontier, third, the cost frontier, and lastly, the speed frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=236" target="_blank">00:03:56.080</a></span> | <span class="t">There's other frontiers out there that we benchmark, but we'll focus on these key ones today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=246" target="_blank">00:04:06.080</a></span> | <span class="t">Starting with reasoning models, what we've done here is we've taken our intelligence index and looked at that relative to the output tokens used to run the intelligence index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=257" target="_blank">00:04:17.980</a></span> | <span class="t">So, we've measured all of how many tokens each model took to run our seven evaluations, and we've plotted it on this chart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=266" target="_blank">00:04:26.980</a></span> | <span class="t">And you can see two distinct groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=268" target="_blank">00:04:28.980</a></span> | <span class="t">It's helpful to think about these separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=271" target="_blank">00:04:31.880</a></span> | <span class="t">So, non-reasoning models, which offer less intelligence, but require fewer output tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=278" target="_blank">00:04:38.880</a></span> | <span class="t">And reasoning models, which use more output tokens, but offer greater intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=283" target="_blank">00:04:43.880</a></span> | <span class="t">And this is important to look at because more output tokens comes with trade-offs, both for request latency as well as cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=293" target="_blank">00:04:53.780</a></span> | <span class="t">We're going to bring some numbers to draw that out and look at the real differences here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=298" target="_blank">00:04:58.780</a></span> | <span class="t">So, starting with output tokens and the verbosity of these models, just how yappy these reasoning models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=309" target="_blank">00:05:09.680</a></span> | <span class="t">We can see that there's an order of magnitude difference between reasoning and non-reasoning models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=316" target="_blank">00:05:16.680</a></span> | <span class="t">It's not just that feeling, "Oh, this is taking a long time."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=319" target="_blank">00:05:19.680</a></span> | <span class="t">It's real.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=320" target="_blank">00:05:20.680</a></span> | <span class="t">It's an order of magnitude.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=321" target="_blank">00:05:21.680</a></span> | <span class="t">So, between GPT 4.1, it required 7 million tokens to run our intelligence index evaluations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=331" target="_blank">00:05:31.580</a></span> | <span class="t">But then, O4 Mini High took 72 million tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=334" target="_blank">00:05:34.580</a></span> | <span class="t">And the yappiest of them all, Gemini 2.5 Pro, took 130 million tokens to run our intelligence index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=343" target="_blank">00:05:43.580</a></span> | <span class="t">And, as mentioned, this has implications for cost as well as end-to-end latency, responsiveness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=350" target="_blank">00:05:50.580</a></span> | <span class="t">So, looking at latency, we benchmark the API latency of how long it takes to receive a response when accessing these models via their APIs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=363" target="_blank">00:06:03.480</a></span> | <span class="t">Here, we can see that GPT 4.1, on media and across our requests, took 4.7 seconds to return a full response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=372" target="_blank">00:06:12.480</a></span> | <span class="t">And, O4 Mini High took over 40 seconds, roughly another 10x, or order of magnitude, increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=382" target="_blank">00:06:22.380</a></span> | <span class="t">This has implications for applications and uses which require responsiveness, even enterprise kind of chatbots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=391" target="_blank">00:06:31.380</a></span> | <span class="t">We don't always reach for O3 in ChatGPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=396" target="_blank">00:06:36.280</a></span> | <span class="t">And, Facebook's done a lot of studies on this, where they've looked at the, for consumer apps, where they've looked at user drop-off by latent, application latency, which clearly demonstrate this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=409" target="_blank">00:06:49.280</a></span> | <span class="t">Sorry, do you mind if we jump back a slide?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=411" target="_blank">00:06:51.280</a></span> | <span class="t">And, it also has implications for how we're building.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=421" target="_blank">00:07:01.680</a></span> | <span class="t">So, I think particularly with agents, whereby 30 queries in succession is not uncommon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=429" target="_blank">00:07:09.180</a></span> | <span class="t">It has, it's a multiplier effect on the latencies for your application and how you can build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=436" target="_blank">00:07:16.880</a></span> | <span class="t">If you have faster responses, maybe you can make that 30, 100 queries, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=442" target="_blank">00:07:22.180</a></span> | <span class="t">And so, putting numbers to that, in terms of agents, 30 is normal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=447" target="_blank">00:07:27.080</a></span> | <span class="t">And so, even less than, than O4 Mini, maybe you're at 10 seconds for a reasoning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=452" target="_blank">00:07:32.780</a></span> | <span class="t">If you're running 30 queries, that's 300 seconds that a user might be waiting for a response, or an application might be waiting for a response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=460" target="_blank">00:07:40.680</a></span> | <span class="t">That's five minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=461" target="_blank">00:07:41.480</a></span> | <span class="t">If, with the order of magnitudes that we're dealing with here, if that 10 seconds was one second, then those 30 queries takes 30 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=470" target="_blank">00:07:50.980</a></span> | <span class="t">30 seconds versus five minutes impacts what you can build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=475" target="_blank">00:07:55.580</a></span> | <span class="t">30 minutes, think of a contact center application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=478" target="_blank">00:07:58.880</a></span> | <span class="t">That might, maybe 30 seconds is okay there, but five minutes, definitely not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=482" target="_blank">00:08:02.780</a></span> | <span class="t">Who likes waiting on the phone that long?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=484" target="_blank">00:08:04.980</a></span> | <span class="t">Or, imagine if you had to use Google, and each time that you wanted to use a function, it took five minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=491" target="_blank">00:08:11.980</a></span> | <span class="t">This impacts how we can build with these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=495" target="_blank">00:08:15.080</a></span> | <span class="t">And so, I think bringing numbers to these trade-offs is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=498" target="_blank">00:08:18.980</a></span> | <span class="t">I'd encourage everybody to measure them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=504" target="_blank">00:08:24.980</a></span> | <span class="t">Next, we're going to move to the open weights frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=509" target="_blank">00:08:29.680</a></span> | <span class="t">Around the time of GPT-4, there was a huge delta in terms of open weights intelligence versus proprietary intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=519" target="_blank">00:08:39.280</a></span> | <span class="t">Lama 65b or Lama 270b wasn't close to the intelligence of GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=525" target="_blank">00:08:45.480</a></span> | <span class="t">What I'd like to show you here is where we plot our intelligence index by release date is that that gap, it closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=533" target="_blank">00:08:53.980</a></span> | <span class="t">It closed until, with great models like Mixtrelate times 7 and Lama 405b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=541" target="_blank">00:09:01.180</a></span> | <span class="t">But, 01 broke away in late 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=545" target="_blank">00:09:05.680</a></span> | <span class="t">But, then of course, I think we remember Deep Seek released v3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=552" target="_blank">00:09:12.680</a></span> | <span class="t">I think December 26th ruined some of my Christmas holiday plans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=560" target="_blank">00:09:20.380</a></span> | <span class="t">Had to tell my family I need to go read this paper, it's really exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=566" target="_blank">00:09:26.380</a></span> | <span class="t">And then, of course, R1 in January.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=569" target="_blank">00:09:29.380</a></span> | <span class="t">The gap between open weights intelligence and proprietary model intelligence is less than it's ever been.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=577" target="_blank">00:09:37.880</a></span> | <span class="t">Particularly with the recent R1 release in the last couple of weeks, which is only a couple of points different in our intelligence index to the leading models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=589" target="_blank">00:09:49.580</a></span> | <span class="t">You can't talk about open weights intelligence without talking about China.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=594" target="_blank">00:09:54.580</a></span> | <span class="t">The leading open weights models across both reasoning models and non-reasoning models are from China-based AI labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=603" target="_blank">00:10:03.580</a></span> | <span class="t">Deep Seek is leading in both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=606" target="_blank">00:10:06.080</a></span> | <span class="t">Alibaba with their QEN 3 series is coming in second in reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=613" target="_blank">00:10:13.580</a></span> | <span class="t">We also have other labs, such as Meta and NVIDIA with their Nemo Tron fine-tunes of Lama coming in close as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=626" target="_blank">00:10:26.580</a></span> | <span class="t">Let's look at the cost frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=628" target="_blank">00:10:28.280</a></span> | <span class="t">This is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=629" target="_blank">00:10:29.280</a></span> | <span class="t">I think similar to N10 latency impacts what you can build.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=634" target="_blank">00:10:34.580</a></span> | <span class="t">So bringing some numbers here, we can really see these order of magnitudes play out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=640" target="_blank">00:10:40.580</a></span> | <span class="t">N103 cost us almost $2,000 to run our intelligence index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=645" target="_blank">00:10:45.580</a></span> | <span class="t">TechCrunch actually wrote an article about how much money we were spending on running evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=652" target="_blank">00:10:52.580</a></span> | <span class="t">We didn't want to read it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=655" target="_blank">00:10:55.580</a></span> | <span class="t">You can see 4.1, a great model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=660" target="_blank">00:11:00.580</a></span> | <span class="t">It's 30 times roughly cheaper in terms of the cost to run our intelligence index compared to 01.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=667" target="_blank">00:11:07.580</a></span> | <span class="t">And 4.1 nano, over 500 times cheaper to run our intelligence index than 03.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=674" target="_blank">00:11:14.580</a></span> | <span class="t">You should think about these when building applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=678" target="_blank">00:11:18.580</a></span> | <span class="t">The cost structure of your application might dictate what you can use here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=683" target="_blank">00:11:23.580</a></span> | <span class="t">And how you use them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=686" target="_blank">00:11:26.580</a></span> | <span class="t">Those 30 sequential API calls for your agentic application could be 500 and still be cheaper than an 03 query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=697" target="_blank">00:11:37.580</a></span> | <span class="t">A key point to note here with this cost to run intelligence index and why we don't just look at the per token price is that, and the labs maybe don't want you to think this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=712" target="_blank">00:11:52.580</a></span> | <span class="t">But you're paying for the cost per token, but then you're also paying for how verbose the models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=719" target="_blank">00:11:59.580</a></span> | <span class="t">All the reasoning tokens that are output when these models are in their thinking mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=724" target="_blank">00:12:04.580</a></span> | <span class="t">You pay for those as output tokens, even if some of the labs hide them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=730" target="_blank">00:12:10.580</a></span> | <span class="t">And so you need to think about this and measure it in your application and benchmark not just by the cost per million tokens, but also considering how many reasoning tokens there are and how verbose these models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=745" target="_blank">00:12:25.580</a></span> | <span class="t">You can see even amongst the non reasoning models, there's big differences between how verbose these models are in responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=754" target="_blank">00:12:34.580</a></span> | <span class="t">So for instance, we'll go to the next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=758" target="_blank">00:12:38.580</a></span> | <span class="t">Do you mind if we go back one, please?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=763" target="_blank">00:12:43.580</a></span> | <span class="t">So what we've done here is we're now going to look at the trends in terms of cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=776" target="_blank">00:12:56.580</a></span> | <span class="t">And so what you can see here is we've bucketed models by how intelligent they are, intelligence bands, if you will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=787" target="_blank">00:13:07.580</a></span> | <span class="t">And what we can see here is that accessing GPT-4 level of intelligence has fallen over 100 times since mid-23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=797" target="_blank">00:13:17.580</a></span> | <span class="t">This is the case across all quality bands.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=801" target="_blank">00:13:21.580</a></span> | <span class="t">You can see that even when a new quality band, a new frontier is reached, O1 mini in late '24.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=810" target="_blank">00:13:30.580</a></span> | <span class="t">Quickly, within only a few months, the cost of accessing that level of intelligence halved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=815" target="_blank">00:13:35.580</a></span> | <span class="t">This is moving quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=818" target="_blank">00:13:38.580</a></span> | <span class="t">And so what I would say to you is when building applications, think about what if cost wasn't a barrier when you're building.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=827" target="_blank">00:13:47.580</a></span> | <span class="t">It's a very important kind of cost exercise because it might well be that if you build for a cost structure that doesn't work now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=836" target="_blank">00:13:56.580</a></span> | <span class="t">then maybe in six months' time, that will be possible and it will be feasible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=848" target="_blank">00:14:08.580</a></span> | <span class="t">Next, we're going to look at the speed frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=851" target="_blank">00:14:11.580</a></span> | <span class="t">So this is how quickly you're receiving tokens, the output speed, output tokens per second that you're receiving after sending an API request.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=863" target="_blank">00:14:23.580</a></span> | <span class="t">This has been increasing and has increased dramatically since early '23 as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=870" target="_blank">00:14:30.580</a></span> | <span class="t">So similarly, because there's a trade-off typically between intelligence and speed, we've grouped models into certain buckets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=879" target="_blank">00:14:39.580</a></span> | <span class="t">And we can see here that they've all increased in terms of how quickly you can access a level of intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=886" target="_blank">00:14:46.580</a></span> | <span class="t">So 4.0, I believe, was around 40 output tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=893" target="_blank">00:14:53.580</a></span> | <span class="t">Now you can access -- that was in 2023.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=896" target="_blank">00:14:56.580</a></span> | <span class="t">Who remembers hitting -- it wasn't a reasoning model -- hitting enter in chat.tpt and just waiting for it to output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=903" target="_blank">00:15:03.580</a></span> | <span class="t">Especially code, which you want to just copy straight into your editor and, you know, hit run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=908" target="_blank">00:15:08.580</a></span> | <span class="t">See if it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=910" target="_blank">00:15:10.580</a></span> | <span class="t">Now you can access that level of intelligence at over 300 tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=915" target="_blank">00:15:15.580</a></span> | <span class="t">A few drivers here that I'll go through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=918" target="_blank">00:15:18.580</a></span> | <span class="t">It's not the focus of the talk, but important to reference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=921" target="_blank">00:15:21.580</a></span> | <span class="t">Model sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=922" target="_blank">00:15:22.580</a></span> | <span class="t">So we're seeing more MOEs, mixture of experts models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=926" target="_blank">00:15:26.580</a></span> | <span class="t">And they activate only a proportion of parameters at inference time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=933" target="_blank">00:15:33.580</a></span> | <span class="t">Less compute per token, which means it can go faster, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=938" target="_blank">00:15:38.580</a></span> | <span class="t">And MOEs were around back then, but they're getting more and more sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=942" target="_blank">00:15:42.580</a></span> | <span class="t">A smaller proportion of active parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=945" target="_blank">00:15:45.580</a></span> | <span class="t">Next, smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=947" target="_blank">00:15:47.580</a></span> | <span class="t">Smaller models are getting more intelligent, particularly with distillations, you know, 8B distillations, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=956" target="_blank">00:15:56.580</a></span> | <span class="t">Inference software optimizations, like flash attention and speculative decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=963" target="_blank">00:16:03.580</a></span> | <span class="t">And lastly, hardware improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=966" target="_blank">00:16:06.580</a></span> | <span class="t">So H100 was faster than A100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=969" target="_blank">00:16:09.580</a></span> | <span class="t">Now we've recently launched benchmarks of the B200 on our artificial analysis website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=975" target="_blank">00:16:15.580</a></span> | <span class="t">And it's getting over 1,000 output tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=977" target="_blank">00:16:17.580</a></span> | <span class="t">Think about that relative to the 40 output tokens per second of GPT-4 in 23.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=984" target="_blank">00:16:24.580</a></span> | <span class="t">There's also specialized accelerators like Cerebra, SambaNova, Grok.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=990" target="_blank">00:16:30.580</a></span> | <span class="t">I want to share a house view here to frame things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=997" target="_blank">00:16:37.580</a></span> | <span class="t">Yes, things are getting more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=999" target="_blank">00:16:39.580</a></span> | <span class="t">Yes, the cost of accessing the same level of intelligence is decreasing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1004" target="_blank">00:16:44.580</a></span> | <span class="t">And hardware is getting better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1005" target="_blank">00:16:45.580</a></span> | <span class="t">We're getting more system output throughput on the chips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1011" target="_blank">00:16:51.580</a></span> | <span class="t">But our view is that demand for compute is going to continue to increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1016" target="_blank">00:16:56.580</a></span> | <span class="t">We're going to see larger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1018" target="_blank">00:16:58.580</a></span> | <span class="t">I mean, DeepSeq.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1019" target="_blank">00:16:59.580</a></span> | <span class="t">It's over 600 billion active, sorry, not active, total parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1027" target="_blank">00:17:07.580</a></span> | <span class="t">And the demand for more intelligence is insatiable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1031" target="_blank">00:17:11.580</a></span> | <span class="t">Reasoning models, as we saw, the YAPI models, they require more compute at inference time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1038" target="_blank">00:17:18.580</a></span> | <span class="t">And lastly, agents, whereby 20, 30, 100 plus sequential requests to models is not uncommon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1049" target="_blank">00:17:29.580</a></span> | <span class="t">These actors multiplies on the demand for compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1052" target="_blank">00:17:32.580</a></span> | <span class="t">And so the house view playing with these numbers is net net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1056" target="_blank">00:17:36.580</a></span> | <span class="t">We're going to continue to see compute demand increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1060" target="_blank">00:17:40.580</a></span> | <span class="t">Thanks, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sRpqPgKeXNk&t=1064" target="_blank">00:17:44.580</a></span> | <span class="t">Thanks, everyone.</span></div></div></body></html>