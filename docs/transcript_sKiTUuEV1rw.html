<html><head><title>[Full Workshop from Microsoft] Github Copilot - The World's Most Widely Adopted AI Developer Tool</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>[Full Workshop from Microsoft] Github Copilot - The World's Most Widely Adopted AI Developer Tool</h2><a href="https://www.youtube.com/watch?v=sKiTUuEV1rw" target="_blank"><img src="https://i.ytimg.com/vi_webp/sKiTUuEV1rw/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Well, I hope that everybody has had a great first half of your day so far and had a good lunch. Thank you all for being here. This is a talk, this is going to be a workshop about the world's most widely adopted AI developer tool, and that, of course, is GitHub Copilot.</p><p>I'm Christina Warren. I'm a senior developer advocate at GitHub, and I'm going to be assisted today in this workshop by my colleagues Dave, Alex, and Harold, who are all here in the front. Well, Alex is in the back, so we've got people sorted around. We did a version of this workshop earlier this morning.</p><p>This is going to be our second try at it. We learned some things, so we'll see how things go. And then as we go along, if you have any questions or if you need help, I will have some resources where you can get help online and GitHub, but also feel free to raise your hand, and we will do what we can to help you out.</p><p>And this is just kind of going to be a little bit of an overview of how we're going to be doing this today. So we're going to start off with an overview of GitHub Copilot, and then we're going to talk about getting set up and access for the repository that we'll be working out of and getting access to GitHub Copilot if you don't already have access.</p><p>I would love, just out of curiosity, for the people in the room, how many of you have used GitHub Copilot before? Okay. Awesome. Awesome. That's great to see. We also will have some resource links. Then we're going to get into the labs, and then, of course, we will be doing some feedback.</p><p>This right here, this QR code, and I'm sorry that I was not able to make the URL larger, but this is going to be kind of ground zero for this workshop, this repository. So if you want to follow along with us, going to this URL, gh.io/ai-fair-workshop, is going to take you to a repository which will show you everything that you need to get is in this repository.</p><p>This slide deck is also in that repository, so if there's anything you want from that, that has been added to that as well. But this is going to be our central resource and where we're going to be working. I will have this up again a little bit later, but you want to go ahead and either go to this URL now or use the QR code, that would be great.</p><p>All right. So let's just do a little bit of an overview about GitHub Copilot, which is your AI pair programmer. Okay, so GitHub Copilot is an extension. I mean, it works in multiple places, actually. We're not going to talk too much about the integration for GitHub Enterprise and some of the things we're doing on GitHub.com.</p><p>We're going to primarily be focused on the experience inside your IDE. But this is, you know, one of the very first coding assistants was introduced in Private Preview in 2021, formally released in 2022. So we've been at this a while now. And basically, it's going to be using the context that you provided, and then it's going to synthesize code to match that context.</p><p>And so using files that you have open, tabs that you have open in your editor, as well as comments that you're writing, or you can query directly with GitHub Copilot chat in a chat interface, to get coding suggestions, to, you know, convert comments into code, so I can make a comment, say, you know, write a function to parse the URL, or to autofill for repetitive code, if I'm using the same code block, over and over again, in the same pattern, over and over again, Copilot is going to know how to do that.</p><p>It can also help streamline creating tests, and show, you know, alternative ways of achieving the same task. And, as I said, you know, this is your Copilot for an AI-powered software development lifecycle. Like, we're taking place in the IDE and the terminal. We have support for Visual Studio Code, which is what we're going to be showing off.</p><p>Visual Studio also has support for GitHub Copilot, as well as a number of the JetBrains IDEs, and there's also a NeoVim extension, which I learned last session actually does have some GitHub Copilot chat support, which is cool. And Copilot, beyond that, also can work on GitHub.com, and so if you have an enterprise account and have GitHub Copilot for enterprise, there are different things you can do on GitHub.com to also integrate with with Copilot to aid in the creation of pull requests and fixing potentially insecure code.</p><p>And, you know, even we're going to have a session tomorrow with one of our colleagues kind of going from ideation of a kind of an idea of how you want to plan something out to actually implementing that whole plan. And so, one of the questions that we get a lot is, is Copilot training, you know, the model on my data?</p><p>And the answer to that is no. There is an option if you want to contribute your code completion stuff into the larger model, but that can be turned off. And by default, that is off for business and enterprise accounts. And so, how this is working is we are actually just taking the context from the editor, and my colleague Carol can talk a little bit more about how that's working, and kind of getting an idea of what I'm doing.</p><p>That's going to be sending it to OpenAI's GPT model, and then sending suggestions back. Now, that model itself has been trained on public source code, and that's how that's gotten better. But your code itself is not feeding our feeding our model. There's an option, as I said, if you wanted to do that, but that is not how it works by design.</p><p>So some of the, again, some of the answers to the common questions, you know, does it train on my code? No, when you're using it for business or for enterprise, none of your code is stored by GitHub, and even on the individual plans, there's an option to be able to turn that feature off.</p><p>Turn that feature off. What is included in the context? So this is going to be the files that you're working on. And so that could be, you know, tabs that are open in your IDE starting from the current file. And, you know, another question we often get is, you know, can't I just, you know, write the code on my own?</p><p>Well, yeah, but how often have you ever been in a situation where you have to write the same piece of boilerplate code over and over and over again? And oftentimes doing the same thing with minor changes. We love to automate ourselves out of jobs, make things easier on us so we can focus on things that we actually enjoy doing more.</p><p>And this is one of the great things about, I think, AI and about coding assistance like Copilot in general. So one of the features that we introduced, gosh, I guess it was a year and a half ago now, is GitHub Copilot chat, which basically brings kind of a chat GPT-like experience into your editor.</p><p>So you can actually chat directly inside your editor with Copilot, and you can ask questions. You can also ask for specific queries about the code that is in your editor itself. So you can say -- you can select some code and say, hey, how can I optimize this? How can I make this better?</p><p>Or write me a unit test for this code. And it can do that. You can also ask more general queries. It's also great for being able to translate, you know, code from one language to another. Sometimes that actually works really well. You'd be surprised. And you can also, you know, ask it to propose fixes for bugs and to help make code more secure.</p><p>And this support for Copilot chat is in VS Code. Visual Studio, JetBrains, and as we learned recently, yes, NeoVim. How well it works in NeoVim, I don't think all the features are quite there, but it is there. And one of the things when we talk about -- and we're going to be getting to some demos in just a second -- but one of the things that we talk a lot about when we talk about, you know, coding assistance are prompt crafting.</p><p>Because a lot about how well these things work depends on what prompts you're writing and what information you're giving Copilot. Because, again, it is a Copilot. It is not doing it for you. And so writing good prompts can help get better results. And so some of the ways that we've seen that really improve is by making sure that you have the right amount of context.</p><p>This can be really useful, again, like I said, having what tabs you have open, starting with, like, the most recent file, or the central most file, rather, and what other files are open, that can provide Copilot with context about what you're doing. Also, if you put things in a comment about what you're trying to do, or even in some cases, giving it, you know, an example of code that you want it to replicate or that represents what you're trying to achieve, that can go a long way with helping Copilot understand what you're trying to achieve.</p><p>Also, being very clear about what your intent is, and making that clear in your prompt. You know, I want to create a function to parse a URL. This is actually going to be one of the examples in the labs. Being very specific about what purpose you have in mind in that prompt.</p><p>Being vague can work okay. But the more specific you can be, the better results you're going to get. Also, focusing on clarity. Being easy to understand. That's going to also help the AI give better results. And being specific. And that means, you know, saying, okay, I don't want it to look like this.</p><p>I want it to look like this. So, being detailed and precise will also go a long way, getting the most out of Copilot. And the workshop that we have will go through all these different steps. Yes. Yes. Sure. Yeah. Can we get the second mic on? Yes. Can we get the second mic on?</p><p>Yes. Yes. Yes. Sure. Sure. Yes. Sure. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Can we get the second mic on? Yes. Yes. Yes. Okay. There we go. Yay. So, the question was about the context that we provide to Copilot. Is there any way to have more control over it?</p><p>So, I assume that given the context length, the GitHub does some smart things underneath to kind make it smaller. So, obviously, it won't load all of the ten full pages of 2,000 lines of codes. And sometimes, it will give you subpar results because of that. Because, obviously, it doesn't know the exact method that I have in mind right now.</p><p>So, yeah. So, yeah. How about that? Actually, I'm going to let my colleague Harold answer that question because he has way more insight into this than I do. Second, there's also a talk tomorrow, I think, at the booth theater. We're going to go more into context as well. Okay.</p><p>So, there's an attach button. Once you open chat, you'll see you can actually reference file specifically. So, that's one way. Otherwise, the best way if you have code selected that you want to talk about, Copilot will include it as long as it fits into the context window. And that's kind of the best trick to keep in mind.</p><p>Just having codes like the same for inline chat as well, if you try that out. No. No, it hasn't. It just came in the last release. So, it's a sneak preview to tomorrow's show. But, otherwise, the add workspace, if you try that out, that's in the plan as well.</p><p>That will give you a more general overview of your project using kind of rack techniques and giving context related to the question. So, the better you ask your question to code patterns that are actually in your code base, the more it will find. It will show you. Always check the references that are in chat to see what context is being used.</p><p>So, that's kind of the main tips there. Yeah. And there's a question in the back. Yeah. So, to follow up on the question that I was just asked, one of the things that I've dealt with is the same problem where it seems like I don't have the full context being loaded in when I ask questions.</p><p>Just a question on sort of best practice. If I reference a method that exists in another file, is that method still being loaded in in the context? If I ask the question? Or do I need to explicitly have that file open in the tab? Yeah. You can use the attach button as well.</p><p>And we're working on that you can also attach symbols from your code base. So, that would be also possible. Yeah. So, as a best practice then, I always need to bring in the additional context. Yeah. It's not smart enough to figure out that I'm . Not yet. We're working on that as well.</p><p>So, there's definitely more inference coming. But as that can easily go wrong, given the broad lots of questions people can ask. So, the more explicit, if you know what you're talking about, just show it to co-pilot just like you would do with anybody else. That's at workspace. Yeah. So, at workspace is a rack pattern where we look at the whole code base.</p><p>Yeah. Yeah. Not to think. Yeah. Awesome. Awesome. Yeah. And if you continue to have more questions, yeah, please just raise your hands. Because this is great. And we have really good people here to answer those questions. Again, some additional, like, good best practices. Using good names are going .</p><p>We'll refer in a little bit to, we've set up discussions. Get up good discussions. And similar question came up this morning. So, we already have a set of links out there to dive into different resources to learn about exactly everything that co-pilot pulls in for context. So, from that, you've got some documentation and references to dig into to figure out in your situation, your situation, what's going to work best.</p><p>Yeah. And also attend Harold's talk tomorrow, too. Because that is a plug for you, Harold. Thank you very much. But, yeah, just quickly going ahead. This is linked in the main repo that I linked to before. This is the discussions tab. But this also goes directly there. So, if you want to -- if you have any questions during the workshop or if you want to see some of the previous questions that were asked before, we've got that stuff there as well.</p><p>So, again, just real quickly, some best practices in addition to offering context because that is an important thing. Using good names because co-pilot is designed to understand natural language. So, not being, you know, using overly genericized things, actually giving things names the way you would actually name a function or class is useful.</p><p>Spelling out variable names. You know, single letter variables and abbreviations can be ambiguous. So, if you can spell those out and be specific with them, you can get better results. Keeping functions functional is a good thing, too. And then the biggest thing, honestly, is being consistent. So, you know, the generated code is going to follow contextual patterns.</p><p>And so, again, to what Harold was mentioning before, if you select some code and you can offer that as context specifically in addition to attaching code, that new feature that just are recently shipped, that will help get better code, too. But, yeah, I mean, good code does beget better code.</p><p>And so, one of, I think, the misnomers about a lot of the coding assistance is that it's really primarily aimed at people who aren't skilled developers. And that's true that it can help people who don't have a strong development background. But I think what we found in the last three years is that the people who really get the most out of this are people who are developers day in and day out and who do work a lot with it because they are getting the most use cases because they can be the most consistent and are already, you know, committing, no pun intended, you know, many of the best practices.</p><p>Okay. One of the other things, too, and we're going to go through this because this is actually one of the fun parts of the workshop is that, you know, if at first you don't succeed, you know, iterate, try and try again. That can be frustrating sometimes, certainly. We've all been there where, you know, you get so close to getting something that you want and it's not quite there.</p><p>But at the same time, this can be a really great way of learning what sorts of prompts work well and what sort of prompts don't work as well. And so, for instance, in this image, I'll just describe this, you know, the initial comment said, you know, write a JavaScript function that finds the maximum value in an array.</p><p>And that's pretty broad. And so what the result is is also fairly broad and generic and might not work for what you're wanting to do. But the second prompt, by being more specific, by being more intentional, create a JavaScript function, find max that takes an array and returns the string with the maximum length.</p><p>Ensure the function works correctly for arrays that are embedded as well as arrays that contain both strings and numbers. That's going to give us a much better result. So these are things that, again, be more specific with your comments either in your code or with your queries to the chat agent.</p><p>It can work really well. And as you find things that work, you know, playing around with things and iterating is the best way of doing that. Another best practice is providing examples in your prompt to clarify what you want co-pilot to do and what you want it to take into consideration.</p><p>So you can, inside that prompt, you can put some sample code and say, hey, consider this example data. I want it to work correctly for things like this. That's something that the AI can understand very, very well. And then, again, keeping files open that are relevant to the requirements of your current file.</p><p>This is great. If you want to call in other things from your broader code base, you can use the @workspace agent. But keeping those relevant files open, that's going to help as well. And, again, like there are still going to be limitations by how much context everything can take in.</p><p>But this is getting better all the time. And then, you know, above all else, you know, just go with the flow. Be open to things sometimes working, sometimes things not working, and experimenting. Okay. All this is great. Got a, you know, quick kind of overview. When can I write code?</p><p>Well, we can start to do that right now. But the first thing we want to kind of talk through is getting set up. And so, again, this is going to be our main Ground Zero repo. So this is gh.io/aifair-workshop. So if you want to go to that repository now, that would be great.</p><p>Now, if you already have GitHub Copilot, if you already have access either on your personal account or through your company, that's all you're going to need to do. But if you don't have GitHub Copilot, we do have a coupon code that will give you access. And that is going to be at this URL.</p><p>That is actually linked on the repo. It's one of the very first things linked in the repo, both as a physical link as well as a QR code. But if you want to go ahead and scan this QR code, you can activate a coupon. This will give you a seven-day free trial of GitHub Copilot.</p><p>If you run into any problems getting this activated, what we did find last time, there are a couple of steps after you enter in your information where you will want to go ahead and make sure you click apply. I think like two or three times. this is what it looks like.</p><p>Basically, you're going to click on apply coupon. And then there's going to be another prompt to basically say activate GitHub Copilot. So we'll just give people some time here. And if you need to see one of the other screens, let me know about that. And if anybody has any problems getting their coupons redeemed, please feel free to raise your hands and we will send people out to do that.</p><p>This is that main repo, by the way. And you just click on this link, which is in the repo. This should take you directly to the page. I already have Copilot, so I don't need to do this. But where you just need to fill out your information and then you can go from there.</p><p>You can click on this link. This is the link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there.</p><p>You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there.</p><p>You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there.</p><p>You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there.</p><p>You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there.</p><p>You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. You can click on this link in there. And the next thing you're going to want to do after you've redeemed your coupon, and again, we'll make sure that we take time if anybody has any issues to get that set up, is in, again, this copilot hands-on repo, you're going to want to go ahead and fork this repo and make your own copy so that if you need to make any changes yourself, you can.</p><p>But the more important aspect here is that to run this workshop, we are going to use a feature called the GitHub Codespaces. Now, I am curious. How many people in the audience are familiar with GitHub Codespaces? Way less. Okay. Cool. So GitHub Codespaces are actually pretty awesome. Think about it as a virtual cloud environment.</p><p>So, basically, it is a pre-configured VM in the cloud that is set up specifically to have certain features that you've defined in your repository. So, there is a devcontainer.json file, which is basically kind of like a fancy Docker file, which will outline these are going to be the various dependencies and versions of different things that we have installed specific to this repository.</p><p>You can also designate what extensions in Visual Studio Code you want to have installed because we are going to be running that either in a web browser instance or you can open it up in the VS Code client as well. It also works with JetBrains, doesn't it? I believe.</p><p>Yeah. But we are going to be using VS Code today. And this is just a really great way of not having to bother with the process of setting up your environment. It instead can be configured exactly as it is in the repo rules. So, in this case, I have already forked my repo.</p><p>So, to get started, you want to just fork your own copy of the repository. I have already done this. I am not going to do this a second time, but I could. And just go ahead and create fork. And once you have done that, what we are going to do is click on this code button here.</p><p>And there is a drop down menu. And you will probably by default see local options. We are not going to do that. If you wanted to do this locally, you could. But it is actually going to be faster and a little bit easier with Codespaces. Go ahead and click on create Codespace on main.</p><p>And what this is going to do, this is going to take a few minutes. This is going to create a -- my case is wanting to open it in Visual Studio Code. I will actually fake that. Because it is my editor preferences usually. What it is going to do is it is going to spin up a VM.</p><p>You could, in another instance, Dave was talking about this before. And we could do a longer and different talk about how you could configure your Codespace to have more cores and more RAM and be faster. But for the purpose of this demo, just the default is just fine. Also, with every GitHub account, you get a certain amount of free Codespace minutes.</p><p>So that works great for us in these purposes too. And what this is doing right now is this is basically Visual Studio Code but in the browser. And so this is now spun up a browser instance of VS Code. And it is setting up a remote connection. And it is now building out this Codespace.</p><p>So it is installing all the various dependencies as defined in this dev container file. And it is getting them ready to run. So just for anybody who might be curious, because we will actually look at this a little bit later. But this is what that dev container file looks like.</p><p>And this is just a JSON file. And you can see basically that it is installing -- has certain settings set up for VS Code, including extensions for, you know, a code spell checker, the GitHub style markdown preview, GitHub copilot chat and GitHub copilot, as well as using an image from -- a Docker image that had -- it is configured for Python 3.11.</p><p>So this is now what the Codespace looks like. And so this is going to show how these things work. And what is interesting is it is going ahead and -- it is installing these extensions. And if I click on these extensions, you can see that right now nothing is installed.</p><p>But in a couple of minutes, or hopefully moments, we will start to see our copilot stuff come in, as well as our files. Yeah, here we go. So now it is going ahead and -- it is installing all of these extensions that we have access to. If you wanted to open this up in a local instance of VS Code, you could do that.</p><p>You would need to go ahead and manually install some of those extensions. That is the nice thing about doing it in the browser. And these are all the files that are from our repository. So this is basically identical to what we were looking at before. And actually, one thing you might want to do is -- in that main repository that we've been kind of focused on, at the very bottom, there is a labs document that says a copilot hands-on lab.</p><p>This is a great tip that Dave had, and it's in the documentation. But I'm going to go ahead and do this here, too. If you want to open that up in its own tab in your browser, and then maybe put it in its own window, like so. Using, you know, whatever window manager you use -- I'm on a Mac, so window management sucks.</p><p>If you want to tile it off to the left hand side of your screen, and you can make it bigger or smaller. Having this visible throughout the entire lab will be really, really good. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here.</p><p>So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. And that's one pro tip here. So that's one pro tip here. So that's one pro tip here.</p><p>So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's one pro tip here. So that's two pro tip here. So that's one pro tip here. So that's one pro tip here.</p><p>Did I lose? Again, I'm on a Mac, so window management sucks. I know, right? I know. We're finally going to get some tiling management. Hopefully. It's ridiculous. Okay. And then there's this. Here we go. Great. Okay. So I've now got these things configured that I have. This is my code space.</p><p>And then this is my hands on lab. And at this point, on the bottom lower corner of your code space is this copilot icon. And you can see that the status is ready. And that copilot chat has been enabled. Now, the chat window is here. This is the chat window.</p><p>And in this way, I can either talk directly to the code base. I can also use, there's an agent's feature where I can talk directly to, for instance, I can, if I have a GitHub Enterprise account, I can use @Github. And I can ask questions about my GitHub account.</p><p>These are primarily enterprise-related features. @Terminal is great. This will show you how to do things in your terminal. It's really, really good for FFMPEG commands and other things that you don't want to memorize. And @VSCode will ask you questions in general about VS code. It's also a really good way of kind of managing settings.</p><p>And then @Workspace, this is going to be asking questions in general about the workspace that we have open. So if you wanted to ask a more general question or continue to add context from what you're working on, that's an agent that you can use using the chat extension. I'm just going to put this back on screen for a minute.</p><p>These are the main links and resources. So the first thing is the co-pilot coupon. Then we have the main repo where everything is taking place. And then we have the labs readme. So these are kind of the main three things that we're working on. And if anybody has any problems getting access to anything, again, please raise your hand and we will do our best to assist you.</p><p>So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more.</p><p>So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more.</p><p>So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. So we're going to do a little bit more. Oh, yes. Sure. So we're going to do a little bit more.</p><p>Do you know, Harold, what's the shortcut on Mac OS to see the multiple, the various code suggestions? Yeah, I think that's not working for you. I think that's not working for you. All right. All right. All right. I lost connection to my code space. because the demo gods are with me today.</p><p>So this is reloading things. That's okay. That's the one pratfall of using the cloud code environments. Sometimes they can run afoul. But in general, they work pretty well. So again, I've got my labs hands-on stuff here. And we can just, once you've got this set up, we can go through the first lab.</p><p>And this is a fairly basic one. So if this is something that you are still setting up, don't worry about that. So the first thing that we're going to be doing is creating a new file. In this case, an index.js file. So yes, we are using JavaScript in this example.</p><p>Don't be afraid. All right. Now, you will notice that it says I can press, in my case, Command-I. And I think that it's Control-I on Windows, where I can ask Copilot for something. Or I can type a slash command. Now, these slash commands are ways that you can basically, I guess, simplify certain operations with Copilot.</p><p>So for instance, doing slash doc will add a documentation comment around a symbol. So that you can have a notice to go back and document your code more. Slash explain is actually really great. This is going to explain how your code in the active editor works. And it's pretty good.</p><p>There are some hallucinations, as there are with all LLMs. But it's pretty good. And it's improved quite a lot, even in just the last few months. That's one thing that I didn't mention earlier. We are updating our Copilot model fairly frequently. And we're rebasing on the various versions of GPT that come out from OpenAI all the time.</p><p>But we are also then fine-tuning on top of that. And so the Copilot that, if you first tried GitHub Copilot a year ago, you're going to get a different experience now. Not just from the tooling perspective, but also in terms of the results. Because the models are getting better all the time.</p><p>And we're actively committed to improving those models all the time. Slash fix will help you propose a fix if there's a problem in your code. A fix test failure will propose a fix for a failing test. Because writing tests is one of the best things of using a coding assistant.</p><p>And then again, slash tests will help you generate unit tests for your selected code block. So those are those options right there. All right. I've created this index.js file that I've got open. Now, I just need to add a comment in my code. If I can spell correctly with one hand.</p><p>And if I can spell function correctly, that would be even better. Although you will notice that it actually picks up on typos fairly well. Okay. So, I have this by hitting enter. The first thing that it's giving me is something pretty minor. And it is very similar to what our example says.</p><p>Now, I can hit tab and I can accept this. And this, by the way, this text right here. Let's see if I can zoom in on this more. No, it is not letting me zoom while I'm in full screen. Great. Love you, Apple. Okay. When you see this gray text here, this is what's known as ghost text.</p><p>And this is something that Harold can actually talk more about. If you want to maybe explain the different types of text and whatnot that people see when they're using co-pilot. Yeah. So, I think, yeah. We start with completions. And that's the, in the flow, you don't even think about it mode of co-pilot.</p><p>It always works. And when you get really used to it, you kind of ignore it when you're in the flow, when you're coding, when you know what you want. And then once you want co-pilot to kick in, you just wait a bit. It kicks in. You might even try different suggestions.</p><p>You kind of flip through. Maybe you accept it word by word. And let's see what, how else it continues. But it's really, you don't even think about it. And you notice that when you sit in the airplane and you stare at your screen for a minute, that you're in airplane mode and co-pilot doesn't give you anything.</p><p>So, that's when you know you build a habit. Then the other one is further next. It's inline chat. And that's where you, you can open up a little natural language input over your code. And that's where you can apply AI directly to your code. And that's, in chat with you, you keep kind of having long winded conversations about code.</p><p>And that's where inline chat is much more powerful in giving your way, quickly iterate on code to generate and edit both code. And then lastly, chat panel is kind of the OG idea of ChatGPT in your VS code. And editor chatting with context, being more in the exploration mode where you want to have conversations.</p><p>Where you want to treat it like an actual AI pair programmer, which can challenge you, which can critique your code. Where you can go a little bit more back and forth and explore a solution. So, these are kind of the three modes and when you want to use them.</p><p>Thank you, Harold. You're the best. Thank you. I was just curious to know if there is a difference, you know, between the ghost text that generated here. So, just doing it, you know, inline in the editor. Versus if I was to have that function and then use the control I to say, write me the rest of this function.</p><p>Like, is there a difference in how the context being provided to the underlying model? Is there any, like, reason we should use comments to get it to generate versus control I or chat in terms of context? Yeah. My personal take is you will see when code was generated with copilot ghost text because it has all these not very purposeful comments written in that very much describe the code below.</p><p>Which are not typical code comments. You want to describe your reasoning and your thinking behind it, but that's not how you prompt craft with get a copilot. So, inline chat allows you to pull that prompt crafting into the inline chat and just get the code out and be more specific and elaborate in how you can describe it in that inline chat prompt.</p><p>So, you're less iterating on the comment, but you iterate in a natural language input, which is sometimes more intuitive. So, you will find if you like writing long-winded comments and tweak on the comments and then hit enter again and see what copilot provides, that's basically a much faster, more direct flow with inline chat.</p><p>Yeah, that makes a heap of sense from a user experience perspective. Just also keen to understand, is there a difference in terms of how that has been provided to the model between the two different ways of doing it? Yeah, definitely. The chat, the ghost text is more grounded in the neighboring tabs.</p><p>It has a much smaller context window because it needs to write it much quicker. And then inline chat has way more freedom to include other context dependency documentation. So, depending on which commands you're executing, it will do more work upfront to get context and more context. Awesome, thank you.</p><p>Yeah. And there's a question in the back. Okay. Fantastic. Awesome. Okay. And actually, just to show you here, there are these sparkles that come up where you will see certain things. And you can actually choose options to fix using copilot or explain using copilot. In this place, I'm just going to show explaining.</p><p>And this is just going to say, okay, this is the reference it's using. And this is just giving me an explanation of the non-existent code that we basically have already created at this point. So, going further through the lab, this is what it's going to generate now. Now, when you do create -- when you do get your code suggestion, if you are not happy with what that you're getting, there is -- you will sometimes be able to get other options for other code suggestions.</p><p>But just kind of going through this -- let me go on. Okay. So, when I press control, enter, and it is control on both Mac and Windows, this is going to give me a list of suggestions here. And so, I have seven -- I have -- wow -- eight different suggestions that all could be very, very different.</p><p>Because, again, I gave it a really, really generic prompt. Which goes to one of our first lessons. We were kind of talking about be specific. Be specific with what you want to achieve. Because I have eight different suggestions that all vary in what they're wanting to do. But this is one way that you can actually choose different suggestions.</p><p>So, in this case, if I wanted to accept one of these suggestions, I could do that. And it's going to put those lines in place. Now, we are going to go ahead and not accept any of these. Instead, what I'm going to do is I'm going to -- -- sorry.</p><p>Typing with one hand is difficult. And you can see the ghost text coming up as I'm typing things out. So, it's actually going ahead and putting further things here. I don't actually want all of this. But I can just have this aspect here. And this is going to go ahead and potentially complete this code for me.</p><p>And if I wanted to select that, I just press the tab key. And that's where this will come. And so, basically, this is just our first lab. This is just going through how this works. Harold, if you wanted to come up again and just go through lab two, which I think I already showed off the bulk of it.</p><p>But this might be useful for the folks. Okay. Harold, I did adjust it. So, it should work. Okay. That's not my point. Good. Let's go. Lab two. So, I think I showed it before. So, once you're in ghost text mode, one good way I just showed over here is to hover the ghost text.</p><p>Everybody, make some ghost text and hover it. That little widget gives you the right keyboard shortcuts and a little three-dot menu on the end. I'm going to demo it here. This is what you're showing. How are you talking? I wish I had larger hands. Okay. So, the classic is single line completions.</p><p>And this, especially in function buddies, copilot will go into multi-line, which are easier to hover to. In the top right, you now see the little widget which has one slash three and an accept and accept word. So, remember the accept word shortcut. That's a good keyboard shortcut to impress into your brain.</p><p>Because that lets you get out of this mode where you have to accept the whole function buddy that copilot hallucinated for you. But start with the initial one that has the highest confidence. Like, yeah, that's the part I want. And then the rest I can work out with copilot and fill in the rest as I figure out what I actually want.</p><p>But that's one way. So, hitting the command next, the command right. That's one way. If you want to do the full line, there's also... Come on. What is this? What is this? Why is this so far away? Okay. So, accept line is also here. If you want to apply a shortcut to that, it's VS code, so go ahead.</p><p>And then, but the other one is the open completions panel, which gives you this idea of more suggestions. So, especially in multi-line, there's a lot more freedom for the AI to explore how it could solve this problem that it thinks you're having. So, these are usually a good way to understand your options.</p><p>And, there's nothing else missing. Then we have Inland Chat. So, the other way you can now solve this, I can just take this out. So, let's do a, just delete this and then write, that's where I mentioned the panel, Inland Chat versus Panel Chat. Write a URL parser function.</p><p>And, that's the mode where Inland Chat is used to generate code. Now, you don't start with a function body. You just give it an idea of what you want to get out of it. And, you just get a function that will parse the thing. And, actually, I did this before with Copilot completions and it got to the same conclusion.</p><p>So, you see how you can get to the same end result with different paths. I could also give feedback now. Maybe this way, oh, it generated an A element. Maybe I want to use a library for that. So, make sure you use that follow-up field to tweak it. Don't just accept the first suggestion as with all AIs.</p><p>They can get things wrong. So, make sure you keep iterating. And then, function-wise, now when I accept this, maybe I make a mistake. Classic mistake. Just typing. Parse. So, you can go every time if you have a linting error. And, those can come from extensions. Those are not just built-in VS code things.</p><p>If you have X, for example, you also can get accessibility squiggles. You can go into quick fix. And, now, you can see... Actually, for this one, not because it's a code spell one. There's not too much linting stuff in this project. I only get code spell errors. Okay. Let's show you the other way.</p><p>If you have code you want to fix, you can do the thing I just showed you in the hover. Or, you can just do right click. How do you right click here? Let's see. Okay. You can go into here and then modify using co-pilot. And, that's where you also open the inline chat and just hit fix.</p><p>And, if I actually do insert the mistake, which I haven't got to have with parses, it will find it. Fix will automatically look at error messages. It will look at potential issues with the code. And, it's generally prompted to fix any other issues it finds. So, in this case, even though it wasn't a general actual issue, it found that parses is misspelled.</p><p>And, it gives me the change. If you want to see a better view in inline chat to see the change, you can also toggle changes down here. And, see the actual line versus line comparison. This is just optimized right now for space. That's what makes inline chat very compact and easy to use in all kind of window setups.</p><p>So, accept this. And, that's the quickest way to get rid of squiggle lines with AI. I think this concludes lab two. Should I just move on? Should I just move on? Okay. Going with lab three is explain. Again, same thing. Just open inline chat and do slash explain. So, it's one of the many commands we provide.</p><p>Oftentimes, with AI, you end up in a very prompt, heavy flow where you keep describing the same thing. Please explain this code to me or please explain what this code does. So, these shortcuts allow you to not be overly prescriptive for very common tasks like fixing bugs and explaining code.</p><p>So, slash explain in this case will not just look at the current code, but will also look at the dependencies you're using. The symbol definitions, the type definitions, the documentation, and all hand is all over to VS Code and Copile Chat to make the best sense of it. In this case, now we're using this tiny view to just give you an overview of the code and how it works.</p><p>Like, how do I get a URL out of a link element in this case? But you can also now open this up in the chat panel on the left to give it some more breathing room and make it easier to read. What's important? All these answers are also linked nicely.</p><p>So, we do a lot of effort in making this code in any explanations really shine and easy to interact with. So, you can easily navigate and click around in these answers as well. That's a recent improvement that landed. If you actually generate code in the side panel, it will also have linked elements and have IntelliSense and you get all the benefits of getting really readable code.</p><p>There's explanations. If you want to have a custom explanation though, you can still do that. So, if you want to have a command I slash explain classic prompting like I am five. It's not really doing the five thing. I'm a very smart five-year-old. So, you can also like, yes, if you want to, yes, I can do it.</p><p>If you want to actually switch that, if you look for co-pilot settings, switch it to different languages, you can actually switch the language of co-pilot. So, the local, if you go into your settings in VS Code, there's a local override locale and you can switch it to different locales.</p><p>So, by default, co-pilot will be using a language that you pick it in. So, you can mix up languages. My original language is German. I also speak Spanish and French. So, when I don't know a word when I speak to my wife, we just switch languages. Co-pilot will be as well understanding you if you just high level describe it.</p><p>So, in some cases, it's good, like, later on, we'll show you at workspace. It's good to be hitting the right words of concepts that you expect in your code base. So, if you know the word, make sure if you know reference, be specific. But, otherwise, it's interesting also to be vague and add some ambiguity to let co-pilot figure out and give you some ideas on how you can solve a problem.</p><p>Okay, so, this should get you through the inline chat parts and into the chat panel, which we already went to. Okay, chat panel is trick number one. Remember this button up here to open a new chat. The same shortcut is for clearing a terminal. So, it's control L. Very important.</p><p>Very fun. Quick way to start a new conversation. Yeah. Because co-pilot will look at your history every time you answer. Just like in chat GPT or in beginnings of Bing search AI, where it fell in love with reporters after 50 back and forth conversation was stuck in a very romantic mode.</p><p>But, co-pilot can also get stuck on concepts and anything you talked, discussed before. So, if you start a new conversation, reflect that by actually opening a new chat. You can still switch back and forth in the history, but make sure you begin new conversations actually in a new conversation.</p><p>Inline chat does not have history right now. It has, remember that you can, just close this, you can go up and down to get to old history items. So, use that. But, I thought we do store these in history now. So, I have to get back to you on that.</p><p>I know they end up somewhere. But, it's all locally kept. So, none of these things are synchronized to GitHub. It's all just kept in a place where VS Code keeps its data. It's in your local profile, not anywhere online. So, a quick trick here, if you want to see everything that co-pilot can do, it's slash help.</p><p>And, that's providing you, anytime you forget which commands and what they do, and why there's so many, that's the best way. So, that's giving you all, like, learning commands for common things like slash fix, slash explain. Play around with the more edge casey ones like slash new, which allows you to bootstrap a whole new project.</p><p>But, it's also the context variables. But, all of these are also auto-completed. As soon as you hit slash or hash, you will see those as well as you play around. Oh, he stepped out. There was a question about how to use the new notebook command there. And, so, in the discussions on the DevOps 30 version of the repo, I added a little discussion that says, "How do you use that command?" And, it points to, you have to feed it some more context.</p><p>I'm not a Jupyter Notebooks guy, but it's like, you know, using this data set, I want to generate this kind of a thing or whatever. So, yeah, some of those commands, you'll have to get a little bit familiar with exactly how to follow up that command. What do you need to feed it?</p><p>Yeah. And, things like that. So, they can be very powerful once you know how to use them. Yeah. Who's using notebooks in VS Code? Awesome. Yeah. It's definitely, if you do data science and not using notebooks, it's a great place to move from data analysis to actually writing code and shipping it.</p><p>So, and I use it tomorrow in my demo as well. Unintended. Product placement here. OK. Generate tests. I think nothing more to add here. Just play around with it. It's a great way. Again, it shows off ghost tags and slash tests. If you use InlandShed, for example, another case where InlandShed does a bit more work, it actually tries to detect which testing framework you're using, which folder they should go in.</p><p>But it also, if you use ghost tags and you have the file open implementation that you're trying to test and you just start writing tests, it also gives you this magical experience of just understanding the implementation and writing tests for it. So, depending on which flow you like most, those are interesting places to explore it.</p><p>So, heck away and ask questions. Yeah, hit the button. I was just going to say at this point, you can just kind of go through the labs and play around. And if you have any questions, raise your hand. Also, use the discussions. Let me go back to that slide.</p><p>Thank you so much, Harold. Go back to the slide where all the links are. We'll be happy to help you out. Please continue working on your labs if you want to do that. But I just wanted to throw this out there. If anybody has any kind of general co-pilot questions that you want to ask us, especially Harold, frankly, because he actually works on the VS Code team and works on a lot of this stuff, this is a great opportunity.</p><p>So, if anybody has any questions out there, we'd love to see if we can answer them. So, the question was, what model are we using? That's my favorite question. Yes. That's my favorite question. I just closed the issue on GPT-4 Turbo. It's no public announcement. But I closed the issue on GPT-4 Turbo.</p><p>It's rolled out. So, you can ask, actually, in the chat panel, you can ask it which model it's using. And if you're on the latest version, if not, I'll find you. It will say GPT-4 Turbo. Because I prompted it so. See? If you tell GPT-4 Turbo it's using 4 Turbo.</p><p>If you tell it used GPT-5, it will also tell you that it's using GPT-5. So, we rolled out four O's and it works as well. Inline chat is still 3.5 because 4 is too slow so far. So, that's interesting. Maybe talk more about that. Yes. So, even Turbo, even though you just closed the issue on Turbo, 4 is still too slow for inline chat.</p><p>Yeah. So, we're constantly running this multi-model flow. Especially because we want to always balance the performance developer experience where it can stay in the flow. And that's why, for completions, we're using 3.5, a very small context window to stay under these 250 milliseconds instant ghost text popping up for most users.</p><p>So, on average. And then, for inline chat, 3.5, when we ran experiments, when we ran offline evals, and the whole systems we have in place to constantly assess new models, we actually noticed that there's only a slight lift in quality and only in some languages for inline chat going to GPT-4.</p><p>But then, when we ran an experiment, a lot of people churned out because the time-to-first token is still a lot slower. So, we're always taking this approach of balancing this performance and quality aspect and what provides the best experience in the right job. So, that's why, yeah. So, every panel chat question you ask, right now, did get GPT-4.</p><p>Now, it gets GPT-4 Turbo. And inline chat, just for that performance aspect, still uses 3.5. Very cool. All right. Now, when is GPT-4 coming? It's coming. It's coming. I had to ask. Follow the issue. I'll close it. With all of the work that's being done by Microsoft, and many others besides, with small models like PHY-3, et cetera, are you looking at testing those and implementing them?</p><p>Yes, we're testing those. Yeah. Interesting. There's actually, if you look at the PHY-3 cookbook on GitHub, there's a chat extension sample for VS Code to use PHY-3 locally in chat. Awesome. Out of interest, are you able to share what sorts of things you look at when you test new models within the GitHub Copilot product?</p><p>Like what metrics you might look at? The behind the curtain. So we have, so most of our offline evals are code-based. So if you think /fix, /docs, and we have a smaller test set that runs generally on each change, code change, where we test specific patterns if they improve or regress.</p><p>And that's mostly based on issues we've seen in the past, like, oh, it didn't get this pattern right, or it didn't look at specific files. So we're trying to just assess the general competence and what we expected. And then there's larger offline evals where we run it across different repos and try to understand how well these /fix and /docs work.</p><p>So these are not public benchmarks, but they're trying to assess basically against open source code that we can get to run on GitHub. So if you have a repo on GitHub, make sure the test runs. Yeah. So, yeah, that's basically the baseline. They never pass 100%. There's always, like, we're trying to get the pass rate up, but it's typical offline eval of we're trying to get better over time.</p><p>And interestingly, a lot of the big model updates, they don't necessarily 10x those metrics. There's always-- some languages get better, some languages get worse. So it's really-- and they always require prompt changes. So don't run after the latest model just because somebody told you. So there's always the nuance of actually optimizing for the models and then testing out each model separately.</p><p>I didn't quite catch that. Did you mean that we can test the PHY model locally? Did I understand that right? Check. Search GitHub for the PHY3 cookbook. And there's an example there, too. Yeah. Okay. Thank you. So we're going to get to the ground. VS Code has an-- has extension APIs.</p><p>Many, many thereof. So all the cool extensions you're running are in VS Code or using these extension APIs to provide the functionality. We have extension APIs for co-pilot chat. You can actually add your own participant and you can actually provide your own models in the chat as well. So these are two extensions.</p><p>There's also a blog post that just came out yesterday. I think that you can check out on the VS Code blog that talks more about extensions. So if you want to play around, a lot of companies do this internally for their own developers, just bringing their own developer experience and knowledge into co-pilot chat and some custom flows.</p><p>And other companies like Stripe and Postgres and other projects are bringing these chat participants via extensions into co-pilot chat for some custom workflows for all the developers that they're serving. But that kind of allows you to plug into the existing, I don't know, experience. Both. Is it possible or will it be possible to replace the connection to whatever GPT model would be used at the time to be able to use the co-pilot at the companies which currently do not allow it because that would mean sharing your code as you go with an external provider?</p><p>Currently not. So that's not a replacement of the underlying model, but the 5.3 cookbook shows you how to do it in a more sandboxed way to replace parts. Yeah, I understand. But is there any chance that it's coming anytime in the future or something like that? It's on the list of things we're looking into.</p><p>Yeah. So we're looking at models. All right. Thank you. So the next one is, is there any intended use difference between the Control-I inline mode and sorry, the Command-I and Command-Shift-I modes? Yeah. Yeah. Right? And then you also have Command-Shift-I, right? Oh, Quick Chat. Yeah. If you haven't tried it yet, Control-A is Quick Chat.</p><p>That's the idea is, so we're exploring different just form factors for chat and that's more of a quick question overlay. So that's kind of the idea. You don't want to go into panel chat, especially on a small viewport. Yeah, all right, all right. It's mostly your UX preference. But this will have the conversation, the history conversation then?</p><p>Yeah. All right. And does it also have a new chat then? You would just close it. You would just, yeah. Oh, right. So it holds the conversation as long as-- Right. And Control-L works, works, works as well. You can just empty it. Perfect. Yeah. And let me just ask that question so everybody can hear.</p><p>The question was basically, is there a way to take advantage of, I guess, like local documentation and agents that might exist in a code base that might be more updated than what the model itself is. Is that getting close? Yeah, but not for local documentation, but rather the libraries.</p><p>Right. So for libraries, so it's not locally, but for other libraries, but for things that just the model hasn't been updated for because things are changing all the time. Yeah. So Copilot for Enterprise has knowledge bases. I think there's some links in the training in the end to Copilot for Enterprise and some of that documentation.</p><p>So knowledge bases allow you to import a repository with Markdown as a grounding and then reference that in your question, especially on GitHub.com, but also in the add GitHub agent in BIOS code. And we're trying to make that more accessible and naturally to apply to code. But that's one way that you could bring in external documentation.</p><p>Just have it, have it indexed, have it referenced. Then you curate it in these knowledge bases as something you can reference, like using our LM stack. And then you reference the combination of different projects that you're depending on. And you can update them as needed. There's more work there.</p><p>I think to bring knowledge bases to a more usable and easier to update flow. So that's something we're also looking at. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. So the question is, do you, I guess, two questions. One is, is there some knowledge retrieval we can do with this?</p><p>Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. So the question is, do you, I guess two questions. So one is, is there some knowledge retrieval we can do with these chat participants? The chat prisons can provide context variables. So if there is specific context that those participants want to provide, like their API documentation, I guess two questions.</p><p>So one is, is there some knowledge retrieval we can do with these chat participants? Chat participants can provide context variables. So if there is specific context that those participants want to provide, like their API documentation, or more dynamic things, like which actual databases are used in your product and what is their schema, they can be exposed as context variables.</p><p>So you can combine them with other contexts. And we actually, yeah, we're just looking into more automatic inference, how to get this context in. So you don't have to know. Rather than also providing the context of other, like, I would say. So, like, the . Yeah. So you're basically saying, if you ask the question and Copilot didn't know it, but it was about something that you had an extension installed with or an agent installed with, it could automatically then send that to the agent.</p><p>Yeah, that's actually on the expiration for within the next six months. So we're looking-- because we see when people discover extensions, it's like, oh, this is amazing. I want to use this every day. And then they keep forgetting about it, or they never discovered it upfront. So that's a big problem we saw in our user testing.</p><p>So the automatic inference is a big part of just Copilot being aware of what you have installed, what you're working on, and then calling the extensions as needed. so that's coming. Thank you so much, Harold, for this impromptu Q&A that I just, like, threw at you at the last minute.</p><p>We really appreciate that. No, no, no, no, no, no, no. You were fantastic. I just-- this was the idea that the Dave and I had. We were like, well, while we have, you know, a member of the product team. And Harold mentioned the blog post from Visual Studio Code that I didn't even realize was out yesterday.</p><p>So on the main repo link, there's a githubcopilotresources.md file. And I already had a reference to the Copilot Extensions blog post that came out in conjunction with Microsoft Build. So in that section, I said, "And see also this Visual Studio Code." So we're, you know, we're being as dynamic as we can here.</p><p>There you go. Yeah. So that blog post is now-- you don't have to remember this URL. Just the repo your-- or the devops resources-- or sorry, the devops version of the GitHub Copilot Lab. Check out the resources file there. Also, check out the discussions. I wasn't able to find what Harold was referring to with the cookbook 5.3 or whatever, but we can throw-- oh, awesome, awesome.</p><p>So, yeah, when you go to those discussions, you know, we have those links and things there. We'll even-- I'll keep an eye on those discussions for a few days. So if something else comes up, feel free to throw a question or a comment out there. And since we are-- we got about a little over 20 minutes left here, I will put in a plug, too.</p><p>There's a couple of survey questions there. Please let us know what you thought about this lab. Please let us know what you thought about GitHub Copilot in general. And, you know, add comments on either one of those if you would like to. Anybody else have any other questions or anything?</p><p>Is GitHub Copilot Workspace going to replace Copilot chat or does it have any other purpose? Yeah, I think from my perspective, yeah, I'll let Harold talk about it. But I think there's, you know, a purpose for Copilot Workspace and a purpose for Copilot chat and everything. I'll let Harold address it.</p><p>So who has seen Copilot Workspace? Okay. Let me describe you in great stroke. So Copilot Workspace is a standalone experience where you can start from a repository or a PR or other places. There's great demos, there's great videos out there. There's an early adopter audience already playing around and getting really excited about these flows.</p><p>But you can start with a task. It breaks it down into a plan. And it allows you to make cross-repo, cross-file edits and implement features, fix bugs that are more complicated. But it's a great end-to-end flow that keeps the developer in control at every step and allows you to give feedback and correct the plan and correct the plan items and stay in the loop as the AI proposes changes.</p><p>The outcome is that you end up with a div and the div can even run the actions and the repository. You have a terminal where you can run through your build step and have other ways. So there's a lot of pieces built in that suddenly look like an editor.</p><p>So my hopes, this is eventually something that if you ask a more complicated question in chat, it will just use a workspace-like flow to apply these edits. But there's also other explorations that we're applying. How can this look like in PR reviews? What's the best workspace flow for the many things developers hit in a day-to-day?</p><p>How can AI help with reviewing a PR? So all of these things are where a more specialized AI breaks down a task into multiple steps and keeping you in the loop and in control. So I don't think it's a replacement. It's a necessary flow. It's something you can already do in GitHub Copilot chat.</p><p>But just start with asking it about a plan and then work with Copilot through a plan. So with proper prompt crafting, you can get pretty close to that experience and stay in control. I think that's the main one there. So I think that's the main thing that we can do in the loop and do it in the loop and do it in the loop and do it in the loop.</p><p>So I think that's the main thing that we can do in the loop and do it in the loop. So I think that's the main thing that's the main thing that we can do in the loop and do it in the loop and do it in the loop. So the question was Copilot Enterprise has a couple of really cool features such as PR summaries or knowledge bases.</p><p>I'm keen to understand in terms of those features living in VS or like in the code editor, so like I'm doing a PR. I'd like to immediately summarize what the PR is about. Is there some sort of outlook on what that's going to look like? So there is a way, it's a different experience, but there is a way to summarize PRs in VS Code right now.</p><p>So there is a way to do that that is separate from the GitHub Enterprise feature. But this is actually, I think, and you can talk more about this. This is something we're trying to kind of balance the two experiences between, you know, how things work, you know, kind of on GitHub.com and then the way that people are using their editor.</p><p>You want to say more about that in the process. Some hot takes. Now, so I think in the beginning you saw definitely more Copilot experience sprouting up under the Copilot X moment that happened at Universe. So just people are trying out, like, what does AI mean in developer flow?</p><p>So PR descriptions went out there and then this new tier SKU for Copilot for Enterprise. So I think a lot of it is like, what is a value add that we can bring to enterprises that work for, if you have a thousand repos and team to manage 2,000 developers, so the different use cases that are not as simplistic of like a typical day in the developer working on a project.</p><p>So there's a lot of that thinking right now happening, like how to best separate those. I think a lot of times we are focusing on bringing everything that's happening on GitHub.com, making sure it can shift left into the developer experience in the editor as well. So as you can see, we already have like in code VS code, you already get the little magical AI buttons that can write you a commit message that can write you a simple PR description.</p><p>But if you look at PR descriptions on GitHub, they go to way more lengths of looking at more files. So as you go on to see those features just become more powerful and more grounded and more suited to the needs of enterprise. So I think that's that's the best way to think about those.</p><p>And yes, there are some features that are locked in right now because they're still they need more enterprise feedback and they need to scale to enterprise needs. But otherwise, yeah, there's definitely a shift shift left of these features happening so they become available in the editor as we figure out how to best provide them in the editor.</p><p>But no, so if you haven't tried out in the code space, you see the PR extension is automatically installed in the code space. So you can manage your issues, your PRs, if you can create PRs. So that's also what what you can do in code spaces. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. We'll see you next time.</p></div></div></body></html>