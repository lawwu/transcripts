<html><head><title>Stanford CS224N NLP with Deep Learning | 2023 | PyTorch Tutorial,  Drew Kaul</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | 2023 | PyTorch Tutorial,  Drew Kaul</h2><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg"><img src="https://i.ytimg.com/vi/Uv0AIRr3ptg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Uv0AIRr3ptg.html">Whisper Transcript</a> | <a href="./transcript_Uv0AIRr3ptg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">[no audio]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=5" target="_blank">00:00:05.500</a></span> | <span class="t">And so today I kind of just want to cover the fundamentals of PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=10" target="_blank">00:00:10.400</a></span> | <span class="t">really just kind of see what are the similarities between PyTorch and NumPy and Python,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=15" target="_blank">00:00:15.700</a></span> | <span class="t">which you guys are used to at this point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=18" target="_blank">00:00:18.700</a></span> | <span class="t">and see how we can build up a lot of the building blocks that we'll need in order to define more complex models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=25" target="_blank">00:00:25.500</a></span> | <span class="t">So specifically we're going to talk today about tensors, what are tensor objects, how do we manipulate them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=32" target="_blank">00:00:32.300</a></span> | <span class="t">what is AutoGrad, how PyTorch helps us compute different gradients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=37" target="_blank">00:00:37.500</a></span> | <span class="t">and finally how we actually do optimization and how we write the training loop for our neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=42" target="_blank">00:00:42.500</a></span> | <span class="t">And if we have time at the end, then we'll try and go through a bit of a demo to kind of put everything together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=48" target="_blank">00:00:48.800</a></span> | <span class="t">and see how everything comes together when you want to solve an actual NLP task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=56" target="_blank">00:00:56.200</a></span> | <span class="t">All right, so let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=58" target="_blank">00:00:58.800</a></span> | <span class="t">So if you go to the course website, there is a notebook,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=62" target="_blank">00:01:02.700</a></span> | <span class="t">and you can just make a copy of this Colab notebook and then just run the cells as we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=68" target="_blank">00:01:08.800</a></span> | <span class="t">And so to start, today we're talking about PyTorch, like I said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=73" target="_blank">00:01:13.400</a></span> | <span class="t">It's a deep learning framework that really does two main things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=76" target="_blank">00:01:16.900</a></span> | <span class="t">One is it makes it very easy to author and manipulate tensors and make use of your GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=82" target="_blank">00:01:22.800</a></span> | <span class="t">so that you can actually leverage a lot of that capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=85" target="_blank">00:01:25.800</a></span> | <span class="t">And two is it makes the process of authoring neural networks much simpler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=91" target="_blank">00:01:31.300</a></span> | <span class="t">You can now use different building blocks like linear layers and different loss functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=96" target="_blank">00:01:36.300</a></span> | <span class="t">and compose them in different ways in order to author the types of models that you need for your specific use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=103" target="_blank">00:01:43.300</a></span> | <span class="t">And so PyTorch is one of the two main frameworks along with TensorFlow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=108" target="_blank">00:01:48.200</a></span> | <span class="t">In this class, we'll focus on PyTorch, but they're quite similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=111" target="_blank">00:01:51.800</a></span> | <span class="t">And so we'll start by importing Torch, and we'll import the neural network module, which is Torch.nn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=118" target="_blank">00:01:58.800</a></span> | <span class="t">And for this first part of the tutorial, I want to talk a bit about tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=124" target="_blank">00:02:04.300</a></span> | <span class="t">One thing that you guys are all familiar with now is NumPy arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=129" target="_blank">00:02:09.800</a></span> | <span class="t">And so pretty much you can think about tensors as the equivalent in PyTorch to NumPy arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=136" target="_blank">00:02:16.800</a></span> | <span class="t">They're essentially multi-dimensional arrays that you can manipulate in different ways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=141" target="_blank">00:02:21.800</a></span> | <span class="t">and you'll essentially use them to represent your data, to be able to actually manipulate it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=148" target="_blank">00:02:28.300</a></span> | <span class="t">and perform all the different matrix operations that underlie your neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=153" target="_blank">00:02:33.800</a></span> | <span class="t">And so in this case, for example, if we're thinking of an image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=158" target="_blank">00:02:38.800</a></span> | <span class="t">one way you can think about it in terms of a tensor is it's a 256 by 256 tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=164" target="_blank">00:02:44.800</a></span> | <span class="t">where it has a width of 256 pixels and a height of 256 pixels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=170" target="_blank">00:02:50.300</a></span> | <span class="t">And for instance, if we have a batch of images, and those images contain three channels, like red, green, and blue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=177" target="_blank">00:02:57.300</a></span> | <span class="t">then we might have a four-dimensional tensor, which is the batch size by the number of channels, by the width and the height.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=184" target="_blank">00:03:04.800</a></span> | <span class="t">And so everything we're going to see today is all going to be represented as tensors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=188" target="_blank">00:03:08.800</a></span> | <span class="t">which you can just think of as multi-dimensional arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=192" target="_blank">00:03:12.800</a></span> | <span class="t">And so to kind of get some intuition about this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=195" target="_blank">00:03:15.800</a></span> | <span class="t">we're going to spend a little bit of time going through essentially lists of lists,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=200" target="_blank">00:03:20.800</a></span> | <span class="t">and how we can convert them into tensors, and how we can manipulate them with different operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=206" target="_blank">00:03:26.800</a></span> | <span class="t">So to start off with, we just have a simple list of lists that you're all familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=212" target="_blank">00:03:32.800</a></span> | <span class="t">In this case, it's a two-by-three list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=215" target="_blank">00:03:35.800</a></span> | <span class="t">And now we want to create a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=218" target="_blank">00:03:38.800</a></span> | <span class="t">And so here, the way we'll create this tensor is by doing torch dot tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=224" target="_blank">00:03:44.800</a></span> | <span class="t">and then essentially writing the same syntax that we had before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=229" target="_blank">00:03:49.800</a></span> | <span class="t">Just write out the list of lists that represents that particular tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=235" target="_blank">00:03:55.800</a></span> | <span class="t">And so in this case, we get back a tensor object, which is the same shape and contains the same data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=242" target="_blank">00:04:02.800</a></span> | <span class="t">And so now, the second thing with a tensor is that it contains a data type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=246" target="_blank">00:04:06.800</a></span> | <span class="t">So there's different data types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=248" target="_blank">00:04:08.800</a></span> | <span class="t">For instance, there are different varying level of precision floating point numbers that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=253" target="_blank">00:04:13.800</a></span> | <span class="t">You can have integers. You can have different data types that actually populate your tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=258" target="_blank">00:04:18.800</a></span> | <span class="t">And so by default, I believe this will be float 32,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=261" target="_blank">00:04:21.800</a></span> | <span class="t">but you can explicitly specify which data type your tensor is by passing in the dtype argument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=268" target="_blank">00:04:28.800</a></span> | <span class="t">And so we see here now, even though we wrote in a bunch of integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=273" target="_blank">00:04:33.800</a></span> | <span class="t">they have a decimal point, which indicates that they're floating point numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=277" target="_blank">00:04:37.800</a></span> | <span class="t">And so same thing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=279" target="_blank">00:04:39.800</a></span> | <span class="t">We can create another tensor, in this case with data type float 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=285" target="_blank">00:04:45.800</a></span> | <span class="t">And in this third example, you see that we create another tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=291" target="_blank">00:04:51.800</a></span> | <span class="t">We don't actually specify the data type, but PyTorch essentially implicitly takes the data type to be floating point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=299" target="_blank">00:04:59.800</a></span> | <span class="t">since we actually passed in a floating point number into this tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=303" target="_blank">00:05:03.800</a></span> | <span class="t">So pretty much at a high level, tensors are like multi-dimensional arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=309" target="_blank">00:05:09.800</a></span> | <span class="t">We can specify the data type for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=311" target="_blank">00:05:11.800</a></span> | <span class="t">We can populate them just like NumPy arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=313" target="_blank">00:05:13.800</a></span> | <span class="t">Okay, so now, great, we know how to create tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=317" target="_blank">00:05:17.800</a></span> | <span class="t">We know that ultimately everything that we work with, all the data we have is going to be expressed as tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=323" target="_blank">00:05:23.800</a></span> | <span class="t">Now the question is, what are the functions that we have to manipulate them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=327" target="_blank">00:05:27.800</a></span> | <span class="t">And so we have some basic utilities that can help us instantiate tensors easily,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=332" target="_blank">00:05:32.800</a></span> | <span class="t">specifically torch.0s and torch.1s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=336" target="_blank">00:05:36.800</a></span> | <span class="t">These are two ways to create tensors of a particular shape, in this case tensors of all zeros or tensors of all ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=345" target="_blank">00:05:45.800</a></span> | <span class="t">And you'll see that this will be very helpful when you do your homeworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=350" target="_blank">00:05:50.800</a></span> | <span class="t">Typically, you'll want to just need to create a bunch of a zero matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=354" target="_blank">00:05:54.800</a></span> | <span class="t">and it'll be very easy to just specify the shape here without having to write everything out super explicitly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=360" target="_blank">00:06:00.800</a></span> | <span class="t">And then you can update that tensor as needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=364" target="_blank">00:06:04.800</a></span> | <span class="t">Another thing you can do is, just like we have ranges in Python,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=369" target="_blank">00:06:09.800</a></span> | <span class="t">so if you want to loop over a bunch of numbers, you can specify a range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=374" target="_blank">00:06:14.800</a></span> | <span class="t">You can also use torch.a_range to be able to actually instantiate a tensor with a particular range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=382" target="_blank">00:06:22.800</a></span> | <span class="t">In this case, we just looped over the numbers 1 through 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=386" target="_blank">00:06:26.800</a></span> | <span class="t">You could reshape this and make it 1 through 5 and then 6 through 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=390" target="_blank">00:06:30.800</a></span> | <span class="t">That's another way to be able to instantiate tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=394" target="_blank">00:06:34.800</a></span> | <span class="t">And finally, something to note is that when we apply particular operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=401" target="_blank">00:06:41.800</a></span> | <span class="t">such as just simple Python operations like addition or multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=406" target="_blank">00:06:46.800</a></span> | <span class="t">by default they're going to be element-wise, so they'll apply to all the elements in our tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">So in this case, we took our tensor, I think this one was probably from earlier above,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=418" target="_blank">00:06:58.800</a></span> | <span class="t">and we added 2 everywhere. Here we multiplied everything by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=424" target="_blank">00:07:04.800</a></span> | <span class="t">But pretty much the PyTorch semantics for broadcasting work pretty much the same as the NumPy semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=430" target="_blank">00:07:10.800</a></span> | <span class="t">So if you pretty much have different matrix operations where you need to batch across a particular dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=439" target="_blank">00:07:19.800</a></span> | <span class="t">PyTorch will be smart about it and it will actually make sure that you broadcast over the appropriate dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=445" target="_blank">00:07:25.800</a></span> | <span class="t">Although of course you have to make sure that the shapes are compatible based on the actual broadcasting rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=451" target="_blank">00:07:31.800</a></span> | <span class="t">So we'll get to that in a little bit when we look at reshaping and how different operations have those semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=460" target="_blank">00:07:40.800</a></span> | <span class="t">In this case, we have to define the, I guess I'm not personally aware of how you would define kind of a jagged tensor that has unequal dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=470" target="_blank">00:07:50.800</a></span> | <span class="t">But typically we don't want to do that because it makes our computation a lot more complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=476" target="_blank">00:07:56.800</a></span> | <span class="t">And so in cases where we have, you know, for instance, we have different sentences that we turn into tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=483" target="_blank">00:08:03.800</a></span> | <span class="t">we might have different length sentences in our training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=486" target="_blank">00:08:06.800</a></span> | <span class="t">We'll actually pad all the dimensions to be the same because ultimately we want to do everything with matrix operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=493" target="_blank">00:08:13.800</a></span> | <span class="t">And so in order to do that, we need to have a matrix of a fixed shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=496" target="_blank">00:08:16.800</a></span> | <span class="t">But yeah, that's a good point. I'm not sure if there is a way to do that, but typically we just get around this by padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=505" target="_blank">00:08:25.800</a></span> | <span class="t">Okay, so now we know how to define tensors. We can do some interesting things with them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=510" target="_blank">00:08:30.800</a></span> | <span class="t">So here we've created two tensors. One of them is a 3 by 2 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=516" target="_blank">00:08:36.800</a></span> | <span class="t">The other one is a 2 by 4 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=519" target="_blank">00:08:39.800</a></span> | <span class="t">And I think the answer is written up here, but what do we expect is the shape when we multiply these two tensors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=527" target="_blank">00:08:47.800</a></span> | <span class="t">So we have a 3 by 2 tensor and a 2 by 4 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=532" target="_blank">00:08:52.800</a></span> | <span class="t">Yeah, 3 by 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=534" target="_blank">00:08:54.800</a></span> | <span class="t">And so more generally, we can use Matmul in order to do matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=542" target="_blank">00:09:02.800</a></span> | <span class="t">It also implements batched matrix multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=546" target="_blank">00:09:06.800</a></span> | <span class="t">And so I won't go over the entire review of broadcasting semantics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=551" target="_blank">00:09:11.800</a></span> | <span class="t">but the main gist is that the dimensions of two tensors are compatible if you can left pad the tensors with ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=559" target="_blank">00:09:19.800</a></span> | <span class="t">so that the dimensions that line up either A, have the same number in that dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=565" target="_blank">00:09:25.800</a></span> | <span class="t">or B, one of them is a dummy dimension. One of them has a 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=568" target="_blank">00:09:28.800</a></span> | <span class="t">And in that case, in those dummy dimensions, PyTorch will actually make sure to copy over the tensor as many times as needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=576" target="_blank">00:09:36.800</a></span> | <span class="t">so that you can then actually perform the operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=579" target="_blank">00:09:39.800</a></span> | <span class="t">And that's useful when you want to do things like batched dot products or batched matrix multiplications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=585" target="_blank">00:09:45.800</a></span> | <span class="t">And I guess the final point here is there's also a shorthand notation that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=592" target="_blank">00:09:52.800</a></span> | <span class="t">So instead of kind of having to type out Matmul every time, you can just use the at operator, similar to NumPy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=599" target="_blank">00:09:59.800</a></span> | <span class="t">Effectively, that's kind of where we get into how batching works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=603" target="_blank">00:10:03.800</a></span> | <span class="t">So for example, if you had, let's say, two tensors that have some batch dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=613" target="_blank">00:10:13.800</a></span> | <span class="t">and then one of them is M by 1, and the other one is 1 by N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=619" target="_blank">00:10:19.800</a></span> | <span class="t">And if you do a batched matrix multiply to those two tensors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=623" target="_blank">00:10:23.800</a></span> | <span class="t">now what you effectively do is you preserve the batch dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=627" target="_blank">00:10:27.800</a></span> | <span class="t">and then you're doing a matrix multiplication between an M by 1 tensor and a 1 by N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=632" target="_blank">00:10:32.800</a></span> | <span class="t">So you get something that's the batch dimension by M by N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=636" target="_blank">00:10:36.800</a></span> | <span class="t">So effectively, they're kind of more, I think the full semantics are written out on the PyTorch website</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=642" target="_blank">00:10:42.800</a></span> | <span class="t">for how the matrix multiplication works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=644" target="_blank">00:10:44.800</a></span> | <span class="t">But you're right, you don't just have these cases where you have two two-dimensional tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=648" target="_blank">00:10:48.800</a></span> | <span class="t">You can have arbitrary number of dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=650" target="_blank">00:10:50.800</a></span> | <span class="t">and as long as the dimensions match up based on those semantics I was saying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=655" target="_blank">00:10:55.800</a></span> | <span class="t">then you can multiply it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=656" target="_blank">00:10:56.800</a></span> | <span class="t">Alternatively, you can do what I do, which is just multiply it anyways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=660" target="_blank">00:11:00.800</a></span> | <span class="t">and then if it throws an error, print out the shapes and kind of work from there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=664" target="_blank">00:11:04.800</a></span> | <span class="t">That tends to be faster in my opinion in a lot of ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=667" target="_blank">00:11:07.800</a></span> | <span class="t">But yeah, that's a good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=670" target="_blank">00:11:10.800</a></span> | <span class="t">All right, so yeah, let's keep going through some of the other different functionalities here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=677" target="_blank">00:11:17.800</a></span> | <span class="t">So we can define another tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=680" target="_blank">00:11:20.800</a></span> | <span class="t">and kind of one of the key things that we always want to look at is the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=685" target="_blank">00:11:25.800</a></span> | <span class="t">So in this case, we just have a 1D tensor of length 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=689" target="_blank">00:11:29.800</a></span> | <span class="t">so the torch.size just gives us 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=693" target="_blank">00:11:33.800</a></span> | <span class="t">In general, this is kind of one of the key debugging steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=696" target="_blank">00:11:36.800</a></span> | <span class="t">and something that I'll try and emphasize a lot throughout this session,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=700" target="_blank">00:11:40.800</a></span> | <span class="t">which is printing the shapes of all of your tensors is probably your best resource.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=705" target="_blank">00:11:45.800</a></span> | <span class="t">When it comes to debugging, it's kind of one of the hardest things to intuit exactly what's going on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=710" target="_blank">00:11:50.800</a></span> | <span class="t">once you start stacking a lot of different operations together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=714" target="_blank">00:11:54.800</a></span> | <span class="t">So printing out the shapes at each point and seeing do they match what you expect is something important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=720" target="_blank">00:12:00.800</a></span> | <span class="t">and it's better to rely on that than just on the error message that PyTorch gives you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=726" target="_blank">00:12:06.800</a></span> | <span class="t">because under the hood, PyTorch might implement certain optimizations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=730" target="_blank">00:12:10.800</a></span> | <span class="t">and actually reshape the underlying tensor you have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=733" target="_blank">00:12:13.800</a></span> | <span class="t">so you may not see the numbers you expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=735" target="_blank">00:12:15.800</a></span> | <span class="t">So it's always great to print out the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=738" target="_blank">00:12:18.800</a></span> | <span class="t">And so yeah, let's...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=742" target="_blank">00:12:22.800</a></span> | <span class="t">So again, we can always print out the shape,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=746" target="_blank">00:12:26.800</a></span> | <span class="t">and we can have a more complex, in this case, 3-dimensional tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=752" target="_blank">00:12:32.800</a></span> | <span class="t">which is 3 by 2 by 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=754" target="_blank">00:12:34.800</a></span> | <span class="t">and we can print out the shape and we can see all of the dimensions here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=759" target="_blank">00:12:39.800</a></span> | <span class="t">And so now you're like, "Okay, great, we have tensors, we can look at their shapes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=764" target="_blank">00:12:44.800</a></span> | <span class="t">but what do we actually do with them?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=766" target="_blank">00:12:46.800</a></span> | <span class="t">And so now let's get into kind of what are the operations that we can apply to these tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=772" target="_blank">00:12:52.800</a></span> | <span class="t">And so one of them is it's very easy to reshape tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=778" target="_blank">00:12:58.800</a></span> | <span class="t">So in this case, we're creating this 15-dimensional tensor that's...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=783" target="_blank">00:13:03.800</a></span> | <span class="t">the number is 1 to 15,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=786" target="_blank">00:13:06.800</a></span> | <span class="t">and now we're reshaping it so now it's a 5 by 3 tensor here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=791" target="_blank">00:13:11.800</a></span> | <span class="t">And so you might wonder, "Well, like, what's the point of that?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=795" target="_blank">00:13:15.800</a></span> | <span class="t">And it's because a lot of times when we are doing machine learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=800" target="_blank">00:13:20.800</a></span> | <span class="t">we actually want to learn in batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=802" target="_blank">00:13:22.800</a></span> | <span class="t">and so we might take our data and we might reshape it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=805" target="_blank">00:13:25.800</a></span> | <span class="t">so now that instead of kind of being a long, flattened list of things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=809" target="_blank">00:13:29.800</a></span> | <span class="t">we actually have a set of batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=811" target="_blank">00:13:31.800</a></span> | <span class="t">or in some cases we have a set of batches of a set of sentences or sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=817" target="_blank">00:13:37.800</a></span> | <span class="t">of a particular length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=819" target="_blank">00:13:39.800</a></span> | <span class="t">and each of the elements in that sequence has an embedding of a particular dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=824" target="_blank">00:13:44.800</a></span> | <span class="t">And so based on the types of operations that you're trying to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=828" target="_blank">00:13:48.800</a></span> | <span class="t">you'll sometimes need to reshape those tensors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=831" target="_blank">00:13:51.800</a></span> | <span class="t">and sometimes you'll want to particularly sometimes transpose dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=836" target="_blank">00:13:56.800</a></span> | <span class="t">if you want to, for instance, reorganize your data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=840" target="_blank">00:14:00.800</a></span> | <span class="t">So that's another operation to keep in mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=844" target="_blank">00:14:04.800</a></span> | <span class="t">I believe the difference is view will...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=849" target="_blank">00:14:09.800</a></span> | <span class="t">view will create a view of the underlying tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=851" target="_blank">00:14:11.800</a></span> | <span class="t">and so I think the underlying tensor will still have this same shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=855" target="_blank">00:14:15.800</a></span> | <span class="t">It will actually modify the tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=862" target="_blank">00:14:22.800</a></span> | <span class="t">All right, and then finally, like I said at the beginning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=865" target="_blank">00:14:25.800</a></span> | <span class="t">your intuition about PyTorch tensors can simply be they're kind of a nice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=870" target="_blank">00:14:30.800</a></span> | <span class="t">easy way to work with NumPy arrays,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=873" target="_blank">00:14:33.800</a></span> | <span class="t">but they have all these great properties,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=875" target="_blank">00:14:35.800</a></span> | <span class="t">like now we can essentially use them with GPUs and it's very optimized,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=880" target="_blank">00:14:40.800</a></span> | <span class="t">and we can also compute gradients quickly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=884" target="_blank">00:14:44.800</a></span> | <span class="t">and to kind of just emphasize this point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=887" target="_blank">00:14:47.800</a></span> | <span class="t">if you have some NumPy code and you have a bunch of NumPy arrays,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=890" target="_blank">00:14:50.800</a></span> | <span class="t">you can directly convert them into PyTorch sensors by simply casting them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=896" target="_blank">00:14:56.800</a></span> | <span class="t">and you can also take those tensors and convert them back to NumPy arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=903" target="_blank">00:15:03.800</a></span> | <span class="t">All right, and so one of the things you might be asking is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=906" target="_blank">00:15:06.800</a></span> | <span class="t">why do we care about tensors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=909" target="_blank">00:15:09.800</a></span> | <span class="t">What makes them good?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=911" target="_blank">00:15:11.800</a></span> | <span class="t">And one of the great things about them is that they support vectorized operations very easily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=916" target="_blank">00:15:16.800</a></span> | <span class="t">Essentially, we can parallelize a lot of different computations and do them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=921" target="_blank">00:15:21.800</a></span> | <span class="t">for instance, across a batch of data all at once,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=924" target="_blank">00:15:24.800</a></span> | <span class="t">and one of those operations you might want to do, for instance, is a sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=928" target="_blank">00:15:28.800</a></span> | <span class="t">So you can take, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=932" target="_blank">00:15:32.800</a></span> | <span class="t">a tensor which is shaped five by seven,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=936" target="_blank">00:15:36.800</a></span> | <span class="t">and it looks like that's not working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=941" target="_blank">00:15:41.800</a></span> | <span class="t">You can take a tensor that's shaped five by seven,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=943" target="_blank">00:15:43.800</a></span> | <span class="t">and now you can compute different operations on it that essentially collapse the dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=950" target="_blank">00:15:50.800</a></span> | <span class="t">So the first one is sum, and so you can take it and you can sum across both the rows as well as the columns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=956" target="_blank">00:15:56.800</a></span> | <span class="t">and so one way I like to think about this to kind of keep them straight is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=961" target="_blank">00:16:01.800</a></span> | <span class="t">the dimension that you specify in the sum is the dimension you're collapsing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=966" target="_blank">00:16:06.800</a></span> | <span class="t">So in this case, if you take the data and sum over dimension zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=971" target="_blank">00:16:11.800</a></span> | <span class="t">because you know the shape of the underlying tensor is five by seven,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=975" target="_blank">00:16:15.800</a></span> | <span class="t">you've collapsed the zeroth dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=978" target="_blank">00:16:18.800</a></span> | <span class="t">so you should be left with something that's just shaped seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=981" target="_blank">00:16:21.800</a></span> | <span class="t">And if you see the actual tensor, you've got 75, 80, 85, 90,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=986" target="_blank">00:16:26.800</a></span> | <span class="t">you've got this tensor which is shaped seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=989" target="_blank">00:16:29.800</a></span> | <span class="t">Alternatively, you can think about whether or not you're kind of summing across the rows or summing across the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=995" target="_blank">00:16:35.800</a></span> | <span class="t">But it's not just sum, it applies to other operations as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=999" target="_blank">00:16:39.800</a></span> | <span class="t">You can compute standard deviations, you can normalize your data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1003" target="_blank">00:16:43.800</a></span> | <span class="t">you can do other operations which essentially batch across the entire set of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1008" target="_blank">00:16:48.800</a></span> | <span class="t">And not only do these apply over one dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1012" target="_blank">00:16:52.800</a></span> | <span class="t">but here you can see that if you don't specify any dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1015" target="_blank">00:16:55.800</a></span> | <span class="t">then by default the operation actually applies to the entire tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1019" target="_blank">00:16:59.800</a></span> | <span class="t">So here we end up just taking the sum of the entire thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1023" target="_blank">00:17:03.800</a></span> | <span class="t">So if you think about it, the zeroth dimension is the number of rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1026" target="_blank">00:17:06.800</a></span> | <span class="t">there are five rows and there are seven columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1028" target="_blank">00:17:08.800</a></span> | <span class="t">So if we sum out the rows, then we're actually summing across the columns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1036" target="_blank">00:17:16.800</a></span> | <span class="t">and so now we only have seven values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1039" target="_blank">00:17:19.800</a></span> | <span class="t">But I like to think about more just in terms of the dimensions to keep it straight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1042" target="_blank">00:17:22.800</a></span> | <span class="t">rather than rows or columns because it can get confusing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1045" target="_blank">00:17:25.800</a></span> | <span class="t">If you're summing out dimension zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1047" target="_blank">00:17:27.800</a></span> | <span class="t">then effectively you've taken something which has some shape that's dimension zero by dimension one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1052" target="_blank">00:17:32.800</a></span> | <span class="t">to just whatever is the dimension one shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1056" target="_blank">00:17:36.800</a></span> | <span class="t">And then from there you can kind of figure out, okay, which way did I actually sum to check if you were right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1061" target="_blank">00:17:41.800</a></span> | <span class="t">NumPy implements a lot of this vectorization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1065" target="_blank">00:17:45.800</a></span> | <span class="t">and I believe in the homework that you have right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1069" target="_blank">00:17:49.800</a></span> | <span class="t">I think part of your job is to vectorize a lot of these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1072" target="_blank">00:17:52.800</a></span> | <span class="t">So the big advantage with PyTorch is that essentially it's optimized to be able to take advantage of your GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1079" target="_blank">00:17:59.800</a></span> | <span class="t">When we actually start building out neural networks that are bigger, that involve more computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1084" target="_blank">00:18:04.800</a></span> | <span class="t">we're going to be doing a lot of these matrix multiplication operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1088" target="_blank">00:18:08.800</a></span> | <span class="t">that it's going to be a lot better for our processor if we can make use of the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1093" target="_blank">00:18:13.800</a></span> | <span class="t">And so that's where PyTorch really comes in handy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1097" target="_blank">00:18:17.800</a></span> | <span class="t">In addition to also defining a lot of those neural network modules as we'll see later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1102" target="_blank">00:18:22.800</a></span> | <span class="t">um, for you so that now you don't need to worry about, for instance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1106" target="_blank">00:18:26.800</a></span> | <span class="t">implementing a basic linear layer and back propagation from scratch and also your optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1112" target="_blank">00:18:32.800</a></span> | <span class="t">All of those things will be built in and you can just call the respective APIs to make use of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1117" target="_blank">00:18:37.800</a></span> | <span class="t">Whereas in Python and NumPy you might have to do a lot of that coding yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1123" target="_blank">00:18:43.800</a></span> | <span class="t">Yeah. All right. So we'll keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1133" target="_blank">00:18:53.800</a></span> | <span class="t">So this is a quiz except I think it tells you the answer so it's not much of a quiz.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1139" target="_blank">00:18:59.800</a></span> | <span class="t">But pretty much, you know, what would you do if now I told you instead of, you know, summing over this tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1147" target="_blank">00:19:07.800</a></span> | <span class="t">I want you to compute the average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1150" target="_blank">00:19:10.800</a></span> | <span class="t">And so there's, there's two different ways you could compute the average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1153" target="_blank">00:19:13.800</a></span> | <span class="t">You could compute the average across the rows or across the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1158" target="_blank">00:19:18.800</a></span> | <span class="t">And so essentially now we kind of get back to this question of, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1163" target="_blank">00:19:23.800</a></span> | <span class="t">which dimension am I actually going to reduce over?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1165" target="_blank">00:19:25.800</a></span> | <span class="t">And so here if we want to preserve the rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1168" target="_blank">00:19:28.800</a></span> | <span class="t">then we need to actually sum over the second dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1172" target="_blank">00:19:32.800</a></span> | <span class="t">Um, there are really the first, uh, zeroth and first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1176" target="_blank">00:19:36.800</a></span> | <span class="t">So the first dimension is what we add to sum over, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1180" target="_blank">00:19:40.800</a></span> | <span class="t">because we want to preserve the zeroth dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1183" target="_blank">00:19:43.800</a></span> | <span class="t">And so that's why for row average you see the dim equals one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1187" target="_blank">00:19:47.800</a></span> | <span class="t">And for column average, same reasoning is why you see the dim equals zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1192" target="_blank">00:19:52.800</a></span> | <span class="t">And so if we run this code, we'll see kind of what are the shapes that we expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1199" target="_blank">00:19:59.800</a></span> | <span class="t">If we're taking the average over rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1201" target="_blank">00:20:01.800</a></span> | <span class="t">then an object that's two by three should just become an object that's two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1206" target="_blank">00:20:06.800</a></span> | <span class="t">It's just a one-dimensional, almost a vector you can think of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1210" target="_blank">00:20:10.800</a></span> | <span class="t">And if we are averaging across the columns, there's three columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1215" target="_blank">00:20:15.800</a></span> | <span class="t">So now our average should have three values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1217" target="_blank">00:20:17.800</a></span> | <span class="t">And so now we're left with a three-dim- uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1220" target="_blank">00:20:20.800</a></span> | <span class="t">one-dimensional tensor of length three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1223" target="_blank">00:20:23.800</a></span> | <span class="t">So yeah, does that kind of make sense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1226" target="_blank">00:20:26.800</a></span> | <span class="t">I guess is this general intuition about how we deal with shapes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1229" target="_blank">00:20:29.800</a></span> | <span class="t">and how some of these operations manipulate shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1232" target="_blank">00:20:32.800</a></span> | <span class="t">So now we'll get into indexing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1234" target="_blank">00:20:34.800</a></span> | <span class="t">This can get a little bit tricky,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1237" target="_blank">00:20:37.800</a></span> | <span class="t">but I think you'll find that the semantics are very similar to NumPy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1243" target="_blank">00:20:43.800</a></span> | <span class="t">So one of the things that you can do in NumPy is that you can take these NumPy arrays,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1249" target="_blank">00:20:49.800</a></span> | <span class="t">and you can slice across them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1251" target="_blank">00:20:51.800</a></span> | <span class="t">In many different ways, you can create copies of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1254" target="_blank">00:20:54.800</a></span> | <span class="t">and you can index across particular dimensions to select out different elements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1260" target="_blank">00:21:00.800</a></span> | <span class="t">different rows, or different columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1262" target="_blank">00:21:02.800</a></span> | <span class="t">And so in this case, let's take this example tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1265" target="_blank">00:21:05.800</a></span> | <span class="t">which is three by two by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1269" target="_blank">00:21:09.800</a></span> | <span class="t">Um, and first thing you always want to do when you have a new tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1274" target="_blank">00:21:14.800</a></span> | <span class="t">print out its shape, understand what you're working with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1277" target="_blank">00:21:17.800</a></span> | <span class="t">And so I guess, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1281" target="_blank">00:21:21.800</a></span> | <span class="t">I may have shown this already, but what will x bracket zero print out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1287" target="_blank">00:21:27.800</a></span> | <span class="t">What happens if we index into just the first element?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1290" target="_blank">00:21:30.800</a></span> | <span class="t">What's the shape of this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1293" target="_blank">00:21:33.800</a></span> | <span class="t">Yeah, two by two, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1297" target="_blank">00:21:37.800</a></span> | <span class="t">Because if you think about it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1298" target="_blank">00:21:38.800</a></span> | <span class="t">our tensor is really just a list of three things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1301" target="_blank">00:21:41.800</a></span> | <span class="t">Each of those things happens to also be a two by two tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1305" target="_blank">00:21:45.800</a></span> | <span class="t">So we get a two by two object,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1307" target="_blank">00:21:47.800</a></span> | <span class="t">in this case, the first thing, one, two, three, four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1310" target="_blank">00:21:50.800</a></span> | <span class="t">And so just like NumPy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1313" target="_blank">00:21:53.800</a></span> | <span class="t">if you provide a colon in a particular dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1316" target="_blank">00:21:56.800</a></span> | <span class="t">it means essentially copy over that dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1319" target="_blank">00:21:59.800</a></span> | <span class="t">So if we do x bracket zero implicitly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1323" target="_blank">00:22:03.800</a></span> | <span class="t">we're essentially putting a colon for all the other dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1326" target="_blank">00:22:06.800</a></span> | <span class="t">So it's essentially saying, grab the first thing along the zeroth dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1331" target="_blank">00:22:11.800</a></span> | <span class="t">and then grab everything along the other two dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1334" target="_blank">00:22:14.800</a></span> | <span class="t">If we now take, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1338" target="_blank">00:22:18.800</a></span> | <span class="t">just the zeroth along- the element along the first dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1342" target="_blank">00:22:22.800</a></span> | <span class="t">um, what are we going to get?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1345" target="_blank">00:22:25.800</a></span> | <span class="t">Well, ultimately, we're going to get- now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1349" target="_blank">00:22:29.800</a></span> | <span class="t">if you look, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1351" target="_blank">00:22:31.800</a></span> | <span class="t">the kind of first dimension where these three things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1354" target="_blank">00:22:34.800</a></span> | <span class="t">the second dimension is now each of these two rows within those things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1359" target="_blank">00:22:39.800</a></span> | <span class="t">So like one, two, and three, four, five, six, and seven,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1362" target="_blank">00:22:42.800</a></span> | <span class="t">eight, nine, 10, and 11, 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1364" target="_blank">00:22:44.800</a></span> | <span class="t">So if we index into the second dimen- or the first dimension and get the zeroth element,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1371" target="_blank">00:22:51.800</a></span> | <span class="t">then we're going to end up with one, two, five, six, and nine, 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1376" target="_blank">00:22:56.800</a></span> | <span class="t">And even if that's a little bit tricky,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1380" target="_blank">00:23:00.800</a></span> | <span class="t">you can kind of go back to the trick I mentioned before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1383" target="_blank">00:23:03.800</a></span> | <span class="t">where we're slicing across the first dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1387" target="_blank">00:23:07.800</a></span> | <span class="t">So if we look at the shape of our tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1389" target="_blank">00:23:09.800</a></span> | <span class="t">it's three by two by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1391" target="_blank">00:23:11.800</a></span> | <span class="t">If we collapse the first dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1394" target="_blank">00:23:14.800</a></span> | <span class="t">that two in the middle, we're left with something that's three by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1397" target="_blank">00:23:17.800</a></span> | <span class="t">So it might seem a little bit trivial kind of going through this in a lot of detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1403" target="_blank">00:23:23.800</a></span> | <span class="t">but I think it's important because it can get tricky when your tensor shapes get more complicated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1408" target="_blank">00:23:28.800</a></span> | <span class="t">how to actually reason about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1410" target="_blank">00:23:30.800</a></span> | <span class="t">And so I won't go through every example here since a lot of them kind of reinforce the same thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1416" target="_blank">00:23:36.800</a></span> | <span class="t">but I'll just highlight a few things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1418" target="_blank">00:23:38.800</a></span> | <span class="t">Just like NumPy, you can choose to get a range of elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1423" target="_blank">00:23:43.800</a></span> | <span class="t">Uh, in this case, we're taking this new tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1429" target="_blank">00:23:49.800</a></span> | <span class="t">which is one to- one through 15 rearranged as a five by three tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1435" target="_blank">00:23:55.800</a></span> | <span class="t">And if we take the zeroth through third row, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1439" target="_blank">00:23:59.800</a></span> | <span class="t">exclusive, we'll get the first three rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1442" target="_blank">00:24:02.800</a></span> | <span class="t">And we can do the same thing, but now with slicing across multiple dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1447" target="_blank">00:24:07.800</a></span> | <span class="t">And I think the final point I want to talk about here is list indexing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1455" target="_blank">00:24:15.800</a></span> | <span class="t">List indexing is also present in NumPy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1458" target="_blank">00:24:18.800</a></span> | <span class="t">and it's a very clever shorthand for being able to essentially select out multiple elements at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1465" target="_blank">00:24:25.800</a></span> | <span class="t">So in this case, what you can do is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1468" target="_blank">00:24:28.800</a></span> | <span class="t">if you want to get the zeroth, the second, and the fourth element of our matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1475" target="_blank">00:24:35.800</a></span> | <span class="t">you can just, instead of indexing with a particular number or set of numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1480" target="_blank">00:24:40.800</a></span> | <span class="t">index with a list of indices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1483" target="_blank">00:24:43.800</a></span> | <span class="t">So in this case, if we go up to our tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1487" target="_blank">00:24:47.800</a></span> | <span class="t">if we take out the zeroth, the second, and the fourth,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1491" target="_blank">00:24:51.800</a></span> | <span class="t">we should see those three rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1494" target="_blank">00:24:54.800</a></span> | <span class="t">and that's what we end up getting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1499" target="_blank">00:24:59.800</a></span> | <span class="t">Yeah, again, these are kind of a lot of examples to just reiterate the same point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1504" target="_blank">00:25:04.800</a></span> | <span class="t">which is that you can slice across your data in multiple ways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1508" target="_blank">00:25:08.800</a></span> | <span class="t">and at different points you're going to need to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1511" target="_blank">00:25:11.800</a></span> | <span class="t">So being familiar with the shapes that you understand what's the underlying output that you expect is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1518" target="_blank">00:25:18.800</a></span> | <span class="t">In this case, for instance, we're slicing across the first and the second dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1523" target="_blank">00:25:23.800</a></span> | <span class="t">and we're keeping the first, the zeroth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1527" target="_blank">00:25:27.800</a></span> | <span class="t">And so we're going to end up getting essentially kind of the top left element of each of those three things in our tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1535" target="_blank">00:25:35.800</a></span> | <span class="t">If we scroll all the way up here, we'll get this one, we'll get this five, and we'll get this nine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1541" target="_blank">00:25:41.800</a></span> | <span class="t">because we go across all of the zeroth dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1545" target="_blank">00:25:45.800</a></span> | <span class="t">and then across the first and the second, we only take the first, the zeroth element in both of those positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1553" target="_blank">00:25:53.800</a></span> | <span class="t">And so that's why we get one, five, nine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1560" target="_blank">00:26:00.800</a></span> | <span class="t">And also, of course, you can, you know, apply all of the colons to get back the original tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1571" target="_blank">00:26:11.800</a></span> | <span class="t">Okay, and then I think the last thing when it comes to indexing is conversions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1577" target="_blank">00:26:17.800</a></span> | <span class="t">So typically, when we're writing code with neural networks, ultimately we're going to, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1584" target="_blank">00:26:24.800</a></span> | <span class="t">process some data through a network, and we're going to get a loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1587" target="_blank">00:26:27.800</a></span> | <span class="t">And that loss needs to be a scalar, and then we're going to compute gradients with respect to that loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1592" target="_blank">00:26:32.800</a></span> | <span class="t">So one thing to keep in mind is that sometimes you might have an operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1597" target="_blank">00:26:37.800</a></span> | <span class="t">and it fails because it was actually expecting a scalar value rather than a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1602" target="_blank">00:26:42.800</a></span> | <span class="t">And so you can extract out the scalar from this one-by-one tensor by just calling dot item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1610" target="_blank">00:26:50.800</a></span> | <span class="t">So in this case, you know, if you have a tensor, which is just literally one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1614" target="_blank">00:26:54.800</a></span> | <span class="t">then you can actually get the Python scalar that corresponds to it by calling dot item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1620" target="_blank">00:27:00.800</a></span> | <span class="t">So now we can get into the more interesting stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1622" target="_blank">00:27:02.800</a></span> | <span class="t">One of the really cool things with PyTorch is Autograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1626" target="_blank">00:27:06.800</a></span> | <span class="t">And what Autograd is, is PyTorch essentially provides an automatic differentiation package,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1634" target="_blank">00:27:14.800</a></span> | <span class="t">where when you define your neural network, you're essentially defining many nodes that compute some function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1643" target="_blank">00:27:23.800</a></span> | <span class="t">And in the forward pass, you're kind of running your data through those nodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1647" target="_blank">00:27:27.800</a></span> | <span class="t">But what PyTorch is doing on the back end is that at each of those points,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1651" target="_blank">00:27:31.800</a></span> | <span class="t">it's going to actually store the gradients and accumulate them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1656" target="_blank">00:27:36.800</a></span> | <span class="t">so that every time you do your backwards pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1659" target="_blank">00:27:39.800</a></span> | <span class="t">you apply the chain rule to be able to calculate all these different gradients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1663" target="_blank">00:27:43.800</a></span> | <span class="t">and PyTorch caches those gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1666" target="_blank">00:27:46.800</a></span> | <span class="t">And then you will have access to all of those gradients to be able to actually then run your favorite optimizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1672" target="_blank">00:27:52.800</a></span> | <span class="t">and optimize, you know, with SGD or with Atom or whichever optimizer you choose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1679" target="_blank">00:27:59.800</a></span> | <span class="t">And so that's kind of one of the great features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1682" target="_blank">00:28:02.800</a></span> | <span class="t">You don't have to worry about actually writing the code that computes all of these gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1686" target="_blank">00:28:06.800</a></span> | <span class="t">and actually caches all of them properly, applies the chain rule, does all these steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1691" target="_blank">00:28:11.800</a></span> | <span class="t">You can abstract all of that away with just one call to dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1696" target="_blank">00:28:16.800</a></span> | <span class="t">And so in this case, we'll run through a little bit of an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1700" target="_blank">00:28:20.800</a></span> | <span class="t">where we'll see the gradients getting computed automatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1705" target="_blank">00:28:25.800</a></span> | <span class="t">So in this case, we're going to initialize a tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1712" target="_blank">00:28:32.800</a></span> | <span class="t">and requires grad is true by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1715" target="_blank">00:28:35.800</a></span> | <span class="t">It just means that by default for a given tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1718" target="_blank">00:28:38.800</a></span> | <span class="t">Python, PyTorch will store the gradient associated with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1724" target="_blank">00:28:44.800</a></span> | <span class="t">And you might wonder, well, you know, why do we have this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1729" target="_blank">00:28:49.800</a></span> | <span class="t">You know, wouldn't we always want to store the gradient?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1731" target="_blank">00:28:51.800</a></span> | <span class="t">And the answer is, at train time, you need the gradients in order to actually train your network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1736" target="_blank">00:28:56.800</a></span> | <span class="t">But at inference time, you'd actually want to disable your gradients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1740" target="_blank">00:29:00.800</a></span> | <span class="t">and you can actually do that because it's a lot of extra computation that's not needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1744" target="_blank">00:29:04.800</a></span> | <span class="t">since you're not making any updates to your network anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1748" target="_blank">00:29:08.800</a></span> | <span class="t">And so let's create this right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1753" target="_blank">00:29:13.800</a></span> | <span class="t">We don't have any gradients being computed because we haven't actually called backwards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1758" target="_blank">00:29:18.800</a></span> | <span class="t">to actually compute some quantity with respect to this particular tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1765" target="_blank">00:29:25.800</a></span> | <span class="t">We haven't actually computed those gradients yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1769" target="_blank">00:29:29.800</a></span> | <span class="t">So right now, the dot grad feature, which will actually store the gradient associated with that tensor, is none.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1776" target="_blank">00:29:36.800</a></span> | <span class="t">And so now let's just define a really simple function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1779" target="_blank">00:29:39.800</a></span> | <span class="t">We have x. We're going to define the function y equals 3x squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1785" target="_blank">00:29:45.800</a></span> | <span class="t">And so now we're going to call y dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1789" target="_blank">00:29:49.800</a></span> | <span class="t">And so now what happens is when we actually print out x dot grad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1794" target="_blank">00:29:54.800</a></span> | <span class="t">what we should expect to see is number 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1798" target="_blank">00:29:58.800</a></span> | <span class="t">And the reason is that our function y is 3x squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1803" target="_blank">00:30:03.800</a></span> | <span class="t">If we compute the gradient of that function, we're going to get 6x, and our actual value was 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1811" target="_blank">00:30:11.800</a></span> | <span class="t">So the actual gradient is going to be 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1815" target="_blank">00:30:15.800</a></span> | <span class="t">And we see that when we print out x dot grad, that's what we get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1820" target="_blank">00:30:20.800</a></span> | <span class="t">And now we'll just run it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1823" target="_blank">00:30:23.800</a></span> | <span class="t">Let's set z equal to 3x squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1825" target="_blank">00:30:25.800</a></span> | <span class="t">We call z dot backwards, and we print out x dot grad again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1829" target="_blank">00:30:29.800</a></span> | <span class="t">And now we see that- I may not run this in the right order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1835" target="_blank">00:30:35.800</a></span> | <span class="t">Okay. So here in the second one that I re-ran, we see that it says 24.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1842" target="_blank">00:30:42.800</a></span> | <span class="t">And so you might be wondering, well, I just did the same thing twice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1845" target="_blank">00:30:45.800</a></span> | <span class="t">Shouldn't I see 12 again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1847" target="_blank">00:30:47.800</a></span> | <span class="t">And the answer is that by default, PyTorch will accumulate the gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1852" target="_blank">00:30:52.800</a></span> | <span class="t">So it won't actually rewrite the gradient each time you compute it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1857" target="_blank">00:30:57.800</a></span> | <span class="t">It will sum it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1858" target="_blank">00:30:58.800</a></span> | <span class="t">And the reason is because when you actually have backpropagation for your network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1862" target="_blank">00:31:02.800</a></span> | <span class="t">you want to accumulate the gradients, you know, across all of your examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1866" target="_blank">00:31:06.800</a></span> | <span class="t">and then actually apply your update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1868" target="_blank">00:31:08.800</a></span> | <span class="t">You don't want to overwrite the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1870" target="_blank">00:31:10.800</a></span> | <span class="t">But this also means that every time you have a training iteration for your network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1875" target="_blank">00:31:15.800</a></span> | <span class="t">you need to zero out the gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1877" target="_blank">00:31:17.800</a></span> | <span class="t">because you don't want the previous gradients from the last epoch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1881" target="_blank">00:31:21.800</a></span> | <span class="t">where you iterated through all of your training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1883" target="_blank">00:31:23.800</a></span> | <span class="t">to mess with the current update that you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1887" target="_blank">00:31:27.800</a></span> | <span class="t">So that's kind of one thing to note, which is that that's essentially why we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1894" target="_blank">00:31:34.800</a></span> | <span class="t">when we actually write the training loop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1896" target="_blank">00:31:36.800</a></span> | <span class="t">you have to run zero grad in order to zero out the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1899" target="_blank">00:31:39.800</a></span> | <span class="t">Yes, so I accidentally ran the cells in the wrong order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1904" target="_blank">00:31:44.800</a></span> | <span class="t">Maybe to make it more clear, let me put this one first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1908" target="_blank">00:31:48.800</a></span> | <span class="t">So this is actually what it should look like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1913" target="_blank">00:31:53.800</a></span> | <span class="t">which is that we ran it once, and I ran this cell first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1916" target="_blank">00:31:56.800</a></span> | <span class="t">And it has 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1918" target="_blank">00:31:58.800</a></span> | <span class="t">And then we ran it a second time, and we get 24.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1922" target="_blank">00:32:02.800</a></span> | <span class="t">Yes, so if you have all of your tensors defined,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1926" target="_blank">00:32:06.800</a></span> | <span class="t">then when you actually call dot backwards,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1928" target="_blank">00:32:08.800</a></span> | <span class="t">if it's a function of multiple variables,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1930" target="_blank">00:32:10.800</a></span> | <span class="t">it's going to compute all of those partials, all of those gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1934" target="_blank">00:32:14.800</a></span> | <span class="t">Yes, so what's happening here is that the way PyTorch works is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1938" target="_blank">00:32:18.800</a></span> | <span class="t">it's storing the accumulated gradient at x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1943" target="_blank">00:32:23.800</a></span> | <span class="t">And so we've essentially made two different backwards passes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1948" target="_blank">00:32:28.800</a></span> | <span class="t">We've called it once on this function y, which is a function of x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1953" target="_blank">00:32:33.800</a></span> | <span class="t">and we've called it once on z, which is also a function of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1956" target="_blank">00:32:36.800</a></span> | <span class="t">And so you're right, we can't actually disambiguate which came from what,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1960" target="_blank">00:32:40.800</a></span> | <span class="t">we just see the accumulated gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1962" target="_blank">00:32:42.800</a></span> | <span class="t">But typically, that's actually exactly what we want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1966" target="_blank">00:32:46.800</a></span> | <span class="t">because what we want is to be able to run our network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1969" target="_blank">00:32:49.800</a></span> | <span class="t">and accumulate the gradient across all of the training examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1973" target="_blank">00:32:53.800</a></span> | <span class="t">that define our loss, and then perform our optimizer step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1977" target="_blank">00:32:57.800</a></span> | <span class="t">So yes, even with respect to one thing, it doesn't matter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1980" target="_blank">00:33:00.800</a></span> | <span class="t">because in practice, each of those things is really a different example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1983" target="_blank">00:33:03.800</a></span> | <span class="t">in our set of training examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1985" target="_blank">00:33:05.800</a></span> | <span class="t">And so we're not interested in, you know, the gradient from one example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1989" target="_blank">00:33:09.800</a></span> | <span class="t">we're actually interested in the overall gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1992" target="_blank">00:33:12.800</a></span> | <span class="t">So going back to this example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1994" target="_blank">00:33:14.800</a></span> | <span class="t">what's happening here is that in the backwards pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=1997" target="_blank">00:33:17.800</a></span> | <span class="t">what it's doing is, you can imagine there's the x tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2002" target="_blank">00:33:22.800</a></span> | <span class="t">and then there's the dot grad attribute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2004" target="_blank">00:33:24.800</a></span> | <span class="t">which is another separate tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2006" target="_blank">00:33:26.800</a></span> | <span class="t">It's going to be the same shape as x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2008" target="_blank">00:33:28.800</a></span> | <span class="t">And what that is storing is it's storing the accumulated gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2012" target="_blank">00:33:32.800</a></span> | <span class="t">from every single time that you've called dot backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2016" target="_blank">00:33:36.800</a></span> | <span class="t">on a quantity that, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2019" target="_blank">00:33:39.800</a></span> | <span class="t">essentially has some dependency on x that will have a non-zero gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2023" target="_blank">00:33:43.800</a></span> | <span class="t">And so the first time we call it, the gradient will be 12,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2026" target="_blank">00:33:46.800</a></span> | <span class="t">because 6x, 6 times 2, 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2029" target="_blank">00:33:49.800</a></span> | <span class="t">The second time we do it with z, it's also still 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2033" target="_blank">00:33:53.800</a></span> | <span class="t">But the point is that dot grad doesn't actually overwrite the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2036" target="_blank">00:33:56.800</a></span> | <span class="t">each time you call dot backwards, it simply adds them, it accumulates them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2040" target="_blank">00:34:00.800</a></span> | <span class="t">And kind of the intuition there is that ultimately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2044" target="_blank">00:34:04.800</a></span> | <span class="t">you're going to want to compute the gradient with respect to the loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2049" target="_blank">00:34:09.800</a></span> | <span class="t">and that loss is going to be made up of many different examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2052" target="_blank">00:34:12.800</a></span> | <span class="t">And so you need to accumulate the gradient from all of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2055" target="_blank">00:34:15.800</a></span> | <span class="t">in order to make a single update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2057" target="_blank">00:34:17.800</a></span> | <span class="t">And then of course, you'll have to zero that out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2059" target="_blank">00:34:19.800</a></span> | <span class="t">because every time you make one pass through all of your data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2062" target="_blank">00:34:22.800</a></span> | <span class="t">you don't want that next batch of data to also be double counting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2066" target="_blank">00:34:26.800</a></span> | <span class="t">the previous batch's update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2067" target="_blank">00:34:27.800</a></span> | <span class="t">You want to keep those separate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2069" target="_blank">00:34:29.800</a></span> | <span class="t">And so we'll see that in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2072" target="_blank">00:34:32.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2077" target="_blank">00:34:37.800</a></span> | <span class="t">All right. So now we're going to move on to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2081" target="_blank">00:34:41.800</a></span> | <span class="t">one of the final pieces of the puzzle, which is neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2084" target="_blank">00:34:44.800</a></span> | <span class="t">How do we actually use them in PyTorch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2087" target="_blank">00:34:47.800</a></span> | <span class="t">And once we have that and we have our optimization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2091" target="_blank">00:34:51.800</a></span> | <span class="t">we'll finally be able to figure out how do we actually train a neural network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2094" target="_blank">00:34:54.800</a></span> | <span class="t">What does that look like and why it's so clean and efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2098" target="_blank">00:34:58.800</a></span> | <span class="t">when you do it in PyTorch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2101" target="_blank">00:35:01.800</a></span> | <span class="t">So the first thing that you want to do is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2104" target="_blank">00:35:04.800</a></span> | <span class="t">we're going to be defining neural networks in terms of existing building blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2109" target="_blank">00:35:09.800</a></span> | <span class="t">in terms of existing APIs, which will implement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2112" target="_blank">00:35:12.800</a></span> | <span class="t">for instance, linear layers or different activation functions that we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2117" target="_blank">00:35:17.800</a></span> | <span class="t">So we're going to import torch.nn because that is the neural network package</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2122" target="_blank">00:35:22.800</a></span> | <span class="t">that we're going to make use of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2124" target="_blank">00:35:24.800</a></span> | <span class="t">And so let's start with the linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2127" target="_blank">00:35:27.800</a></span> | <span class="t">The way the linear layer works in PyTorch is it takes in two arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2131" target="_blank">00:35:31.800</a></span> | <span class="t">It takes in the input dimension and then the output dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2136" target="_blank">00:35:36.800</a></span> | <span class="t">And so pretty much what it does is it takes in some input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2141" target="_blank">00:35:41.800</a></span> | <span class="t">which has some arbitrary amount of dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2145" target="_blank">00:35:45.800</a></span> | <span class="t">and then finally the input dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2148" target="_blank">00:35:48.800</a></span> | <span class="t">And it will essentially output it to that same set of dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2152" target="_blank">00:35:52.800</a></span> | <span class="t">except the output dimension in the very last place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2156" target="_blank">00:35:56.800</a></span> | <span class="t">And you can think of the linear layer as essentially just performing a simple AX plus B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2162" target="_blank">00:36:02.800</a></span> | <span class="t">By default, it's going to, um, it's going to apply a bias,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2168" target="_blank">00:36:08.800</a></span> | <span class="t">but you can also disable that if you don't want a bias term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2172" target="_blank">00:36:12.800</a></span> | <span class="t">And so let's look at a small example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2174" target="_blank">00:36:14.800</a></span> | <span class="t">So- so here we have our input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2185" target="_blank">00:36:25.800</a></span> | <span class="t">and we're going to create a linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2189" target="_blank">00:36:29.800</a></span> | <span class="t">in this case, as an input size of four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2192" target="_blank">00:36:32.800</a></span> | <span class="t">an output size of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2195" target="_blank">00:36:35.800</a></span> | <span class="t">And all we're going to do is once we define it by instantiating with nn dot linear,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2201" target="_blank">00:36:41.800</a></span> | <span class="t">whatever the name of our layer is, in this case, we called it linear,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2205" target="_blank">00:36:45.800</a></span> | <span class="t">we just essentially apply it with parentheses as if it were a function to whatever input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2210" target="_blank">00:36:50.800</a></span> | <span class="t">And that actually does the actual forward pass through this linear layer to get our output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2220" target="_blank">00:37:00.800</a></span> | <span class="t">And so you can see that the original shape was two by three by four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2226" target="_blank">00:37:06.800</a></span> | <span class="t">Then we pass it through this linear layer which has an output dimension of size two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2230" target="_blank">00:37:10.800</a></span> | <span class="t">And so ultimately our output is two by three by two, which is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2235" target="_blank">00:37:15.800</a></span> | <span class="t">That's what we expect. That's not shape error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2238" target="_blank">00:37:18.800</a></span> | <span class="t">But, you know, something common, um, that you'll see is, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2241" target="_blank">00:37:21.800</a></span> | <span class="t">maybe, uh, you decide to- you get a little confused and maybe you do, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2249" target="_blank">00:37:29.800</a></span> | <span class="t">let's say, uh, two by two, you match the wrong dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2254" target="_blank">00:37:34.800</a></span> | <span class="t">And so here we're going to get, uh, shape error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2258" target="_blank">00:37:38.800</a></span> | <span class="t">And you see that the error message isn't as helpful because it's actually changed the shape of what we were working with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2263" target="_blank">00:37:43.800</a></span> | <span class="t">We said this was two by three by four under the hood,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2266" target="_blank">00:37:46.800</a></span> | <span class="t">PyTorch has changed this to a six by four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2268" target="_blank">00:37:48.800</a></span> | <span class="t">But if we, you know, in this case it's obvious because we instantiated it with the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2275" target="_blank">00:37:55.800</a></span> | <span class="t">But if we didn't have the shape, then one simple thing we could do is actually just print out the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2281" target="_blank">00:38:01.800</a></span> | <span class="t">And we'd see, okay, this last dimension is size four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2283" target="_blank">00:38:03.800</a></span> | <span class="t">so I actually need to change my input dimension in my linear layer to be size four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2294" target="_blank">00:38:14.800</a></span> | <span class="t">And you'll also notice on this, um, output we have this grad function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2298" target="_blank">00:38:18.800</a></span> | <span class="t">And so that's because we're actually computing and storing the gradients here, uh, for our tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2311" target="_blank">00:38:31.800</a></span> | <span class="t">Yeah, so typically we think of the first dimension as the batch dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2316" target="_blank">00:38:36.800</a></span> | <span class="t">So in this case it's set n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2317" target="_blank">00:38:37.800</a></span> | <span class="t">This, you can think of as if you had a batch of images, it would be the number of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2321" target="_blank">00:38:41.800</a></span> | <span class="t">If you had a training corpus of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2323" target="_blank">00:38:43.800</a></span> | <span class="t">it would be essentially the number of sentences or sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2328" target="_blank">00:38:48.800</a></span> | <span class="t">Um, pretty much that is usually considered the batch dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2332" target="_blank">00:38:52.800</a></span> | <span class="t">The star indicates that there can be an arbitrary number of dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2335" target="_blank">00:38:55.800</a></span> | <span class="t">So for instance, if we had images, this could be a four-dimensional tensor object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2341" target="_blank">00:39:01.800</a></span> | <span class="t">It could be the batch size by the number of channels, by the height, by the width.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2346" target="_blank">00:39:06.800</a></span> | <span class="t">But in general, there's no fixed number of dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2350" target="_blank">00:39:10.800</a></span> | <span class="t">Your input tensor can be any number of dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2353" target="_blank">00:39:13.800</a></span> | <span class="t">The key is just that that last dimension needs to match up with the input dimension of your linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2360" target="_blank">00:39:20.800</a></span> | <span class="t">The two is the output size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2362" target="_blank">00:39:22.800</a></span> | <span class="t">So essentially we're saying that we're going to map this last dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2368" target="_blank">00:39:28.800</a></span> | <span class="t">which is four-dimensional to now two-dimensional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2371" target="_blank">00:39:31.800</a></span> | <span class="t">Um, so in general, you know, you can think of this as if we're stacking a neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2375" target="_blank">00:39:35.800</a></span> | <span class="t">this is kind of the input dimension size, and this would be like the hidden dimension size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2383" target="_blank">00:39:43.800</a></span> | <span class="t">And so one thing we can do is we can actually print out the parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2386" target="_blank">00:39:46.800</a></span> | <span class="t">and we can actually see what are the values of our linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2389" target="_blank">00:39:49.800</a></span> | <span class="t">or in general for any layer that we define in our neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2393" target="_blank">00:39:53.800</a></span> | <span class="t">what are the actual parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2395" target="_blank">00:39:55.800</a></span> | <span class="t">And in this case, we see that there's two sets of parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2400" target="_blank">00:40:00.800</a></span> | <span class="t">because we have a bias as well as the actual, um, the actual linear layer itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2407" target="_blank">00:40:07.800</a></span> | <span class="t">And so both of them store the gradients, and in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2413" target="_blank">00:40:13.800</a></span> | <span class="t">um, you know, these are, these are what the current values of these parameters are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2419" target="_blank">00:40:19.800</a></span> | <span class="t">and they'll change as we train the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2423" target="_blank">00:40:23.800</a></span> | <span class="t">Okay, so now let's go through some of the other module layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2427" target="_blank">00:40:27.800</a></span> | <span class="t">Um, so in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2432" target="_blank">00:40:32.800</a></span> | <span class="t">nn.linear is one of the layers you have access to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2435" target="_blank">00:40:35.800</a></span> | <span class="t">You have a couple of other different layers that are pretty common.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2438" target="_blank">00:40:38.800</a></span> | <span class="t">You have 2D convolutions, you have transpose convolutions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2442" target="_blank">00:40:42.800</a></span> | <span class="t">you have batch norm layers when you need to do normalization in your network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2446" target="_blank">00:40:46.800</a></span> | <span class="t">You can do upsampling, you can do max pooling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2449" target="_blank">00:40:49.800</a></span> | <span class="t">you can do lots of different operators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2451" target="_blank">00:40:51.800</a></span> | <span class="t">But the main key here is that all of them are built-in building blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2454" target="_blank">00:40:54.800</a></span> | <span class="t">that you can just call, just like we did with nn.linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2458" target="_blank">00:40:58.800</a></span> | <span class="t">And so let's just go, I guess, I'm running out of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2463" target="_blank">00:41:03.800</a></span> | <span class="t">but let's just try and go through these last few layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2466" target="_blank">00:41:06.800</a></span> | <span class="t">and then I'll wrap up by kind of showing an example that puts it all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2470" target="_blank">00:41:10.800</a></span> | <span class="t">So in this case, we can define an activation function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2474" target="_blank">00:41:14.800</a></span> | <span class="t">which is typical with our networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2476" target="_blank">00:41:16.800</a></span> | <span class="t">We need to introduce nonlinearities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2478" target="_blank">00:41:18.800</a></span> | <span class="t">In this case, we use the sigmoid function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2480" target="_blank">00:41:20.800</a></span> | <span class="t">And so now we can define our, our network as this very simple thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2483" target="_blank">00:41:23.800</a></span> | <span class="t">which had one linear layer and then an activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2488" target="_blank">00:41:28.800</a></span> | <span class="t">And in general, when we compose these layers together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2492" target="_blank">00:41:32.800</a></span> | <span class="t">we don't need to actually write every single line by line applying the next layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2497" target="_blank">00:41:37.800</a></span> | <span class="t">We can actually stack all of them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2499" target="_blank">00:41:39.800</a></span> | <span class="t">In this case, we can use nn.sequential and list all of the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2503" target="_blank">00:41:43.800</a></span> | <span class="t">So here we have our linear layer followed by our sigmoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2507" target="_blank">00:41:47.800</a></span> | <span class="t">And then now we're just essentially passing the input through this whole set of layers all at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2514" target="_blank">00:41:54.800</a></span> | <span class="t">So we take our input, we call a block on the input, and we get the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2520" target="_blank">00:42:00.800</a></span> | <span class="t">And so let's just kind of see putting it all together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2523" target="_blank">00:42:03.800</a></span> | <span class="t">what does it look like to define a network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2525" target="_blank">00:42:05.800</a></span> | <span class="t">and what does it look like when we train one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2527" target="_blank">00:42:07.800</a></span> | <span class="t">So here we're going to actually define a multilayer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2531" target="_blank">00:42:11.800</a></span> | <span class="t">And the way it works is to define a neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2534" target="_blank">00:42:14.800</a></span> | <span class="t">you extend the nn.module class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2537" target="_blank">00:42:17.800</a></span> | <span class="t">The key here is there's really two main things you have to define when you create your own network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2541" target="_blank">00:42:21.800</a></span> | <span class="t">One is the initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2543" target="_blank">00:42:23.800</a></span> | <span class="t">So in the init function, you actually initialize all the parameters you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2547" target="_blank">00:42:27.800</a></span> | <span class="t">In this case, we initialize an input size, a hidden size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2551" target="_blank">00:42:31.800</a></span> | <span class="t">and we actually define the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2554" target="_blank">00:42:34.800</a></span> | <span class="t">In this case, it's a simple model which consists of a linear layer followed by an activation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2561" target="_blank">00:42:41.800</a></span> | <span class="t">followed by another linear layer, followed by a final activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2565" target="_blank">00:42:45.800</a></span> | <span class="t">And the second function we have to define is the forward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2568" target="_blank">00:42:48.800</a></span> | <span class="t">which actually does the forward pass of the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2571" target="_blank">00:42:51.800</a></span> | <span class="t">And so here our forward function takes in our input x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2576" target="_blank">00:42:56.800</a></span> | <span class="t">In general, it could take in some arbitrary amount of inputs into this function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2581" target="_blank">00:43:01.800</a></span> | <span class="t">but essentially it needs to figure out how are you actually computing the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2585" target="_blank">00:43:05.800</a></span> | <span class="t">And in this case, it's very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2587" target="_blank">00:43:07.800</a></span> | <span class="t">It just takes in the input x and returns it back into the network that we just defined and return the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2594" target="_blank">00:43:14.800</a></span> | <span class="t">And again, you could do this more explicitly by kind of doing what we did earlier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2599" target="_blank">00:43:19.800</a></span> | <span class="t">where we could actually write out all of the layers individually instead of wrapping them into one object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2606" target="_blank">00:43:26.800</a></span> | <span class="t">and then doing a line-by-line operation for each one of these layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2613" target="_blank">00:43:33.800</a></span> | <span class="t">If we define our class, it's very simple to use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2616" target="_blank">00:43:36.800</a></span> | <span class="t">We can now just instantiate some input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2619" target="_blank">00:43:39.800</a></span> | <span class="t">instantiate our model by calling multilayer perceptron with our parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2623" target="_blank">00:43:43.800</a></span> | <span class="t">and then just pass it through our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2628" target="_blank">00:43:48.800</a></span> | <span class="t">So that's great, but this is all just the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2631" target="_blank">00:43:51.800</a></span> | <span class="t">How do we actually train the network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2633" target="_blank">00:43:53.800</a></span> | <span class="t">How do we actually make it better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2635" target="_blank">00:43:55.800</a></span> | <span class="t">And so this is the final step, which is we have optimization built in to PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2640" target="_blank">00:44:00.800</a></span> | <span class="t">So we have this backward function, which goes and computes all these gradients in the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2645" target="_blank">00:44:05.800</a></span> | <span class="t">And now the only step left is to actually update the parameters using those gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2650" target="_blank">00:44:10.800</a></span> | <span class="t">And so here, we'll import the torch.optim package, which contains all of the optimizers that you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2658" target="_blank">00:44:18.800</a></span> | <span class="t">Essentially, this part is just creating some random data so that we can actually decide how to fit our data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2665" target="_blank">00:44:25.800</a></span> | <span class="t">But this is really the key here, which is we'll instantiate our model that we defined.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2670" target="_blank">00:44:30.800</a></span> | <span class="t">We'll define the atom optimizer, and we'll define it with a particular learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2677" target="_blank">00:44:37.800</a></span> | <span class="t">We'll define a loss function, which is again another built-in module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2681" target="_blank">00:44:41.800</a></span> | <span class="t">In this case, we're using the cross-entropy loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2684" target="_blank">00:44:44.800</a></span> | <span class="t">And finally, to calculate our predictions, all we do simply is just call model on our actual input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2691" target="_blank">00:44:51.800</a></span> | <span class="t">And to calculate our loss, we just call our loss function on our predictions and our true labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2697" target="_blank">00:44:57.800</a></span> | <span class="t">And we extract the scalar here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2700" target="_blank">00:45:00.800</a></span> | <span class="t">And now when we put it all together, this is what the training loop looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2705" target="_blank">00:45:05.800</a></span> | <span class="t">We have some number of epochs that we want to train our network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2708" target="_blank">00:45:08.800</a></span> | <span class="t">For each of these epochs, the first thing we do is we take our optimizer and we zero out the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2713" target="_blank">00:45:13.800</a></span> | <span class="t">And the reason we do that is because, like many of you noted, we actually are accumulating the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2719" target="_blank">00:45:19.800</a></span> | <span class="t">We're not resetting it every time we call dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2721" target="_blank">00:45:21.800</a></span> | <span class="t">So we zero out the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2723" target="_blank">00:45:23.800</a></span> | <span class="t">We get our model predictions by doing a forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2727" target="_blank">00:45:27.800</a></span> | <span class="t">We then compute the loss between the predictions and the true values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2733" target="_blank">00:45:33.800</a></span> | <span class="t">Finally, we call loss dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2735" target="_blank">00:45:35.800</a></span> | <span class="t">This is what actually computes all of the gradients in the backward pass from our loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2741" target="_blank">00:45:41.800</a></span> | <span class="t">And the final step is we call dot step on our optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2744" target="_blank">00:45:44.800</a></span> | <span class="t">In this case, we're using atom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2747" target="_blank">00:45:47.800</a></span> | <span class="t">And this will take a step on our loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2749" target="_blank">00:45:49.800</a></span> | <span class="t">And so if we run this code, we end up seeing that we're able to start with some training loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2755" target="_blank">00:45:55.800</a></span> | <span class="t">which is relatively high, and in 10 epochs, we're able to essentially completely fit our data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2762" target="_blank">00:46:02.800</a></span> | <span class="t">And if we print out our model parameters and we printed them out from the start as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2766" target="_blank">00:46:06.800</a></span> | <span class="t">we'd see that they've changed as we've actually done this optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2771" target="_blank">00:46:11.800</a></span> | <span class="t">And so I'll kind of wrap it up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2773" target="_blank">00:46:13.800</a></span> | <span class="t">But I think the key takeaway is that a lot of the things that you're doing at the beginning of this class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2779" target="_blank">00:46:19.800</a></span> | <span class="t">are really about understanding the basics of how neural networks work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2783" target="_blank">00:46:23.800</a></span> | <span class="t">how you actually implement them, how you implement the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2787" target="_blank">00:46:27.800</a></span> | <span class="t">The great thing about PyTorch is that once you get to the very next assignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2790" target="_blank">00:46:30.800</a></span> | <span class="t">you'll see that now that you have a good underlying understanding of those things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2794" target="_blank">00:46:34.800</a></span> | <span class="t">you can abstract a lot of the complexity of how do you do backprop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2798" target="_blank">00:46:38.800</a></span> | <span class="t">how do you store all these gradients, how do you compute them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2801" target="_blank">00:46:41.800</a></span> | <span class="t">how do you actually run the optimizer, and let PyTorch handle all of that for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2805" target="_blank">00:46:45.800</a></span> | <span class="t">And you can use all of these building blocks, all these different neural network layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2809" target="_blank">00:46:49.800</a></span> | <span class="t">to now define your own networks that you can use to solve whatever problems you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2814" target="_blank">00:46:54.800</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Uv0AIRr3ptg&t=2816" target="_blank">00:46:56.800</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>