<html><head><title>Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Quantization explained with PyTorch - Post-Training Quantization, Quantization-Aware Training</h2><a href="https://www.youtube.com/watch?v=0VdNflU08yA"><img src="https://i.ytimg.com/vi_webp/0VdNflU08yA/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=70">1:10</a> What is quantization?<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=222">3:42</a> Integer representation<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=445">7:25</a> Floating-point representation<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=556">9:16</a> Quantization (details)<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=830">13:50</a> Asymmetric vs Symmetric Quantization<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=938">15:38</a> Asymmetric Quantization<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1114">18:34</a> Symmetric Quantization<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1257">20:57</a> Asymmetric vs Symmetric Quantization (Python Code)<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1456">24:16</a> Dynamic Quantization & Calibration<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1677">27:57</a> Multiply-Accumulate Block<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1805">30:5</a> Range selection strategies<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2080">34:40</a> Quantization granularity<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2149">35:49</a> Post-Training Quantization<br><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2585">43:5</a> Training-Aware Quantization<br><br><div style="text-align: left;"><a href="./0VdNflU08yA.html">Whisper Transcript</a> | <a href="./transcript_0VdNflU08yA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are gonna talk about quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=4" target="_blank">00:00:04.000</a></span> | <span class="t">Let's review the topics of today. I will start by showing what is quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=9" target="_blank">00:00:09.000</a></span> | <span class="t">and why we need quantization and later we will briefly introduce what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=13" target="_blank">00:00:13.400</a></span> | <span class="t">numerical representation for integers and floating-point numbers in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=16" target="_blank">00:00:16.560</a></span> | <span class="t">hardware, so in CPUs and GPUs. I will show you later what is quantization at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=21" target="_blank">00:00:21.920</a></span> | <span class="t">neural network level by giving you some examples and later we will go into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=27" target="_blank">00:00:27.000</a></span> | <span class="t">detail of the types of quantization, so the asymmetric and the symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=30" target="_blank">00:00:30.120</a></span> | <span class="t">quantization, what we mean by the range and the granularity and later we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=34" target="_blank">00:00:34.400</a></span> | <span class="t">see also post-training quantization and quantization-aware training. For all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=38" target="_blank">00:00:38.540</a></span> | <span class="t">these topics I will also show you the Python, the PyTorch and the Python code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=42" target="_blank">00:00:42.560</a></span> | <span class="t">on how to do it from scratch. So actually we will build the asymmetric and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=46" target="_blank">00:00:46.840</a></span> | <span class="t">quantization and the symmetric quantization from scratch using PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">and then later we will also apply it to a sample neural network using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=53" target="_blank">00:00:53.600</a></span> | <span class="t">post-training quantization and quantization-aware training. What do I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=57" target="_blank">00:00:57.380</a></span> | <span class="t">expect you guys to already know before watching this video is basically you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=61" target="_blank">00:01:01.320</a></span> | <span class="t">have some basic understanding of neural networks and then you have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=65" target="_blank">00:01:05.740</a></span> | <span class="t">background in mathematics, just high school mathematics is enough. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=71" target="_blank">00:01:11.060</a></span> | <span class="t">start our journey. Let's see what is quantization first of all. So quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=75" target="_blank">00:01:15.680</a></span> | <span class="t">aims to solve a problem. The problem is that most modern deep neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=79" target="_blank">00:01:19.720</a></span> | <span class="t">are made up of millions if not billions of parameters. For example the smallest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=84" target="_blank">00:01:24.480</a></span> | <span class="t">Lama 2 has a 7 billion parameters. Now if every parameter is a 32-bit then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=90" target="_blank">00:01:30.600</a></span> | <span class="t">need 28 gigabyte just to store the parameters on the disk. Also when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=95" target="_blank">00:01:35.900</a></span> | <span class="t">inference the model we need to load all the parameters of the model in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=99" target="_blank">00:01:39.320</a></span> | <span class="t">memory. If we are using the CPU for example for inference then we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=102" target="_blank">00:01:42.660</a></span> | <span class="t">load it in the RAM but if you are using the GPU we need to load it in the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=106" target="_blank">00:01:46.240</a></span> | <span class="t">of the GPU. Of course big models cannot easily be loaded inside the CPU, the RAM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=112" target="_blank">00:01:52.640</a></span> | <span class="t">or the GPU in case we are using a standard PC or small device like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=116" target="_blank">00:01:56.480</a></span> | <span class="t">smartphone. And also just like humans computers are slow at computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=123" target="_blank">00:02:03.080</a></span> | <span class="t">floating-point operations compared to integer operations. For example if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=127" target="_blank">00:02:07.320</a></span> | <span class="t">try to do mentally 3 multiplied by 6 and also mentally 1.21 multiplied by 2.897</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=134" target="_blank">00:02:14.560</a></span> | <span class="t">of course you are able to do much faster the 3 by 6 multiplication and the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=139" target="_blank">00:02:19.360</a></span> | <span class="t">goes on with computers. So the solution is quantization. Quantization basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=145" target="_blank">00:02:25.000</a></span> | <span class="t">aims to reduce the number of the amount of bits required to represent each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=150" target="_blank">00:02:30.280</a></span> | <span class="t">parameter using by usually by converting the floating-point numbers into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=154" target="_blank">00:02:34.800</a></span> | <span class="t">integers. This way for example a model that normally occupies many gigabyte</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=159" target="_blank">00:02:39.320</a></span> | <span class="t">can be compressed to much less smaller size. Also please note that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=164" target="_blank">00:02:44.840</a></span> | <span class="t">quantization doesn't mean that we just round up or round down all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=168" target="_blank">00:02:48.600</a></span> | <span class="t">floating-point numbers to the nearest integer, this is not what quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=172" target="_blank">00:02:52.300</a></span> | <span class="t">does. We will see later how it works so please don't be confused. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=177" target="_blank">00:02:57.120</a></span> | <span class="t">quantization can also speed up computation because as working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=180" target="_blank">00:03:00.520</a></span> | <span class="t">smaller data types is faster for example the computer is much faster as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=184" target="_blank">00:03:04.720</a></span> | <span class="t">multiplying matrices made up of integers than two matrices made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=189" target="_blank">00:03:09.360</a></span> | <span class="t">floating-point numbers. And later we will see actually how this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=193" target="_blank">00:03:13.720</a></span> | <span class="t">multiplication works at the GPU level also. So what is the advantage of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=198" target="_blank">00:03:18.720</a></span> | <span class="t">quantization? First of all we have less memory consumption when loading models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=202" target="_blank">00:03:22.560</a></span> | <span class="t">so the model can be compressed into a much smaller size and we have less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=207" target="_blank">00:03:27.720</a></span> | <span class="t">inference time because of using simpler data types so for example integers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=212" target="_blank">00:03:32.440</a></span> | <span class="t">instead of floating-point numbers. And these two combinations lead to less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=216" target="_blank">00:03:36.720</a></span> | <span class="t">energy consumption which is very important for like for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=220" target="_blank">00:03:40.000</a></span> | <span class="t">smartphones. Okay now let's go review how numbers are represented in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=226" target="_blank">00:03:46.000</a></span> | <span class="t">hardware so in the CPU level or in the GPU level. So computers use a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=232" target="_blank">00:03:52.120</a></span> | <span class="t">fixed number of bits to represent any piece of data. For example to represent a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=236" target="_blank">00:03:56.880</a></span> | <span class="t">number or a character or a pixel color we always use the fixed number of bit. A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=240" target="_blank">00:04:00.800</a></span> | <span class="t">bit string that is made up of n bits can represent up to 2 to the power of n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=245" target="_blank">00:04:05.360</a></span> | <span class="t">distinct numbers. For example with 3 bit we can represent 8 possible numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=250" target="_blank">00:04:10.720</a></span> | <span class="t">from 0 to 7 and for each number you can see its binary representation. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=256" target="_blank">00:04:16.560</a></span> | <span class="t">always convert the binary representation in the decimal representation by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=260" target="_blank">00:04:20.960</a></span> | <span class="t">multiplying each digit with the power of 2 to the power of its position to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=267" target="_blank">00:04:27.960</a></span> | <span class="t">to the position of the digit inside the bit string. And in most CPUs actually the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=274" target="_blank">00:04:34.240</a></span> | <span class="t">numbers the integer numbers are represented using the twos complement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=278" target="_blank">00:04:38.200</a></span> | <span class="t">which means that the first bit of the number indicates the sign so 0 means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=283" target="_blank">00:04:43.480</a></span> | <span class="t">positive and 1 means negative while the rest of the bits indicate the absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=289" target="_blank">00:04:49.000</a></span> | <span class="t">value of the number in case it's positive or its complement in case it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=292" target="_blank">00:04:52.360</a></span> | <span class="t">negative. The reason we use the twos complement is because we want one unique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=297" target="_blank">00:04:57.120</a></span> | <span class="t">representation for the zero so the plus zero and the minus zero have the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=300" target="_blank">00:05:00.920</a></span> | <span class="t">binary representation. But of course you may argue okay computers use a fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=306" target="_blank">00:05:06.200</a></span> | <span class="t">number of bits to represent numbers but how can Python handle such big numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=311" target="_blank">00:05:11.800</a></span> | <span class="t">without any problems like when you run 2 to the power of 9 9 9 9 on Python you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=317" target="_blank">00:05:17.200</a></span> | <span class="t">will get a result which is much bigger than any 64-bit number and how can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=323" target="_blank">00:05:23.080</a></span> | <span class="t">Python handle these huge numbers without any problem? Well Python uses the so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=327" target="_blank">00:05:27.760</a></span> | <span class="t">called the big num arithmetic so as we saw before in this table the number 6</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=334" target="_blank">00:05:34.280</a></span> | <span class="t">when it's represented in base 10 only needs one digit but when it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=338" target="_blank">00:05:38.520</a></span> | <span class="t">represented in a base 2 it reads three digits so this is actually a rule so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=343" target="_blank">00:05:43.880</a></span> | <span class="t">smaller the base the bigger the number of digits we need to represent the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=347" target="_blank">00:05:47.880</a></span> | <span class="t">number and Python does the inverse so it saves all these numbers as an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=353" target="_blank">00:05:53.000</a></span> | <span class="t">array of digits in which each digit is the digit of the number in base 2 to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=358" target="_blank">00:05:58.040</a></span> | <span class="t">power of 30 so overall we need less digits to store very big numbers for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=362" target="_blank">00:06:02.960</a></span> | <span class="t">example if this number which is the result of 2 to the power of 9 9 9 9 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=367" target="_blank">00:06:07.600</a></span> | <span class="t">represented as a decimal number we would need an array of 3,000 digits to store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=372" target="_blank">00:06:12.720</a></span> | <span class="t">it in memory while Python stores this number as an array of digits in base 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=379" target="_blank">00:06:19.240</a></span> | <span class="t">to the power of 30 so it only needs 334 elements in which all the elements are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=386" target="_blank">00:06:26.880</a></span> | <span class="t">zero except the most significant one which is equal to 512 and as a matter of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=391" target="_blank">00:06:31.880</a></span> | <span class="t">fact you can check by yourself that by doing 512 multiplied by the base so 2 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=398" target="_blank">00:06:38.360</a></span> | <span class="t">the power of 30 then to the power of the position of this digit in the array we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=404" target="_blank">00:06:44.480</a></span> | <span class="t">will obtain the number 2 to the power of 9 9 9 9 I also want you to notice that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=409" target="_blank">00:06:49.760</a></span> | <span class="t">this is something that is implemented by CPython which is the Python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=413" target="_blank">00:06:53.760</a></span> | <span class="t">interpreter not by the CPU so it's not the CPU that is doing this big num</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=418" target="_blank">00:06:58.400</a></span> | <span class="t">arithmetic for us it's the Python interpreter for example when you compile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=422" target="_blank">00:07:02.440</a></span> | <span class="t">C++ code the code will run directly on the hardware on the CPU which means also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=428" target="_blank">00:07:08.080</a></span> | <span class="t">that the C++ code is compiled for the specific hardware it will run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=433" target="_blank">00:07:13.320</a></span> | <span class="t">on while Python code we never compile it because the CPython will take care of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=437" target="_blank">00:07:17.760</a></span> | <span class="t">translating our Python instructions into machine code and in a process called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=442" target="_blank">00:07:22.920</a></span> | <span class="t">just-in-time compilation okay let's review how floating-point numbers are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=448" target="_blank">00:07:28.560</a></span> | <span class="t">represented now decimal numbers are just numbers that also include the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=454" target="_blank">00:07:34.240</a></span> | <span class="t">powers of the base for example the number 85.612 can be written as each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=459" target="_blank">00:07:39.640</a></span> | <span class="t">number multiplied so each digit multiplied by a power of the base which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=464" target="_blank">00:07:44.400</a></span> | <span class="t">is 10 but the decimal part have negative powers of 10 as you can see 10 to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=469" target="_blank">00:07:49.880</a></span> | <span class="t">power of minus 1 minus 2 and minus 3 and this same reasoning is used to in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=474" target="_blank">00:07:54.800</a></span> | <span class="t">standard IEEE 754 which defines the representation of floating-point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=480" target="_blank">00:08:00.320</a></span> | <span class="t">numbers in 32-bit basically we divided the 32-bit string into three parts the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=486" target="_blank">00:08:06.200</a></span> | <span class="t">first bit indicates the sign so 0 means positive the next 8 bit indicated the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=491" target="_blank">00:08:11.920</a></span> | <span class="t">exponent which also indicates the magnitude of the number so how big is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=495" target="_blank">00:08:15.560</a></span> | <span class="t">the number and the last 23 bits indicate the fractional part of the number so all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=501" target="_blank">00:08:21.520</a></span> | <span class="t">the digits corresponding to the negative powers of 2 to convert this bit string</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=507" target="_blank">00:08:27.880</a></span> | <span class="t">into a value decimal value we just need to do this so the sign multiplied by 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=512" target="_blank">00:08:32.660</a></span> | <span class="t">to the power of the exponent minus 127 multiplied by the fraction 1 plus all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=519" target="_blank">00:08:39.240</a></span> | <span class="t">the negative powers of 2 and should correspond to the number 0.15625</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=527" target="_blank">00:08:47.680</a></span> | <span class="t">most modern GPUs also support a 16-bit floating-point number but of course this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=534" target="_blank">00:08:54.360</a></span> | <span class="t">results in less precision because we have less bits dedicated to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=538" target="_blank">00:08:58.160</a></span> | <span class="t">fractional part the last bit dedicated to the exponent and of course they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=543" target="_blank">00:09:03.140</a></span> | <span class="t">smaller so they have less it means that they can represent the floating-point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=547" target="_blank">00:09:07.800</a></span> | <span class="t">numbers with less precision so we don't we cannot have too many digits after the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=552" target="_blank">00:09:12.160</a></span> | <span class="t">comma for example okay let's go inside the details of quantization now first of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=559" target="_blank">00:09:19.920</a></span> | <span class="t">all we review how neural networks work so we start with an input which could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=564" target="_blank">00:09:24.180</a></span> | <span class="t">a tensor and we give it to a layer which could be a linear layer for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=568" target="_blank">00:09:28.200</a></span> | <span class="t">which then maps to another linear layer and finally we have an output we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=573" target="_blank">00:09:33.780</a></span> | <span class="t">usually a target we compare the output and the target through a loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=578" target="_blank">00:09:38.440</a></span> | <span class="t">and we calculate the gradient of the loss function with respect to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=582" target="_blank">00:09:42.360</a></span> | <span class="t">parameter and we run back propagation to update these parameters the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=588" target="_blank">00:09:48.960</a></span> | <span class="t">network can be made up of many different layers for example a linear layer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=593" target="_blank">00:09:53.340</a></span> | <span class="t">made up of two matrices one is called the weight and one is called the bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=597" target="_blank">00:09:57.320</a></span> | <span class="t">which are commonly represented using floating-point numbers quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=602" target="_blank">00:10:02.120</a></span> | <span class="t">aims to use integer numbers to represent these two matrices while maintaining the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=606" target="_blank">00:10:06.960</a></span> | <span class="t">accuracy of the model let's see how so this linear layer for example the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=612" target="_blank">00:10:12.180</a></span> | <span class="t">linear layer of this neural network represents an operation which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=616" target="_blank">00:10:16.080</a></span> | <span class="t">input multiplied by a weight matrix which are the parameters of this linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=622" target="_blank">00:10:22.200</a></span> | <span class="t">layer plus a bias which are also the parameters of this linear layer and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=628" target="_blank">00:10:28.520</a></span> | <span class="t">the goal of the quantization is to quantize the input the weight matrix and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=634" target="_blank">00:10:34.280</a></span> | <span class="t">the bias matrix into integers such that we perform all these operations here as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=639" target="_blank">00:10:39.720</a></span> | <span class="t">integer operations because they are much faster compared to floating-point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=644" target="_blank">00:10:44.080</a></span> | <span class="t">operations we take then the output we dequantize it and we feed it to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=649" target="_blank">00:10:49.640</a></span> | <span class="t">layer and we dequantize in such a way that the next layer should not even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=654" target="_blank">00:10:54.840</a></span> | <span class="t">realize that there have been a quantization in the previous layer so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=658" target="_blank">00:10:58.360</a></span> | <span class="t">want to do quantization in such a way that the model's output should not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=662" target="_blank">00:11:02.640</a></span> | <span class="t">change because of quantization so we want to keep the model's performance the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=667" target="_blank">00:11:07.600</a></span> | <span class="t">accuracy of the model but we want to perform all these operations using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=671" target="_blank">00:11:11.840</a></span> | <span class="t">integers so we need to find a mapping between floating-point numbers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=676" target="_blank">00:11:16.440</a></span> | <span class="t">integers and a reversible mapping of course so we can go from floating-point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=681" target="_blank">00:11:21.000</a></span> | <span class="t">to integers and from integers to floating-point but in such a way that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=685" target="_blank">00:11:25.640</a></span> | <span class="t">don't lose the precision of the model but at the same time we want to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=691" target="_blank">00:11:31.200</a></span> | <span class="t">the space occupation of the model inside the RAM and on the disk and we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=696" target="_blank">00:11:36.640</a></span> | <span class="t">make it faster to compute these operations because as we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=699" target="_blank">00:11:39.880</a></span> | <span class="t">computing integer operations is much faster than computing floating-point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=704" target="_blank">00:11:44.600</a></span> | <span class="t">operations the main benefit is that the integer operations is much faster in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=710" target="_blank">00:11:50.160</a></span> | <span class="t">most hardware than floating-point operations plus in most embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=715" target="_blank">00:11:55.040</a></span> | <span class="t">hardware especially very very small embedded device we don't even have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=719" target="_blank">00:11:59.000</a></span> | <span class="t">floating-point numbers so we are forced to use integer operations in those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=723" target="_blank">00:12:03.880</a></span> | <span class="t">devices okay let's see how it works so this hidden layer here for example may</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=729" target="_blank">00:12:09.240</a></span> | <span class="t">have a weight matrix which could be a 5 by 5 matrix that we can see here the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=734" target="_blank">00:12:14.080</a></span> | <span class="t">goal of quantization is to reduce the precision of each number that we see in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=740" target="_blank">00:12:20.760</a></span> | <span class="t">this matrix by mapping it into a range that occupies less bits so this is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=745" target="_blank">00:12:25.960</a></span> | <span class="t">floating-point number and occupies 4 bytes so 32 bits we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=752" target="_blank">00:12:32.720</a></span> | <span class="t">quantize using only 8 bits so each number should be represented only using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=757" target="_blank">00:12:37.400</a></span> | <span class="t">8 bit now with 8 bit we can represent the range from -128 to +127 but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=765" target="_blank">00:12:45.240</a></span> | <span class="t">usually we sacrifice the -128 to obtain a symmetric range so we map each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=771" target="_blank">00:12:51.200</a></span> | <span class="t">number into its 8 bit representation in such a way that we can then map back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=776" target="_blank">00:12:56.600</a></span> | <span class="t">the original array in an operation that is first called quantization and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=780" target="_blank">00:13:00.640</a></span> | <span class="t">second is called dequantization now during the quantization we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=784" target="_blank">00:13:04.840</a></span> | <span class="t">obtain the original array the original tensor or matrix but we usually lose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=791" target="_blank">00:13:11.000</a></span> | <span class="t">some precision so for example if you look at the first value it's exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=795" target="_blank">00:13:15.040</a></span> | <span class="t">same as the original matrix but the second value here is similar but not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=799" target="_blank">00:13:19.840</a></span> | <span class="t">exactly the same and this is to say that with quantization we introduce some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=805" target="_blank">00:13:25.140</a></span> | <span class="t">error so the model will not be as accurate as the not quantized model but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=811" target="_blank">00:13:31.720</a></span> | <span class="t">we want to make it the quantization process in such a way that we lose the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=816" target="_blank">00:13:36.000</a></span> | <span class="t">least accuracy possible so we don't want to lose precision so we want to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=820" target="_blank">00:13:40.720</a></span> | <span class="t">this error that we introduce okay let's go into the details of quantization now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=827" target="_blank">00:13:47.240</a></span> | <span class="t">so by reviewing the types of quantization we have available first of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=832" target="_blank">00:13:52.120</a></span> | <span class="t">all I will show you the difference between asymmetric and symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=834" target="_blank">00:13:54.760</a></span> | <span class="t">quantization so imagine we have a tensor which is made up of 10 values that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=839" target="_blank">00:13:59.760</a></span> | <span class="t">can see here the goal of asymmetric quantization is to map the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=844" target="_blank">00:14:04.640</a></span> | <span class="t">tensor which is distributed between this range so minus 44.23 which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=850" target="_blank">00:14:10.880</a></span> | <span class="t">smallest number in this tensor and 43.31 which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=855" target="_blank">00:14:15.360</a></span> | <span class="t">biggest number in this tensor we want to map it into another range that is made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=860" target="_blank">00:14:20.320</a></span> | <span class="t">up of integers that are between 0 and 255 which are the integers that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=866" target="_blank">00:14:26.200</a></span> | <span class="t">represent using 8-bit for example and if we do this operation we will obtain a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=871" target="_blank">00:14:31.640</a></span> | <span class="t">new tensor that will map for example this first number into 255 this number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=876" target="_blank">00:14:36.560</a></span> | <span class="t">here into 0 this number here into 130 etc the other type of quantization is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=883" target="_blank">00:14:43.720</a></span> | <span class="t">the symmetric quantization which aims to map a symmetric range so we take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=891" target="_blank">00:14:51.040</a></span> | <span class="t">tensor and we we treat it as a symmetric range even if it's not symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=897" target="_blank">00:14:57.400</a></span> | <span class="t">because as you can see the biggest value here is 43.31 and the smallest value is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=903" target="_blank">00:15:03.000</a></span> | <span class="t">minus 44.93 so they are not symmetric with respect to the zero but if they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=908" target="_blank">00:15:08.960</a></span> | <span class="t">then we can use a symmetric range which aims to basically map the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=913" target="_blank">00:15:13.400</a></span> | <span class="t">symmetric range into another symmetric range also using 8-bit in our case such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=918" target="_blank">00:15:18.960</a></span> | <span class="t">that however this gives you the advantage that the zero is always mapped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=922" target="_blank">00:15:22.800</a></span> | <span class="t">into the zero in the quantized numbers I will show you later how actually we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=928" target="_blank">00:15:28.840</a></span> | <span class="t">this computation so how do we compute the quantized version using the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=934" target="_blank">00:15:34.680</a></span> | <span class="t">tensor and also how to de-quantize back so let's go in the case of a symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=940" target="_blank">00:15:40.560</a></span> | <span class="t">quantization imagine we have an original tensor that is like this so these 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=946" target="_blank">00:15:46.600</a></span> | <span class="t">items we can see here we quantize using the following formula so the quantized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=952" target="_blank">00:15:52.660</a></span> | <span class="t">version of each of these numbers is equal to the floating point number so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=956" target="_blank">00:15:56.680</a></span> | <span class="t">the original floating point number divided by a parameter called S which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=960" target="_blank">00:16:00.640</a></span> | <span class="t">stands for scale we round down or round up to the nearest integer plus a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=968" target="_blank">00:16:08.080</a></span> | <span class="t">number Z and if the result of this operation is smaller than zero then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=975" target="_blank">00:16:15.560</a></span> | <span class="t">clamp it to zero and if it's bigger than 2 to the power of n minus 1 then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=980" target="_blank">00:16:20.040</a></span> | <span class="t">clamp it to 2 to the power of n minus 1 what is n? n is the number of bits that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=985" target="_blank">00:16:25.600</a></span> | <span class="t">we want to use for quantization so we want to quantize for example all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=989" target="_blank">00:16:29.840</a></span> | <span class="t">floating point numbers into 8 bits so we will choose n equal to 8 how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=995" target="_blank">00:16:35.080</a></span> | <span class="t">calculate this S parameter the S parameter is given by alpha minus beta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=999" target="_blank">00:16:39.920</a></span> | <span class="t">divided by the range of the the output range basically so how many numbers the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1004" target="_blank">00:16:44.680</a></span> | <span class="t">output range can represent what is beta and alpha they are the biggest number in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1010" target="_blank">00:16:50.640</a></span> | <span class="t">the original tensor and the smallest number in the original tensor so we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1014" target="_blank">00:16:54.600</a></span> | <span class="t">basically the range of the original tensor and we squeeze it into the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1020" target="_blank">00:17:00.040</a></span> | <span class="t">range by means of this scale parameter and then we center it using the Z</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1025" target="_blank">00:17:05.200</a></span> | <span class="t">parameter this Z parameter is computed as minus 1 multiplied by beta divided by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1032" target="_blank">00:17:12.160</a></span> | <span class="t">S and then rounded to the nearest integer so the Z parameter is an integer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1037" target="_blank">00:17:17.120</a></span> | <span class="t">while the scale parameter is not an integer it is a floating point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1041" target="_blank">00:17:21.440</a></span> | <span class="t">number if we do this operation so we take each floating point and we run it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1046" target="_blank">00:17:26.320</a></span> | <span class="t">through this formula we will obtain this quantized vector what we can see first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1053" target="_blank">00:17:33.800</a></span> | <span class="t">of all the biggest number using a symmetric quantization is always mapped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1057" target="_blank">00:17:37.040</a></span> | <span class="t">to the biggest number in the output range and the smallest number is always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1060" target="_blank">00:17:40.520</a></span> | <span class="t">mapped to the zero in the output range the zero number in the original vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1065" target="_blank">00:17:45.760</a></span> | <span class="t">is mapped into the Z parameter so this 130 is actually the Z parameter if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1070" target="_blank">00:17:50.320</a></span> | <span class="t">compute it and all the other numbers are mapped into something that is in between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1075" target="_blank">00:17:55.160</a></span> | <span class="t">0 and 255 we can then dequantize using the following formula so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1082" target="_blank">00:18:02.040</a></span> | <span class="t">dequantize to obtain the floating point number back we just need to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1085" target="_blank">00:18:05.360</a></span> | <span class="t">multiply the scale multiplied by the quantized number minus Z and we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1091" target="_blank">00:18:11.720</a></span> | <span class="t">obtain the original tensor but you should see that the numbers are similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1097" target="_blank">00:18:17.320</a></span> | <span class="t">but not exactly the same because the quantization introduces some error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1101" target="_blank">00:18:21.600</a></span> | <span class="t">because we are trying to squeeze a range that could be very big because with 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1106" target="_blank">00:18:26.400</a></span> | <span class="t">bit we can represent a very big range into a range that is much smaller with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1110" target="_blank">00:18:30.880</a></span> | <span class="t">8 bit so of course we will introduce some error let's see the symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1116" target="_blank">00:18:36.400</a></span> | <span class="t">quantization symmetric quantization as we saw before we aim to transform a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1121" target="_blank">00:18:41.280</a></span> | <span class="t">symmetric input range into a symmetric output range so imagine we still have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1125" target="_blank">00:18:45.800</a></span> | <span class="t">this tensor what we do we compute the quantized values as follows so each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1131" target="_blank">00:18:51.440</a></span> | <span class="t">number the floating point number divided by a parameter S so the scale and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1136" target="_blank">00:18:56.360</a></span> | <span class="t">clamped between these two limits this one and this one where n is the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1141" target="_blank">00:19:01.000</a></span> | <span class="t">of bits that we want to use for quantizing and the S parameter is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1145" target="_blank">00:19:05.400</a></span> | <span class="t">calculated as the absolute value of alpha where alpha is the biggest number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1150" target="_blank">00:19:10.160</a></span> | <span class="t">here in absolute terms in this case it's the number minus 44.93 because in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1156" target="_blank">00:19:16.060</a></span> | <span class="t">absolute terms is the biggest value and we can then quantize this tensor and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1163" target="_blank">00:19:23.480</a></span> | <span class="t">should obtain something like this we should notice that the the zero in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1167" target="_blank">00:19:27.840</a></span> | <span class="t">case is mapped into the zero which is very useful we can then dequantize using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1174" target="_blank">00:19:34.040</a></span> | <span class="t">the formula we can see here so to obtain the floating point number we take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1178" target="_blank">00:19:38.000</a></span> | <span class="t">quantized number multiplied by the scale parameter so the S parameter and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1182" target="_blank">00:19:42.400</a></span> | <span class="t">should obtain the original vector but of course we will lose some precision so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1187" target="_blank">00:19:47.880</a></span> | <span class="t">lose some as you can see the original number was for 43.31 the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1192" target="_blank">00:19:52.280</a></span> | <span class="t">dequantized number is a 43.16 so we lost some precision but our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1197" target="_blank">00:19:57.920</a></span> | <span class="t">goal of course is to have have it as similar as possible to the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1202" target="_blank">00:20:02.440</a></span> | <span class="t">array and there are of course the best ways to just increase the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1207" target="_blank">00:20:07.640</a></span> | <span class="t">bits of the quantization but of course we cannot just choose any number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1213" target="_blank">00:20:13.680</a></span> | <span class="t">bits because as we saw before we want to run this the matrix multiplication in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1218" target="_blank">00:20:18.120</a></span> | <span class="t">the linear layer to be accelerated by the CPU and the CPU always works with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1222" target="_blank">00:20:22.960</a></span> | <span class="t">the fixed number of bits and the operations in the side of the CPU are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1226" target="_blank">00:20:26.360</a></span> | <span class="t">optimized for a fixed number of bits so for example we have optimization for 8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1230" target="_blank">00:20:30.680</a></span> | <span class="t">bits 16 bit 32 bit and 64 bit but of course if we choose 11 bits as the for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1236" target="_blank">00:20:36.440</a></span> | <span class="t">quantization the CPU may not support the acceleration of operations using 11 bits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1242" target="_blank">00:20:42.440</a></span> | <span class="t">so we have to be careful to choose a good compromise between the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1246" target="_blank">00:20:46.600</a></span> | <span class="t">bits and also the availability of the hardware later we will also see how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1252" target="_blank">00:20:52.240</a></span> | <span class="t">GPU computes the matrix multiplication in the accelerated form okay I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1259" target="_blank">00:20:59.280</a></span> | <span class="t">shown you the symmetric and the asymmetric quantization now it's time to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1262" target="_blank">00:21:02.720</a></span> | <span class="t">actually look at the code on how it is implemented in reality let's have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1267" target="_blank">00:21:07.160</a></span> | <span class="t">okay I created a very simple notebook in which basically I generated 20 random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1274" target="_blank">00:21:14.560</a></span> | <span class="t">numbers between -50 and 150 I modified these numbers in such a way that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1280" target="_blank">00:21:20.440</a></span> | <span class="t">first number is the biggest one and the second number is the smallest one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1284" target="_blank">00:21:24.480</a></span> | <span class="t">then the third is a zero so we can check the effect of the quantization on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1288" target="_blank">00:21:28.080</a></span> | <span class="t">biggest number on the smallest number and on the zero suppose this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1292" target="_blank">00:21:32.440</a></span> | <span class="t">original numbers so this array of 20 numbers we define the functions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1297" target="_blank">00:21:37.520</a></span> | <span class="t">will quantize this vector so asymmetric quantization basically it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1303" target="_blank">00:21:43.200</a></span> | <span class="t">compute the alpha as the maximum value the beta as the minimum value it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1307" target="_blank">00:21:47.160</a></span> | <span class="t">calculate the scale and the zero using the formula that we saw on the slide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1310" target="_blank">00:21:50.800</a></span> | <span class="t">before and then it will quantize using the same formula that we saw before and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1315" target="_blank">00:21:55.080</a></span> | <span class="t">the same goes for the symmetric quantization we calculate the alpha the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1319" target="_blank">00:21:59.120</a></span> | <span class="t">scale parameter the upper bound and the lower bound for clamping and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1324" target="_blank">00:22:04.000</a></span> | <span class="t">also dequantize using the same formula that we saw on the slide so in the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1327" target="_blank">00:22:07.840</a></span> | <span class="t">of asymmetric is this one with the zero and in the case of symmetric we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1331" target="_blank">00:22:11.520</a></span> | <span class="t">have the zero because the zero is always mapped into the zero we can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1336" target="_blank">00:22:16.080</a></span> | <span class="t">calculate the quantization error by comparing the original values and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1339" target="_blank">00:22:19.840</a></span> | <span class="t">dequantized values by using the mean squared error so let's try to see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1344" target="_blank">00:22:24.960</a></span> | <span class="t">is the effect on quantization so this is our original array of floating point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1349" target="_blank">00:22:29.040</a></span> | <span class="t">numbers if we quantize it using asymmetric quantization we will obtain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1352" target="_blank">00:22:32.880</a></span> | <span class="t">this array here in which we can see that the biggest value is mapped into 255</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1359" target="_blank">00:22:39.360</a></span> | <span class="t">which is the biggest value of the output range the smallest value is mapped into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1363" target="_blank">00:22:43.960</a></span> | <span class="t">the zero and the zero is mapped into the Z parameter which is a 61 and as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1369" target="_blank">00:22:49.000</a></span> | <span class="t">can see the zero is mapped into the 61 while with the symmetric quantization we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1375" target="_blank">00:22:55.320</a></span> | <span class="t">have that the zero is mapped into the zero so the third element of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1379" target="_blank">00:22:59.080</a></span> | <span class="t">original vector is mapped into the third element of the symmetric range and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1382" target="_blank">00:23:02.600</a></span> | <span class="t">the zero if we dequantize back the quantized parameters we will see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1388" target="_blank">00:23:08.760</a></span> | <span class="t">they are similar to the original vector but not exactly the same as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1394" target="_blank">00:23:14.640</a></span> | <span class="t">we lose a little bit of the precision and we can measure this precision using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1398" target="_blank">00:23:18.640</a></span> | <span class="t">the mean squared error for example and we can see that the error is much bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1402" target="_blank">00:23:22.800</a></span> | <span class="t">for the symmetric quantization why because the original vector is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1408" target="_blank">00:23:28.680</a></span> | <span class="t">symmetric the original vector is between -50 and 150 so what we are doing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1415" target="_blank">00:23:35.120</a></span> | <span class="t">symmetric quantization is that we are calculating let's see here with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1419" target="_blank">00:23:39.440</a></span> | <span class="t">symmetric quantization basically we are checking the biggest value in absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1424" target="_blank">00:23:44.960</a></span> | <span class="t">terms so the biggest value in absolute terms is 127 which will means that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1430" target="_blank">00:23:50.320</a></span> | <span class="t">symmetric quantization will map a range that is between -127 and +127 but all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1437" target="_blank">00:23:57.620</a></span> | <span class="t">numbers between -127 and -40 don't do not appear in this array so we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1443" target="_blank">00:24:03.840</a></span> | <span class="t">sacrificing a lot of the range will be unused and that's why all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1448" target="_blank">00:24:08.160</a></span> | <span class="t">numbers will suffer from this bad distribution let's say and this is why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1454" target="_blank">00:24:14.040</a></span> | <span class="t">the symmetric quantization has a bigger error okay let's review again how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1459" target="_blank">00:24:19.160</a></span> | <span class="t">quantization will work in our case of the linear layer so if we never quantize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1464" target="_blank">00:24:24.280</a></span> | <span class="t">this network we will have a weight matrix a bias matrix the output of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1469" target="_blank">00:24:29.560</a></span> | <span class="t">layer will be a weight multiplied by the input of this layer plus the bias and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1474" target="_blank">00:24:34.760</a></span> | <span class="t">the output will be another floating-point number so all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1477" target="_blank">00:24:37.640</a></span> | <span class="t">matrices are floating-point numbers but when we quantize we quantize the weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1483" target="_blank">00:24:43.560</a></span> | <span class="t">matrix which is a fixed matrix because we pretend the network has already been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1487" target="_blank">00:24:47.300</a></span> | <span class="t">trained so the weight matrix is fixed and we can quantize it by calculating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1492" target="_blank">00:24:52.320</a></span> | <span class="t">the alpha and the beta that we saw before using the symmetric quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1495" target="_blank">00:24:55.900</a></span> | <span class="t">or the asymmetric quantization the beta parameters can also be quantized because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1500" target="_blank">00:25:00.960</a></span> | <span class="t">it's a fixed vector and we can calculate the alpha and the beta of this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1506" target="_blank">00:25:06.080</a></span> | <span class="t">and we can quantize using 8 bits we want our goal is to perform all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1512" target="_blank">00:25:12.080</a></span> | <span class="t">operations using integers so how can we quantize the X matrix because this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1516" target="_blank">00:25:16.880</a></span> | <span class="t">the X matrix is an input which depends on the input the network receives one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1522" target="_blank">00:25:22.480</a></span> | <span class="t">way is called the dynamic quantization dynamic quantization means that for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1526" target="_blank">00:25:26.720</a></span> | <span class="t">every input we receive on the fly we calculate the alpha and the beta because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1532" target="_blank">00:25:32.000</a></span> | <span class="t">we have a vector so we can calculate the alpha and the beta and then we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1535" target="_blank">00:25:35.440</a></span> | <span class="t">quantize it on the fly okay now we have quantized also the input matrix by using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1541" target="_blank">00:25:41.020</a></span> | <span class="t">for example dynamic quantization we can perform this matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1545" target="_blank">00:25:45.240</a></span> | <span class="t">will be which will become a integer matrix multiplication the output will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1550" target="_blank">00:25:50.620</a></span> | <span class="t">Y which is an integer matrix but this matrix here is not the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1557" target="_blank">00:25:57.280</a></span> | <span class="t">floating-point number of the not quantized network it's a quantized value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1562" target="_blank">00:26:02.640</a></span> | <span class="t">how can we map it back to the original floating-point number well we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1569" target="_blank">00:26:09.340</a></span> | <span class="t">a process called calibration calibration means that we take the point the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1574" target="_blank">00:26:14.600</a></span> | <span class="t">network we run some input through the network and we check what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1579" target="_blank">00:26:19.220</a></span> | <span class="t">typical values of Y by using these typical values of Y we can check what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1584" target="_blank">00:26:24.880</a></span> | <span class="t">could be a reasonable alpha and the reasonable beta for these values that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1589" target="_blank">00:26:29.680</a></span> | <span class="t">observe of Y and then we can use the output of this integer matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1595" target="_blank">00:26:35.040</a></span> | <span class="t">multiplication and use the scale and the zero parameter that we have computed by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1601" target="_blank">00:26:41.660</a></span> | <span class="t">visual by collecting statistics about this Y to dequantize this output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1608" target="_blank">00:26:48.520</a></span> | <span class="t">here such that it's mapped back into a floating-point number such that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1613" target="_blank">00:26:53.880</a></span> | <span class="t">network output doesn't differ too much from what is the not quantized network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1619" target="_blank">00:26:59.780</a></span> | <span class="t">so the goal of quantization is to reduce the number of bits required to represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1624" target="_blank">00:27:04.620</a></span> | <span class="t">each parameter and also to speed up the computation but our goal is to obtain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1629" target="_blank">00:27:09.720</a></span> | <span class="t">the same output for the same input or at least to obtain a very similar output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1634" target="_blank">00:27:14.600</a></span> | <span class="t">for the same input so we don't want to lose the precision so we need to find a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1639" target="_blank">00:27:19.120</a></span> | <span class="t">way to of course map back into the floating-point numbers each output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1644" target="_blank">00:27:24.520</a></span> | <span class="t">each linear layer and this is how we do it so the input matrix we can observe it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1650" target="_blank">00:27:30.000</a></span> | <span class="t">every time by using dynamic quantization so on the fly we can quantize it the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1654" target="_blank">00:27:34.440</a></span> | <span class="t">output we can observe it for a few samples so we know what are the typical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1659" target="_blank">00:27:39.000</a></span> | <span class="t">maximum and the minimum values such that we can then use them as alpha and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1664" target="_blank">00:27:44.160</a></span> | <span class="t">beta and then we can dequantize the output Y using these values that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1669" target="_blank">00:27:49.520</a></span> | <span class="t">observed we will see later this practically with the post-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1672" target="_blank">00:27:52.880</a></span> | <span class="t">quantization we will actually watch the code of how it works I also want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1678" target="_blank">00:27:58.680</a></span> | <span class="t">give you a glimpse into how GPU perform matrix multiplication so when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1684" target="_blank">00:28:04.440</a></span> | <span class="t">calculate the product X multiplied by W plus B which is a matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1689" target="_blank">00:28:09.760</a></span> | <span class="t">followed by a matrix addition the result is a list of dot products between each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1695" target="_blank">00:28:15.240</a></span> | <span class="t">row of the X matrix and each column of the Y matrix summing the corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1701" target="_blank">00:28:21.720</a></span> | <span class="t">element of the bias vector B this operation so the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1707" target="_blank">00:28:27.140</a></span> | <span class="t">plus bias can be accelerated by the GPU using a block called the multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1712" target="_blank">00:28:32.600</a></span> | <span class="t">accumulate in which for example imagine each matrix is made up of vectors of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1717" target="_blank">00:28:37.480</a></span> | <span class="t">four elements so we load the vector of the X the first row of the VEX matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1723" target="_blank">00:28:43.680</a></span> | <span class="t">and then the first column of the W matrix and we compute the corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1730" target="_blank">00:28:50.080</a></span> | <span class="t">product so X 1 1 with W 1 1 then X 1 2 with W 1 2 X 1 3 with W 3 1 etc etc and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1737" target="_blank">00:28:57.320</a></span> | <span class="t">then we sum all this value into a register called the accumulator now here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1743" target="_blank">00:29:03.080</a></span> | <span class="t">this is a 8-bit integer this is an 8-bit integer because we quantize them so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1750" target="_blank">00:29:10.720</a></span> | <span class="t">result of a multiplication of two 8-bit integers may not be an 8-bit integer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1755" target="_blank">00:29:15.920</a></span> | <span class="t">of course can be 16-bit or more and for this reason we use the accumulator here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1762" target="_blank">00:29:22.720</a></span> | <span class="t">is used as is a usually 32-bit and this is also the reason we quantize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1769" target="_blank">00:29:29.440</a></span> | <span class="t">vector here as a 32-bit because the accumulator here is initialized already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1774" target="_blank">00:29:34.680</a></span> | <span class="t">with the bias element so this GPU will perform this operation in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1779" target="_blank">00:29:39.840</a></span> | <span class="t">parallel for every row and column of the initial matrices using many blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1785" target="_blank">00:29:45.400</a></span> | <span class="t">like this and this is how the GPU acceleration works for matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1788" target="_blank">00:29:48.800</a></span> | <span class="t">multiplication if you are interested in how this happens on low level on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1793" target="_blank">00:29:53.520</a></span> | <span class="t">algorithmic level I recommend watching this article from Google in their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1798" target="_blank">00:29:58.680</a></span> | <span class="t">general matrix multiplication library which is a low precision matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1802" target="_blank">00:30:02.640</a></span> | <span class="t">multiplication library okay now that we have seen what is the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1809" target="_blank">00:30:09.560</a></span> | <span class="t">symmetric and asymmetric quantization we may also want to understand how do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1814" target="_blank">00:30:14.840</a></span> | <span class="t">choose the beta and the alpha parameter we saw before one way of course is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1819" target="_blank">00:30:19.000</a></span> | <span class="t">choose for in the case of asymmetric quantization to choose beta and alpha to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1822" target="_blank">00:30:22.460</a></span> | <span class="t">be the smallest and the biggest value and for the symmetric for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1826" target="_blank">00:30:26.560</a></span> | <span class="t">quantization to choose alpha as the biggest value in absolute terms but this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1830" target="_blank">00:30:30.860</a></span> | <span class="t">is not the only strategy and they have pros and cons so let's review all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1834" target="_blank">00:30:34.360</a></span> | <span class="t">strategies we have the strategies that we use before is called the minimax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1838" target="_blank">00:30:38.680</a></span> | <span class="t">strategy which means that we choose alpha as the biggest value in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1841" target="_blank">00:30:41.720</a></span> | <span class="t">original tensor and beta as the minimum value in the original tensor this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1846" target="_blank">00:30:46.520</a></span> | <span class="t">however is a sensitive to outliers because imagine we have a vector that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1851" target="_blank">00:30:51.320</a></span> | <span class="t">more or less distributed around the minus 50 and plus 50 but then we have an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1856" target="_blank">00:30:56.680</a></span> | <span class="t">outlier that is a very big number here the problem with this strategy is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1861" target="_blank">00:31:01.280</a></span> | <span class="t">the outlier will make the quantization so the quantization error of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1867" target="_blank">00:31:07.840</a></span> | <span class="t">numbers very big so all the numbers as you can see when we quantize and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1872" target="_blank">00:31:12.760</a></span> | <span class="t">dequantize using asymmetric quantization with minimax strategy we see that all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1878" target="_blank">00:31:18.320</a></span> | <span class="t">the numbers are not very similar to the original they are actually quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1881" target="_blank">00:31:21.840</a></span> | <span class="t">different so this is 43.31 this is 45.08 so actually it's a quite a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1886" target="_blank">00:31:26.920</a></span> | <span class="t">error for the quantization a better strategy to avoid the outliers ruining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1892" target="_blank">00:31:32.720</a></span> | <span class="t">the original the input range is to use the percentile strategy so we set the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1897" target="_blank">00:31:37.960</a></span> | <span class="t">range alpha and beta basically to be a percentile of the original distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1902" target="_blank">00:31:42.760</a></span> | <span class="t">so not the maximum or the minimum but using the percent for example the 99%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1907" target="_blank">00:31:47.080</a></span> | <span class="t">percentile and if we use the percentile we will see that the quantization error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1913" target="_blank">00:31:53.440</a></span> | <span class="t">is reduced for all the terms and the only term that will suffer a lot from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1917" target="_blank">00:31:57.480</a></span> | <span class="t">the quantization error is the outlier itself okay let's have a look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1922" target="_blank">00:32:02.560</a></span> | <span class="t">code to see how this minimax strategy and percentile strategy differ so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1928" target="_blank">00:32:08.480</a></span> | <span class="t">open this one in which we again have a lot of numbers so 10,000 numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1935" target="_blank">00:32:15.820</a></span> | <span class="t">distributed between -50 and 150 and then we introduce an outlier let's say the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1940" target="_blank">00:32:20.640</a></span> | <span class="t">last number is an outlier so it's equal to 1000 all the other numbers are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1945" target="_blank">00:32:25.040</a></span> | <span class="t">distributed between -50 and 150 we compare these two strategies so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1951" target="_blank">00:32:31.300</a></span> | <span class="t">asymmetric quantization using the minimax strategy and the asymmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1954" target="_blank">00:32:34.960</a></span> | <span class="t">quantization using the percentile strategy as you can see the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1957" target="_blank">00:32:37.960</a></span> | <span class="t">modification between these two methods is how we compute alpha and beta here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1962" target="_blank">00:32:42.280</a></span> | <span class="t">alpha is computed as the maximum value here alpha is computed as a percentile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1965" target="_blank">00:32:45.920</a></span> | <span class="t">percentile of 99.99 and we can compare what are the quantized value we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1975" target="_blank">00:32:55.320</a></span> | <span class="t">here and then we can dequantize and when we dequantize we will see that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1981" target="_blank">00:33:01.280</a></span> | <span class="t">all the values using the minimax strategy suffer from a big quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1986" target="_blank">00:33:06.640</a></span> | <span class="t">error while when we use the percentile we will see that the only value that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1990" target="_blank">00:33:10.040</a></span> | <span class="t">suffers from a big quantization error is the outlier itself and as we can see if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1995" target="_blank">00:33:15.880</a></span> | <span class="t">we exclude the outlier and we compute the quantization error on the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=1999" target="_blank">00:33:19.440</a></span> | <span class="t">terms we will see that with the percentile we have a much smaller error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2004" target="_blank">00:33:24.120</a></span> | <span class="t">while with the minimax strategy we have a very big error for all the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2008" target="_blank">00:33:28.440</a></span> | <span class="t">except the outlier. Other two strategies that are commonly used for choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2013" target="_blank">00:33:33.480</a></span> | <span class="t">alpha and beta are the mean squared error and the cross entropy. Mean squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2017" target="_blank">00:33:37.560</a></span> | <span class="t">error means that we choose alpha and beta such that the mean squared error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2021" target="_blank">00:33:41.120</a></span> | <span class="t">between the original values and the quantized values is minimized so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2026" target="_blank">00:33:46.160</a></span> | <span class="t">usually use a grid search for this and the cross entropy is used as a strategy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2031" target="_blank">00:33:51.760</a></span> | <span class="t">whenever we are dealing for example with a language model as you know in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2035" target="_blank">00:33:55.640</a></span> | <span class="t">language model we have the last layer which is a linear layer plus softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2039" target="_blank">00:33:59.240</a></span> | <span class="t">which allow us to choose a token from the vocabulary. The goal of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2045" target="_blank">00:34:05.040</a></span> | <span class="t">softmax layer is to create a distribution probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2048" target="_blank">00:34:08.680</a></span> | <span class="t">in which usually we use the grid strategy or the top of the strategy so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2052" target="_blank">00:34:12.560</a></span> | <span class="t">what we are concerned about are not the values inside this distribution but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2056" target="_blank">00:34:16.920</a></span> | <span class="t">they actually the distribution itself so the biggest number should remain the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2061" target="_blank">00:34:21.400</a></span> | <span class="t">biggest number also in the quantized values and the intermediate numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2064" target="_blank">00:34:24.760</a></span> | <span class="t">should not change the relative distribution and for this case we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2068" target="_blank">00:34:28.720</a></span> | <span class="t">the cross entropy strategy which means that we choose alpha and beta such that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2072" target="_blank">00:34:32.680</a></span> | <span class="t">the cross entropy between the quantized value and the dequantized, not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2077" target="_blank">00:34:37.240</a></span> | <span class="t">quantized value so the original values and the dequantized value is minimized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2081" target="_blank">00:34:41.760</a></span> | <span class="t">and another topic when we are doing quantization which comes to play every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2088" target="_blank">00:34:48.480</a></span> | <span class="t">time we have a convolutional layer is the granularity. As you know convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2092" target="_blank">00:34:52.440</a></span> | <span class="t">layers are made up of many filters or kernels and each kernel is run through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2097" target="_blank">00:34:57.560</a></span> | <span class="t">the for example the image to calculate specific features. Now for example these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2103" target="_blank">00:35:03.280</a></span> | <span class="t">kernels are made of parameters which may be distributed differently for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2108" target="_blank">00:35:08.160</a></span> | <span class="t">we may have a kernel that is distributed for example between minus 5 and plus 5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2112" target="_blank">00:35:12.640</a></span> | <span class="t">another one that is distributed between minus 10 and plus 10 and another one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2117" target="_blank">00:35:17.680</a></span> | <span class="t">that is distributed for example between minus 6 and plus 6. If we use the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2123" target="_blank">00:35:23.040</a></span> | <span class="t">alpha and beta for all of them we will have that some kernels are wasting their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2128" target="_blank">00:35:28.840</a></span> | <span class="t">quantization range here and here for example so in this case it's better to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2134" target="_blank">00:35:34.480</a></span> | <span class="t">perform a channel wise quantization which means that for each kernel we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2139" target="_blank">00:35:39.040</a></span> | <span class="t">calculate an alpha and beta and they will be different for each basic kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2143" target="_blank">00:35:43.800</a></span> | <span class="t">which results in a higher quality quantization so we lose less precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2148" target="_blank">00:35:48.800</a></span> | <span class="t">this way. And now let's look at what is post training quantization. So post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2154" target="_blank">00:35:54.320</a></span> | <span class="t">training quantization means that we have a pre-trained model that we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2158" target="_blank">00:35:58.040</a></span> | <span class="t">quantize. How do we do that? Well we need the pre-trained model and we need some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2163" target="_blank">00:36:03.040</a></span> | <span class="t">data which is unlabeled data so we don't we do not need the original training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2167" target="_blank">00:36:07.560</a></span> | <span class="t">data we just need some data that we can run inference on. For example imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2171" target="_blank">00:36:11.800</a></span> | <span class="t">that the pre-trained model is a model that can classify dogs and cats what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2176" target="_blank">00:36:16.560</a></span> | <span class="t">need as data we just need some pictures of dogs and cats which may also not come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2180" target="_blank">00:36:20.640</a></span> | <span class="t">from the training set and what we do is basically we take this pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2187" target="_blank">00:36:27.000</a></span> | <span class="t">and we attach some observers that will collect some statistics while we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2193" target="_blank">00:36:33.040</a></span> | <span class="t">running inference on the model and this statistics will be used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2198" target="_blank">00:36:38.440</a></span> | <span class="t">calculate the Z and the S parameter for each layer of the model and then we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2204" target="_blank">00:36:44.160</a></span> | <span class="t">use it to quantize the model. Let's see how this works in code. In this case I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2210" target="_blank">00:36:50.680</a></span> | <span class="t">will be creating a very simple model so first we import some libraries but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2215" target="_blank">00:36:55.280</a></span> | <span class="t">basically just a torch and then we import the data set we will be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2220" target="_blank">00:37:00.800</a></span> | <span class="t">MNIST in our case. I define a very simple model for classifying MNIST</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2226" target="_blank">00:37:06.160</a></span> | <span class="t">digits which is made up of three linear layers with ReLU activations. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2231" target="_blank">00:37:11.340</a></span> | <span class="t">create this network I run a training on this network so this is just a basic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2236" target="_blank">00:37:16.780</a></span> | <span class="t">training training loop you can see here and we save this network as in this file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2245" target="_blank">00:37:25.480</a></span> | <span class="t">so we train it for I don't remember how many epochs for five epochs and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2249" target="_blank">00:37:29.640</a></span> | <span class="t">save it in a file. We define the testing loop which is just for validating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2256" target="_blank">00:37:36.040</a></span> | <span class="t">what is the accuracy of this model. So first let's look at the model the not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2261" target="_blank">00:37:41.560</a></span> | <span class="t">quantized model so the pre-trained model for example. In this case let's look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2266" target="_blank">00:37:46.120</a></span> | <span class="t">the weights of the first linear layer. In this case we can see that the linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2269" target="_blank">00:37:49.840</a></span> | <span class="t">layer is made up of a weight matrix which is made up of many numbers which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2273" target="_blank">00:37:53.800</a></span> | <span class="t">are floating point of 32 bits. Floating point numbers of 32 bits. The size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2279" target="_blank">00:37:59.800</a></span> | <span class="t">model before quantization is 360 kilobyte. If we run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2287" target="_blank">00:38:07.120</a></span> | <span class="t">testing loop on this model we will see that the accuracy is 96% which is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2292" target="_blank">00:38:12.360</a></span> | <span class="t">bad. Of course our goal is to quantize which means that we want to speed up the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2297" target="_blank">00:38:17.000</a></span> | <span class="t">computation we want to reduce the size of the model but while maintaining the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2300" target="_blank">00:38:20.460</a></span> | <span class="t">accuracy. Let's see how it works. The first thing we do is we create a copy of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2305" target="_blank">00:38:25.520</a></span> | <span class="t">the model by introducing some observers. So as you can see this is a quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2311" target="_blank">00:38:31.940</a></span> | <span class="t">stub and this is a de-quantization stub that is used by PyTorch to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2316" target="_blank">00:38:36.960</a></span> | <span class="t">quantization on the fly. And then we introduce also some observers in all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2323" target="_blank">00:38:43.080</a></span> | <span class="t">intermediate layers. So we take this new model that is with observers we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2328" target="_blank">00:38:48.480</a></span> | <span class="t">basically take the weights from the pre-trained model and copy it into this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2333" target="_blank">00:38:53.600</a></span> | <span class="t">new model that we have created. So we are not training a new model we are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2337" target="_blank">00:38:57.000</a></span> | <span class="t">copying the weights of the pre-trained model into this new model that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2341" target="_blank">00:39:01.280</a></span> | <span class="t">defined which is exactly the same as the original one just with some observers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2345" target="_blank">00:39:05.840</a></span> | <span class="t">And we also insert some observers in all the intermediate layers. Let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2352" target="_blank">00:39:12.120</a></span> | <span class="t">these observers basically they are the some special class of objects made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2358" target="_blank">00:39:18.560</a></span> | <span class="t">available by PyTorch that for each linear layer they will observe some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2362" target="_blank">00:39:22.440</a></span> | <span class="t">statistics when we run some inference on this model. And as you can see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2367" target="_blank">00:39:27.760</a></span> | <span class="t">the statistic they collect is just the minimum value they see and the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2371" target="_blank">00:39:31.540</a></span> | <span class="t">value they see for each layer also for the input and this is why we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2376" target="_blank">00:39:36.520</a></span> | <span class="t">quant stub as input. And we calibrate the model using the test. So if we run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2382" target="_blank">00:39:42.680</a></span> | <span class="t">inference on the model using the test set for example which is we just need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2386" target="_blank">00:39:46.720</a></span> | <span class="t">some data to run inference on the model so that these observers will collect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2391" target="_blank">00:39:51.160</a></span> | <span class="t">statistics. We do it so this will calculate the this will run inference of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2397" target="_blank">00:39:57.520</a></span> | <span class="t">all the test set on the model so we are not training anything we're just running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2401" target="_blank">00:40:01.960</a></span> | <span class="t">inference. The observers after running inference we will have collected some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2407" target="_blank">00:40:07.760</a></span> | <span class="t">statistics so for example the input observer here has collected some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2411" target="_blank">00:40:11.720</a></span> | <span class="t">statistics. The observer for the first linear layer also have collected some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2416" target="_blank">00:40:16.160</a></span> | <span class="t">statistics the second and the third etc etc. We can use the statistics that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2422" target="_blank">00:40:22.160</a></span> | <span class="t">have collected to create the quantized model so the actual quantization happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2427" target="_blank">00:40:27.160</a></span> | <span class="t">after we have collected these statistics and then we run this method which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2431" target="_blank">00:40:31.760</a></span> | <span class="t">quantization.convert which will create the quantized model. And we can now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2437" target="_blank">00:40:37.440</a></span> | <span class="t">see that after we quantize it each layer will become a quantized layer so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2442" target="_blank">00:40:42.880</a></span> | <span class="t">before quantization it's just a linear layer but after they become a quantized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2446" target="_blank">00:40:46.720</a></span> | <span class="t">linear layer. Each of them has some special parameter that is the S and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2452" target="_blank">00:40:52.500</a></span> | <span class="t">Z parameter that we saw in the slide so the scale and the zero point. And we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2458" target="_blank">00:40:58.320</a></span> | <span class="t">also print the weight matrix after quantization and we can see that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2461" target="_blank">00:41:01.600</a></span> | <span class="t">weight matrix has become an integer of 8 bits so as you can see here. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2467" target="_blank">00:41:07.040</a></span> | <span class="t">compare the dequantized weights and the original weights so the original weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2471" target="_blank">00:41:11.200</a></span> | <span class="t">were floating-point numbers of 32 bits while the dequantized weights so after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2478" target="_blank">00:41:18.560</a></span> | <span class="t">we dequantize of course we obtain back the floating-point numbers so these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2482" target="_blank">00:41:22.640</a></span> | <span class="t">the how they are stored on the disk but of course when we want to dequantize we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2487" target="_blank">00:41:27.720</a></span> | <span class="t">obtain something that is very similar to the original weight matrix but not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2492" target="_blank">00:41:32.360</a></span> | <span class="t">exactly the same because we introduce some error because of the quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2496" target="_blank">00:41:36.560</a></span> | <span class="t">So the dequantized weights are very similar to the original number but not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2501" target="_blank">00:41:41.360</a></span> | <span class="t">exactly the same. For example the first number is quite different the second one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2506" target="_blank">00:41:46.160</a></span> | <span class="t">is quite similar the third one is quite similar etc etc. We can check the size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2511" target="_blank">00:41:51.920</a></span> | <span class="t">the model after it's been quantized and we can see that the new size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2515" target="_blank">00:41:55.560</a></span> | <span class="t">model is 94 kilobyte. Originally it was 360 if I remember correctly so it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2522" target="_blank">00:42:02.040</a></span> | <span class="t">been reduced by four times. Why? Because each number instead of being a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2526" target="_blank">00:42:06.480</a></span> | <span class="t">floating-point number of 32 bits is now an integer plus some overhead because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2531" target="_blank">00:42:11.960</a></span> | <span class="t">need to save some other data because for example we need to save all this scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2536" target="_blank">00:42:16.720</a></span> | <span class="t">the scale value the zero point value and also PyTorch saves some other values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2542" target="_blank">00:42:22.040</a></span> | <span class="t">We can also check the accuracy of the quantized model and we see that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2546" target="_blank">00:42:26.360</a></span> | <span class="t">model didn't suffer much from actually didn't suffer at all from from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2551" target="_blank">00:42:31.320</a></span> | <span class="t">quantization so the accuracy remained practically the same. In reality okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2555" target="_blank">00:42:35.480</a></span> | <span class="t">this is a very simple example and the model is quite big so I think the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2559" target="_blank">00:42:39.040</a></span> | <span class="t">has plenty of parameters to to predict well. But in reality usually when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2566" target="_blank">00:42:46.720</a></span> | <span class="t">we quantize a model we will lose some accuracy and we will see later a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2571" target="_blank">00:42:51.120</a></span> | <span class="t">training approach that makes the model more robust to quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2576" target="_blank">00:42:56.000</a></span> | <span class="t">which is called the quantization aware training. So this is the post-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2582" target="_blank">00:43:02.240</a></span> | <span class="t">quantization and that's all for this one. Let's look at the next quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2586" target="_blank">00:43:06.920</a></span> | <span class="t">strategy which is the quantization aware training. What we do basically is that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2591" target="_blank">00:43:11.960</a></span> | <span class="t">insert some fake modules in the computational graph of the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2596" target="_blank">00:43:16.760</a></span> | <span class="t">simulate the effect of quantization during training. So before we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2600" target="_blank">00:43:20.400</a></span> | <span class="t">talking about how to quantize a model after we have already trained it. In this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2606" target="_blank">00:43:26.000</a></span> | <span class="t">case we want to train a model such that the model is more robust to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2610" target="_blank">00:43:30.640</a></span> | <span class="t">quantization effect. So this is done during training not after the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2616" target="_blank">00:43:36.560</a></span> | <span class="t">training. And basically what we do is we have our model which has input then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2622" target="_blank">00:43:42.640</a></span> | <span class="t">have some linear layers, we have output, we have a target, we compute the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2626" target="_blank">00:43:46.480</a></span> | <span class="t">What we do basically is we insert between each layer some special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2633" target="_blank">00:43:53.620</a></span> | <span class="t">operations of quantize and dequantize operations, some fake operations. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2638" target="_blank">00:43:58.120</a></span> | <span class="t">actually we are not quantizing the model or the weights because the model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2642" target="_blank">00:44:02.080</a></span> | <span class="t">getting trained. But we do some quantization on the fly. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2646" target="_blank">00:44:06.800</a></span> | <span class="t">every time we see an input here we quantize it and dequantize it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2650" target="_blank">00:44:10.280</a></span> | <span class="t">immediately and run it to the next layer. Then this will produce some output. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2654" target="_blank">00:44:14.920</a></span> | <span class="t">quantize it and dequantize it immediately and we give it to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2658" target="_blank">00:44:18.520</a></span> | <span class="t">because this will introduce some quantization error and we hope that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2663" target="_blank">00:44:23.360</a></span> | <span class="t">loss function will learn to be more robust to handle this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2668" target="_blank">00:44:28.500</a></span> | <span class="t">quantization error that is introduced by this fake quantization that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2672" target="_blank">00:44:32.360</a></span> | <span class="t">introducing. So the goal of introducing these operations is just to introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2676" target="_blank">00:44:36.640</a></span> | <span class="t">some quantization error so that the loss function can get ready to counter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2682" target="_blank">00:44:42.920</a></span> | <span class="t">affect the effects of quantization. Let's look at the code of how it is done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2689" target="_blank">00:44:49.960</a></span> | <span class="t">So we go to quantization aware training. Okay we import the necessary libraries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2696" target="_blank">00:44:56.440</a></span> | <span class="t">just like before. We import the data set, in our case it's MNIST. We define a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2701" target="_blank">00:45:01.760</a></span> | <span class="t">model which is exactly the same as before but we notice that here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2705" target="_blank">00:45:05.760</a></span> | <span class="t">already start with a quantization model that is ready for quantization because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2710" target="_blank">00:45:10.300</a></span> | <span class="t">here we want to train the model in a way that it's already aware of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2714" target="_blank">00:45:14.720</a></span> | <span class="t">quantization. That's why it's called quantization aware training and the rest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2719" target="_blank">00:45:19.520</a></span> | <span class="t">of the structure of the model is the same as before. We insert the minimax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2723" target="_blank">00:45:23.720</a></span> | <span class="t">observers in the model for every layer so as you can see this model is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2728" target="_blank">00:45:28.240</a></span> | <span class="t">trained and we are insert already some observers. These observers are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2733" target="_blank">00:45:33.880</a></span> | <span class="t">calibrated because we never run any inference or we never run any training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2737" target="_blank">00:45:37.400</a></span> | <span class="t">on this model so all these values are plus and minus infinity. Then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2743" target="_blank">00:45:43.400</a></span> | <span class="t">train the model using the MNIST and we train it for one epoch and we check the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2750" target="_blank">00:45:50.760</a></span> | <span class="t">statistics collected by these observers during training and we can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2756" target="_blank">00:45:56.200</a></span> | <span class="t">during training they have collected some statistics so the minimum and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2760" target="_blank">00:46:00.000</a></span> | <span class="t">maximum value and you can see that when we do quantization aware training we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2765" target="_blank">00:46:05.360</a></span> | <span class="t">have this weight fake quant so this is actually all the fake quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2770" target="_blank">00:46:10.360</a></span> | <span class="t">observers that we have introduced during the training and they have collected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2775" target="_blank">00:46:15.280</a></span> | <span class="t">some some values or some statistics. We can then quantize the model by using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2782" target="_blank">00:46:22.160</a></span> | <span class="t">statistics that have been collected during training and we can print the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2787" target="_blank">00:46:27.160</a></span> | <span class="t">values scale and zero point of the quantized model and we can see them here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2792" target="_blank">00:46:32.520</a></span> | <span class="t">We can also print the weights of the quantized model and you can see that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2797" target="_blank">00:46:37.040</a></span> | <span class="t">weight matrix of the first linear layer is actually an integer matrix and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2801" target="_blank">00:46:41.720</a></span> | <span class="t">also run the accuracy and we can see that the accuracy of this model is 0.952</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2807" target="_blank">00:46:47.200</a></span> | <span class="t">okay in this case it's a little worse than the other case but this is not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2812" target="_blank">00:46:52.320</a></span> | <span class="t">rule usually quantization aware training makes the model more robust to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2816" target="_blank">00:46:56.200</a></span> | <span class="t">effects of quantization so usually when we do post training quantization the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2821" target="_blank">00:47:01.200</a></span> | <span class="t">model loses more accuracy compared to quantization when we train a model with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2826" target="_blank">00:47:06.000</a></span> | <span class="t">quantization aware training. Let's go back to the slides. Now there is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2832" target="_blank">00:47:12.420</a></span> | <span class="t">thing that we should notice that with quantization aware training we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2835" target="_blank">00:47:15.800</a></span> | <span class="t">introducing some observers between each layer some special quantized and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2842" target="_blank">00:47:22.520</a></span> | <span class="t">dequantized operation between each layer and then we do it while training. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2847" target="_blank">00:47:27.400</a></span> | <span class="t">means that the backpropagation algorithm should also be able to calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2852" target="_blank">00:47:32.220</a></span> | <span class="t">gradient of the loss function with respect to this operation that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2857" target="_blank">00:47:37.840</a></span> | <span class="t">doing but we do this the operation of quantization is not differentiable so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2862" target="_blank">00:47:42.520</a></span> | <span class="t">how can the backpropagation algorithm algorithm calculate the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2867" target="_blank">00:47:47.120</a></span> | <span class="t">quantization operation that we are doing during the forward loop? Well we usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2873" target="_blank">00:47:53.920</a></span> | <span class="t">approximate the gradient using the straight through estimator which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2878" target="_blank">00:47:58.360</a></span> | <span class="t">that for all the values that fall in between the beta and the alpha parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2884" target="_blank">00:48:04.720</a></span> | <span class="t">we give a gradient of 1 and for all the other values that are outside of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2889" target="_blank">00:48:09.520</a></span> | <span class="t">range we approximate the gradient with 0 and this is because the quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2896" target="_blank">00:48:16.040</a></span> | <span class="t">operation is not differentiable this is why we need to approximate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2899" target="_blank">00:48:19.800</a></span> | <span class="t">using this approximator. The next thing that we should notice is why does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2906" target="_blank">00:48:26.840</a></span> | <span class="t">quantization aware training works I mean what is the effect of quantization aware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2911" target="_blank">00:48:31.200</a></span> | <span class="t">training on the loss function because as I told you before our goal is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2915" target="_blank">00:48:35.520</a></span> | <span class="t">introduce the quantization error during training such that the loss function can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2920" target="_blank">00:48:40.200</a></span> | <span class="t">react to it but how? Now imagine we do post training quantization when we train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2926" target="_blank">00:48:46.040</a></span> | <span class="t">a model that that has no notion of quantization imagine we only have a one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2931" target="_blank">00:48:51.920</a></span> | <span class="t">weight and the loss function is computed for this particular weight the goal of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2938" target="_blank">00:48:58.000</a></span> | <span class="t">the backpropagation algorithm or the gradient descent is actually to of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2943" target="_blank">00:49:03.580</a></span> | <span class="t">gradient descent algorithm is to calculate the weights of the model such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2948" target="_blank">00:49:08.520</a></span> | <span class="t">that we minimize the loss and usually suppose we end up meaning this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2954" target="_blank">00:49:14.240</a></span> | <span class="t">loss function and we end up in this local minima here. The goal of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2959" target="_blank">00:49:19.240</a></span> | <span class="t">quantization aware training is to make the model reach a local minima that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2965" target="_blank">00:49:25.400</a></span> | <span class="t">more wide. Why? Because the weight value here after we quantize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2971" target="_blank">00:49:31.800</a></span> | <span class="t">it will change and for example if we do it without quantization aware training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2977" target="_blank">00:49:37.280</a></span> | <span class="t">if the loss was here and the weight value was here after quantization this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2982" target="_blank">00:49:42.520</a></span> | <span class="t">weight value will be changed of course so it may go here but the loss will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2986" target="_blank">00:49:46.360</a></span> | <span class="t">increase a lot for example but with quantization aware training we choose a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2991" target="_blank">00:49:51.480</a></span> | <span class="t">local minima or a minima that is more wide so that if the weight after the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=2996" target="_blank">00:49:56.820</a></span> | <span class="t">quantization moves a little bit the loss will not increase by much and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3002" target="_blank">00:50:02.320</a></span> | <span class="t">why quantization aware training works. Thank you guys for watching my video I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3007" target="_blank">00:50:07.600</a></span> | <span class="t">hope you enjoyed learning about quantization I didn't talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3011" target="_blank">00:50:11.400</a></span> | <span class="t">advanced topic like GPTQ or AWQ which I hope to do in my next videos. If you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3018" target="_blank">00:50:18.080</a></span> | <span class="t">liked the video please subscribe and like the video and share it with your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3021" target="_blank">00:50:21.560</a></span> | <span class="t">friends or colleagues and the students. I have other videos about deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3027" target="_blank">00:50:27.840</a></span> | <span class="t">and machine learning so please let me know if there is something you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3030" target="_blank">00:50:30.760</a></span> | <span class="t">understand and be free to connect with me on LinkedIn or on social media</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3037" target="_blank">00:50:37.200</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3039" target="_blank">00:50:39.260</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3041" target="_blank">00:50:41.320</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3043" target="_blank">00:50:43.380</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3045" target="_blank">00:50:45.440</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3047" target="_blank">00:50:47.500</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3049" target="_blank">00:50:49.560</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=0VdNflU08yA&t=3051" target="_blank">00:50:51.620</a></span> | <span class="t">you</span></div></div></body></html>