<html><head><title>Flash Attention derived and coded from first principles with Triton (Python)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Flash Attention derived and coded from first principles with Triton (Python)</h2><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM"><img src="https://i.ytimg.com/vi/zy8ChVd_oTM/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=190">3:10</a> Multi-Head Attention<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=546">9:6</a> Why Flash Attention<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=770">12:50</a> Safe Softmax<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1623">27:3</a> Online Softmax<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2384">39:44</a> Online Softmax (Proof)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2846">47:26</a> Block Matrix Multiplication<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5318">88:38</a> Flash Attention forward (by hand)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6241">104:1</a> Flash Attention forward (paper)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6653">110:53</a> Intro to CUDA with examples<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8788">146:28</a> Tensor Layouts<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9648">160:48</a> Intro to Triton with examples<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10466">174:26</a> Flash Attention forward (coding)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15731">262:11</a> LogSumExp trick in Flash Attention 2<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16373">272:53</a> Derivatives, gradients, Jacobians<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17154">285:54</a> Autograd<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18000">300:0</a> Jacobian of the MatMul operation<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18974">316:14</a> Jacobian through the Softmax<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20853">347:33</a> Flash Attention backwards (paper)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22391">373:11</a> Flash Attention backwards (coding)<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26470">441:10</a> Triton Autotuning<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26609">443:29</a> Triton tricks: software pipelining<br><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27218">453:38</a> Running the code<br><br><div style="text-align: left;"><a href="./zy8ChVd_oTM.html">Whisper Transcript</a> | <a href="./transcript_zy8ChVd_oTM.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=0" target="_blank">00:00:00.400</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are going to explore FlashAttention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4" target="_blank">00:00:04.640</a></span> | <span class="t">Now, we are going to explore FlashAttention from first principle which means that not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9" target="_blank">00:00:09.600</a></span> | <span class="t">we will code FlashAttention, we will actually derive it. So we pretend that the paper, FlashAttention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14" target="_blank">00:00:14.640</a></span> | <span class="t">paper, never existed and we look at the attention computation and we look at the problem it has and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20" target="_blank">00:00:20.240</a></span> | <span class="t">we try to solve it step by step pretending that FlashAttention never existed. This will give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25" target="_blank">00:00:25.120</a></span> | <span class="t">a deep understanding of how it works and also we will combine theory with practice because we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=30" target="_blank">00:00:30.320</a></span> | <span class="t">code it. Now, in order to code FlashAttention we will need to write a kernel for our GPU and in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=37" target="_blank">00:00:37.120</a></span> | <span class="t">our specific case I will be using an NVIDIA GPU so a CUDA kernel but instead of writing C++ code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=42" target="_blank">00:00:42.560</a></span> | <span class="t">we will use a Triton which is a way of converting Python directly into CUDA kernels that can run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">directly on the GPU and Triton you can think of it as a compiler that takes in Python and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=54" target="_blank">00:00:54.880</a></span> | <span class="t">converts it into something that can run on the GPU. So let's look at the topics for today. First</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=60" target="_blank">00:01:00.880</a></span> | <span class="t">of all I will give an introduction to multi-head attention because we need to look at what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=65" target="_blank">00:01:05.040</a></span> | <span class="t">attention and how it's computed and what are the problems in computing this attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=68" target="_blank">00:01:08.960</a></span> | <span class="t">Then we will look at actually the most critical part of the attention computation is this Softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=74" target="_blank">00:01:14.080</a></span> | <span class="t">and how it impacts the computation and complexity. We will look at what is online Softmax. Then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=80" target="_blank">00:01:20.240</a></span> | <span class="t">will explore what is the GPU because we are going to write a kernel that will run on the GPU so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=84" target="_blank">00:01:24.480</a></span> | <span class="t">need to understand what is the difference for example the CPU and the GPU and what is a kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=88" target="_blank">00:01:28.480</a></span> | <span class="t">and how it differs from a normal program that you write for the CPU. We will look at how tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=94" target="_blank">00:01:34.560</a></span> | <span class="t">are laid out in memory so row major layout, column major layout or etc strides. We are going to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=101" target="_blank">00:01:41.760</a></span> | <span class="t">at block matrix multiplication, Triton, software pipeline and all the optimization that Triton does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=106" target="_blank">00:01:46.640</a></span> | <span class="t">to our code. Finally we will be able to code the FlashAttention forward pass but of course we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=112" target="_blank">00:01:52.320</a></span> | <span class="t">not satisfied only by coding the forward pass. We also want to code the backward pass but in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=117" target="_blank">00:01:57.040</a></span> | <span class="t">to code the backward pass we also need to understand how autograd works and the gradient descent works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=122" target="_blank">00:02:02.400</a></span> | <span class="t">in the case of custom operations so we need to understand what are derivatives, what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=126" target="_blank">00:02:06.880</a></span> | <span class="t">gradients, what are Jacobians and then we calculate the gradient of the common operations that we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=131" target="_blank">00:02:11.680</a></span> | <span class="t">in FlashAttention and finally we will have enough knowledge to code the backward pass. For this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=136" target="_blank">00:02:16.800</a></span> | <span class="t">reason this video is going to be super long but I hope you don't mind because we are going to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=141" target="_blank">00:02:21.360</a></span> | <span class="t">a lot. Of course you may be wondering all of this requires a lot of knowledge that you may not have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=147" target="_blank">00:02:27.280</a></span> | <span class="t">but that's not a problem because that's my problem because in this video I will make sure that if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=151" target="_blank">00:02:31.680</a></span> | <span class="t">only have high school calculus so you know what are derivatives you have basics of linear algebra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=156" target="_blank">00:02:36.320</a></span> | <span class="t">like you know what is matrix multiplication or what is the transpose of a matrix and you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=160" target="_blank">00:02:40.880</a></span> | <span class="t">a basic knowledge of attention mechanism so like for example you have watched my previous video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=165" target="_blank">00:02:45.040</a></span> | <span class="t">the attention is all you need paper and you have a lot of patience that should be enough to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=170" target="_blank">00:02:50.880</a></span> | <span class="t">all of this video because all the topics that I will introduce I will always introduce them in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=175" target="_blank">00:02:55.360</a></span> | <span class="t">such a way that I pretend that you don't know anything about the topic so we try to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=180" target="_blank">00:03:00.640</a></span> | <span class="t">everything from first principle everything from scratch. Okay now that we have seen the introduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">let's go see the first part of the video which is the multi-head attention. All right let's talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=192" target="_blank">00:03:12.160</a></span> | <span class="t">about multi-head attention. Now I am using the slides from my previous video attention is all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=197" target="_blank">00:03:17.040</a></span> | <span class="t">you need so we can look at very fast at what multi-head attention is and how it works. I hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=203" target="_blank">00:03:23.120</a></span> | <span class="t">you remember the formula softmax of the query multiplied by the transpose of the key divided by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=207" target="_blank">00:03:27.440</a></span> | <span class="t">dk all multiplied by b because we will be using that a lot throughout the video. Now multi-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=213" target="_blank">00:03:33.760</a></span> | <span class="t">attention starts from an input sequence or two input sequence in case we are talking about cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=218" target="_blank">00:03:38.400</a></span> | <span class="t">attention. In the simple case of self-attention we have one input sequence which is a sequence of in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=224" target="_blank">00:03:44.240</a></span> | <span class="t">the case of language model a sequence of tokens where we have sec number of tokens and each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=231" target="_blank">00:03:51.440</a></span> | <span class="t">is represented by an embedding so a vector with d model dimensions. The first thing that we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=238" target="_blank">00:03:58.480</a></span> | <span class="t">is we convert this input sequence into query key and values through three linear projections one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=246" target="_blank">00:04:06.240</a></span> | <span class="t">called wq one called wk one called wv which in pytorch are represented through linear layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=253" target="_blank">00:04:13.520</a></span> | <span class="t">and these linear layers are of d model by d model so they do not change the shape of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=259" target="_blank">00:04:19.520</a></span> | <span class="t">tensor and then after we do this job of projecting them they become three different sequences one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=269" target="_blank">00:04:29.200</a></span> | <span class="t">called query one called key and one called value so here i'm calling them q prime k prime and v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=274" target="_blank">00:04:34.960</a></span> | <span class="t">prime then we divide them into smaller embeddings so each of this token which is made up of d model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=283" target="_blank">00:04:43.920</a></span> | <span class="t">dimensions we divide it into smaller tokens each one suppose we have four heads each one will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=291" target="_blank">00:04:51.680</a></span> | <span class="t">d model divided by four dimensions so this one is a sequence of tokens where each token is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=298" target="_blank">00:04:58.400</a></span> | <span class="t">the entire token but a part of the embedding of each token and this one is a another part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=303" target="_blank">00:05:03.920</a></span> | <span class="t">embedding of the tokens and this one is another part of the embedding of the token etc and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=308" target="_blank">00:05:08.720</a></span> | <span class="t">do this job for the query key and value sequence then we compute the attention as follows so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=315" target="_blank">00:05:15.520</a></span> | <span class="t">softmax of the query multiplied by the transpose of the key divided by the the square root of dk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=320" target="_blank">00:05:20.960</a></span> | <span class="t">where dk is the dimension of each head so how many dimensions each head is working with and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=329" target="_blank">00:05:29.360</a></span> | <span class="t">do the multiplication with v and this will give us the output of the attention mechanism for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=334" target="_blank">00:05:34.480</a></span> | <span class="t">head and this job is done independently for each head this should be clear to you if it's not please</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=341" target="_blank">00:05:41.120</a></span> | <span class="t">watch my previous video on the attention mechanism because we will be working with this scenario a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=347" target="_blank">00:05:47.120</a></span> | <span class="t">lot now then we take this o the output of each head and then we concatenate it back in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=357" target="_blank">00:05:57.200</a></span> | <span class="t">get the representation of each token as a full embedding so before we split this embedding into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=365" target="_blank">00:06:05.360</a></span> | <span class="t">smaller embeddings this one here is called the q1 q2 q3 q4 then after we compute the attention we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=372" target="_blank">00:06:12.640</a></span> | <span class="t">get back the output of each head and we concatenate it together to get back the full embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=377" target="_blank">00:06:17.760</a></span> | <span class="t">dimension which is this edge here we run it through another linear projection called wo which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=383" target="_blank">00:06:23.120</a></span> | <span class="t">will be the output of the multi head attention now flash attention is not concerned with all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=389" target="_blank">00:06:29.440</a></span> | <span class="t">these operations actually flash attention is only concerned with the operation that require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=394" target="_blank">00:06:34.240</a></span> | <span class="t">optimization and the operations that require optimizations are this one so the softmax of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=400" target="_blank">00:06:40.400</a></span> | <span class="t">the query multiplied by the transpose of the key divided by the square root of dk multiplied by v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=404" target="_blank">00:06:44.800</a></span> | <span class="t">which means that the projection of the input sequence through wq wk and wv is not something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=411" target="_blank">00:06:51.440</a></span> | <span class="t">that flash attention is concerned about because that's a matrix multiplication so when you use a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=416" target="_blank">00:06:56.960</a></span> | <span class="t">linear layer it's just a matrix multiplication of the input with the weight matrix of the linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=421" target="_blank">00:07:01.760</a></span> | <span class="t">layer and this kind of operation so the matrix multiplication is one of the most um optimized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=428" target="_blank">00:07:08.560</a></span> | <span class="t">operation that we have in the gpu because the manufacturer of the gpu usually also releases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=434" target="_blank">00:07:14.480</a></span> | <span class="t">um the necessary library for computing the the matrix multiplication so actually these are quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=440" target="_blank">00:07:20.400</a></span> | <span class="t">fast and they do not require any optimization so flash attention will pretend that the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=445" target="_blank">00:07:25.520</a></span> | <span class="t">is has already been passed through by wq and the key has already passed through wk and the v has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=451" target="_blank">00:07:31.600</a></span> | <span class="t">already passed from wb moreover flash attention will not be concerned with the projection with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=457" target="_blank">00:07:37.840</a></span> | <span class="t">wo because that's also a matrix multiplication because the wo is always represented in pytorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=462" target="_blank">00:07:42.960</a></span> | <span class="t">as a linear layer so it's a matrix multiplication and matrix multiplication as we have seen are very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=469" target="_blank">00:07:49.680</a></span> | <span class="t">optimized so there is nothing to optimize there but what we need to optimize in terms of speed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=475" target="_blank">00:07:55.840</a></span> | <span class="t">is this operation here softmax of the query multiplied by the transpose of the keys divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=479" target="_blank">00:07:59.440</a></span> | <span class="t">by the square root of vk multiplied by v all right guys so now we have rehearsed what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=485" target="_blank">00:08:05.440</a></span> | <span class="t">multi-head attention i also want to give you a lot of visualization which is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=490" target="_blank">00:08:10.240</a></span> | <span class="t">here in the paper of the multi-head attention we can see that we have the input that is v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=498" target="_blank">00:08:18.160</a></span> | <span class="t">k and q so q k and v each of them runs through a linear layer which is the w q w k and w v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=506" target="_blank">00:08:26.080</a></span> | <span class="t">then we do the scaled dot product attention which is done independently for each head so each head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=513" target="_blank">00:08:33.760</a></span> | <span class="t">will do query multiplied by the transpose of the key divided by the square root of dk where each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=519" target="_blank">00:08:39.040</a></span> | <span class="t">query and each key is not the full embedding of each token but a part of the embedding of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=523" target="_blank">00:08:43.680</a></span> | <span class="t">token because we split them into smaller embeddings and eventually we take all the output of each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=529" target="_blank">00:08:49.440</a></span> | <span class="t">this head which are computed in parallel so that's why you see this dimension edge in the depth we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=535" target="_blank">00:08:55.200</a></span> | <span class="t">concatenate them and then we run them through w o what are we concerned with we are concerned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=540" target="_blank">00:09:00.560</a></span> | <span class="t">with optimizing this particular block here the scaled dot product attention so let's start our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=545" target="_blank">00:09:05.840</a></span> | <span class="t">journey one thing that is very important to understand is why do we even need a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=552" target="_blank">00:09:12.640</a></span> | <span class="t">implementation of the attention mechanism and if you look at the flash attention paper you will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=557" target="_blank">00:09:17.360</a></span> | <span class="t">notice the following part this is the paper flash attention one and in the flash attention one paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=563" target="_blank">00:09:23.680</a></span> | <span class="t">they describe the attention implement implementation as it's done naively when using pytorch so first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=569" target="_blank">00:09:29.760</a></span> | <span class="t">we do the multiplication of the query multiplied by the transpose of the keys then we apply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=575" target="_blank">00:09:35.680</a></span> | <span class="t">softmax to the output of this operation and finally we multiply the output of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=580" target="_blank">00:09:40.240</a></span> | <span class="t">with the v matrix to obtain the output of the attention the way this implementation is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=586" target="_blank">00:09:46.160</a></span> | <span class="t">by pytorch without any optimization is as follows so we load the first of all these tensors are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=595" target="_blank">00:09:55.120</a></span> | <span class="t">residing in the gpu the gpu is made up of two main memories one is called the hbm which is the dram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=603" target="_blank">00:10:03.200</a></span> | <span class="t">which is the the ram of the gpu which is the 40 gigabyte of the a100 for example so it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=610" target="_blank">00:10:10.880</a></span> | <span class="t">biggest memory that we have in the gpu and then there are there is the shared memory so the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=619" target="_blank">00:10:19.360</a></span> | <span class="t">of the gpu is that accessing this hbm so the global it's also called the global memory it's very very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=625" target="_blank">00:10:25.680</a></span> | <span class="t">slow compared to the shared memory however the shared memory it's much much smaller compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=630" target="_blank">00:10:30.560</a></span> | <span class="t">the hbm and what they claim in the flash attention paper is that the operation of the attention is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=637" target="_blank">00:10:37.200</a></span> | <span class="t">i/o bound meaning that if we keep accessing the global memory the overall operation of computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=647" target="_blank">00:10:47.120</a></span> | <span class="t">the attention is not because computing all these operations it's slow but because we keep accessing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=652" target="_blank">00:10:52.880</a></span> | <span class="t">the global memory which is slow so we call this kind of operations i/o bound so the only way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=660" target="_blank">00:11:00.080</a></span> | <span class="t">improve this situation is to compute the attention inside the shared memory of the gpu which is much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=666" target="_blank">00:11:06.480</a></span> | <span class="t">smaller which is much closer to the cores that actually do the computation so we will need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=673" target="_blank">00:11:13.120</a></span> | <span class="t">kind of also split the attention computation into smaller blocks that can reside in the shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=679" target="_blank">00:11:19.600</a></span> | <span class="t">memory and we will see later in how this is possible through block matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=685" target="_blank">00:11:25.760</a></span> | <span class="t">and this is in the paper here they call it the tiling and it's a very how to say use the technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=692" target="_blank">00:11:32.640</a></span> | <span class="t">when doing when writing kernels for the gpu which are usually involve some kind of matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=700" target="_blank">00:11:40.640</a></span> | <span class="t">so now we know what problem the flash attention is trying to solve it's trying to make sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=707" target="_blank">00:11:47.120</a></span> | <span class="t">we do not need to access the hbm so the high bandwidth memory when computing the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=713" target="_blank">00:11:53.680</a></span> | <span class="t">but copying only a part of each matrix inside the local memory so the shared memory of the gpu that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=721" target="_blank">00:12:01.440</a></span> | <span class="t">is closer to the cores and computing a part of the output matrix there then copying that part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=729" target="_blank">00:12:09.120</a></span> | <span class="t">to the output in that is residing in the hbm and keep doing it for all the blocks in which we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=735" target="_blank">00:12:15.120</a></span> | <span class="t">divide this query key and value matrices and later we will see how this blocked computation is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=741" target="_blank">00:12:21.920</a></span> | <span class="t">but also we will see that the biggest problem in computing this block computation is the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=747" target="_blank">00:12:27.280</a></span> | <span class="t">because the softmax needs to access all the row of the s matrix to apply the softmax because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=754" target="_blank">00:12:34.720</a></span> | <span class="t">the the softmax needs to have a normalization factor which is the sum of all the exponentials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=762" target="_blank">00:12:42.400</a></span> | <span class="t">of all the values to which it is applied row wise and we will see later how we will solve this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=767" target="_blank">00:12:47.680</a></span> | <span class="t">problem so let's move on all right guys um okay when i say guys i mean guys and girls because i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=775" target="_blank">00:12:55.840</a></span> | <span class="t">don't know in my usually i just say guys too you know but please girls don't feel excluded so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=783" target="_blank">00:13:03.280</a></span> | <span class="t">saw that first of all flash attention is only concerned in optimizing this softmax of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=788" target="_blank">00:13:08.960</a></span> | <span class="t">transpose of softmax of the query multiplied by three divided by the square root of dk multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=794" target="_blank">00:13:14.480</a></span> | <span class="t">by b and we need to introduce a little bit of notation so that we don't get lost in the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=800" target="_blank">00:13:20.480</a></span> | <span class="t">slides first of all this is the formulas i took from the flash attention paper but for now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=805" target="_blank">00:13:25.920</a></span> | <span class="t">let's pretend flash attention never existed so we are trying to solve the problem step by step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=810" target="_blank">00:13:30.080</a></span> | <span class="t">now um we should treat this q as something that has as the sequence that is the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=817" target="_blank">00:13:37.760</a></span> | <span class="t">of the input sequence that has already passed through wq the k as something that has already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=823" target="_blank">00:13:43.280</a></span> | <span class="t">passed through wk and v as something that has already passed through wv because we don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=829" target="_blank">00:13:49.040</a></span> | <span class="t">to optimize the matrix multiplication because it's already fast enough another thing is let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=835" target="_blank">00:13:55.040</a></span> | <span class="t">talk about what are the dimensions of these matrices so we can then understand what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=839" target="_blank">00:13:59.520</a></span> | <span class="t">the dimensions of the output of this operation so we will see treat q as a sequence of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=847" target="_blank">00:14:07.280</a></span> | <span class="t">with n tokens so n tokens where each token is d has d dimensions so lowercase d dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=856" target="_blank">00:14:16.160</a></span> | <span class="t">why because usually we take the queries and then we split them into multiple heads so we have we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=862" target="_blank">00:14:22.480</a></span> | <span class="t">pretend we have already done this splitting so we pretend we are ready to cover input sequence we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=867" target="_blank">00:14:27.040</a></span> | <span class="t">already run it through wq and then we have already split it into multiple heads and each of this head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=873" target="_blank">00:14:33.280</a></span> | <span class="t">will do the following operation so the the one we already saw and so the usual formula query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=880" target="_blank">00:14:40.400</a></span> | <span class="t">multiply the transpose of the keys and each of this head will work with these dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=885" target="_blank">00:14:45.920</a></span> | <span class="t">for the query for the key and for the value sequence so now let's look at the the dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=892" target="_blank">00:14:52.560</a></span> | <span class="t">of the output so the first operation that we will do is the query multiplied by the transpose of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=896" target="_blank">00:14:56.800</a></span> | <span class="t">keys where the transpose of the keys is a matrix that originally is n by d but become but with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=903" target="_blank">00:15:03.040</a></span> | <span class="t">transpose will be d by n so d by n and the result will be a matrix that is n by n because in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=910" target="_blank">00:15:10.480</a></span> | <span class="t">matrix multiplication the outer dimensions become the dimension of the output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=914" target="_blank">00:15:14.480</a></span> | <span class="t">what do what is the next operation that we do we take the output of this operation so the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=920" target="_blank">00:15:20.400</a></span> | <span class="t">multiply by transpose of the keys and we run it through a softmax operation and we will see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=924" target="_blank">00:15:24.320</a></span> | <span class="t">is the softmax operation which preserves the shape of the input so it doesn't change the shape of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=930" target="_blank">00:15:30.720</a></span> | <span class="t">input matrix it just changes the values of it and then we take the output of the softmax and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=936" target="_blank">00:15:36.480</a></span> | <span class="t">we multiply it by v which will change the which will change the of course the shape because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=943" target="_blank">00:15:43.840</a></span> | <span class="t">p matrix is n by n so this one is n by n and v is n by d so this one the output will be n by d the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=954" target="_blank">00:15:54.800</a></span> | <span class="t">outer dimensions of this matrix multiplication now let's look at the details of each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=960" target="_blank">00:16:00.080</a></span> | <span class="t">operations so when we do query multiply by transpose of the keys we will get a matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=964" target="_blank">00:16:04.560</a></span> | <span class="t">is n by n where each value in this matrix is a dot product of a row of q and a column of k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=973" target="_blank">00:16:13.840</a></span> | <span class="t">in particular the first element of this matrix will be the dot product of the first query with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=979" target="_blank">00:16:19.840</a></span> | <span class="t">the first key vector the second element will be the dot product of the first query with the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=986" target="_blank">00:16:26.080</a></span> | <span class="t">key vector and the third element will be the first query with the third key etc etc and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=991" target="_blank">00:16:31.920</a></span> | <span class="t">let's say the the last row of this matrix will be the dot product of the last query with the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=999" target="_blank">00:16:39.680</a></span> | <span class="t">key then the last query with the second key the last query with the third key etc etc until the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1004" target="_blank">00:16:44.960</a></span> | <span class="t">last query with the last key you may also notice that here i have written query transpose the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1013" target="_blank">00:16:53.440</a></span> | <span class="t">because when we what is q1 first of all q1 is the first row of the query matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1020" target="_blank">00:17:00.640</a></span> | <span class="t">so a little bit of background on matrix multiplication so we know that when we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1026" target="_blank">00:17:06.800</a></span> | <span class="t">matrix multiplication each output element is one row of the first matrix with one column of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1032" target="_blank">00:17:12.560</a></span> | <span class="t">second matrix but we are doing the product of the first matrix with the transpose of the second so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1038" target="_blank">00:17:18.480</a></span> | <span class="t">it will be the dot product of the one row of the query matrix with one row of the key matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1044" target="_blank">00:17:24.960</a></span> | <span class="t">because we are doing the multiplication with key k transposed when you take a vector from a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1054" target="_blank">00:17:34.240</a></span> | <span class="t">the usual notation so the in in as in how to say in in in mathematics in a linear algebra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1062" target="_blank">00:17:42.240</a></span> | <span class="t">we always pretend that a vector is a column vector so we cannot just write q multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1068" target="_blank">00:17:48.240</a></span> | <span class="t">by k because that would be mean that would mean we are doing the dot product of we are doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1075" target="_blank">00:17:55.920</a></span> | <span class="t">kind of the matrix multiplication of one column matrix with one column matrix that is not possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1082" target="_blank">00:18:02.480</a></span> | <span class="t">because the shapes do not match so as a notation we write that we do the dot product of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1088" target="_blank">00:18:08.160</a></span> | <span class="t">matrix the transpose which is a column vector but we transpose it so it becomes a row vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1093" target="_blank">00:18:13.520</a></span> | <span class="t">with the second vector this is just because of notation guys so you just need to pretend that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1100" target="_blank">00:18:20.320</a></span> | <span class="t">this is the first query with the first key then the first query with the second key the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1104" target="_blank">00:18:24.640</a></span> | <span class="t">query with the third key etc etc etc so we are doing dot products of vectors then we apply this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1111" target="_blank">00:18:31.840</a></span> | <span class="t">softmax operation the softmax operation what it will do it will transform each of these dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1118" target="_blank">00:18:38.960</a></span> | <span class="t">which are scalars so the output of a dot product is a scalar and it will transform each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1125" target="_blank">00:18:45.280</a></span> | <span class="t">numbers in such a way that they become kind of a probability distribution row wise which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1132" target="_blank">00:18:52.000</a></span> | <span class="t">each of these numbers is between 0 and 1 and when we sum up these numbers together they are sum up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1138" target="_blank">00:18:58.880</a></span> | <span class="t">to 1 and this condition this property will be valid for each row so this row also will sum up to 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1145" target="_blank">00:19:05.760</a></span> | <span class="t">this row will sum up to 1 and this row will sum up to 1 etc etc etc let's see what is the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1152" target="_blank">00:19:12.720</a></span> | <span class="t">operation now given a vector so let's call it x which is made up of n dimensions the softmax is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1161" target="_blank">00:19:21.760</a></span> | <span class="t">defined as follows so it is the the softmax basically transforms this transforms this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1169" target="_blank">00:19:29.680</a></span> | <span class="t">into another vector with the same dimension where each item of the output vector is calculated as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1175" target="_blank">00:19:35.280</a></span> | <span class="t">follows so the height element of the output vector is the exponential of the element input element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1182" target="_blank">00:19:42.400</a></span> | <span class="t">divided by the summation of all the exponentials of all the dimensions of the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1188" target="_blank">00:19:48.240</a></span> | <span class="t">basically this is called the normalization factor to make it all these numbers between 0 and 1 we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1195" target="_blank">00:19:55.600</a></span> | <span class="t">usually normalize that's why it's called the normalization factor and we use the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1201" target="_blank">00:20:01.520</a></span> | <span class="t">because we want each of these numbers to be positive we don't want the stuff the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1206" target="_blank">00:20:06.480</a></span> | <span class="t">of this operation to be negative so that's why we use the exponential but there is a problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1211" target="_blank">00:20:11.840</a></span> | <span class="t">the problem is imagine our input vector is made up of many numbers that are maybe large so for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1218" target="_blank">00:20:18.000</a></span> | <span class="t">example let's say x1 is equal to 100 x2 is equal to 200 x3 is equal to 300 which is can happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1226" target="_blank">00:20:26.160</a></span> | <span class="t">if we do the exponential of these numbers so the exponential of 100 that is going to be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1231" target="_blank">00:20:31.600</a></span> | <span class="t">huge number it's going to very close to infinity at least compared to what we can store in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1237" target="_blank">00:20:37.120</a></span> | <span class="t">computer so the output of exponential of 100 may not fit into a floating point 32 or a floating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1244" target="_blank">00:20:44.240</a></span> | <span class="t">point 16 number or even an integer of 32 bit so we cannot compute it because it will overflow our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1253" target="_blank">00:20:53.440</a></span> | <span class="t">our variable our integer that is storing this value this output so we talk in this case about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1260" target="_blank">00:21:00.080</a></span> | <span class="t">numerical instability so every time you hear the term numerical instability in computer science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1265" target="_blank">00:21:05.840</a></span> | <span class="t">it means that the number cannot be represented within a fixed representation with the bits we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1271" target="_blank">00:21:11.520</a></span> | <span class="t">have available which are usually 32 bit or 16 bit we have also 64 bit but that will be too expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1278" target="_blank">00:21:18.720</a></span> | <span class="t">to use so let's try to find a solution to make this stuff here computable and numerically stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1285" target="_blank">00:21:25.440</a></span> | <span class="t">in order to make this softmax operation numerically stable which means that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1291" target="_blank">00:21:31.600</a></span> | <span class="t">these numbers to not explode or to become too small that they are not representable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1296" target="_blank">00:21:36.880</a></span> | <span class="t">we need to find a solution and luckily it's quite easy so the softmax as we have seen before it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1302" target="_blank">00:21:42.960</a></span> | <span class="t">the following formula so each number is exponentiated and then we divide it by this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1307" target="_blank">00:21:47.040</a></span> | <span class="t">normalization factor which is just the sum of the exponential of each input dimension of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1311" target="_blank">00:21:51.600</a></span> | <span class="t">input vector if we multiply the numerator and the denominator of a fraction with a constant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1318" target="_blank">00:21:58.000</a></span> | <span class="t">with a number then the fraction will not change so that's what we are going to do we are multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1321" target="_blank">00:22:01.760</a></span> | <span class="t">the numerator and the denominator with this factor c as long as c is not equal to zero of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1328" target="_blank">00:22:08.080</a></span> | <span class="t">then we can take this c and by using the distributive property of the product with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1336" target="_blank">00:22:16.320</a></span> | <span class="t">respect to the sum we can bring this c inside of the summation as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1340" target="_blank">00:22:20.320</a></span> | <span class="t">then we can also write every number as the exponential of the log of itself because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1347" target="_blank">00:22:27.520</a></span> | <span class="t">exponential and the log will cancel out and then we can by using the properties of the exponentials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1355" target="_blank">00:22:35.520</a></span> | <span class="t">we know that the product of two exponential is equal to the sum of the is equal to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1360" target="_blank">00:22:40.400</a></span> | <span class="t">exponential of the sum of the arguments of each exponential and we do it on the numerator and in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1366" target="_blank">00:22:46.000</a></span> | <span class="t">the denominator then we just call this quantity minus log c equal to k or k is equal to minus k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1374" target="_blank">00:22:54.960</a></span> | <span class="t">is equal to log c so we can replace this quantity with k we can do that because this is a constant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1381" target="_blank">00:23:01.600</a></span> | <span class="t">that we have chosen and we just are assigning it to another constant so basically by doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1388" target="_blank">00:23:08.480</a></span> | <span class="t">this derivation we can see that we can sneak in a value inside of this exponential that if chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1395" target="_blank">00:23:15.440</a></span> | <span class="t">carefully can reduce the argument of this exponential and we will choose this k equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1401" target="_blank">00:23:21.360</a></span> | <span class="t">to the maximum element inside of the input vector that we are applying the softmax to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1406" target="_blank">00:23:26.480</a></span> | <span class="t">so that each of this argument will be either zero in case xi is equal to the maximum element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1413" target="_blank">00:23:33.840</a></span> | <span class="t">that we are processing of the vector or it will be less than zero and we know that the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1420" target="_blank">00:23:40.320</a></span> | <span class="t">when it's equal to zero will be equal to the output of the exponential will be one so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1425" target="_blank">00:23:45.200</a></span> | <span class="t">argument when it's zero it will be equal to one and when it's smaller than zero so it's in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1430" target="_blank">00:23:50.160</a></span> | <span class="t">negative range it will be between zero and one so which is easily representable with floating point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1436" target="_blank">00:23:56.080</a></span> | <span class="t">32 for example so this exponential will not explode anymore so basically to apply the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1444" target="_blank">00:24:04.400</a></span> | <span class="t">to a vector in a numerically safe way we need to find a k constant which is the maximum value of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1452" target="_blank">00:24:12.800</a></span> | <span class="t">this vector and when we apply it we need to subtract each element minus this constant that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1458" target="_blank">00:24:18.400</a></span> | <span class="t">we have chosen so let's look at the algorithm to compute the softmax so first of all given a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1465" target="_blank">00:24:25.760</a></span> | <span class="t">or given an n by n matrix because we want to apply the softmax to this matrix here which is n by n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1472" target="_blank">00:24:32.400</a></span> | <span class="t">we need to go through each row of this matrix and for each row we need to find the maximum value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1478" target="_blank">00:24:38.800</a></span> | <span class="t">among the elements which takes time complexity linear with respect to the size of the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1484" target="_blank">00:24:44.720</a></span> | <span class="t">to the size of the row to which we are applying the softmax then we need to compute the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1489" target="_blank">00:24:49.760</a></span> | <span class="t">factor which is this stuff here and we we cannot compute it before the step number one because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1496" target="_blank">00:24:56.720</a></span> | <span class="t">need to have the maximum element to compute this summation here and after we have calculated the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1502" target="_blank">00:25:02.960</a></span> | <span class="t">normalization factor we can then divide each element's exponential by the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1508" target="_blank">00:25:08.640</a></span> | <span class="t">and we cannot do the step number three before calculating the normalization factor because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1513" target="_blank">00:25:13.200</a></span> | <span class="t">we need to divide each number by the normalization factor so if you like pseudocode for algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1520" target="_blank">00:25:20.080</a></span> | <span class="t">this is an algorithm for computing the softmax that we have seen right now so first we find the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1525" target="_blank">00:25:25.120</a></span> | <span class="t">maximum of the row to which we are applying the softmax then we compute the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1531" target="_blank">00:25:31.200</a></span> | <span class="t">and then we apply the softmax to each element which means that we calculate compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1535" target="_blank">00:25:35.360</a></span> | <span class="t">exponential of each element minus the maximum value of the vector divided by the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1541" target="_blank">00:25:41.840</a></span> | <span class="t">factor now this pseudocode is an algorithm that is quite slow because look at a practical example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1550" target="_blank">00:25:50.240</a></span> | <span class="t">imagine we have this vector here first we need to do step one find the maximum value in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1555" target="_blank">00:25:55.520</a></span> | <span class="t">vector which is number five and this takes linear time computation then we need to calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1560" target="_blank">00:26:00.960</a></span> | <span class="t">normalization constant which is the sum of the exponential of each element minus the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1566" target="_blank">00:26:06.960</a></span> | <span class="t">value so e to the power of 3 minus 5 plus e to the power of 2 minus 5 etc etc this we will call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1574" target="_blank">00:26:14.000</a></span> | <span class="t">l and then each we need to go again through this vector again and take the exponential of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1580" target="_blank">00:26:20.160</a></span> | <span class="t">element minus the maximum divided by the normalization factor so to apply the softmax to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1587" target="_blank">00:26:27.040</a></span> | <span class="t">an n by n matrix we need to go through each element of this matrix three times and these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1593" target="_blank">00:26:33.760</a></span> | <span class="t">operations must be done sequentially so we cannot start operation two until we have done operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1598" target="_blank">00:26:38.880</a></span> | <span class="t">one and we cannot start operation three until we have done one and two so this is quite slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1605" target="_blank">00:26:45.440</a></span> | <span class="t">only to apply an operation that doesn't even change the shape of the matrix it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1611" target="_blank">00:26:51.280</a></span> | <span class="t">uh normal uh normalizing the values so there must be a better way that that does not involve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1617" target="_blank">00:26:57.760</a></span> | <span class="t">three sequential operations in which we need to go through this matrix three times let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1622" target="_blank">00:27:02.480</a></span> | <span class="t">all right guys let's rehearse what is the problem that we are trying to solve the problem statement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1628" target="_blank">00:27:08.640</a></span> | <span class="t">is the following can we find a better way to compute the softmax that does not involve going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1635" target="_blank">00:27:15.120</a></span> | <span class="t">through the vector three times because let's look at the pseudocode of the algorithm for computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1640" target="_blank">00:27:20.480</a></span> | <span class="t">the local the softmax that we have found so far imagine we have a vector made up of four elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1646" target="_blank">00:27:26.080</a></span> | <span class="t">the first thing that we need to do is to compute the maximum element in this vector which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1650" target="_blank">00:27:30.960</a></span> | <span class="t">going through this for loop here that allow us to compute the maximum element in this vector which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1656" target="_blank">00:27:36.800</a></span> | <span class="t">means that we start from the left side of the vector and iteratively go to the right side so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1661" target="_blank">00:27:41.520</a></span> | <span class="t">we start from the first element arrive to the end and we compare the previously found maximum with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1667" target="_blank">00:27:47.680</a></span> | <span class="t">the current element to find the global maximum basically this means that uh i i know that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1673" target="_blank">00:27:53.360</a></span> | <span class="t">is very simple uh i'm probably sure that you don't need to this example but making this example will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1678" target="_blank">00:27:58.720</a></span> | <span class="t">help us understand what we will do next so please bear with me even if it's super simple what i'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1683" target="_blank">00:28:03.520</a></span> | <span class="t">doing okay we at the beginning m0 is equal to minus infinity m1 is basically the for loop at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1694" target="_blank">00:28:14.000</a></span> | <span class="t">the iteration number one which means that we are m1 will be equal to the maximum of the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1699" target="_blank">00:28:19.920</a></span> | <span class="t">estimate of the m which is minus infinity with the current element which is three so it will become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1706" target="_blank">00:28:26.960</a></span> | <span class="t">equal to three then m2 will be equal to the maximum of the previously computed maximum so m1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1713" target="_blank">00:28:33.840</a></span> | <span class="t">so three with the current element which is two so it will be equal to three m3 will be equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1720" target="_blank">00:28:40.400</a></span> | <span class="t">the maximum of the previously computed maximum so three with the current three with the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1726" target="_blank">00:28:46.880</a></span> | <span class="t">element which is five so it will be equal to five and m4 will be equal to the maximum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1733" target="_blank">00:28:53.200</a></span> | <span class="t">previously computed maximum and the current element so it will be equal to five so this allow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1737" target="_blank">00:28:57.920</a></span> | <span class="t">us to compute the maximum element so at the fourth iteration we will have the maximum the global</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1742" target="_blank">00:29:02.320</a></span> | <span class="t">maximum independently of what is the input array um delete okay after we have computed the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1750" target="_blank">00:29:10.080</a></span> | <span class="t">which we know is five we can compute the normalization factor so let's start with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1755" target="_blank">00:29:15.520</a></span> | <span class="t">l0 l0 is equal to zero l1 will be equal to the exponential of l0 so actually sorry it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1763" target="_blank">00:29:23.120</a></span> | <span class="t">l0 plus the exponential of the current element so three minus the maximum element we have found in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1769" target="_blank">00:29:29.680</a></span> | <span class="t">the previous for loop so five then l2 will be equal to l1 plus the exponential of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1775" target="_blank">00:29:35.920</a></span> | <span class="t">the current element so it's two minus the maximum then l3 will be equal to l2 plus the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1785" target="_blank">00:29:45.360</a></span> | <span class="t">of the current element five minus five then l4 will be equal to the l3 plus exponential of one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1794" target="_blank">00:29:54.080</a></span> | <span class="t">minus five if you expand this l this will be basically equal to e to the power of three minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1802" target="_blank">00:30:02.160</a></span> | <span class="t">five plus e to the power of two minus five plus e to the power of five minus five plus e to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1807" target="_blank">00:30:07.760</a></span> | <span class="t">power of one minus one minus five after we have computed this normalization factor we can use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1816" target="_blank">00:30:16.320</a></span> | <span class="t">to normalize the each element in the input vector which means that the x new x1 so x1 prime let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1823" target="_blank">00:30:23.040</a></span> | <span class="t">see will be equal to e to the power of what's the first element three minus five divided by l that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1834" target="_blank">00:30:34.240</a></span> | <span class="t">we computed in the previous for loop so the l at the fourth iteration the new x2 so x2 prime will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1843" target="_blank">00:30:43.360</a></span> | <span class="t">be equal to the e to the power of two minus five divided by l4 and x3 prime will be equal to the e</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1850" target="_blank">00:30:50.640</a></span> | <span class="t">to the power of five minus five divided by l4 etc etc for all the elements i know this is super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1858" target="_blank">00:30:58.080</a></span> | <span class="t">simple but it will help us later so in this for loop we have that we need to go through the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1865" target="_blank">00:31:05.600</a></span> | <span class="t">three times because first we need to compute this for loop then we need to compute this for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1870" target="_blank">00:31:10.560</a></span> | <span class="t">and then we need to compute another for loop we cannot do them not in this sequence because in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1875" target="_blank">00:31:15.680</a></span> | <span class="t">order to compute this for loop we need to have the maximum element because we need it here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1879" target="_blank">00:31:19.680</a></span> | <span class="t">and we cannot compute this for loop until we have computed the previous one because we need to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1884" target="_blank">00:31:24.240</a></span> | <span class="t">the normalization factor however we are stubborn and let's try to fuse these two operations into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1890" target="_blank">00:31:30.800</a></span> | <span class="t">one for loop which means that we go through the array and simultaneously compute mi and in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1897" target="_blank">00:31:37.440</a></span> | <span class="t">same iteration we also try to compute lj of course we will not be able to compute lj because we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1903" target="_blank">00:31:43.440</a></span> | <span class="t">have the global maximum because we didn't go through the old array yet however let's try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1910" target="_blank">00:31:50.080</a></span> | <span class="t">use the locally and whatever estimate we have of the maximum so far so let's try to use instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1916" target="_blank">00:31:56.240</a></span> | <span class="t">mn let's try to use mi so the local maximum that we have computed so far so if we apply the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1922" target="_blank">00:32:02.720</a></span> | <span class="t">in this way in this fused way to this vector we will have the following iterations so this is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1930" target="_blank">00:32:10.160</a></span> | <span class="t">array or vector and the first step is mi so m1 will be equal to the previous maximum which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1937" target="_blank">00:32:17.440</a></span> | <span class="t">minus infinity with the current element so the maximum minus infinity and the current element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1942" target="_blank">00:32:22.480</a></span> | <span class="t">is equal to 3 and l1 will be equal to the previous l so l0 which is starts from 0 plus e to the power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1951" target="_blank">00:32:31.600</a></span> | <span class="t">of the current element minus we should be using the global maximum but we don't have the global</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1956" target="_blank">00:32:36.240</a></span> | <span class="t">maximum so let's use the whatever maximum we have so far so we can use 3 now at the second iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1962" target="_blank">00:32:42.560</a></span> | <span class="t">we are at this element of the vector and we compute the maximum so far so the maximum so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1968" target="_blank">00:32:48.480</a></span> | <span class="t">is the previous maximum and the current element so the maximum of the previous maximum and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1972" target="_blank">00:32:52.640</a></span> | <span class="t">current element which is the maximum between 3 and 2 which is 3 and the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1979" target="_blank">00:32:59.120</a></span> | <span class="t">is the previous normalization factor plus exponential of 2 minus 3 which is the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1984" target="_blank">00:33:04.720</a></span> | <span class="t">element minus whatever maximum we have so far now if our array were made only of these two elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1992" target="_blank">00:33:12.640</a></span> | <span class="t">so 3 and 2 then whatever we have computed is actually correct because the maximum that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=1999" target="_blank">00:33:19.120</a></span> | <span class="t">have found is a 3 and it's actually the global maximum and the normalization factor that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2004" target="_blank">00:33:24.800</a></span> | <span class="t">have computed is actually correct because each of the exponential has been computed with the global</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2009" target="_blank">00:33:29.680</a></span> | <span class="t">maximum because the first element was computed using 3 as the with the argument minus 3 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2017" target="_blank">00:33:37.200</a></span> | <span class="t">also the second element was computed with the argument with the argument having minus 3 in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2022" target="_blank">00:33:42.720</a></span> | <span class="t">in the argument which is the global maximum of the vector however when we arrive at the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2028" target="_blank">00:33:48.560</a></span> | <span class="t">iteration so let me delete this vector so let me arrive here at the third iteration the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2034" target="_blank">00:33:54.400</a></span> | <span class="t">will change which will also cause our normalization factor to get to to be wrong because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2040" target="_blank">00:34:00.640</a></span> | <span class="t">we arrive at the element number 3 so the number 5 here and we compute the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2047" target="_blank">00:34:07.760</a></span> | <span class="t">so the maximum is the comparison of the previous maximum and the current element so the new maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2053" target="_blank">00:34:13.520</a></span> | <span class="t">becomes 5 and the normalization factor is the previous normalization factor so l2 plus the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2059" target="_blank">00:34:19.760</a></span> | <span class="t">exponential of the current element minus the current estimate of the maximum which is 5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2066" target="_blank">00:34:26.240</a></span> | <span class="t">however if you look at this l3 this is wrong why because l3 is equal to if you expand this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2074" target="_blank">00:34:34.240</a></span> | <span class="t">summation it will be equal to e to the power of 3 minus 3 plus e to the power of 2 minus 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2082" target="_blank">00:34:42.720</a></span> | <span class="t">plus e to the power of 5 minus 5 this exponential here is using 5 as the global maximum this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2090" target="_blank">00:34:50.320</a></span> | <span class="t">exponential here is using 3 as the global maximum and this one is using 3 as the global maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2095" target="_blank">00:34:55.440</a></span> | <span class="t">so the first two elements have been computed thinking that the global maximum is 3 but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2100" target="_blank">00:35:00.560</a></span> | <span class="t">actually we later we found a better global maximum which is 5 so which makes this normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2105" target="_blank">00:35:05.920</a></span> | <span class="t">factor wrong however can we fix at the third iteration whatever normalization we have computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2113" target="_blank">00:35:13.280</a></span> | <span class="t">so far up to the second iteration actually we can because if we expand this so as we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2120" target="_blank">00:35:20.800</a></span> | <span class="t">here we have expanded it what we need here is here to have a minus 5 because that's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2127" target="_blank">00:35:27.360</a></span> | <span class="t">the global maximum that we have found so far not the minus 3 that we had at the previous iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2132" target="_blank">00:35:32.880</a></span> | <span class="t">so and here we also need to fix this replace this minus 3 with minus 5 how can we do that well if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2139" target="_blank">00:35:39.040</a></span> | <span class="t">we multiply this one here and this one here with a correction factor that will sneak in a new maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2147" target="_blank">00:35:47.520</a></span> | <span class="t">inside of this exponential then we solve the problem and actually this correction factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2152" target="_blank">00:35:52.160</a></span> | <span class="t">is very easy to calculate because at the third iteration if we multiply l2 so the previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2157" target="_blank">00:35:57.840</a></span> | <span class="t">computed normalization factor with this factor here which is the exponential of the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2163" target="_blank">00:36:03.040</a></span> | <span class="t">estimate of the maximum minus the current estimate of the maximum so 5 we will see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2168" target="_blank">00:36:08.960</a></span> | <span class="t">e by the properties of the exponentials this one here will become e to the power of 3 minus 3 plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2176" target="_blank">00:36:16.800</a></span> | <span class="t">3 minus 5 so this minus 3 will cancel out with this 3 and also the second factor will have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2183" target="_blank">00:36:23.200</a></span> | <span class="t">3 will cancel out with this minus 3 will cancel out with this 3 and they will become e to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2188" target="_blank">00:36:28.400</a></span> | <span class="t">power of 3 minus 5 and 2 to the power of e to the power of 2 minus 5 which is actually correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2194" target="_blank">00:36:34.640</a></span> | <span class="t">because at the third iteration we should be actually happy we should be using minus 5 as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2200" target="_blank">00:36:40.400</a></span> | <span class="t">maximum of the array so far so basically what we have found is a way to fix whatever normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2208" target="_blank">00:36:48.960</a></span> | <span class="t">factor we have computed so far while iterating through the array when we found we when we find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2215" target="_blank">00:36:55.680</a></span> | <span class="t">a better maximum compared to what we have so far and when we don't need to fix anything then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2222" target="_blank">00:37:02.160</a></span> | <span class="t">formula still stands because what we did here as a multiplication as a correction factor so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2228" target="_blank">00:37:08.080</a></span> | <span class="t">the correction factor this correction factor is nothing more than the previous maximum so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2235" target="_blank">00:37:15.840</a></span> | <span class="t">previous estimate of the maximum minus the current estimates of the maximum at the current iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2241" target="_blank">00:37:21.440</a></span> | <span class="t">so the current max so this is basically m of i minus 1 and this is m of i so the current maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2250" target="_blank">00:37:30.400</a></span> | <span class="t">at the current iteration and let me delete it otherwise it remains forever in my slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2255" target="_blank">00:37:35.440</a></span> | <span class="t">so basically when we arrive to the last element we will see that the maximum doesn't change because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2262" target="_blank">00:37:42.480</a></span> | <span class="t">we compare the previous maximum with the current element which is less than the previous maximum so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2267" target="_blank">00:37:47.760</a></span> | <span class="t">the maximum doesn't change and we don't need to fix anything because the the the previous l3 so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2275" target="_blank">00:37:55.600</a></span> | <span class="t">the previously computed normalization factor is correct because they have all been using the minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2281" target="_blank">00:38:01.200</a></span> | <span class="t">5 so when we don't need to fix anything we just multiply by e to the power of the previous maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2288" target="_blank">00:38:08.080</a></span> | <span class="t">minus the current maximum which is e to the power of zero in this case so it's not fixing anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2293" target="_blank">00:38:13.440</a></span> | <span class="t">so we have found a way to fix the previously computed normalization factor while going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2300" target="_blank">00:38:20.240</a></span> | <span class="t">through the array even if at the current iteration we don't have the global maximum yet so that every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2307" target="_blank">00:38:27.200</a></span> | <span class="t">time the maximum changes we can fix and every time it doesn't change we just multiply with e to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2312" target="_blank">00:38:32.400</a></span> | <span class="t">power of zero which is like multiplying with one so the new algorithm that we have found for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2318" target="_blank">00:38:38.320</a></span> | <span class="t">softmax is the following so we start with m0 equal to minus infinity we start with l0 equal to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2324" target="_blank">00:38:44.560</a></span> | <span class="t">we go through the array we compute the locally the local maximum so up so the maximum so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2332" target="_blank">00:38:52.640</a></span> | <span class="t">from the zeroth element to the ith element so to the element at which we are doing the iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2339" target="_blank">00:38:59.680</a></span> | <span class="t">and the previously computed li can be fixed by using this correction factor which is e to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2346" target="_blank">00:39:06.160</a></span> | <span class="t">power of the previous maximum minus the current maximum plus the exponential of the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2352" target="_blank">00:39:12.560</a></span> | <span class="t">element minus the current estimate of the maximum in this way we go through the array only once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2359" target="_blank">00:39:19.120</a></span> | <span class="t">and we obtain two values the global maximum at at the end at the same time the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2367" target="_blank">00:39:27.120</a></span> | <span class="t">normalization factor and then we can use it to compute the softmax so we made three transformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2373" target="_blank">00:39:33.520</a></span> | <span class="t">three passes through the array into two passes through the array and this is very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2380" target="_blank">00:39:40.080</a></span> | <span class="t">and we will see how we actually use it to derive flash attention the example that i have given you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2386" target="_blank">00:39:46.880</a></span> | <span class="t">so far is not really a proof that our algorithm will work in every case because we made a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2392" target="_blank">00:39:52.800</a></span> | <span class="t">simple example by using a vector made up of four elements but does our new algorithm work in every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2400" target="_blank">00:40:00.400</a></span> | <span class="t">single case with whatever the numbers are we need to prove that so we will prove that by induction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2406" target="_blank">00:40:06.960</a></span> | <span class="t">so what first of all what are we trying to prove we have fused the first two for loops into one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2413" target="_blank">00:40:13.280</a></span> | <span class="t">for loop as you can see here what we expect is that at the end of this for loop this mn so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2421" target="_blank">00:40:21.200</a></span> | <span class="t">m at the last iteration will be actually the global maximum in the vector and this ln so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2427" target="_blank">00:40:27.840</a></span> | <span class="t">l at the last iteration will be equal to the sum of all the exponential of all the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2434" target="_blank">00:40:34.240</a></span> | <span class="t">minus the maximum element of the vector so the global maximum of the vector and we need to prove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2441" target="_blank">00:40:41.440</a></span> | <span class="t">that because what i did before was an example and that was not really a rigorous proof and the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2447" target="_blank">00:40:47.520</a></span> | <span class="t">we will prove it is by induction which is a typical way of proving this kind of theorems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2453" target="_blank">00:40:53.120</a></span> | <span class="t">now proof by induction basically works in the following way we need to prove that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2459" target="_blank">00:40:59.200</a></span> | <span class="t">our algorithm works for a base case for example with n equal to one and then we pretend we assume</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2468" target="_blank">00:41:08.800</a></span> | <span class="t">that the algorithm works on n and we need to prove that it also works for n plus one if this holds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2477" target="_blank">00:41:17.120</a></span> | <span class="t">then we have proven our algorithm for every possible n because it will work for the base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2482" target="_blank">00:41:22.720</a></span> | <span class="t">case so for example n equal to one and then by using the induction step we say so this if it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2489" target="_blank">00:41:29.200</a></span> | <span class="t">works for n and then it also works for n plus one then it means that it will also work for two but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2494" target="_blank">00:41:34.080</a></span> | <span class="t">then if it works for two then it should also work for three because of the induction step that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2498" target="_blank">00:41:38.240</a></span> | <span class="t">will prove and if it works for three then it will also work for four etc etc up to infinity so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2504" target="_blank">00:41:44.560</a></span> | <span class="t">prove it for the base case which is n equal to one it's very simple so at n equal to one this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2511" target="_blank">00:41:51.680</a></span> | <span class="t">for loop will only have one iteration so m m1 and l1 m1 will be the maximum of the previous m which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2519" target="_blank">00:41:59.120</a></span> | <span class="t">is minus infinity because we initialize m0 equal to minus infinity so it will be equal to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2527" target="_blank">00:42:07.600</a></span> | <span class="t">maximum of the previous m and the current element which is x1 so it will be equal to x1 whatever x1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2533" target="_blank">00:42:13.200</a></span> | <span class="t">we is uh x1 usually will never be equal it cannot be equal to minus infinity um because it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2540" target="_blank">00:42:20.960</a></span> | <span class="t">number in a fixed representation so it cannot be minus infinity um so the the x the m1 at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2549" target="_blank">00:42:29.040</a></span> | <span class="t">so it will because we have only one element n equal to one this is m1 is also the last um m of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2557" target="_blank">00:42:37.280</a></span> | <span class="t">this it of this for loop it will be equal to the global maximum of the vector made up of only one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2562" target="_blank">00:42:42.560</a></span> | <span class="t">element and l1 will be equal to the previous l which we start from zero so l0 multiplied by a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2570" target="_blank">00:42:50.480</a></span> | <span class="t">correction factor which will be in this case e to the power of minus infinity because the correction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2574" target="_blank">00:42:54.960</a></span> | <span class="t">factor is the previous estimate of the max of the max minus the current estimate of the max but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2581" target="_blank">00:43:01.360</a></span> | <span class="t">previous estimate of the max is minus infinity minus x1 it is equal to minus infinity so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2587" target="_blank">00:43:07.120</a></span> | <span class="t">one will be this will be cancelled out and then plus e to the power of x1 minus the current maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2594" target="_blank">00:43:14.640</a></span> | <span class="t">which is x1 so m1 and if this one will be equal to the sum of all the elements of the vector which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2604" target="_blank">00:43:24.000</a></span> | <span class="t">is made up of only one element minus the maximum element in the array which is x1 so we have proven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2611" target="_blank">00:43:31.280</a></span> | <span class="t">that it works for n equal to one now we assume that it works for n does it also work for an array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2619" target="_blank">00:43:39.200</a></span> | <span class="t">of vector or with a vector of size n plus one so let's see what happens at the n plus one iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2629" target="_blank">00:43:49.200</a></span> | <span class="t">at the n plus one iteration we will be doing the maximum of the previous estimate of m which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2634" target="_blank">00:43:54.880</a></span> | <span class="t">the m at the nth iteration and the current element so xn of plus one this by the properties of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2643" target="_blank">00:44:03.120</a></span> | <span class="t">max function it will be actually equal to the maximum of the global vector up to n plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2650" target="_blank">00:44:10.400</a></span> | <span class="t">because the maximum will choose whatever is the maximum between the previous estimate and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2656" target="_blank">00:44:16.320</a></span> | <span class="t">current estimate and ln plus one which is the normalization factor at the n plus one iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2663" target="_blank">00:44:23.520</a></span> | <span class="t">will be equal to the ln so the previous estimate not previous estimate but the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2667" target="_blank">00:44:27.920</a></span> | <span class="t">normalization factor at the nth iteration multiplied by the correction factor which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2675" target="_blank">00:44:35.040</a></span> | <span class="t">previous maximum minus the current maximum plus the exponential of x the current element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2683" target="_blank">00:44:43.600</a></span> | <span class="t">minus the current estimate of the maximum but ln we have we assume that this property so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2694" target="_blank">00:44:54.160</a></span> | <span class="t">algorithm works up to n so ln is for sure equal to the sum of all the exponentials of the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2703" target="_blank">00:45:03.600</a></span> | <span class="t">of the vector up to n minus the local maximum of the vector up to the nth element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2712" target="_blank">00:45:12.800</a></span> | <span class="t">which is mn we multiply by the correction factor if there is something to correct which will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2721" target="_blank">00:45:21.360</a></span> | <span class="t">previous maximum minus the current maximum plus the exponential of the current element minus the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2726" target="_blank">00:45:26.880</a></span> | <span class="t">current estimate of the maximum now by the properties of the exponentials so we can bring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2735" target="_blank">00:45:35.440</a></span> | <span class="t">this one inside of the summation and we will see that this mn and this mn will cancel out because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2742" target="_blank">00:45:42.240</a></span> | <span class="t">it will be exponential of xj minus mn plus mn minus mn plus one so this mn and this mn will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2749" target="_blank">00:45:49.520</a></span> | <span class="t">cancel out and we obtain this one plus this factor here that remains unchanged however you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2755" target="_blank">00:45:55.920</a></span> | <span class="t">that this stuff here is exactly the argument of this summation for the at the iteration n plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2764" target="_blank">00:46:04.800</a></span> | <span class="t">so it is this one is e to the power of xj where j is going from one to n minus mn plus one plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2773" target="_blank">00:46:13.280</a></span> | <span class="t">e to the power of xn plus one minus mn plus one so the j only appears here and it's equal maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2781" target="_blank">00:46:21.360</a></span> | <span class="t">to n and this is similar to being a j with n plus one so we can increase the index of this summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2789" target="_blank">00:46:29.200</a></span> | <span class="t">by one and it will be the same and it will result in the same summation so we have proven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2796" target="_blank">00:46:36.240</a></span> | <span class="t">that also at the n plus one iteration we will have that the l will be equal to the sum of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2803" target="_blank">00:46:43.920</a></span> | <span class="t">the elements of the array the exponential of all the elements of the array up to the n plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2809" target="_blank">00:46:49.760</a></span> | <span class="t">element minus the maximum up to the n plus one element so we have proven that if it works and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2817" target="_blank">00:46:57.840</a></span> | <span class="t">then it also works for n plus one this is enough to prove that it works for all size of arrays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2824" target="_blank">00:47:04.880</a></span> | <span class="t">don't worry if you didn't get the proof by induction it is if it's the first time you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2831" target="_blank">00:47:11.680</a></span> | <span class="t">seeing this kind of proof it may take a little bit to to get it if you want to learn a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2837" target="_blank">00:47:17.520</a></span> | <span class="t">bit more about proof by induction i recommend watching some other proof it's very simple it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2842" target="_blank">00:47:22.000</a></span> | <span class="t">just you need to get into the right mindset anyway let's move forward all right let's talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2849" target="_blank">00:47:29.680</a></span> | <span class="t">block matrix multiplication i know that you want to jump to the code immediately and we will go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2854" target="_blank">00:47:34.720</a></span> | <span class="t">there we just need a little more theory actually so imagine we are doing a matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2860" target="_blank">00:47:40.800</a></span> | <span class="t">so we have a matrix a we want to multiply it with a matrix b and it will produce an output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2867" target="_blank">00:47:47.200</a></span> | <span class="t">c imagine the dimensions of the first matrix are m by k the second matrix is a k by n it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2874" target="_blank">00:47:54.800</a></span> | <span class="t">produce an output matrix that is m by n now imagine we want to parallelize the computation of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2881" target="_blank">00:48:01.600</a></span> | <span class="t">output matrix i know that i didn't talk about gpus yet so we will not talk about gpus we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2888" target="_blank">00:48:08.800</a></span> | <span class="t">talk about parallelization in the case of a multi-core cpu with which you are very probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2894" target="_blank">00:48:14.720</a></span> | <span class="t">familiar with because right now in nowadays when you buy a computer you have a cpu and usually you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2901" target="_blank">00:48:21.120</a></span> | <span class="t">can buy a single core cpu or multi-core like a two core four core eight core etc etc each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2907" target="_blank">00:48:27.760</a></span> | <span class="t">these cores are actually kind of small cpus inside your cpu that can execute operations in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2913" target="_blank">00:48:33.520</a></span> | <span class="t">how to parallelize the matrix multiplication imagine you have this matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2919" target="_blank">00:48:39.520</a></span> | <span class="t">to parallelize each of the output element in this c matrix is a dot product of a row of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2926" target="_blank">00:48:46.320</a></span> | <span class="t">a matrix with a column of the b matrix for example this element on the top left is the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2933" target="_blank">00:48:53.280</a></span> | <span class="t">of the first row of a and the first column of b this element on the top right of c is the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2940" target="_blank">00:49:00.240</a></span> | <span class="t">product of the first row of a and the last column of b this element on the bottom left is the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2947" target="_blank">00:49:07.040</a></span> | <span class="t">product of the last row of a and the first column of b etc etc for all the other elements now to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2953" target="_blank">00:49:13.600</a></span> | <span class="t">parallelize this computation we need as many cores as is as there are elements in c if we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2960" target="_blank">00:49:20.080</a></span> | <span class="t">parallelize it so if m and n are very small then maybe we have enough cores but imagine m and n are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2968" target="_blank">00:49:28.080</a></span> | <span class="t">quite big we imagine like 100 by 100 we don't have 10 000 cores right now in the cpus so how can we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2976" target="_blank">00:49:36.720</a></span> | <span class="t">parallelize a matrix operation by using less cores than there are elements in the matrix itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2984" target="_blank">00:49:44.640</a></span> | <span class="t">that's when we talk about block matrix multiplication basically block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2990" target="_blank">00:49:50.160</a></span> | <span class="t">multiplication means that you can divide the original matrix into smaller blocks of elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=2997" target="_blank">00:49:57.360</a></span> | <span class="t">and then the operations of matrix multiplication can be computed between these blocks for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3004" target="_blank">00:50:04.800</a></span> | <span class="t">imagine we have a matrix that is 8 by 4 it means that it has 8 rows and 4 columns which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3013" target="_blank">00:50:13.680</a></span> | <span class="t">it has 32 elements and then we are multiplying it with another matrix that is 4 by 8 so it has 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3022" target="_blank">00:50:22.560</a></span> | <span class="t">rows and 8 columns so it also has 32 elements the output matrix will should have 64 elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3031" target="_blank">00:50:31.680</a></span> | <span class="t">we don't have 64 cores so how can we parallelize it imagine we only have 8 cores now with 8 cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3039" target="_blank">00:50:39.600</a></span> | <span class="t">we can divide this original matrix a into 4 blocks where the first block is this top left block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3047" target="_blank">00:50:47.520</a></span> | <span class="t">2 by no 4 by 2 elements so um let's say um 8 elements on the top left and then 8 elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3058" target="_blank">00:50:58.960</a></span> | <span class="t">on the top right of this matrix then 8 elements on the bottom left and 8 elements in the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3064" target="_blank">00:51:04.240</a></span> | <span class="t">right of this matrix these are 4 blocks then we divide also the b matrix into um 8 blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3071" target="_blank">00:51:11.200</a></span> | <span class="t">where each block is made up of 4 elements so this b11 is the top left 4 elements in the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3079" target="_blank">00:51:19.680</a></span> | <span class="t">matrix this b4 is the top right 4 elements in the original matrix this b21 is the um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3088" target="_blank">00:51:28.080</a></span> | <span class="t">bottom left 4 elements in the original matrix etc etc etc how do we do this block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3093" target="_blank">00:51:33.600</a></span> | <span class="t">multiplication we can watch these matrices as made only by their blocks so we can view this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3100" target="_blank">00:51:40.720</a></span> | <span class="t">matrix here as made up only by its blocks we can view this matrix here as made up only by its blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3108" target="_blank">00:51:48.320</a></span> | <span class="t">and the output of this multiplication will be a matrices that is computed in the same way as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3115" target="_blank">00:51:55.600</a></span> | <span class="t">original matrix but where the output of each dot product will not be a single element of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3122" target="_blank">00:52:02.880</a></span> | <span class="t">matrix but it will be a block of elements of the output matrix for example the top left block here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3130" target="_blank">00:52:10.720</a></span> | <span class="t">is the dot product of the first row of this matrix with the first column of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3138" target="_blank">00:52:18.480</a></span> | <span class="t">and it will be computed as follows so it will be a11 multiplied by b11 plus a12 multiplied by b21</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3145" target="_blank">00:52:25.920</a></span> | <span class="t">and this output will not be a single scalar but it will be uh well let me count it should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3153" target="_blank">00:52:33.600</a></span> | <span class="t">eight elements so it should be four um made up it should be a block of four elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3160" target="_blank">00:52:40.320</a></span> | <span class="t">or eight elements let me let me count actually so because we have eight blocks and it should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3167" target="_blank">00:52:47.600</a></span> | <span class="t">made up of eight elements let's we can see that here um how to find the dimensions of this output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3175" target="_blank">00:52:55.520</a></span> | <span class="t">block well we can check what is a11 a11 is four by two so it's eight elements in a smaller matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3185" target="_blank">00:53:05.920</a></span> | <span class="t">made up of eight elements where the elements are distributed in four rows and two columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3190" target="_blank">00:53:10.960</a></span> | <span class="t">we are multiplying it by b11 which is a smaller matrix compared to the original made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3196" target="_blank">00:53:16.560</a></span> | <span class="t">two by two elements so four elements so when we multiply four by two multiplied by two by two it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3203" target="_blank">00:53:23.280</a></span> | <span class="t">will produce a four by two output block matrix so block so if we do this computation here block by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3212" target="_blank">00:53:32.240</a></span> | <span class="t">block it will produce a block of output elements of the original matrix so not not a single scalar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3218" target="_blank">00:53:38.400</a></span> | <span class="t">but a block of outputs which makes it very easy to parallelize because if we have only eight cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3224" target="_blank">00:53:44.320</a></span> | <span class="t">we can assign each output block to one core and each core will not produce one output element of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3230" target="_blank">00:53:50.480</a></span> | <span class="t">the original matrix but it will produce eight elements of the original matrix as a four by two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3236" target="_blank">00:53:56.400</a></span> | <span class="t">matrix so basically block matrix allow us to to do the matrix multiplication either by element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3245" target="_blank">00:54:05.360</a></span> | <span class="t">by element so like in the original matrix so each row with each column or blocks by blocks in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3250" target="_blank">00:54:10.320</a></span> | <span class="t">same way like we do normal matrix multiplication because the the matrix multiplication that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3255" target="_blank">00:54:15.280</a></span> | <span class="t">doing between blocks is the same way as we do matrix multiplication with the original matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3261" target="_blank">00:54:21.120</a></span> | <span class="t">and it will produce not a scalar but a block and now let's see why this is very important for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3266" target="_blank">00:54:26.320</a></span> | <span class="t">so why should we care about block matrix multiplication because we are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3273" target="_blank">00:54:33.920</a></span> | <span class="t">compute the following operation so the query multiplied by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3278" target="_blank">00:54:38.640</a></span> | <span class="t">and then we will should apply the softmax of this operation and then we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3282" target="_blank">00:54:42.240</a></span> | <span class="t">multiply the output of the softmax with v for now let's ignore the softmax let's pretend that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3288" target="_blank">00:54:48.960</a></span> | <span class="t">we are not going to apply any softmax so we take the output of the query multiplied by the transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3294" target="_blank">00:54:54.400</a></span> | <span class="t">of the keys and we just multiply it by v to obtain the output of that edge which is wrong of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3298" target="_blank">00:54:58.560</a></span> | <span class="t">but it simplifies our tractation of what we are going to do next so for for this moment let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3304" target="_blank">00:55:04.800</a></span> | <span class="t">pretend that we are not going to apply any softmax so we just do the query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3308" target="_blank">00:55:08.400</a></span> | <span class="t">transpose of the keys and directly we multiply the result of this operation with v this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3312" target="_blank">00:55:12.720</a></span> | <span class="t">result in a matrix that is n by d so n tokens each made up of an embedding of d dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3319" target="_blank">00:55:19.680</a></span> | <span class="t">so lowercase d dimensions and we know that query key and values are themselves matrices of n by d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3328" target="_blank">00:55:28.320</a></span> | <span class="t">dimensions so the n tokens which made up of an embedding of d dimensions so imagine we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3336" target="_blank">00:55:36.400</a></span> | <span class="t">query matrix and the key and the value matrix that are 8 by 128 so we have 8 tokens each token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3344" target="_blank">00:55:44.000</a></span> | <span class="t">made up of 128 dimensions we can divide as we have seen each when we compute a matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3351" target="_blank">00:55:51.600</a></span> | <span class="t">we can divide our matrix into blocks how we choose the blocks is up to us as long as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3359" target="_blank">00:55:59.760</a></span> | <span class="t">operating the shapes of the blocks match when doing the matrix multiplication so for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3365" target="_blank">00:56:05.760</a></span> | <span class="t">in the previous case we divided our matrix a into blocks such that the the shape of the block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3373" target="_blank">00:56:13.360</a></span> | <span class="t">so the matrix that is made up only of the blocks is compatible with the block matrix b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3380" target="_blank">00:56:20.080</a></span> | <span class="t">so that this operation is possible so this is the only requirement that we need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3385" target="_blank">00:56:25.040</a></span> | <span class="t">aware when doing the block matrix multiplication the shapes of the blocked matrix so the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3390" target="_blank">00:56:30.000</a></span> | <span class="t">that is made only of the blocks should match in the matrix multiplication for the rest it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3395" target="_blank">00:56:35.280</a></span> | <span class="t">matter how we divide it so imagine that we choose to divide this query matrix into blocks of rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3402" target="_blank">00:56:42.960</a></span> | <span class="t">and we can do that we don't have to necessarily divide also the columns we can just divide the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3407" target="_blank">00:56:47.200</a></span> | <span class="t">rows so that each q is not a single row but it's a group of two rows so q1 is a group of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3414" target="_blank">00:56:54.640</a></span> | <span class="t">two rows of the q matrix of the q sequence q2 is the group of the second two rows of the q sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3421" target="_blank">00:57:01.120</a></span> | <span class="t">etc etc and we do the same also for v for k we don't do it because we are actually going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3427" target="_blank">00:57:07.760</a></span> | <span class="t">multiply with k transposed so we do the subdivision directly on k transposed so we so we have the q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3435" target="_blank">00:57:15.680</a></span> | <span class="t">which has been divided into groups of rows and then we have a k transposed which is a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3441" target="_blank">00:57:21.920</a></span> | <span class="t">that is 108 by 8 because it's the transpose of the keys which is 8 by 108 and we decide to divide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3451" target="_blank">00:57:31.120</a></span> | <span class="t">each of the column group of columns of k into a single block so the k1 is the first two columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3458" target="_blank">00:57:38.960</a></span> | <span class="t">of k transposed k2 is the second group of two columns in k transposed etc etc until k4 which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3466" target="_blank">00:57:46.400</a></span> | <span class="t">is the last two columns in k transposed the first operation that we do is the multiplication query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3471" target="_blank">00:57:51.840</a></span> | <span class="t">multiplied by the transpose of the keys which basically means that we need to multiply each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3476" target="_blank">00:57:56.640</a></span> | <span class="t">query with all the keys then the second query with all the keys etc etc now each query is not a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3484" target="_blank">00:58:04.960</a></span> | <span class="t">row of the q sequence it's a group of two rows of this q sequence and each k is not a single column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3492" target="_blank">00:58:12.080</a></span> | <span class="t">of k transposed it's a group of two columns of k transposed but doesn't matter because we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3497" target="_blank">00:58:17.200</a></span> | <span class="t">seen that the matrix multiplication if we write the matrices as made up of blocks we just compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3502" target="_blank">00:58:22.880</a></span> | <span class="t">it in the same way when we do a normal matrix multiplication so we are multiplying this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3508" target="_blank">00:58:28.720</a></span> | <span class="t">by this matrix and for what we know this matrix here is made up of four rows with some dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3516" target="_blank">00:58:36.640</a></span> | <span class="t">which is 128 dimensions and this one here is made up of how many rows 128 rows and four columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3526" target="_blank">00:58:46.960</a></span> | <span class="t">i didn't draw the columns because it's too many to draw here but you need to pretend it's a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3533" target="_blank">00:58:53.760</a></span> | <span class="t">dimensions one for each 128 for each vector and here you need to pretend that this is 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3540" target="_blank">00:59:00.080</a></span> | <span class="t">rows when we do the matrix multiplication we apply the normal matrix multiplication procedure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3545" target="_blank">00:59:05.760</a></span> | <span class="t">which is each output element so this first of all the output shape of this matrix of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3550" target="_blank">00:59:10.400</a></span> | <span class="t">multiplication will be four by four because it's the outer dimensions of the two metrics that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3554" target="_blank">00:59:14.560</a></span> | <span class="t">are multiplying the first element of the output will be the dot product of this vector here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3561" target="_blank">00:59:21.280</a></span> | <span class="t">with this vector here the second element so this one here will be the dot product of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3567" target="_blank">00:59:27.120</a></span> | <span class="t">vector here with this vector here however this is not vector and this is not a vector so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3573" target="_blank">00:59:33.840</a></span> | <span class="t">actually a matrix multiplication in this case this element here is not a scalar it is a group of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3580" target="_blank">00:59:40.320</a></span> | <span class="t">elements of the output matrix because we are doing block matrix multiplication and how many elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3585" target="_blank">00:59:45.760</a></span> | <span class="t">it will be well we know that the original q1 is a 2 by 128 the k1 is 108 by 2 so it will be a group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3594" target="_blank">00:59:54.640</a></span> | <span class="t">of 2 by 2 elements of the output matrix so we are doing the matrix multiplication of the q1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3601" target="_blank">01:00:01.840</a></span> | <span class="t">with k1 then q1 with k2 then q1 with k3 q1 with k4 etc etc for the first row and then the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3610" target="_blank">01:00:10.240</a></span> | <span class="t">row will be q2 with all the k's and the q3 with all the k's and q4 with all the k's so as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3616" target="_blank">01:00:16.000</a></span> | <span class="t">see when we do matrix multiplication we don't even care if what is underlying is a block or a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3622" target="_blank">01:00:22.880</a></span> | <span class="t">or a scalar we just apply the same procedure first row of the black block matrix multiplication with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3630" target="_blank">01:00:30.320</a></span> | <span class="t">the first column of the matrix of the second matrix and then the first row with the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3636" target="_blank">01:00:36.400</a></span> | <span class="t">column the first row with the third column etc etc let's then multiply because the formula says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3643" target="_blank">01:00:43.920</a></span> | <span class="t">that we need to multiply query with the transpose of the keys and then multiply by b all of these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3649" target="_blank">01:00:49.280</a></span> | <span class="t">block matrices now as you can see from my using of colors every time i refer to the original matrix i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3657" target="_blank">01:00:57.680</a></span> | <span class="t">use the blue color and every time i refer to the block matrix i use the pink color so we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3664" target="_blank">01:01:04.160</a></span> | <span class="t">multiply the output of the query multiply by the transpose of the key then by v because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3668" target="_blank">01:01:08.720</a></span> | <span class="t">skipping for now the softmax and later we will see why so if we want to do this multiplication we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3674" target="_blank">01:01:14.560</a></span> | <span class="t">need to do the following so it will be uh this matrix is made up of blocks and block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3682" target="_blank">01:01:22.080</a></span> | <span class="t">multiplication just ignores this fact and just does the matrix multiplication like it is a normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3686" target="_blank">01:01:26.560</a></span> | <span class="t">matrix multiplication so we do the first row with the first column then the first row with the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3693" target="_blank">01:01:33.200</a></span> | <span class="t">column then the third row the first row with the third column etc etc so the first block of row how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3700" target="_blank">01:01:40.320</a></span> | <span class="t">is going to be calculated this output in the output matrix of this matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3706" target="_blank">01:01:46.080</a></span> | <span class="t">well it will be the first row so the dot product of the first row the dot product because it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3715" target="_blank">01:01:55.280</a></span> | <span class="t">really a dot product it's the actually the matrix multiplication of the first row but in a dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3720" target="_blank">01:02:00.320</a></span> | <span class="t">product way let's say with the first column which is made up of v1 v2 v3 and v4 so it will be this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3729" target="_blank">01:02:09.360</a></span> | <span class="t">element with v1 plus this element with v2 plus this element with v3 plus this element with v4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3736" target="_blank">01:02:16.640</a></span> | <span class="t">and this will be the first output element the second output block will be this row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3746" target="_blank">01:02:26.080</a></span> | <span class="t">with this column which will be this element with v1 this element plus this element with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3753" target="_blank">01:02:33.360</a></span> | <span class="t">v2 plus this element with v3 plus this element with v4 and this will produce the second output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3759" target="_blank">01:02:39.280</a></span> | <span class="t">block etc etc also for the third and the fourth block output let's look at what is each block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3767" target="_blank">01:02:47.360</a></span> | <span class="t">made up of so each block is made up of the um the first element so query one multiplied by key one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3774" target="_blank">01:02:54.240</a></span> | <span class="t">because um it's the result of the query multiplied by the keys with the v1 of the second matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3781" target="_blank">01:03:01.600</a></span> | <span class="t">plus the this element with this one plus this element with this one plus this element with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3787" target="_blank">01:03:07.760</a></span> | <span class="t">this one so the pseudocode for generating this output of this attention mechanism which is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3794" target="_blank">01:03:14.560</a></span> | <span class="t">really attention mechanism because we skip the softmax but i just want you to get into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3799" target="_blank">01:03:19.120</a></span> | <span class="t">habit of thinking in terms of blocks is the following so we take each query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3805" target="_blank">01:03:25.920</a></span> | <span class="t">we go through each query and as you can see let's look at actually what this output is made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3814" target="_blank">01:03:34.080</a></span> | <span class="t">it is made up of the query one multiplied by key one and the result multiplied by v1 then the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3820" target="_blank">01:03:40.400</a></span> | <span class="t">one with k2 then the result multiplied by b2 then the query one with k3 and the result multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3827" target="_blank">01:03:47.280</a></span> | <span class="t">by v3 plus the query one with the k4 and result multiplied by v4 this is basically what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3833" target="_blank">01:03:53.280</a></span> | <span class="t">doing is the dot product of this row with this column made up of blocks so the the pseudocode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3842" target="_blank">01:04:02.160</a></span> | <span class="t">for generating this first row is the query is then query number one and then we iterate through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3849" target="_blank">01:04:09.280</a></span> | <span class="t">keys and the values from one to four and we sum iteratively so for each block basically to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3856" target="_blank">01:04:16.720</a></span> | <span class="t">this output matrix and if you for each row we will see that it's a different query with all the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3862" target="_blank">01:04:22.800</a></span> | <span class="t">and values and then this will be the the query number three with all the keys and values and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3867" target="_blank">01:04:27.600</a></span> | <span class="t">this will be the query four with all the keys and values so to generate this output matrix we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3873" target="_blank">01:04:33.040</a></span> | <span class="t">to do we iterate through the queries and this will be one row of this output matrix and then we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3879" target="_blank">01:04:39.120</a></span> | <span class="t">to do this iterative sum of the query i that we are iterating through multiplied by the jth k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3886" target="_blank">01:04:46.640</a></span> | <span class="t">and v and we keep summing them iteratively and that would that will produce the output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3893" target="_blank">01:04:53.040</a></span> | <span class="t">or you can see here i know that what i have done so far is not useless not useful for flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3899" target="_blank">01:04:59.280</a></span> | <span class="t">attention but it's useful for us to get into the mindset of computing this product by blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3904" target="_blank">01:05:04.800</a></span> | <span class="t">because later we will use it also with the softmax all right guys i i know that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3911" target="_blank">01:05:11.680</a></span> | <span class="t">computed what we have computed so far is not really the softmax operation it's not sorry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3916" target="_blank">01:05:16.240</a></span> | <span class="t">they're really the attention mechanism because we have skipped the softmax so somehow we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3921" target="_blank">01:05:21.680</a></span> | <span class="t">restore it and the the following few i think 10 20 minutes we are going to be really really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3928" target="_blank">01:05:28.320</a></span> | <span class="t">challenging because i am going to do a lot of operations that will involve a lot of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3934" target="_blank">01:05:34.240</a></span> | <span class="t">blocks and a lot of different matrix multiplication and the variants of the softmax so it may be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3940" target="_blank">01:05:40.880</a></span> | <span class="t">difficult to follow however don't give up you can watch this part twice three times and every time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3947" target="_blank">01:05:47.600</a></span> | <span class="t">you it will have a better understanding i also recommend watch it until we reach the flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3953" target="_blank">01:05:53.440</a></span> | <span class="t">attention algorithm before we start restarting from to to go back to re-watch it because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3960" target="_blank">01:06:00.480</a></span> | <span class="t">watch it we reach the flash attention algorithm and it will give you a better understanding of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3965" target="_blank">01:06:05.920</a></span> | <span class="t">what has happened so far and then you can re-watch it to deepen your understanding another thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3971" target="_blank">01:06:11.760</a></span> | <span class="t">i recommend is take pen and paper and write exactly the operations that you are seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3977" target="_blank">01:06:17.440</a></span> | <span class="t">and write the shapes of each of these blocks of these elements that are made in the that are part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3984" target="_blank">01:06:24.400</a></span> | <span class="t">in this matrix multiplications so that you better understand what is happening and you better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3992" target="_blank">01:06:32.480</a></span> | <span class="t">remember what when i refer to a particular element or a particular block okay after giving this small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=3999" target="_blank">01:06:39.760</a></span> | <span class="t">motivational speech let's start so what we have done so far was query multiplied by the transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4006" target="_blank">01:06:46.880</a></span> | <span class="t">of the keys however each query is not a single row of the query sequence but it's a block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4014" target="_blank">01:06:54.000</a></span> | <span class="t">queries it's a block of rows in our particular case this q1 is not one row of the query sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4022" target="_blank">01:07:02.000</a></span> | <span class="t">it's two rows of the query sequence because we have chosen as a block size a group of two rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4027" target="_blank">01:07:07.280</a></span> | <span class="t">and this k transposed one is not one column of the k transposed matrix is two columns of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4036" target="_blank">01:07:16.240</a></span> | <span class="t">k transposed matrix because we have chosen it like this and if you don't remember let's go back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4041" target="_blank">01:07:21.040</a></span> | <span class="t">see it here we have chosen k1 is two columns and q1 is two rows of the query original matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4050" target="_blank">01:07:30.000</a></span> | <span class="t">and every time i use the blue color i am referring to the original shape and every time i'm using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4056" target="_blank">01:07:36.560</a></span> | <span class="t">pink or violet whatever it is i am referring to the block matrix so it's a block of elements of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4063" target="_blank">01:07:43.920</a></span> | <span class="t">the original matrix okay now the first thing that we have done was a query multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4070" target="_blank">01:07:50.880</a></span> | <span class="t">transpose of the keys and this produces a block matrix as output that we will call s where each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4078" target="_blank">01:07:58.560</a></span> | <span class="t">element sij so the s11 element of this matrix will be the query one with the k transposed one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4086" target="_blank">01:08:06.720</a></span> | <span class="t">this s12 will be query one with k transpose the two s13 will be query one with k transpose the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4094" target="_blank">01:08:14.720</a></span> | <span class="t">three etc etc for all the rows and for all the columns then we should be applying the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4100" target="_blank">01:08:20.400</a></span> | <span class="t">because if you remember the formula is softmax of the query multiplied by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4104" target="_blank">01:08:24.800</a></span> | <span class="t">however i want to restore the softmax operation but with a twist which means that we will apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4111" target="_blank">01:08:31.520</a></span> | <span class="t">the simplified version of the softmax and we will call it softmax star which is just the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4118" target="_blank">01:08:38.160</a></span> | <span class="t">without the normalization so let me write it for you what it means let's do it with the same color</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4125" target="_blank">01:08:45.520</a></span> | <span class="t">that i chose for the softmax which is orange so the softmax if you remember correctly if we remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4132" target="_blank">01:08:52.800</a></span> | <span class="t">it's the softmax of a vector we apply it element wise so each element is modified according to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4143" target="_blank">01:09:03.760</a></span> | <span class="t">following formula so the ith element of the output vector to which we are applying the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4148" target="_blank">01:09:08.640</a></span> | <span class="t">is equal to the exponential of the ith element of the input vector minus the maximum element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4160" target="_blank">01:09:20.080</a></span> | <span class="t">in the input vector divided by a normalization factor that is calculated according to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4166" target="_blank">01:09:26.320</a></span> | <span class="t">summation that is going from j equal to 1 up to n of the exponential of xi minus x max so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4177" target="_blank">01:09:37.920</a></span> | <span class="t">we are doing the exponential of each element minus this x max and why are if you remember correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4184" target="_blank">01:09:44.320</a></span> | <span class="t">why are we subtracting this x max to make this exponential numerically stable computable because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4190" target="_blank">01:09:50.480</a></span> | <span class="t">otherwise it will explode and because we are applying it to the numerator we also need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4194" target="_blank">01:09:54.880</a></span> | <span class="t">apply to the denominator okay the softmax star operation is exactly like the softmax but without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4201" target="_blank">01:10:01.520</a></span> | <span class="t">the normalization part which means that it's just the numerator of the softmax so we will modify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4206" target="_blank">01:10:06.640</a></span> | <span class="t">each element of the vector to which we apply the softmax star according to this formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4211" target="_blank">01:10:11.680</a></span> | <span class="t">let me move it more aligned like this so we just do element element wise operation that is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4219" target="_blank">01:10:19.040</a></span> | <span class="t">exponential of each element minus the maximum of the vector to which we are applying softmax star</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4224" target="_blank">01:10:24.480</a></span> | <span class="t">okay now why did i introduce this softmax star operation because we will be applying it to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4232" target="_blank">01:10:32.240</a></span> | <span class="t">matrix that we have computed so far which is this s matrix so we apply the softmax star to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4239" target="_blank">01:10:39.200</a></span> | <span class="t">element of this s matrix but each element of this s matrix is itself a matrix because it's a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4247" target="_blank">01:10:47.040</a></span> | <span class="t">matrix and each element of this s matrix so for example the element s11 is a two by two matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4253" target="_blank">01:10:53.600</a></span> | <span class="t">because it is coming from the product of two matrices which are a group of rows and a group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4258" target="_blank">01:10:58.880</a></span> | <span class="t">of columns from the q and the k so for example this s11 is what is let's draw it actually this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4267" target="_blank">01:11:07.040</a></span> | <span class="t">s11 will be for example made up of four elements let's call it i don't know a of s11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4275" target="_blank">01:11:15.120</a></span> | <span class="t">uh let's let's choose better naming let's call it i don't know a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4281" target="_blank">01:11:21.120</a></span> | <span class="t">b c and d just the generic elements when we apply the softmax star to this s11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4293" target="_blank">01:11:33.520</a></span> | <span class="t">it will result so let's apply the softmax star softmax star it will result in a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4303" target="_blank">01:11:43.360</a></span> | <span class="t">that is each element the exponential of each element minus the maximum for each row now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4312" target="_blank">01:11:52.480</a></span> | <span class="t">don't know which is the maximum so let's choose one suppose that the maximum for this row is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4316" target="_blank">01:11:56.880</a></span> | <span class="t">and the maximum for this row is d the first element of the output of this softmax star applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4323" target="_blank">01:12:03.760</a></span> | <span class="t">to this block s11 will be the exponential of a minus a because that's what we chose as the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4333" target="_blank">01:12:13.440</a></span> | <span class="t">for this row the second element will be the exponential of b minus a because it's the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4339" target="_blank">01:12:19.760</a></span> | <span class="t">for that row then in the bottom row it will be the exponential of c minus d because that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4348" target="_blank">01:12:28.160</a></span> | <span class="t">maximum for the bottom row and this will be the exponential of d minus t and that's the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4354" target="_blank">01:12:34.400</a></span> | <span class="t">that's how the softmax star will modify each block in this block matrix let me delete this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4361" target="_blank">01:12:41.920</a></span> | <span class="t">otherwise it will remain in my slides forever and later i want to share the slides with you guys so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4366" target="_blank">01:12:46.720</a></span> | <span class="t">you can use my same slides so delete delete okay after we have applied the softmax to each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4376" target="_blank">01:12:56.080</a></span> | <span class="t">elements in this s matrix we will call it the p matrix and each element p11 will again be a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4383" target="_blank">01:13:03.920</a></span> | <span class="t">of two by two elements so p11 will be the softmax so p11 will be the softmax star applied to s11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4395" target="_blank">01:13:15.200</a></span> | <span class="t">where s11 is what is a query one k transposed one and the p12 will be the softmax star applied to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4401" target="_blank">01:13:21.760</a></span> | <span class="t">s12 where s12 is what is a query one multiplied by k transposed two etc etc etc for all the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4409" target="_blank">01:13:29.760</a></span> | <span class="t">of s okay now that we have applied this softmax star operation the next operation that we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4416" target="_blank">01:13:36.480</a></span> | <span class="t">be doing according to the formula of the attention is the softmax of the query multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4421" target="_blank">01:13:41.600</a></span> | <span class="t">transpose of the keys then the result of the softmax multiplied by v i know that we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4426" target="_blank">01:13:46.880</a></span> | <span class="t">apply the real softmax we apply the softmax star which is softmax without the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4432" target="_blank">01:13:52.560</a></span> | <span class="t">later we will see how to compensate this lack of normalization because we will do it at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4438" target="_blank">01:13:58.560</a></span> | <span class="t">and it's something that we can do okay so we take this p matrix which is the result of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4446" target="_blank">01:14:06.480</a></span> | <span class="t">star applied to this s matrix and we multiply it by v what how do we do it well it's a block or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4454" target="_blank">01:14:14.640</a></span> | <span class="t">it's a matrix made up of blocks of matrices so p11 is actually not a scalar but it's a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4461" target="_blank">01:14:21.920</a></span> | <span class="t">of two by two elements and we need to multiply it by v but we don't multiply with the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4467" target="_blank">01:14:27.440</a></span> | <span class="t">sequence v but with the blocked sequence v just like before where each v is not one row of v but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4474" target="_blank">01:14:34.320</a></span> | <span class="t">it's a group of rows of v and how many rows is it is it is two rows of v for now please ignore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4482" target="_blank">01:14:42.800</a></span> | <span class="t">completely whatever i have written here because we will use it later so we need to do this product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4487" target="_blank">01:14:47.440</a></span> | <span class="t">of this matrix here which is made up of blocks remember with this matrix here which is made up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4492" target="_blank">01:14:52.880</a></span> | <span class="t">of blocks it is made up of four rows where each row is not really a row it is a block of rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4500" target="_blank">01:15:00.800</a></span> | <span class="t">and this one it is made up of four by four elements where each element is not really a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4504" target="_blank">01:15:04.720</a></span> | <span class="t">scalar but it's a matrix so as you remember in the block matrix multiplication when the algorithm for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4511" target="_blank">01:15:11.760</a></span> | <span class="t">computing the matrix multiplication is the same as the normal matrix multiplication except that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4516" target="_blank">01:15:16.240</a></span> | <span class="t">we use blocks so what i am doing is guys the following operation so let's write it somewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4522" target="_blank">01:15:22.960</a></span> | <span class="t">let's say o is equal to p multiplied by v okay so the first output row a row because it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4533" target="_blank">01:15:33.520</a></span> | <span class="t">really a row but it's a block row will be computed as follows the first row of this block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4541" target="_blank">01:15:41.200</a></span> | <span class="t">with the first with the first column of this v matrix and we are treating it like a block matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4550" target="_blank">01:15:50.800</a></span> | <span class="t">so it will be p11 multiplied by v1 plus p12 multiplied by v2 plus p13 multiplied by v3 plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4562" target="_blank">01:16:02.320</a></span> | <span class="t">p14 multiplied by v4 this will produce the first output row of o but it's not really a row because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4571" target="_blank">01:16:11.920</a></span> | <span class="t">it's a made up of two rows so this stuff here is not one row it is two row and we can prove that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4579" target="_blank">01:16:19.360</a></span> | <span class="t">because what is p11 p11 is let's write it somewhere so p11 is a 2x2 matrix yeah 2x2 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4590" target="_blank">01:16:30.160</a></span> | <span class="t">we are multiplying it with v1 which is a block of two rows of v so it is a two rows by 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4598" target="_blank">01:16:38.720</a></span> | <span class="t">dimensions so it is equal to 2 by 128 so this stuff here is 2 by 128 so this block here the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4610" target="_blank">01:16:50.080</a></span> | <span class="t">output block that we're computing is a block of two rows of the output matrix that we are computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4615" target="_blank">01:16:55.120</a></span> | <span class="t">i know this is really difficult to follow because we are involving blocks so we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4623" target="_blank">01:17:03.120</a></span> | <span class="t">to visualize at the same time matrix as blocks and as the original matrix that's why i highly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4629" target="_blank">01:17:09.360</a></span> | <span class="t">recommend you to pause the video think it through write down whatever you need to write down because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4636" target="_blank">01:17:16.240</a></span> | <span class="t">it's not easy to follow it just by memorizing the shape so you you actually need to write down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4641" target="_blank">01:17:21.600</a></span> | <span class="t">things anyway we are computing the first output block of the output o matrix now if we if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4651" target="_blank">01:17:31.120</a></span> | <span class="t">remember the output the output this output here should be the output of the output of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4662" target="_blank">01:17:42.560</a></span> | <span class="t">multiplied by v now this softmax has not been applied to the entire row of this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4672" target="_blank">01:17:52.560</a></span> | <span class="t">as matrix here basically to compute this softmax star what we did was to compute the softmax star</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4680" target="_blank">01:18:00.880</a></span> | <span class="t">at each block independently from the other blocks which means that the maximum that we are using to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4687" target="_blank">01:18:07.360</a></span> | <span class="t">compute each softmax star is not the global maximum for the row of this s matrix but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4695" target="_blank">01:18:15.680</a></span> | <span class="t">local maximum of each block and this is wrong actually because when we compute the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4702" target="_blank">01:18:22.640</a></span> | <span class="t">we apply the softmax we should be using the global row i want to give you an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4710" target="_blank">01:18:30.080</a></span> | <span class="t">without using blocks because otherwise i think it's not easy to follow so when we do the normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4716" target="_blank">01:18:36.320</a></span> | <span class="t">attention so we have a query multiplied by the transpose of the keys this produces a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4723" target="_blank">01:18:43.120</a></span> | <span class="t">that is n by n so sequence by sequence where each element of this matrix so let's say three four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4730" target="_blank">01:18:50.640</a></span> | <span class="t">five i don't know how many is one two three four five six yeah six two three four and five six</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4739" target="_blank">01:18:59.600</a></span> | <span class="t">should be one two three four five six okay this one here should be the dot product of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4745" target="_blank">01:19:05.520</a></span> | <span class="t">query with the first um let me use because query one transpose the key one uh this is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4756" target="_blank">01:19:16.880</a></span> | <span class="t">as i said before when we do the product of two vectors we always treat them as column vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4763" target="_blank">01:19:23.120</a></span> | <span class="t">so when you want to write the dot product you cannot multiply two column vectors you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4767" target="_blank">01:19:27.840</a></span> | <span class="t">multiply one row vector with one column vector that's why we transpose this one if it confuses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4772" target="_blank">01:19:32.400</a></span> | <span class="t">you you can also write q1 k1 that's totally fine it's just uh wrong from a notation point of view</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4778" target="_blank">01:19:38.880</a></span> | <span class="t">anyway the first one will be the dot product of the query one with the k1 the second element will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4785" target="_blank">01:19:45.760</a></span> | <span class="t">be the dot product of the query one with the k2 the third will be the query one with the k3 etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4792" target="_blank">01:19:52.480</a></span> | <span class="t">etc um so this is a q1 with k1 q1 with the k2 k2 and q1 with the k3 q1 with the k4 um anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4811" target="_blank">01:20:11.120</a></span> | <span class="t">when we do the softmax we actually calculate the maximum on this entire row however what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4818" target="_blank">01:20:18.400</a></span> | <span class="t">doing is we are actually doing a block matrix multiplication and as you remember um when we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4826" target="_blank">01:20:26.800</a></span> | <span class="t">by blocks we are grouping together rows of queries and rows of keys and in this particular case we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4835" target="_blank">01:20:35.920</a></span> | <span class="t">are grouping two queries together to create one uh one group of queries and two keys together to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4843" target="_blank">01:20:43.920</a></span> | <span class="t">create one block of keys so we need another row of this one so it's the let me choose a query one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4851" target="_blank">01:20:51.200</a></span> | <span class="t">k or query 2k1 this should be query 2k1 query 2k2 query 2k3 query 2k4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4864" target="_blank">01:21:04.000</a></span> | <span class="t">query 2k5 and query 2k6 um when we each of this each of this block here is computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4876" target="_blank">01:21:16.160</a></span> | <span class="t">this block here is computing two by two elements of the original matrix if we had never applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4883" target="_blank">01:21:23.600</a></span> | <span class="t">the blocks so it is computing these two four elements here and if we apply the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4892" target="_blank">01:21:32.960</a></span> | <span class="t">star to each of these blocks we are not using the maximum element in this row we are only using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4899" target="_blank">01:21:39.680</a></span> | <span class="t">maximum element in each block which means that when we will use it in the downstream product with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4906" target="_blank">01:21:46.320</a></span> | <span class="t">vmatrix we will be summing values that are wrong because each of these values here will be based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4914" target="_blank">01:21:54.240</a></span> | <span class="t">on a maximum that is not the global maximum for this row it is the local maximum of this block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4921" target="_blank">01:22:01.120</a></span> | <span class="t">here and um and this block here will have the global the low it will use the local maximum of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4928" target="_blank">01:22:08.320</a></span> | <span class="t">this block here and this block here will use the local maximum of this block here etc etc etc so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4935" target="_blank">01:22:15.520</a></span> | <span class="t">what i'm trying to say is that when you sum p11 with v1 p11 may have some maximum local maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4944" target="_blank">01:22:24.480</a></span> | <span class="t">that is different than from the local maximum of p12 and p13 may have a different maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4950" target="_blank">01:22:30.800</a></span> | <span class="t">local maximum that of p1 p11 and p12 so we need to find a way to fix the maximum that was used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4960" target="_blank">01:22:40.720</a></span> | <span class="t">to compute the exponential here with the maximum found here in case the maximum here is higher than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4969" target="_blank">01:22:49.600</a></span> | <span class="t">the one local to p11 so if we have found for example here a maximum that is higher than the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4975" target="_blank">01:22:55.760</a></span> | <span class="t">maximum used here here then we need to fix this one and this one because that maximum in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4980" target="_blank">01:23:00.960</a></span> | <span class="t">softmax should be the maximum for all the row not the one belonging to the each block and this leads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4987" target="_blank">01:23:07.440</a></span> | <span class="t">to our next step how to fix this first of all let me introduce a little pseudo code for computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=4995" target="_blank">01:23:15.040</a></span> | <span class="t">this output matrix here which is an output block matrix and later we will use this pseudo code to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5003" target="_blank">01:23:23.120</a></span> | <span class="t">adjust the error that we have made in some blocks in case the future blocks so the p13 has a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5010" target="_blank">01:23:30.640</a></span> | <span class="t">maximum than p11 or p12 so to compute this output matrix o we go through so for example to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5019" target="_blank">01:23:39.280</a></span> | <span class="t">the first row we choose well p11 is what is is let's go back p11 is let me delete also this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5026" target="_blank">01:23:46.880</a></span> | <span class="t">it's not needed anymore p11 is the softmax star of q1 k1 p12 is the softmax star of q1 k2 p13 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5038" target="_blank">01:23:58.960</a></span> | <span class="t">the softmax star of q1 k3 p14 is the softmax star of q1 k4 which means that to compute this block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5050" target="_blank">01:24:10.960</a></span> | <span class="t">here here we first need to compute the p11 what is p11 well p11 is the softmax star of a block of q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5059" target="_blank">01:24:19.520</a></span> | <span class="t">and another block of k which in the case of the first row of the output matrix means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5066" target="_blank">01:24:26.320</a></span> | <span class="t">it is the query 1 with the softmax star of the query 1 with q1 the softmax star of the query 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5074" target="_blank">01:24:34.160</a></span> | <span class="t">with k2 the softmax star of the query 1 with k3 the softmax star of the query 1 with k4 which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5079" target="_blank">01:24:39.920</a></span> | <span class="t">means that we need to go we need to make a for loop through all the keys while keeping the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5086" target="_blank">01:24:46.400</a></span> | <span class="t">fixed so to compute the first output row we need to do the softmax star to produce p11 we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5093" target="_blank">01:24:53.680</a></span> | <span class="t">do the softmax star of query 1 k1 and we sum it initially to zeros because we don't we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5102" target="_blank">01:25:02.640</a></span> | <span class="t">initialize our output somehow and we initialize it with zeros then we sum the next p12 which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5110" target="_blank">01:25:10.320</a></span> | <span class="t">the query 1 with the k2 and then we sum the next p13 which is a query 1 with the k3 etc etc that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5116" target="_blank">01:25:16.960</a></span> | <span class="t">why we have this inner loop here all right so however this output that we are computing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5123" target="_blank">01:25:23.280</a></span> | <span class="t">wrong because i told you we have computed the softmax star using statistics the maximum value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5130" target="_blank">01:25:30.000</a></span> | <span class="t">that is belonging to each block and not the one that is the overall row of the original matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5135" target="_blank">01:25:35.360</a></span> | <span class="t">how to fix that we have a tool actually we have computed before an algorithm called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5141" target="_blank">01:25:41.520</a></span> | <span class="t">online softmax i don't know if i referred to it before as the online softmax but it's called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5145" target="_blank">01:25:45.680</a></span> | <span class="t">online softmax that allows to fix previous iterations when we are computing the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5152" target="_blank">01:25:52.400</a></span> | <span class="t">iteration based how well let's review the online softmax we start imagine we are working with one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5160" target="_blank">01:26:00.560</a></span> | <span class="t">single vector so we are a vector made up of n elements what we do is we do a for loop where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5167" target="_blank">01:26:07.600</a></span> | <span class="t">we compute iteratively the maximum up to the height element and we fix the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5175" target="_blank">01:26:15.920</a></span> | <span class="t">computed in previous iteration in case we found a better maximum at the current element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5182" target="_blank">01:26:22.480</a></span> | <span class="t">if this is not clear guys go back and watch the online software because this is very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5187" target="_blank">01:26:27.360</a></span> | <span class="t">because this is what we are going to use to fix this p11 p12 blocks in case we found better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5193" target="_blank">01:26:33.280</a></span> | <span class="t">maximum in p13 or p14 etc so let's see how to apply this online softmax to this case here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5202" target="_blank">01:26:42.240</a></span> | <span class="t">so that we can compute so you may be wondering why are we going through all these troubles i mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5209" target="_blank">01:26:49.360</a></span> | <span class="t">why the real reason is when first of all why did we introduce block matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5215" target="_blank">01:26:55.920</a></span> | <span class="t">because we want to compute matrix multiplication in parallel so you can think that each of this p11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5222" target="_blank">01:27:02.000</a></span> | <span class="t">because they are independent from each other and because each of them are using the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5226" target="_blank">01:27:06.720</a></span> | <span class="t">belonging to each block they can be computed independently from each other then however we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5231" target="_blank">01:27:11.760</a></span> | <span class="t">need to somehow aggregate their value and to aggregate the value we need to fix the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5237" target="_blank">01:27:17.920</a></span> | <span class="t">that have been calculated independently because we didn't when computing values independently we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5242" target="_blank">01:27:22.480</a></span> | <span class="t">don't have a global view we have a local view so we compute local blocks p11 p12 p13 etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5249" target="_blank">01:27:29.440</a></span> | <span class="t">and then when we aggregate these values we need to fix them so that's why we are trying to come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5256" target="_blank">01:27:36.240</a></span> | <span class="t">up with this system of fixing values that have been calculated independently so how to fix this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5264" target="_blank">01:27:44.000</a></span> | <span class="t">let's look at the following algorithm first of all this o block here as i said before it is a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5272" target="_blank">01:27:52.400</a></span> | <span class="t">of two rows where each row is made up of 128 dimensions and we have seen that before by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5279" target="_blank">01:27:59.600</a></span> | <span class="t">checking the dimensions of p11 and v1 the result of p11 v1 which means that for each output block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5286" target="_blank">01:28:06.880</a></span> | <span class="t">we need to take care of two maximums and two normalization factors so up to now i didn't use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5293" target="_blank">01:28:13.840</a></span> | <span class="t">the normalization factor we said that we are applying the softmax star which is the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5297" target="_blank">01:28:17.600</a></span> | <span class="t">without the normalization but eventually we will need to compute this normalization so we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5303" target="_blank">01:28:23.760</a></span> | <span class="t">create an algorithm that fixes the maximum used to compute each of this p11 and also computes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5310" target="_blank">01:28:30.640</a></span> | <span class="t">simultaneously the normalization factor and at the end we will apply this normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5316" target="_blank">01:28:36.240</a></span> | <span class="t">and the way we will do it is as follows we start with initializing the maximum to minus infinity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5323" target="_blank">01:28:43.040</a></span> | <span class="t">one for each row that we are computing so our output block is made up of two rows so we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5327" target="_blank">01:28:47.840</a></span> | <span class="t">one maximum for the top row and one maximum for the bottom row and also the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5333" target="_blank">01:28:53.600</a></span> | <span class="t">which we initialize with zero because we didn't sum anything for now and the output we initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5338" target="_blank">01:28:58.880</a></span> | <span class="t">it with all zeros because we didn't sum anything to this output for now we compute the we uh to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5347" target="_blank">01:29:07.040</a></span> | <span class="t">compute the output row so this output block here so this output block here we need to go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5354" target="_blank">01:29:14.320</a></span> | <span class="t">all the keys uh to produce this p11 p12 p13 p14 while the query is the query number one the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5361" target="_blank">01:29:21.840</a></span> | <span class="t">block number one so the first step that we do is we compute the maximum of the first block p11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5370" target="_blank">01:29:30.880</a></span> | <span class="t">which is the row max so the maximum for each row of the block q1 k1 this is not p11 it's s1 sorry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5382" target="_blank">01:29:42.400</a></span> | <span class="t">guys this is s11 so we compute the maximum of this one and we call it actually s1 as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5390" target="_blank">01:29:50.800</a></span> | <span class="t">and then we can calculate p11 which is the softmax star which is the exponential of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5400" target="_blank">01:30:00.080</a></span> | <span class="t">query multiple query one k1 so s1 minus the maximum in the local group s1 and we add it to our output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5411" target="_blank">01:30:11.920</a></span> | <span class="t">for now the output is initialized with zero so for now ignore this part here i will explain it later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5419" target="_blank">01:30:19.440</a></span> | <span class="t">so for now all one should be equal only to p11 v1 now at the step number two we may find in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5429" target="_blank">01:30:29.760</a></span> | <span class="t">local group s12 so this one is s12 we may find a better maximum for the top row and the bottom row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5439" target="_blank">01:30:39.040</a></span> | <span class="t">and this maximum is the m2 which may be better than the previous maximum for each of these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5446" target="_blank">01:30:46.320</a></span> | <span class="t">row but may also not be so we need to find a way to fix in case it's better and to not fix anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5452" target="_blank">01:30:52.720</a></span> | <span class="t">in case it's not better and the way we do it is this so we compute the new maximum of the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5459" target="_blank">01:30:59.200</a></span> | <span class="t">local row query two we calculate the p12 which is the softmax star of s2 which is s2 minus m2 which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5471" target="_blank">01:31:11.200</a></span> | <span class="t">is the local maximum and then we need to add it to the output however in this case we may have found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5478" target="_blank">01:31:18.800</a></span> | <span class="t">a better maximum so how to fix the o1 which only used the maximum that was local to s1 well we know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5488" target="_blank">01:31:28.400</a></span> | <span class="t">that we can fix that by using exponentials because each of this element of o1 is just an exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5495" target="_blank">01:31:35.920</a></span> | <span class="t">without the normalization because we are applying softmax star so how to fix an exponential with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5501" target="_blank">01:31:41.920</a></span> | <span class="t">another exponential so basically we are saying that we multiply o1 which is a matrix so let me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5509" target="_blank">01:31:49.920</a></span> | <span class="t">show you what is this matrix so o1 is a matrix made up of two rows so as you can see here i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5518" target="_blank">01:31:58.160</a></span> | <span class="t">have the shape of o1 it's a 2x128 matrix so this is the top row so o11 o12 blah blah until o1 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5531" target="_blank">01:32:11.680</a></span> | <span class="t">then o21 o22 blah blah and o2128 we need to fix this value how we basically just using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5545" target="_blank">01:32:25.120</a></span> | <span class="t">exponential that we have used in the online softmax that we have seen before so if we multiply this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5551" target="_blank">01:32:31.120</a></span> | <span class="t">matrix here by a diagonal matrix that is made as follows it's a diagonal matrix made up of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5559" target="_blank">01:32:39.280</a></span> | <span class="t">elements because the exponential of m1 minus m2 will be a vector of two elements and exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5566" target="_blank">01:32:46.160</a></span> | <span class="t">of a element wise exponential is another vector of two elements and this basically means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5573" target="_blank">01:32:53.280</a></span> | <span class="t">diagonal matrix where in the diagonal we have the elements of the vector to which we are applying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5578" target="_blank">01:32:58.800</a></span> | <span class="t">this diag operation which means that this value here will be the exponential of the first element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5586" target="_blank">01:33:06.000</a></span> | <span class="t">of m1 so let me show you how to write it exponential of m1 minus m2 minus m2 so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5601" target="_blank">01:33:21.840</a></span> | <span class="t">first element so let's call it one here here is a zero here will be zero and let's delete this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5608" target="_blank">01:33:28.720</a></span> | <span class="t">and we write another one here exponential m1 minus m2 but the second element of this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5618" target="_blank">01:33:38.320</a></span> | <span class="t">so basically the diag this notation here diag means basically just take the vector and distribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5624" target="_blank">01:33:44.560</a></span> | <span class="t">it over a n by n matrix where n is the size of the vector to which is applied and all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5630" target="_blank">01:33:50.720</a></span> | <span class="t">elements of this matrix should be zeros this is what this diag means if we do this operation here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5637" target="_blank">01:33:57.040</a></span> | <span class="t">we will see that the output of this multiplication will fix each element of the top row using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5646" target="_blank">01:34:06.400</a></span> | <span class="t">exponential and the bottom row with this exponential which will basically cancel out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5652" target="_blank">01:34:12.880</a></span> | <span class="t">this m1 that was computed in the previous iteration and introduce the m2 that we have computed in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5658" target="_blank">01:34:18.720</a></span> | <span class="t">current iteration in each of these elements in this o block matrix okay so this output will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5672" target="_blank">01:34:32.240</a></span> | <span class="t">this element will multiply by this one so it will fix o11 with this factor here and o21 will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5681" target="_blank">01:34:41.520</a></span> | <span class="t">not be fixed by will be multiplied by zero so it will not contribute to this first output element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5687" target="_blank">01:34:47.920</a></span> | <span class="t">so this element here will only depend on o11 fixed by the exponential of m1 minus m2 but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5696" target="_blank">01:34:56.400</a></span> | <span class="t">first element of this vector and then o12 will also be fixed by um o12 will be fixed by this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5704" target="_blank">01:35:04.240</a></span> | <span class="t">exponential here but not by this one and all the dimensions of the first row will be fixed by this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5708" target="_blank">01:35:08.800</a></span> | <span class="t">exponential and all the dimensions of the second row here will be fixed by this exponential here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5715" target="_blank">01:35:15.920</a></span> | <span class="t">this this scalar here which is the second element of the vector exp of m1 minus m2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5722" target="_blank">01:35:22.240</a></span> | <span class="t">okay it was really challenging this one so so what we are doing is we compute p12 and we fix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5732" target="_blank">01:35:32.400</a></span> | <span class="t">all the elements in p1 by multiplying by this matrix here by multiplying by this factor here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5739" target="_blank">01:35:39.520</a></span> | <span class="t">matrix factor here and when we will compute step 3 we will fix step 2 etc etc etc now let's talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5748" target="_blank">01:35:48.240</a></span> | <span class="t">about the normalization factor because for now we have been ignoring it the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5754" target="_blank">01:35:54.160</a></span> | <span class="t">is something that we can compute while computing these maximums because it is provided in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5760" target="_blank">01:36:00.560</a></span> | <span class="t">pseudocode of the online algorithm that we have seen before for the softmax so while computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5765" target="_blank">01:36:05.680</a></span> | <span class="t">the maximum we can actually compute the normalization factor by fixing the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5771" target="_blank">01:36:11.040</a></span> | <span class="t">factor of the previous iteration and this is exactly what we are doing here so at the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5776" target="_blank">01:36:16.000</a></span> | <span class="t">iteration we compute the normalization factor using the local maximum and at the second iteration so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5782" target="_blank">01:36:22.000</a></span> | <span class="t">you can for now ignore uh this one because we are not fixing l0 with anything because l0 will be 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5787" target="_blank">01:36:27.840</a></span> | <span class="t">so we are just basically um we are just computing this summation here so l0 will be 0 so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5795" target="_blank">01:36:35.520</a></span> | <span class="t">factor here will be 0 um and when computing l2 so the normalization step at the second iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5803" target="_blank">01:36:43.120</a></span> | <span class="t">we will fix l1 with an exponential which guess what it's exactly the same exponential that fixes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5810" target="_blank">01:36:50.800</a></span> | <span class="t">the maximum uh the p11 so it is the previous estimation of the maximum minus the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5817" target="_blank">01:36:57.920</a></span> | <span class="t">estimation of the maximum plus the new uh normalization factor using the local maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5824" target="_blank">01:37:04.320</a></span> | <span class="t">and we keep doing this job at the end we will obtain a correct output for this uh matrix for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5833" target="_blank">01:37:13.760</a></span> | <span class="t">for this block here but without the normalization how to apply the normalization well the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5840" target="_blank">01:37:20.400</a></span> | <span class="t">normalization is something that is we need to divide each element of this o by the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5847" target="_blank">01:37:27.200</a></span> | <span class="t">factor but because we are keeping while iterating through these four loops we also calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5854" target="_blank">01:37:34.400</a></span> | <span class="t">normalization factor we keep accumulating it until we reach the end of the iteration and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5860" target="_blank">01:37:40.240</a></span> | <span class="t">we apply the normalization factor so we take the last output and we just divide it by l4 which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5866" target="_blank">01:37:46.240</a></span> | <span class="t">the normalization factor calculated as the fourth iteration and that will fix the softmax all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5873" target="_blank">01:37:53.760</a></span> | <span class="t">guys so now that we have derived the algorithm of how to compute this output of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5880" target="_blank">01:38:00.000</a></span> | <span class="t">blockwise while also fixing the softmax which is done independently in each single block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5885" target="_blank">01:38:05.600</a></span> | <span class="t">we know that the normalization is done at the end i want to also prove it so what we done when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5892" target="_blank">01:38:12.560</a></span> | <span class="t">we introduced this algorithm that computes the softmax in an online way we proved by induction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5899" target="_blank">01:38:19.120</a></span> | <span class="t">that this algorithm is correct so at the end of this algorithm this l of the last iteration will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5905" target="_blank">01:38:25.760</a></span> | <span class="t">actually be the normalization factor that we can apply to get the softmax so we don't apply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5913" target="_blank">01:38:33.360</a></span> | <span class="t">normalization while computing this output in an online way iteratively way by multiplying the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5919" target="_blank">01:38:39.600</a></span> | <span class="t">query with all the blocks of keys we apply it at the end of this four iteration and at the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5927" target="_blank">01:38:47.840</a></span> | <span class="t">this four iteration we will have the last output and we also know that the last l will contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5935" target="_blank">01:38:55.280</a></span> | <span class="t">exact normalization factor that we need to apply to each row because this o of four is a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5942" target="_blank">01:39:02.160</a></span> | <span class="t">of output rows which is if you remember from the attention mechanism each output the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5949" target="_blank">01:39:09.920</a></span> | <span class="t">of the attention has the same shape as the input query vector which is a sequence of tokens so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5957" target="_blank">01:39:17.280</a></span> | <span class="t">o is a sequence of tokens that we need to apply the normalization to and we know that the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5963" target="_blank">01:39:23.600</a></span> | <span class="t">factor is l4 so let's prove this simple formula l4 is a vector one for that contains as many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5971" target="_blank">01:39:31.200</a></span> | <span class="t">elements as there are rows in o4 so in this o block of rows suppose that it contains two rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5982" target="_blank">01:39:42.080</a></span> | <span class="t">like in the algorithm that i have described so far in which we pretend that we are grouping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5986" target="_blank">01:39:46.480</a></span> | <span class="t">two rows of queries with two columns of keys together so the output o the block o will contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=5994" target="_blank">01:39:54.960</a></span> | <span class="t">two rows of the output so we will have two normalization factor in this l4 vector here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6001" target="_blank">01:40:01.600</a></span> | <span class="t">what we are doing with this formula is we are taking this l4 vector and we are creating a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6008" target="_blank">01:40:08.480</a></span> | <span class="t">diagonal matrix with it and then we are computing the inverse of this diagonal matrix so l4 is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6016" target="_blank">01:40:16.240</a></span> | <span class="t">vector that contains two normalization factors so it's l i don't know let's call it l l4 element 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6024" target="_blank">01:40:24.720</a></span> | <span class="t">and l4 element 2 this is our l4 vector then we have o4 o4 is a matrix as you can see from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6038" target="_blank">01:40:38.160</a></span> | <span class="t">shape is a 2 by 128 matrix so o is let's copy it actually oh no it's not copied o4 is a matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6048" target="_blank">01:40:48.720</a></span> | <span class="t">is two rows with 128 elements so the first row with 128 dimensions and the second row with 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6058" target="_blank">01:40:58.400</a></span> | <span class="t">dimensions the first thing that we are doing with this l4 is we are converting it into a diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6064" target="_blank">01:41:04.080</a></span> | <span class="t">matrix which will be a diagonal matrix 2 by 2 because it contains two elements so it will become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6070" target="_blank">01:41:10.240</a></span> | <span class="t">something like this so it will be l4 the first element of l4 0 and then 0 l4 the second element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6079" target="_blank">01:41:19.520</a></span> | <span class="t">of this vector then we are computing the inverse of this matrix the inverse of a diagonal matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6086" target="_blank">01:41:26.640</a></span> | <span class="t">is just the diagonal matrix with each element on the diagonal that becomes its reciprocal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6093" target="_blank">01:41:33.760</a></span> | <span class="t">this is from linear algebra it's not i'm making it i'm making this up so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6098" target="_blank">01:41:38.320</a></span> | <span class="t">the inverse of this matrix here is equal to the same diagonal matrix but where each element is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6108" target="_blank">01:41:48.720</a></span> | <span class="t">1 over l4 the first element of l4 0 0 and 1 over l4 the second element of l4 and then we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6120" target="_blank">01:42:00.000</a></span> | <span class="t">multiplying this stuff here so let me delete some stuff so this stuff here is getting multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6126" target="_blank">01:42:06.880</a></span> | <span class="t">by o which is a matrix that is a 2 by 128 so we are doing this multiplication now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6135" target="_blank">01:42:15.440</a></span> | <span class="t">multiply now the output of this so this is a 2 let me write it 2 by 2 multiplied by 2 by 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6147" target="_blank">01:42:27.920</a></span> | <span class="t">will be a matrix that is 2 by 128 where the first dimension of the first row of the output of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6160" target="_blank">01:42:40.640</a></span> | <span class="t">operation will be the dot product of this call this row here with the first column so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6169" target="_blank">01:42:49.600</a></span> | <span class="t">we are dividing this element here by l4 the first element of l4 the second output element here will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6176" target="_blank">01:42:56.240</a></span> | <span class="t">be the dot product of this row with the second column so we are only multiplying we are dividing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6182" target="_blank">01:43:02.160</a></span> | <span class="t">the the second element here of this input vector here by l4 the first element of l4 because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6189" target="_blank">01:43:09.680</a></span> | <span class="t">all the elements of the second row will be multiplied by 0 so they will not contribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6193" target="_blank">01:43:13.440</a></span> | <span class="t">to this output row while the second output row will be the dot this element here will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6198" target="_blank">01:43:18.480</a></span> | <span class="t">dot product of this row with the first column the first element here is multiplied by 0 so it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6205" target="_blank">01:43:25.280</a></span> | <span class="t">not contribute to this output so it's only the second element the first row of the second this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6210" target="_blank">01:43:30.800</a></span> | <span class="t">first element of the second row of the input matrix here will be divided by l4 2 so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6217" target="_blank">01:43:37.040</a></span> | <span class="t">this will be applied will divide all the elements in the second row and this will divide all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6222" target="_blank">01:43:42.720</a></span> | <span class="t">element in the first row in producing this one here which is exactly what we need to do when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6227" target="_blank">01:43:47.040</a></span> | <span class="t">we want to normalize we need to apply this normalization factor and this should help you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6232" target="_blank">01:43:52.080</a></span> | <span class="t">better visualize why this operation is normalizing the vectors of the output at the end and still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6237" target="_blank">01:43:57.920</a></span> | <span class="t">obtaining the same result now let's proceed further all right guys finally we are ready to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6244" target="_blank">01:44:04.400</a></span> | <span class="t">see the flash attention forward pass by also comparing it with what we have derived so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6251" target="_blank">01:44:11.120</a></span> | <span class="t">so if you look at the flash attention paper first of all this is the flash attention 2 forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6256" target="_blank">01:44:16.720</a></span> | <span class="t">and later i will explain what are the differences between the flash attention 1 and the flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6260" target="_blank">01:44:20.240</a></span> | <span class="t">attention 2 i didn't want to jump directly to this forward pass because i believe that even if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6267" target="_blank">01:44:27.440</a></span> | <span class="t">derivation like the derivation was a little uh difficult to follow i believe that it gave you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6273" target="_blank">01:44:33.280</a></span> | <span class="t">some intuition into what is happening so even if you understand 50 percent of it that's enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6277" target="_blank">01:44:37.760</a></span> | <span class="t">because later we will also code it and you should reach like a 90 percent of understanding so every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6283" target="_blank">01:44:43.200</a></span> | <span class="t">time we introduce some new information it should improve your your understanding so basically in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6289" target="_blank">01:44:49.600</a></span> | <span class="t">flash attention what we are flash attention 2 especially we take our as input we have our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6296" target="_blank">01:44:56.400</a></span> | <span class="t">query key and values which are a sequence of tokens each token is made up of a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6301" target="_blank">01:45:01.760</a></span> | <span class="t">of d dimensions and the d lowercase d dimensions and we divide this query guess what into blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6309" target="_blank">01:45:09.920</a></span> | <span class="t">in how many blocks well depending on this parameter br which is the size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6316" target="_blank">01:45:16.800</a></span> | <span class="t">query block that we want to choose so how many rows of query we want to group together into one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6322" target="_blank">01:45:22.400</a></span> | <span class="t">block and we also do it with the k and v and we divided that into blocks of depending on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6332" target="_blank">01:45:32.080</a></span> | <span class="t">parameter bc then we also initialize the output which is the output that we want to produce so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6338" target="_blank">01:45:38.480</a></span> | <span class="t">what is the flash attention computing well the flash attention is computing the following so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6343" target="_blank">01:45:43.760</a></span> | <span class="t">it's computing the softmax softmax of the query multiplied by the transpose of the keys divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6351" target="_blank">01:45:51.280</a></span> | <span class="t">by the some normalization factor multiply that by b and so that's what it's going to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6359" target="_blank">01:45:59.920</a></span> | <span class="t">and it's going to compute it this way first of all there is an outer loop through the queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6365" target="_blank">01:46:05.520</a></span> | <span class="t">which corresponds to the same pseudo code that we have seen before because we want to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6371" target="_blank">01:46:11.520</a></span> | <span class="t">each block of the output matrix in parallel with the with respect to the others so basically we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6379" target="_blank">01:46:19.120</a></span> | <span class="t">want to compute this output block and this block output block independently this output block here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6386" target="_blank">01:46:26.480</a></span> | <span class="t">depends on the query one and all the keys this output block here depends on the k query two and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6393" target="_blank">01:46:33.040</a></span> | <span class="t">all the keys this output block here depends on the query three and all the keys where query one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6398" target="_blank">01:46:38.560</a></span> | <span class="t">is not the first query but it's the first group of queries or first block of queries query two is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6404" target="_blank">01:46:44.320</a></span> | <span class="t">not the first query two is not the second row of the very metric but it's the second block of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6409" target="_blank">01:46:49.360</a></span> | <span class="t">query matrix etc etc and so that's why we have this outer iteration among all the blocks because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6419" target="_blank">01:46:59.760</a></span> | <span class="t">we want to compute all those blocks of the output matrix in parallel but to compute each of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6425" target="_blank">01:47:05.360</a></span> | <span class="t">output block we need to go to an iteration among all the keys that's why we have an inner loop on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6431" target="_blank">01:47:11.520</a></span> | <span class="t">the keys and we do exactly the same operation that we have done so far by hand so first we compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6438" target="_blank">01:47:18.560</a></span> | <span class="t">the s matrix which is what the each block of query with the corresponding block of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6444" target="_blank">01:47:24.080</a></span> | <span class="t">then we compute the local maximum to the current s block this is the local maximum and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6452" target="_blank">01:47:32.160</a></span> | <span class="t">compare it with the maximum of the previous iteration because that's what we do in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6456" target="_blank">01:47:36.400</a></span> | <span class="t">online softmax then we compute the p block which is the softmax star of the s block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6465" target="_blank">01:47:45.200</a></span> | <span class="t">minus the local maximum of the s block then we compute the normalization factor what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6473" target="_blank">01:47:53.440</a></span> | <span class="t">normalization factor it is the summation of all the exponential of the softmax star but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6481" target="_blank">01:48:01.200</a></span> | <span class="t">by fixing the normalization factor of the previous step and we know how to fix the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6487" target="_blank">01:48:07.760</a></span> | <span class="t">normalization factor because we just multiply by an exponential which is the previous maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6493" target="_blank">01:48:13.680</a></span> | <span class="t">minus the current maximum that's what this factor is and then we compute the output exactly using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6499" target="_blank">01:48:19.520</a></span> | <span class="t">the same correction factor that we have seen before which is the diagonal matrix made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6504" target="_blank">01:48:24.720</a></span> | <span class="t">the diagonal where on the diagonal you have the elements of this vector here which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6510" target="_blank">01:48:30.800</a></span> | <span class="t">exponential of the previous maximum minus the current maximum multiplied by the output of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6516" target="_blank">01:48:36.320</a></span> | <span class="t">previous step because we want to fix the previous step because it was based on the previous p which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6520" target="_blank">01:48:40.640</a></span> | <span class="t">was using the maximum of the local previous p plus the current p v which is based on the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6527" target="_blank">01:48:47.920</a></span> | <span class="t">local maximum and it will be fixed by the next iteration okay and at the end after we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6535" target="_blank">01:48:55.520</a></span> | <span class="t">gone through all the case so we have computed all the output block but we didn't apply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6542" target="_blank">01:49:02.000</a></span> | <span class="t">normalization factor and it's applied at the end because while going through each key we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6547" target="_blank">01:49:07.200</a></span> | <span class="t">calculating the l normalization factor for the softmax because inside of this for loop we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6553" target="_blank">01:49:13.360</a></span> | <span class="t">just computing the softmax star so we are not normalizing each value so at the end someone has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6558" target="_blank">01:49:18.720</a></span> | <span class="t">to normalize it and it will be this instruction here which is use the normalization factor that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6568" target="_blank">01:49:28.640</a></span> | <span class="t">we have computed over all the iterations and apply it to each element of O because the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6574" target="_blank">01:49:34.240</a></span> | <span class="t">between the softmax star and the actual softmax is just the division by the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6582" target="_blank">01:49:42.720</a></span> | <span class="t">and this instruction here is actually dividing each O with the corresponding normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6588" target="_blank">01:49:48.080</a></span> | <span class="t">factor one for each row of the block each row in the output block that we are computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6594" target="_blank">01:49:54.800</a></span> | <span class="t">later we will see also what do we do what is what does it what is this SRAM what is the HBM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6603" target="_blank">01:50:03.040</a></span> | <span class="t">for now i just want you to concentrate on the operations that we are doing and they are exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6608" target="_blank">01:50:08.320</a></span> | <span class="t">the same operations that we have done so far later we will see also why do we need to save this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6614" target="_blank">01:50:14.160</a></span> | <span class="t">here and etc etc but for now you should have enough knowledge to be able to follow what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6620" target="_blank">01:50:20.880</a></span> | <span class="t">written in the flash attention paper for with respect to the forward pass algorithm and what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6627" target="_blank">01:50:27.360</a></span> | <span class="t">we are doing basically is just block matrix multiplication and while computing this block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6632" target="_blank">01:50:32.160</a></span> | <span class="t">we fix the previous block by using tricks of the exponential all right now that we have seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6639" target="_blank">01:50:39.920</a></span> | <span class="t">forward pass of the flash attention before we can implement it we still lack a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6644" target="_blank">01:50:44.560</a></span> | <span class="t">knowledge because we don't know anything about the GPUs and we don't know anything about CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6649" target="_blank">01:50:49.360</a></span> | <span class="t">and we don't know anything about Triton so that's what we are going to see next all right guys it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6655" target="_blank">01:50:55.280</a></span> | <span class="t">time for us to explore finally the GPU and the CUDA programming model well let's start by comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6662" target="_blank">01:51:02.240</a></span> | <span class="t">the CPU and the GPU and this will let us understand how CUDA works then so first of all what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6667" target="_blank">01:51:07.440</a></span> | <span class="t">CUDA and what is the GPU the GPU is the hardware unit that we are that we buy and CUDA is a software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6673" target="_blank">01:51:13.600</a></span> | <span class="t">stack made by made by NVIDIA to write software for this GPU that they sell AMD has its own software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6681" target="_blank">01:51:21.760</a></span> | <span class="t">stack and other manufacturer have their own in this particular video we will be seeing example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6687" target="_blank">01:51:27.280</a></span> | <span class="t">of CUDA kernels but the knowledge that you will get can apply also to other GPUs now the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6693" target="_blank">01:51:33.680</a></span> | <span class="t">difference between a CPU and the GPU is its purpose the your computer is right now running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6700" target="_blank">01:51:40.240</a></span> | <span class="t">on a CPU and your operating system is interfacing with the CPU in using the the so-called scheduler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6708" target="_blank">01:51:48.080</a></span> | <span class="t">so right now probably you are running a browser you are also running some other software on your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6712" target="_blank">01:51:52.240</a></span> | <span class="t">computers on your computer and the scheduler is tasked with switching between them very fast on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6718" target="_blank">01:51:58.720</a></span> | <span class="t">your CPU in such a way that it looks like to you that the processes are running concurrently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6723" target="_blank">01:52:03.200</a></span> | <span class="t">this actually is a fake kind of parallelism unless your CPU also has multiple cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6730" target="_blank">01:52:10.560</a></span> | <span class="t">which nowadays CPUs do have so a CPU usually has one or multiple cores but not so many of them so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6737" target="_blank">01:52:17.120</a></span> | <span class="t">usually have a dual core or quad core or eight core CPU and each of these cores can execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6743" target="_blank">01:52:23.680</a></span> | <span class="t">instructions in parallel the CPU is tasked the the main purpose of the CPU is to execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6752" target="_blank">01:52:32.320</a></span> | <span class="t">many different tasks and switching between them very fast so maybe you have a browser that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6758" target="_blank">01:52:38.240</a></span> | <span class="t">running a small game and then you have another movie player but then you have a word processor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6763" target="_blank">01:52:43.440</a></span> | <span class="t">and then you maybe have some utility to manage your to download files etc so most of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6770" target="_blank">01:52:50.640</a></span> | <span class="t">programs actually are not compute intensive are actually I/O bound meaning that most of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6775" target="_blank">01:52:55.120</a></span> | <span class="t">they are either waiting for the network or they are waiting for the disk and they are very different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6780" target="_blank">01:53:00.640</a></span> | <span class="t">from each other in the purpose so the browser is completely different from a movie player</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6784" target="_blank">01:53:04.640</a></span> | <span class="t">and it's completely different from a word processor so the job of the CPU is to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6790" target="_blank">01:53:10.720</a></span> | <span class="t">reduce the latencies of processing all these operations and it's highly optimized to process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6797" target="_blank">01:53:17.040</a></span> | <span class="t">to optimize each of these execution units called the cores which means that each core has a part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6804" target="_blank">01:53:24.000</a></span> | <span class="t">that is tasked to understand first of all what is the next instruction to run or to predict the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6811" target="_blank">01:53:31.200</a></span> | <span class="t">branch of how the what the next operation may be based on the conditions that you are running for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6817" target="_blank">01:53:37.920</a></span> | <span class="t">example if you have a if condition the branch predictor can understand what is the more most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6822" target="_blank">01:53:42.560</a></span> | <span class="t">likely next instruction and can do some optimizations also the CPU is has a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6828" target="_blank">01:53:48.560</a></span> | <span class="t">caches to reduce the latencies in the loading data from all the devices it can interface with it can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6834" target="_blank">01:53:54.000</a></span> | <span class="t">interface with the the RAM for sure but it can also interface with the disk it can also interface</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6839" target="_blank">01:53:59.680</a></span> | <span class="t">with some peripherals like the printer like the mouse like the keyboard etc etc on the other hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6845" target="_blank">01:54:05.680</a></span> | <span class="t">the GPU is not tasked to do many different things at the same time but it's tasked to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6850" target="_blank">01:54:10.960</a></span> | <span class="t">one thing or a few things but on a massive amount of data so the operations that we do on the GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6858" target="_blank">01:54:18.160</a></span> | <span class="t">are requires a lot of computation and for that for this reason most of the area so the physical area</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6866" target="_blank">01:54:26.000</a></span> | <span class="t">of the GPU is dedicated to compute units so this green stuff that you can see here and these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6872" target="_blank">01:54:32.080</a></span> | <span class="t">called cores and you can see that the part that is dedicated to the control area so the part that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6878" target="_blank">01:54:38.720</a></span> | <span class="t">tasked with understanding what is the next instruction to run or to do some optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6884" target="_blank">01:54:44.000</a></span> | <span class="t">in this the program is very little you may be thinking well does it make it does it make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6890" target="_blank">01:54:50.800</a></span> | <span class="t">GPU less fast compared to the GPU to the CPU well not really because we have many more cores that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6896" target="_blank">01:54:56.720</a></span> | <span class="t">can compensate for these higher latencies okay i can give you a lot of knowledge about the GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6904" target="_blank">01:55:04.800</a></span> | <span class="t">from a theoretical point of view i think the best way to understand the CUDA programming model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6909" target="_blank">01:55:09.040</a></span> | <span class="t">just to jump into the code so we don't get bored okay imagine we have a very simple task and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6915" target="_blank">01:55:15.280</a></span> | <span class="t">have a vector we have two vectors a and b and we want to calculate the sum of these two vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6922" target="_blank">01:55:22.640</a></span> | <span class="t">into and save the result into another vector c where each item is the element wise sum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6929" target="_blank">01:55:29.120</a></span> | <span class="t">corresponding item of a and b how would you proceed with this task on the CPU well you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6936" target="_blank">01:55:36.560</a></span> | <span class="t">do a for loop for example so for example you would make a for loop that starts from the first index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6943" target="_blank">01:55:43.120</a></span> | <span class="t">so the index is zero and c of zero is equal to a of zero plus b of zero then c of one is equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6950" target="_blank">01:55:50.560</a></span> | <span class="t">a of one plus b of one etc and you do a for loop on all the elements of this vector in the GPU we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6959" target="_blank">01:55:59.600</a></span> | <span class="t">want to do the same operation but in parallel because we have a lot of compute units called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6964" target="_blank">01:56:04.480</a></span> | <span class="t">cores and we want all of them to work in parallel so the first thing that we need to understand is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6969" target="_blank">01:56:09.200</a></span> | <span class="t">how to divide the work that we are going to do into sub units of work and dedicate each core</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6976" target="_blank">01:56:16.640</a></span> | <span class="t">to one subunit one simple subdivision would be okay the first core should do this summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6982" target="_blank">01:56:22.640</a></span> | <span class="t">the second core should do this summation the third core should do the summation etc etc so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6988" target="_blank">01:56:28.720</a></span> | <span class="t">imagine we have a eight element vector we need eight cores to do this element wise summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6996" target="_blank">01:56:36.640</a></span> | <span class="t">we will call the course threads because it should also remind you of the multi-threading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7002" target="_blank">01:56:42.000</a></span> | <span class="t">that we already use in operating system so multiple threads work concurrently on the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7006" target="_blank">01:56:46.800</a></span> | <span class="t">on the same or similar job in the GPU let's look at the code now the code that i am going to show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7013" target="_blank">01:56:53.520</a></span> | <span class="t">you is a CUDA kernel and it's written in c but you don't have to understand c and you don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7019" target="_blank">01:56:59.040</a></span> | <span class="t">understand this code what i want you to understand is the intuition behind it because later we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7023" target="_blank">01:57:03.760</a></span> | <span class="t">need this knowledge and convert it into triton which is python and you should already be familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7028" target="_blank">01:57:08.000</a></span> | <span class="t">with python so let's go to the code and i have a very simple vector addition we can see it here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7036" target="_blank">01:57:16.240</a></span> | <span class="t">okay first of all how to do a vector summation usually the gpu is interfaced with a cpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7046" target="_blank">01:57:26.000</a></span> | <span class="t">and the cpu has to first of all tell the gpu what is the data it is going to work with so the cpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7053" target="_blank">01:57:33.600</a></span> | <span class="t">needs to have these vectors it needs to transfer them to the gpu then the gpu needs to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7059" target="_blank">01:57:39.920</a></span> | <span class="t">vector summation then the cpu has to copy back the information from the output from the gpu to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7065" target="_blank">01:57:45.920</a></span> | <span class="t">the cpu and then make it available to the program this is what we are going to do here so we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7072" target="_blank">01:57:52.080</a></span> | <span class="t">going to allocate a three vectors of size n one called a one called b and one is the output vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7080" target="_blank">01:58:00.560</a></span> | <span class="t">we initialize their items randomly so the a of i is a random number between 0 and 100 excluded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7089" target="_blank">01:58:09.440</a></span> | <span class="t">then we allocate memory on the gpu to hold these vectors and then we copy them to the gpu so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7096" target="_blank">01:58:16.800</a></span> | <span class="t">copy the a vector to the gpu and the b vector to the gpu of course we don't copy the result because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7101" target="_blank">01:58:21.760</a></span> | <span class="t">that's what we want the gpu to populate with the output so we just allocate it on the gpu what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7108" target="_blank">01:58:28.000</a></span> | <span class="t">don't copy our output vector on the gpu because it's it's made of random values then what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7116" target="_blank">01:58:36.400</a></span> | <span class="t">is we launch the kernel the launching the kernel means that we launch a program that the gpu should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7121" target="_blank">01:58:41.680</a></span> | <span class="t">execute in parallel on multiple threads or multiple cores each of these threads should do a unit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7128" target="_blank">01:58:48.000</a></span> | <span class="t">operation a unit of work that is independent from the others actually they can be dependent on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7132" target="_blank">01:58:52.800</a></span> | <span class="t">others and but we will not be talking about synchronization so we launch this kernel and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7138" target="_blank">01:58:58.720</a></span> | <span class="t">what we are saying in this line is launch one block of threads and later we will see what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7144" target="_blank">01:59:04.960</a></span> | <span class="t">blocks but you can think of you can ignore this one for now what we are saying here is launch n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7150" target="_blank">01:59:10.320</a></span> | <span class="t">threads so n parallel operations on with the following arguments so the output where we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7158" target="_blank">01:59:18.160</a></span> | <span class="t">to save data the input array a and the b input b and the number of elements let's see what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7165" target="_blank">01:59:25.760</a></span> | <span class="t">inside of this method this method is following a particular syntax that is um how to say CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7173" target="_blank">01:59:33.360</a></span> | <span class="t">specific so this global is actually added it's a like a superset of the c language where we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7179" target="_blank">01:59:39.120</a></span> | <span class="t">some additional keywords that belong to CUDA so it's not really c it's CUDA c so it's a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7188" target="_blank">01:59:48.080</a></span> | <span class="t">simple method as you can see and the first thing that we need to do is CUDA cannot know what each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7195" target="_blank">01:59:55.760</a></span> | <span class="t">thread should do it's we should tell each thread what to do so the mapping between the data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7202" target="_blank">02:00:02.400</a></span> | <span class="t">the what each thread should do it's up to us as software engineer CUDA what we'll do is when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7208" target="_blank">02:00:08.880</a></span> | <span class="t">ask it to launch n threads in parallel it will allocate n threads and assign a unique identifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7215" target="_blank">02:00:15.600</a></span> | <span class="t">to each of these threads in our simple case we can see it like this so it will assign the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7223" target="_blank">02:00:23.120</a></span> | <span class="t">thread the index zero so we are asking for example imagine we have a vector of eight elements it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7229" target="_blank">02:00:29.760</a></span> | <span class="t">assign the first thread index zero here i call it one but it's it's wrong but we can write another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7236" target="_blank">02:00:36.480</a></span> | <span class="t">number here so this will be actually thread zero this will be thread one this will be thread two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7241" target="_blank">02:00:41.120</a></span> | <span class="t">thread three thread four thread five thread six and thread seven so let me delete this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7248" target="_blank">02:00:48.080</a></span> | <span class="t">so we don't get confused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7251" target="_blank">02:00:51.680</a></span> | <span class="t">and what we are saying is that the item that each thread should process is equal to its thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7268" target="_blank">02:01:08.160</a></span> | <span class="t">index so this is the thread zero so it should process the item with index zero this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7274" target="_blank">02:01:14.240</a></span> | <span class="t">thread one and it should process the item with index one this is the thread number two and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7279" target="_blank">02:01:19.040</a></span> | <span class="t">should process the item with index two and this is what we are doing in this line of code we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7284" target="_blank">02:01:24.480</a></span> | <span class="t">saying which item each thread should process which is exactly the thread identifier so the thread id</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7293" target="_blank">02:01:33.120</a></span> | <span class="t">later we will see why why do we have this dot x but that's for later next thing that you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7300" target="_blank">02:01:40.080</a></span> | <span class="t">see is okay we are doing the output of the height position is equal to the a vector as the height</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7308" target="_blank">02:01:48.240</a></span> | <span class="t">position plus the b vector as the height position so it's a very simple summation element wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7312" target="_blank">02:01:52.480</a></span> | <span class="t">you may have noticed this if statement why do we need an if statement if we already know that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7318" target="_blank">02:01:58.640</a></span> | <span class="t">are going to launch eight threads and of course i will be between um we already know that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7327" target="_blank">02:02:07.040</a></span> | <span class="t">going to launch n threads so i should of course be less than n because each thread id will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7332" target="_blank">02:02:12.160</a></span> | <span class="t">between zero and n minus one so why do we need this if condition this is needed because when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7338" target="_blank">02:02:18.640</a></span> | <span class="t">CUDA when it launches a number of threads this number of threads is always a multiple of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7346" target="_blank">02:02:26.720</a></span> | <span class="t">unit which is a 32 in the case of the CUDA so if we have like 34 elements in a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7354" target="_blank">02:02:34.080</a></span> | <span class="t">and we ask CUDA to launch 34 threads CUDA will not launch 34 exactly it will launch 64 threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7360" target="_blank">02:02:40.960</a></span> | <span class="t">so multiple of 32 which is the warp size by the way um and uh what we need to do is we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7369" target="_blank">02:02:49.840</a></span> | <span class="t">ask these threads to only work for we only need to ask the threads that have a corresponding element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7378" target="_blank">02:02:58.240</a></span> | <span class="t">to work and all the others that don't have a corresponding element because the the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7383" target="_blank">02:03:03.120</a></span> | <span class="t">is not large enough for all of them to not do anything so do not enter this uh if statement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7388" target="_blank">02:03:08.400</a></span> | <span class="t">there is another thing that we should learn which is actually the threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7396" target="_blank">02:03:16.320</a></span> | <span class="t">actually when we have a group of threads in in a CUDA programming model but i believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7401" target="_blank">02:03:21.360</a></span> | <span class="t">also in other GPUs a group of threads of 32 threads is called a warp and this 32 threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7410" target="_blank">02:03:30.480</a></span> | <span class="t">will share the same control unit so let's go back to the slide so as you so as you can see here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7419" target="_blank">02:03:39.600</a></span> | <span class="t">have this yellow unit here in the GPU and a group of threads will share the same control unit which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7425" target="_blank">02:03:45.280</a></span> | <span class="t">means that what is this control unit it's a part of the hardware of the GPU that is tasked with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7430" target="_blank">02:03:50.960</a></span> | <span class="t">understanding what is the next instruction to run now if the group of threads is sharing the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7437" target="_blank">02:03:57.120</a></span> | <span class="t">unit it means that this group of thread will always execute the same statement at any time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7443" target="_blank">02:04:03.520</a></span> | <span class="t">they will always work in synchrony will always work on the same instruction they it's it cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7450" target="_blank">02:04:10.000</a></span> | <span class="t">be like this thread is working on one instruction and this one is working on another instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7454" target="_blank">02:04:14.480</a></span> | <span class="t">what does this mean on a programming level it means that if when we launch a group of threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7461" target="_blank">02:04:21.200</a></span> | <span class="t">of course CUDA will spawn more threads than we need if the if the number of elements of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7466" target="_blank">02:04:26.720</a></span> | <span class="t">vector is not a multiple of 32 this means that when we did this thread they will first execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7473" target="_blank">02:04:33.520</a></span> | <span class="t">this operation and each of them will have its own value of this thread id so they will execute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7480" target="_blank">02:04:40.640</a></span> | <span class="t">same instruction but the data at each instruction may be different because each of them have their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7485" target="_blank">02:04:45.840</a></span> | <span class="t">own registers which means that they will always they will for example reach this statement here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7492" target="_blank">02:04:52.560</a></span> | <span class="t">and the first thread will have i equal to zero the second thread will have i equal to one etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7497" target="_blank">02:04:57.200</a></span> | <span class="t">etc even if they are executing the same instruction this programming model is called the single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7501" target="_blank">02:05:01.760</a></span> | <span class="t">instruction multiple data CUDA likes to call it a single instruction multiple thread doesn't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7507" target="_blank">02:05:07.120</a></span> | <span class="t">for us it just means that they will always execute the same instruction but the value of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7512" target="_blank">02:05:12.480</a></span> | <span class="t">variables may be different then after executing this statement they will reach this statement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7519" target="_blank">02:05:19.040</a></span> | <span class="t">here the if statement and of course some of them will evaluate this statement to true and some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7524" target="_blank">02:05:24.640</a></span> | <span class="t">them will execute the statement to false which also means that some of them should enter this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7530" target="_blank">02:05:30.480</a></span> | <span class="t">if statement and some of them should not enter this if statement however because the control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7535" target="_blank">02:05:35.920</a></span> | <span class="t">unit is the same for all of them they will be forced to enter this if statement even if they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7541" target="_blank">02:05:41.040</a></span> | <span class="t">should not so how CUDA manages this control divergence it will basically make work like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7547" target="_blank">02:05:47.200</a></span> | <span class="t">this all the threads for which this if statement is equal to true will enter this if and will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7552" target="_blank">02:05:52.960</a></span> | <span class="t">execute the instructions inside of this if and all the threads that have this statement equal to false</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7560" target="_blank">02:06:00.800</a></span> | <span class="t">so the condition of this if equal to false they will enter the for loop because they cannot not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7566" target="_blank">02:06:06.080</a></span> | <span class="t">enter it because they should be always executing the same instruction at any time but they will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7570" target="_blank">02:06:10.720</a></span> | <span class="t">just not do any operations inside of this for loop they will just sit idle this is um called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7578" target="_blank">02:06:18.000</a></span> | <span class="t">control divergence and it can reduce the um the the throughput of your program so you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7583" target="_blank">02:06:23.920</a></span> | <span class="t">minimize it but you may be wondering why doesn't the gpu dedicate a control unit to each core so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7591" target="_blank">02:06:31.360</a></span> | <span class="t">that they can work independently from each other because the control unit is expensive to add in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7595" target="_blank">02:06:35.600</a></span> | <span class="t">the chip area of the gpu it's much more efficient to add more workers instead of adding a control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7602" target="_blank">02:06:42.000</a></span> | <span class="t">area control units for each worker so this is a design choice of the gpu and it works fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7608" target="_blank">02:06:48.560</a></span> | <span class="t">okay now that we have seen how a kernel works let's move forward to another example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7614" target="_blank">02:06:54.880</a></span> | <span class="t">all right the next example that we are going to see is the following is the same as the as before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7622" target="_blank">02:07:02.000</a></span> | <span class="t">so we are going to do a vector addition but imagine that we have a very large vector so imagine that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7627" target="_blank">02:07:07.360</a></span> | <span class="t">we have a vector with 1 million elements of course we could do like before so we launch a kernel with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7633" target="_blank">02:07:13.760</a></span> | <span class="t">1 million threads the problem is CUDA will reject it because it's a i don't have 1 million threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7640" target="_blank">02:07:20.240</a></span> | <span class="t">to run in parallel so how can we proceed in this case because usually we are working with very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7645" target="_blank">02:07:25.040</a></span> | <span class="t">big matrices or very big vectors so we need to process a massive amount of data so how to manage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7651" target="_blank">02:07:31.680</a></span> | <span class="t">a parallel um let's say parallel computation when we do not have enough uh computation cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7660" target="_blank">02:07:40.400</a></span> | <span class="t">one way is to divide the input vector into blocks of elements for example we may decide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7668" target="_blank">02:07:48.000</a></span> | <span class="t">for example imagine our gpu only has 32 cores in total we may divide our input vector into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7676" target="_blank">02:07:56.480</a></span> | <span class="t">blocks of size 32 such that the first 32 element are the first block the next 32 element are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7682" target="_blank">02:08:02.800</a></span> | <span class="t">second block the third 32 element the third block and the last 32 element are the last block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7688" target="_blank">02:08:08.880</a></span> | <span class="t">in this way we can ask the gpu to work on one block at a time so we can say okay work on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7696" target="_blank">02:08:16.560</a></span> | <span class="t">first block and after it has processed the first block it can work on the second block and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7702" target="_blank">02:08:22.640</a></span> | <span class="t">the third block and the fourth block this also allows the gpu itself to manage a subunit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7709" target="_blank">02:08:29.200</a></span> | <span class="t">work because imagine now we have blocks of 32 elements but we have a gpu of 64 cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7716" target="_blank">02:08:36.320</a></span> | <span class="t">the gpu we can also schedule two blocks at the same time because it has enough cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7722" target="_blank">02:08:42.480</a></span> | <span class="t">so we need to give some granularity uh we need to reduce the ground increase the granularity of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7729" target="_blank">02:08:49.040</a></span> | <span class="t">data to let the gpu decide how many blocks to schedule this is the reason we introduce blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7736" target="_blank">02:08:56.080</a></span> | <span class="t">inside of CUDA so let me make a concrete example but with a very simple assumption imagine our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7742" target="_blank">02:09:02.400</a></span> | <span class="t">gpu only has two cores or let's say four cores actually so we have n is equal to eight elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7750" target="_blank">02:09:10.480</a></span> | <span class="t">eight and we have four cores in total so what we can do for example is to is divide this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7760" target="_blank">02:09:20.320</a></span> | <span class="t">vector into groups of either four cores or even less let's say two two elements at a time so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7767" target="_blank">02:09:27.120</a></span> | <span class="t">is the block number one this is the block number two this is the block number three and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7774" target="_blank">02:09:34.000</a></span> | <span class="t">the block number four we can ask CUDA to launch a kernel that is made up of four blocks and where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7783" target="_blank">02:09:43.120</a></span> | <span class="t">each block is made up of two threads so when we launch the CUDA kernel we can show the code now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7791" target="_blank">02:09:51.040</a></span> | <span class="t">we ask the CUDA where is the instruction this first instruction tells CUDA how many blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7800" target="_blank">02:10:00.240</a></span> | <span class="t">we have and the second part of this in this symbols tells how many threads we have for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7809" target="_blank">02:10:09.920</a></span> | <span class="t">block in our case we want n divided by the block size number of blocks where the block size in my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7818" target="_blank">02:10:18.560</a></span> | <span class="t">picture is two so how many blocks we will have we will have a number of blocks so the number of blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7830" target="_blank">02:10:30.640</a></span> | <span class="t">is n divided by two where two is the block size so this is the block size and this will be equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7840" target="_blank">02:10:40.800</a></span> | <span class="t">to four blocks each of size equal to two and this is what we are doing here so we are saying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7848" target="_blank">02:10:48.880</a></span> | <span class="t">the number of blocks is okay the ceiling because it may not be a multiple of the block size n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7855" target="_blank">02:10:55.200</a></span> | <span class="t">of n divided by the block size and this tells how many blocks we have and this is will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7861" target="_blank">02:11:01.280</a></span> | <span class="t">this will define our grid it means the grid is basically telling how many blocks we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7865" target="_blank">02:11:05.920</a></span> | <span class="t">and then each block is made up of block size number of threads then the problem is how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7872" target="_blank">02:11:12.240</a></span> | <span class="t">we assign the work to do to each of these threads when we launch a kernel like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7878" target="_blank">02:11:18.800</a></span> | <span class="t">with this configuration so the number of blocks and the number of threads per block CUDA will do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7884" target="_blank">02:11:24.720</a></span> | <span class="t">the following job it will assign this block each block a index called the block id where the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7894" target="_blank">02:11:34.880</a></span> | <span class="t">id of the first block is zero so let me write here so this will have the first block will have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7900" target="_blank">02:11:40.560</a></span> | <span class="t">block id equal to zero and in each block it will assign a thread id and the thread id of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7909" target="_blank">02:11:49.920</a></span> | <span class="t">thread of each block will be the thread zero and the second thread will be the thread number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7915" target="_blank">02:11:55.440</a></span> | <span class="t">the second block will have a block id block id equal to one and the first thread of this block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7923" target="_blank">02:12:03.600</a></span> | <span class="t">will be the thread number zero and the second thread of this block will be the thread number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7927" target="_blank">02:12:07.840</a></span> | <span class="t">the third block will have a block id block id equal to two and the first thread will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7936" target="_blank">02:12:16.160</a></span> | <span class="t">thread number zero and the second thread will be thread number one etc until the last block which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7941" target="_blank">02:12:21.760</a></span> | <span class="t">will be equal to three this will be thread number zero and thread number one the problem is now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7947" target="_blank">02:12:27.280</a></span> | <span class="t">based only on the index of the block and the index of the thread how can we map it to what element of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7954" target="_blank">02:12:34.480</a></span> | <span class="t">the vector each thread should work with one simple assignment would be to just do well you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7961" target="_blank">02:12:41.920</a></span> | <span class="t">that in this case we need the this vector this thread here to work with element zero this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7969" target="_blank">02:12:49.200</a></span> | <span class="t">should work with element one this one should work with element number two this one to the element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7974" target="_blank">02:12:54.560</a></span> | <span class="t">number three this one four this one five six and seven this five is so ugly so let me write it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7985" target="_blank">02:13:05.280</a></span> | <span class="t">again how can we find the mapping given only the block id and the thread id how can we find which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7993" target="_blank">02:13:13.200</a></span> | <span class="t">element it should correspond to well it's very simple formula so you can see that the element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=7998" target="_blank">02:13:18.080</a></span> | <span class="t">let's call it the element id which in the code i call it i is equal to the block id</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8008" target="_blank">02:13:28.640</a></span> | <span class="t">multiplied by the size of each block which is a block size let's call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8013" target="_blank">02:13:33.200</a></span> | <span class="t">block size yeah i have it block size plus the thread id</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8020" target="_blank">02:13:40.400</a></span> | <span class="t">because in the case of the first thread this will be equal to zero multiplied by two plus zero which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8029" target="_blank">02:13:49.280</a></span> | <span class="t">is zero in this case it will be equal to zero multiplied by two which is zero plus one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8034" target="_blank">02:13:54.640</a></span> | <span class="t">it will be equal to one in this case it will be equal to one because block id is equal to one one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8039" target="_blank">02:13:59.600</a></span> | <span class="t">multiplied by two is equal to two plus zero is equal to two etc etc and you can see that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8044" target="_blank">02:14:04.400</a></span> | <span class="t">formula works for all the threads so the mapping when we launch a CUDA kernel we are telling the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8050" target="_blank">02:14:10.800</a></span> | <span class="t">gpu how many blocks we want and how many threads there are in each block but CUDA has no notion of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8057" target="_blank">02:14:17.760</a></span> | <span class="t">how to map each CUDA has no way of knowing how to map each thread into the element it should work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8068" target="_blank">02:14:28.720</a></span> | <span class="t">with that's up to us and that's what we are doing here when we are creating this kernel here so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8076" target="_blank">02:14:36.560</a></span> | <span class="t">are telling that each element each thread should work with the ith element of the vector where i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8083" target="_blank">02:14:43.360</a></span> | <span class="t">is calculated as follows the block id to which this thread belongs multiplied by the block size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8088" target="_blank">02:14:48.960</a></span> | <span class="t">so how many threads there are in each block plus the thread id and this will tell the ith element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8096" target="_blank">02:14:56.400</a></span> | <span class="t">this particular thread should work with by giving in let's go back to the slides by choosing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8106" target="_blank">02:15:06.240</a></span> | <span class="t">block size equal to two and having four cores the gpu can choose to run one block or two block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8114" target="_blank">02:15:14.000</a></span> | <span class="t">concurrently if it has enough free cores so that's why we want to work with by block by block because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8120" target="_blank">02:15:20.080</a></span> | <span class="t">it allows the gpu to choose how it want to parallelize the operations if it has enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8125" target="_blank">02:15:25.120</a></span> | <span class="t">cores and we don't need to have n cores for n element vector we can divide it into smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8132" target="_blank">02:15:32.240</a></span> | <span class="t">blocks and let the gpu manage the scheduling let's see one last example and then we move on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8137" target="_blank">02:15:37.360</a></span> | <span class="t">to triton imagine now we want to do a matrix addition instead of doing a vector addition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8143" target="_blank">02:15:43.360</a></span> | <span class="t">now in a matrix addition we have data that we can see on two axes one is the rows and one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8151" target="_blank">02:15:51.200</a></span> | <span class="t">the columns it's usually we represent the vertical axis as the y-axis and the horizontal axis as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8160" target="_blank">02:16:00.080</a></span> | <span class="t">x-axis by using the same blocked intuition that we used before so dividing the data input data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8169" target="_blank">02:16:09.760</a></span> | <span class="t">into blocks this is how we can divide the labor of our matrix addition into blocks for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8176" target="_blank">02:16:16.880</a></span> | <span class="t">we can divide our rows into blocks and call this one the block zero and this one in the block one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8183" target="_blank">02:16:23.120</a></span> | <span class="t">and this one is the block two the same we can do on the x-axis so we can choose this one as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8190" target="_blank">02:16:30.080</a></span> | <span class="t">block zero this one as the block one and this one as the block two on the x-axis with x is the column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8196" target="_blank">02:16:36.320</a></span> | <span class="t">axis and the y is the row axis we don't even have to choose the same block size for the rows and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8203" target="_blank">02:16:43.440</a></span> | <span class="t">columns we can even choose the to group together three columns and two rows instead of doing two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8210" target="_blank">02:16:50.400</a></span> | <span class="t">and two in this case we need to find because as we said before when we launch a CUDA kernel CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8216" target="_blank">02:16:56.800</a></span> | <span class="t">will just assign ids to the blocks and the threads in each block then it's up to us understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8223" target="_blank">02:17:03.760</a></span> | <span class="t">what to how to map the id of the block and its corresponding thread id into the data element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8229" target="_blank">02:17:09.360</a></span> | <span class="t">that this particular thread should work it should work with so in the case of matrix addition we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8235" target="_blank">02:17:15.120</a></span> | <span class="t">could say that each thread should work with one output element of the output matrix c so it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8241" target="_blank">02:17:21.600</a></span> | <span class="t">become the sum of the a element plus the b element and it should map it to the c matrix output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8250" target="_blank">02:17:30.640</a></span> | <span class="t">so how to do it imagine we have six rows and we have six columns one easy way would be to divide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8258" target="_blank">02:17:38.800</a></span> | <span class="t">these rows into three blocks each made up of two rows and each column into three blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8265" target="_blank">02:17:45.920</a></span> | <span class="t">each block made up of two columns CUDA will launch as many blocks as there are the combinations of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8275" target="_blank">02:17:55.200</a></span> | <span class="t">the rows and column blocks so in this case we have three blocks for the columns and three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8280" target="_blank">02:18:00.720</a></span> | <span class="t">blocks for the rows so it will launch nine blocks so this is the block number 00 because it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8290" target="_blank">02:18:10.000</a></span> | <span class="t">CUDA will identify the dimensions of the block based on the axis in which we have divided it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8297" target="_blank">02:18:17.920</a></span> | <span class="t">so we will call this the x dimension the columns and the rows we will call it the y dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8302" target="_blank">02:18:22.960</a></span> | <span class="t">so it will launch as many blocks as there are combinations of x and y in this case we have nine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8308" target="_blank">02:18:28.240</a></span> | <span class="t">so this will be the block 00 this will be the block 01 this will be the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8313" target="_blank">02:18:33.040</a></span> | <span class="t">02 this one will be the block 10 11 and 12 etc etc inside of each block we will also divide the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8322" target="_blank">02:18:42.480</a></span> | <span class="t">threads into x threads and y threads along the two dimensions so this will be the thread 0 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8328" target="_blank">02:18:48.720</a></span> | <span class="t">the thread 1 along the x axis in the x block and this will be the thread 0 and the thread 1 in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8336" target="_blank">02:18:56.080</a></span> | <span class="t">y in the in the block 0 of the y axis and each block will have two threads and they will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8343" target="_blank">02:19:03.840</a></span> | <span class="t">identified as thread 0 and the thread 1 so let's look at how the launch grid works in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8350" target="_blank">02:19:10.560</a></span> | <span class="t">so imagine we have a matrix with number num rows number of rows and num columns num calls number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8359" target="_blank">02:19:19.360</a></span> | <span class="t">of columns and we want to divide each row the rows into block size number of rows and the calls</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8367" target="_blank">02:19:27.600</a></span> | <span class="t">block size number of columns we define basically the number of blocks that we need is this one so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8375" target="_blank">02:19:35.440</a></span> | <span class="t">this is just a fancy way of writing the ceiling of the num rows divided by the rows block size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8381" target="_blank">02:19:41.440</a></span> | <span class="t">and this is just a fancy way of writing the ceiling of the number of columns divided by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8386" target="_blank">02:19:46.800</a></span> | <span class="t">calls block size this tells us how many blocks we will have on the rows and how many we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8390" target="_blank">02:19:50.800</a></span> | <span class="t">have on the columns the grid you can see here which tells us how many blocks we have is a tuple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8397" target="_blank">02:19:57.040</a></span> | <span class="t">that accepts three values which tells how many blocks we want on the x dimension how many we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8402" target="_blank">02:20:02.960</a></span> | <span class="t">want on the y dimension and how many we want on the z dimension we are not going to use the z</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8408" target="_blank">02:20:08.480</a></span> | <span class="t">dimension because we only have a matrix then inside of each block how many threads we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8414" target="_blank">02:20:14.240</a></span> | <span class="t">for the x dimension and for the y dimension as the x dimension we have chosen the columns so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8420" target="_blank">02:20:20.720</a></span> | <span class="t">are saying how many blocks we want the columns and how many blocks we want for the rows and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8425" target="_blank">02:20:25.440</a></span> | <span class="t">inside of each block how many threads we want for the column block and how many threads we want for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8430" target="_blank">02:20:30.720</a></span> | <span class="t">the row block this will define our launch grid and what CUDA will do it will just launch this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8437" target="_blank">02:20:37.280</a></span> | <span class="t">following configuration so it will launch as many blocks as there are combinations of x and y's and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8442" target="_blank">02:20:42.400</a></span> | <span class="t">inside of each x and y it will assign a thread id in such a way that the thread zero on the x-axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8450" target="_blank">02:20:50.720</a></span> | <span class="t">so there will be two threads on the x-axis and the two threads on the y-axis of each block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8455" target="_blank">02:20:55.840</a></span> | <span class="t">now let's try to understand how to map just based on the block id on the x-axis just based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8462" target="_blank">02:21:02.800</a></span> | <span class="t">block id on the y-axis and the thread id on the x and y-axis how to map it to the one element of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8469" target="_blank">02:21:09.840</a></span> | <span class="t">the output matrix let's look at the code so first we can use the following formula to identify which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8477" target="_blank">02:21:17.680</a></span> | <span class="t">row this element should work with which the which because each element of a matrix is identified by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8486" target="_blank">02:21:26.160</a></span> | <span class="t">two indices one is the row identifier and one is the column identifier the row identifier we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8491" target="_blank">02:21:31.680</a></span> | <span class="t">look at it like the block id multiplied by the block size plus the thread id let's see why it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8498" target="_blank">02:21:38.400</a></span> | <span class="t">makes sense so in this case for example this thread will work with the row zero because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8504" target="_blank">02:21:44.640</a></span> | <span class="t">block id is on the y-axis is zero and the thread id zero so it's a block id multiplied by the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8511" target="_blank">02:21:51.600</a></span> | <span class="t">size so zero plus zero it will be zero so this element will be working with the row number zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8518" target="_blank">02:21:58.160</a></span> | <span class="t">and which column it will be working with well it will be working with the block id zero multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8524" target="_blank">02:22:04.000</a></span> | <span class="t">by the block size on the column which is again zero i mean this block size is two but multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8529" target="_blank">02:22:09.760</a></span> | <span class="t">by zero it will be zero plus the thread zero so it will be zero this element here on the here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8536" target="_blank">02:22:16.560</a></span> | <span class="t">it will be the block id of the y-axis multiplied by the block size plus the thread so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8544" target="_blank">02:22:24.720</a></span> | <span class="t">the element zero on the row and for the columns it will be the element one let's see another one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8551" target="_blank">02:22:31.200</a></span> | <span class="t">for example here uh for example this element here so this um how this thread will uh which element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8560" target="_blank">02:22:40.000</a></span> | <span class="t">it will work with well it will be the block size on the y-axis multiplied by the the block id on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8565" target="_blank">02:22:45.760</a></span> | <span class="t">the y-axis multiplied by the block size so it will be one multiplied by two so that will be our row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8571" target="_blank">02:22:51.840</a></span> | <span class="t">so the row number two uh which makes sense because it's the um this is the row zero this is the row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8578" target="_blank">02:22:58.160</a></span> | <span class="t">one and this is the row two and the column will be the block id on the x-axis which in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8587" target="_blank">02:23:07.040</a></span> | <span class="t">is equal to one multiplied by the block size which is equal to two so two plus one is equal to three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8593" target="_blank">02:23:13.120</a></span> | <span class="t">so this thread here will work with the element number two three and this formula now makes sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8600" target="_blank">02:23:20.400</a></span> | <span class="t">so this is how we use the block id and the thread id inside of each block to map it to which element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8606" target="_blank">02:23:26.720</a></span> | <span class="t">this particular thread should work with so as i said before cuda has no notion of knowing which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8613" target="_blank">02:23:33.200</a></span> | <span class="t">element this particular thread should work with this is up to us just based on the block id and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8618" target="_blank">02:23:38.640</a></span> | <span class="t">the thread id that cuda assigns then we make sure that the row index is less than the number of row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8625" target="_blank">02:23:45.440</a></span> | <span class="t">and the column index is less the number of columns why because as i said before when we launch um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8630" target="_blank">02:23:50.480</a></span> | <span class="t">blocks and threads cuda will round up that number to a multiple of 32 in the case of the threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8636" target="_blank">02:23:56.960</a></span> | <span class="t">so which means that some of this thread should not work with any data so we make sure that all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8641" target="_blank">02:24:01.520</a></span> | <span class="t">the threads that should not have the corresponding element to work with they should be just sit idle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8646" target="_blank">02:24:06.960</a></span> | <span class="t">inside of this if statement but the one that have it they should go enter and do some job so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8653" target="_blank">02:24:13.680</a></span> | <span class="t">calculate the index of the element of the matrix that this particular thread should work with as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8660" target="_blank">02:24:20.960</a></span> | <span class="t">follows which is the row index multiplied by the number of columns plus the column index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8666" target="_blank">02:24:26.080</a></span> | <span class="t">this is just another way of writing a or for example this is just another way of writing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8676" target="_blank">02:24:36.160</a></span> | <span class="t">a of row index call index but the way we allocate arrays in c or c++ is a flattened array where all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8689" target="_blank">02:24:49.680</a></span> | <span class="t">the rows are one after another so we need to identify the element inside of the array based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8695" target="_blank">02:24:55.360</a></span> | <span class="t">on its row index and column index and this is the formula that we use to identify it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8699" target="_blank">02:24:59.760</a></span> | <span class="t">if you have never worked with um arrays in c++ or c then it doesn't matter because later we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8708" target="_blank">02:25:08.800</a></span> | <span class="t">tensor layouts and this will be much more clear but if you have already worked with then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8713" target="_blank">02:25:13.280</a></span> | <span class="t">already know how to index an element inside of a multi-dimensional array in c++ and then we compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8720" target="_blank">02:25:20.400</a></span> | <span class="t">the output as as usual so i know that this has been a lot of information so what should we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8726" target="_blank">02:25:26.560</a></span> | <span class="t">should we remember from this the first thing that we should remember is that we decide how to divide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8731" target="_blank">02:25:31.040</a></span> | <span class="t">the work on whatever matrix we are working with or whatever thread we are working whatever vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8736" target="_blank">02:25:36.720</a></span> | <span class="t">we are working with we tell cuda how many blocks we want and we tell cuda how many threads we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8742" target="_blank">02:25:42.000</a></span> | <span class="t">in each block based on the identifier of the block id and the thread id we should come up with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8748" target="_blank">02:25:48.080</a></span> | <span class="t">strategy on how to map it to a subunit of work so which part of the matrix or which part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8753" target="_blank">02:25:53.760</a></span> | <span class="t">vector that particular thread should work with um now the next step for us is to understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8760" target="_blank">02:26:00.880</a></span> | <span class="t">tensor layouts because we are going to work with the tensors and we need to understand how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8766" target="_blank">02:26:06.240</a></span> | <span class="t">tensors are layout in the memory of the gpu or in the cpu as well actually so we need to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8773" target="_blank">02:26:13.120</a></span> | <span class="t">what is the row column row major layout and the column major layout what is the stride etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8778" target="_blank">02:26:18.800</a></span> | <span class="t">and convert all the knowledge that we have about cuda into triton so that we can then code with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8784" target="_blank">02:26:24.400</a></span> | <span class="t">triton our kernel so let's go all right guys finally it's time for us to explore tensor layouts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8792" target="_blank">02:26:32.640</a></span> | <span class="t">now why do we need to explore tensor layouts because before we we have seen some examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8798" target="_blank">02:26:38.480</a></span> | <span class="t">of cuda kernels and when you give a matrix to cuda or to a cuda kernel or a vector to cuda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8806" target="_blank">02:26:46.160</a></span> | <span class="t">kernel cuda will not give you will not give you the entire matrix like like in python where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8811" target="_blank">02:26:51.440</a></span> | <span class="t">can access each element by its index cuda will just give you a pointer a pointer to the starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8817" target="_blank">02:26:57.600</a></span> | <span class="t">element of that particular matrix or the starting element of that particular vector then it's up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8823" target="_blank">02:27:03.440</a></span> | <span class="t">you to calculate the memory address of all the remaining elements so suppose that we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8829" target="_blank">02:27:09.120</a></span> | <span class="t">simple vector in pytorch this simple vector could be the following which is a vector of shape 7</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8836" target="_blank">02:27:16.480</a></span> | <span class="t">because it's a tensor with only one dimension with shape 7 which is the number of elements in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8841" target="_blank">02:27:21.200</a></span> | <span class="t">first dimension for now ignore this property called the stride and later i will explain it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8846" target="_blank">02:27:26.880</a></span> | <span class="t">what is it how this tensor will be saved in the memory of the cpu or in the gpu it will be saved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8854" target="_blank">02:27:34.080</a></span> | <span class="t">as follows suppose that the starting address of the first element is the address 100 and suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8860" target="_blank">02:27:40.560</a></span> | <span class="t">that each element is made up of a floating point of 16 bit so it means that each element will occupy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8866" target="_blank">02:27:46.640</a></span> | <span class="t">two bytes so the start address of the second element will be the address 102 and the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8872" target="_blank">02:27:52.480</a></span> | <span class="t">element will be 104 and the fourth element will be 106 etc etc etc so this is exactly what you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8880" target="_blank">02:28:00.720</a></span> | <span class="t">when you in c you get you allocate a vector or a matrix with malloc so when you allocate in c a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8888" target="_blank">02:28:08.240</a></span> | <span class="t">vector or a memory with malloc c or the memory allocator will just allocate enough memory to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8895" target="_blank">02:28:15.520</a></span> | <span class="t">store all the elements and it will give you a pointer to the start address of this memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8900" target="_blank">02:28:20.400</a></span> | <span class="t">then it's up to you to understand where each of these elements is stored in that block of memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8905" target="_blank">02:28:25.200</a></span> | <span class="t">and this is to to do this we introduce a property called the stride the stride tells us how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8912" target="_blank">02:28:32.640</a></span> | <span class="t">elements we need to skip to arrive to the next element in the particular dimension in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8918" target="_blank">02:28:38.640</a></span> | <span class="t">for example in the case of a vector we only have one dimension which is the x dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8923" target="_blank">02:28:43.760</a></span> | <span class="t">or the columns dimension you can think of it so this is the first column this is the second the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8929" target="_blank">02:28:49.120</a></span> | <span class="t">third the fourth fifth etc etc um so in order to arrive from one element to the next we just need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8935" target="_blank">02:28:55.040</a></span> | <span class="t">to skip one element so to go from here we need to just increase our pointer by one element and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8940" target="_blank">02:29:00.320</a></span> | <span class="t">then to go here we need to increase again pointer by one element etc this allow us to do a for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8946" target="_blank">02:29:06.320</a></span> | <span class="t">on this tensor let's look at a more complicated case like the matrix so the matrix is a two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8953" target="_blank">02:29:13.040</a></span> | <span class="t">dimensional and suppose we have the following matrix which is made up of six elements with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8958" target="_blank">02:29:18.800</a></span> | <span class="t">two rows and three columns so the shape of this tensor will be two by three because if we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8964" target="_blank">02:29:24.400</a></span> | <span class="t">two rows and three columns how this matrix will be saved in the memory in the memory it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8971" target="_blank">02:29:31.840</a></span> | <span class="t">just a flattened matrix it means and this is called the row major layout but there is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8978" target="_blank">02:29:38.400</a></span> | <span class="t">another one called column major layout that we will not be discussing so how it will be stored</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8984" target="_blank">02:29:44.720</a></span> | <span class="t">in the memory is as follows it will be the first elements of the first row so the elements of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8990" target="_blank">02:29:50.000</a></span> | <span class="t">first row followed immediately by the elements of the second row so that the memory address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=8996" target="_blank">02:29:56.400</a></span> | <span class="t">imagine with this the memory address of the first element is 62 to go to the next element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9001" target="_blank">02:30:01.200</a></span> | <span class="t">we need to increase the memory address by the number of bytes that each element occupies which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9006" target="_blank">02:30:06.080</a></span> | <span class="t">is two bytes so the the address of the second element will be 64 the third element will be 66</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9012" target="_blank">02:30:12.960</a></span> | <span class="t">and the next row will start immediately after the end of the first row let's introduce this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9019" target="_blank">02:30:19.200</a></span> | <span class="t">property stride so the stride is what the stride tells us how many elements you need to skip in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9024" target="_blank">02:30:24.560</a></span> | <span class="t">each dimension to arrive to the next element of that dimension for example imagine we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9032" target="_blank">02:30:32.320</a></span> | <span class="t">address we want to get the element so all the elements of the first row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9036" target="_blank">02:30:36.640</a></span> | <span class="t">so let's call this tensor here let's call it t so t of zero and this basically this indexing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9049" target="_blank">02:30:49.040</a></span> | <span class="t">says give me all the elements of the first row so in the first row select the all only the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9054" target="_blank">02:30:54.400</a></span> | <span class="t">row and give me all the elements of that row how to how does this indexing work well by starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9061" target="_blank">02:31:01.040</a></span> | <span class="t">from the pointer to the first element it will select only the first row and then it will move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9068" target="_blank">02:31:08.560</a></span> | <span class="t">the index here one element after another so it will select the first one the second one the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9075" target="_blank">02:31:15.760</a></span> | <span class="t">one how does it know that it needs to move one element by one element because in this dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9080" target="_blank">02:31:20.560</a></span> | <span class="t">the stride is one so the stride tells us how many elements you need to skip to arrive to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9085" target="_blank">02:31:25.920</a></span> | <span class="t">element in that dimension imagine now that we want to get the t of let's say zero and one well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9096" target="_blank">02:31:36.240</a></span> | <span class="t">in this case let's say t of one actually and all the elements of the first row it will first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9104" target="_blank">02:31:44.560</a></span> | <span class="t">it needs to skip some elements from the first dimension it needs to skip the element zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9109" target="_blank">02:31:49.680</a></span> | <span class="t">because we don't we are not selecting it we only want to select the element one of the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9114" target="_blank">02:31:54.080</a></span> | <span class="t">dimension which basically means the row with index one so because it will start from the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9120" target="_blank">02:32:00.640</a></span> | <span class="t">pointer to the first element it will it needs to know how many elements to skip and how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9126" target="_blank">02:32:06.160</a></span> | <span class="t">element to skip is given by the stride so the stride tells us how many elements you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9130" target="_blank">02:32:10.480</a></span> | <span class="t">skip to arrive to the next element of the first dimension so in this case it will take the pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9135" target="_blank">02:32:15.520</a></span> | <span class="t">to the first element skip three elements and it will be starting with the second row and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9140" target="_blank">02:32:20.640</a></span> | <span class="t">inside this row it will go through the second in the the index of the second dimension in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9146" target="_blank">02:32:26.160</a></span> | <span class="t">the stride is one so it will just go one after another and it will return only this part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9151" target="_blank">02:32:31.040</a></span> | <span class="t">memory so to rehearse the stride is just a a number that tells us how many elements you need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9160" target="_blank">02:32:40.880</a></span> | <span class="t">in each dimension to arrive to the next index in that dimension so it means that to go from one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9166" target="_blank">02:32:46.880</a></span> | <span class="t">row to the other we need to skip three elements to go from one column to the other we need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9171" target="_blank">02:32:51.200</a></span> | <span class="t">one element why is the stride useful well the stride is useful because it allows us to reshape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9179" target="_blank">02:32:59.360</a></span> | <span class="t">tensors very easily and without doing any computation let's see okay imagine we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9186" target="_blank">02:33:06.080</a></span> | <span class="t">reshape a matrix imagine initially the shape of this matrix is a two by three so we have a two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9191" target="_blank">02:33:11.680</a></span> | <span class="t">row by three columns and we have a stride calculated as follows means that to go from one row to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9196" target="_blank">02:33:16.800</a></span> | <span class="t">other you need to skip three elements and to go from one column one row to the other you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9200" target="_blank">02:33:20.960</a></span> | <span class="t">skip three elements and to go from one column to the next you need to skip one element so you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9207" target="_blank">02:33:27.040</a></span> | <span class="t">to jump by one element if we want to reshape it into this shape so three by two basically we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9213" target="_blank">02:33:33.600</a></span> | <span class="t">to have three rows and two columns we can reshape it without actually changing its memory layout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9225" target="_blank">02:33:45.760</a></span> | <span class="t">just by changing the stride because look at this physical configuration of the tensor and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9233" target="_blank">02:33:53.280</a></span> | <span class="t">access this same tensor as this shape or as this shape exactly by using the same physical view</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9240" target="_blank">02:34:00.160</a></span> | <span class="t">because to go from one row to the next here the stride is a three so we need to skip three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9246" target="_blank">02:34:06.400</a></span> | <span class="t">elements it means that the starting address the starting element of the second row is given by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9252" target="_blank">02:34:12.720</a></span> | <span class="t">the start pointer plus three elements so exactly here the second row will start and each element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9259" target="_blank">02:34:19.760</a></span> | <span class="t">of the second row is one after another because the stride of the second dimension is one so you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9266" target="_blank">02:34:26.000</a></span> | <span class="t">see that to get the second row we can just start from here and then go one after another and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9272" target="_blank">02:34:32.080</a></span> | <span class="t">all these elements which is exactly the second row suppose we want to obtain the second row of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9277" target="_blank">02:34:37.120</a></span> | <span class="t">this view here of this shape of this reshaped matrix how to do that let's look at the stride</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9283" target="_blank">02:34:43.360</a></span> | <span class="t">the stride now is a two in the row it means that to go from one row to the next we need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9288" target="_blank">02:34:48.640</a></span> | <span class="t">two elements so if we want to select this row here we go from the starting point of the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9295" target="_blank">02:34:55.360</a></span> | <span class="t">so this start pointer we skip the first two elements because the stride says that to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9301" target="_blank">02:35:01.920</a></span> | <span class="t">from one row to the next you need to skip two elements so we arrive here and then we select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9306" target="_blank">02:35:06.480</a></span> | <span class="t">exactly two elements which are one after another because the stride in the second dimension is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9311" target="_blank">02:35:11.360</a></span> | <span class="t">so the stride allow us to reshape the tensor without changing the physical layout on how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9321" target="_blank">02:35:21.440</a></span> | <span class="t">is stored in the memory moreover the stride also allow us to get the transpose of a matrix without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9329" target="_blank">02:35:29.280</a></span> | <span class="t">changing the shape of how it is stored in the memory so without changing the arrangement of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9333" target="_blank">02:35:33.600</a></span> | <span class="t">the elements in the memory and this is very cool because we can view the same matrix as without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9339" target="_blank">02:35:39.760</a></span> | <span class="t">the transpose and also the transpose version of the matrix without changing anything in the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9344" target="_blank">02:35:44.400</a></span> | <span class="t">so it comes for free just by working with the index and the stride so to transpose the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9350" target="_blank">02:35:50.320</a></span> | <span class="t">along two dimensions we just need to swap the stride along these two dimensions that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9354" target="_blank">02:35:54.480</a></span> | <span class="t">to transpose so in this case for example imagine we want to get the transpose of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9359" target="_blank">02:35:59.440</a></span> | <span class="t">we just need to swap the strides so if we want to get the second row of the transpose matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9364" target="_blank">02:36:04.880</a></span> | <span class="t">how to get that well you we always have the pointer to the first element where the tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9371" target="_blank">02:36:11.280</a></span> | <span class="t">is stored so at the beginning of where the tensor is stored in the memory and it says that in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9378" target="_blank">02:36:18.240</a></span> | <span class="t">to go to from one row to the next we need to skip one element which is correct because as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9385" target="_blank">02:36:25.680</a></span> | <span class="t">see the second element is exactly the second element also in the memory so we just skip by one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9391" target="_blank">02:36:31.280</a></span> | <span class="t">and we get the starting point of the second row and then to go from one element to the next in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9398" target="_blank">02:36:38.080</a></span> | <span class="t">within the same row we need to skip three elements so the second element of the second row will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9404" target="_blank">02:36:44.480</a></span> | <span class="t">after three elements after the first element of the second row so after two we need to skip three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9410" target="_blank">02:36:50.880</a></span> | <span class="t">elements so we skip this one we skip this one and we arrive to this one eight which is exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9415" target="_blank">02:36:55.600</a></span> | <span class="t">second column of the second of the second row so basically the the stride as you can see allow us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9422" target="_blank">02:37:02.160</a></span> | <span class="t">to do two things one is it allow us to reshape the tensor without having to reallocate it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9428" target="_blank">02:37:08.880</a></span> | <span class="t">another configuration in the memory secondly it allow us to transpose a matrix without having to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9434" target="_blank">02:37:14.400</a></span> | <span class="t">rearrange the elements in the memory which is great because moving memory around is expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9439" target="_blank">02:37:19.040</a></span> | <span class="t">and rearranging the memory is expensive so that it's great that this this stuff comes for free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9445" target="_blank">02:37:25.360</a></span> | <span class="t">basically another thing okay for example if you try to you know that in pytorch there are two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9453" target="_blank">02:37:33.760</a></span> | <span class="t">methods to reshape a tensor one is called the reshape method and one is called the view method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9459" target="_blank">02:37:39.280</a></span> | <span class="t">the after transposing a matrix by swiping the by swiping the stride of the two dimensions that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9467" target="_blank">02:37:47.760</a></span> | <span class="t">want to transpose you cannot reshape for free the tensor anymore because um the tensor basically what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9476" target="_blank">02:37:56.240</a></span> | <span class="t">is the stride the stride how it is computed the stride is just the uh let me show you with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9482" target="_blank">02:38:02.880</a></span> | <span class="t">concrete example the stride is just the product of all the shape uh after um in the future dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9491" target="_blank">02:38:11.840</a></span> | <span class="t">so the stride of the zeroth dimension is just the product of the elements in the shape of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9498" target="_blank">02:38:18.400</a></span> | <span class="t">the future dimension so the stride of zero is just the product of all the shape starting from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9503" target="_blank">02:38:23.520</a></span> | <span class="t">the index number one uh it's not easy to see with the 2d matrix because we don't have enough elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9508" target="_blank">02:38:28.880</a></span> | <span class="t">so let's do it with a 3d matrix so this is a tensor with the three dimensions so it is a shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9515" target="_blank">02:38:35.360</a></span> | <span class="t">of two four three which means that we have two matrices each matrix is made up of four rows and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9520" target="_blank">02:38:40.960</a></span> | <span class="t">each made and three columns the stride is calculated as follows so the zeroth dimension stride is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9529" target="_blank">02:38:49.280</a></span> | <span class="t">the product of four by three and this three here comes the with the product of just a three with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9535" target="_blank">02:38:55.360</a></span> | <span class="t">its with one because we don't have any future dimension of the three so when we transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9540" target="_blank">02:39:00.960</a></span> | <span class="t">this stride property is lost and we cannot um after transposing this matrix by swapping the strides we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9549" target="_blank">02:39:09.840</a></span> | <span class="t">cannot do further reshaping operations so basically the the tensor is not log contiguous so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9556" target="_blank">02:39:16.560</a></span> | <span class="t">this is a very advanced okay property if you it doesn't matter if you know it or not but if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9562" target="_blank">02:39:22.080</a></span> | <span class="t">are curious basically in pytorch you cannot um view a tensor after it has been transposed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9569" target="_blank">02:39:29.600</a></span> | <span class="t">because the pytorch to transpose a tensor will just swap the two strides but it loses the stride</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9575" target="_blank">02:39:35.440</a></span> | <span class="t">property which is basically the stride will not be anymore the product of the future shapes so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9582" target="_blank">02:39:42.560</a></span> | <span class="t">is not anymore two this should be two for example and this should be one but after transposing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9590" target="_blank">02:39:50.160</a></span> | <span class="t">property is lost so you need to actually reallocate the tensor if you want to reshape it after it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9595" target="_blank">02:39:55.360</a></span> | <span class="t">been transposed it doesn't matter if you remember this it's just a curiosity anyway so what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9601" target="_blank">02:40:01.200</a></span> | <span class="t">transposed what is the stride used for is the stride for the stride is used for two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9605" target="_blank">02:40:05.680</a></span> | <span class="t">first of all it is used to understand how to index this tensor so just by having a pointer to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9612" target="_blank">02:40:12.400</a></span> | <span class="t">first to the starting address of this tensor we can index this tensor however we like so we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9619" target="_blank">02:40:19.440</a></span> | <span class="t">access any row any column moreover it allow us to reshape this tensor for free so without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9626" target="_blank">02:40:26.880</a></span> | <span class="t">rearranging the elements inside the memory and third it allow us to transpose the tensor however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9633" target="_blank">02:40:33.280</a></span> | <span class="t">we like just by swapping the strides of two uh the two dimensions that we want to transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9638" target="_blank">02:40:38.400</a></span> | <span class="t">now that we have seen also how the tensor is stored in the memory we can finally go to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9644" target="_blank">02:40:44.880</a></span> | <span class="t">triton um and see some examples all right guys now that we have seen how uh tensors work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9653" target="_blank">02:40:53.440</a></span> | <span class="t">tensor layout works how CUDA works now we can see some examples of triton kernels to see how triton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9659" target="_blank">02:40:59.040</a></span> | <span class="t">differs from CUDA now if you go on the triton website you will find some tutorials like in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9667" target="_blank">02:41:07.600</a></span> | <span class="t">this section here and let's do let's work one tutorial together to understand how triton is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9673" target="_blank">02:41:13.280</a></span> | <span class="t">different from CUDA so if you go to the tutorial there are many examples so first of all the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9679" target="_blank">02:41:19.200</a></span> | <span class="t">that i will be coding for flash attention is based on this tutorial here fused attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9683" target="_blank">02:41:23.040</a></span> | <span class="t">that you can see here but with some modifications because i simplified the code a lot i removed for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9688" target="_blank">02:41:28.160</a></span> | <span class="t">example the fp8 implementation i also for example um this code here on the fused attention only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9694" target="_blank">02:41:34.800</a></span> | <span class="t">works in the backward pass only for the causal attention while my code will work for the causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9699" target="_blank">02:41:39.360</a></span> | <span class="t">and non-causal attention uh the second another modification i did is instead of using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9704" target="_blank">02:41:44.720</a></span> | <span class="t">exponential tool that they use here to make things faster drawing because the exponential tool is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9709" target="_blank">02:41:49.360</a></span> | <span class="t">implemented with a faster unit i i use the the original implementation of flash attention which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9716" target="_blank">02:41:56.800</a></span> | <span class="t">use the exponential with the base e etc so i simplified my code as much as possible to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9722" target="_blank">02:42:02.800</a></span> | <span class="t">it simple to follow instead of making it optimized so for sure my code will be slower than the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9728" target="_blank">02:42:08.240</a></span> | <span class="t">fused attention that you see here but mine should be more comprehensible more easy to follow anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9734" target="_blank">02:42:14.880</a></span> | <span class="t">let's go to the vector addition tutorial and if you go to the vector addition tutorial there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9740" target="_blank">02:42:20.080</a></span> | <span class="t">some examples on how to do a vector addition with triton this should allow you to get into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9745" target="_blank">02:42:25.280</a></span> | <span class="t">mindset of how to write kernels with triton instead of writing first the kernel and then calling it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9751" target="_blank">02:42:31.920</a></span> | <span class="t">let's do the opposite so let's see how to call this kernel and let's explore how it works so i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9757" target="_blank">02:42:37.360</a></span> | <span class="t">have already copied the tutorial vector addition from the website so let's look at first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9762" target="_blank">02:42:42.960</a></span> | <span class="t">what we want to achieve we have an input vector x and an input vector y and we want to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9769" target="_blank">02:42:49.920</a></span> | <span class="t">the vector addition which means that with the torch we want to do the following operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9774" target="_blank">02:42:54.160</a></span> | <span class="t">and also we want to do the same operation also with the triton by calling this method add and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9779" target="_blank">02:42:59.200</a></span> | <span class="t">then we want to compare the two vectors output and they should be equal or at least the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9784" target="_blank">02:43:04.480</a></span> | <span class="t">should be very very small because of course there is always some rounding error in case you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9788" target="_blank">02:43:08.160</a></span> | <span class="t">working with floating point numbers the size of this vector is 98 000 elements and we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9795" target="_blank">02:43:15.680</a></span> | <span class="t">work in a blocked way so as you remember before with the cuda you can do vector addition by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9801" target="_blank">02:43:21.920</a></span> | <span class="t">spawning a lot of number of threads each doing one operation but when the number of threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9807" target="_blank">02:43:27.360</a></span> | <span class="t">that you have is not enough then you need to divide the input vector into blocks and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9812" target="_blank">02:43:32.000</a></span> | <span class="t">is what we are going to do here so let's look at this add method so this add method basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9817" target="_blank">02:43:37.600</a></span> | <span class="t">will first of all allocate the necessary memory for the output vector then it will compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9825" target="_blank">02:43:45.360</a></span> | <span class="t">launch grid the launch grid tells triton just like in cuda how many kernels we want to how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9832" target="_blank">02:43:52.240</a></span> | <span class="t">many blocks we want to launch how many blocks of threads we want to launch if you remember in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9838" target="_blank">02:43:58.960</a></span> | <span class="t">cuda kernel we specify how many blocks we want and then how many threads we want for each block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9845" target="_blank">02:44:05.360</a></span> | <span class="t">in the case of triton we tell how many blocks we want and then we don't force how many threads to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9855" target="_blank">02:44:15.360</a></span> | <span class="t">launch it will be triton that will choose how many threads to launch we just tell what each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9862" target="_blank">02:44:22.320</a></span> | <span class="t">group of threads should do so in this case for example we divide our number of elements so n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9869" target="_blank">02:44:29.600</a></span> | <span class="t">so which is 98 000 into blocks of size block size which is initialized as 1024 this is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9879" target="_blank">02:44:39.280</a></span> | <span class="t">saying take them to calculate the grid size you do the ceiling division so basically this means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9885" target="_blank">02:44:45.840</a></span> | <span class="t">ceiling of seal of n elements divided by block size this is the meaning of this one so how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9896" target="_blank">02:44:56.160</a></span> | <span class="t">blocks we want now what each block should do is inside of the kernel so let's go to the kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9903" target="_blank">02:45:03.280</a></span> | <span class="t">and when we launch the the kernel we we can specify the launch grid in this square parentheses and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9909" target="_blank">02:45:09.120</a></span> | <span class="t">then in the round parentheses we specify the arguments of this kernel so let's go to the kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9914" target="_blank">02:45:14.960</a></span> | <span class="t">we see that python triton will not give us access to the tensor x it will give us a pointer to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9925" target="_blank">02:45:25.040</a></span> | <span class="t">first element of this tensor and this takes us back to the tensor layouts so the reason we studied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9930" target="_blank">02:45:30.560</a></span> | <span class="t">the tensor layouts and the strides and all the stuff is because triton this code this add kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9937" target="_blank">02:45:37.840</a></span> | <span class="t">will run on the gpu and the gpu cannot um does not index tensors like pytorch by using all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9946" target="_blank">02:45:46.400</a></span> | <span class="t">dimension and with the broadcasting and all this fancy stuff the gpu will just give you the pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9952" target="_blank">02:45:52.800</a></span> | <span class="t">to the first element of this tensor in the memory and then it's up to you to compute all the indexes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9958" target="_blank">02:45:58.960</a></span> | <span class="t">of all the elements that you want to access so this x ptr is the pointer to the first element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9965" target="_blank">02:46:05.280</a></span> | <span class="t">of the x vector this y pointer is the first the pointer to the first element of the y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9970" target="_blank">02:46:10.960</a></span> | <span class="t">vector then we have the pointer to the output vector where we want to store the result of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9977" target="_blank">02:46:17.120</a></span> | <span class="t">matrix addition we specify how many elements our vectors have and what is the block size so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9983" target="_blank">02:46:23.120</a></span> | <span class="t">how many items each block should process which may not correspond to how many threads each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9990" target="_blank">02:46:30.480</a></span> | <span class="t">each kernel will have you may be confused because okay in triton in coda we specified how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=9999" target="_blank">02:46:39.360</a></span> | <span class="t">threads each block should have so the granularity that we manage is the thread level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10006" target="_blank">02:46:46.480</a></span> | <span class="t">here we are saying it's a group of thread that should work with this quantity of data then it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10012" target="_blank">02:46:52.480</a></span> | <span class="t">up to triton to optimize the number of threads that it will actually use actually there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10017" target="_blank">02:46:57.600</a></span> | <span class="t">tricks there are ways to say how many threads we actually want by specifying the number of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10021" target="_blank">02:47:01.840</a></span> | <span class="t">but we will see that later for now just remember that this thread this kernel here will process a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10028" target="_blank">02:47:08.720</a></span> | <span class="t">number of elements in the input vectors how many number how many elements block size number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10034" target="_blank">02:47:14.800</a></span> | <span class="t">elements first of all we need to identify which block we are we are in coda we use the the variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10044" target="_blank">02:47:24.320</a></span> | <span class="t">called the block id.x to identify the identifier of the block which tells us which group of elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10050" target="_blank">02:47:30.320</a></span> | <span class="t">we should be working with in triton you do the same by using program id and in coda the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10058" target="_blank">02:47:38.640</a></span> | <span class="t">id can be along the x y and z axis in triton these are called the dimension 0 1 and 2 here we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10067" target="_blank">02:47:47.200</a></span> | <span class="t">one dimensional data so we only use one axis to specify the block index so we get the block index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10074" target="_blank">02:47:54.160</a></span> | <span class="t">which is the p id in this day in triton this is called the program id it's more intuitive to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10080" target="_blank">02:48:00.800</a></span> | <span class="t">of it as the program like this is a kind of a program that is running in parallel with other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10085" target="_blank">02:48:05.360</a></span> | <span class="t">programs that will have different program id and based on the program id we can understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10091" target="_blank">02:48:11.120</a></span> | <span class="t">what is the starting element this program should work with so this blue block of threads should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10096" target="_blank">02:48:16.240</a></span> | <span class="t">work with them and together that is just the p id multiplied by the block size so the p id 0 should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10102" target="_blank">02:48:22.080</a></span> | <span class="t">be working with the element that starts from the element 0 the p id 1 should start with the element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10107" target="_blank">02:48:27.680</a></span> | <span class="t">1024 and the p id 2 should start from the element 2048 so it should skip the first 2048 elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10114" target="_blank">02:48:34.960</a></span> | <span class="t">and start with the element with index 2048 next we define how to load these elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10122" target="_blank">02:48:42.400</a></span> | <span class="t">based on the pointer in which of the x and the y vector to do that we specify a list of offsets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10133" target="_blank">02:48:53.280</a></span> | <span class="t">with respect to the starting address that we want to load so because each program in triton works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10139" target="_blank">02:48:59.680</a></span> | <span class="t">with a group of data so not one single element but a block of elements we mean we need to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10148" target="_blank">02:49:08.480</a></span> | <span class="t">which elements to load so the offset of these elements in the case of the program id 0 it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10153" target="_blank">02:49:13.920</a></span> | <span class="t">load the block start so 0 plus the elements from index 0 to 1024 excluded with the program element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10167" target="_blank">02:49:27.920</a></span> | <span class="t">1 this basically will result in a vector that is well the program start with p id equal to 1 will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10175" target="_blank">02:49:35.600</a></span> | <span class="t">be 1024 then 1025 1026 1027 etc etc until 2047 with the program number let's say 2 this this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10190" target="_blank">02:49:50.960</a></span> | <span class="t">offset will be the elements 2048 2049 blah blah blah until 3000 and something now we also as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10203" target="_blank">02:50:03.520</a></span> | <span class="t">remember when we create when we launch a grid the number of threads is not always based on the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10213" target="_blank">02:50:13.520</a></span> | <span class="t">of elements in the block or the number of elements in your vector it is always a multiple of a base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10219" target="_blank">02:50:19.120</a></span> | <span class="t">number which is usually 32 which means that the grid this program may have more threads that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10226" target="_blank">02:50:26.320</a></span> | <span class="t">needs so some threads should not be doing anything so should not be loading any data and should not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10231" target="_blank">02:50:31.920</a></span> | <span class="t">be computing any summation so what we this is what we why we need this mask this means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10237" target="_blank">02:50:37.600</a></span> | <span class="t">if all these offsets that we are loading it should be at most up to n elements because imagine you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10245" target="_blank">02:50:45.440</a></span> | <span class="t">have not 1000 2000 imagine you have a vector of 2060 elements which means that this offset for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10256" target="_blank">02:50:56.240</a></span> | <span class="t">the the third program of this kernel will load the offset that go from 2048 2049 blah blah 2060 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10267" target="_blank">02:51:07.360</a></span> | <span class="t">then also 2061 2062 etc etc but we said that we only have a 2060 elements so all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10275" target="_blank">02:51:15.280</a></span> | <span class="t">elements of 2061 62 etc until 3000 and something they don't exist so we need to tell somehow that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10283" target="_blank">02:51:23.360</a></span> | <span class="t">all the threads that are working with these elements should not load anything that's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10287" target="_blank">02:51:27.920</a></span> | <span class="t">we need this mask this mask tells load among all the offsets that this block should work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10294" target="_blank">02:51:34.720</a></span> | <span class="t">only those elements that actually exist for which this mask is true then we load the elements of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10302" target="_blank">02:51:42.800</a></span> | <span class="t">this current program which is a group of elements defined by these offsets and only the one that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10310" target="_blank">02:51:50.960</a></span> | <span class="t">for which this mask is true so only the one that actually exists all the others should be ignored</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10315" target="_blank">02:51:55.680</a></span> | <span class="t">and we can also specify what it should load in case this the mask is false with another parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10324" target="_blank">02:52:04.160</a></span> | <span class="t">but we will not see that here we also load the group of elements of the y vector and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10329" target="_blank">02:52:09.680</a></span> | <span class="t">compute the output x plus y so if you remember previously in CUDA we we did something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10336" target="_blank">02:52:16.800</a></span> | <span class="t">like the output of i is equal to the x of i plus the y of i so we did it one element at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10346" target="_blank">02:52:26.080</a></span> | <span class="t">because each thread was working with one index here we are working with a group of elements so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10351" target="_blank">02:52:31.040</a></span> | <span class="t">this x is a group of elements is a block of elements at most of size block size actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10358" target="_blank">02:52:38.720</a></span> | <span class="t">of size block size and it's this y is a group of elements from the y vector and we are computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10366" target="_blank">02:52:46.720</a></span> | <span class="t">the output group by group so this this is summing a group of elements of x with the corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10374" target="_blank">02:52:54.800</a></span> | <span class="t">group in y and writing it in output then we need to restore this output we need to store it in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10382" target="_blank">02:53:02.160</a></span> | <span class="t">output tensor output ptr that you can see here which is a pointer to the first element of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10387" target="_blank">02:53:07.440</a></span> | <span class="t">output vector and we say that where should we store this output vector which is of size shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10394" target="_blank">02:53:14.240</a></span> | <span class="t">of this vector here is block size where should we save it well in the same offset to where which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10401" target="_blank">02:53:21.840</a></span> | <span class="t">loaded x so if this program worked with the index 2048 2049 etc etc then all this output should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10410" target="_blank">02:53:30.720</a></span> | <span class="t">written in the same offset 2048 2049 etc up to 3000 and something using the mask as well because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10419" target="_blank">02:53:39.360</a></span> | <span class="t">we don't want to write all the values of this block size because maybe we don't have enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10423" target="_blank">02:53:43.600</a></span> | <span class="t">elements so only write the one that are actually present in the vector so the reason we need the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10429" target="_blank">02:53:49.520</a></span> | <span class="t">mask is because CUDA will launch a number of thread that is always a multiple of a base unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10433" target="_blank">02:53:53.920</a></span> | <span class="t">that may not be a multiple of the vector size that we are working with so we need to find a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10440" target="_blank">02:54:00.720</a></span> | <span class="t">tell some threads to not do anything for those that the data is not available so let's rehearse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10447" target="_blank">02:54:07.360</a></span> | <span class="t">what you have seen so far in CUDA the program that we write is at the thread level so each thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10452" target="_blank">02:54:12.400</a></span> | <span class="t">what it should do in triton it's this block of data we work with a block of threads what data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10461" target="_blank">02:54:21.360</a></span> | <span class="t">this block of thread should work with all right guys the final finally the moment has come so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10469" target="_blank">02:54:29.920</a></span> | <span class="t">we are going to code the flash attention for our pass right now in triton but let's rehearse the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10476" target="_blank">02:54:36.480</a></span> | <span class="t">algorithm so the goal of the attention mechanism in specifically in triton in flash attention is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10483" target="_blank">02:54:43.200</a></span> | <span class="t">to compute the attention output which is we want to compute the output of the following formula so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10488" target="_blank">02:54:48.240</a></span> | <span class="t">the query multiplied by the transpose of the key divided by the square root of the head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10492" target="_blank">02:54:52.000</a></span> | <span class="t">all multiply we apply the softmax and then all multiply by b now we in this video we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10501" target="_blank">02:55:01.680</a></span> | <span class="t">coding the forward pass and also the backward pass but before coding the backward pass we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10507" target="_blank">02:55:07.200</a></span> | <span class="t">to understand how the autograd works we need to understand what is the gradient what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10512" target="_blank">02:55:12.080</a></span> | <span class="t">jacobian how to derive the gradient of the softmax operation how to derive the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10516" target="_blank">02:55:16.960</a></span> | <span class="t">of the matrix multiplication operation etc etc so that is going to be another part of the video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10521" target="_blank">02:55:21.840</a></span> | <span class="t">for now let's concentrate on the forward pass right now we have some tools so we know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10526" target="_blank">02:55:26.880</a></span> | <span class="t">we have this thing called the gpu that can parallelize operation among multiple cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10531" target="_blank">02:55:31.200</a></span> | <span class="t">we know that in cuda we can parallelize operations by telling by writing a program that is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10536" target="_blank">02:55:36.960</a></span> | <span class="t">definition of what each thread should do or we can follow the triton programming mode which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10543" target="_blank">02:55:43.440</a></span> | <span class="t">telling in python what each group of threads should do the mapping between the what each thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10551" target="_blank">02:55:51.600</a></span> | <span class="t">should do and the which element that should try to work with is up to us to the programmers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10558" target="_blank">02:55:58.080</a></span> | <span class="t">the same happens in triton we tell we how many blocks of threads we want how much data each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10565" target="_blank">02:56:05.840</a></span> | <span class="t">thread should block of thread should process so that's the block size that we saw in the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10571" target="_blank">02:56:11.440</a></span> | <span class="t">addition but then the mapping between the elements of the vector and the the identity of each group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10580" target="_blank">02:56:20.160</a></span> | <span class="t">of threads so the program id that we saw is up to us and the same will happen when we record</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10586" target="_blank">02:56:26.080</a></span> | <span class="t">flash attention let's see what can we parallelize in this flash attention so first of all this code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10593" target="_blank">02:56:33.360</a></span> | <span class="t">that you see in the forward pass of the flash attention is takes as input query key and value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10600" target="_blank">02:56:40.160</a></span> | <span class="t">that is a vector that is a matrices of n by d however usually in a transformer network we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10608" target="_blank">02:56:48.640</a></span> | <span class="t">have only one sequence made up of d dimensions we have many sequences made up of d dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10615" target="_blank">02:56:55.520</a></span> | <span class="t">and this d is the lowercase d which is the the number of dimensions dedicated for each head but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10622" target="_blank">02:57:02.720</a></span> | <span class="t">we don't have only one head we have multiple head so the algorithm that you see here is what each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10629" target="_blank">02:57:09.360</a></span> | <span class="t">head should work so each head of each batch should do moreover we have seen before when talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10639" target="_blank">02:57:19.680</a></span> | <span class="t">block matrix multiplication that we can parallelize the computation of the output because this output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10645" target="_blank">02:57:25.600</a></span> | <span class="t">block here depends on the query one and all the keys this one here depends on the query group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10651" target="_blank">02:57:31.040</a></span> | <span class="t">block of query two with all the keys and this one here is the query tree with all the keys etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10657" target="_blank">02:57:37.440</a></span> | <span class="t">so because this one only depends on query the group the block query one and this one only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10662" target="_blank">02:57:42.960</a></span> | <span class="t">depends on the block query two they can work independently from each other by sharing of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10667" target="_blank">02:57:47.920</a></span> | <span class="t">course work the keys another thing that we need to understand about triton is the shared memory so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10675" target="_blank">02:57:55.920</a></span> | <span class="t">um the in the gpu we have the high bandwidth memory and which is the kind of the ram so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10684" target="_blank">02:58:04.720</a></span> | <span class="t">when you buy an a100 they tell you that it has a 40 gigabyte that's the amount of memory in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10690" target="_blank">02:58:10.640</a></span> | <span class="t">high bandwidth memory so the dram so let's look at actually the structure of the gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10695" target="_blank">02:58:15.440</a></span> | <span class="t">which is here we have this dram which is the big memory that we that the gpu has and then each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10704" target="_blank">02:58:24.640</a></span> | <span class="t">streaming multiprocessor so it's a let's call it a block of threads actually also have a shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10712" target="_blank">02:58:32.160</a></span> | <span class="t">memory so inside of the gpu actually we have we have these streaming multiprocessors and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10717" target="_blank">02:58:37.280</a></span> | <span class="t">these streaming multiprocessors have a part of memory called the shared memory which is much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10722" target="_blank">02:58:42.160</a></span> | <span class="t">smaller than the dram like much much much smaller what changes between these two memories the access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10729" target="_blank">02:58:49.360</a></span> | <span class="t">to the dram is very slow and the access to the shared memory is very very very fast so one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10736" target="_blank">02:58:56.080</a></span> | <span class="t">that is different between cuda and triton is that whenever you load some information in cuda you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10742" target="_blank">02:59:02.080</a></span> | <span class="t">loading that information directly from the global memory because when we launch a cuda kernel first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10747" target="_blank">02:59:07.680</a></span> | <span class="t">of all as you remember in my c++ code we first copy the tensors from or the vectors from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10754" target="_blank">02:59:14.480</a></span> | <span class="t">cpu to the gpu and they reside in the global memory of the gpu then we load these elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10761" target="_blank">02:59:21.600</a></span> | <span class="t">directly from the global memory but the access to the global memory usually it's much much much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10767" target="_blank">02:59:27.120</a></span> | <span class="t">slower so what happens with the flash attention is that the flash attention computation in its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10772" target="_blank">02:59:32.160</a></span> | <span class="t">the attention computation in its naive version the one that we can do with the torch is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10777" target="_blank">02:59:37.280</a></span> | <span class="t">slow because the access to the global memory is very slow so we want to use as much as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10783" target="_blank">02:59:43.120</a></span> | <span class="t">the shared memory so we want to reuse the elements loaded from the global memory into the shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10788" target="_blank">02:59:48.480</a></span> | <span class="t">memory so that we don't need to access the global memory every time to load elements from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10792" target="_blank">02:59:52.640</a></span> | <span class="t">vectors or the matrices and this is what happens also in triton so in triton whenever you load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10798" target="_blank">02:59:58.800</a></span> | <span class="t">some data you are copying the information from the global memory to the shared memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10803" target="_blank">03:00:03.120</a></span> | <span class="t">then whatever operations that you are doing is done on the shared memory and then when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10808" target="_blank">03:00:08.240</a></span> | <span class="t">you store the information you are copying the data from the shared memory to the global memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10812" target="_blank">03:00:12.400</a></span> | <span class="t">this makes it much faster so we always work with the elements that have been loaded in the shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10818" target="_blank">03:00:18.640</a></span> | <span class="t">memory and this shared memory basically it's shared for all the threads that belong to the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10825" target="_blank">03:00:25.440</a></span> | <span class="t">block in triton we have an abstraction level that doesn't make us work directly with the threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10832" target="_blank">03:00:32.160</a></span> | <span class="t">so we always work with a group of threads that belong to the same block that share this shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10836" target="_blank">03:00:36.560</a></span> | <span class="t">memory so in triton we are copying information from the global memory to the shared memory we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10841" target="_blank">03:00:41.280</a></span> | <span class="t">do some operation with it and then we store back to the global memory and this is what we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10845" target="_blank">03:00:45.360</a></span> | <span class="t">to do with flash attention now let's review the algorithm of flash attention so in flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10850" target="_blank">03:00:50.880</a></span> | <span class="t">we have to go an outer for loop that is among all the between all the keys and then an inner loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10857" target="_blank">03:00:57.760</a></span> | <span class="t">that is sorry between all the query blocks and then an inner loop that is through all the key block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10864" target="_blank">03:01:04.080</a></span> | <span class="t">in the original flash attention algorithm the flash attention one the outer block was on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10870" target="_blank">03:01:10.880</a></span> | <span class="t">keys and inner block was on the queries this made it less parallelizable why because the outer loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10877" target="_blank">03:01:17.840</a></span> | <span class="t">is on the queries and we have seen before that the the output of this attention can be computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10884" target="_blank">03:01:24.720</a></span> | <span class="t">independently for each block of queries so it's much easier to parallelize so this outer for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10890" target="_blank">03:01:30.240</a></span> | <span class="t">actually we don't have to run a for loop we just spawn many kernels each working with one iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10895" target="_blank">03:01:35.760</a></span> | <span class="t">of this outer for loop so each working with a different query block of this outer for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10901" target="_blank">03:01:41.120</a></span> | <span class="t">and the inner for loop is something that we have to iterate through so each triton kernel will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10907" target="_blank">03:01:47.920</a></span> | <span class="t">work with one query block and then iterate through all the key blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10912" target="_blank">03:01:52.720</a></span> | <span class="t">and inside of this key block we have already seen the operations that we are going to do which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10919" target="_blank">03:01:59.200</a></span> | <span class="t">the we explored before and at the end of this for loop we need to store back the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10925" target="_blank">03:02:05.840</a></span> | <span class="t">in the high bandwidth memory and this is how it's gonna we are going to work another thing that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10933" target="_blank">03:02:13.760</a></span> | <span class="t">should notice is that this query key value are n by d so as i said before but usually in in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10941" target="_blank">03:02:21.600</a></span> | <span class="t">transformer model we don't have only one sequence we have many sequences so we can also parallelize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10948" target="_blank">03:02:28.800</a></span> | <span class="t">on the number of sequences that we have in the batch because each batch can work independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10953" target="_blank">03:02:33.120</a></span> | <span class="t">from each other and inside each and each head each sequence has multiple heads so each head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10960" target="_blank">03:02:40.720</a></span> | <span class="t">also can work independently from each other because that we know from the attention is all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10964" target="_blank">03:02:44.320</a></span> | <span class="t">unit paper that's what's the meaning of head that's what's the meaning of multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10968" target="_blank">03:02:48.640</a></span> | <span class="t">so that each head can compute the attention independently from each other so we will also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10972" target="_blank">03:02:52.560</a></span> | <span class="t">parallelize along the head dimension and moreover if you look at this definition of the query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10979" target="_blank">03:02:59.520</a></span> | <span class="t">we can also split the query into blocks and each query block can work independently from the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10984" target="_blank">03:03:04.320</a></span> | <span class="t">query blocks by in producing one output block this is how we are going to parallelize so we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10990" target="_blank">03:03:10.240</a></span> | <span class="t">to parallelize each sequence in the batch but inside of each sequence we are going to parallelize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=10995" target="_blank">03:03:15.600</a></span> | <span class="t">each head and inside of each head we are going to parallelize each query block so how many programs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11001" target="_blank">03:03:21.920</a></span> | <span class="t">we we will have working in parallel at most it will be the sequence the number of batches so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11008" target="_blank">03:03:28.080</a></span> | <span class="t">the batch the number of sequences in the batch so the batch size it will be the batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11014" target="_blank">03:03:34.080</a></span> | <span class="t">multiplied by the number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11017" target="_blank">03:03:37.600</a></span> | <span class="t">multiplied by the number of blocks that we will divide the query sequence into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11027" target="_blank">03:03:47.200</a></span> | <span class="t">so let's call it the i don't know block size q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11030" target="_blank">03:03:50.480</a></span> | <span class="t">the block size q all right now that we have seen this one let's go actually code it so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11042" target="_blank">03:04:02.640</a></span> | <span class="t">i have already introduced a little bit the differences between my implementation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11048" target="_blank">03:04:08.160</a></span> | <span class="t">flash attention and the one that you can find on the triton documentation which is first of all i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11052" target="_blank">03:04:12.480</a></span> | <span class="t">don't work with fp8 because i believe this is unnecessary for our explanation it's of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11058" target="_blank">03:04:18.240</a></span> | <span class="t">much faster because the recent gpus also support fp8 second difference is that in the um in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11065" target="_blank">03:04:25.920</a></span> | <span class="t">flash attention on the triton website the backward pass is only implemented for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11071" target="_blank">03:04:31.840</a></span> | <span class="t">causal attention but in my case i implement it for the causal and the non-causal attention even if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11076" target="_blank">03:04:36.400</a></span> | <span class="t">it's slower and later i actually i want to give you an exercise on how to improve it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11081" target="_blank">03:04:41.680</a></span> | <span class="t">and the third difference main difference is that i made make explicit use of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11088" target="_blank">03:04:48.080</a></span> | <span class="t">softmax scale so i actually use the scale when needed another difference is that in the online</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11096" target="_blank">03:04:56.480</a></span> | <span class="t">triton computation of the flash attention is this x is not really e to the power of x but it's 2 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11103" target="_blank">03:05:03.120</a></span> | <span class="t">the power of x and then they compensate it with by by using the logarithm however because probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11110" target="_blank">03:05:10.160</a></span> | <span class="t">the implementation of 2 to the power of x is faster than the e to the power of x but in my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11115" target="_blank">03:05:15.600</a></span> | <span class="t">case i retain the original exponential because i want to follow the original algorithm to make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11120" target="_blank">03:05:20.480</a></span> | <span class="t">simpler to visualize the code along with the algorithm as in the flash attention paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11125" target="_blank">03:05:25.120</a></span> | <span class="t">so i know i have created a lot of hype so let's do it let's start by creating a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11134" target="_blank">03:05:34.720</a></span> | <span class="t">file let's call it a program.py just like before when i introduced triton i will start by coding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11141" target="_blank">03:05:41.120</a></span> | <span class="t">first the code that will use our kernel and then we code the kernel and we will only be coding the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11145" target="_blank">03:05:45.920</a></span> | <span class="t">forward pass of the kernel so let's start by importing what we need to import which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11153" target="_blank">03:05:53.280</a></span> | <span class="t">the torch and the triton and secondly let's start by let me check okay the copilot is already off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11160" target="_blank">03:06:00.400</a></span> | <span class="t">so i don't have to worry about that let's start to implement the code that will test our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11165" target="_blank">03:06:05.840</a></span> | <span class="t">implementation of the triton and compare it with the naive implementation of the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11170" target="_blank">03:06:10.240</a></span> | <span class="t">so we create our query key and value sequence for testing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11177" target="_blank">03:06:17.760</a></span> | <span class="t">which is if you remember it's a query is the batch size and it has the dimension batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11184" target="_blank">03:06:24.160</a></span> | <span class="t">because we have multiple sequences each sequence has a number of heads and it's made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11190" target="_blank">03:06:30.080</a></span> | <span class="t">sql and tokens and each token is identified by a head dim number of dimensions if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11197" target="_blank">03:06:37.440</a></span> | <span class="t">and then this is because we have already split each token into smaller tokens each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11203" target="_blank">03:06:43.440</a></span> | <span class="t">each with its own head dimension if you remove the num heads dimension then you put back you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11210" target="_blank">03:06:50.000</a></span> | <span class="t">concatenate all the dimensions of this head dim we initialize the query key and the value sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11217" target="_blank">03:06:57.040</a></span> | <span class="t">by using a normal distribution this code i already took from the tutorial of triton so it's nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11222" target="_blank">03:07:02.960</a></span> | <span class="t">different and we require the gradient because we want to compute the gradient with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11228" target="_blank">03:07:08.080</a></span> | <span class="t">to query key and value and we will see later why because because we want to implement the back we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11232" target="_blank">03:07:12.960</a></span> | <span class="t">want to test also the backward pass even though we will not be coding it now so the first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11238" target="_blank">03:07:18.160</a></span> | <span class="t">that we do is we define our softmax scale which is as you remember the formula is a query multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11245" target="_blank">03:07:25.840</a></span> | <span class="t">by the transpose of the keys and then divided by the square root of head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11253" target="_blank">03:07:33.040</a></span> | <span class="t">so dk or dd head sometimes it's called and then we need to so we need to compute this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11262" target="_blank">03:07:42.240</a></span> | <span class="t">one we can already compute it it's this this is the one over the square root of the head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11266" target="_blank">03:07:46.880</a></span> | <span class="t">and then we also define do and later we will see what is this but this is basically we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11275" target="_blank">03:07:55.200</a></span> | <span class="t">needed needed for the backward pass um don't worry if you don't understand what is do later we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11284" target="_blank">03:08:04.400</a></span> | <span class="t">see it let's do the naive implementation of the attention which is very simple which is first we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11290" target="_blank">03:08:10.960</a></span> | <span class="t">define the mask and we use this mask only if the attention we are computing is causal so as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11296" target="_blank">03:08:16.720</a></span> | <span class="t">see we pass this parameter called the causal that tells if we want to compute the causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11302" target="_blank">03:08:22.480</a></span> | <span class="t">or the not causal attention and the d type which is a float 16 because we want to work directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11308" target="_blank">03:08:28.160</a></span> | <span class="t">with 16 bit floating point numbers we will not be working with fp8 just uh because we don't we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11314" target="_blank">03:08:34.880</a></span> | <span class="t">don't want to implement my implementation is actually not as fast as the one in the tutorial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11319" target="_blank">03:08:39.840</a></span> | <span class="t">of the triton website but i believe it's much more easier to comprehend so we define the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11328" target="_blank">03:08:48.320</a></span> | <span class="t">we compute the the product the query multiplied by the transpose of the key divided by the square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11333" target="_blank">03:08:53.680</a></span> | <span class="t">root of the head dimension so that's why we are multiplying by softmax scale if the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11338" target="_blank">03:08:58.160</a></span> | <span class="t">we are computing is causal then we use this mask that we have computed so we replace all the points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11344" target="_blank">03:09:04.160</a></span> | <span class="t">all the dot products where this mask is equal to zero with minus infinities and then the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11349" target="_blank">03:09:09.920</a></span> | <span class="t">will replace this minus infinities with zeros because then we are applying the softmax and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11354" target="_blank">03:09:14.800</a></span> | <span class="t">the softmax is applied by rows just like the normal attention we compute okay the second thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11361" target="_blank">03:09:21.120</a></span> | <span class="t">that we do is we want to um so the output is the product of the output of the softmax with the v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11368" target="_blank">03:09:28.240</a></span> | <span class="t">so this is the reference output on the naive implementation of um flash of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11374" target="_blank">03:09:34.640</a></span> | <span class="t">mechanism then we want to compute we want to also derive the gradients of the output with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11382" target="_blank">03:09:42.320</a></span> | <span class="t">the um inputs and in this case it's the the the v the k and the q later we will see what are we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11391" target="_blank">03:09:51.600</a></span> | <span class="t">doing here then we want also to we want to compare this reference implementation with our triton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11398" target="_blank">03:09:58.480</a></span> | <span class="t">implementation so let's do it so our triton implementation will be implemented as a class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11403" target="_blank">03:10:03.680</a></span> | <span class="t">called triton attention that we will call using this method called apply and later we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11409" target="_blank">03:10:09.200</a></span> | <span class="t">what is this method in which we pass the query key and value if we want to compute the causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11413" target="_blank">03:10:13.520</a></span> | <span class="t">attention the softmax scale that it should be using and it should produce some output which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11418" target="_blank">03:10:18.560</a></span> | <span class="t">is the output of the output of the softmax multiplied by v then we can run also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11423" target="_blank">03:10:23.680</a></span> | <span class="t">backward and this backward will be the the same backward that we will compute with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11428" target="_blank">03:10:28.560</a></span> | <span class="t">triton attention and then we compare okay and then we can compare uh the result of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11438" target="_blank">03:10:38.000</a></span> | <span class="t">implementation so this triton attention dot apply with the reference implementation which is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11443" target="_blank">03:10:43.440</a></span> | <span class="t">one here and this should be uh we use the the function all close which basically compares</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11449" target="_blank">03:10:49.280</a></span> | <span class="t">the elements of two tensors and make sure that their absolute difference is no more than this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11454" target="_blank">03:10:54.560</a></span> | <span class="t">one we are not using the relative distance we are just using the absolute distance between the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11459" target="_blank">03:10:59.440</a></span> | <span class="t">elements which corresponding elements of two vectors this uh implementation that you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11464" target="_blank">03:11:04.720</a></span> | <span class="t">that we will build will work with the causal attention and also with not causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11468" target="_blank">03:11:08.480</a></span> | <span class="t">while the uh the one that we saw in the website of triton it only works with the uh the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11475" target="_blank">03:11:15.120</a></span> | <span class="t">pass actually works with the causal and non-causal while the backward pass only works in the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11479" target="_blank">03:11:19.200</a></span> | <span class="t">the causal attention um okay but it's highly optimized the one online so if you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11485" target="_blank">03:11:25.120</a></span> | <span class="t">learn a little more tricks on how to optimize triton kernels there is a lot of knowledge there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11490" target="_blank">03:11:30.320</a></span> | <span class="t">anyway guys now let's try to uh implement this triton attention at least the forward pass so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11496" target="_blank">03:11:36.080</a></span> | <span class="t">let's go to implement this triton attention class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11499" target="_blank">03:11:39.600</a></span> | <span class="t">okay here every time you want to introduce a new operation into torch you need to derive the um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11510" target="_blank">03:11:50.560</a></span> | <span class="t">you need to implement your operation by deriving from this autograd dot function class so every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11516" target="_blank">03:11:56.400</a></span> | <span class="t">operation in torch actually if it's the softmax or it's the um i don't know the the relu or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11523" target="_blank">03:12:03.360</a></span> | <span class="t">zwiglu or whatever there is it is always implemented as a function is a class that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11528" target="_blank">03:12:08.240</a></span> | <span class="t">derives from this function and it should provide two methods one called the forward pass and one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11533" target="_blank">03:12:13.200</a></span> | <span class="t">called the backward pass the forward should produce the output of this operation and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11537" target="_blank">03:12:17.280</a></span> | <span class="t">backward should compute the gradient um the gradient with the of the loss with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11542" target="_blank">03:12:22.960</a></span> | <span class="t">that the the input of that function and later we will see how that works for now let's concentrate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11548" target="_blank">03:12:28.480</a></span> | <span class="t">on the forward pass to implement the forward pass we need to create a static method that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11552" target="_blank">03:12:32.880</a></span> | <span class="t">is called forward which takes as input one thing called the context so as you know in autograd in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11561" target="_blank">03:12:41.600</a></span> | <span class="t">when training neural networks we have the forward pass and the backward when computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11566" target="_blank">03:12:46.240</a></span> | <span class="t">backward pass we need to reuse the activations of each of the computation nodes during the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11571" target="_blank">03:12:51.600</a></span> | <span class="t">pass and this context basically allow us to save the information to uh for the necessary activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11577" target="_blank">03:12:57.520</a></span> | <span class="t">that we will need during the backward pass and later we will see in the triton um in the flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11582" target="_blank">03:13:02.400</a></span> | <span class="t">attention algorithm what information we need to save in order to compute the backward pass for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11587" target="_blank">03:13:07.680</a></span> | <span class="t">example what we will need to save during the backward pass we will need to recompute on the fly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11593" target="_blank">03:13:13.040</a></span> | <span class="t">the soft the query multiplied by the transport of the keys for each block but we don't want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11598" target="_blank">03:13:18.800</a></span> | <span class="t">recompute the normalization factor or the maximum value for each row so we will save those two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11603" target="_blank">03:13:23.520</a></span> | <span class="t">values and actually we will not save two values we will save one value we do a trick called the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11608" target="_blank">03:13:28.720</a></span> | <span class="t">sum exploit log sum exploit that we will see later anyway this context is just a kind of a storage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11615" target="_blank">03:13:35.920</a></span> | <span class="t">area where we can save some stuff that will be necessary for us to recompute the backward and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11621" target="_blank">03:13:41.040</a></span> | <span class="t">you can see whatever you like then we have the input of this operation which is the query key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11627" target="_blank">03:13:47.360</a></span> | <span class="t">and value which is a three tensors with the causal if we are going to compute the causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11632" target="_blank">03:13:52.720</a></span> | <span class="t">and the softmax scale that we should apply based on the one over the square root of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11636" target="_blank">03:13:56.800</a></span> | <span class="t">head dimension which we could also compute it on the fly actually by the way by by checking the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11644" target="_blank">03:14:04.240</a></span> | <span class="t">shape of this but okay it doesn't matter anyway so um the first thing that we are going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11649" target="_blank">03:14:09.520</a></span> | <span class="t">is to extract the shapes of these objects and make sure all the shapes are what we expect them to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11654" target="_blank">03:14:14.560</a></span> | <span class="t">so the shape of the query key and value is a batch size by number of heads by sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11659" target="_blank">03:14:19.840</a></span> | <span class="t">by head dimension we make sure that the head dimension matches for the query key and value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11665" target="_blank">03:14:25.920</a></span> | <span class="t">they should match because each vector should should be of the same size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11670" target="_blank">03:14:30.560</a></span> | <span class="t">and then we declare what we pre-allocate the output vector so where we should save our output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11678" target="_blank">03:14:38.640</a></span> | <span class="t">so as you remember the output in the attention mechanism has the same same shape as the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11684" target="_blank">03:14:44.720</a></span> | <span class="t">key and value sequence where the query key and value sequence i want to remind you is not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11689" target="_blank">03:14:49.440</a></span> | <span class="t">query key and value of the input of the attention which is a sequence of tokens but it's the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11694" target="_blank">03:14:54.880</a></span> | <span class="t">already of the wqwk and wv because flash attention is not concerned with optimizing those metrics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11701" target="_blank">03:15:01.040</a></span> | <span class="t">multiplication but only the output of the wqwk and wv so we pre-allocate the output tensor where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11708" target="_blank">03:15:08.640</a></span> | <span class="t">we will store this output which has the same shape as the query key and sequence uh matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11716" target="_blank">03:15:16.240</a></span> | <span class="t">actually actually no not true actually it has the same shape as the query but it may not be the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11723" target="_blank">03:15:23.600</a></span> | <span class="t">as the key and value why because there is this thing called cross attention where the query key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11729" target="_blank">03:15:29.920</a></span> | <span class="t">and value are transposition are different projection through wqwk wv not of the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11736" target="_blank">03:15:36.560</a></span> | <span class="t">input sequence but of two sequences so cross attention happens when we have a query that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11741" target="_blank">03:15:41.440</a></span> | <span class="t">comes from one uh sequence and the key and value come from another sequence and they pass through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11747" target="_blank">03:15:47.440</a></span> | <span class="t">their own wk wv and they may not have the same sequence length so the shapes of the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11753" target="_blank">03:15:53.280</a></span> | <span class="t">the attention only depends on the shape of the query sequence not of the key and value sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11758" target="_blank">03:15:58.880</a></span> | <span class="t">this is happens during cross attention but usually in language models we always work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11763" target="_blank">03:16:03.360</a></span> | <span class="t">with the self-attention so that should not happen at least in the causal language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11768" target="_blank">03:16:08.480</a></span> | <span class="t">then we have the stage and later we will see what is this stage basically the stage it's just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11776" target="_blank">03:16:16.400</a></span> | <span class="t">number that tells if the operation that we are going to do later is for the causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11782" target="_blank">03:16:22.160</a></span> | <span class="t">or for the not causal attention and then we need to define our launch grid the launch grid tells</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11788" target="_blank">03:16:28.400</a></span> | <span class="t">us how many parallel process we need to be launched by triton actually they will be launched by cuda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11794" target="_blank">03:16:34.800</a></span> | <span class="t">but by we always work with the triton as an interface to cuda so by triton so in triton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11801" target="_blank">03:16:41.840</a></span> | <span class="t">as i said before we want to parallelize along the batch dimension so each batch each sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11808" target="_blank">03:16:48.560</a></span> | <span class="t">in the batch should work independently from each other not only each inside of each sequence in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11814" target="_blank">03:16:54.160</a></span> | <span class="t">the batch each head should work independently from each other so at least we have a batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11818" target="_blank">03:16:58.720</a></span> | <span class="t">multiplied by number of heads programs and for each of this program we have another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11825" target="_blank">03:17:05.840</a></span> | <span class="t">dimension called the we divide the query into blocks of queries so as you remember when talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11835" target="_blank">03:17:15.200</a></span> | <span class="t">about a block matrix multiplication we don't work with the query as the original matrix query matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11840" target="_blank">03:17:20.560</a></span> | <span class="t">so where each query is one vector or one token we work with group of queries so each block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11847" target="_blank">03:17:27.840</a></span> | <span class="t">queries is a group of tokens in the query sequence so we are saying that we want to launch at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11854" target="_blank">03:17:34.560</a></span> | <span class="t">number of kernels or blocks of threads or a group of threads along two dimensions so just like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11864" target="_blank">03:17:44.480</a></span> | <span class="t">cuda kernel can be launched along two dimension x and y here we are launching programs along two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11870" target="_blank">03:17:50.000</a></span> | <span class="t">dimensions one dimension that tells us which batch which head of which batch we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11876" target="_blank">03:17:56.240</a></span> | <span class="t">to work with so which head of which batch element are we going to work with and inside this we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11887" target="_blank">03:18:07.040</a></span> | <span class="t">going to say okay this is a sequence which group of queries are we going to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11896" target="_blank">03:18:16.400</a></span> | <span class="t">are we going to going to work with so overall and the group of queries is what is the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11905" target="_blank">03:18:25.840</a></span> | <span class="t">length divided by the number of queries that we want to group together so the block size cube</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11910" target="_blank">03:18:30.880</a></span> | <span class="t">tells us how many queries are there in each block of queries so this cdiv is just the ceiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11917" target="_blank">03:18:37.200</a></span> | <span class="t">division so it is equal to let me write it here this is equal to ceiling of sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11924" target="_blank">03:18:44.960</a></span> | <span class="t">divided by the block size q this tells us how many blocks of q we have so let's rehearse we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11937" target="_blank">03:18:57.840</a></span> | <span class="t">have a tensor that is q that is batch size by number of heads and each flash attention algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11945" target="_blank">03:19:05.680</a></span> | <span class="t">will work with the following the sequence length head dimension moreover we have seen that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11950" target="_blank">03:19:10.720</a></span> | <span class="t">flash attention has two loops one is the outer loop among all the query blocks one is the inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11957" target="_blank">03:19:17.200</a></span> | <span class="t">loop along all the key block we have seen that the query block can work independently from each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11962" target="_blank">03:19:22.960</a></span> | <span class="t">other so we can spawn as many programs in parallel as there are number of blocks of q because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11968" target="_blank">03:19:28.880</a></span> | <span class="t">can work in parallel so this grid tells us how many programs there are that can work in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11974" target="_blank">03:19:34.080</a></span> | <span class="t">then it will be the gpu that based on its resources will decide how many program actually to work in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11979" target="_blank">03:19:39.360</a></span> | <span class="t">parallel if it has enough resources to make them all work in parallel wonderful if it doesn't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11985" target="_blank">03:19:45.200</a></span> | <span class="t">enough resources to make them work in parallel it will launch them sequentially one after another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11988" target="_blank">03:19:48.880</a></span> | <span class="t">and the last dimension is this is like the z dimension in the cuda in the cuda launch grid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=11997" target="_blank">03:19:57.760</a></span> | <span class="t">and we don't want to use it because we don't want an additional level of parallelism all right this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12005" target="_blank">03:20:05.040</a></span> | <span class="t">is our launch grid so we will launch a number of programs that is this one a number of programs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12012" target="_blank">03:20:12.240</a></span> | <span class="t">of parallel programs or number of parallel kernels and each kernel in triton work is a group of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12018" target="_blank">03:20:18.800</a></span> | <span class="t">threads which is a batch size multiplied by number of heads multiplied by a number of blocks of q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12030" target="_blank">03:20:30.000</a></span> | <span class="t">so how many blocks we have we divided the q sequence into okay let's continue so then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12038" target="_blank">03:20:38.320</a></span> | <span class="t">will see what is this one so this m is another matrix that we will need and it's the log sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12044" target="_blank">03:20:44.400</a></span> | <span class="t">expo for the backward pass and we will see at the end of this video what is not at the end of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12049" target="_blank">03:20:49.280</a></span> | <span class="t">video but at the end of the forward pass what it's needed for but basically this is you can think of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12054" target="_blank">03:20:54.960</a></span> | <span class="t">it as the maximum for each row um you we to to recompute the query multiplied by the key in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12062" target="_blank">03:21:02.560</a></span> | <span class="t">backward pass we should also have if we don't want to recompute the maximum for each row and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12067" target="_blank">03:21:07.120</a></span> | <span class="t">the normalization factor of the softmax we should save two things one is the maximum for each row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12072" target="_blank">03:21:12.000</a></span> | <span class="t">and one is the the normalization factor however by using the log sum exp trick we can only save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12079" target="_blank">03:21:19.680</a></span> | <span class="t">one value which is the as you can see in the algorithm of flash attention it's this stuff here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12086" target="_blank">03:21:26.800</a></span> | <span class="t">which is let's see here it's this stuff here so this li which is the maximum for each row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12096" target="_blank">03:21:36.480</a></span> | <span class="t">plus the logarithm of the of the normalization factor and basically in when computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12106" target="_blank">03:21:46.320</a></span> | <span class="t">backward pass we need to recompute on the fly this block here so this query multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12111" target="_blank">03:21:51.360</a></span> | <span class="t">transpose but to apply the softmax as you remember we need to have the maximum for each row and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12115" target="_blank">03:21:55.120</a></span> | <span class="t">normalization factor so we don't we don't recompute them during the backward because we have already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12121" target="_blank">03:22:01.440</a></span> | <span class="t">computed them during the forward so we save this information but we don't need to save these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12125" target="_blank">03:22:05.600</a></span> | <span class="t">information separately we can aggregate it into one single value called li and later we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12130" target="_blank">03:22:10.720</a></span> | <span class="t">how we can use it all right so we have defined also this one and we can proceed further so now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12140" target="_blank">03:22:20.800</a></span> | <span class="t">we launch our grid our kernel don't be scared it's going to be a little long so here so we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12149" target="_blank">03:22:29.920</a></span> | <span class="t">launching the the kernel for the forward pass by defining what is the launch grid so how many of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12156" target="_blank">03:22:36.720</a></span> | <span class="t">this program should run in parallel at most we are passing the query we are passing the key we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12162" target="_blank">03:22:42.160</a></span> | <span class="t">are passing the values we are passing the softmax scale the m which is the information that we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12167" target="_blank">03:22:47.120</a></span> | <span class="t">to save for the backward pass it's actually the l in the code of the pseudo code of the flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12173" target="_blank">03:22:53.760</a></span> | <span class="t">attention algorithm here i call it m i think because also in the original code it was called m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12180" target="_blank">03:23:00.640</a></span> | <span class="t">the o where the our kernel should save its output and then as you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12187" target="_blank">03:23:07.280</a></span> | <span class="t">we don't get all the nice access by indexing tensor like we are used to in torch we only get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12198" target="_blank">03:23:18.080</a></span> | <span class="t">a pointer to the starting element of q a pointer to the starting element of k and to the starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12203" target="_blank">03:23:23.680</a></span> | <span class="t">element of v and then we have to figure out all the index in the memory of the other elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12210" target="_blank">03:23:30.080</a></span> | <span class="t">how to calculate the index we need the stride because the stride tells us how many elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12215" target="_blank">03:23:35.440</a></span> | <span class="t">to skip to go from one dimension to the other and that's why we are passing the stride for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12220" target="_blank">03:23:40.400</a></span> | <span class="t">each dimension of each tensor actually in our case we are only working with q k and v that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12228" target="_blank">03:23:48.400</a></span> | <span class="t">actually of the same d type and of the same shape so we should not need actually to pass all all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12235" target="_blank">03:23:55.360</a></span> | <span class="t">strides for each of these tensors because they should have the same strides however in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12241" target="_blank">03:24:01.520</a></span> | <span class="t">original code i believe they were passing it so i kept it so the stride allow will allow us to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12247" target="_blank">03:24:07.440</a></span> | <span class="t">index this pointer so to understand to access the elements of of this tensor just by using its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12256" target="_blank">03:24:16.160</a></span> | <span class="t">starting the pointer to the starting element and then the strides we will be able to index any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12261" target="_blank">03:24:21.360</a></span> | <span class="t">element we want in the tensor then we pass the information of these shapes so the batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12268" target="_blank">03:24:28.320</a></span> | <span class="t">the number of heads the sequence length and the head dimension and which is the same for all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12274" target="_blank">03:24:34.720</a></span> | <span class="t">them and then the stage the stage indicates if we are going to compute the causal attention or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12279" target="_blank">03:24:39.680</a></span> | <span class="t">causal attention so let's not implement it and let's continue writing this method so then then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12286" target="_blank">03:24:46.560</a></span> | <span class="t">we need to save some information that we will be needed for the backward pass which is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12290" target="_blank">03:24:50.400</a></span> | <span class="t">context variable that i told you before so we save some information for the backward pass which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12295" target="_blank">03:24:55.840</a></span> | <span class="t">the query key and value which are the tensor for which we want to compute the gradient during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12301" target="_blank">03:25:01.200</a></span> | <span class="t">backward pass and then we need to store also this m tensor and this o tensor and then we can all we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12313" target="_blank">03:25:13.280</a></span> | <span class="t">need to also store the causal variable so because if we computed the causal attention during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12318" target="_blank">03:25:18.640</a></span> | <span class="t">forward forward pass then during the backward pass we need to have this information because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12325" target="_blank">03:25:25.440</a></span> | <span class="t">we need to mask out the things that we don't want to contribute to the gradient but we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12331" target="_blank">03:25:31.040</a></span> | <span class="t">see that later when computing the backward pass for now let's concentrate on this attention forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12335" target="_blank">03:25:35.600</a></span> | <span class="t">so we need to implement this forward kernel that you can see so underscore attention underscore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12341" target="_blank">03:25:41.600</a></span> | <span class="t">forward method now a triton kernel is just a python method with a particular decorator called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12348" target="_blank">03:25:48.400</a></span> | <span class="t">triton.git so we copy and paste the signature so this is what makes a method become a triton kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12356" target="_blank">03:25:56.800</a></span> | <span class="t">and as you can see here we pass the query key and value matrix along with other information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12363" target="_blank">03:26:03.360</a></span> | <span class="t">the emmetrix so please don't confuse the emmetrix with the mask that we will apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12367" target="_blank">03:26:07.680</a></span> | <span class="t">on the fly we will generate it on the fly because we are only concerned in this case with a causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12373" target="_blank">03:26:13.760</a></span> | <span class="t">attention or not causal attention we do not accept custom masks here and then we pass the strides of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12381" target="_blank">03:26:21.520</a></span> | <span class="t">all these tensors the batch size the number number of heads the sequence length the head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12386" target="_blank">03:26:26.160</a></span> | <span class="t">which is the shape of each of these tensors and the block size q and the block size kv the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12393" target="_blank">03:26:33.600</a></span> | <span class="t">size q indicates how many queries we want to group together to make one block of the q matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12399" target="_blank">03:26:39.920</a></span> | <span class="t">and how the kv indicates how many keys and values we want to put together to make one block of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12406" target="_blank">03:26:46.160</a></span> | <span class="t">k and v matrix which is what we do when we do block matrix multiplication this stage is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12412" target="_blank">03:26:52.720</a></span> | <span class="t">number that indicates if it's a causal or not causal attention we are doing so it will be three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12418" target="_blank">03:26:58.640</a></span> | <span class="t">in case it's a causal and one in case it's not causal okay the first thing that we do is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12424" target="_blank">03:27:04.000</a></span> | <span class="t">verify some information so we verify that the um the block size of the kv is less than or equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12432" target="_blank">03:27:12.560</a></span> | <span class="t">the head dimension to be honest i don't think we need it with my code because i removed most of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12438" target="_blank">03:27:18.640</a></span> | <span class="t">constraints so this uh this check was also present in the original code so i kept it but it all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12444" target="_blank">03:27:24.160</a></span> | <span class="t">depends on how we are later we will see what is the auto tuning process and later we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12449" target="_blank">03:27:29.680</a></span> | <span class="t">what variables we are going to auto tune for and how many stages we will choose how many warps we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12454" target="_blank">03:27:34.560</a></span> | <span class="t">will choose etc etc so let's leave it for later you can comment it or you can keep it it shouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12459" target="_blank">03:27:39.600</a></span> | <span class="t">matter um the first thing that we do as i said before we launch a grid so a grid is a series</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12466" target="_blank">03:27:46.880</a></span> | <span class="t">of programs where we will have some identifiers like in the cuda we had an identifier for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12472" target="_blank">03:27:52.240</a></span> | <span class="t">blocks on the x-axis and on the y-axis in triton we get this identifier for the programs we launched</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12479" target="_blank">03:27:59.600</a></span> | <span class="t">um um uh sequence length divided by block size q number of programs along the zeroth axis and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12489" target="_blank">03:28:09.600</a></span> | <span class="t">the batch size multiplied by number of heads on along the first axis of the grid of the launch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12496" target="_blank">03:28:16.880</a></span> | <span class="t">grid which will help us identify which um part of the query we are going to work with in this program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12505" target="_blank">03:28:25.200</a></span> | <span class="t">in this kernel and also in which batch and on which head this program should work with so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12513" target="_blank">03:28:33.040</a></span> | <span class="t">what we are going to do now we are trying to understand what part of the input we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12517" target="_blank">03:28:37.680</a></span> | <span class="t">work with based on the ids of the program which corresponds to the block id in cuda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12523" target="_blank">03:28:43.680</a></span> | <span class="t">so let me copy so the program id zero indicates it's this stuff here tells us which part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12533" target="_blank">03:28:53.760</a></span> | <span class="t">queries so which block of the queries we are going to work with why do we have a block on the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12539" target="_blank">03:28:59.520</a></span> | <span class="t">because as we saw before the output can be computed independently for each block of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12543" target="_blank">03:29:03.760</a></span> | <span class="t">queries while each block of the query has to iterate through all the key and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12548" target="_blank">03:29:08.960</a></span> | <span class="t">so this is what will tell us what is the index of the block of the queries we are going to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12555" target="_blank">03:29:15.040</a></span> | <span class="t">with in this particular program then we can understand also which index which batch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12560" target="_blank">03:29:20.800</a></span> | <span class="t">which head with this program is associated with the program id number one is the product of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12566" target="_blank">03:29:26.880</a></span> | <span class="t">batch size and the number of heads it means that we will have as many programs on the axis number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12572" target="_blank">03:29:32.960</a></span> | <span class="t">one as there are indicated by this product so this product lets us understand this product will tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12581" target="_blank">03:29:41.360</a></span> | <span class="t">us which batch and which head this particular program is associated with so to get the id of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12588" target="_blank">03:29:48.400</a></span> | <span class="t">the batch we just divide this number by the number of heads and it will give us the head index and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12594" target="_blank">03:29:54.160</a></span> | <span class="t">to get the head index inside this batch we just do the this number here modulus the number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12600" target="_blank">03:30:00.960</a></span> | <span class="t">okay the next thing that we need to do we need to okay first of all when we pass a tensor because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12612" target="_blank">03:30:12.720</a></span> | <span class="t">as you can see here the q parameter to this attention forward method is a tensor because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12618" target="_blank">03:30:18.480</a></span> | <span class="t">it's the input of this function forward function and this forward function is called here when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12624" target="_blank">03:30:24.320</a></span> | <span class="t">do attention dot apply and it's this q stuff here and this q stuff here has been created as a tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12631" target="_blank">03:30:31.040</a></span> | <span class="t">so when we pass a tensor to a triton kernel it's not really a tensor it is a pointer to the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12636" target="_blank">03:30:36.720</a></span> | <span class="t">element of that tensor in the memory now we need to understand because now we know which batch we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12642" target="_blank">03:30:42.800</a></span> | <span class="t">are going to work with and which head we are going to work with we need to index this tensor to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12647" target="_blank">03:30:47.680</a></span> | <span class="t">select the right batch and the right head inside of the right batch which means that basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12653" target="_blank">03:30:53.840</a></span> | <span class="t">we have this q tensor so we need to do some some sort of like some stuff like this like q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12661" target="_blank">03:31:01.440</a></span> | <span class="t">of the index batch and the number of the number of heads indicates the which head we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12671" target="_blank">03:31:11.200</a></span> | <span class="t">to work with so it should be index of head and we need to select everything that is inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12677" target="_blank">03:31:17.040</a></span> | <span class="t">these indices so we are we need to enter the tensor at the right location where the particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12683" target="_blank">03:31:23.840</a></span> | <span class="t">sequence length and head dimension for this batch and for this head starts for that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12689" target="_blank">03:31:29.520</a></span> | <span class="t">generate an offset in which we need to move this tensor from because this tensor this pointer sorry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12696" target="_blank">03:31:36.160</a></span> | <span class="t">this not answer this pointer from because this pointer is pointing at the beginning of the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12701" target="_blank">03:31:41.040</a></span> | <span class="t">tensor so we need to move in the batch size dimension and in the number of heads dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12705" target="_blank">03:31:45.840</a></span> | <span class="t">to do that we generate the following offset which will tell us where this uh where this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12712" target="_blank">03:31:52.640</a></span> | <span class="t">particular batch and where this particular head starts in this tensor and to do that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12718" target="_blank">03:31:58.640</a></span> | <span class="t">do the strides we need to use the strides so what we are going to do is we are creating we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12723" target="_blank">03:32:03.520</a></span> | <span class="t">to create the qkv offset this should be the sequence length which will be the index batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12731" target="_blank">03:32:11.520</a></span> | <span class="t">multiplied by the stride for the batch dimension which will tell us how many elements we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12737" target="_blank">03:32:17.680</a></span> | <span class="t">skip to get to the next batch and it's based and we multiply it by the index of the batch that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12743" target="_blank">03:32:23.600</a></span> | <span class="t">want so for the zeroth batch we don't skip anything because we are already pointing to the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12747" target="_blank">03:32:27.760</a></span> | <span class="t">element of that batch but if we are at the batch one we will skip that many number of elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12752" target="_blank">03:32:32.720</a></span> | <span class="t">plus we also need to skip the some heads how many head we need to skip based on which head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12758" target="_blank">03:32:38.320</a></span> | <span class="t">we are going to work with and what tells us how how to go from one head to the next the stride</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12764" target="_blank">03:32:44.080</a></span> | <span class="t">of the head dimension so we multiply the index head so the head that we should be working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12769" target="_blank">03:32:49.280</a></span> | <span class="t">with the stride q head all right then we select now triton helps us with a new function that i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12782" target="_blank">03:33:02.560</a></span> | <span class="t">think it was quite recent that helps us index element inside of a tensor without having to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12788" target="_blank">03:33:08.880</a></span> | <span class="t">deal with all the complex indexing maths that can be confusing for beginners so i will be using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12796" target="_blank">03:33:16.960</a></span> | <span class="t">few methods to help us with this with this indexing and this function is called make block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12805" target="_blank">03:33:25.760</a></span> | <span class="t">pointer and it's this following so basically this make block pointer takes as input a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12813" target="_blank">03:33:33.600</a></span> | <span class="t">and sorry a pointer not a vector it takes as input a pointer in this case we are saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12821" target="_blank">03:33:41.920</a></span> | <span class="t">create a block that has the following shape that is sequence length by head dimension so let me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12828" target="_blank">03:33:48.720</a></span> | <span class="t">do it one by one actually i don't want to confuse you guys with all this stuff altogether okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12834" target="_blank">03:33:54.400</a></span> | <span class="t">take a start there is a pointer that is right now pointing at q plus q kv offset so right now it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12842" target="_blank">03:34:02.320</a></span> | <span class="t">not pointing at the first batch but it's pointing exactly to our batch so the the batch that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12848" target="_blank">03:34:08.160</a></span> | <span class="t">particular program should be working with and inside this batch to the particular head that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12853" target="_blank">03:34:13.200</a></span> | <span class="t">this program should be working with which is basically saying that we have we are pointing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12859" target="_blank">03:34:19.760</a></span> | <span class="t">to a tensor that is as follows so we are pointing to the following tensors which is the right head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12866" target="_blank">03:34:26.480</a></span> | <span class="t">the right sorry the right batch and the right head and then we are selecting everything inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12874" target="_blank">03:34:34.160</a></span> | <span class="t">so it's pointing to the first element of this particular tensor this tensor particular tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12881" target="_blank">03:34:41.040</a></span> | <span class="t">because we have already selected the batch and the head it is a two-dimensional tensor with this the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12886" target="_blank">03:34:46.560</a></span> | <span class="t">following shape because the following dimensions are sequence length and head dim so we are saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12892" target="_blank">03:34:52.480</a></span> | <span class="t">take this pointer which contains a tensor of the following shape sequence length and head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12900" target="_blank">03:35:00.640</a></span> | <span class="t">and i'm also giving you the strides of these dimensions that are in this pointer so the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12907" target="_blank">03:35:07.280</a></span> | <span class="t">two dimensions that are that we need are the sequence dimension and the head dim dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12912" target="_blank">03:35:12.320</a></span> | <span class="t">which is this one for the q tensor and um and in this um in this query tensor we want to select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12927" target="_blank">03:35:27.040</a></span> | <span class="t">a block of queries based on the query on the block of queries that this program should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12933" target="_blank">03:35:33.040</a></span> | <span class="t">working with so i think i need to maybe probably use the ipad otherwise it can be very confusing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12939" target="_blank">03:35:39.840</a></span> | <span class="t">to visualize so uh let's do it actually let me see if i can create another here and let's use the ipad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12953" target="_blank">03:35:53.840</a></span> | <span class="t">all right okay so we have a q vector q tensor because this construct we will be using it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12962" target="_blank">03:36:02.960</a></span> | <span class="t">all the other tests so if you understand it for one tensor you understand it for all the others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12967" target="_blank">03:36:07.040</a></span> | <span class="t">we have a q tensor that is a batch by number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12974" target="_blank">03:36:14.800</a></span> | <span class="t">number of heads then the sequence length and then the head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12979" target="_blank">03:36:19.840</a></span> | <span class="t">with the following line so the this line here so when we create a q plus qkv offset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=12992" target="_blank">03:36:32.400</a></span> | <span class="t">we are already selecting the right batch dimension and already the right head dimension which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13000" target="_blank">03:36:40.000</a></span> | <span class="t">that we have already forwarded our q to not point to the first batch and the first head but to point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13008" target="_blank">03:36:48.800</a></span> | <span class="t">to the exact batch that this program is working with and the exact head that this program is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13013" target="_blank">03:36:53.040</a></span> | <span class="t">working with which basically means that right now it is pointing at a tensor that is made up of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13018" target="_blank">03:36:58.960</a></span> | <span class="t">two dimensions now inside of this tensor we also need to select the right block of query that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13027" target="_blank">03:37:07.840</a></span> | <span class="t">program should work with and this dimension here so the sequence dimension is all the queries so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13034" target="_blank">03:37:14.880</a></span> | <span class="t">we need to select the right queries so we need to skip some queries how to skip some queries well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13041" target="_blank">03:37:21.200</a></span> | <span class="t">we say that we need to skip block index multiplied by block size q number of queries because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13048" target="_blank">03:37:28.560</a></span> | <span class="t">will be processed by another by another program that will have this number here the program id</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13056" target="_blank">03:37:36.480</a></span> | <span class="t">will be different so we are selecting with this line not only inside of the q the right index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13064" target="_blank">03:37:44.880</a></span> | <span class="t">and the head but also the right position in this dimension in the sequence length dimension that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13070" target="_blank">03:37:50.000</a></span> | <span class="t">will point to the exact to the starting point of the exact query block that this particular program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13076" target="_blank">03:37:56.160</a></span> | <span class="t">should be working with this is what is happening and we are also creating this block basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13082" target="_blank">03:38:02.080</a></span> | <span class="t">later we will see how it can be used to to create a block of the shape we are telling what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13089" target="_blank">03:38:09.840</a></span> | <span class="t">the size of this tensor so this tensor has two dimensions because we are pointing to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13096" target="_blank">03:38:16.960</a></span> | <span class="t">beginning of the right query sequence so it has only two dimensions the sequence dimension and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13102" target="_blank">03:38:22.400</a></span> | <span class="t">the head dim dimension so it's the last dimension and we are already pointing to the right beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13109" target="_blank">03:38:29.280</a></span> | <span class="t">of the sequence dimension because we have already skipped some queries why we are skipping some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13115" target="_blank">03:38:35.440</a></span> | <span class="t">queries because these queries will be handled by another program that will have a block index q to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13120" target="_blank">03:38:40.000</a></span> | <span class="t">some other values and this order actually i don't know what is this order you can try to put 0 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13128" target="_blank">03:38:48.400</a></span> | <span class="t">and 1 2 i think it's some optimization that triton does i have read the online documentation and i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13132" target="_blank">03:38:52.880</a></span> | <span class="t">couldn't find anything about it so this is something that i will investigate but actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13138" target="_blank">03:38:58.160</a></span> | <span class="t">even if you put 0 1 it doesn't matter so i think it's something that you tell triton if this you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13145" target="_blank">03:39:05.600</a></span> | <span class="t">want the transposed of this block or you want the not transposed version of this block and later we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13150" target="_blank">03:39:10.080</a></span> | <span class="t">will see actually how we can transpose the key block without doing any transpose operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13155" target="_blank">03:39:15.280</a></span> | <span class="t">actually we will just change the strides like we have seen before so um now this make block pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13163" target="_blank">03:39:23.280</a></span> | <span class="t">is not something that is necessary but it makes our life easier when we will index this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13170" target="_blank">03:39:30.000</a></span> | <span class="t">pointer so we can treat this pointer nearly as nearly in the same way when we work with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13176" target="_blank">03:39:36.320</a></span> | <span class="t">tensor in pytorch we will be able to skip one increase one index in one dimension without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13183" target="_blank">03:39:43.600</a></span> | <span class="t">having to do the computation of the strides later when doing the backward pass i will not use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13189" target="_blank">03:39:49.360</a></span> | <span class="t">one and do all the pointer indexing by hand so you can check the differences of indexing a tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13194" target="_blank">03:39:54.560</a></span> | <span class="t">by using make block pointer and not by using it anyway to rehearse what are we creating we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13200" target="_blank">03:40:00.800</a></span> | <span class="t">creating a pointer to the right index in the batch to the right index in the head dimension and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13207" target="_blank">03:40:07.920</a></span> | <span class="t">are already skipping some queries based on the block index queue so this pointer is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13215" target="_blank">03:40:15.040</a></span> | <span class="t">pointing to the right block of queries that this particular program should be working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13218" target="_blank">03:40:18.800</a></span> | <span class="t">let's look instead at the v and the k block now so let's copy the v block now which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13228" target="_blank">03:40:28.960</a></span> | <span class="t">similar to the query but we are not going inside we are only indexing by the index batch and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13235" target="_blank">03:40:35.360</a></span> | <span class="t">index head so what this one actually let me write it here is already skipping so this amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13243" target="_blank">03:40:43.200</a></span> | <span class="t">queries this is what we are indexing with this make block pointer so we are in the right batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13250" target="_blank">03:40:50.400</a></span> | <span class="t">in the right head and we are skipping some queries here we are just indexing by batch and by head so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13258" target="_blank">03:40:58.880</a></span> | <span class="t">we are doing v of index batch index head and we are not selecting we are not skipping anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13267" target="_blank">03:41:07.680</a></span> | <span class="t">because you see this offset is equal to zero in the first dimension in the second dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13272" target="_blank">03:41:12.000</a></span> | <span class="t">so we are not skipping anything on the sequence length and we are not skipping anything in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13276" target="_blank">03:41:16.160</a></span> | <span class="t">head dimension dimension head dimension dimension um all right so let's look at the k block pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13285" target="_blank">03:41:25.520</a></span> | <span class="t">and this is different because as you know when computing the flash attention algorithm we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13290" target="_blank">03:41:30.880</a></span> | <span class="t">to have access to the block of queries and all the block of the key transposed so when accessing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13298" target="_blank">03:41:38.720</a></span> | <span class="t">key we shouldn't access it like we are accessing q we should invert the two dimensions that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13306" target="_blank">03:41:46.000</a></span> | <span class="t">to transpose for and that's very simple with make block ptr and you can see it here we say that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13313" target="_blank">03:41:53.040</a></span> | <span class="t">want to point to the right index and to the right head and the tensor inside of it so let's let me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13320" target="_blank">03:42:00.640</a></span> | <span class="t">write it here so later i can explain in line by line so what we are doing here is go to the k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13325" target="_blank">03:42:05.440</a></span> | <span class="t">tensor select the right batch select the right head select everything that is inside so it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13333" target="_blank">03:42:13.120</a></span> | <span class="t">tensor of two dimensions with the sequence length and the head dim because we you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13338" target="_blank">03:42:18.400</a></span> | <span class="t">here sequence length and head dims etc but we don't want first sequence length and then head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13347" target="_blank">03:42:27.680</a></span> | <span class="t">dim we want first head dim and then sequence length so we want to transpose it how to transpose it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13352" target="_blank">03:42:32.400</a></span> | <span class="t">we just say that you need to read this tensor with the two strides transposed so we are saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13357" target="_blank">03:42:37.840</a></span> | <span class="t">first use the stride of the dimension dimension and then use the stride of the sequence dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13364" target="_blank">03:42:44.400</a></span> | <span class="t">and the shape of this tensor is not sequence head dim it's a head dim sequence and it's a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13375" target="_blank">03:42:55.360</a></span> | <span class="t">of kvs why we are not putting directly the sequence dimension here because we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13384" target="_blank">03:43:04.000</a></span> | <span class="t">skip block by block later so we are not selecting all the sequence length in the sequence dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13389" target="_blank">03:43:09.200</a></span> | <span class="t">we are just selecting a block of kvs and later we will use another method to go to the next block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13394" target="_blank">03:43:14.960</a></span> | <span class="t">so i hope that by showing you the indexing like this it's a little easier to follow the indexing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13402" target="_blank">03:43:22.000</a></span> | <span class="t">so for each tensor we are going in the right batch in the right head dimension and for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13407" target="_blank">03:43:27.280</a></span> | <span class="t">query we are skipping some query blocks because each each program will work with a small different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13413" target="_blank">03:43:33.120</a></span> | <span class="t">query block but for the key and value each program needs to iterate through all the key and value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13420" target="_blank">03:43:40.000</a></span> | <span class="t">so we just point it to the first key and value block and then we will advance by one block by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13425" target="_blank">03:43:45.600</a></span> | <span class="t">we will advance one block by one during the for loop that we will do later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13431" target="_blank">03:43:51.120</a></span> | <span class="t">then in the output also we need we can make a tensor block tensor this basically creates a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13440" target="_blank">03:44:00.480</a></span> | <span class="t">pointer just like in the query key and value case in which we select the right index batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13446" target="_blank">03:44:06.080</a></span> | <span class="t">so what we are doing is we are indexing by batch we are indexing by head and we are selecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13452" target="_blank">03:44:12.560</a></span> | <span class="t">everything is that inside i know we are not selecting everything inside we are skipping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13457" target="_blank">03:44:17.040</a></span> | <span class="t">also in this case some blocks of queries because as i said before the output has the same shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13464" target="_blank">03:44:24.240</a></span> | <span class="t">as the query so um this particular block this particular program that we that will have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13472" target="_blank">03:44:32.080</a></span> | <span class="t">particular block index queue will only work with one block of the queries which will produce only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13477" target="_blank">03:44:37.840</a></span> | <span class="t">one block of the output matrix and we need to select exactly that one so we we can point this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13484" target="_blank">03:44:44.320</a></span> | <span class="t">pointer exactly to the point where we should start writing so let's skip also in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13489" target="_blank">03:44:49.680</a></span> | <span class="t">block index q multiplied by block size q rows so we select exactly the block that our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13501" target="_blank">03:45:01.120</a></span> | <span class="t">our program this particular program will produce when i speak about this particular program i mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13508" target="_blank">03:45:08.960</a></span> | <span class="t">the program that is identified by this program id in the x0 axis and this program id in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13515" target="_blank">03:45:15.200</a></span> | <span class="t">first axis because each of this program will run in parallel hopefully and each of them will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13520" target="_blank">03:45:20.480</a></span> | <span class="t">a different value for the block index q and index batch head okay now we have pointed our pointers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13528" target="_blank">03:45:28.560</a></span> | <span class="t">to the right position where they should either read some information or they should either write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13532" target="_blank">03:45:32.800</a></span> | <span class="t">some information by using make block pointer these pointers can also be treated directly as tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13539" target="_blank">03:45:39.040</a></span> | <span class="t">so that's why we specify the shapes of this tensor because python triton right now provides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13544" target="_blank">03:45:44.160</a></span> | <span class="t">some methods to work directly with blocks of to work directly with the pointers like they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13552" target="_blank">03:45:52.880</a></span> | <span class="t">we are accessing um tensors so we can index them like tensors all right so basically just try it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13561" target="_blank">03:46:01.440</a></span> | <span class="t">on doing some calculation for you based on the strides so you don't have to do it by hand but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13565" target="_blank">03:46:05.760</a></span> | <span class="t">later when we do the backward pass we will avoid using big block pointer and we will see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13570" target="_blank">03:46:10.400</a></span> | <span class="t">indexing done by hand all right um as you know we are processing a single block of queries so let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13580" target="_blank">03:46:20.800</a></span> | <span class="t">let's go back to the algorithm otherwise we we lose the sight of what we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13586" target="_blank">03:46:26.640</a></span> | <span class="t">so let's go here and let's show my ipad all right so as you know each program we will parallelize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13596" target="_blank">03:46:36.720</a></span> | <span class="t">along the query block dimension so each program will work with a different query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13600" target="_blank">03:46:40.560</a></span> | <span class="t">and then we need to do a for loop on all the key and value blocks right now we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13609" target="_blank">03:46:49.200</a></span> | <span class="t">moved our pointers to the right position to select the right query block that we should work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13614" target="_blank">03:46:54.320</a></span> | <span class="t">and to the beginning of the keys and values block that we should work with based on which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13619" target="_blank">03:46:59.840</a></span> | <span class="t">index and which head this particular program should be working with all right now that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13626" target="_blank">03:47:06.240</a></span> | <span class="t">have pointed our pointers to the right position in which our program should be working it inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13631" target="_blank">03:47:11.040</a></span> | <span class="t">of the big pointers that are inside of the big tensors that are the that have the batch dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13639" target="_blank">03:47:19.360</a></span> | <span class="t">the number of heads dimension the sequence length dimension and the heading dimension we have because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13645" target="_blank">03:47:25.920</a></span> | <span class="t">we are pointing to the right batch and we are pointing to the right head these tensors have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13649" target="_blank">03:47:29.840</a></span> | <span class="t">become two-dimensional tensors so they only work on the they are only tensors on the sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13655" target="_blank">03:47:35.760</a></span> | <span class="t">and on the head dimension now we need some more information that we will use later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13662" target="_blank">03:47:42.320</a></span> | <span class="t">the first information that we need is the offsets of each query inside of the current block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13670" target="_blank">03:47:50.640</a></span> | <span class="t">queries that this particular program should be working with and that is given by the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13676" target="_blank">03:47:56.320</a></span> | <span class="t">line so let me copy and paste which is this one so the offsets of the queries are the first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13685" target="_blank">03:48:05.840</a></span> | <span class="t">they are how many of them block size q because each block of queries is made up of a block size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13692" target="_blank">03:48:12.320</a></span> | <span class="t">q number of queries what is each query it's a token and it's on the head dimension is the dim</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13699" target="_blank">03:48:19.920</a></span> | <span class="t">dimension is not the all the embedding of the token but a part of the embedding of each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13706" target="_blank">03:48:26.160</a></span> | <span class="t">which part the part corresponding to the head that this particular program is going to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13712" target="_blank">03:48:32.000</a></span> | <span class="t">so we are generating the offsets that will load this particular number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13716" target="_blank">03:48:36.560</a></span> | <span class="t">this particular queries from the big tensor that contains all queries and we know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13721" target="_blank">03:48:41.840</a></span> | <span class="t">our queries start at the block index q multiplied by block size q position so if this is the program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13728" target="_blank">03:48:48.880</a></span> | <span class="t">number zero they will the imagine block size is equal to four they will be the query with index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13735" target="_blank">03:48:55.120</a></span> | <span class="t">zero one two and three but imagine we are the program number three which means that we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13740" target="_blank">03:49:00.880</a></span> | <span class="t">to skip three multiplied by four so 12 so it will point to the query number 13 14 15 and 16 etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13750" target="_blank">03:49:10.400</a></span> | <span class="t">etc all right and we do the same for the key and values initially the key and values is a range of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13760" target="_blank">03:49:20.160</a></span> | <span class="t">keys and values that we need at each iteration and at the beginning because our pointer for the k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13766" target="_blank">03:49:26.800</a></span> | <span class="t">and v is pointing to the beginning of the sequence of key and value for this particular batch and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13772" target="_blank">03:49:32.480</a></span> | <span class="t">this particular head we are pointing to the first block of key and value so we are not skipping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13777" target="_blank">03:49:37.760</a></span> | <span class="t">anything in the query case we are skipping because our program will only work with one single block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13782" target="_blank">03:49:42.560</a></span> | <span class="t">of queries in this case we don't skip anything because we need to iterate through this key and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13787" target="_blank">03:49:47.280</a></span> | <span class="t">values so we are pointing to the first block of key values so imagine block size kv is equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13793" target="_blank">03:49:53.600</a></span> | <span class="t">four so this stuff here will be equal to zero one two and three all right now we need as you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13802" target="_blank">03:50:02.400</a></span> | <span class="t">inside of the flash attention algorithm we need to compute a block of query multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13807" target="_blank">03:50:07.760</a></span> | <span class="t">transpose of the keys and to each of this block we need to apply the softmax star if you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13813" target="_blank">03:50:13.200</a></span> | <span class="t">what is the softmax star it is the softmax of without the normalization so while computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13820" target="_blank">03:50:20.000</a></span> | <span class="t">the softmax star we also actually compute the normalization factor without applying it and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13825" target="_blank">03:50:25.040</a></span> | <span class="t">apply the normalization factor at the end so for each block of query multiplied by transpose of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13830" target="_blank">03:50:30.480</a></span> | <span class="t">the keys we need to have the maximum for each row in this particular block and the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13836" target="_blank">03:50:36.720</a></span> | <span class="t">factor for each row so that's why why we need these two following statistics which is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13844" target="_blank">03:50:44.080</a></span> | <span class="t">and this is basically a block it's a block of numbers how many based on how many queries we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13852" target="_blank">03:50:52.000</a></span> | <span class="t">have in our block of queries each one initialized with minus infinity just like in my algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13857" target="_blank">03:50:57.600</a></span> | <span class="t">that i showed before so let me go back to the slides in case we forgot or actually you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13863" target="_blank">03:51:03.680</a></span> | <span class="t">also check the flash attention algorithm we initialize it with minus infinities so so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13867" target="_blank">03:51:07.520</a></span> | <span class="t">we are creating this stuff here so we are initializing the mi we are we will be initializing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13871" target="_blank">03:51:11.680</a></span> | <span class="t">the li and we will initializing the o and then we will show the inner loop here and this is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13879" target="_blank">03:51:19.440</a></span> | <span class="t">the algorithm that we have seen before so we initialize m with minus infinities now we initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13885" target="_blank">03:51:25.200</a></span> | <span class="t">also the l's so let me go back to the code all right so the l's are initialized with this number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13897" target="_blank">03:51:37.040</a></span> | <span class="t">here so here in the o blocks as we can see from the flash attention algorithm they are in the o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13904" target="_blank">03:51:44.240</a></span> | <span class="t">block is initialized with zeros so that's why we initialize a block this is the output block that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13909" target="_blank">03:51:49.840</a></span> | <span class="t">this particular program will compute which is based on the position in the batch and the position in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13915" target="_blank">03:51:55.520</a></span> | <span class="t">the index so it is one block of the size of block size q so how many queries there are in this block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13921" target="_blank">03:52:01.200</a></span> | <span class="t">by head dimension which if you want to visualize it let's go back to the slides it is equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13929" target="_blank">03:52:09.440</a></span> | <span class="t">one block of this matrix here so it's one block of the output matrix so one row of blocks one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13939" target="_blank">03:52:19.600</a></span> | <span class="t">block of rows okay so let's go back to the code now all right so now we have initialized a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13949" target="_blank">03:52:29.520</a></span> | <span class="t">stuff here so the output the mi and li where mi is the maximum for each row in this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13956" target="_blank">03:52:36.480</a></span> | <span class="t">query block and li is the normalization factor for each of the items in the query for each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13964" target="_blank">03:52:44.720</a></span> | <span class="t">rows in our query block now we need to do the for loop the inner loop in the flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13973" target="_blank">03:52:53.200</a></span> | <span class="t">algorithm we will create a separate method that will run the inner loop so let's let me copy it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13981" target="_blank">03:53:01.040</a></span> | <span class="t">here and i am following the same structure of the code that you see in the tutorial of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13986" target="_blank">03:53:06.560</a></span> | <span class="t">the triton website so basically if we are running the causal attention or even if we are not running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=13994" target="_blank">03:53:14.480</a></span> | <span class="t">the causal attention we make this for loop and then we will make another for loop and i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14000" target="_blank">03:53:20.160</a></span> | <span class="t">show you why so let me first write it and then we will see so this function here will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14007" target="_blank">03:53:27.280</a></span> | <span class="t">inner loop this inner loop needs to go through all key and value blocks one by one and for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14015" target="_blank">03:53:35.440</a></span> | <span class="t">query and value block it needs to fix the previous calculated block of the the previous softmax star</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14023" target="_blank">03:53:43.440</a></span> | <span class="t">block so basically what we are doing here we will need to create a function as the following where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14030" target="_blank">03:53:50.720</a></span> | <span class="t">we are going to iterate on all the key value block we will need to compute the query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14036" target="_blank">03:53:56.000</a></span> | <span class="t">the transpose of the keys using the query block that is fixed for this program and the key is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14041" target="_blank">03:54:01.360</a></span> | <span class="t">block is the one that we are iterating it through and for each of these queries we need to calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14046" target="_blank">03:54:06.400</a></span> | <span class="t">what is the maximum for each row we need to compute the softmax star so the softmax without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14053" target="_blank">03:54:13.040</a></span> | <span class="t">the normalization factor we need to keep this statistics l which is the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14058" target="_blank">03:54:18.400</a></span> | <span class="t">that we will apply at the end of the iteration of the for loop and at the same time we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14066" target="_blank">03:54:26.080</a></span> | <span class="t">update the output so as you remember the output is p11 multiplied by v1 plus p12 multiplied by v2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14074" target="_blank">03:54:34.080</a></span> | <span class="t">but we need to fix the previous p11 so to fix that we need to every time we sum to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14079" target="_blank">03:54:39.920</a></span> | <span class="t">o to the output we need to fix the output of the previous iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14084" target="_blank">03:54:44.960</a></span> | <span class="t">and then we increase introduce the p and v block of the current iteration so here the author of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14097" target="_blank">03:54:57.040</a></span> | <span class="t">code for the the one that you see on the triton website decided to split this for loop into two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14103" target="_blank">03:55:03.520</a></span> | <span class="t">steps why because in the causal attention we need to when we have a causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14110" target="_blank">03:55:10.560</a></span> | <span class="t">we have a group of we we don't we don't want the query to attend the keys that come after it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14117" target="_blank">03:55:17.360</a></span> | <span class="t">while in the non-causal attention we let all the queries attend to all the keys which also means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14123" target="_blank">03:55:23.920</a></span> | <span class="t">that we will need to have some kind of if statement inside of this if in the side of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14129" target="_blank">03:55:29.120</a></span> | <span class="t">this for loop through all the key and values in which we need to check if this particular query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14135" target="_blank">03:55:35.120</a></span> | <span class="t">that we are working with is comes before or after the key and value in case we are doing the causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14141" target="_blank">03:55:41.360</a></span> | <span class="t">attention so instead of iterating through all the key and values also in the case of the causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14148" target="_blank">03:55:48.160</a></span> | <span class="t">attention by splitting it into two steps we are saying first let's iterate through all the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14156" target="_blank">03:55:56.000</a></span> | <span class="t">and values for which the index is smaller than the current queries block and for this we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14162" target="_blank">03:56:02.960</a></span> | <span class="t">to compute the attention in the case of the causal and non-causal case then for all the elements on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14169" target="_blank">03:56:09.280</a></span> | <span class="t">the right of this block so for which the key index is more than the q index in the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14175" target="_blank">03:56:15.760</a></span> | <span class="t">causal attention we don't need to compute anything because it will be masked out because in the soft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14180" target="_blank">03:56:20.800</a></span> | <span class="t">max it will become zeros so it will not contribute to the output so we don't even have to compute it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14186" target="_blank">03:56:26.320</a></span> | <span class="t">this is why we split this this for loop into two steps so first we iterate to all the parts that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14194" target="_blank">03:56:34.400</a></span> | <span class="t">are left to the diagonal of the query multiplied by the key matrix so for all the values for which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14201" target="_blank">03:56:41.200</a></span> | <span class="t">the query index is less than the key index then we and then we skip all the parts to the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14209" target="_blank">03:56:49.120</a></span> | <span class="t">of this diagonal in case we are working with the causal mask but in case of the non-causal mask we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14213" target="_blank">03:56:53.760</a></span> | <span class="t">compute the left part and the right part of this diagonal all right don't worry when we record</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14219" target="_blank">03:56:59.600</a></span> | <span class="t">this for loop it will be more clear so i just wanted to give a little introduction so let's go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14224" target="_blank">03:57:04.800</a></span> | <span class="t">uh code this inner loop what will this inner loop do it will work with this particular query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14230" target="_blank">03:57:10.400</a></span> | <span class="t">that we have found so this q block it will uh right i don't see the q block because i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14237" target="_blank">03:57:17.520</a></span> | <span class="t">load it well yeah let's load it so we need to load the query block actually we forgot to load it so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14245" target="_blank">03:57:25.120</a></span> | <span class="t">as you remember in triton we load data from the high bandwidth memory to the sram so to the shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14251" target="_blank">03:57:31.680</a></span> | <span class="t">memory by using the load statement and we are telling load the query block that we should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14257" target="_blank">03:57:37.280</a></span> | <span class="t">working with because this pointer q block ptr is already pointing to the right block that we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14263" target="_blank">03:57:43.360</a></span> | <span class="t">be working with so it's already skipping all the blocks that other programs should be working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14268" target="_blank">03:57:48.560</a></span> | <span class="t">and it will load a a tensor of size of block size q head dim so the right block of queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14277" target="_blank">03:57:57.840</a></span> | <span class="t">and we pass it to this inner loop to which we pass the output so where it should write this output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14285" target="_blank">03:58:05.200</a></span> | <span class="t">the li and mi which are the statistics for the rows and for the maximum for each row of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14292" target="_blank">03:58:12.960</a></span> | <span class="t">query and the li which is the normalization factor for each query and the query block this program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14299" target="_blank">03:58:19.360</a></span> | <span class="t">should be working with the beginning of the key and value block pointer because we need to iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14305" target="_blank">03:58:25.200</a></span> | <span class="t">through them so we just point it to the beginning and then inside the for inner for loop we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14310" target="_blank">03:58:30.000</a></span> | <span class="t">iterate through them then the softmax scale that we should use when computing query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14314" target="_blank">03:58:34.880</a></span> | <span class="t">the transpose of the keys the block size so how many queries we have in each block of q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14320" target="_blank">03:58:40.560</a></span> | <span class="t">and how many key and value we have in each block of kv this is a stage that tells us what if we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14328" target="_blank">03:58:48.480</a></span> | <span class="t">on the left side of the diagonal or on the right side of the diagonal so it will tell us if we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14332" target="_blank">03:58:52.800</a></span> | <span class="t">to apply the causal mask or not based on where we are and if we are need to apply the causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14340" target="_blank">03:59:00.160</a></span> | <span class="t">the offset q and the offset kv are just the offsets of the query and key inside of each q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14346" target="_blank">03:59:06.480</a></span> | <span class="t">and kv block which is a list of indices that tells us how many queries we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14354" target="_blank">03:59:14.160</a></span> | <span class="t">and then the sequence length the entire sequence length because in the for loop we need to iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14360" target="_blank">03:59:20.400</a></span> | <span class="t">to all the sequence length block by block so block of kv block of kv block of kv all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14366" target="_blank">03:59:26.800</a></span> | <span class="t">let's write this let's write this method and later we actually need to continue this method again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14372" target="_blank">03:59:32.080</a></span> | <span class="t">so let's go and let me go here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14376" target="_blank">03:59:36.240</a></span> | <span class="t">all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14380" target="_blank">03:59:40.960</a></span> | <span class="t">so this method we have already seen the signature so it's just another kernel so it can be called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14390" target="_blank">03:59:50.640</a></span> | <span class="t">by the first kernel and this is something you can also do in cuda you can actually call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14395" target="_blank">03:59:55.920</a></span> | <span class="t">call one cuda kernel from another cuda kernel and then we based on the stage of this inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14403" target="_blank">04:00:03.680</a></span> | <span class="t">loop we decide what we need to do so when we are using a causal attention so we only want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14412" target="_blank">04:00:12.560</a></span> | <span class="t">apply the attention to the queries for which the index is less than or equal to the key so we all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14420" target="_blank">04:00:20.000</a></span> | <span class="t">want the query to know or attend to key and value that come after it then we pass the value three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14428" target="_blank">04:00:28.240</a></span> | <span class="t">for the stage parameter now when we in the causal case this will become four minus three it is equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14435" target="_blank">04:00:35.760</a></span> | <span class="t">to one so what will happen is that we will only work with the range of keys and values that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14444" target="_blank">04:00:44.240</a></span> | <span class="t">from zero up to the current block of q so all the keys that whose index is less than or less than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14451" target="_blank">04:00:51.600</a></span> | <span class="t">the the index of the queries we are working with so to the left part of the causal mask let me draw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14458" target="_blank">04:00:58.720</a></span> | <span class="t">it otherwise i think it's going to be very difficult to follow so let's do it actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14463" target="_blank">04:01:03.120</a></span> | <span class="t">so let's open a new one and let's go here all right so we have been using this one before so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14471" target="_blank">04:01:11.040</a></span> | <span class="t">we can do it again clear page all right in this now i i want you to think of the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14479" target="_blank">04:01:19.840</a></span> | <span class="t">matrix as a block matrix so let's draw it in pink because i have been drawing it all in pink</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14484" target="_blank">04:01:24.800</a></span> | <span class="t">we know that in the rows of this query multiplied by the transpose of the keys we have a uh the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14491" target="_blank">04:01:31.520</a></span> | <span class="t">queries blocks of queries so we are not watching one single block we are watching all the blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14497" target="_blank">04:01:37.120</a></span> | <span class="t">right now so this is the query block one this is the query block two this is the query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14501" target="_blank">04:01:41.840</a></span> | <span class="t">three this is the query block four each of this query block is made up of multiple tokens of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14506" target="_blank">04:01:46.960</a></span> | <span class="t">queries and then we have the key the key blocks let's do it like this very ugly but okay key one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14517" target="_blank">04:01:57.520</a></span> | <span class="t">key block two key block three key block four when apply calculating the attention when you calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14524" target="_blank">04:02:04.000</a></span> | <span class="t">the causal attention so like with the causal mask you want only the query to attend to keys that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14532" target="_blank">04:02:12.480</a></span> | <span class="t">come before it so when we apply the causal mask this stuff here will be made up of zeros this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14538" target="_blank">04:02:18.240</a></span> | <span class="t">stuff here will be made up of zeros this stuff here will be made up of zeros and this stuff here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14542" target="_blank">04:02:22.480</a></span> | <span class="t">and this stuff here and this stuff here all made up of zeros we never have to mask out anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14548" target="_blank">04:02:28.720</a></span> | <span class="t">when we are in this case because well when we are in this particular scenario actually in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14554" target="_blank">04:02:34.160</a></span> | <span class="t">particular scenario we don't need to mask out anything for sure why because all the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14559" target="_blank">04:02:39.520</a></span> | <span class="t">keys in this block so in this block of keys will have an index that is smaller than the index of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14566" target="_blank">04:02:46.800</a></span> | <span class="t">the corresponding queries in case the the key the block size of the query and the key matches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14572" target="_blank">04:02:52.880</a></span> | <span class="t">so imagine each query is made up of three queries so each block of query is made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14577" target="_blank">04:02:57.920</a></span> | <span class="t">three queries so this is the query number 0 1 and 2 this is the query number 3 4 5 3 4 5 yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14587" target="_blank">04:03:07.600</a></span> | <span class="t">this will be the number 6 7 and 8 and this will be the query number 9 10 and 11 in total we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14595" target="_blank">04:03:15.200</a></span> | <span class="t">12 queries we will have the same indices also for the keys in case we choose the same size for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14601" target="_blank">04:03:21.920</a></span> | <span class="t">blocks so this key block here will be the key number 0 1 and 2 this will be the key number 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14612" target="_blank">04:03:32.400</a></span> | <span class="t">4 5 this will be the 6 6 7 and 8 etc etc now what happens is that in this case as you can see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14622" target="_blank">04:03:42.720</a></span> | <span class="t">key indices of the keys are always smaller than the indices of the queries so we don't need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14628" target="_blank">04:03:48.960</a></span> | <span class="t">mask out anything even in the case of the causal mask because we are sure that in this case all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14634" target="_blank">04:03:54.240</a></span> | <span class="t">of these dot products will never be masked out also in this case all these dot products will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14639" target="_blank">04:03:59.360</a></span> | <span class="t">never be masked out and also in this case will never be masked out will never be masked out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14642" target="_blank">04:04:02.960</a></span> | <span class="t">and will never be masked out and in this case however along the diagonal some of the queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14649" target="_blank">04:04:09.920</a></span> | <span class="t">will be more have will have an index that is bigger than that of the keys and some of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14656" target="_blank">04:04:16.000</a></span> | <span class="t">will not be will not have an index that is bigger than that of the keys because these are blocks of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14662" target="_blank">04:04:22.160</a></span> | <span class="t">queries and blocks of keys some of them need to be masked out and some of them don't need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14666" target="_blank">04:04:26.720</a></span> | <span class="t">masked out so we are dividing our for loop into multiple steps the first step that we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14672" target="_blank">04:04:32.720</a></span> | <span class="t">is all to the left of this diagonal in which we don't need to mask out anything then we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14678" target="_blank">04:04:38.240</a></span> | <span class="t">another step here in which we we need to mask out and then everything to the right of this will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14686" target="_blank">04:04:46.800</a></span> | <span class="t">we will not even compute in the case of causal attention because we already know it's made up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14690" target="_blank">04:04:50.400</a></span> | <span class="t">of zero so it will not compute so the product query multiplied by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14695" target="_blank">04:04:55.280</a></span> | <span class="t">after the softmax will be made up of zeros so if you look at the flash attention algorithm so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14699" target="_blank">04:04:59.840</a></span> | <span class="t">this stuff here the contribution will be zero because we are multiplying zero with v it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14705" target="_blank">04:05:05.920</a></span> | <span class="t">be zero so we don't need to change the output so why even compute this part of the matrix if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14711" target="_blank">04:05:11.200</a></span> | <span class="t">already know it's not going to contribute to the output so we just skip all those iterations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14715" target="_blank">04:05:15.280</a></span> | <span class="t">and this is why we are splitting the for loop i hope now it's much more clear all right so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14722" target="_blank">04:05:22.560</a></span> | <span class="t">go back um okay so uh we are now to the left part of the diagonal in case of the stage number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14729" target="_blank">04:05:29.520</a></span> | <span class="t">in the case of the stage number two it's the part in exactly on the diagonal so in which we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14735" target="_blank">04:05:35.680</a></span> | <span class="t">do some dot products and some other dot products we don't need to do and then for the non-causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14740" target="_blank">04:05:40.560</a></span> | <span class="t">attention we just go through from zero to the sequence length without doing this multi-step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14745" target="_blank">04:05:45.120</a></span> | <span class="t">because we don't need to mask out anything so this is why we have this stage this tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14751" target="_blank">04:05:51.920</a></span> | <span class="t">what is the lower and higher index of the key block that this particular stage should be working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14759" target="_blank">04:05:59.440</a></span> | <span class="t">all right um now this function here multiple of is just telling triton that this number here is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14767" target="_blank">04:06:07.920</a></span> | <span class="t">multiple of this number so triton can make some optimizations so the stage one happens when when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14774" target="_blank">04:06:14.400</a></span> | <span class="t">we are doing a causal attention so stage number three in this function and four minus three will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14781" target="_blank">04:06:21.120</a></span> | <span class="t">become one so imagine we are in the causal attention we will go through the key and value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14786" target="_blank">04:06:26.400</a></span> | <span class="t">block that are to the left of the diagonal with respect to the query block that we are working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14792" target="_blank">04:06:32.320</a></span> | <span class="t">with um in the case we are doing not causal attention in this first call to the inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14800" target="_blank">04:06:40.320</a></span> | <span class="t">function this the stage will be one so the four minus stage will be equal to three so we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14806" target="_blank">04:06:46.960</a></span> | <span class="t">execute this part of the if statement so we will go to all the key and values in case for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14815" target="_blank">04:06:55.120</a></span> | <span class="t">causal attention only as you can see here we will do another iteration here that will only be done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14820" target="_blank">04:07:00.960</a></span> | <span class="t">along the diagonal in which we need to mask out something and we don't need to mask out something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14825" target="_blank">04:07:05.120</a></span> | <span class="t">because inside of each blocks there will be some keys that have the index below the index of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14831" target="_blank">04:07:11.280</a></span> | <span class="t">query and some that have above the index of the query so only in the causal attention we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14836" target="_blank">04:07:16.240</a></span> | <span class="t">call this function twice the first time with the stage equal to one and the second time with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14842" target="_blank">04:07:22.160</a></span> | <span class="t">stage equal to two and the second time we will only iterate through the group of key v blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14849" target="_blank">04:07:29.360</a></span> | <span class="t">that are exactly on the diagonal of the matrix query multiply by transpose of the keys the big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14856" target="_blank">04:07:36.160</a></span> | <span class="t">matrix that is made up of all the blocks all right now that this should be clear let's proceed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14862" target="_blank">04:07:42.320</a></span> | <span class="t">further so let's um because we need to do the for loop the inner for loop of the flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14869" target="_blank">04:07:49.120</a></span> | <span class="t">let's go and load the first blocks of key and values which is exactly the one that the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14875" target="_blank">04:07:55.920</a></span> | <span class="t">and v blocks are currently pointing at which is the 0 0 block so uh we we define the the pointers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14885" target="_blank">04:08:05.200</a></span> | <span class="t">basically um we we we point the key and value blocks to the first uh key and value block that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14892" target="_blank">04:08:12.720</a></span> | <span class="t">this um for loop should be working with which will be based on the stage so if it's the first call to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14899" target="_blank">04:08:19.040</a></span> | <span class="t">this function they will be pointing to the first block in the case of the causal and not causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14905" target="_blank">04:08:25.600</a></span> | <span class="t">if it's the second call to this function which only happens in the case of the causal attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14910" target="_blank">04:08:30.800</a></span> | <span class="t">they will be pointing exactly to the key and value block to the diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14914" target="_blank">04:08:34.400</a></span> | <span class="t">all right then we need to make the for loop so let's loop over all the for loop so let's do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14924" target="_blank">04:08:44.640</a></span> | <span class="t">so loop over the key and value and what we do is um okay we we let the compiler know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14932" target="_blank">04:08:52.960</a></span> | <span class="t">this number here the start kv will always be a multiple of the block size kv because we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14938" target="_blank">04:08:58.400</a></span> | <span class="t">moving from one kv block to the next kv block block by block so we let the compiler know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14944" target="_blank">04:09:04.320</a></span> | <span class="t">this number here start kv is a multiple of block size kv it doesn't change anything from a logic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14948" target="_blank">04:09:08.880</a></span> | <span class="t">point of view we are just telling giving some hint to the compiler so it can do some other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14953" target="_blank">04:09:13.280</a></span> | <span class="t">optimization that triton does now the first thing that we see in the flash attention algorithm is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14960" target="_blank">04:09:20.880</a></span> | <span class="t">we need to compute the product of the query so this is the particular block of the query that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14965" target="_blank">04:09:25.600</a></span> | <span class="t">we are working with with the current kv block in this iteration so let's do it so we compute k and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14973" target="_blank">04:09:33.200</a></span> | <span class="t">b so we load the the query have already been loaded by the caller of this function we have loaded it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14980" target="_blank">04:09:40.480</a></span> | <span class="t">here here we have already loaded the query but we need to load the current block of k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14986" target="_blank">04:09:46.880</a></span> | <span class="t">so we load the current block of k indicated by the k pointer and we multi we do the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=14994" target="_blank">04:09:54.720</a></span> | <span class="t">multiplication of the current block of query the the block of query with the current block of k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15000" target="_blank">04:10:00.480</a></span> | <span class="t">which is already transposed because when we loaded this k k when we defined the k block pointer we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15008" target="_blank">04:10:08.640</a></span> | <span class="t">defined it already with the stride changed so we are reading the tensor already transposed so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15014" target="_blank">04:10:14.800</a></span> | <span class="t">are doing the query multiplied by the transpose of the keys basically okay now let's do here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15022" target="_blank">04:10:22.560</a></span> | <span class="t">this part here basically saying okay if the stage is two when the stage is two is when we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15030" target="_blank">04:10:30.560</a></span> | <span class="t">exactly on the diagonal we know that some of the queries will have an index that is bigger than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15035" target="_blank">04:10:35.760</a></span> | <span class="t">that of the keys and some of them we have an index that is smaller than that of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15040" target="_blank">04:10:40.080</a></span> | <span class="t">so we need to apply the causal mask only in this case so basically what we do is we define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15046" target="_blank">04:10:46.960</a></span> | <span class="t">the mask that we should be applying so the mask will mask out all the values for which this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15053" target="_blank">04:10:53.600</a></span> | <span class="t">mask is not true so when this mask is true when the index of the query is more than the index of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15060" target="_blank">04:11:00.960</a></span> | <span class="t">the k and v's and we okay we apply the softmax scale so as you remember we here we only computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15069" target="_blank">04:11:09.440</a></span> | <span class="t">query multiplied by transpose of the keys but we also need to divide by the square root of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15073" target="_blank">04:11:13.600</a></span> | <span class="t">head dimension and we do it here and then we because we already computed the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15082" target="_blank">04:11:22.960</a></span> | <span class="t">the product we can calculate the maximum for each row and then we we we subtract because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15091" target="_blank">04:11:31.440</a></span> | <span class="t">when later in the flash attention algorithm we have another operation which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15096" target="_blank">04:11:36.080</a></span> | <span class="t">which i call the softmax star and as you remember the softmax star needs to do each row my each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15106" target="_blank">04:11:46.560</a></span> | <span class="t">element of the s matrix so the query multiplied by the transpose of the keys minus the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15111" target="_blank">04:11:51.600</a></span> | <span class="t">for each row so we can already compute the maximum for each row and we can also before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15118" target="_blank">04:11:58.640</a></span> | <span class="t">computing the maximum for each row we need to mask out all the elements that will be masked out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15124" target="_blank">04:12:04.000</a></span> | <span class="t">in the stage number two which is along the diagonal and how to mask out we just replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15130" target="_blank">04:12:10.960</a></span> | <span class="t">with minus infinity before applying the softmax all the values for which the mask is false</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15135" target="_blank">04:12:15.840</a></span> | <span class="t">so right now we are we have computed what we have computed the query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15140" target="_blank">04:12:20.960</a></span> | <span class="t">transpose of the keys we have masked out in case we need to mask and when we need to mask only when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15145" target="_blank">04:12:25.840</a></span> | <span class="t">we are along the diagonal in all the other cases we don't need to mask out anything we just multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15150" target="_blank">04:12:30.480</a></span> | <span class="t">by the softmax scale and then we we subtract the mij the mij is the maximum value for each row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15159" target="_blank">04:12:39.120</a></span> | <span class="t">because we need to compute the softmax star operation which is the softmax without the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15163" target="_blank">04:12:43.280</a></span> | <span class="t">normalization which in the flash attention algorithm is exactly this operation which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15167" target="_blank">04:12:47.520</a></span> | <span class="t">will produce the pij okay so let's go here so now we can compute the pij block which is this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15176" target="_blank">04:12:56.080</a></span> | <span class="t">here which is the exponential of the query kv block variable here which have already subtracted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15183" target="_blank">04:13:03.920</a></span> | <span class="t">the m so we have already subtracted this mi at the previous instruction so now we can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15191" target="_blank">04:13:11.680</a></span> | <span class="t">apply the exponential and this is what we are doing here okay then we need to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15200" target="_blank">04:13:20.000</a></span> | <span class="t">sum of the the rows for the before the normalization factor so for the current block we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15209" target="_blank">04:13:29.760</a></span> | <span class="t">have a list of we have we have the pij block for the current kv block to compute the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15218" target="_blank">04:13:38.960</a></span> | <span class="t">factor for the softmax we need to keep summing up these exponentials and later we will fix the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15225" target="_blank">04:13:45.200</a></span> | <span class="t">exponentials the the normalization factor that we computed at the previous step but we will do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15232" target="_blank">04:13:52.080</a></span> | <span class="t">later so now we just computed the normalization factor for the current block which is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15236" target="_blank">04:13:56.560</a></span> | <span class="t">sum of all the values on a single row which is the same as what we did before here as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15244" target="_blank">04:14:04.400</a></span> | <span class="t">here when i show you the algorithm so for each block we do the row sum as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15253" target="_blank">04:14:13.440</a></span> | <span class="t">of the p matrix what is the p matrix is the exponential of the s minus m and for now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15261" target="_blank">04:14:21.280</a></span> | <span class="t">didn't apply the the correction to the previous block that's it so we computed the lij for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15268" target="_blank">04:14:28.240</a></span> | <span class="t">current k and v block and then we compute the correction factor for the previous block so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15274" target="_blank">04:14:34.400</a></span> | <span class="t">correction factor for the previous block if you remember the formula from the paper is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15279" target="_blank">04:14:39.200</a></span> | <span class="t">is the exponential of the previous estimate of the maximum minus the current estimate of the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15284" target="_blank">04:14:44.160</a></span> | <span class="t">which is exactly this one so the previous estimate of the maximum minus the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15287" target="_blank">04:14:47.760</a></span> | <span class="t">estimates of the maximum we will see later why mi is the previous estimate of the maximum and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15294" target="_blank">04:14:54.000</a></span> | <span class="t">mij is the current estimate of the maximum because it is coming from the current block that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15298" target="_blank">04:14:58.640</a></span> | <span class="t">computing mi is the let's say the the one that it is the the one of the previous iteration because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15306" target="_blank">04:15:06.480</a></span> | <span class="t">later we will override mi with mij but i'm just following the flash attention algorithm so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15313" target="_blank">04:15:13.840</a></span> | <span class="t">so i am computing the correction factor of the previous li which in the flash attention algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15319" target="_blank">04:15:19.600</a></span> | <span class="t">is let me show you this stuff here so it is this stuff here this one here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15327" target="_blank">04:15:27.440</a></span> | <span class="t">okay and then we apply it so apply the correction factor so we apply it so we apply the previous li</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15339" target="_blank">04:15:39.440</a></span> | <span class="t">with the correction factor plus the current li which is the one coming from the current p block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15344" target="_blank">04:15:44.480</a></span> | <span class="t">the one that will be computed with the current k and v with the current iteration and right now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15350" target="_blank">04:15:50.320</a></span> | <span class="t">are doing this operation so li is equal to the previous li multiplied by the correction factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15355" target="_blank">04:15:55.520</a></span> | <span class="t">all right and then what we need to do okay we need to as you remember the formula is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15362" target="_blank">04:16:02.240</a></span> | <span class="t">we calculate the p block and then we need to multiply by the v block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15367" target="_blank">04:16:07.520</a></span> | <span class="t">so we need to load the v block so let's load it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15373" target="_blank">04:16:13.680</a></span> | <span class="t">we load the v block based on the pointer of the v block to which this um to to which the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15380" target="_blank">04:16:20.240</a></span> | <span class="t">pointer v is is pointing to at the beginning of this iteration in case we are in stage number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15386" target="_blank">04:16:26.400</a></span> | <span class="t">three so in case we are doing for example not causal attention this will be pointing to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15390" target="_blank">04:16:30.400</a></span> | <span class="t">first k v block v block and then okay here there is just a type conversion so we make sure this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15399" target="_blank">04:16:39.760</a></span> | <span class="t">in floating point 16 and then we compute the output block so we are computing the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15408" target="_blank">04:16:48.160</a></span> | <span class="t">so we just take v p multiplied by v and we add it to the output and this is what we are doing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15414" target="_blank">04:16:54.960</a></span> | <span class="t">we take p we multiply it by v and add it to the o block let's go actually to this line one by one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15422" target="_blank">04:17:02.800</a></span> | <span class="t">so first of all we need to fix the previous output block with the correction factor correction factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15427" target="_blank">04:17:07.920</a></span> | <span class="t">that we have here so we can fix the previous block with this alpha term here which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15433" target="_blank">04:17:13.600</a></span> | <span class="t">correction factor for the previous block and so we just fix the previous block for now but we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15439" target="_blank">04:17:19.760</a></span> | <span class="t">didn't add the new pv so to add the new pv we do the dot product of p and v and this third argument</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15446" target="_blank">04:17:26.160</a></span> | <span class="t">tells the dot this not dot product it's actually the matrix multiplication tell this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15451" target="_blank">04:17:31.840</a></span> | <span class="t">multiplication to use this element here as the accumulator so this is exactly the same as doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15458" target="_blank">04:17:38.080</a></span> | <span class="t">p block multiplied by the v block o block plus equal to p block multiplied by the v block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15467" target="_blank">04:17:47.920</a></span> | <span class="t">this is just optimized because anyway this dot function here needs some place where to store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15475" target="_blank">04:17:55.040</a></span> | <span class="t">the intermediate results so why not just store it where it should actually go and because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15481" target="_blank">04:18:01.600</a></span> | <span class="t">the dot the the matrix multiplication is just a dot product and the dot product is just a repeated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15487" target="_blank">04:18:07.120</a></span> | <span class="t">sum this accumulator will be will this dot will keep summing the result to this block here which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15496" target="_blank">04:18:16.000</a></span> | <span class="t">will exactly result in this instruction like we have done the matrix multiplication separately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15502" target="_blank">04:18:22.960</a></span> | <span class="t">and we added it to the o block so this is uh that's why this argument is called the accumulator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15510" target="_blank">04:18:30.080</a></span> | <span class="t">okay all right so we have also computed the output and then we save the new estimation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15517" target="_blank">04:18:37.440</a></span> | <span class="t">the maximum for the current iteration and it becomes mi so at the next iteration we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15523" target="_blank">04:18:43.440</a></span> | <span class="t">it to calculate the correction factor and then we have finished for the current block and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15529" target="_blank">04:18:49.760</a></span> | <span class="t">can move on to the next block so we advance our k and v pointers by one block of k and v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15537" target="_blank">04:18:57.840</a></span> | <span class="t">we advance it differently because we know that the v block is a pointer to a tensor of shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15545" target="_blank">04:19:05.280</a></span> | <span class="t">let me write it here this is a tensor of shape sequence length head dim so we need to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15555" target="_blank">04:19:15.040</a></span> | <span class="t">the sequence length by one kv the block size kv while the k block is actually the k transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15563" target="_blank">04:19:23.200</a></span> | <span class="t">block so we need to and it is a transpose because we have exchanged the strides and the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15568" target="_blank">04:19:28.720</a></span> | <span class="t">so it is head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15571" target="_blank">04:19:31.120</a></span> | <span class="t">head dimension sequence length so we don't change the head dimension we just advance the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15579" target="_blank">04:19:39.360</a></span> | <span class="t">length by sequence block size kv so basically we are just going to point to the next block of k and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15585" target="_blank">04:19:45.920</a></span> | <span class="t">to the next block of v i hope you were able to follow the algorithm of flash attention i try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15592" target="_blank">04:19:52.720</a></span> | <span class="t">use the same names i try to use the more or less the same logic and always writing the formula that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15597" target="_blank">04:19:57.280</a></span> | <span class="t">i'm referring to so hopefully you didn't get lost i think the only difference that there is between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15602" target="_blank">04:20:02.400</a></span> | <span class="t">the flash attention algorithm as written on the paper and this code is probably this alpha which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15606" target="_blank">04:20:06.800</a></span> | <span class="t">is the correction factor but i hope it's easily understandable anyway then we just return the o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15613" target="_blank">04:20:13.440</a></span> | <span class="t">block so o block li which is the the normalization factor for each row in the current output block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15624" target="_blank">04:20:24.480</a></span> | <span class="t">which is also a q block because we are working with one q block independently from the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15630" target="_blank">04:20:30.000</a></span> | <span class="t">programs and mi is the maximum value for each row which will be needed for the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15637" target="_blank">04:20:37.280</a></span> | <span class="t">because when in the backward pass we will compute the qquery multiplied by transpose of the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15642" target="_blank">04:20:42.480</a></span> | <span class="t">block on the fly we need to also apply the softmax but instead of re-computing the stuff which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15647" target="_blank">04:20:47.680</a></span> | <span class="t">already computed during the forward pass we just save them and reuse them during the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15652" target="_blank">04:20:52.480</a></span> | <span class="t">which will save us some computation now i know it's time to talk about the log sum x trick because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15659" target="_blank">04:20:59.200</a></span> | <span class="t">we are going to use it so let's go back to the old method so let's go here all right so we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15666" target="_blank">04:21:06.640</a></span> | <span class="t">computed two calls of this function in case we are working with causal attention in case of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15672" target="_blank">04:21:12.880</a></span> | <span class="t">we are computing causal attention we call this function once to work with all the query blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15677" target="_blank">04:21:17.520</a></span> | <span class="t">that are to the left side of the diagonal of the query key matrix then we do another call of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15683" target="_blank">04:21:23.280</a></span> | <span class="t">function to work only with those blocks of keys that exactly lie on the diagonal of the query key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15690" target="_blank">04:21:30.800</a></span> | <span class="t">matrix because in this case some of the values need to be masked out and some of them do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15697" target="_blank">04:21:37.520</a></span> | <span class="t">need to be masked out moreover by doing this we can avoid computing the dot products for all those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15704" target="_blank">04:21:44.400</a></span> | <span class="t">values in the causal math in the causal case for which the key is index of the key is higher than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15711" target="_blank">04:21:51.760</a></span> | <span class="t">the index of the query saving some computation because anyway they will be resulting after the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15716" target="_blank">04:21:56.560</a></span> | <span class="t">softmax in zeros and they will not contribute to the output so it should be faster okay now let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15723" target="_blank">04:22:03.520</a></span> | <span class="t">go back to the this method here so calling method and there is one last thing that we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15728" target="_blank">04:22:08.480</a></span> | <span class="t">which is we need to compute the log sum exp and now i will show you what is it so in order for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15736" target="_blank">04:22:16.320</a></span> | <span class="t">the backward pass to recompute the softmax without having to recalculate the normalization factor and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15741" target="_blank">04:22:21.760</a></span> | <span class="t">the maximum value for each row we should be actually saving two different stuff one is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15746" target="_blank">04:22:26.640</a></span> | <span class="t">maximum for each row in the query block and one is the normalization factor for each query in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15752" target="_blank">04:22:32.400</a></span> | <span class="t">query block however there is a trick and the trick is okay it's not really called log sum exp trick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15758" target="_blank">04:22:38.960</a></span> | <span class="t">because the log sum exp trick is used for another purpose but let's call it log sum exp trick number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15764" target="_blank">04:22:44.720</a></span> | <span class="t">two so the log sum exp trick number two is something like this so let me open the slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15772" target="_blank">04:22:52.640</a></span> | <span class="t">so when we do query multiply that transpose of the keys we get a matrix that is made up of dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15779" target="_blank">04:22:59.680</a></span> | <span class="t">products so something like this like this is one dot product so let's call it query one transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15785" target="_blank">04:23:05.920</a></span> | <span class="t">the key one query one transpose the key two this is a query two transpose the key one and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15793" target="_blank">04:23:13.200</a></span> | <span class="t">a query two transpose the key two then we need to apply the softmax right so the softmax is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15800" target="_blank">04:23:20.480</a></span> | <span class="t">is the let's write the formula of the softmaxes for each of these vectors so this is a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15805" target="_blank">04:23:25.040</a></span> | <span class="t">and this is a vector because we applied it by rows for each of these vectors this will modify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15810" target="_blank">04:23:30.080</a></span> | <span class="t">element wise each element as follows so the softmax of x i is equal to the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15817" target="_blank">04:23:37.040</a></span> | <span class="t">of x i minus oh my god i didn't leave enough space so let's move this stuff here back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15824" target="_blank">04:23:44.000</a></span> | <span class="t">and this stuff here please left all right it will be the softmax of the exponential of each element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15832" target="_blank">04:23:52.960</a></span> | <span class="t">minus the maximum for the current vector to which we are applying the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15837" target="_blank">04:23:57.600</a></span> | <span class="t">divided by the normalization factor which is the summation over all possible j's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15845" target="_blank">04:24:05.360</a></span> | <span class="t">where n in this case is equal to 2 because we have each vector is made up of two elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15849" target="_blank">04:24:09.280</a></span> | <span class="t">of the exponential of x i minus x max now imagine we already have x max and we already have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15861" target="_blank">04:24:21.200</a></span> | <span class="t">summation in the flash attention algorithm in the forward pass this stuff here is called l i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15866" target="_blank">04:24:26.240</a></span> | <span class="t">and this stuff here is called m i what we are going to save in the code you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15872" target="_blank">04:24:32.880</a></span> | <span class="t">we are saving actually not m i and l i separately we will be saving m i plus the logarithm of l i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15881" target="_blank">04:24:41.680</a></span> | <span class="t">so we are going to save m i plus the log of l i so what will happen is that when we will compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15894" target="_blank">04:24:54.240</a></span> | <span class="t">compute the backward pass we need to recreate this matrix here on the fly which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15899" target="_blank">04:24:59.680</a></span> | <span class="t">we need to recompute the query multiply by the transpose of the keys and we to and then we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15906" target="_blank">04:25:06.080</a></span> | <span class="t">apply the softmax to apply the softmax we should need this stuff and this stuff here but we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15911" target="_blank">04:25:11.200</a></span> | <span class="t">only this stuff here so this is the m i plus the logarithm of l i so when we're computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15917" target="_blank">04:25:17.040</a></span> | <span class="t">softmax we will compute the following so we will compute the softmax as follows we will define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15923" target="_blank">04:25:23.600</a></span> | <span class="t">let's call it a new softmax so let me use another color here we will apply the softmax as follows so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15933" target="_blank">04:25:33.680</a></span> | <span class="t">softmax of x i let's call it the softmax 2 because it's a i don't want to confuse softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15946" target="_blank">04:25:46.880</a></span> | <span class="t">is equal to the exponential of each element minus we will subtract this value here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15954" target="_blank">04:25:54.400</a></span> | <span class="t">the one corresponding to the current row to which we are applying the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15958" target="_blank">04:25:58.400</a></span> | <span class="t">so it will be the exponential of x i minus m i minus the log of l i if we expand this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15969" target="_blank">04:26:09.040</a></span> | <span class="t">this will become the exponential of because the exponential the sum of two exponential of the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15976" target="_blank">04:26:16.800</a></span> | <span class="t">is equal to the product of the two exponentials we can also write it like this so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15981" target="_blank">04:26:21.040</a></span> | <span class="t">the exponential of x i minus m i divided by the exponential of the log of l i which guess what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=15996" target="_blank">04:26:36.800</a></span> | <span class="t">it is equal to the exponential of x i minus m i divided by l i which is exactly the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16005" target="_blank">04:26:45.440</a></span> | <span class="t">factor and we also have m i so instead of saving two values we save only one value and when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16011" target="_blank">04:26:51.040</a></span> | <span class="t">apply it the exponential's properties will take care of actually also normalizing each value to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16016" target="_blank">04:26:56.080</a></span> | <span class="t">which we apply it if you don't remember the properties of the exponential it is very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16021" target="_blank">04:27:01.040</a></span> | <span class="t">so the exponential of a plus b is equal to the exponential of a multiplied by the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16030" target="_blank">04:27:10.320</a></span> | <span class="t">of b and the exponential of a not exponential it's the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16038" target="_blank">04:27:18.080</a></span> | <span class="t">a minus b is equal to the exponential of a divided by the exponential of b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16048" target="_blank">04:27:28.080</a></span> | <span class="t">and this is the the trick that we're using so that's why we don't need to save two different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16052" target="_blank">04:27:32.000</a></span> | <span class="t">values we just need to save one value and then when we apply it it will automatically be taken</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16056" target="_blank">04:27:36.800</a></span> | <span class="t">care will take care of normalizing because of the properties of the exponential all right let's move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16062" target="_blank">04:27:42.560</a></span> | <span class="t">forward so we have also created this value that we will use during the backward pass now as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16071" target="_blank">04:27:51.280</a></span> | <span class="t">remember in the flash attrition algorithm we don't normalize each block while computing it we normalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16077" target="_blank">04:27:57.200</a></span> | <span class="t">the output at the end and this is exactly what we are going to do here so we normalize the block at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16084" target="_blank">04:28:04.000</a></span> | <span class="t">the end after we have computed all the normalization factors that we need for all the rows that belong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16088" target="_blank">04:28:08.560</a></span> | <span class="t">to the current output block we save this m i so we save it this m i is what is the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16097" target="_blank">04:28:17.920</a></span> | <span class="t">factor and the maximum for each row that we will need for the backward pass so we need to save it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16104" target="_blank">04:28:24.640</a></span> | <span class="t">in a tensor that we will use during the backward pass so we need to understand which tensor is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16109" target="_blank">04:28:29.920</a></span> | <span class="t">and it's the tensor that we called m which is a tensor of a batch size num heads and sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16115" target="_blank">04:28:35.440</a></span> | <span class="t">length dimensions so we need to select the right point in this tensor to select to where we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16122" target="_blank">04:28:42.160</a></span> | <span class="t">save this m i values so we need to select the right batch size index and the right number of head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16129" target="_blank">04:28:49.120</a></span> | <span class="t">index so we advance this pointer by the following offset which is m plus the index batch head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16141" target="_blank">04:29:01.520</a></span> | <span class="t">because each index okay the index batch head is what is the index of the current program that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16149" target="_blank">04:29:09.520</a></span> | <span class="t">includes information about which head we are working with and which batch we are working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16154" target="_blank">04:29:14.640</a></span> | <span class="t">because each of this for each batch and for each head we have a sequence length we can skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16163" target="_blank">04:29:23.440</a></span> | <span class="t">a number of sequence length based on which index is okay what we are doing is basically we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16172" target="_blank">04:29:32.240</a></span> | <span class="t">skipping for each batch and for each head we will have a sequence length because each token in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16182" target="_blank">04:29:42.800</a></span> | <span class="t">sequence has a maximum value and each token in the sequence will have normalization value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16187" target="_blank">04:29:47.520</a></span> | <span class="t">so based on the current combination of batch and head we can skip a number of sequence length that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16194" target="_blank">04:29:54.240</a></span> | <span class="t">other programs will process so because in this tensor we have the sequence length as the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16201" target="_blank">04:30:01.440</a></span> | <span class="t">dimension and we have what is the combined index of the batch size and number of head size we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16208" target="_blank">04:30:08.160</a></span> | <span class="t">skip a number of sequence of length based on the combined index which is given by the program index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16213" target="_blank">04:30:13.760</a></span> | <span class="t">number one which is the index batch head that we have here and this is why we skip here a sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16220" target="_blank">04:30:20.240</a></span> | <span class="t">length number multiplied by the index batch head this m is pointing to the first element of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16229" target="_blank">04:30:29.600</a></span> | <span class="t">entire tensor so we are skipping the heads and the batch based on the combined index index batch head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16237" target="_blank">04:30:37.920</a></span> | <span class="t">that this particular program is working with and then we have off skew off skew is because each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16243" target="_blank">04:30:43.920</a></span> | <span class="t">of these kernels the attention forward method will work with one query block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16252" target="_blank">04:30:52.240</a></span> | <span class="t">each query block has some indices for the exact queries it includes and this is given by off skew</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16261" target="_blank">04:31:01.600</a></span> | <span class="t">variable that you can see here which is how many blocks of queries we need to skip because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16266" target="_blank">04:31:06.560</a></span> | <span class="t">will be processed by other programs plus the range of queries that this particular that not this that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16272" target="_blank">04:31:12.800</a></span> | <span class="t">a particular block of queries has so imagine this particular program is working with the queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16280" target="_blank">04:31:20.080</a></span> | <span class="t">that go from i don't know from 12 to 16 then this will be 12 13 14 15 so the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16289" target="_blank">04:31:29.280</a></span> | <span class="t">and the maximum value for each row we only have that for the disk for this indices of query queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16296" target="_blank">04:31:36.960</a></span> | <span class="t">so 12 13 14 and 15 and that's why we need to also skip the number of queries that this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16302" target="_blank">04:31:42.880</a></span> | <span class="t">program works with which is already included in this offset of skew variable all right so now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16310" target="_blank">04:31:50.480</a></span> | <span class="t">can store the mi so because we have the pointer to which where it should be saved and we can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16316" target="_blank">04:31:56.400</a></span> | <span class="t">store the output which was computed of by our inner for loop and this guys is the forward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16323" target="_blank">04:32:03.520</a></span> | <span class="t">of the attention flash attention now we should go forward which is we should compute the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16331" target="_blank">04:32:11.600</a></span> | <span class="t">pass we also have all the ingredients for computing the backward pass because we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16335" target="_blank">04:32:15.520</a></span> | <span class="t">already seen this trick which is the log sum x trick so we already know what um how to use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16342" target="_blank">04:32:22.000</a></span> | <span class="t">to compute the query key block during the backward pass on the fly what we miss to understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16347" target="_blank">04:32:27.840</a></span> | <span class="t">backward pass well we need to understand what is the first of all what is the backward pass why do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16352" target="_blank">04:32:32.160</a></span> | <span class="t">we even need a backward pass we need to understand what is the autograd of pytorch how does it work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16357" target="_blank">04:32:37.680</a></span> | <span class="t">how to compute the gradient what is the gradient how to compute do we need to what is the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16363" target="_blank">04:32:43.200</a></span> | <span class="t">when computing the gradient on the backward pass do we even need to compute that so we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16367" target="_blank">04:32:47.520</a></span> | <span class="t">derive all the formulas of the backward pass by hand so if you are in for the challenge let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16372" target="_blank">04:32:52.240</a></span> | <span class="t">continue all right so now before looking at the flash attentions backward pass at the algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16379" target="_blank">04:32:59.920</a></span> | <span class="t">we need to understand why we even need a backward pass and to understand why we even need a backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16384" target="_blank">04:33:04.320</a></span> | <span class="t">pass so before looking at the autograd of pytorch we should be looking at what is what are derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16390" target="_blank">04:33:10.480</a></span> | <span class="t">what are gradients what are jacobians so that when we talk about derivatives gradients and jacobians</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16394" target="_blank">04:33:14.480</a></span> | <span class="t">we don't feel lost so i will do a very fast let's say rehearsal of what these topics are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16402" target="_blank">04:33:22.240</a></span> | <span class="t">now what is the derivative when you have a function that takes as input a real value and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16407" target="_blank">04:33:27.840</a></span> | <span class="t">outputs a real value we talk about derivatives which is defined as follows the derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16413" target="_blank">04:33:33.680</a></span> | <span class="t">the function with respect to its variable x is defined as the limit for a step size that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16419" target="_blank">04:33:39.520</a></span> | <span class="t">goes to zero of the function evaluated at x plus h so x plus the step size minus f evaluated at the x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16428" target="_blank">04:33:48.240</a></span> | <span class="t">at x divided by the step size so intuitively we are saying is the ratio of how much the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16436" target="_blank">04:33:56.160</a></span> | <span class="t">change for a small change for how much the input has changed in the function that this also gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16442" target="_blank">04:34:02.400</a></span> | <span class="t">you the intuitive intuition of why the gradient is the derivative is also the tells you the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16451" target="_blank">04:34:11.600</a></span> | <span class="t">inclination of the tangent line of the to the function at the point in which it's evaluated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16456" target="_blank">04:34:16.880</a></span> | <span class="t">i will use also the following notation to denote the derivative so the derivative i am used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16464" target="_blank">04:34:24.960</a></span> | <span class="t">write it as like this so f prime of x but it's also possible to write it as a d of f of x with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16470" target="_blank">04:34:30.720</a></span> | <span class="t">respect to the x or d of y where y is the output of the function with respect to x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16475" target="_blank">04:34:35.600</a></span> | <span class="t">and they are all equal to the same thing which is the definition above if we invert this formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16481" target="_blank">04:34:41.760</a></span> | <span class="t">here and we take h to the left side we can also write the follows so if we want to evaluate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16489" target="_blank">04:34:49.200</a></span> | <span class="t">function at the position x plus h we can also evaluate it as f prime of h so the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16500" target="_blank">04:35:00.000</a></span> | <span class="t">of the function in the point x multiplied by h which is the step size plus f of x this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16507" target="_blank">04:35:07.280</a></span> | <span class="t">actually also how we derive the Euler rule for computing the differential equations but that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16512" target="_blank">04:35:12.960</a></span> | <span class="t">not the topic of today so this h we can also call it delta x so f of x plus delta x is more or less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16521" target="_blank">04:35:21.200</a></span> | <span class="t">because here we have a limit that says when this only happens when h is very very very small so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16525" target="_blank">04:35:25.600</a></span> | <span class="t">that's why we put this more or less approximately so f of x plus delta x is more or less equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16533" target="_blank">04:35:33.280</a></span> | <span class="t">f prime of x multiplied by delta x plus f of x this you can also read it as follows that if by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16541" target="_blank">04:35:41.440</a></span> | <span class="t">inverting this formula if x changes by a little amount and this little amount is delta x how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16552" target="_blank">04:35:52.320</a></span> | <span class="t">y will change? y will change by this exact amount which is the derivative of y with respect to x so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16560" target="_blank">04:36:00.240</a></span> | <span class="t">dy with respect to dx multiplied by how much x has changed so this dy dx tells us how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16568" target="_blank">04:36:08.320</a></span> | <span class="t">y will change with a small change of x if we multiply with the actual change of x it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16574" target="_blank">04:36:14.560</a></span> | <span class="t">tell us how exactly y will be affected i don't want to use stay too much on this but i would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16583" target="_blank">04:36:23.520</a></span> | <span class="t">like to use this intuition to introduce the chain rule because imagine we have a function of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16591" target="_blank">04:36:31.600</a></span> | <span class="t">function so imagine we have z is equal to f of g of x we can think of x being mapped into a variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16599" target="_blank">04:36:39.600</a></span> | <span class="t">y through the function g and then y being mapped into a variable z through the function f if x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16606" target="_blank">04:36:46.880</a></span> | <span class="t">changes by a little bit and by a little bit i mean delta x how much y will change? well y will change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16613" target="_blank">04:36:53.200</a></span> | <span class="t">by delta y what is delta y? delta y is the derivative of y with respect to x multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16618" target="_blank">04:36:58.400</a></span> | <span class="t">by the step size of x but if y changes it will also affect z because there is a direct mapping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16625" target="_blank">04:37:05.920</a></span> | <span class="t">between y and z so how much z will change for a small change in y? let's see so if y changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16633" target="_blank">04:37:13.360</a></span> | <span class="t">from the old y by a small step delta y then z will also change by some delta z and this delta z</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16641" target="_blank">04:37:21.520</a></span> | <span class="t">is the dz on dy multiplied by delta y if we replace this delta y with the delta y that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16649" target="_blank">04:37:29.840</a></span> | <span class="t">have computed in the expression above we arrive to the chain rule it will tell us how z will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16656" target="_blank">04:37:36.800</a></span> | <span class="t">affected so this is delta z what is the effect on z for a small change on x and it's the product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16665" target="_blank">04:37:45.600</a></span> | <span class="t">of the two derivatives the one with the of y with respect to s and one z with respect to y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16672" target="_blank">04:37:52.240</a></span> | <span class="t">and this is the chain rule that we study in high school so it is if you want to compute dz on dx</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16679" target="_blank">04:37:59.440</a></span> | <span class="t">it is dz on dy multiplied by dy dx which is very intuitive if you think about the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16687" target="_blank">04:38:07.360</a></span> | <span class="t">example so you can think of z as the price of cars and x as the price of the oil how much will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16696" target="_blank">04:38:16.880</a></span> | <span class="t">a small change in the price of oil affect the price of a car? well this small change in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16703" target="_blank">04:38:23.200</a></span> | <span class="t">price of the oil will affect for example a variable y which could be the price of electricity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16710" target="_blank">04:38:30.000</a></span> | <span class="t">so if how much the price of electricity will affect the price of a car it's through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16717" target="_blank">04:38:37.360</a></span> | <span class="t">derivative of the price of the electricity with respect to the the price of the car with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16722" target="_blank">04:38:42.080</a></span> | <span class="t">to the electricity so to get the effect of the price of oil on the price of the car we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16729" target="_blank">04:38:49.280</a></span> | <span class="t">multiply the two effects and this is the intuition behind the chain rule anyway let's talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16735" target="_blank">04:38:55.200</a></span> | <span class="t">gradients so when we have a function that as input takes a vector and produces a scalar we talk not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16742" target="_blank">04:39:02.320</a></span> | <span class="t">anymore about derivatives we talk about gradients so imagine we have a function that takes as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16749" target="_blank">04:39:09.280</a></span> | <span class="t">a vector made up of two dimensions but n dimension in general and it produces a scalar when do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16755" target="_blank">04:39:15.680</a></span> | <span class="t">have to deal with this kind of function for example loss functions loss functions are something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16761" target="_blank">04:39:21.200</a></span> | <span class="t">are always a scalar as output and as input they take tensors so for example imagine the cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16767" target="_blank">04:39:27.760</a></span> | <span class="t">entropy loss it will take a sequence of tokens each tokens with its own logics and it will compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16774" target="_blank">04:39:34.800</a></span> | <span class="t">one single number which is the loss so how to view the effect on the output with respect to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16784" target="_blank">04:39:44.160</a></span> | <span class="t">in this case well if x changes by a little amount and this little amount is not anymore a number but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16790" target="_blank">04:39:50.400</a></span> | <span class="t">it's a vector so if change the x the old x plus delta x is a vector sum then y will also be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16800" target="_blank">04:40:00.400</a></span> | <span class="t">affected by what y will be affected by dy on dx multiplied by delta x however this delta x is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16808" target="_blank">04:40:08.720</a></span> | <span class="t">a number anymore it's a vector because x1 may change by a little bit x2 will change by a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16814" target="_blank">04:40:14.880</a></span> | <span class="t">bit x3 will change by a little bit x4 until xn will change by a little bit so this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16820" target="_blank">04:40:20.880</a></span> | <span class="t">a dot product of this vector multiplied by this vector why a dot product because y will be affected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16828" target="_blank">04:40:28.880</a></span> | <span class="t">by the change in x1 it will be affected by the change in x2 it will be changed affected by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16834" target="_blank">04:40:34.880</a></span> | <span class="t">change in x3 up to xn and each of the contribution of the contribution of x1 will be the partial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16842" target="_blank">04:40:42.080</a></span> | <span class="t">derivative of y with respect to x1 multiplied by how much x1 has changed plus the contribution of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16849" target="_blank">04:40:49.520</a></span> | <span class="t">x2 will be the partial derivative of y with respect to x2 multiplied by how much x2 has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16855" target="_blank">04:40:55.280</a></span> | <span class="t">changed blah blah blah until the last contribution of xn so and the chain rule in this case also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16862" target="_blank">04:41:02.320</a></span> | <span class="t">applies in the same way as in the scalar case so the formula does not change also for the chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16866" target="_blank">04:41:06.960</a></span> | <span class="t">rule here i just want you to to remember to remind you that in this case we are talking about a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16873" target="_blank">04:41:13.280</a></span> | <span class="t">gradient and the gradient is just a vector made up of all the partial derivatives of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16880" target="_blank">04:41:20.160</a></span> | <span class="t">with respect each of the input variables that are in the input vector when we talk about a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16887" target="_blank">04:41:27.760</a></span> | <span class="t">that have as input a vector and produces a vector then we don't talk about gradient anymore we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16894" target="_blank">04:41:34.000</a></span> | <span class="t">about jacobians so if our input x the input x of this function changes by a little amount and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16901" target="_blank">04:41:41.840</a></span> | <span class="t">delta x is a vector then the output y will also change and this output y will change by a delta y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16909" target="_blank">04:41:49.520</a></span> | <span class="t">that is not a number anymore it is a vector and this vector is the result of this quantity dy on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16916" target="_blank">04:41:56.400</a></span> | <span class="t">the x multiplied by delta x delta x is a vector so this one has to be a vector it has this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16924" target="_blank">04:42:04.560</a></span> | <span class="t">here has to be a matrix and this matrix is called the jacobian it is a matrix that has as many rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16931" target="_blank">04:42:11.120</a></span> | <span class="t">later we will talk about the denotations so it has as many rows as there are output variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16936" target="_blank">04:42:16.880</a></span> | <span class="t">and as many columns as there are input variables the first row will be the partial derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16944" target="_blank">04:42:24.080</a></span> | <span class="t">the first output variable with respect to all the input variables the second row will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16949" target="_blank">04:42:29.440</a></span> | <span class="t">partial derivative of the second output variable with respect to all the input variables and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16955" target="_blank">04:42:35.200</a></span> | <span class="t">last row will be the partial derivatives of the last output variable with respect to all the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16961" target="_blank">04:42:41.200</a></span> | <span class="t">variable in the input vector now let's talk about the notations the jacobian that i have written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16968" target="_blank">04:42:48.640</a></span> | <span class="t">here is a is written according to the numerator layout this is called the numerator layout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16976" target="_blank">04:42:56.240</a></span> | <span class="t">and there is another convention called the not layout sorry guys it's called the numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16982" target="_blank">04:43:02.000</a></span> | <span class="t">convention and there is another convention called denominator convention or notation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16986" target="_blank">04:43:06.480</a></span> | <span class="t">in which the rows are not the the number of rows is not the equivalent to the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=16995" target="_blank">04:43:15.440</a></span> | <span class="t">output variables but equal to the number of input variables so the fact that i have we we choose to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17002" target="_blank">04:43:22.640</a></span> | <span class="t">write the jacobian as follows is based on a convention you can also write the the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17009" target="_blank">04:43:29.840</a></span> | <span class="t">according to the denominator convention just by transposing this jacobian here and also the formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17015" target="_blank">04:43:35.200</a></span> | <span class="t">for the chain rule changes accordingly for now i want to keep the formula for the chain rule just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17020" target="_blank">04:43:40.480</a></span> | <span class="t">like the one for the scalar case so that's why i am using this notation here but later we can change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17026" target="_blank">04:43:46.000</a></span> | <span class="t">between one notation to the other just by doing a transposition okay now that we have reviewed what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17032" target="_blank">04:43:52.000</a></span> | <span class="t">is derivative what is a gradient and what is a jacobian let's talk about what happens when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17039" target="_blank">04:43:59.360</a></span> | <span class="t">take derivatives with respect to tensors of a tensor with respect to another tensor in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17044" target="_blank">04:44:04.400</a></span> | <span class="t">we talk about the jacobian but it's called the generalized jacobian so if we have the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17050" target="_blank">04:44:10.400</a></span> | <span class="t">that is at input takes a tensor of dx dimensions where the first shape this is kind of the shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17059" target="_blank">04:44:19.120</a></span> | <span class="t">of the tensor so the first element of the shape is n1 the second element of the shape of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17064" target="_blank">04:44:24.320</a></span> | <span class="t">vector is n2 etc etc until n dx and it produces an output tensor that has this shape so m1 m2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17074" target="_blank">04:44:34.000</a></span> | <span class="t">mdy in this case the formula for the chain rule doesn't change and if x changes by a little amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17084" target="_blank">04:44:44.000</a></span> | <span class="t">so by delta x which is a tensor y will also be affected by how much by dy on dx multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17093" target="_blank">04:44:53.840</a></span> | <span class="t">delta x and this is a tensor product it will be a jacobian this is called generalized jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17101" target="_blank">04:45:01.520</a></span> | <span class="t">with the following shape so all the dimensions of the output multiplied by all the dimensions of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17107" target="_blank">04:45:07.280</a></span> | <span class="t">the input all right this is very abstract for now we will see actually a concrete case of this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17114" target="_blank">04:45:14.320</a></span> | <span class="t">because we will be deriving the gradient of the output of a matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17120" target="_blank">04:45:20.240</a></span> | <span class="t">the gradient of the loss when computing backward pass with respect to each of the input of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17126" target="_blank">04:45:26.800</a></span> | <span class="t">matrix multiplication operation and we will do it also for the softmax and we will do it also for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17130" target="_blank">04:45:30.800</a></span> | <span class="t">the attention so i don't want to jump to too many topics i just wanted us to get into the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17135" target="_blank">04:45:35.440</a></span> | <span class="t">mindset so we know that derivatives when we have scalar functions gradients when the output is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17140" target="_blank">04:45:40.880</a></span> | <span class="t">scalar input is a vector jacobian when the input and output are both vectors generalized jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17146" target="_blank">04:45:46.640</a></span> | <span class="t">when the input and output are tensors the chain rule always works in the same way all right let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17154" target="_blank">04:45:54.560</a></span> | <span class="t">talk about autogradient i will do the scalar case and then we will extend it to the tensor case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17161" target="_blank">04:46:01.040</a></span> | <span class="t">so imagine we have a very simple computation graph why we have computation graph because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17165" target="_blank">04:46:05.120</a></span> | <span class="t">we are talking about neural networks and neural networks are nothing more than computation graphs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17169" target="_blank">04:46:09.680</a></span> | <span class="t">where we have some input we have some parameters and we do some operations with this input and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17173" target="_blank">04:46:13.440</a></span> | <span class="t">parameters suppose that you have an input a and this input a is multiplied by a weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17179" target="_blank">04:46:19.440</a></span> | <span class="t">it's a parameter weight it's just a scalar and it produces an output y1 this y1 is then summed up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17186" target="_blank">04:46:26.800</a></span> | <span class="t">with another number called b1 and it produces y2 this y2 is then raised to the power of 2 so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17193" target="_blank">04:46:33.040</a></span> | <span class="t">is e to the power of 2 it's just the power of 2 of the input and it produces y3 and this y3 becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17199" target="_blank">04:46:39.520</a></span> | <span class="t">our loss function so it's a scalar now what we want to do to apply gradient descent is we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17207" target="_blank">04:46:47.040</a></span> | <span class="t">to compute the gradient of the loss function with respect to each of the input of this computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17213" target="_blank">04:46:53.920</a></span> | <span class="t">graph so each of the leaves of this computation graphs what are the leaves it's this node here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17218" target="_blank">04:46:58.800</a></span> | <span class="t">so the parameter nodes and input nodes and to do that there are two ways one is if you have access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17227" target="_blank">04:47:07.440</a></span> | <span class="t">to the expression that relates directly the input to the output so the to the loss then you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17235" target="_blank">04:47:15.760</a></span> | <span class="t">directly compute the gradient the derivative in this case because it's not a gradient it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17240" target="_blank">04:47:20.640</a></span> | <span class="t">scalar versus color so in this case imagine you want to compute the derivative of the loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17245" target="_blank">04:47:25.760</a></span> | <span class="t">respect to w1 imagine we have access to the exact expression that relates the w1 to to the phi which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17255" target="_blank">04:47:35.200</a></span> | <span class="t">is our loss we can compute it as follows so we just derive this expression with respect to w1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17260" target="_blank">04:47:40.960</a></span> | <span class="t">which is two times because this is the power of two of a function so it is two multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17266" target="_blank">04:47:46.880</a></span> | <span class="t">function multiplied by the derivative of the content of this function with respect to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17271" target="_blank">04:47:51.840</a></span> | <span class="t">variable that we are deriving so it will become the following expression there is another way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17277" target="_blank">04:47:57.600</a></span> | <span class="t">which is by using the chain rule so we can use the derivative of phi with respect to yw1 is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17284" target="_blank">04:48:04.800</a></span> | <span class="t">derivative of phi with respect to y3 which is the previous output of the previous node then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17291" target="_blank">04:48:11.760</a></span> | <span class="t">derivative of phi3 with respect to the previous the output of the previous node so and then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17297" target="_blank">04:48:17.600</a></span> | <span class="t">multiplied by the derivative of y2 with respect to the output of the previous node and then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17302" target="_blank">04:48:22.720</a></span> | <span class="t">derivative of y1 with respect to w1 if we do all this chain of multiplication we will obtain the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17309" target="_blank">04:48:29.040</a></span> | <span class="t">same result and you can see that here this stuff here is exactly equal to this stuff here by doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17315" target="_blank">04:48:35.920</a></span> | <span class="t">this procedure here we will note something that is i want to zoom out a little bit okay to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17324" target="_blank">04:48:44.240</a></span> | <span class="t">the derivative of phi with respect to w1 we are doing all this chain of multiplication but what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17333" target="_blank">04:48:53.120</a></span> | <span class="t">is each item in what is each factor in this sequence of multiplications well this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17341" target="_blank">04:49:01.120</a></span> | <span class="t">here is nothing more than the derivative of phi with respect to y2 these multiplications here are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17348" target="_blank">04:49:08.080</a></span> | <span class="t">nothing more than the derivative of phi with respect to w to respect to y1 and all of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17354" target="_blank">04:49:14.960</a></span> | <span class="t">combined are the derivative of phi with respect to w1 what pytorch will do it will do the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17362" target="_blank">04:49:22.560</a></span> | <span class="t">pytorch will do the backward pass because pytorch knows what is the computation graph that relates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17370" target="_blank">04:49:30.000</a></span> | <span class="t">the output so the loss function in this case and the variable for which we want to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17376" target="_blank">04:49:36.080</a></span> | <span class="t">gradient right now we are talking about derivatives so it's not gradient but the mechanism is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17382" target="_blank">04:49:42.080</a></span> | <span class="t">the same so pytorch will say it will pytorch is like a person that knocks the door of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17392" target="_blank">04:49:52.400</a></span> | <span class="t">operation and says hey operation exponential power of two if i give you the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17403" target="_blank">04:50:03.360</a></span> | <span class="t">loss with respect to y3 which is one because the loss and y3 are actually the same can you give me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17409" target="_blank">04:50:09.840</a></span> | <span class="t">the gradient of the loss with respect to y2 because pytorch actually does not implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17415" target="_blank">04:50:15.440</a></span> | <span class="t">an autograd system in the sense that it does not know the symbolic operations that led to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17421" target="_blank">04:50:21.280</a></span> | <span class="t">output it just knows what are the functions that computed the output and each function has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17427" target="_blank">04:50:27.760</a></span> | <span class="t">function each function is a class in python that implements two methods one is the forward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17433" target="_blank">04:50:33.760</a></span> | <span class="t">and one is the backward step the forward step takes the input so in this case y2 and computes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17438" target="_blank">04:50:38.560</a></span> | <span class="t">the output y3 the backward step will take the gradient of the loss with respect to its output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17445" target="_blank">04:50:45.920</a></span> | <span class="t">and needs to compute the gradient of the loss with respect to its input how can we do that well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17451" target="_blank">04:50:51.680</a></span> | <span class="t">it's very simple because a pytorch will knock the door as let me copy it and this stuff here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17458" target="_blank">04:50:58.560</a></span> | <span class="t">otherwise it's not easy to go back and forth so okay and let's paste it here pytorch will knock</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17466" target="_blank">04:51:06.320</a></span> | <span class="t">the door of this function here and we'll say hey if i give you the loss of the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17474" target="_blank">04:51:14.480</a></span> | <span class="t">function with respect to your output can you give me the gradient of the loss function with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17479" target="_blank">04:51:19.200</a></span> | <span class="t">to your input yes the function can do it why because of the chain rule this operator here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17485" target="_blank">04:51:25.120</a></span> | <span class="t">this function here can just do take the loss the gradient of the loss function with respect to its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17490" target="_blank">04:51:30.480</a></span> | <span class="t">output multiply it by the jacobian or in this case the derivative of its output with respect to its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17498" target="_blank">04:51:38.640</a></span> | <span class="t">input and it will be equal to the gradient of the loss with respect to its input then pytorch will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17504" target="_blank">04:51:44.080</a></span> | <span class="t">take this one and knock the door at the next operator which is this one this summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17508" target="_blank">04:51:48.400</a></span> | <span class="t">and we'll say hey if i give you the gradient of the loss with respect to your output can you give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17514" target="_blank">04:51:54.720</a></span> | <span class="t">me the gradient of the loss with respect to your input yes this operator can do it because this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17519" target="_blank">04:51:59.920</a></span> | <span class="t">operator just needs to apply the chain rule so it will take the gradient of the loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17523" target="_blank">04:52:03.920</a></span> | <span class="t">to y2 which is provided by pytorch and by multiplying it with the the jacobian in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17531" target="_blank">04:52:11.600</a></span> | <span class="t">case it's the derivative the derivative of the its output with respect to its input it can compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17536" target="_blank">04:52:16.480</a></span> | <span class="t">the the gradient of the loss with respect to its input then pytorch will take this output of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17543" target="_blank">04:52:23.360</a></span> | <span class="t">backward pass and will knock the door of the next operator which is this product and will ask again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17548" target="_blank">04:52:28.800</a></span> | <span class="t">the same question hey if i give you the gradient of the loss with respect to your output can you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17554" target="_blank">04:52:34.080</a></span> | <span class="t">give me the gradient of the loss with respect to your input yes this will do the same exact job it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17559" target="_blank">04:52:39.200</a></span> | <span class="t">will take the gradient of the loss with respect to the output multiplied by the jacobian of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17564" target="_blank">04:52:44.400</a></span> | <span class="t">output with respect to the input and obtain the gradient of the loss with respect to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17568" target="_blank">04:52:48.640</a></span> | <span class="t">input and this is how pytorch runs the backward step it runs one operator at a time backwards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17576" target="_blank">04:52:56.800</a></span> | <span class="t">in the computation graph knocking the door of each operator and asking always the same question if i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17583" target="_blank">04:53:03.040</a></span> | <span class="t">give you the output the gradient of the loss with respect to your output can you give me the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17587" target="_blank">04:53:07.680</a></span> | <span class="t">of the loss with respect to your input and each operator will just apply the chain rule to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17592" target="_blank">04:53:12.240</a></span> | <span class="t">to to get this to get this gradient to calculate this gradient that pytorch needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17597" target="_blank">04:53:17.520</a></span> | <span class="t">why pytorch cannot do it by itself because pytorch does not do symbolic mathematics it does not have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17605" target="_blank">04:53:25.200</a></span> | <span class="t">access to the exact expression that each function is computing it just uses the function as a black</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17610" target="_blank">04:53:30.800</a></span> | <span class="t">box that computes forward and backward however with the jacobian we have a problem and let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17617" target="_blank">04:53:37.280</a></span> | <span class="t">see what is the problem all right so up to now we have been working with a computation graph that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17623" target="_blank">04:53:43.680</a></span> | <span class="t">made up of scalars but the things that we have said they work in the scalar case but also in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17628" target="_blank">04:53:48.640</a></span> | <span class="t">the tensor case so let's go back see what is our computation graph we have seen that pytorch will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17635" target="_blank">04:53:55.600</a></span> | <span class="t">go operator by operator asking always the same question if i give you the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17640" target="_blank">04:54:00.400</a></span> | <span class="t">with respect to your output can you compute me the gradient of the loss with respect to your input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17645" target="_blank">04:54:05.120</a></span> | <span class="t">and each operator can just apply the chain rule to compute that imagine now that all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17651" target="_blank">04:54:11.840</a></span> | <span class="t">operators are working not with scalars but are working with tensors which means that the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17657" target="_blank">04:54:17.600</a></span> | <span class="t">of the output with respect to the input of each operator is not a derivative it will be a jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17663" target="_blank">04:54:23.600</a></span> | <span class="t">because the output will be a tensor a generalized jacobian and input will be a tensor which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17670" target="_blank">04:54:30.000</a></span> | <span class="t">also that this quantity here so the derivative of the loss with respect to the input in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17675" target="_blank">04:54:35.120</a></span> | <span class="t">will not be a derivative it will be a gradient because the output the loss is a number always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17679" target="_blank">04:54:39.920</a></span> | <span class="t">while the input in this case y1 will be a tensor so number output input is a tensor then we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17689" target="_blank">04:54:49.120</a></span> | <span class="t">about gradients so this will be a gradient and we will call it the downstream gradient that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17694" target="_blank">04:54:54.880</a></span> | <span class="t">operator needs to compute this will be the upstream gradient that pytorch will give to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17699" target="_blank">04:54:59.920</a></span> | <span class="t">each of these operators so the gradient of the loss with respect to the output of each operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17705" target="_blank">04:55:05.520</a></span> | <span class="t">and each operator needs to come up with this downstream gradient by using the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17711" target="_blank">04:55:11.760</a></span> | <span class="t">however the jacobian has a problem let's see so imagine we are implementing a simple operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17718" target="_blank">04:55:18.640</a></span> | <span class="t">that is the matrix multiplication and the matrix multiplication is takes as input a x tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17726" target="_blank">04:55:26.000</a></span> | <span class="t">it multiplies it by a w matrix made up of parameters and produces a y matrix as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17731" target="_blank">04:55:31.920</a></span> | <span class="t">suppose that x is let's call it n by d matrix w is let's say d by m matrix and so y will be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17745" target="_blank">04:55:45.920</a></span> | <span class="t">n by m matrix usually the input x is a sequence of tensor of let's say vectors each of each with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17758" target="_blank">04:55:58.880</a></span> | <span class="t">d dimensions so you can think of it as a sequence of tokens each token is a vector made up of d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17764" target="_blank">04:56:04.880</a></span> | <span class="t">dimensions usually we have many tokens so suppose that n usually is at least 1024 at least in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17773" target="_blank">04:56:13.440</a></span> | <span class="t">most recent language models we even have millions of tokens actually so and d is also actually quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17780" target="_blank">04:56:20.880</a></span> | <span class="t">big it usually it is at least 1024 also so also this one is 1024 and d and m m is also at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17791" target="_blank">04:56:31.600</a></span> | <span class="t">1024 so we can actually become 2028 let's say so i i like the powers of two by the way so the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17799" target="_blank">04:56:39.920</a></span> | <span class="t">of the jacobian is this if we compute want to compute this downstream gradient by multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17805" target="_blank">04:56:45.040</a></span> | <span class="t">the upstream gradient with the jacobian this jacobian matrix is huge because look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17811" target="_blank">04:56:51.520</a></span> | <span class="t">dimensions here this will be a matrix that is it will be well n by m multiplied so it will be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17822" target="_blank">04:57:02.400</a></span> | <span class="t">generalized jacobian so it will be a tensor that has a shape n m and then the input is x so it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17830" target="_blank">04:57:10.800</a></span> | <span class="t">n by d so how many elements it will have well it will have 1024 multiplied by m which is 2048</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17840" target="_blank">04:57:20.320</a></span> | <span class="t">multiplied by 1024 multiplied by d which is 1024 so it is at least wow it's a billions more than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17851" target="_blank">04:57:31.200</a></span> | <span class="t">1 billion elements so it is impossible actually to materialize this matrix here in the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17859" target="_blank">04:57:39.440</a></span> | <span class="t">because in the ram of the gpu because it will be too big so but we need to compute this downstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17866" target="_blank">04:57:46.640</a></span> | <span class="t">gradient because pytorch needs it to continue calculating the gradient of the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17871" target="_blank">04:57:51.760</a></span> | <span class="t">with respect to each of the nodes in the computation graph so how can we proceed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17876" target="_blank">04:57:56.640</a></span> | <span class="t">the first thing that we should notice is that this this jacobian is actually a sparse matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17884" target="_blank">04:58:04.080</a></span> | <span class="t">and i want to show you why it is actually is a super super super sparse matrix because if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17890" target="_blank">04:58:10.240</a></span> | <span class="t">look at the input what is the effect of the input on the output the input is a sequence of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17897" target="_blank">04:58:17.120</a></span> | <span class="t">so this is the token number one it's a vector of some dimensions 1024 dimension then we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17904" target="_blank">04:58:24.320</a></span> | <span class="t">another token as input then we have another tokens as input then we have another tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17909" target="_blank">04:58:29.280</a></span> | <span class="t">as input and we multiply by the w matrix which is made up of some columns some columns so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17917" target="_blank">04:58:37.680</a></span> | <span class="t">one is n by d right yes and w is d by m so d by m this will produce a matrix that is n by m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17930" target="_blank">04:58:50.080</a></span> | <span class="t">so it will be also a sequence of tokens each made up of m dimensions so it will be a matrix like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17936" target="_blank">04:58:56.480</a></span> | <span class="t">this so this will be the first output token this will be the second output token this will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17942" target="_blank">04:59:02.560</a></span> | <span class="t">third output token and this will be the fourth output token now this output row here is the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17951" target="_blank">04:59:11.200</a></span> | <span class="t">product of this input row with all the columns so the derivative of each of these dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17959" target="_blank">04:59:19.280</a></span> | <span class="t">with respect to the dimensions of all the other tokens will be zero because they do not contribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17964" target="_blank">04:59:24.320</a></span> | <span class="t">to this output so the jacobian will have zeros every time the we are calculating the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17972" target="_blank">04:59:32.960</a></span> | <span class="t">of this first dimension with respect to any other element of other tokens that's why we always can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17980" target="_blank">04:59:40.960</a></span> | <span class="t">come up with a better formula for computing this downstream gradient that does not involve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17986" target="_blank">04:59:46.000</a></span> | <span class="t">materialization of the jacobian because the matter the jacobian itself is sparse so let's see how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17991" target="_blank">04:59:51.920</a></span> | <span class="t">can optimize this computation without materializing the jacobian in the case of matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=17997" target="_blank">04:59:57.280</a></span> | <span class="t">because we need it for flash attention all right guys so before proceeding to the backward watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18004" target="_blank">05:00:04.800</a></span> | <span class="t">the formulas of the backward path of the flash attention let's look at how to compute the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18010" target="_blank">05:00:10.640</a></span> | <span class="t">of the matrix multiplication operation with respect to its input so imagine we are creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18015" target="_blank">05:00:15.920</a></span> | <span class="t">okay pytorch already have actually how to compute the gradient of the inputs of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18023" target="_blank">05:00:23.040</a></span> | <span class="t">matrix multiplication with the gradient of the loss with respect to the input of the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18026" target="_blank">05:00:26.720</a></span> | <span class="t">multiplication operation but in flash attention we are creating a custom kernel which means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18032" target="_blank">05:00:32.480</a></span> | <span class="t">the custom kernel is fusing multiple operations into one operation so when pytorch will knock</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18039" target="_blank">05:00:39.360</a></span> | <span class="t">the door of our operator it will ask the our operator which is the triton attention operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18045" target="_blank">05:00:45.280</a></span> | <span class="t">that we have built what is the gradient of the loss function with respect to q k and v because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18050" target="_blank">05:00:50.080</a></span> | <span class="t">that's the input of our function so if we look at the code that we have built so far you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18055" target="_blank">05:00:55.280</a></span> | <span class="t">that our triton attention will be a node in the computation graph that takes as input q k and v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18062" target="_blank">05:01:02.560</a></span> | <span class="t">and produces an output then pytorch will give us the gradient of the loss with respect to that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18069" target="_blank">05:01:09.040</a></span> | <span class="t">output so it will give us a d o so the derivative of the loss with the gradient of the loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18074" target="_blank">05:01:14.160</a></span> | <span class="t">respect to o and then we'll ask this class here so triton attention to compute the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18081" target="_blank">05:01:21.440</a></span> | <span class="t">of the loss with respect to q k and b because we are fusing multiple operations together so we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18086" target="_blank">05:01:26.720</a></span> | <span class="t">computing on the fly the softmax of query multiply by the transpose of the key and then multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18092" target="_blank">05:01:32.000</a></span> | <span class="t">doing the softmax and multiplying it by v to compute the output we need to compute this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18098" target="_blank">05:01:38.640</a></span> | <span class="t">gradient internally to compute this the gradient of the inputs so because in this operation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18105" target="_blank">05:01:45.280</a></span> | <span class="t">we are doing fusing together there is a matrix multiplication we need to derive by hand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18110" target="_blank">05:01:50.240</a></span> | <span class="t">matrix multiplication the gradient of the of the loss function with respect to the input of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18116" target="_blank">05:01:56.480</a></span> | <span class="t">matrix multiplication operation so that we can provide it to pytorch that's why we need to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18123" target="_blank">05:02:03.760</a></span> | <span class="t">this formula i will derive it in the simple in a very simple way and and then we will do it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18132" target="_blank">05:02:12.000</a></span> | <span class="t">the softmax as well because these are the two things that we need to derive by hand to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18135" target="_blank">05:02:15.840</a></span> | <span class="t">the formula of the flash attention's backward pass so let's start imagine we have a computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18143" target="_blank">05:02:23.680</a></span> | <span class="t">graph a node in the computation graph called the matrix multiplication and this node in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18149" target="_blank">05:02:29.200</a></span> | <span class="t">computation graph is doing a matrix multiplication so it is computing the following operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18153" target="_blank">05:02:33.360</a></span> | <span class="t">y is equal to x multiplied by w now what pytorch will give us as input when computing the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18163" target="_blank">05:02:43.600</a></span> | <span class="t">pass of this node pytorch will give us the gradient of the loss so it will give us d phi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18169" target="_blank">05:02:49.840</a></span> | <span class="t">with respect to dy so the output of this node and will ask us to compute the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18178" target="_blank">05:02:58.400</a></span> | <span class="t">loss function so the gradient of the loss function with respect to dx and the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18184" target="_blank">05:03:04.400</a></span> | <span class="t">function with respect to dw the easiest one to work with and the one that i will be showing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18190" target="_blank">05:03:10.560</a></span> | <span class="t">the other one i will not show in the video but i will attach the pdf slide on how it is computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18194" target="_blank">05:03:14.960</a></span> | <span class="t">because they are very similar in the way they are computed so i don't want to make the video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18199" target="_blank">05:03:19.440</a></span> | <span class="t">too long for unnecessary reasons let's compute the gradient of the loss function with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18208" target="_blank">05:03:28.240</a></span> | <span class="t">to the input so with respect to x all right so how to do that by hand without materializing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18217" target="_blank">05:03:37.120</a></span> | <span class="t">jacobian because as we have seen we cannot just use the chain rule by materializing the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18222" target="_blank">05:03:42.160</a></span> | <span class="t">which would be the easiest way because the jacobian is very big matrix that cannot even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18226" target="_blank">05:03:46.960</a></span> | <span class="t">fit in the memory of the gpu so we need to find a smarter way we exploit the fact that the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18233" target="_blank">05:03:53.120</a></span> | <span class="t">is sparse so hopefully we will get formula that does not involve the materialization of a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18238" target="_blank">05:03:58.560</a></span> | <span class="t">big sparse jacobian let's see so uh let's see um let's when dealing with these kind of derivations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18247" target="_blank">05:04:07.360</a></span> | <span class="t">i always recommend to make some example tensors so suppose that that x is a tensor of size let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18255" target="_blank">05:04:15.920</a></span> | <span class="t">n by d and where n let's say n is equal to one and d is equal to let's say three and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18265" target="_blank">05:04:25.200</a></span> | <span class="t">w is a tensor also or a matrix with the shape let's say d by m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18276" target="_blank">05:04:36.800</a></span> | <span class="t">where m is equal to let's say four and y will have as a consequence the shape n by m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18287" target="_blank">05:04:47.440</a></span> | <span class="t">so it will have the shape well one by four what pytorch will give us and pytorch will give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18298" target="_blank">05:04:58.320</a></span> | <span class="t">the following quantity so it will give us this stuff here so the gradient of the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18303" target="_blank">05:05:03.840</a></span> | <span class="t">with respect to the output of this operator which is y so it will give us a vector or a tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18310" target="_blank">05:05:10.240</a></span> | <span class="t">actually with the following dimension which is n by m and we need to compute the gradient of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18319" target="_blank">05:05:19.280</a></span> | <span class="t">loss function with respect to x which should be a tensor of shape n by d because when dealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18326" target="_blank">05:05:26.240</a></span> | <span class="t">with the gradient it always has the shape of the input variable because it's the output which is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18333" target="_blank">05:05:33.200</a></span> | <span class="t">scalar with respect to each element in the input so it has the same shape as the denominator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18338" target="_blank">05:05:38.160</a></span> | <span class="t">all right so when dealing with this kind of problems i always recommend to create example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18344" target="_blank">05:05:44.160</a></span> | <span class="t">matrices and then work out what happens to the output and then try to work out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18349" target="_blank">05:05:49.280</a></span> | <span class="t">the gradient matrix so let's do it so let's see that what is how is the output computed well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18357" target="_blank">05:05:57.440</a></span> | <span class="t">the output will be a matrix that is a one by four computed as follows it will be the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18365" target="_blank">05:06:05.920</a></span> | <span class="t">so one by three so let's call the input x one one x one two x one three it will be multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18376" target="_blank">05:06:16.160</a></span> | <span class="t">by another matrix w that it has dimension three by four so it will be three rows by four columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18385" target="_blank">05:06:25.280</a></span> | <span class="t">so it will be w 1 1 w 1 2 w 1 3 w 1 4 then w 2 1 w 2 2 w 2 3 w 2 4 w 3 1 w 3 2 w 3 3 w 3 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18408" target="_blank">05:06:48.560</a></span> | <span class="t">if we do this matrix multiplication it will be well it will produce the following matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18415" target="_blank">05:06:55.040</a></span> | <span class="t">that is okay this is one row by three columns this is three column three rows by four columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18421" target="_blank">05:07:01.040</a></span> | <span class="t">so the output will be a matrix that is one by four so one row by four columns so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18428" target="_blank">05:07:08.560</a></span> | <span class="t">let me write it with a smaller because otherwise it will never fit here so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18435" target="_blank">05:07:15.520</a></span> | <span class="t">let's do it like this it will be x 1 1 multiplied by w 1 1 plus x 1 2 multiplied by w 2 1 plus x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18449" target="_blank">05:07:29.280</a></span> | <span class="t">1 3 multiplied by w 3 1 and this will be the first element of the output the second element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18457" target="_blank">05:07:37.120</a></span> | <span class="t">of the output will be x 1 1 with w 1 2 x 1 1 with w 1 2 plus x 1 2 with 1 2 with w 2 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18473" target="_blank">05:07:53.360</a></span> | <span class="t">plus x 1 3 with w 3 2 this will be the second element of the output matrix the third element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18484" target="_blank">05:08:04.800</a></span> | <span class="t">of the output matrix will be let me move this stuff on the left otherwise it will never fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18488" target="_blank">05:08:08.720</a></span> | <span class="t">so okay i think now it can fit this will be x i need to watch this one so x 1 1 with w 1 3 x 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18500" target="_blank">05:08:20.720</a></span> | <span class="t">x 1 1 with w 1 3 plus x 1 2 with w 2 3 plus x 1 3 with w 3 3 and then we multiply the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18515" target="_blank">05:08:35.440</a></span> | <span class="t">row with the last column so it will be x 1 1 w 1 4 plus x 1 2 w 2 4 plus x 1 3 w 3 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18529" target="_blank">05:08:49.760</a></span> | <span class="t">this will be the output y if we do the matrix multiplication what pytorch will give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18536" target="_blank">05:08:56.880</a></span> | <span class="t">it will give us the gradient of the loss so it will give us delta phi with respect to delta y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18544" target="_blank">05:09:04.000</a></span> | <span class="t">because it's a gradient it has the same shape as the denominator so it has a shape that is 1 by 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18550" target="_blank">05:09:10.640</a></span> | <span class="t">let's call it because we don't know what this value will be they will be provided to us by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18555" target="_blank">05:09:15.600</a></span> | <span class="t">pytorch let's just give them generic name like d y 1 1 d y 1 2 d y 1 3 and d y 1 4 like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18568" target="_blank">05:09:28.880</a></span> | <span class="t">now to compute the the downstream gradient that we need to provide to pytorch we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18577" target="_blank">05:09:37.360</a></span> | <span class="t">be computing the we should be materializing the jacobian which is which is okay let's write the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18586" target="_blank">05:09:46.560</a></span> | <span class="t">chain the chain rule formula so we need to provide delta phi to with respect to delta x which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18594" target="_blank">05:09:54.880</a></span> | <span class="t">equal to delta phi with respect to delta y this is provided by pytorch multiplied by the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18602" target="_blank">05:10:02.800</a></span> | <span class="t">which is delta y with respect to delta x now instead of materializing this jacobian let's try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18610" target="_blank">05:10:10.320</a></span> | <span class="t">to do this let's materialize it now and let's do the multiplication of these two quantities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18616" target="_blank">05:10:16.640</a></span> | <span class="t">to see if something simplifies so this stuff here will be dy with respect to dx which means the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18624" target="_blank">05:10:24.400</a></span> | <span class="t">derivative of every output y with respect to every input x how many output we have we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18632" target="_blank">05:10:32.080</a></span> | <span class="t">four elements as the output which is this stuff here and we have three element as input in the x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18639" target="_blank">05:10:39.280</a></span> | <span class="t">matrix so it will be as follows i don't know how to let me copy it because my screen is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18647" target="_blank">05:10:47.600</a></span> | <span class="t">not big enough and i remember that x is x 1 1 and x x 2 so delta y with respect to delta x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18658" target="_blank">05:10:58.400</a></span> | <span class="t">will have the following entries so the y1 with respect to x 1 1 and as you can see y1 only has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18669" target="_blank">05:11:09.760</a></span> | <span class="t">one x 1 1 appearing as multiplied by w 1 1 so the derivative with respect to x 1 1 will be w 1 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18676" target="_blank">05:11:16.560</a></span> | <span class="t">then y 1 1 so this stuff with respect to x 1 2 it will be w 2 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18687" target="_blank">05:11:27.360</a></span> | <span class="t">then x y 1 1 with respect to x 1 3 will be w 3 1 the second row of this matrix will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18697" target="_blank">05:11:37.920</a></span> | <span class="t">derivative of the partial derivative of the second output so w y 2 with respect to all the x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18705" target="_blank">05:11:45.680</a></span> | <span class="t">inputs which will be the derivative partial derivatives of this stuff here with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18711" target="_blank">05:11:51.520</a></span> | <span class="t">every x which is w 1 2 w 2 2 i guess and w 3 2 now let me check if it's what i'm doing is correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18725" target="_blank">05:12:05.200</a></span> | <span class="t">yes because i've already done it so i can always double check and then we have w the partial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18733" target="_blank">05:12:13.840</a></span> | <span class="t">derivatives of this stuff here with respect to all the x which is w 1 3 w 2 3 and w 3 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18745" target="_blank">05:12:25.280</a></span> | <span class="t">then the partial derivatives of the last output so y 4 with respect to all the x which will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18753" target="_blank">05:12:33.600</a></span> | <span class="t">w 1 w 1 4 w 2 4 and w 3 4 we obtain the following jacobian if um but this jacobian as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18770" target="_blank">05:12:50.160</a></span> | <span class="t">is just equal to w transposed so we don't need to materialize the jacobian we can just do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18777" target="_blank">05:12:57.520</a></span> | <span class="t">multiplication of whatever gradient pytorch is giving us multiply it by w transposed and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18785" target="_blank">05:13:05.120</a></span> | <span class="t">we will get the downstream gradient so let me rewrite so we know have we know what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18790" target="_blank">05:13:10.720</a></span> | <span class="t">doing so d phi on d dx is equal to d phi with respect to y multiplied by dy on dx but we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18803" target="_blank">05:13:23.760</a></span> | <span class="t">seen that dy on dx is just equal to w transposed so this is equal to d phi on dx dy multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18813" target="_blank">05:13:33.840</a></span> | <span class="t">w transposed and this gives us the downstream gradient so in order to provide the downstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18818" target="_blank">05:13:38.960</a></span> | <span class="t">gradient that pytorch need we just need to take whatever gradient pytorch will give us multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18823" target="_blank">05:13:43.280</a></span> | <span class="t">by w transposed and it will give us the gradient of the loss function with respect to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18828" target="_blank">05:13:48.880</a></span> | <span class="t">x of the matrix multiplication in the same way we can also write the formula for the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18835" target="_blank">05:13:55.520</a></span> | <span class="t">of the loss function with respect to w and it is equal to x transposed multiplied by d phi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18843" target="_blank">05:14:03.360</a></span> | <span class="t">with respect to dw dy how to remember these formulas these are there is a mnemonic rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18854" target="_blank">05:14:14.720</a></span> | <span class="t">which is these are the only possible ways for this to have the shape of x and this to have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18864" target="_blank">05:14:24.480</a></span> | <span class="t">shape of w because this one's this stuff here will have the same shape of y so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18870" target="_blank">05:14:30.880</a></span> | <span class="t">n by m this stuff here will have shape of w transposed w is d by m so w transpose should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18882" target="_blank">05:14:42.400</a></span> | <span class="t">m by d and the resulting operation of this matrix multiplication or tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18888" target="_blank">05:14:48.960</a></span> | <span class="t">multiplication will be n by d which is exactly the same shape as x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18896" target="_blank">05:14:56.320</a></span> | <span class="t">in this case we will have that xt is the transposed of t and it is n by d so it's d by n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18909" target="_blank">05:15:09.360</a></span> | <span class="t">multiplied by d phi with respect to dy which is a gradient so it has the same shape as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18915" target="_blank">05:15:15.040</a></span> | <span class="t">denominator so it has n by m and the output will have shape d by m which is exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18929" target="_blank">05:15:29.520</a></span> | <span class="t">the shape of w so if you if to remember them this is the only way this shape work out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18937" target="_blank">05:15:37.280</a></span> | <span class="t">otherwise they don't work out so this is a mnemonic formula on how to remember how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18940" target="_blank">05:15:40.880</a></span> | <span class="t">compute the gradient of the inputs of a matrix multiplication given the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18946" target="_blank">05:15:46.160</a></span> | <span class="t">with respect to the output of the matrix multiplication and the inputs to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18949" target="_blank">05:15:49.600</a></span> | <span class="t">metric multiplication are the input matrix and the parameter matrix w now we need to derive the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18955" target="_blank">05:15:55.600</a></span> | <span class="t">gradient of the output of the softmax with respect to the input of the softmax because that's another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18961" target="_blank">05:16:01.360</a></span> | <span class="t">operation that we do in our fused attention because we are fusing many operations together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18965" target="_blank">05:16:05.280</a></span> | <span class="t">which is matrix multiplication and the softmax so this is the second ingredient that we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18969" target="_blank">05:16:09.680</a></span> | <span class="t">to understand the backward pass of flash attention so let's do it i will use to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18975" target="_blank">05:16:15.440</a></span> | <span class="t">this derivation i will use the same notation as in the flash attention paper so first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18981" target="_blank">05:16:21.040</a></span> | <span class="t">let's write the title of this stuff which is the gradient through the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=18993" target="_blank">05:16:33.840</a></span> | <span class="t">the first operation that we do in during computation of the attention is we compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19004" target="_blank">05:16:44.400</a></span> | <span class="t">the product of the query multiplied by the transpose of the keys we do in a blockwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19008" target="_blank">05:16:48.320</a></span> | <span class="t">way it means that we do it block by block but it doesn't matter because the end result is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19012" target="_blank">05:16:52.800</a></span> | <span class="t">so we can also we can write s equal to q multiplied by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19018" target="_blank">05:16:58.240</a></span> | <span class="t">and then we apply the softmax to this operation to the result of this operation and we call this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19025" target="_blank">05:17:05.040</a></span> | <span class="t">output p which is the softmax of s and after the uh we have applied the softmax we take the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19035" target="_blank">05:17:15.440</a></span> | <span class="t">of the softmax we multiply it by v to obtain the output so the output is equal to p multiplied by v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19041" target="_blank">05:17:21.520</a></span> | <span class="t">now we need to understand how to because as i said before pytorch autograd works in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19051" target="_blank">05:17:31.280</a></span> | <span class="t">following way pytorch will treat our attention computation as a black box so we will have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19057" target="_blank">05:17:37.040</a></span> | <span class="t">computation graph like the following we will have a query input a key input and a value input which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19064" target="_blank">05:17:44.560</a></span> | <span class="t">are sequences of tokens each one with some embedding dimension these are fed to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19070" target="_blank">05:17:50.640</a></span> | <span class="t">black box called the attention which is our implementation of the attention which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19076" target="_blank">05:17:56.800</a></span> | <span class="t">function that we started coding before this will be fed as input to this node in the computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19083" target="_blank">05:18:03.200</a></span> | <span class="t">graph and the computation graph will output a an output tensor o what pytorch will give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19090" target="_blank">05:18:10.480</a></span> | <span class="t">pytorch will give us the gradient of the loss with respect to the output so as you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19097" target="_blank">05:18:17.680</a></span> | <span class="t">pytorch knocks the door knocks the door at each operator and says if i give you the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19103" target="_blank">05:18:23.920</a></span> | <span class="t">of the loss with respect to your output can you give me the gradient of the loss with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19108" target="_blank">05:18:28.560</a></span> | <span class="t">your inputs and this is what we need to figure out so given the gradient of the loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19114" target="_blank">05:18:34.400</a></span> | <span class="t">to the output we need to understand how to compute the gradient of the loss with respect to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19120" target="_blank">05:18:40.400</a></span> | <span class="t">wq the gradient of the loss with respect to wk the gradient of the loss with respect to wb however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19128" target="_blank">05:18:48.480</a></span> | <span class="t">there is no direct connection between q and o or k and o because there are two intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19134" target="_blank">05:18:54.560</a></span> | <span class="t">operations so one there is a first a matrix multiplication then there is a softmax then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19139" target="_blank">05:18:59.120</a></span> | <span class="t">there is an additional matrix multiplication however we have tools that allow us to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19144" target="_blank">05:19:04.080</a></span> | <span class="t">how the gradient propagates through multiple operations when they are applied in sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19148" target="_blank">05:19:08.880</a></span> | <span class="t">and that's called the chain rule however we have seen that applying the chain rule in its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19154" target="_blank">05:19:14.000</a></span> | <span class="t">naive way by materializing the jacobian is infeasible so we need to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19159" target="_blank">05:19:19.120</a></span> | <span class="t">how to apply the chain rule without materializing the jacobian and that's what we are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19164" target="_blank">05:19:24.480</a></span> | <span class="t">figure out for one of the operations inside of this attention computation which is the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19169" target="_blank">05:19:29.840</a></span> | <span class="t">and that's why we are going to do this derivation which i promise is the last one that we will do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19174" target="_blank">05:19:34.320</a></span> | <span class="t">and then we will finally go to code the backward pass of flash attention we cannot proceed directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19180" target="_blank">05:19:40.000</a></span> | <span class="t">to coding the backward pass of the flash attention because if we look at the formulas on how it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19183" target="_blank">05:19:43.680</a></span> | <span class="t">computed we will not understand how the the derivation comes out okay now we can start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19191" target="_blank">05:19:51.440</a></span> | <span class="t">so let me delete this stuff delete and imagine for simplicity now we apply the softmax to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19201" target="_blank">05:20:01.680</a></span> | <span class="t">row wise to this s matrix so each row is softmaxed independently from the others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19208" target="_blank">05:20:08.640</a></span> | <span class="t">so let's see what happens to one single row of this matrix and for simplicity i will call it s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19217" target="_blank">05:20:17.040</a></span> | <span class="t">so s is a single row of the s matrix i could also call it s of i but if i do it like this we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19226" target="_blank">05:20:26.160</a></span> | <span class="t">have to carry over the index okay guys just just do it we will carry over the index all right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19233" target="_blank">05:20:33.280</a></span> | <span class="t">let's call si one row of the s matrix so si is equal to let's say it's the in tensor notation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19240" target="_blank">05:20:40.720</a></span> | <span class="t">pytorch tensor notation it will be like this so from the matrix s from the tensor s we take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19247" target="_blank">05:20:47.840</a></span> | <span class="t">ith row and all the columns this is the definition of si i know it's very ugly notation but it helps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19253" target="_blank">05:20:53.440</a></span> | <span class="t">you understand and this is a vector of size and dimensions we apply the softmax to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19262" target="_blank">05:21:02.000</a></span> | <span class="t">vector and we will obtain an output vector and we call it pi pi is equal to the softmax softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19272" target="_blank">05:21:12.800</a></span> | <span class="t">of si so as we have seen the softmax operation does not change the shape of the input it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19280" target="_blank">05:21:20.240</a></span> | <span class="t">changed element wise each number so the output will also be a vector of size r to the power of n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19288" target="_blank">05:21:28.240</a></span> | <span class="t">now what is the softmax so the softmax is defined as follows the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19298" target="_blank">05:21:38.480</a></span> | <span class="t">of well p i j so the jth element of the p ith vector is equal to the exponential of the jth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19311" target="_blank">05:21:51.760</a></span> | <span class="t">element of the s ith vector divided by a normalization factor that is computed as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19321" target="_blank">05:22:01.600</a></span> | <span class="t">with let's say not j let's use k in this case not even k let's use l</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19328" target="_blank">05:22:08.160</a></span> | <span class="t">is equal to one up to n of e to the power of s i l all right so first of all you may be wondering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19341" target="_blank">05:22:21.840</a></span> | <span class="t">the softmax that we are that we apply during the forward pass of the computation of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19348" target="_blank">05:22:28.080</a></span> | <span class="t">is not really this softmax because in if you remember what we applied before we were applying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19353" target="_blank">05:22:33.520</a></span> | <span class="t">the softmax where each of the argument of the exponential is reduced by the maximum element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19359" target="_blank">05:22:39.760</a></span> | <span class="t">in the vector to which we apply the softmax so it was more or less like this so s i j minus s i max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19368" target="_blank">05:22:48.240</a></span> | <span class="t">so the maximum element in the s i j s i vector and also the argument of the denominator was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19375" target="_blank">05:22:55.760</a></span> | <span class="t">reduced by s i max however we also proved that this stuff here is equivalent to the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19385" target="_blank">05:23:05.680</a></span> | <span class="t">softmax without this reduction in the argument because this reduction in the argument is only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19391" target="_blank">05:23:11.760</a></span> | <span class="t">added because we want to make it numerically safe to compute but there is it's equivalent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19398" target="_blank">05:23:18.080</a></span> | <span class="t">to do it without but from a mathematical point of view on the computer of course it will become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19403" target="_blank">05:23:23.920</a></span> | <span class="t">numerically unstable but from a mathematical point of view it is the same thing which also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19409" target="_blank">05:23:29.600</a></span> | <span class="t">means that doesn't doesn't matter how you compute the forward pass if it's equivalent to another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19416" target="_blank">05:23:36.320</a></span> | <span class="t">mathematical definition you can always use the other mathematical definition to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19420" target="_blank">05:23:40.160</a></span> | <span class="t">backward pass it will result in the same value if you didn't understand what i said let me give you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19426" target="_blank">05:23:46.080</a></span> | <span class="t">a more simple example which is imagine you have a do you remember the formula from high school</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19434" target="_blank">05:23:54.320</a></span> | <span class="t">this one so cosine cosine of squared of x plus sine squared of x is equal to one now imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19444" target="_blank">05:24:04.480</a></span> | <span class="t">we compute an output y is equal to cosine squared of x and then we need to compute the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19454" target="_blank">05:24:14.000</a></span> | <span class="t">of y with respect to x it doesn't matter if you compute it as the derivative of cosine squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19464" target="_blank">05:24:24.880</a></span> | <span class="t">of x with respect to x or if you compute it as the derivative of one minus sine squared of x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19477" target="_blank">05:24:37.440</a></span> | <span class="t">with respect to x because they will result in exactly the same result because the two definitions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19484" target="_blank">05:24:44.320</a></span> | <span class="t">are equivalent and this is why we don't need to add this this factor in the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19489" target="_blank">05:24:49.360</a></span> | <span class="t">because the two definitions are equivalent mathematically we just use the numerically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19494" target="_blank">05:24:54.560</a></span> | <span class="t">safe one because when computed on the on the computer we need something that is numerically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19500" target="_blank">05:25:00.080</a></span> | <span class="t">stable that will not overflow all right now what do we want to obtain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19507" target="_blank">05:25:07.280</a></span> | <span class="t">so we want to obtain the gradient of the loss with respect to the input vector of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19516" target="_blank">05:25:16.080</a></span> | <span class="t">which is the s_i vector given the gradient of the loss with respect to the output of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19524" target="_blank">05:25:24.160</a></span> | <span class="t">which is the p_i vector and we can obtain that with the chain rule multiply that by the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19533" target="_blank">05:25:33.200</a></span> | <span class="t">p_i with respect to s_i now we the chain rule is always valid let's see what does this jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19547" target="_blank">05:25:47.520</a></span> | <span class="t">look like all right so this jacobian will be the p_i with respect to delta s_i well we need to do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19560" target="_blank">05:26:00.800</a></span> | <span class="t">let's look at what each element in this jacobian will look like so the jth element with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19568" target="_blank">05:26:08.240</a></span> | <span class="t">the let's say the kth element so we are um we are computing the the we are looking at what each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19581" target="_blank">05:26:21.840</a></span> | <span class="t">element in this jacobian will look like which is what is the jacobian it's each element in the out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19588" target="_blank">05:26:28.320</a></span> | <span class="t">in the numerator of the jacobian derived with respect to each element in the denominator of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19593" target="_blank">05:26:33.840</a></span> | <span class="t">jacobian in this fraction here so we are saying for each element in the output vector derived</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19602" target="_blank">05:26:42.480</a></span> | <span class="t">with respect to each element in the input vector this is what we are writing here so what is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19608" target="_blank">05:26:48.880</a></span> | <span class="t">is the output vector obtained well p_ij we know that it is equal to by the definition of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19615" target="_blank">05:26:55.040</a></span> | <span class="t">softmax is obtained as follows so e to the power of s_ij divided by the normalization factor let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19627" target="_blank">05:27:07.120</a></span> | <span class="t">call it l is equal to one to n e to the power of s_il all derived with respect to s_ik</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19644" target="_blank">05:27:24.160</a></span> | <span class="t">i k so what we are trying to do is we know that the p vector is suppose it's a vector with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19652" target="_blank">05:27:32.400</a></span> | <span class="t">three elements so this is p_1 this is well p_11 p_12 and p_13 the s vector will be a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19665" target="_blank">05:27:45.440</a></span> | <span class="t">also with the three elements so it will be the s_11 s_12 and s_13 what we are trying to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19674" target="_blank">05:27:54.400</a></span> | <span class="t">the calculate what the jacobian will be the derivative of this one with respect to all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19678" target="_blank">05:27:58.480</a></span> | <span class="t">the input vector then then the second row of the jacobian will be the derivative of this one with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19683" target="_blank">05:28:03.600</a></span> | <span class="t">respect to each of this input element then the third row of the jacobian will be this stuff here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19689" target="_blank">05:28:09.200</a></span> | <span class="t">with respect to the derived with respect to each of the input element of the s vector we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19694" target="_blank">05:28:14.720</a></span> | <span class="t">to understand what does the generic element in this jacobian look like based on the j date element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19700" target="_blank">05:28:20.800</a></span> | <span class="t">of the output vector so this j index refers to the output vector and the kth element in the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19706" target="_blank">05:28:26.000</a></span> | <span class="t">vector all right so what can happen when we do this jacobian is that we have a this one here is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19717" target="_blank">05:28:37.200</a></span> | <span class="t">derivative of a fraction of two functions and we know from high school that the derivative of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19724" target="_blank">05:28:44.400</a></span> | <span class="t">fraction of two functions is as follows so the derivative of the derivative let me write like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19732" target="_blank">05:28:52.160</a></span> | <span class="t">this of f of x with respect to g of x prime is equal to with respect to x by the way is equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19744" target="_blank">05:29:04.240</a></span> | <span class="t">f prime oops of x multiplied by g of x minus g prime of x f of x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19758" target="_blank">05:29:18.880</a></span> | <span class="t">all divided by the g of x to the power of two like this now let's apply it here so this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19768" target="_blank">05:29:28.160</a></span> | <span class="t">become here we will have two cases either the variable that we are deriving with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19773" target="_blank">05:29:33.840</a></span> | <span class="t">so this sik has the same index as the variable being derived so either we are doing a p11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19781" target="_blank">05:29:41.840</a></span> | <span class="t">with respect to s11 or we are doing a p11 with respect to something else that has not the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19787" target="_blank">05:29:47.440</a></span> | <span class="t">index so like p11 with respect to s12 or s13 so there are two cases that we need to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19793" target="_blank">05:29:53.280</a></span> | <span class="t">suppose that we are deriving p11 with respect to s11 or we are deriving p12 with respect to s12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19799" target="_blank">05:29:59.360</a></span> | <span class="t">or we are deriving p13 with respect to s13 so we are deriving the element of the output with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19805" target="_blank">05:30:05.360</a></span> | <span class="t">respect to the same element in the input with the same index so in this case the this this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19814" target="_blank">05:30:14.880</a></span> | <span class="t">derivative will look like the following so it's the derivative of f so the numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19821" target="_blank">05:30:21.920</a></span> | <span class="t">with respect to the denominator that has the same index so we are saying that in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19827" target="_blank">05:30:27.360</a></span> | <span class="t">j is equal to k so the numerator with respect to sij with respect to e to the power of sij</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19840" target="_blank">05:30:40.720</a></span> | <span class="t">with respect to sij will be e to the power of sij so because e to the power of x1 with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19848" target="_blank">05:30:48.960</a></span> | <span class="t">x1 will be e to the power of x1 so this is equal to i am reducing the size now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19854" target="_blank">05:30:54.480</a></span> | <span class="t">e to the power of sij then we need to multiply that by the denominator of the fraction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19865" target="_blank">05:31:05.360</a></span> | <span class="t">which is this summation here so the summation over all possible l of e to the power of sil</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19874" target="_blank">05:31:14.320</a></span> | <span class="t">minus the derivative of the denominator with respect to the variable being derived so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19884" target="_blank">05:31:24.000</a></span> | <span class="t">denominator is the sum of all the exponentials of all the input elements if we derive it with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19890" target="_blank">05:31:30.400</a></span> | <span class="t">respect to one particular input element there will be at least one term that contains that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19895" target="_blank">05:31:35.040</a></span> | <span class="t">input element and so the all the other terms will result in zero so the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19900" target="_blank">05:31:40.400</a></span> | <span class="t">derivative that will survive will be the e to the power of sik with respect to sik</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19905" target="_blank">05:31:45.520</a></span> | <span class="t">so we write minus e to the power of sik</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19917" target="_blank">05:31:57.280</a></span> | <span class="t">multiplied by the numerator which is e to the power of sij</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19920" target="_blank">05:32:00.480</a></span> | <span class="t">all this divided by the denominator to the power of two which is this summation here so l equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19932" target="_blank">05:32:12.080</a></span> | <span class="t">one up to n e to the power of sil all to the power of two and this stuff here will be equal to well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19942" target="_blank">05:32:22.400</a></span> | <span class="t">we can see that they this two term this one and this one have a one term factor in common which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19948" target="_blank">05:32:28.480</a></span> | <span class="t">is e to the power of sij so we can collect that so e to the power of sij multiplied by the summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19958" target="_blank">05:32:38.320</a></span> | <span class="t">minus e to the power of sik</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19971" target="_blank">05:32:51.040</a></span> | <span class="t">all this divided by the denominator which is the power of two of this stuff here so let me just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19977" target="_blank">05:32:57.440</a></span> | <span class="t">copy and paste it which is let me rotate it also because i don't know why i always write little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19982" target="_blank">05:33:02.000</a></span> | <span class="t">little yeah all right and this stuff here is equal to well we can separate the two terms so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=19994" target="_blank">05:33:14.000</a></span> | <span class="t">we can separate this term here and this term here because the denominator is to the power of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20000" target="_blank">05:33:20.560</a></span> | <span class="t">so we can write it also as e to the power of sij divided by the denominator so which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20008" target="_blank">05:33:28.960</a></span> | <span class="t">summation of l equal one to n e to the power of sil multiplied by this stuff here so this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20020" target="_blank">05:33:40.960</a></span> | <span class="t">here divided by the same denominator so there's summation of l equal one up to n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20029" target="_blank">05:33:49.520</a></span> | <span class="t">e to the power of sil minus e to the power of sik i am sik divided by the same denominator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20048" target="_blank">05:34:08.400</a></span> | <span class="t">sil now this one can be written as this stuff here is nothing more than the output element pij</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20058" target="_blank">05:34:18.480</a></span> | <span class="t">because this one is just the softmax applied to the sij element which we know that the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20065" target="_blank">05:34:25.120</a></span> | <span class="t">the softmax applied to the sij element is called pij because it's one element of the output vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20070" target="_blank">05:34:30.480</a></span> | <span class="t">which we call the p so this stuff here is equal to pij multiplied by this stuff here will be equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20079" target="_blank">05:34:39.440</a></span> | <span class="t">to one minus this stuff here what is this stuff here is the output of the softmax applied to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20086" target="_blank">05:34:46.560</a></span> | <span class="t">sik element so it will be pik so it is equal to one minus pik okay and this is in the case the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20099" target="_blank">05:34:59.360</a></span> | <span class="t">variable with respect to which we derive has the same index as the numerator in this fraction here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20109" target="_blank">05:35:09.520</a></span> | <span class="t">in this derivative here the other case is when the two variables so the output the index of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20117" target="_blank">05:35:17.680</a></span> | <span class="t">output with respect to the index of the input are not the same in this case we will have another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20123" target="_blank">05:35:23.680</a></span> | <span class="t">case so we will have that j let me write it again so this stuff here i hope i can copy it all without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20135" target="_blank">05:35:35.840</a></span> | <span class="t">in the other case in which s is not equal to j</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20139" target="_blank">05:35:39.120</a></span> | <span class="t">uh yes it's j not equal to k so j is not equal to k what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20147" target="_blank">05:35:47.840</a></span> | <span class="t">in this case it will be well the derivative of the numerator because we need to apply again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20155" target="_blank">05:35:55.760</a></span> | <span class="t">this formula here so derivative of the numerator with respect to something that is not the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20160" target="_blank">05:36:00.640</a></span> | <span class="t">variable it will be zero because it's like computing the derivative e to the power of x1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20166" target="_blank">05:36:06.400</a></span> | <span class="t">with respect to x2 it will be zero so it will be zero so all the first term here will become zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20174" target="_blank">05:36:14.000</a></span> | <span class="t">no matter what is g of x minus the derivative of the denominator of this fraction here with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20181" target="_blank">05:36:21.680</a></span> | <span class="t">to the variable sik g prime of sik so this is all the variable in the input and we are deriving it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20191" target="_blank">05:36:31.920</a></span> | <span class="t">with respect to one particular variable of the input so only one item in the summation will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20197" target="_blank">05:36:37.360</a></span> | <span class="t">survive so it will be the item sik so it will be e to the power of sik multiplied by f of x which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20210" target="_blank">05:36:50.160</a></span> | <span class="t">is the numerator in this fraction which is e to the power oh we forgot a minus e to the power of sij</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20216" target="_blank">05:36:56.240</a></span> | <span class="t">let me see if i forgot something all divided by the denominator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20222" target="_blank">05:37:02.800</a></span> | <span class="t">of this fraction here to the power of two so it is equal to the summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20231" target="_blank">05:37:11.040</a></span> | <span class="t">l equal one up to n of e to the power of sil all to the power of two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20238" target="_blank">05:37:18.160</a></span> | <span class="t">i believe i didn't forget anything so let's continue so here also we can see that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20246" target="_blank">05:37:26.640</a></span> | <span class="t">one here is because uh okay let's separate it minus e to the power of sik divided by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20256" target="_blank">05:37:36.400</a></span> | <span class="t">the summation l equal one up to n of e to the power of sil multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20263" target="_blank">05:37:43.840</a></span> | <span class="t">e to the power of sij divided by the summation l equal one up to n of e to the power of sil</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20276" target="_blank">05:37:56.160</a></span> | <span class="t">this stuff here is nothing more than the softmax applied to the kth element of the si vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20282" target="_blank">05:38:02.880</a></span> | <span class="t">this one here is nothing more than the softmax applied to the jth element of the si vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20288" target="_blank">05:38:08.480</a></span> | <span class="t">so we know what these are we know that we call them p minus pik pij</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20297" target="_blank">05:38:17.920</a></span> | <span class="t">so in the end we have two cases one is the derivative of this stuff here looks like the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20310" target="_blank">05:38:30.160</a></span> | <span class="t">each item in the jacobian looks like the following when the numerator and the denominator have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20315" target="_blank">05:38:35.280</a></span> | <span class="t">same index so j equal to k this stuff here is equal to now this notation here is wrong so i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20323" target="_blank">05:38:43.760</a></span> | <span class="t">shouldn't be writing it with the equal sign but doesn't matter guys it's we are doing a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20330" target="_blank">05:38:50.560</a></span> | <span class="t">okay so pij pij multiplied by one minus pik let me check yes the other case is when the j</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20342" target="_blank">05:39:02.480</a></span> | <span class="t">is not equal to k then this stuff here let me write it like this will be equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20349" target="_blank">05:39:09.920</a></span> | <span class="t">minus pik multiplied pij now that we know what the two typical cases of this jacobian look like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20361" target="_blank">05:39:21.760</a></span> | <span class="t">let's actually look at what this jacobian look like in the matrix form so this jacobian will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20368" target="_blank">05:39:28.880</a></span> | <span class="t">look like the following it will be a matrix that is more or less like the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20376" target="_blank">05:39:36.320</a></span> | <span class="t">it will be an n by n matrix where n is the size of the input vector and the output vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20382" target="_blank">05:39:42.080</a></span> | <span class="t">at here the first element of the jacobian as you saw as you remember the first row of the jacobian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20390" target="_blank">05:39:50.640</a></span> | <span class="t">in the numerator convention is the derivative of the first output with respect to all the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20398" target="_blank">05:39:58.160</a></span> | <span class="t">so this first term here will be the derivative of p11 with respect to s11 so in this case j and k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20408" target="_blank">05:40:08.080</a></span> | <span class="t">match so we know that it will be equal to p11 multiplied by 1 minus p11 the second element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20416" target="_blank">05:40:16.400</a></span> | <span class="t">to the right of this one so the element one two will be the derivative of p12 with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20423" target="_blank">05:40:23.600</a></span> | <span class="t">sorry the p11 with respect to s12 the j and k do not match so we will be in this case here so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20430" target="_blank">05:40:30.240</a></span> | <span class="t">will be minus p11 p12 the third element you can check it by yourself it will be minus p11 p13</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20442" target="_blank">05:40:42.000</a></span> | <span class="t">blah blah blah until the end which will be minus p11 p1n the second row of this jacobian will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20452" target="_blank">05:40:52.000</a></span> | <span class="t">will look like this so it will be the derivative of p12 with respect to s11 the j and k do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20458" target="_blank">05:40:58.880</a></span> | <span class="t">match so we are in this case here so it will be minus p12 p11 then the next element it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20469" target="_blank">05:41:09.440</a></span> | <span class="t">the derivative of p12 with respect to s12 so j and k match so we are in the first case so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20476" target="_blank">05:41:16.880</a></span> | <span class="t">p12 multiplied by 1 minus p12 then this stuff here will be equal to then the third element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20485" target="_blank">05:41:25.840</a></span> | <span class="t">will be minus p12 with respect to p13 blah blah blah and until we arrive to the last one which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20493" target="_blank">05:41:33.680</a></span> | <span class="t">is minus p12 with respect to p1n not with respect to multiplied by b1 and all the elements like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20501" target="_blank">05:41:41.440</a></span> | <span class="t">this until the last row the last row will be the the first element of the last row will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20506" target="_blank">05:41:46.160</a></span> | <span class="t">derivative of the last output element with respect to the first input element so it will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20513" target="_blank">05:41:53.520</a></span> | <span class="t">derivative of p1n with respect to s11 so the two indices do not match so we are in the second case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20522" target="_blank">05:42:02.880</a></span> | <span class="t">so it will be minus p1n p11 this will be minus p1n p12 etc etc etc now let me do also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20536" target="_blank">05:42:16.000</a></span> | <span class="t">third element since we are here so minus p1n p13 etc etc etc until the last element of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20544" target="_blank">05:42:24.560</a></span> | <span class="t">last row which will be minus p1n p1n i guess oh oh no that's wrong guys because the two indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20555" target="_blank">05:42:35.440</a></span> | <span class="t">match so it should be p1n multiplied by 1 minus p1n this is what the jacobian will look like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20565" target="_blank">05:42:45.360</a></span> | <span class="t">let's see if we can find a better how to generate this jacobian with some pattern recognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20571" target="_blank">05:42:51.920</a></span> | <span class="t">let's write it in a different way first of all the thing first thing that we can notice is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20577" target="_blank">05:42:57.600</a></span> | <span class="t">this jacobian is symmetric so you can see that this element is equal to this element if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20582" target="_blank">05:43:02.000</a></span> | <span class="t">expand the third row you will see that it's equal to this element this one on the top right corner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20587" target="_blank">05:43:07.040</a></span> | <span class="t">is equal to the one in the top bottom left corner so this matrix is symmetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20594" target="_blank">05:43:14.720</a></span> | <span class="t">the second thing that we can notice is that only the element in the diagonal are different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20600" target="_blank">05:43:20.880</a></span> | <span class="t">they have an additional term because you can look at this element here so let me write this element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20608" target="_blank">05:43:28.320</a></span> | <span class="t">here can also be written as p11 minus p11 multiplied by p11 the second element here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20617" target="_blank">05:43:37.600</a></span> | <span class="t">in the second row so the second diagonal element of this matrix is p12 minus p12 multiplied by p12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20628" target="_blank">05:43:48.320</a></span> | <span class="t">so this element on the diagonal actually look like just like the other elements they just have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20633" target="_blank">05:43:53.920</a></span> | <span class="t">an additional term which is p11 in the first diagonal element p12 in the second diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20642" target="_blank">05:44:02.560</a></span> | <span class="t">element so we can also say that this matrix here is the product of all the possible combinations of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20651" target="_blank">05:44:11.200</a></span> | <span class="t">p_ij with p_ik which we can obtain with an outer product or even with the product of one column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20660" target="_blank">05:44:20.320</a></span> | <span class="t">with the transpose of the same column so if you do one column vector for example imagine p is a column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20667" target="_blank">05:44:27.840</a></span> | <span class="t">vector and you do p multiplied by p_t you obtain all the possible combinations of products of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20674" target="_blank">05:44:34.640</a></span> | <span class="t">two vectors because this will be one i can do a simple case so p11 p1 let's call it p2 p3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20683" target="_blank">05:44:43.040</a></span> | <span class="t">multiplied by the row vector p1 p2 p3 this will generate all the possible combinations of products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20693" target="_blank">05:44:53.440</a></span> | <span class="t">between p1 and the p the first vector and the second vector because this will be a three by one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20699" target="_blank">05:44:59.680</a></span> | <span class="t">this is one by three so it will be generated three by three vector and it will be equal to p1 p1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20706" target="_blank">05:45:06.960</a></span> | <span class="t">p1 p2 p1 p2 p1 p3 etc etc etc moreover we can see that in the diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20718" target="_blank">05:45:18.480</a></span> | <span class="t">of the matrix we have this additional term this additional term p1 in the first diagonal element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20726" target="_blank">05:45:26.880</a></span> | <span class="t">p1 p12 in the second diagonal element p13 in the third diagonal element i actually call it p1 it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20734" target="_blank">05:45:34.720</a></span> | <span class="t">wrong because i should call it p_i that's why i didn't want to bring the i indices so it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20740" target="_blank">05:45:40.000</a></span> | <span class="t">really p1 it should be p_i p_i p_i p_i because we are doing it for the generic height p_i vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20749" target="_blank">05:45:49.040</a></span> | <span class="t">so let me fix the indices p_i_n p_i_3 this is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20765" target="_blank">05:46:05.440</a></span> | <span class="t">p_i and p_i okay so this is p_i p_i p_i p_i p_i p_i p_i okay we can obtain so we can write the this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20781" target="_blank">05:46:21.440</a></span> | <span class="t">this jacobian here also as the diagonal matrix that in the diagonal has all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20790" target="_blank">05:46:30.240</a></span> | <span class="t">element of the p_i vector minus the p vector multiplied by the transpose of itself so with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20798" target="_blank">05:46:38.800</a></span> | <span class="t">itself but transposed because we need all the elements to be kind of a combination of one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20804" target="_blank">05:46:44.720</a></span> | <span class="t">element of p with itself with another element of p plus only on the diagonal we need some this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20810" target="_blank">05:46:50.320</a></span> | <span class="t">additional term which are the elements of p and all the elements of the the the output of this p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20816" target="_blank">05:46:56.800</a></span> | <span class="t">multiplied by p transposed are negated that's why we need this minus sign so if you look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20821" target="_blank">05:47:01.840</a></span> | <span class="t">flash attention paper they give you this formula here they say that if y is equal to the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20828" target="_blank">05:47:08.160</a></span> | <span class="t">of x then the jacobian will look like the following will be diagonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20841" target="_blank">05:47:21.360</a></span> | <span class="t">of y minus y y transposed where y is the is a column vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20852" target="_blank">05:47:32.080</a></span> | <span class="t">all right guys i know this has been long so let's take a pause and we are going to now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20858" target="_blank">05:47:38.320</a></span> | <span class="t">code finally first of all let's check the mathematics of the backward path of flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20864" target="_blank">05:47:44.160</a></span> | <span class="t">attention we will see it briefly i will not do any more derivation but i will explain it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20869" target="_blank">05:47:49.920</a></span> | <span class="t">and then we finally switch to coding it so let's go all right guys now finally we can see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20877" target="_blank">05:47:57.120</a></span> | <span class="t">the backward path of the flash attention so we will be looking at the algorithm and if you look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20883" target="_blank">05:48:03.760</a></span> | <span class="t">at the the the appendix of the flash attention paper you will see this part b.2 where they derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20889" target="_blank">05:48:09.680</a></span> | <span class="t">the backward path step by step now i don't want to do all the same all the steps of this derivation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20897" target="_blank">05:48:17.120</a></span> | <span class="t">because it's going to be too long but i want to give you all the tools necessary to understand it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20901" target="_blank">05:48:21.920</a></span> | <span class="t">now let's start from what kind of what say conventions they are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20910" target="_blank">05:48:30.240</a></span> | <span class="t">notations they are using in this paper so the first thing that we need to rehearse is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20916" target="_blank">05:48:36.640</a></span> | <span class="t">naming of what is what is the name of each matrix as you know in the forward attention in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20924" target="_blank">05:48:44.080</a></span> | <span class="t">forward pass we do the query multiply by the transpose of the key and the output of this we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20928" target="_blank">05:48:48.400</a></span> | <span class="t">call it s then we apply the softmax to this s matrix and it becomes the p matrix the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20935" target="_blank">05:48:55.840</a></span> | <span class="t">is applied by rows then we talk take this p matrix and we multiply by a v matrix to obtain the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20942" target="_blank">05:49:02.560</a></span> | <span class="t">of the attention let's look at for example how the computation of the height row of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20952" target="_blank">05:49:12.080</a></span> | <span class="t">is computed based on the p matrix and the v matrix so we can understand this kind of notation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20957" target="_blank">05:49:17.120</a></span> | <span class="t">they are using here in the paper because the way i read this formula here is the height</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20962" target="_blank">05:49:22.320</a></span> | <span class="t">row of the output which is a column vector because in when we write in in mathematics in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20968" target="_blank">05:49:28.560</a></span> | <span class="t">linear algebra whenever we write the name of a vector it is always by convention a column vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20975" target="_blank">05:49:35.440</a></span> | <span class="t">but the origin of this particular vector is actually a row of the output matrix let's try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20982" target="_blank">05:49:42.320</a></span> | <span class="t">to understand what is the output row of a matrix in a matrix multiplication now so that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20990" target="_blank">05:49:50.960</a></span> | <span class="t">understand how to go from here to here so let's write a generic matrix multiplication for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=20998" target="_blank">05:49:58.400</a></span> | <span class="t">an a matrix let's say that it is the following and we only write one row actually let me zoom again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21007" target="_blank">05:50:07.440</a></span> | <span class="t">and i want to write smaller so we have enough space so we make a matrix that has a row let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21014" target="_blank">05:50:14.000</a></span> | <span class="t">call it a 1 a 2 a 3 and then we multiply this will be a matrix with many rows like the this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21023" target="_blank">05:50:23.760</a></span> | <span class="t">because we want to study the effect only of one row and we multiply it by another matrix let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21030" target="_blank">05:50:30.000</a></span> | <span class="t">call it this one is the matrix a and it has i don't know let's say n rows by three columns then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21039" target="_blank">05:50:39.200</a></span> | <span class="t">we should have another matrix b with three columns and some number of three rows and some number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21047" target="_blank">05:50:47.280</a></span> | <span class="t">column let's say four columns so we call the first row let's call it let me zoom a more so it's b11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21058" target="_blank">05:50:58.640</a></span> | <span class="t">b12 b13 b14 then this one should be b21 b22 b23 b24 this should be b31 b32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21077" target="_blank">05:51:17.120</a></span> | <span class="t">b33 b34 etc i know i am not very rigorous in my notation i should have called all these elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21086" target="_blank">05:51:26.960</a></span> | <span class="t">with the capital letter a and the capital letter b so this is the notation that you use when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21091" target="_blank">05:51:31.920</a></span> | <span class="t">referring to single item of a matrix but please forgive me for this so the output of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21100" target="_blank">05:51:40.080</a></span> | <span class="t">multiplication will be another matrix that is n by 4 so it will be n by 4 so we will have four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21108" target="_blank">05:51:48.640</a></span> | <span class="t">columns for each row of the output i want to write the output in a different way so i want to write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21118" target="_blank">05:51:58.400</a></span> | <span class="t">it as follows as a vector only so the first output row as a vector and want to understand what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21126" target="_blank">05:52:06.560</a></span> | <span class="t">each dimension of this vector so because otherwise i don't have enough space to write it here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21131" target="_blank">05:52:11.680</a></span> | <span class="t">so the first let's write it so let's call it o i want to write what is o of one which is the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21142" target="_blank">05:52:22.720</a></span> | <span class="t">row of the output but written as a column vector so o of one will be here we should use the small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21152" target="_blank">05:52:32.400</a></span> | <span class="t">letter o of one should be a vector where the first dimension is the dot product of this stuff here so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21160" target="_blank">05:52:40.320</a></span> | <span class="t">the first row of the a matrix with the first column of the b matrix so the first let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21167" target="_blank">05:52:47.760</a></span> | <span class="t">dimension will be a1 with b11 i should also call this one a11 a12 actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21180" target="_blank">05:53:00.160</a></span> | <span class="t">and a13 so a13 because we have many rows in the a matrix so let me use the correct naming so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21190" target="_blank">05:53:10.240</a></span> | <span class="t">will be a11 with b11 a11 b11 plus a12 multiplied by b21 plus a13 with b31 and this will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21208" target="_blank">05:53:28.000</a></span> | <span class="t">first dimension of the first row of the output matrix o the second dimension of the first row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21216" target="_blank">05:53:36.000</a></span> | <span class="t">of the output matrix o will be the dot product of this row of the a matrix with the second column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21223" target="_blank">05:53:43.040</a></span> | <span class="t">of the b matrix and let me write here b so it will be a11 b12 plus a12 b22 plus a13 b32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21243" target="_blank">05:54:03.040</a></span> | <span class="t">the third dimension will be a11 b13 plus a12 b23 plus a13 b33 the fourth dimension will be a11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21264" target="_blank">05:54:24.080</a></span> | <span class="t">b14 plus a12 b24 plus a13 b34 now this is the output the first output row of the o matrix and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21280" target="_blank">05:54:40.160</a></span> | <span class="t">it's a vector called o1 and these are this is the first dimension of this vector this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21285" target="_blank">05:54:45.120</a></span> | <span class="t">is the second this was the third and this is the fourth dimension and each of this stuff here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21289" target="_blank">05:54:49.280</a></span> | <span class="t">one scalar um so the output o1 which is the first row of the output matrix can also be written as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21301" target="_blank">05:55:01.520</a></span> | <span class="t">the first element as you can see in is a sum of many vectors where the first element is a11</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21313" target="_blank">05:55:13.040</a></span> | <span class="t">multiplied let me use a smaller this one but i want to use a smaller i can't change the size here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21319" target="_blank">05:55:19.760</a></span> | <span class="t">okay it doesn't matter so as you can see here there is a1 multiplying a different b number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21326" target="_blank">05:55:26.880</a></span> | <span class="t">every time so this is a b11 b12 b13 b14 what is b11 b12 b13 b14 it is the first row of the b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21336" target="_blank">05:55:36.400</a></span> | <span class="t">matrix so it is equal to b1 and all the dimensions of the first row then plus then we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21346" target="_blank">05:55:46.720</a></span> | <span class="t">element a12 multiplied by b21 b22 b23 etc etc and this is the second row of the b matrix so we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21357" target="_blank">05:55:57.440</a></span> | <span class="t">the tensor notation of pytorch to describe this row which is a b2 and all the dimensions of b2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21367" target="_blank">05:56:07.280</a></span> | <span class="t">so it looks this is a vector scalar product and plus a13 multiplied by b3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21382" target="_blank">05:56:22.320</a></span> | <span class="t">and all the dimensions of b3 this one can also be written as the summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21389" target="_blank">05:56:29.280</a></span> | <span class="t">over all possible i that go from 1 to 3 where 1 to 3 is how many columns there are in the a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21402" target="_blank">05:56:42.400</a></span> | <span class="t">of a ij well a1 let's call let's call this one j actually sorry let's call it j</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21413" target="_blank">05:56:53.360</a></span> | <span class="t">equal to 1 and let's call this the generic ith row of the output matrix will be a i1 a i2 and a i3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21427" target="_blank">05:57:07.840</a></span> | <span class="t">each one multiplied by the corresponding row in the b matrix so we can write it as a i j</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21436" target="_blank">05:57:16.560</a></span> | <span class="t">multiplied by b j where b j is the a row of b we can also write it like this to indicate that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21449" target="_blank">05:57:29.920</a></span> | <span class="t">is a vector and this is exactly what they do here so the output in the output matrix when we do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21457" target="_blank">05:57:37.360</a></span> | <span class="t">multiplication p multiplied by v the ith row of the output matrix we call it o i which is a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21466" target="_blank">05:57:46.320</a></span> | <span class="t">but by notation it is a column vector where the elements of this column vector are actually the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21472" target="_blank">05:57:52.160</a></span> | <span class="t">elements of the ith row of o this is only by notation guys is equal to the ith row of p so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21482" target="_blank">05:58:02.160</a></span> | <span class="t">the ith row of the matrix that is on the left in the matrix multiplication multiplied by all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21487" target="_blank">05:58:07.440</a></span> | <span class="t">columns of the v matrix which can also be written as the summation over all the elements of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21493" target="_blank">05:58:13.680</a></span> | <span class="t">ith row of p so all the elements of the ith row of the first matrix the one on the left in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21499" target="_blank">05:58:19.280</a></span> | <span class="t">matrix multiplication multiplied by each vector in the v matrix where the jth matrix here in v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21507" target="_blank">05:58:27.200</a></span> | <span class="t">is each row of the v matrix so and p i j can also be written as p i j is what is the the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21518" target="_blank">05:58:38.480</a></span> | <span class="t">the softmax so as you know the output of the softmax is e to the power of l the element input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21524" target="_blank">05:58:44.640</a></span> | <span class="t">of the softmax what is the element input of the softmax is the query multiplied by the transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21529" target="_blank">05:58:49.280</a></span> | <span class="t">of the keys so it's a dot product between one query and one key and that's why you have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21534" target="_blank">05:58:54.080</a></span> | <span class="t">stuff here in the exponential so this is the first step in understanding this derivation another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21540" target="_blank">05:59:00.160</a></span> | <span class="t">thing that we have studied so far is how to derive the backward path of the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21546" target="_blank">05:59:06.560</a></span> | <span class="t">and of the softmax so now let's use it in the matrix multiplication let's rehearse the formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21553" target="_blank">05:59:13.600</a></span> | <span class="t">so if given a matrix multiplication that is y equal to x multiplied by w we know that given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21561" target="_blank">05:59:21.600</a></span> | <span class="t">the gradient of the loss function with respect to y so the output of this operation we know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21567" target="_blank">05:59:27.600</a></span> | <span class="t">to derive the gradient of the loss with respect to one of the input of this function which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21573" target="_blank">05:59:33.200</a></span> | <span class="t">x or w to get the gradient with respect to x we need to take the upstream gradient so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21580" target="_blank">05:59:40.080</a></span> | <span class="t">the gradient with respect to the output multiplied by the transpose of w t and to get the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21587" target="_blank">05:59:47.520</a></span> | <span class="t">with respect to w we need to do the xt so the input transposed multiplied by the upstream gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21595" target="_blank">05:59:55.600</a></span> | <span class="t">this one is the formula that we didn't derive and this one is the formula that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21599" target="_blank">05:59:59.440</a></span> | <span class="t">we derived but how to derive them is exactly the same procedure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21603" target="_blank">06:00:03.040</a></span> | <span class="t">in attention we are doing the last product that we are doing is o equal to p multiplied by v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21612" target="_blank">06:00:12.880</a></span> | <span class="t">what pytorch will give us as input during the backward path is the gradient of the loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21618" target="_blank">06:00:18.880</a></span> | <span class="t">respect to the output and we need to use this gradient of the loss with respect to the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21623" target="_blank">06:00:23.680</a></span> | <span class="t">of the attention to derive the gradient of the loss with respect to q with respect to k and with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21629" target="_blank">06:00:29.600</a></span> | <span class="t">respect to v so that it can then be used by the operators in the backward path in the in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21635" target="_blank">06:00:35.920</a></span> | <span class="t">computation graph in the operations before okay so but in order to arrive to the gradient with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21643" target="_blank">06:00:43.040</a></span> | <span class="t">respect to query key and value we need to derive the gradient with respect to each intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21648" target="_blank">06:00:48.480</a></span> | <span class="t">operation so the last operation that we do is o equal to p multiplied by v so the gradient with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21655" target="_blank">06:00:55.840</a></span> | <span class="t">respect to o of the loss with respect to v given the gradient of the loss with respect to o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21664" target="_blank">06:01:04.160</a></span> | <span class="t">it is exactly like computing the gradient of the of the loss with respect to x in a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21670" target="_blank">06:01:10.400</a></span> | <span class="t">multiplication and we know that it is equal to pt so just by analogy guys so this is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21677" target="_blank">06:01:17.280</a></span> | <span class="t">reference point and i am just changing the names here and you should understand what is the analogy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21681" target="_blank">06:01:21.920</a></span> | <span class="t">here so the gradient of the loss with respect to v which is the matrix on the right which is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21688" target="_blank">06:01:28.960</a></span> | <span class="t">computing it with respect to w it is equal to just like this formula here so the transpose of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21694" target="_blank">06:01:34.960</a></span> | <span class="t">the matrix on the left multiplied by the upstream gradient which in the paper they write it as this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21700" target="_blank">06:01:40.960</a></span> | <span class="t">so dv is equal to pt multiplied by do and it's the formula that you said you can see here the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21708" target="_blank">06:01:48.080</a></span> | <span class="t">derivation is how to derive the gradient with respect to dp dp is just like deriving the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21714" target="_blank">06:01:54.400</a></span> | <span class="t">gradient of the loss with respect to the matrix that is on the left side of the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21719" target="_blank">06:01:59.120</a></span> | <span class="t">so it is just like deriving the gradient of the loss with respect to x in the reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21724" target="_blank">06:02:04.400</a></span> | <span class="t">formulas which is equal to the upstream gradient multiplied by the transpose of the other matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21731" target="_blank">06:02:11.840</a></span> | <span class="t">which in the notation of the paper they write it as dp is equal to do multiplied by v transposed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21738" target="_blank">06:02:18.000</a></span> | <span class="t">and it's this formula here how they compute this stuff here is exactly as above so as this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21745" target="_blank">06:02:25.520</a></span> | <span class="t">derivation here they call vj the jth row of the v matrix and they write it as pij multiplied by do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21757" target="_blank">06:02:37.680</a></span> | <span class="t">how to arrive to this formula here well let's do it so let me write let's see okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21765" target="_blank">06:02:45.680</a></span> | <span class="t">theoretically we know that from this derivation here so from this derivation here or from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21771" target="_blank">06:02:51.440</a></span> | <span class="t">derivation here we know that the i-th row of the output in a matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21777" target="_blank">06:02:57.360</a></span> | <span class="t">first of all let's simplify our life every time you see a transpose and you don't like work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21782" target="_blank">06:03:02.000</a></span> | <span class="t">the transpose in a matrix multiplication just give it a different name and then work with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21786" target="_blank">06:03:06.640</a></span> | <span class="t">different name and after when you have derived the formula you resubstitute the transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21792" target="_blank">06:03:12.480</a></span> | <span class="t">operation in this case we are doing dv is equal to p transpose multiplied by do let's call p</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21799" target="_blank">06:03:19.920</a></span> | <span class="t">transposed let's give it a name that we are we didn't use so far so let's call it f i always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21804" target="_blank">06:03:24.800</a></span> | <span class="t">use f when it's available so we call dv is equal to f do we know from above here from this derivation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21818" target="_blank">06:03:38.720</a></span> | <span class="t">here or this derivation here is equivalent that the output of a matrix multiplication so the out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21825" target="_blank">06:03:45.920</a></span> | <span class="t">i-th row of the let's know the j-th row let's call it the j-th row dvj is equal to a summation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21835" target="_blank">06:03:55.680</a></span> | <span class="t">of each element of the j-th row of f of the first matrix so we do the let's see here we do the sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21846" target="_blank">06:04:06.720</a></span> | <span class="t">by i so let's do it by i it's the sum over all possible i of the i-th element in the j-th</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21856" target="_blank">06:04:16.880</a></span> | <span class="t">row of the first matrix so fji multiplied dot product not dot product this is a scalar vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21872" target="_blank">06:04:32.800</a></span> | <span class="t">multiplication multiplied by a vector that is let me check what was the formula so it was the j-th</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21880" target="_blank">06:04:40.000</a></span> | <span class="t">row of the other matrix so in this case it should be the i-th row of the other matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21885" target="_blank">06:04:45.760</a></span> | <span class="t">o of i where i this is the i-th row of i this is the j-th row of the v matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21900" target="_blank">06:05:00.480</a></span> | <span class="t">and but also we don't we know that f is not a matrix that we have it's actually the transpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21906" target="_blank">06:05:06.400</a></span> | <span class="t">of p which means that fji will be equal to pij because in a matrix transposition you invert the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21913" target="_blank">06:05:13.440</a></span> | <span class="t">two indices so this is the summation over all possible i's of p not ji but ij multiplied by o i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21924" target="_blank">06:05:24.320</a></span> | <span class="t">and this should be equal to the same formula that you see on the right here this allows you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21928" target="_blank">06:05:28.960</a></span> | <span class="t">compute one output row in the v matrix okay and we know that pij is just the output of the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21939" target="_blank">06:05:39.440</a></span> | <span class="t">the output of the softmax is the input of the softmax to the exponential of the input of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21945" target="_blank">06:05:45.200</a></span> | <span class="t">softmax divided by the normalization factor associated with that row so because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21952" target="_blank">06:05:52.960</a></span> | <span class="t">iterating through the row of i it will be the i-th the normalization factor associated with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21959" target="_blank">06:05:59.600</a></span> | <span class="t">row of of o i so we know that the formula for the p is equal to the softmax of s now the i-th</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21971" target="_blank">06:06:11.840</a></span> | <span class="t">row of p will be the softmax of the i-th row of s and this is what is written here we know from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21978" target="_blank">06:06:18.560</a></span> | <span class="t">our derivation that the jacobian with respect to the softmax operation so if we have an input x and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21986" target="_blank">06:06:26.640</a></span> | <span class="t">the output is y of the softmax operation the jacobian of this of the y with respect to the x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=21993" target="_blank">06:06:33.440</a></span> | <span class="t">is equal to the diagonal y it's a diagonal matrix of the element of the factor y minus y multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22001" target="_blank">06:06:41.120</a></span> | <span class="t">by y transposed and we have also seen before that this matrix is symmetric however you may not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22008" target="_blank">06:06:48.560</a></span> | <span class="t">understand this formula here because we have seen from our in the chain rule we always write it like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22015" target="_blank">06:06:55.200</a></span> | <span class="t">this we always write that the downstream gradient so the d phi of let's say t x should be equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22027" target="_blank">06:07:07.200</a></span> | <span class="t">the upstream gradient so d phi with respect to d y multiplied by d y and with respect to d x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22037" target="_blank">06:07:17.440</a></span> | <span class="t">this only works if you make this matrix here as a in the numerator convention the numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22046" target="_blank">06:07:26.160</a></span> | <span class="t">convention is one of the two convention in which you can create a jacobian we so far we have always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22051" target="_blank">06:07:31.760</a></span> | <span class="t">written it as the numerator convention if you use the numerator convention this is a row vector and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22058" target="_blank">06:07:38.480</a></span> | <span class="t">this is a row vector however if you want to treat this stuff here as a column vector then you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22065" target="_blank">06:07:45.920</a></span> | <span class="t">to take the transposed or you need to make the jacobian in the denominator convention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22070" target="_blank">06:07:50.720</a></span> | <span class="t">how to get this formula here because this formula here is basically doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22075" target="_blank">06:07:55.520</a></span> | <span class="t">jacobian multiplied by the upstream gradient not the gradient upstream gradient multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22082" target="_blank">06:08:02.160</a></span> | <span class="t">jacobian and it's only because here we treat it as a column vector and when you do the you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22088" target="_blank">06:08:08.240</a></span> | <span class="t">transform a row vector into a column vector you take the transpose of both sides of the equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22092" target="_blank">06:08:12.000</a></span> | <span class="t">and let's do it actually so we apply the transpose to the both side of the equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22100" target="_blank">06:08:20.560</a></span> | <span class="t">okay in a matrix multiplication if you do a b transposed it become b transposed multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22110" target="_blank">06:08:30.480</a></span> | <span class="t">by a transposed so the transposed is applied independently to each input of the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22115" target="_blank">06:08:35.840</a></span> | <span class="t">multiplication but we invert the matrix multiplication and if you remember the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22119" target="_blank">06:08:39.440</a></span> | <span class="t">matrix multiplication is not commutative so what we do here is that we say okay it will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22125" target="_blank">06:08:45.840</a></span> | <span class="t">dphi of dx and here they call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22128" target="_blank">06:08:48.960</a></span> | <span class="t">here they call it dsi so it will basically just become d phi on dx if you treat this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22141" target="_blank">06:09:01.200</a></span> | <span class="t">one as a column vector so this one as a column vector will be equal to dy on dx as a column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22149" target="_blank">06:09:09.120</a></span> | <span class="t">vector as a jacobian in the denominator layout in this case multiplied by d phi on dy as a column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22159" target="_blank">06:09:19.760</a></span> | <span class="t">vector this one is a column vector this is a column vector and this is what you see here that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22164" target="_blank">06:09:24.000</a></span> | <span class="t">why the jacobian is on the left side of the upstream gradient what else we need well i i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22171" target="_blank">06:09:31.120</a></span> | <span class="t">know that there is a lot of things here in this derivation but i prefer actually going directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22175" target="_blank">06:09:35.840</a></span> | <span class="t">to the code otherwise i think it's going to be too boring um so let's go to the code and while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22181" target="_blank">06:09:41.920</a></span> | <span class="t">writing the code i go back to the formulas in which we can find the association of what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22187" target="_blank">06:09:47.360</a></span> | <span class="t">doing and the formula in the paper i think this is the best way so let's proceed further all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22194" target="_blank">06:09:54.640</a></span> | <span class="t">guys now we can finally code the backward pass before we code the backward pass let's look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22199" target="_blank">06:09:59.040</a></span> | <span class="t">the algorithm of the backward pass as written in the paper this is the paper flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22203" target="_blank">06:10:03.920</a></span> | <span class="t">one and i will be because we will follow the structure of the code that is present on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22210" target="_blank">06:10:10.560</a></span> | <span class="t">triton website so it's not my idea to split it like this but i simplified it in such i simplified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22217" target="_blank">06:10:17.840</a></span> | <span class="t">it so it's different than the one that you can find online because mine is a simplified version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22222" target="_blank">06:10:22.960</a></span> | <span class="t">and mine works with the causal and non-causal attention um so first if you look at this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22229" target="_blank">06:10:29.360</a></span> | <span class="t">algorithm you need to you can see that we have an outer loop through all the k and v blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22236" target="_blank">06:10:36.240</a></span> | <span class="t">and an inner loop through all the query blocks however as you can see to compute the dq which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22244" target="_blank">06:10:44.080</a></span> | <span class="t">the downstream gradient of the the loss with respect to the q matrix we need to have an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22251" target="_blank">06:10:51.920</a></span> | <span class="t">iteration through all the k's and to compute each dk block we need to have an iteration through all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22258" target="_blank">06:10:58.960</a></span> | <span class="t">the queues so if we follow the loop like it is it would involve writing to the high bandwidth memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22266" target="_blank">06:11:06.880</a></span> | <span class="t">so to the dram of the gpu at every inner iteration and that could be also that is not so efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22273" target="_blank">06:11:13.120</a></span> | <span class="t">and also if we don't want to write it would require some sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22278" target="_blank">06:11:18.080</a></span> | <span class="t">some sort of synchronization between blocks which is also not very efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22284" target="_blank">06:11:24.080</a></span> | <span class="t">so we split we will split this four into two parts because we can see that each dq depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22290" target="_blank">06:11:30.800</a></span> | <span class="t">on a loop over the k's and each dk depends on a loop over all the queues so to compute dk we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22301" target="_blank">06:11:41.200</a></span> | <span class="t">fix the kth block and iterate through all the q blocks then we will do another iteration in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22306" target="_blank">06:11:46.640</a></span> | <span class="t">we fix the q block and iterate through all the kv blocks to compute the dq this is what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22312" target="_blank">06:11:52.320</a></span> | <span class="t">going to follow and this is an idea that i took from the original implementation that is present</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22316" target="_blank">06:11:56.560</a></span> | <span class="t">on triton website another thing that we can notice here is um where where is it here to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22325" target="_blank">06:12:05.360</a></span> | <span class="t">dq and dk so a dq vector and the dk vector we need this element this information here called the di</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22335" target="_blank">06:12:15.680</a></span> | <span class="t">di and it's shared between the two so we can pre-compute it and then we can reuse it for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22341" target="_blank">06:12:21.840</a></span> | <span class="t">qi vector to compute the qi vector and the dk vector what is this di di is um is uh introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22352" target="_blank">06:12:32.320</a></span> | <span class="t">here and it's the dot product of a vector that is the doi vector multiplied by o vector so the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22361" target="_blank">06:12:41.040</a></span> | <span class="t">thing that we will do is do a loop over all the vectors in o and do and do their dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22368" target="_blank">06:12:48.000</a></span> | <span class="t">to compute this di element then we will use this di element and actually uh let me see yeah and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22376" target="_blank">06:12:56.160</a></span> | <span class="t">then we will use this di element to update to to compute dq and dk and we will also have another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22382" target="_blank">06:13:02.960</a></span> | <span class="t">two loops one in which we fix the q and we iterate through all the keys and one in we fix the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22388" target="_blank">06:13:08.400</a></span> | <span class="t">and iterate to all the queues so let's start so now that we know more or less the structure of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22394" target="_blank">06:13:14.800</a></span> | <span class="t">the code that we're with all right so we start by writing this backward function here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22403" target="_blank">06:13:23.920</a></span> | <span class="t">uh let me check yeah okay so do you remember this is saved tensor these are all the information that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22415" target="_blank">06:13:35.360</a></span> | <span class="t">we save during the forward pass uh to compute the backward pass now to to optimize the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22423" target="_blank">06:13:43.760</a></span> | <span class="t">utilization in flash attention we don't save the query multiplied by the transpose of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22430" target="_blank">06:13:50.160</a></span> | <span class="t">key matrix because that would be a sequence by sequence matrix that is too big to save into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22435" target="_blank">06:13:55.680</a></span> | <span class="t">hbm in the dram during the forward pass and then i re get it back from the hbm into the local memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22442" target="_blank">06:14:02.480</a></span> | <span class="t">because i want to remind you that in triton uh compared to cuda in triton what we do is we load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22449" target="_blank">06:14:09.040</a></span> | <span class="t">stuff from the high bandwidth memory in the shared memory so the sram we do all the operations there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22455" target="_blank">06:14:15.920</a></span> | <span class="t">and then after when we call the store method we save the element from the shared memory into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22462" target="_blank">06:14:22.000</a></span> | <span class="t">high bandwidth memory so in order to not materialize this s matrix in its entirety save it to the hbm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22470" target="_blank">06:14:30.160</a></span> | <span class="t">and then reget it back which could be very slow and secondly actually it is very expensive because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22475" target="_blank">06:14:35.840</a></span> | <span class="t">usually right now we are computing attention on thousands and thousands of tokens so imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22481" target="_blank">06:14:41.200</a></span> | <span class="t">saving a matrix that is 5000 by 5000 that's a big matrix to save for each batch uh for b each batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22489" target="_blank">06:14:49.280</a></span> | <span class="t">and for each head so that would be really too expensive to save so the idea in flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22496" target="_blank">06:14:56.480</a></span> | <span class="t">is to recompute what we can compute on the fly during the backward pass because any way if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22501" target="_blank">06:15:01.760</a></span> | <span class="t">were to load it it would be memory i/o bound so it's faster to recompute than to save it and restore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22509" target="_blank">06:15:09.200</a></span> | <span class="t">it from the memory this is the idea of flash attention okay so we saved some stuff during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22516" target="_blank">06:15:16.080</a></span> | <span class="t">forward pass and now we can access it back during the backward pass and this stuff is saved in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22521" target="_blank">06:15:21.440</a></span> | <span class="t">context and this it's a it's a kind of a dictionary that is made available by by pytorch all right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22529" target="_blank">06:15:29.600</a></span> | <span class="t">we get back the query key and values and as you know pytorch during the autograd will just give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22535" target="_blank">06:15:35.600</a></span> | <span class="t">us the gradient of the loss with respect to the output of our implementation of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22541" target="_blank">06:15:41.440</a></span> | <span class="t">of our attention so this is triton attention and then we need to compute dq dk and dv by using only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22549" target="_blank">06:15:49.200</a></span> | <span class="t">the gradient of the output with respect to the the loss with respect to the output um we do for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22554" target="_blank">06:15:54.800</a></span> | <span class="t">some checks so here i know i could optimize this code and make it even smaller by for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22562" target="_blank">06:16:02.080</a></span> | <span class="t">checking that here the stride that i am using i actually inside of the code i always uh pretend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22567" target="_blank">06:16:07.840</a></span> | <span class="t">that the stride is the same but uh doesn't matter i just take the code from triton and uh try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22574" target="_blank">06:16:14.000</a></span> | <span class="t">simplify it my goal was to simplify it not optimize it so all right we create the um the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22581" target="_blank">06:16:21.520</a></span> | <span class="t">vectors the tensors in which we will store the result of this backward pass which is the dq dk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22588" target="_blank">06:16:28.320</a></span> | <span class="t">and dv and as you know from what we have seen of the definition of the gradient the size of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22594" target="_blank">06:16:34.960</a></span> | <span class="t">output of the gradient vector is the size of the vector with respect to which we calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22601" target="_blank">06:16:41.760</a></span> | <span class="t">gradient because in the numerator is always a scalar and we compute the gradient with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22606" target="_blank">06:16:46.080</a></span> | <span class="t">to all the elements in the input vector so the output the gradient itself is a vector of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22611" target="_blank">06:16:51.200</a></span> | <span class="t">same size of the element by which we compute the gradient with respect to so uh we get some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22618" target="_blank">06:16:58.480</a></span> | <span class="t">information on the bed size blah blah blah and later we will see what is this number of warps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22624" target="_blank">06:17:04.400</a></span> | <span class="t">and the number of stages i will not explain it now it's how pytorch number of parts warps is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22631" target="_blank">06:17:11.360</a></span> | <span class="t">indication on how many threads we want to launch in our grid and number of stages is next to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22635" target="_blank">06:17:15.600</a></span> | <span class="t">number of stages that has used in software pipelining we will see later what is software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22639" target="_blank">06:17:19.360</a></span> | <span class="t">pipelining when we talk about the auto tuning then we define some uh blocks uh in the original um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22649" target="_blank">06:17:29.120</a></span> | <span class="t">in the original code i think they call it a block kv1 kv2 q1 and q2 i think it was confusing i call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22657" target="_blank">06:17:37.920</a></span> | <span class="t">it a block macro and block micro because the thing that we will fix and the things that we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22662" target="_blank">06:17:42.480</a></span> | <span class="t">iterate from will be once it's the query so we fix the query block and we iterate through all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22668" target="_blank">06:17:48.800</a></span> | <span class="t">keys and then we will fix the keys and values block and we iterate through the queries the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22675" target="_blank">06:17:55.200</a></span> | <span class="t">that we iterate on is the micro one and the one that we fix is the macro one this is my uh the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22681" target="_blank">06:18:01.360</a></span> | <span class="t">naming that i am using um then we as i said before we need to pre-compute the di elements that we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22689" target="_blank">06:18:09.440</a></span> | <span class="t">in the paper before so that's the first kernel that we are going to launch and this kernel will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22694" target="_blank">06:18:14.800</a></span> | <span class="t">have its own launch grid because later we want to optimize the the tuning of this kernel later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22701" target="_blank">06:18:21.520</a></span> | <span class="t">we will talk about the tuning with respect to its own parameters so let me see what are we going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22709" target="_blank">06:18:29.280</a></span> | <span class="t">do so here so the first kernel that we are going to launch is this pre-process kernel this pre-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22715" target="_blank">06:18:35.680</a></span> | <span class="t">process kernel will pre-compute all the di elements that we need to compute i remember dk and dv if i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22723" target="_blank">06:18:43.120</a></span> | <span class="t">know dq and dk and this di element depends only on o and do um so let's do it and let's create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22735" target="_blank">06:18:55.360</a></span> | <span class="t">another function called the backward preprocessor what is the process preprocess grid this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22741" target="_blank">06:19:01.440</a></span> | <span class="t">launch grid of this function of this kernel and this will be launched on a independently for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22750" target="_blank">06:19:10.000</a></span> | <span class="t">batch and for each head and moreover it will be work with a block size of vectors of o what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22757" target="_blank">06:19:17.440</a></span> | <span class="t">this block what is this number of vectors of o it will be the block size macro so on 128 vectors of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22765" target="_blank">06:19:25.760</a></span> | <span class="t">o so uh let me copy the signature of this function this is here so let's write it here i think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22774" target="_blank">06:19:34.960</a></span> | <span class="t">fine yeah okay this function takes a the matrix o so it's a pointer to the matrix o it's a pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22783" target="_blank">06:19:43.280</a></span> | <span class="t">to the d o and it's a pointer to the matrix d where we will store this di elements and we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22789" target="_blank">06:19:49.600</a></span> | <span class="t">one for each vector in the output that's why the shape of this d is a batch size number head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22797" target="_blank">06:19:57.840</a></span> | <span class="t">sequence length it means it's one for each of the output element in the output of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22803" target="_blank">06:20:03.040</a></span> | <span class="t">this di where is it actually it's not this one it's this one yeah like m so it has the same shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22811" target="_blank">06:20:11.760</a></span> | <span class="t">as m which is as you can see it is this size here so batch size number heads and sequence length m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22818" target="_blank">06:20:18.320</a></span> | <span class="t">if you remember is the matrix that we saved during the forward pass which includes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22822" target="_blank">06:20:22.160</a></span> | <span class="t">normalization factor of the softmax and also the maximum element but in log sum exp format so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22829" target="_blank">06:20:29.920</a></span> | <span class="t">when we apply it will automatically apply the maximum element for each row and also normalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22834" target="_blank">06:20:34.560</a></span> | <span class="t">at the same time which i think i proved previously so let me do it so we write it like this so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22844" target="_blank">06:20:44.560</a></span> | <span class="t">extract the the index of this program so this program has two index like identifier this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22855" target="_blank">06:20:55.600</a></span> | <span class="t">equivalent to the cuda identifier and this is along the axis 0 so let's see what we what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22861" target="_blank">06:21:01.520</a></span> | <span class="t">what did we launch on the axis 0 so on the axis 0 of this launch grid we defined what is the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22868" target="_blank">06:21:08.400</a></span> | <span class="t">of vectors of the o that this particular will program will work with and the second axis is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22877" target="_blank">06:21:17.280</a></span> | <span class="t">which batch and which head inside of each batch this particular program will work with so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22883" target="_blank">06:21:23.760</a></span> | <span class="t">identifies the block index of q so which group of vectors in the o matrix this particular program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22890" target="_blank">06:21:30.400</a></span> | <span class="t">will work with here is called q i believe because i copied it from the original code where they call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22896" target="_blank">06:21:36.160</a></span> | <span class="t">it q but i could have eventually also call it o um so we define uh so basically this means that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22905" target="_blank">06:21:45.440</a></span> | <span class="t">are for this program we need to skip some query vectors that have been already or that will be or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22912" target="_blank">06:21:52.160</a></span> | <span class="t">have been already processed by other programs in parallel so we will only block with a number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22918" target="_blank">06:21:58.560</a></span> | <span class="t">query vectors inside of o that have the following indices so imagine that the query block size is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22926" target="_blank">06:22:06.480</a></span> | <span class="t">i think it's 128 the way we have defined it but suppose it's a 4 for simplicity so this one will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22934" target="_blank">06:22:14.480</a></span> | <span class="t">be and the query vectors are how many are sequence length number of query vectors we have so some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22942" target="_blank">06:22:22.960</a></span> | <span class="t">imagine the query vectors are in total they are i don't know let's say uh 64 and 32 will be managed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22950" target="_blank">06:22:30.960</a></span> | <span class="t">by other programs so this particular of skew will be equal to 33 34 35 and 36 this tells me which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22960" target="_blank">06:22:40.560</a></span> | <span class="t">query vectors or which vectors in the output o matrix among all the vectors in the o matrix this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22966" target="_blank">06:22:46.880</a></span> | <span class="t">particular program is going to work with okay so then we extract also the index of the batch which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22974" target="_blank">06:22:54.800</a></span> | <span class="t">tells us which batch and which head in each batch this particular program is going to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22981" target="_blank">06:23:01.040</a></span> | <span class="t">which is the dimension one of our launch grid and then we define the offset of the dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22987" target="_blank">06:23:07.440</a></span> | <span class="t">because we need to load all the dimensions of each vector so these are the it's a vector that tells</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22994" target="_blank">06:23:14.320</a></span> | <span class="t">which dimensions we need to load from each vector and we will load all of them so we don't divide on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=22998" target="_blank">06:23:18.400</a></span> | <span class="t">the head dimension dimension we just divide on the sequence length dimension the the load among</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23006" target="_blank">06:23:26.080</a></span> | <span class="t">multiple programs um you will see in this part of the the video so when we are writing the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23013" target="_blank">06:23:33.280</a></span> | <span class="t">pass that we will not be using the make block pointer like we did during the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23019" target="_blank">06:23:39.040</a></span> | <span class="t">so this function here we will work with directly with indexing by using the strides so let's do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23027" target="_blank">06:23:47.200</a></span> | <span class="t">so let's load a single block of rows of o which i want to remind you has the same shape as q and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23037" target="_blank">06:23:57.040</a></span> | <span class="t">that's why we can call it block size q so the o block that we are loading is o so uh the load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23043" target="_blank">06:24:03.840</a></span> | <span class="t">function accepts a pointer to what it should load actually not a pointer it accepts a array of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23051" target="_blank">06:24:11.360</a></span> | <span class="t">pointers or a multi-dimensional array of pointer in case you want to load a multi-dimensional data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23056" target="_blank">06:24:16.720</a></span> | <span class="t">so actually load also allows you to load two-dimensional data in this case we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23063" target="_blank">06:24:23.360</a></span> | <span class="t">to load two-dimensional data which is a block of rows of o which should be a block a tensor of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23071" target="_blank">06:24:31.520</a></span> | <span class="t">shape block size q in this case multiplied by the other dimension being head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23079" target="_blank">06:24:39.120</a></span> | <span class="t">but we don't we need to tell it where in this o matrix it needs to find this one first of all we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23086" target="_blank">06:24:46.160</a></span> | <span class="t">need to skip some batches and some heads based on what the head and the batch that will be processed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23092" target="_blank">06:24:52.800</a></span> | <span class="t">by other programs so based on the index that this um program will process of the batch and the head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23099" target="_blank">06:24:59.920</a></span> | <span class="t">we need to skip all the other batches and heads let's write the shape of this tensor so the o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23107" target="_blank">06:25:07.840</a></span> | <span class="t">tensor has a shape block size not block size batch size a number of heads then sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23117" target="_blank">06:25:17.360</a></span> | <span class="t">and then head dimension each block and each head will have a sequence length multiplied by dim</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23125" target="_blank">06:25:25.360</a></span> | <span class="t">head dim number of items so based on our index we skip how many items our index multiplied by head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23132" target="_blank">06:25:32.960</a></span> | <span class="t">dimension multiplied by sequence length so what i mean is this the batch zero and the head zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23140" target="_blank">06:25:40.480</a></span> | <span class="t">will have a sequence length multiplied by head dimension items the batch zero and the head one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23147" target="_blank">06:25:47.360</a></span> | <span class="t">will also have the same number of items and the batch zero and head two will also have the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23153" target="_blank">06:25:53.120</a></span> | <span class="t">number of items so how many items sequence length multiplied by head dimension do we need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23158" target="_blank">06:25:58.160</a></span> | <span class="t">from the starting of the o tensor it is equal to the index of the current batch and head indicator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23164" target="_blank">06:26:04.320</a></span> | <span class="t">so because this index indicates both the head in the batch and the head inside of each batch because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23171" target="_blank">06:26:11.360</a></span> | <span class="t">it's already the product of the head and the batch so how many we skip indicated by the this index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23180" target="_blank">06:26:20.640</a></span> | <span class="t">and after we point to this starting point of the current batch and the current head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23185" target="_blank">06:26:25.760</a></span> | <span class="t">we need to select a two-dimensional tensor where the offsets are indicated for the rows by off skew</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23193" target="_blank">06:26:33.840</a></span> | <span class="t">and that's why we have this one um the i don't know what this is called this is uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23201" target="_blank">06:26:41.600</a></span> | <span class="t">the the index uh semi-colon index that tells all the all these vectors in off skew will with an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23210" target="_blank">06:26:50.880</a></span> | <span class="t">additional dimension for the columns and these columns will be the off dim so basically this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23216" target="_blank">06:26:56.160</a></span> | <span class="t">will select a tensor of the following shape inside of this big tensor that includes head size and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23223" target="_blank">06:27:03.920</a></span> | <span class="t">number of heads this is what we are doing so we are saying select a tensor of this size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23231" target="_blank">06:27:11.280</a></span> | <span class="t">inside of one that is made up of four dimensions by skipping the elements of all the batch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23238" target="_blank">06:27:18.080</a></span> | <span class="t">heads that will be processed by other programs i always talk in terms of programs because in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23244" target="_blank">06:27:24.400</a></span> | <span class="t">triton these are called programs in coda you would refer to them as kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23248" target="_blank">06:27:28.160</a></span> | <span class="t">all right so this one is done i hope it is decently clear um all right so then we also load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23258" target="_blank">06:27:38.560</a></span> | <span class="t">a single block of d o in the same way because we are going to load a group of vectors from all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23266" target="_blank">06:27:46.880</a></span> | <span class="t">sequence length also from d o and the d o has the same shape as o which has the same shape as q and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23274" target="_blank">06:27:54.320</a></span> | <span class="t">that's why we can use the um the the block index we call it q because it's equivalent because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23280" target="_blank">06:28:00.080</a></span> | <span class="t">have the same shape okay and how to compute this d i element well it's written in the paper so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23288" target="_blank">06:28:08.160</a></span> | <span class="t">we go in the in the what is it man if we go here it shows you how to compute the d i of each given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23297" target="_blank">06:28:17.280</a></span> | <span class="t">a block of d o and a block of o it tells you how to compute d i which is the row sum which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23305" target="_blank">06:28:25.200</a></span> | <span class="t">the sum of by rows for each row we will have one sum for each vector in the o matrix we will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23312" target="_blank">06:28:32.080</a></span> | <span class="t">some of the element wise product so this stuff here is the element wise product of d o i multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23319" target="_blank">06:28:39.360</a></span> | <span class="t">by o i so it's not a matrix multiplication it's element wise product which means each element of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23326" target="_blank">06:28:46.480</a></span> | <span class="t">one matrix with the corresponding element of the second matrix and the output shape it will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23331" target="_blank">06:28:51.280</a></span> | <span class="t">same as the two matrices which must have the same shape okay so we compute this d i block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23340" target="_blank">06:29:00.960</a></span> | <span class="t">which will have shape block size q because we will have one sum for each vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23346" target="_blank">06:29:06.560</a></span> | <span class="t">then well we need to store it somewhere so we need to calculate where to store it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23352" target="_blank">06:29:12.800</a></span> | <span class="t">inside of the d matrix well the d matrix is i remember correctly has the same shape as m so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23360" target="_blank">06:29:20.240</a></span> | <span class="t">it should be batch size a number of heads and sequence length so we need to select the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23369" target="_blank">06:29:29.120</a></span> | <span class="t">batch and the right head and also the right position inside of the sequence length based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23373" target="_blank">06:29:33.440</a></span> | <span class="t">on the block index q that we have okay so let me index okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23383" target="_blank">06:29:43.120</a></span> | <span class="t">all right because we already um so we skip um again just like before we know that the d is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23393" target="_blank">06:29:53.680</a></span> | <span class="t">of this size each batch and each head will have sequence length number of elements so how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23399" target="_blank">06:29:59.520</a></span> | <span class="t">number of elements we need to skip from the starting of the tensor is sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23404" target="_blank">06:30:04.960</a></span> | <span class="t">multiplied by the combined index batch size head number and plus we need to also skip some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23412" target="_blank">06:30:12.640</a></span> | <span class="t">queries based on our block index q and it's already this skipping is already done inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23418" target="_blank">06:30:18.480</a></span> | <span class="t">of off skew so we add off skew and then once we have computed the index where we should store this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23424" target="_blank">06:30:24.720</a></span> | <span class="t">d i block why did i even call it d block let's store it so let me i didn't call it d block i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23434" target="_blank">06:30:34.320</a></span> | <span class="t">think it was already in the original code but this is d i and this big matrix d is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23440" target="_blank">06:30:40.240</a></span> | <span class="t">the matrix that includes all the d i for one for each token in the sequence length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23445" target="_blank">06:30:45.600</a></span> | <span class="t">all right so the pre-processing has been done now we need to do prepare the two for loops as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23454" target="_blank">06:30:54.480</a></span> | <span class="t">remember i said before we will be doing two for loops one in which we fix the query and we iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23460" target="_blank">06:31:00.880</a></span> | <span class="t">through all the keys and values and one in which we fix the key and value block and we iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23465" target="_blank">06:31:05.120</a></span> | <span class="t">through all the queries and while coding it i will always show you the formula from the paper so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23470" target="_blank">06:31:10.720</a></span> | <span class="t">don't worry let's start with the next iteration so first we create the launch grid for the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23476" target="_blank">06:31:16.720</a></span> | <span class="t">iteration as the launch grid is always the same so we first because we we need to keep one block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23484" target="_blank">06:31:24.000</a></span> | <span class="t">fixed and iterate through all the other blocks the block that we keep fixed will define how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23489" target="_blank">06:31:29.600</a></span> | <span class="t">programs we have that run in parallel and the block that is fixed has a block size macro number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23495" target="_blank">06:31:35.760</a></span> | <span class="t">of elements that's why we create a sequence length divided by block size macro number of blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23501" target="_blank">06:31:41.120</a></span> | <span class="t">thread blocks or programs in this axis the axis two in this grid is i could have used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23510" target="_blank">06:31:50.240</a></span> | <span class="t">also the axis one indifferently i think it was already done here in the original code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23515" target="_blank">06:31:55.520</a></span> | <span class="t">it's we will indicate which batch and which head inside of each batch we are going to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23521" target="_blank">06:32:01.920</a></span> | <span class="t">so and just like the forward pass we will also use a variable called the stage that if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23530" target="_blank">06:32:10.560</a></span> | <span class="t">attention that we are computing is causal it will be equal to three and if we are computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23534" target="_blank">06:32:14.880</a></span> | <span class="t">a non-causal attention then it will be equal to one the first iteration we will fix k and v blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23542" target="_blank">06:32:22.960</a></span> | <span class="t">and we will iterate through all the q blocks in size of block size micro number of query vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23552" target="_blank">06:32:32.000</a></span> | <span class="t">so let's look at the signature so we pass we launch it as a launch grid because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23558" target="_blank">06:32:38.800</a></span> | <span class="t">and we have defined how many programs we have so we have how many kv blocks we will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23567" target="_blank">06:32:47.040</a></span> | <span class="t">it's a sequence length divided by the block size macro because that's the the block that we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23571" target="_blank">06:32:51.920</a></span> | <span class="t">keep fixed in this uh for loop in this function and then we go through all the query blocks in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23579" target="_blank">06:32:59.200</a></span> | <span class="t">size of block size micro which i defined it as 32 and later we will talk about auto tuning and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23585" target="_blank">06:33:05.600</a></span> | <span class="t">how to tune these values all right so i pass the query vector the key vector and the v vector uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23592" target="_blank">06:33:12.560</a></span> | <span class="t">sorry not vector tensors now the query tensor k tensor and v tensor and they are pointing to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23598" target="_blank">06:33:18.800</a></span> | <span class="t">beginning of the tensor which means that they are beginning to the first batch and the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23603" target="_blank">06:33:23.520</a></span> | <span class="t">head and the first token and the first dimension of the tensors then we pass the softmax scale we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23610" target="_blank">06:33:30.400</a></span> | <span class="t">pass do dq dk and db m is the one that is needed to compute as you remember from what we said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23619" target="_blank">06:33:39.040</a></span> | <span class="t">before we did not see the p matrix in the hbm because we want to recompute it on the fly doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23625" target="_blank">06:33:45.760</a></span> | <span class="t">the backward pass so the query multiplied by transpose of the keys it's a very big matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23629" target="_blank">06:33:49.760</a></span> | <span class="t">to save in the hbm and restore it so we want to compute it on the fly but we don't need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23634" target="_blank">06:33:54.560</a></span> | <span class="t">recompute the normalization factor and the maximum element for each row to apply the softmax that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23640" target="_blank">06:34:00.480</a></span> | <span class="t">already computed during the forward pass and saved into this matrix m which includes the log sum exp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23646" target="_blank">06:34:06.800</a></span> | <span class="t">of the maximum of each row plus the logarithm of the normalization factor with the log sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23654" target="_blank">06:34:14.080</a></span> | <span class="t">x to 3 we can just apply it and it will also normalize each value then we have the d vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23661" target="_blank">06:34:21.120</a></span> | <span class="t">tensor that we computed here with all the di values one for each vector in the o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23667" target="_blank">06:34:27.120</a></span> | <span class="t">tensor then we need to pass some the number of heads the sequence length the block size that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23674" target="_blank">06:34:34.320</a></span> | <span class="t">we want to use for the kv which is the macro block size and the micros block size is always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23678" target="_blank">06:34:38.480</a></span> | <span class="t">the one that we iterate on i think using this name it should be easier to understand which one we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23683" target="_blank">06:34:43.440</a></span> | <span class="t">iterating and which we want to keep fixed so the fixed one is macro and the iterating one is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23688" target="_blank">06:34:48.160</a></span> | <span class="t">micro head dimension later we will see why we use a different block size to iterate from because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23696" target="_blank">06:34:56.320</a></span> | <span class="t">this is related to the number of stages that triton can divide your for loop into thanks to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23703" target="_blank">06:35:03.200</a></span> | <span class="t">software pipelining then we have head dimension the stage indicates if the attention that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23710" target="_blank">06:35:10.000</a></span> | <span class="t">computed in the forward pass was causal or not causal the number of warps and the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23715" target="_blank">06:35:15.840</a></span> | <span class="t">stages which we defined as fixed but later we will talk about auto tuning so sometimes i repeat the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23722" target="_blank">06:35:22.480</a></span> | <span class="t">same stuff over and over so i should change that okay let's write the signature of this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23733" target="_blank">06:35:33.840</a></span> | <span class="t">let's put it here so we already described what is the signature of this function let's go directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23741" target="_blank">06:35:41.680</a></span> | <span class="t">to the meat so the first thing that we need to do is understand the offset by which we need to move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23747" target="_blank">06:35:47.200</a></span> | <span class="t">this query key and value and the offset is given by the first wall we need to enter the right batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23753" target="_blank">06:35:53.840</a></span> | <span class="t">and the right head inside of each batch we compute the index of the batch just like during the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23758" target="_blank">06:35:58.720</a></span> | <span class="t">forward pass by dividing the program the program index which is a multiplication of the index of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23765" target="_blank">06:36:05.040</a></span> | <span class="t">the head and of the the batch we divided by the number of heads to get which batch this program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23771" target="_blank">06:36:11.920</a></span> | <span class="t">is working with and to get the head we just do the modulus just like in the for loop for one person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23777" target="_blank">06:36:17.040</a></span> | <span class="t">the offset batch head indicates let me check what is it for okay it enters the right batch and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23784" target="_blank">06:36:24.640</a></span> | <span class="t">right head so what is the stride if you remember correctly the stride tells us how many items you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23790" target="_blank">06:36:30.000</a></span> | <span class="t">need to skip in that dimension to arrive to the next index in the same dimension so if we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23794" target="_blank">06:36:34.480</a></span> | <span class="t">skip index number of batch we multiply it by the stride batch which is how many elements you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23801" target="_blank">06:36:41.200</a></span> | <span class="t">to skip to arrive to the next batch plus we also need to enter the right head so we multiply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23808" target="_blank">06:36:48.160</a></span> | <span class="t">index of the head multiplied by the stride of the head to enter exactly in that head in the tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23814" target="_blank">06:36:54.560</a></span> | <span class="t">for each of the q k and v matrices plus we also have this is will be used for if i remember for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23822" target="_blank">06:37:02.640</a></span> | <span class="t">m and d because m and d only don't have the um the head dimension head dimension so they are only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23830" target="_blank">06:37:10.480</a></span> | <span class="t">batch size number of heads sequence length so we just use the index batch multiplied by sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23835" target="_blank">06:37:15.760</a></span> | <span class="t">length because for each batch and on each head we will have sequence length number of items so you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23839" target="_blank">06:37:19.680</a></span> | <span class="t">can think of it at the stride to move from one batch head to the next batch head uh or to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23847" target="_blank">06:37:27.200</a></span> | <span class="t">yeah so uh let's move the pointers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23853" target="_blank">06:37:33.040</a></span> | <span class="t">and this was so we move the pointer q k and v by the offset batch head because we want to enter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23863" target="_blank">06:37:43.360</a></span> | <span class="t">the right um batch and the right head inside of these big tensors and we do it also for d o d q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23870" target="_blank">06:37:50.960</a></span> | <span class="t">d k and d v because they have the same shape as a q k and v and d o also has the same shape as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23876" target="_blank">06:37:56.720</a></span> | <span class="t">q so they have the same shape so we move by the same uh by the same offset all right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23883" target="_blank">06:38:03.680</a></span> | <span class="t">then we move m and d to move them to the right starting point on which the sequence of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23891" target="_blank">06:38:11.440</a></span> | <span class="t">current head and the current batch and the current head starts so they are pointing to the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23898" target="_blank">06:38:18.000</a></span> | <span class="t">vector of the sequence dedicated to the current batch and the current head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23902" target="_blank">06:38:22.400</a></span> | <span class="t">and the same is true for q k and v and the d o d q d k and v okay then we load some other stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23912" target="_blank">06:38:32.400</a></span> | <span class="t">because here we fix in this iteration in this method we are going to do a for loop in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23920" target="_blank">06:38:40.480</a></span> | <span class="t">we fix k v and we iterate through q so we first need to load this deeps block of k v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23927" target="_blank">06:38:47.040</a></span> | <span class="t">and we do it as follows as follows so we know we need to load a 2d tensor so we need to define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23936" target="_blank">06:38:56.160</a></span> | <span class="t">what are the ranges in the second dimension of each vector k and v that we need to load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23944" target="_blank">06:39:04.320</a></span> | <span class="t">and it's defined by this by this vector then we want to understand which kv block this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23954" target="_blank">06:39:14.960</a></span> | <span class="t">program is going to work with so this particular program is going to skip some kvs that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23960" target="_blank">06:39:20.720</a></span> | <span class="t">already be managed by other programs that may be running in parallel and how to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23965" target="_blank">06:39:25.440</a></span> | <span class="t">what this program should be working with in based on the index of the program zero which is defined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23972" target="_blank">06:39:32.560</a></span> | <span class="t">on the sequence length divided by the block size macro and if you remember block size macro is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23977" target="_blank">06:39:37.920</a></span> | <span class="t">thing that we fix so it's telling us this program id zero will tell us how many block size macro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23985" target="_blank">06:39:45.680</a></span> | <span class="t">kv are already being managed by other programs so we shouldn't care about them so we skip them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23992" target="_blank">06:39:52.080</a></span> | <span class="t">so let's go back here and this is the number of vectors that we need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=23997" target="_blank">06:39:57.280</a></span> | <span class="t">so our kv start from start kv and how many we need to load them well depends on what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24004" target="_blank">06:40:04.160</a></span> | <span class="t">block kv this block kv is equal to block size macro so it will be 128 vectors so we define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24014" target="_blank">06:40:14.480</a></span> | <span class="t">our tensors two-dimensional tensors that we will store in the sram because in triton every time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24022" target="_blank">06:40:22.080</a></span> | <span class="t">you load something you load it from the hbm into the sram so we define where they should be saved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24028" target="_blank">06:40:28.080</a></span> | <span class="t">in the sram and they are initially zeros and now we load them so we load them as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24034" target="_blank">06:40:34.000</a></span> | <span class="t">we say that okay in the k in the k tensor pointer which is already pointing to the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24046" target="_blank">06:40:46.320</a></span> | <span class="t">index to the right batch and to the right head because that's something that we did here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24052" target="_blank">06:40:52.160</a></span> | <span class="t">we say we should need we need to load the right sequence of keys which should start from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24060" target="_blank">06:41:00.320</a></span> | <span class="t">offski because this already includes how many we should skip in the sequence length dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24066" target="_blank">06:41:06.000</a></span> | <span class="t">and for each of these vectors we need to load all the dimensions in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24072" target="_blank">06:41:12.640</a></span> | <span class="t">in the head dimension dimension because the k if i want to remind you is batch number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24081" target="_blank">06:41:21.120</a></span> | <span class="t">sequence length and head dim now by using this line we are skipping to the right b and to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24092" target="_blank">06:41:32.800</a></span> | <span class="t">right head so it's like we already indexed here and here we already selected an index so right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24098" target="_blank">06:41:38.720</a></span> | <span class="t">now this k is pointing to the beginning of a tensor of two dimension and we tell okay we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24105" target="_blank">06:41:45.120</a></span> | <span class="t">want all the sequence we want some part of this sequence which part the one that is indicated by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24110" target="_blank">06:41:50.960</a></span> | <span class="t">this start kv and how many of in the sequence length we want well we want uh all right i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24120" target="_blank">06:42:00.320</a></span> | <span class="t">it's easy to write it like this so we can write it that from start kv to start kv plus block kv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24128" target="_blank">06:42:08.480</a></span> | <span class="t">uh so we want this number of tensor exactly at this location and for head dimension what do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24136" target="_blank">06:42:16.480</a></span> | <span class="t">we want to select we want to select all the dimensions so we say that we want from zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24141" target="_blank">06:42:21.280</a></span> | <span class="t">to head dimension which is exactly this offskdim okay uh we do it for the k block and we do it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24151" target="_blank">06:42:31.760</a></span> | <span class="t">the v block here i think i didn't change the comment this should be block kv and this should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24160" target="_blank">06:42:40.320</a></span> | <span class="t">be block kv before it was called the block kv1 right like in the original code i simplified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24166" target="_blank">06:42:46.960</a></span> | <span class="t">a little bit the naming i think this one is better easier to follow because in the original code they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24171" target="_blank">06:42:51.360</a></span> | <span class="t">also do for two for loops but in the second for loop they will do it backward just to not change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24176" target="_blank">06:42:56.400</a></span> | <span class="t">the structure of the loops but i think mine is more verbose but easier to understand and probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24183" target="_blank">06:43:03.120</a></span> | <span class="t">less efficient mine is much less efficient um then we have offsq because we need to understand for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24190" target="_blank">06:43:10.240</a></span> | <span class="t">each block of queries how many vectors we need to load and it's indicated by this offsq and how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24198" target="_blank">06:43:18.000</a></span> | <span class="t">are them it's a block q block q in the color of this method was block size micro so it is 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24206" target="_blank">06:43:26.080</a></span> | <span class="t">vectors okay um now we need to access q vectors and o vectors trans uh no q vectors but already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24218" target="_blank">06:43:38.640</a></span> | <span class="t">transposed and the o vectors also we need to access them because we are going to iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24226" target="_blank">06:43:46.000</a></span> | <span class="t">through queries and o vectors actually also why because let's look at here let's look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24234" target="_blank">06:43:54.720</a></span> | <span class="t">formulas in the paper to compute vj so to compute the dvj that's what we are trying to compute here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24243" target="_blank">06:44:03.360</a></span> | <span class="t">we need to iterate through all the do vectors and to compute dk we need to iterate through all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24250" target="_blank">06:44:10.240</a></span> | <span class="t">qi vectors because the qi is a block of vectors so that's why we need um and why do we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24261" target="_blank">06:44:21.840</a></span> | <span class="t">access a q as a transposed because we need to compute let me show you here pij transposed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24270" target="_blank">06:44:30.000</a></span> | <span class="t">compute pij transposed we need to we need the q transposed because the pij would be the softmax of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24275" target="_blank">06:44:35.920</a></span> | <span class="t">the query multiplied by the transpose of the keys after we apply the softmax it becomes p but if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24281" target="_blank">06:44:41.840</a></span> | <span class="t">want the transposed of p then you need to do query transposed k multiplied by query transposed so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24289" target="_blank">06:44:49.680</a></span> | <span class="t">that's why we access the query transposed instead of queries and the way we access the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24295" target="_blank">06:44:55.600</a></span> | <span class="t">transposed is just by playing with the stride so let's do it like this and i have also written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24302" target="_blank">06:45:02.880</a></span> | <span class="t">the comment on why we can do it so this is equivalent to accessing the query uh how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24312" target="_blank">06:45:12.080</a></span> | <span class="t">first okay what is this um what is this operation uh what is this operation here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24318" target="_blank">06:45:18.160</a></span> | <span class="t">this is saying go to the query starting point starting um pointer to the query which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24326" target="_blank">06:45:26.560</a></span> | <span class="t">already pointing to the right batch and to the right head for which this particular program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24331" target="_blank">06:45:31.680</a></span> | <span class="t">should work with and select a two-dimensional vector where you repeat the query starting point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24338" target="_blank">06:45:38.720</a></span> | <span class="t">along the in this case along the columns but we should be repeating it along the rows because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24344" target="_blank">06:45:44.160</a></span> | <span class="t">want to select rows of queries however if we want to select the query transposed we just invert the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24351" target="_blank">06:45:51.280</a></span> | <span class="t">two dimensions so this is a let me actually show you without doing the query transposed so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24356" target="_blank">06:45:56.720</a></span> | <span class="t">do it simplified like this so to access the query um the query pointers without transposition we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24366" target="_blank">06:46:06.000</a></span> | <span class="t">just do like this go to the query tensor and create a 2d tensor where in the rows you put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24374" target="_blank">06:46:14.720</a></span> | <span class="t">the starting point of each query that you want to get and and replicate each of these points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24382" target="_blank">06:46:22.240</a></span> | <span class="t">also on the column that's the meaning of adding this dimension none this is equivalent to when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24388" target="_blank">06:46:28.000</a></span> | <span class="t">you do in pytorch the unsqueeze like you are calling off q multiplied not unsqueeze i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24400" target="_blank">06:46:40.160</a></span> | <span class="t">one so this is equivalent to adding the column dimension to this tensor and repeating all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24407" target="_blank">06:46:47.360</a></span> | <span class="t">values that are on the row on all the um on the columns how many columns will be there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24413" target="_blank">06:46:53.840</a></span> | <span class="t">it will be broadcasted when we sum it with this tensor here this is a combination of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24420" target="_blank">06:47:00.720</a></span> | <span class="t">unsqueezing and broadcasting so we are taking the query vectors indicated by off skew</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24426" target="_blank">06:47:06.800</a></span> | <span class="t">and then we are for each query vector we are selecting all the head dimensions indicated by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24436" target="_blank">06:47:16.720</a></span> | <span class="t">dim if you invert this broadcasting it will create the transposed of the the the query vector that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24444" target="_blank">06:47:24.480</a></span> | <span class="t">you are trying to access so this stuff here is equivalent to the these two lines so accessing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24451" target="_blank">06:47:31.200</a></span> | <span class="t">query and then transposing and uh it's something that you can do uh i could write down what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24459" target="_blank">06:47:39.680</a></span> | <span class="t">happening at the pointer level so basically you need to think of off skew as being a vector of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24466" target="_blank">06:47:46.640</a></span> | <span class="t">pointers we multiplied by the sequence stride which tells us how many element we need to skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24474" target="_blank">06:47:54.880</a></span> | <span class="t">to go from one query vector to the next because each stride q will be the stride will will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24482" target="_blank">06:48:02.240</a></span> | <span class="t">equal to in the case the head dimension is 128 the stride of the sequence dimension will be 128</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24489" target="_blank">06:48:09.040</a></span> | <span class="t">it means that to go from one query vector to the next you need to um you need to uh go forward by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24497" target="_blank">06:48:17.040</a></span> | <span class="t">128 elements because i want to remind you that in the memory the tensors are always stored like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24503" target="_blank">06:48:23.440</a></span> | <span class="t">flattened like each dimension is flattened with the next dimension so imagine you have three rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24511" target="_blank">06:48:31.040</a></span> | <span class="t">and four columns but the first you will have the first three rows then the sorry the first row so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24516" target="_blank">06:48:36.320</a></span> | <span class="t">the first four columns then the next four columns then the next four columns row after row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24521" target="_blank">06:48:41.040</a></span> | <span class="t">it's difficult to visualize until you write it down so how to write it down take um create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24530" target="_blank">06:48:50.640</a></span> | <span class="t">vector of off skew so what is off skew at the beginning it's is a range that is from here from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24538" target="_blank">06:48:58.720</a></span> | <span class="t">0 to 100 no 0 to 32 0 1 2 3 4 5 6 7 etc etc we are multiplying each one with the stride of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24551" target="_blank">06:49:11.280</a></span> | <span class="t">sequence so this will not skip any element this will skip exactly 128 elements this will skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24558" target="_blank">06:49:18.000</a></span> | <span class="t">exactly implying that the head dimension is 128 this will skip two times 128 elements this will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24565" target="_blank">06:49:25.760</a></span> | <span class="t">skip three times 128 elements and then we are adding also another dimension to this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24573" target="_blank">06:49:33.920</a></span> | <span class="t">so this will be a vector then you broadcast it on head dimension number of columns and to each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24580" target="_blank">06:49:40.400</a></span> | <span class="t">them you add one number so it will become a vector like for okay let me just do it guys otherwise i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24586" target="_blank">06:49:46.960</a></span> | <span class="t">think it's too confusing okay so we have a vector that is as follows so zero then we have 128 then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24596" target="_blank">06:49:56.000</a></span> | <span class="t">we have two times 128 then we have three times 128 etc etc we are adding how many columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24603" target="_blank">06:50:03.120</a></span> | <span class="t">indicated by off dim so off dim has how many columns so it has a head dim number of columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24608" target="_blank">06:50:08.480</a></span> | <span class="t">please for simplicity let's pretend it's not 128 dimensions let's pretend it's four dimensions so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24614" target="_blank">06:50:14.800</a></span> | <span class="t">this will be four this will be two times four this will be three times four we are adding another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24622" target="_blank">06:50:22.320</a></span> | <span class="t">dimension that is the dim dimension each one multiplied by the stride of dim which will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24628" target="_blank">06:50:28.720</a></span> | <span class="t">one because it's the last dimension stride dim so we are adding how many columns four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24638" target="_blank">06:50:38.480</a></span> | <span class="t">so we are adding um one zero one two three i guess zero one two three right also to this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24645" target="_blank">06:50:45.920</a></span> | <span class="t">we are adding oh my god zero one two three and also to this one we are adding zero one two three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24656" target="_blank">06:50:56.720</a></span> | <span class="t">okay and then also to this one we are adding zero one two three so what this will select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24665" target="_blank">06:51:05.280</a></span> | <span class="t">this will select from the starting point of the pointer q it will select the element zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24672" target="_blank">06:51:12.160</a></span> | <span class="t">then the element one then the element two and then the element three which is exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24678" target="_blank">06:51:18.240</a></span> | <span class="t">head dimension of the first vector that we should be selecting then it will select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24683" target="_blank">06:51:23.280</a></span> | <span class="t">the element four from the starting point of the vector the element uh sorry this one let me write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24689" target="_blank">06:51:29.760</a></span> | <span class="t">the result of this operation so this one will be zero one two three then it will select the element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24694" target="_blank">06:51:34.720</a></span> | <span class="t">four five six seven then it will select the element um eight i guess nine ten eleven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24701" target="_blank">06:51:41.600</a></span> | <span class="t">and then it will select the element 12 13 14 15 so from the starting point of where this q is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24712" target="_blank">06:51:52.480</a></span> | <span class="t">pointing it will select the first element right after this q the second element right after this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24717" target="_blank">06:51:57.840</a></span> | <span class="t">q the third element right after this q etc etc and this will be the you can see that this will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24723" target="_blank">06:52:03.600</a></span> | <span class="t">first query vector this will be the second query vector this will be the third query vector this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24727" target="_blank">06:52:07.920</a></span> | <span class="t">the fourth query vector because in the memory they are stored one after another they are flattened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24733" target="_blank">06:52:13.200</a></span> | <span class="t">so in the memory they are stored like this they are stored like the following they are stored</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24739" target="_blank">06:52:19.600</a></span> | <span class="t">like this one after another so it will select all of them and it also create a virtual tensor with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24745" target="_blank">06:52:25.920</a></span> | <span class="t">the right shape that we want to visualize it into so as you saw as we saw before when you work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24752" target="_blank">06:52:32.480</a></span> | <span class="t">a tensor layout in memory you can always view it as whatever shape you like based on the shape that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24758" target="_blank">06:52:38.400</a></span> | <span class="t">you want and the reshaping is always free doesn't involve changing the arrangement of the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24764" target="_blank">06:52:44.960</a></span> | <span class="t">in the memory i hope now it's more clear so now we can proceed further oh my god it was quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24771" target="_blank">06:52:51.200</a></span> | <span class="t">complicated so whenever i get stuck i just draw things and i think you should do it too because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24776" target="_blank">06:52:56.320</a></span> | <span class="t">that's the only way to learn if you try to imagine everything in your head it's always difficult</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24782" target="_blank">06:53:02.080</a></span> | <span class="t">and we do the same job for the o vectors so in the o vectors we don't access it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24788" target="_blank">06:53:08.320</a></span> | <span class="t">access it as a transpose because we don't need it in transpose only the q we need it in transposed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24794" target="_blank">06:53:14.240</a></span> | <span class="t">okay it traced through the sequence dimension of the query so we start from the query number zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24801" target="_blank">06:53:21.280</a></span> | <span class="t">in the current um well in the query we need to go through the all the sequence length dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24807" target="_blank">06:53:27.520</a></span> | <span class="t">because only the key we select the right key that we want to work with so i want to remind you here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24812" target="_blank">06:53:32.720</a></span> | <span class="t">we fix the key and we go through all the queries but the query we need to start from zero until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24818" target="_blank">06:53:38.080</a></span> | <span class="t">sequence length so the number of steps of this for loop will be sequence length divided by block q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24824" target="_blank">06:53:44.800</a></span> | <span class="t">so if we have a 1000 elements in the sequence and block q is 32 it will be 1000 divided by 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24834" target="_blank">06:53:54.320</a></span> | <span class="t">a bad choice of 1000 should be 1024 otherwise it's not divisible so then we go through each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24841" target="_blank">06:54:01.440</a></span> | <span class="t">block in this for loop and we load a block of q the first one indicated by our pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24847" target="_blank">06:54:07.360</a></span> | <span class="t">and at the end of the iteration we will move it to the next to the next block of q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24852" target="_blank">06:54:12.160</a></span> | <span class="t">okay we'll add also the log sum exp values that are stored in the m matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24860" target="_blank">06:54:20.800</a></span> | <span class="t">because we want to compute on the fly pt pt is the transposed of the softmax of query multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24869" target="_blank">06:54:29.520</a></span> | <span class="t">by the keys but we want to not take a query multiply by the transpose of the key and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24874" target="_blank">06:54:34.160</a></span> | <span class="t">do the transpose we just already access q as a transposed so we can already compute the pt instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24880" target="_blank">06:54:40.400</a></span> | <span class="t">of computing p and then transposing it um so we load the offsets of the elements that we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24889" target="_blank">06:54:49.200</a></span> | <span class="t">from this log sum exp matrix which is the m matrix that we computed during the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24897" target="_blank">06:54:57.040</a></span> | <span class="t">and we access a block of q at a time the one we are currently working with in the iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24905" target="_blank">06:55:05.440</a></span> | <span class="t">then we access a query key transposed already so we do the if you want to get the pt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24917" target="_blank">06:55:17.440</a></span> | <span class="t">p should be um this is actually not p because we didn't do the softmax it's actually s t but okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24924" target="_blank">06:55:24.800</a></span> | <span class="t">if you want to get the pt you need to get the softmax of st the softmax of st is what it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24934" target="_blank">06:55:34.560</a></span> | <span class="t">it's transposed of s what is s is a query multiplied by transposed of the key so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24939" target="_blank">06:55:39.440</a></span> | <span class="t">get st you need to do um key transposed no key multiplied by query transposed so as you remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24945" target="_blank">06:55:45.200</a></span> | <span class="t">in the matrix multiplication if you transpose the matrix multiplication you need to also invert the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24951" target="_blank">06:55:51.520</a></span> | <span class="t">two element in the matrix multiplication so that's why we are doing a key multiplied by query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24956" target="_blank">06:55:56.720</a></span> | <span class="t">transposed this will give us s transposed we are also scaling it with the softmax scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24963" target="_blank">06:56:03.440</a></span> | <span class="t">before we apply the to apply the softmax we just need to do the exponential of each element minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24970" target="_blank">06:56:10.640</a></span> | <span class="t">its maximum divide by the normalization value but with the log sum extract we just need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24976" target="_blank">06:56:16.000</a></span> | <span class="t">each element subtracted by the m value which already includes the normalization factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24982" target="_blank">06:56:22.640</a></span> | <span class="t">i think i already did the derivation of this so we don't need to go through that again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24988" target="_blank">06:56:28.000</a></span> | <span class="t">okay so now we have the pt block actually so in this formula i should have written st actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=24995" target="_blank">06:56:35.920</a></span> | <span class="t">okay then when doing the causal attention we also need to mask out some values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25007" target="_blank">06:56:47.760</a></span> | <span class="t">so as you can see here so in this case the causal mask is applied after the softmax has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25016" target="_blank">06:56:56.480</a></span> | <span class="t">been computed because during this one is you are used to compute the apply the soft the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25023" target="_blank">06:57:03.200</a></span> | <span class="t">causal mask before computing the softmax attention but this is actually during the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25028" target="_blank">06:57:08.960</a></span> | <span class="t">because you don't want the normalization factor to be affected by the element that should be zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25034" target="_blank">06:57:14.240</a></span> | <span class="t">but we already computed the normalization factor so it cannot be affected anymore so we can compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25040" target="_blank">06:57:20.880</a></span> | <span class="t">we can mask out after applying the software because the normalization factor has already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25045" target="_blank">06:57:25.200</a></span> | <span class="t">been calculated based on the fact that we applied the mask and that's why we we can apply it after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25050" target="_blank">06:57:30.880</a></span> | <span class="t">applying the softmax so the mask is always the same so if the query is more than the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25058" target="_blank">06:57:38.000</a></span> | <span class="t">index of the query so the mask is true in this case for all the values that do not need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25064" target="_blank">06:57:44.800</a></span> | <span class="t">masked so all the values that do not need to be masked are these ones here and all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25070" target="_blank">06:57:50.640</a></span> | <span class="t">value will be replaced with the zeros all right so after we have the pt block already masked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25080" target="_blank">06:58:00.640</a></span> | <span class="t">we can calculate dv dv i will write i will point to the right formula in the paper so we load a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25087" target="_blank">06:58:07.680</a></span> | <span class="t">block of do why do we not load a block of do let's look at the paper so how to compute the dv block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25094" target="_blank">06:58:14.480</a></span> | <span class="t">so the dv block is computed as the old dv plus so a repeated sum as you can see as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25103" target="_blank">06:58:23.520</a></span> | <span class="t">it's here plus equal the old dv plus pt so here pt dropped indicates the pij after applying the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25114" target="_blank">06:58:34.960</a></span> | <span class="t">dropout in this implementation we don't support the dropout and also very few models actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25120" target="_blank">06:58:40.320</a></span> | <span class="t">use the dropout in the attention so pt multiplied by doi so a block of doi and doi is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25130" target="_blank">06:58:50.720</a></span> | <span class="t">block that should be also doi and ki qi are referring to always the same block of rows in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25138" target="_blank">06:58:58.960</a></span> | <span class="t">the respective tensors that's why because this inner iteration i indicates a block of q and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25148" target="_blank">06:59:08.320</a></span> | <span class="t">block of o but we are always referring to the same positions in the tensors because do has the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25155" target="_blank">06:59:15.520</a></span> | <span class="t">shape as dq so we go through the blocks of query and the do simultaneously because one is needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25163" target="_blank">06:59:23.920</a></span> | <span class="t">for dv so for dv we need do and for dk we need q and that's why we compute the dv as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25172" target="_blank">06:59:32.960</a></span> | <span class="t">just like from the paper so pt block multiplied by do as you can see it's a p transpose multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25178" target="_blank">06:59:38.800</a></span> | <span class="t">by the o block so we have computed computed the do block then we need to load the di element that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25187" target="_blank">06:59:47.600</a></span> | <span class="t">we computed pre-computed initially with the first call to the function called the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25196" target="_blank">06:59:56.000</a></span> | <span class="t">backward pre-process because we will need it for dk so let's see and how many of them we are loading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25206" target="_blank">07:00:06.800</a></span> | <span class="t">exactly the same number of query that we load because they are we load always the same number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25213" target="_blank">07:00:13.200</a></span> | <span class="t">of block size micro number of vectors okay i will copy some stuff and explain it step by step so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25221" target="_blank">07:00:21.520</a></span> | <span class="t">the next operation that we need to do is to compute this dk to compute the dk we need the dst</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25230" target="_blank">07:00:30.240</a></span> | <span class="t">to compute the st we need to to get a dpt so let's go one by one let's go from the back from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25239" target="_blank">07:00:39.200</a></span> | <span class="t">the end to the beginning of this formulas so we don't understand where everything is used to where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25247" target="_blank">07:00:47.280</a></span> | <span class="t">everything is created so let's start from dk if you look at the paper dk is equal to the old dk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25255" target="_blank">07:00:55.120</a></span> | <span class="t">plus ds transposed multiplied by a block of q and this is what is written here so it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25263" target="_blank">07:01:03.360</a></span> | <span class="t">plus equal means basically just the old plus the new some it's an incremental addition so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25270" target="_blank">07:01:10.240</a></span> | <span class="t">increment the old k with some new stuff which is this stuff here so the softmax scale multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25277" target="_blank">07:01:17.200</a></span> | <span class="t">because also there is a softmax scale this tau here multiplied by the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25283" target="_blank">07:01:23.440</a></span> | <span class="t">between dst block and the transposed of um and and q and q you can see here this q but we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25294" target="_blank">07:01:34.960</a></span> | <span class="t">have a q we have a q transpose so we take the transpose of q transpose and it becomes back q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25300" target="_blank">07:01:40.560</a></span> | <span class="t">now let's look at this dst block dst is calculated as follows so in the formula of the paper we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25308" target="_blank">07:01:48.720</a></span> | <span class="t">ds ds is here it is equal yeah it is here and it is equal to a block pij multiplied element wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25319" target="_blank">07:01:59.280</a></span> | <span class="t">with dpi minus di now um we don't need ds we need ds transposed so to compute ds transposed this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25330" target="_blank">07:02:10.000</a></span> | <span class="t">an element wise multiplication not a matrix multiplication which means that when you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25334" target="_blank">07:02:14.800</a></span> | <span class="t">the transport of this operation you don't need to invert anything you just need to take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25338" target="_blank">07:02:18.480</a></span> | <span class="t">transpose of the two operands so to compute the st we take the transposed of p which is the pt and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25345" target="_blank">07:02:25.920</a></span> | <span class="t">we already have that and then the transpose of everything that is inside of the parentheses so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25351" target="_blank">07:02:31.280</a></span> | <span class="t">this dpt minus di where we inverted the rows with the columns so this dpt is what well in the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25360" target="_blank">07:02:40.480</a></span> | <span class="t">we know the formula for dp dp is here and it is equal to d wait dp here and it is equal to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25371" target="_blank">07:02:51.280</a></span> | <span class="t">multiplied by b transposed so but we don't need the dp we need the dpt and in this case it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25378" target="_blank">07:02:58.080</a></span> | <span class="t">an element wise multiplication it is a matrix multiplication so um in order to get not a dp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25385" target="_blank">07:03:05.760</a></span> | <span class="t">dp but dpt we need to take the transpose of these two operands of this matrix multiplication and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25390" target="_blank">07:03:10.960</a></span> | <span class="t">in the matrix multiplication when you take the transpose you need to also invert the order of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25395" target="_blank">07:03:15.280</a></span> | <span class="t">the two operands so we need to take the vt transposed which becomes v so the v block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25402" target="_blank">07:03:22.400</a></span> | <span class="t">matrix multiplied by the other operand so doi transposed and that's why we are doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25409" target="_blank">07:03:29.120</a></span> | <span class="t">transpose of do right now i'm not going through all the single pointers because i already told</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25416" target="_blank">07:03:36.240</a></span> | <span class="t">you how to check what a pointer is pointing to and what an offset is referring to i hope that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25422" target="_blank">07:03:42.400</a></span> | <span class="t">now you have a better understanding on how these pointers work in triton which is also the same way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25428" target="_blank">07:03:48.480</a></span> | <span class="t">in the in which they work in cuda because in the gpu we only get a pointer to the starting point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25435" target="_blank">07:03:55.280</a></span> | <span class="t">to the starting address of the tensor and then we need to work out all these indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25439" target="_blank">07:03:59.040</a></span> | <span class="t">we have computed the dk block so we now go to the next query to the next block of queries and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25447" target="_blank">07:04:07.440</a></span> | <span class="t">so the next block of queries because we are fixing k and v blocks and we are iterating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25456" target="_blank">07:04:16.960</a></span> | <span class="t">through all the queries so we need to move the query transpose the pointers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25462" target="_blank">07:04:22.320</a></span> | <span class="t">by stride sequence which means that how can we go from one query to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25467" target="_blank">07:04:27.200</a></span> | <span class="t">and we multiply it with the current block q which is a vector which indicates the pointers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25474" target="_blank">07:04:34.080</a></span> | <span class="t">to the current element in q that we are accessing and we do it also for do and we use the block q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25479" target="_blank">07:04:39.840</a></span> | <span class="t">as element and the stride q because do and q all have the same shape okay after we have run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25488" target="_blank">07:04:48.480</a></span> | <span class="t">for loop of all the queries we can store this dk and dv block so we write it back as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25496" target="_blank">07:04:56.080</a></span> | <span class="t">and this is the end of our function guys so we save the dv block exactly in the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25505" target="_blank">07:05:05.520</a></span> | <span class="t">inside of the current okay dv is already i believe pointing to the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25510" target="_blank">07:05:10.640</a></span> | <span class="t">batch and to the right head because we incremented it here and also in the case of dk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25516" target="_blank">07:05:16.400</a></span> | <span class="t">then we need to tell it in the sequence dimension where they should save this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25520" target="_blank">07:05:20.320</a></span> | <span class="t">block of k and v and this is indicated by this one we say and we create the the pointers just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25528" target="_blank">07:05:28.240</a></span> | <span class="t">like before guys don't make me do it again it's a really easy if you write it down like you write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25535" target="_blank">07:05:35.120</a></span> | <span class="t">this vector of key and values pointers which is not pointers actually they are a range of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25545" target="_blank">07:05:45.920</a></span> | <span class="t">of key and value that you need to take from the sequence dimension you add another dimension that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25552" target="_blank">07:05:52.320</a></span> | <span class="t">is the column so you repeat each value in the columns and then you add the dimension here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25558" target="_blank">07:05:58.080</a></span> | <span class="t">for the head dimension anyway after we have calculated the pointers where we should store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25563" target="_blank">07:06:03.280</a></span> | <span class="t">the dk and the dv we store them in the the pointers of we store them in the dv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25572" target="_blank">07:06:12.000</a></span> | <span class="t">i mean we store them in the dv tensor and the dk tensor what do we save we save the dv block and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25578" target="_blank">07:06:18.480</a></span> | <span class="t">dk block which is the one that we were incrementally changing in the for loop that we have written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25585" target="_blank">07:06:25.840</a></span> | <span class="t">okay now that we finished this one we can go to the next function that will do the other for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25592" target="_blank">07:06:32.400</a></span> | <span class="t">so let's do it okay so now we do the second part of the iteration which is this one so let me just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25600" target="_blank">07:06:40.160</a></span> | <span class="t">copy it and then we we describe it uh let's write it here okay we use the same launch grid as before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25609" target="_blank">07:06:49.600</a></span> | <span class="t">of course we need to declare this function and again we um we because the grid is defined for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25615" target="_blank">07:06:55.200</a></span> | <span class="t">the block size macro for what is the thing that we keep fixed and then we in the side of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25621" target="_blank">07:07:01.520</a></span> | <span class="t">for iteration we do um steps of block size micro in this case we are fixing q and we are iterating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25629" target="_blank">07:07:09.840</a></span> | <span class="t">through k and v because we need to compute dq right now we have computed dk and dv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25636" target="_blank">07:07:16.320</a></span> | <span class="t">okay the i believe the arguments are the same as before so and actually this is also the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25643" target="_blank">07:07:23.680</a></span> | <span class="t">why in the original implementation on the triton website the author decided to um to use the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25650" target="_blank">07:07:30.880</a></span> | <span class="t">for loop but with different arguments and i believe it was a little confusing so that's why i just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25656" target="_blank">07:07:36.720</a></span> | <span class="t">separated them i just repeat the code twice it's the goal of this video is to be as easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25662" target="_blank">07:07:42.240</a></span> | <span class="t">to understand as possible not to be as efficient as possible so uh let's go uh here so let me copy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25670" target="_blank">07:07:50.400</a></span> | <span class="t">the signature again let me define this function here okay so uh again we need to first move the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25682" target="_blank">07:08:02.720</a></span> | <span class="t">query key and value uh to the right pointer so which will point to the exact batch and the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25688" target="_blank">07:08:08.640</a></span> | <span class="t">head that we are working with in this program so um let's do it let me check where is the code here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25697" target="_blank">07:08:17.520</a></span> | <span class="t">and the first part is exactly the same as the other for loop that we have written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25701" target="_blank">07:08:21.840</a></span> | <span class="t">so let's go here and really is i just copied so it's exactly the same so we check what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25710" target="_blank">07:08:30.720</a></span> | <span class="t">index batch head we move the query key value pointers to the right place the d o d q d k d v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25716" target="_blank">07:08:36.480</a></span> | <span class="t">point to the right place the m and d to the right place exactly like before so i don't think i need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25721" target="_blank">07:08:41.840</a></span> | <span class="t">to explain that again and then we load a block of q the one that we will keep fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25728" target="_blank">07:08:48.880</a></span> | <span class="t">so dq</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25732" target="_blank">07:08:52.880</a></span> | <span class="t">let me load a lot of stuff here actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25738" target="_blank">07:08:58.800</a></span> | <span class="t">okay we define the offset that we will need to load the blocks of k and p in the head dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25745" target="_blank">07:09:05.840</a></span> | <span class="t">because we are going to iterate in the k and v we will access them as transposed blocks so instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25752" target="_blank">07:09:12.880</a></span> | <span class="t">of accessing them directly as a k and v we access access them as a kt and pt and you know that that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25760" target="_blank">07:09:20.240</a></span> | <span class="t">possible just by changing the strides in this case because we are treating them as a 2d vectors we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25766" target="_blank">07:09:26.960</a></span> | <span class="t">treat the offs kv when you want to access k as just not transposed but k you treat this offs kv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25774" target="_blank">07:09:34.880</a></span> | <span class="t">as a row vector sorry a column vector so you repeat on the rows each k offset that you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25783" target="_blank">07:09:43.680</a></span> | <span class="t">access in this case we are repeating it as a we are treating it as a row vector so it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25789" target="_blank">07:09:49.280</a></span> | <span class="t">repeated on the rows um sorry it will be broadcasted on the column dimension and that's how you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25797" target="_blank">07:09:57.520</a></span> | <span class="t">access the transposed version of k and how you can access the transposed version of v another thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25803" target="_blank">07:10:03.760</a></span> | <span class="t">that we are doing is we are loading the q vector which vector well based on offs q which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25809" target="_blank">07:10:09.920</a></span> | <span class="t">q vector which vector well based on offs q which is based on the start q which is based on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25816" target="_blank">07:10:16.240</a></span> | <span class="t">exact starting point in which this particular program should be working with because this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25820" target="_blank">07:10:20.640</a></span> | <span class="t">particular program works as two dimensions the first dimension indicate which batch and which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25826" target="_blank">07:10:26.720</a></span> | <span class="t">head this program should be working with and the second dimension which is the program index number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25831" target="_blank">07:10:31.440</a></span> | <span class="t">zero indicates which among all the sequence length which query this particular program is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25837" target="_blank">07:10:37.120</a></span> | <span class="t">work with this is indicated by the index block this should be actually q in this case i forgot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25843" target="_blank">07:10:43.840</a></span> | <span class="t">to change the name so actually let me change it so it's index q because we start we skip some q how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25850" target="_blank">07:10:50.000</a></span> | <span class="t">many q we skip based on the index of the current program multiplied by how many blocks have already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25857" target="_blank">07:10:57.600</a></span> | <span class="t">been processed by the previous programs this will tell us inside of the sequence length what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25864" target="_blank">07:11:04.400</a></span> | <span class="t">queries that this one needs to select so that's why we use the start query plus the range that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25870" target="_blank">07:11:10.320</a></span> | <span class="t">is block q so imagine the starting query for this program among all the sequence length is 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25876" target="_blank">07:11:16.320</a></span> | <span class="t">then this will load the query row 100 101 102 until 100 plus block q minus 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25884" target="_blank">07:11:24.640</a></span> | <span class="t">this is the range that we of the query vectors that we will load in this program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25892" target="_blank">07:11:32.720</a></span> | <span class="t">we load the block of q by using a q plus the offset repeated on the columns so we treat it as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25901" target="_blank">07:11:41.520</a></span> | <span class="t">column vector but we repeat broadcast it on the rows vector where each column will be one head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25911" target="_blank">07:11:51.200</a></span> | <span class="t">dimension multiplied by the stride in this case we actually can also not multiply by the stride</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25915" target="_blank">07:11:55.680</a></span> | <span class="t">because the stride in the dimension dimension so the last dimension of the batch is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25921" target="_blank">07:12:01.680</a></span> | <span class="t">because to go from one um actually the stride um how it is defined the stride of the last dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25930" target="_blank">07:12:10.000</a></span> | <span class="t">is always one because to go one element to the next element you should move to move it to by one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25936" target="_blank">07:12:16.720</a></span> | <span class="t">element um so we load the dq which is the stuff that we are going to compute in this um iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25947" target="_blank">07:12:27.040</a></span> | <span class="t">and then we have a do that we need to load and the do we use the same offset as q because the do and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25953" target="_blank">07:12:33.680</a></span> | <span class="t">dq have the same shape and they work in the same way so we load a block of q and we load the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25960" target="_blank">07:12:40.080</a></span> | <span class="t">corresponding block of o of do in this case and the do has the same shape as o which has the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25967" target="_blank">07:12:47.440</a></span> | <span class="t">shape as q plus we need to load also the m normalization factors which are in the m matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25974" target="_blank">07:12:54.400</a></span> | <span class="t">which one the chorus the one corresponding to this particular group of queries that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25978" target="_blank">07:12:58.800</a></span> | <span class="t">going to work with in this particular program we start with the offsets are the as you can see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25986" target="_blank">07:13:06.800</a></span> | <span class="t">offsets are the first block of kv starting from the zero position so because we will iterate through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=25993" target="_blank">07:13:13.680</a></span> | <span class="t">all the kvs and we start from the zero kv so the key key vector zero and the v vector zero and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26001" target="_blank">07:13:21.040</a></span> | <span class="t">we move by block kv number of vectors forward at each iteration i hope i didn't go too fast because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26010" target="_blank">07:13:30.080</a></span> | <span class="t">most of the things that are written here are very similar to what we have already done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26014" target="_blank">07:13:34.240</a></span> | <span class="t">in the the other for loop so i don't want to be you know repeat myself too much um what did matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26021" target="_blank">07:13:41.840</a></span> | <span class="t">is actually the formulas that we will use which is exactly the one in the paper so uh we go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26028" target="_blank">07:13:48.880</a></span> | <span class="t">these blocks of kv we load the first block of k transposed and v transposed which is loaded like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26039" target="_blank">07:13:59.200</a></span> | <span class="t">this as usual you tell it what pointers the elements you want to load and what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26044" target="_blank">07:14:04.320</a></span> | <span class="t">pointers of another element that you want to know that it will load the the block that you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26049" target="_blank">07:14:09.360</a></span> | <span class="t">asking triton to load inside of the sram so this stuff all reside in the sram and also q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26055" target="_blank">07:14:15.520</a></span> | <span class="t">resides in the sram and also do reside in the sram um then we compute the query multiplied by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26063" target="_blank">07:14:23.840</a></span> | <span class="t">transpose of the keys because we need to compute the p block so the query the qk block is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26071" target="_blank">07:14:31.520</a></span> | <span class="t">query in the current query block with the k transposed in the current query block and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26076" target="_blank">07:14:36.880</a></span> | <span class="t">current key block um why but we access the query the keys already as a transposed so we don't need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26085" target="_blank">07:14:45.360</a></span> | <span class="t">to transpose it and anyway even if we did if we need to transpose it it's just um it's not it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26093" target="_blank">07:14:53.440</a></span> | <span class="t">doesn't require any computation to transpose matrix we just access it in a different way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26098" target="_blank">07:14:58.480</a></span> | <span class="t">because in the memory layout it's always stored kind of as a flattened array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26106" target="_blank">07:15:06.160</a></span> | <span class="t">then we compute the p block which is the output of the softmax so each of the query key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26110" target="_blank">07:15:10.880</a></span> | <span class="t">we subtract the log sum exp value for the this block of queries that's why for loading the m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26119" target="_blank">07:15:19.680</a></span> | <span class="t">block we use the offsets of the queries that we are loading and as you remember the m block already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26126" target="_blank">07:15:26.400</a></span> | <span class="t">includes also the normalization factor because each m is actually the maximum value for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26132" target="_blank">07:15:32.160</a></span> | <span class="t">row plus the logarithm of the normalization factor that when you apply with the properties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26136" target="_blank">07:15:36.800</a></span> | <span class="t">of the exponential it goes into the denominator okay and then we apply again the autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26144" target="_blank">07:15:44.240</a></span> | <span class="t">masking oops what did i do let me go back to the code here so we have the stage this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26159" target="_blank">07:15:59.520</a></span> | <span class="t">so when we launch the backward pass stage three indicates that it's a in the forward pass we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26165" target="_blank">07:16:05.760</a></span> | <span class="t">computed the causal attention and the one indicates that we computed the non-causal attention so if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26173" target="_blank">07:16:13.520</a></span> | <span class="t">computed the causal attention in the forward pass we also need to mask out these elements in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26179" target="_blank">07:16:19.120</a></span> | <span class="t">backward pass so we check um we create the mask which tells us which index this mask is true for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26188" target="_blank">07:16:28.720</a></span> | <span class="t">only for the elements for which the query index is more than the key index and if this is true</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26196" target="_blank">07:16:36.400</a></span> | <span class="t">then we uh we don't mask otherwise we mask um let's compute the next operation which is to compute dp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26205" target="_blank">07:16:45.600</a></span> | <span class="t">and ds actually i let's compute directly dk and then we explain it like before so we start from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26211" target="_blank">07:16:51.280</a></span> | <span class="t">the end and we go to where this stuff what is needed to compute it so if you look at the formula</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26218" target="_blank">07:16:58.160</a></span> | <span class="t">uh let me check this one we don't need i think okay let's go here to the ipad okay what we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26233" target="_blank">07:17:13.360</a></span> | <span class="t">trying to compute here is dq so dq as you can see in the paper is a dq is equal to the old dq</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26243" target="_blank">07:17:23.920</a></span> | <span class="t">plus tau which is the softmax scale which is this stuff here multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26251" target="_blank">07:17:31.680</a></span> | <span class="t">the matrix multiplication between the ds and the k block so the ds block is here and the k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26261" target="_blank">07:17:41.040</a></span> | <span class="t">block is the transpose of the kt block because we are accessing k already as a transpose block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26267" target="_blank">07:17:47.840</a></span> | <span class="t">we could also access a k directly as not transposed block by inverting if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26274" target="_blank">07:17:54.000</a></span> | <span class="t">want to access it as a transpose block just do like this like here none this will treat it as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26281" target="_blank">07:18:01.920</a></span> | <span class="t">row vector and broadcast along the columns otherwise and also this one you need to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26288" target="_blank">07:18:08.240</a></span> | <span class="t">so this one you shouldn't need to change because this one you need to treat it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26294" target="_blank">07:18:14.720</a></span> | <span class="t">a column vector the dimensions but if you want to access it as a k transpose then you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26300" target="_blank">07:18:20.080</a></span> | <span class="t">invert these two operations i hope i didn't mess up anything so let's move forward um so okay we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26307" target="_blank">07:18:27.840</a></span> | <span class="t">know that the formula for the dq is exactly the same as the as the paper one but what is this ds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26315" target="_blank">07:18:35.760</a></span> | <span class="t">block let's look at the paper this ds block is coming from this stuff here so this i believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26323" target="_blank">07:18:43.360</a></span> | <span class="t">this stuff here ds which is a pi the p block element wise multiplication with the dpi minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26333" target="_blank">07:18:53.040</a></span> | <span class="t">di which is dpi minus di now what is the this p block the p block is exactly the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26342" target="_blank">07:19:02.160</a></span> | <span class="t">the softmax which we already have what is the dp block well the dp block is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26348" target="_blank">07:19:08.000</a></span> | <span class="t">do multiplied by v transposed which is a do which we already loaded and it's here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26354" target="_blank">07:19:14.800</a></span> | <span class="t">multiplied by the transpose of the v which we already load as transposed and this is how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26360" target="_blank">07:19:20.080</a></span> | <span class="t">compute the dq let's include then of course we need to move to the next block of keys so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26369" target="_blank">07:19:29.280</a></span> | <span class="t">increment the pointers just like before so we move to the next block of keys and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26377" target="_blank">07:19:37.760</a></span> | <span class="t">and also remove the pointers um just like before and then we need to store the result of dq and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26385" target="_blank">07:19:45.680</a></span> | <span class="t">this way we only need to do one write to the hbm by dividing the for loop like the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26392" target="_blank">07:19:52.400</a></span> | <span class="t">so if you look at the original algorithm uh i i don't know if the original algorithm actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26398" target="_blank">07:19:58.560</a></span> | <span class="t">corresponds in to to the implementation that they did in cuda but i don't think so because it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26404" target="_blank">07:20:04.400</a></span> | <span class="t">not be so optimized but in the original algorithm in the paper they say that you need to go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26410" target="_blank">07:20:10.800</a></span> | <span class="t">all the keys and then while going through the keys you need to go to all the queues and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26414" target="_blank">07:20:14.960</a></span> | <span class="t">each queue that you visit then you need to write back the queue while you are updating it which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26419" target="_blank">07:20:19.760</a></span> | <span class="t">not optimized that's why we needed to do two for loops one in which we fix the query and we update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26426" target="_blank">07:20:26.160</a></span> | <span class="t">the keys because each key is updated depends only on a particular block of queue on all the blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26432" target="_blank">07:20:32.720</a></span> | <span class="t">of queue sorry and then we fix the queries and we iterate through all the keys because one block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26437" target="_blank">07:20:37.680</a></span> | <span class="t">queue depends on all the blocks of case and this is why we split and this is the second loop that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26443" target="_blank">07:20:43.680</a></span> | <span class="t">we have written now we have written everything that we needed to for flash attention um the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26450" target="_blank">07:20:50.800</a></span> | <span class="t">forward pass and the backward pass so uh we should be ready to uh launch the uh the kernel i hope i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26458" target="_blank">07:20:58.480</a></span> | <span class="t">didn't make any mistake in copying the code so i don't think i will try to launch it and if there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26464" target="_blank">07:21:04.480</a></span> | <span class="t">is any error i will just use my reference code which i have already written that i used as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26468" target="_blank">07:21:08.720</a></span> | <span class="t">copy the only difference up to now between my reference code and the one that we have written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26474" target="_blank">07:21:14.640</a></span> | <span class="t">is the auto tuning which i didn't explain so let's talk about the auto tuning so the auto tuning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26480" target="_blank">07:21:20.240</a></span> | <span class="t">also something that was already present in the original paper and i kept it as is uh i removed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26485" target="_blank">07:21:25.120</a></span> | <span class="t">the auto tuning for the backward pass but in the forward pass you if you check there is this code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26492" target="_blank">07:21:32.160</a></span> | <span class="t">here that indicates the auto tuning configuration for triton so triton basically cannot know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26500" target="_blank">07:21:40.560</a></span> | <span class="t">beforehand what is the best block size or what is the best block size for the query or what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26505" target="_blank">07:21:45.840</a></span> | <span class="t">best block size for the key and values or what is the best block size for another dimension that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26511" target="_blank">07:21:51.520</a></span> | <span class="t">have we need to try based on the hardware that we are running on based on the availability on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26517" target="_blank">07:21:57.840</a></span> | <span class="t">sram based on the thread coarsening that triton can apply so i didn't talk also about thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26523" target="_blank">07:22:03.920</a></span> | <span class="t">coarsening basically in cuda you can choose if each thread does one atomic operation for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26530" target="_blank">07:22:10.000</a></span> | <span class="t">in a matrix addition each thread is doing one addition of one particular element of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26535" target="_blank">07:22:15.840</a></span> | <span class="t">matrix or it's managing multiple elements this is called thread coarsening and i think i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26541" target="_blank">07:22:21.120</a></span> | <span class="t">check the documentation but i believe triton does it for you based on the block size that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26546" target="_blank">07:22:26.320</a></span> | <span class="t">you give it and the number of warps that you want the number of warps is what is a block of threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26553" target="_blank">07:22:33.040</a></span> | <span class="t">of 32 threads that work cooperatively running the same instruction always at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26560" target="_blank">07:22:40.560</a></span> | <span class="t">the number of stages is more interesting it's an optimization that triton does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26568" target="_blank">07:22:48.480</a></span> | <span class="t">basically it is not loop unrolling so actually let's talk about uh let's talk about software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26575" target="_blank">07:22:55.360</a></span> | <span class="t">pipelining because this is the last part that we need to understand from this code which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26578" target="_blank">07:22:58.640</a></span> | <span class="t">the auto tuning so i believe that the most interesting part here is not choosing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26582" target="_blank">07:23:02.720</a></span> | <span class="t">block size q and the block size k because that is just a kind of you try whatever whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26587" target="_blank">07:23:07.920</a></span> | <span class="t">configuration works best based on the timing through cuda triton will actually run all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26594" target="_blank">07:23:14.000</a></span> | <span class="t">configurations for you every time the sequence length or the head dimension changes and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26598" target="_blank">07:23:18.160</a></span> | <span class="t">every pair of head dimension and sequence length it will choose the best configuration that runs in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26603" target="_blank">07:23:23.360</a></span> | <span class="t">the least amount of time that gives you the best throughput actually so let's look at this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26610" target="_blank">07:23:30.240</a></span> | <span class="t">numstages what is it and how it works so let's do it okay so software pipelining is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26618" target="_blank">07:23:38.880</a></span> | <span class="t">it's used when you have a kind of a for loop so you have a sequential operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26624" target="_blank">07:23:44.800</a></span> | <span class="t">in which each iteration does not depend on the previous iteration so the operations that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26629" target="_blank">07:23:49.280</a></span> | <span class="t">you're doing in one iteration are independent from what you have done in the previous iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26632" target="_blank">07:23:52.480</a></span> | <span class="t">which is more or less what we have done before in our for loops actually there i believe there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26638" target="_blank">07:23:58.400</a></span> | <span class="t">how to say conditions in which this doesn't have to be true so like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26643" target="_blank">07:24:03.920</a></span> | <span class="t">the operations can depend on each other and you still can do software pipelining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26649" target="_blank">07:24:09.600</a></span> | <span class="t">so for example imagine you have the following for loop for loop that rose from one to n and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26656" target="_blank">07:24:16.880</a></span> | <span class="t">first you load some data then you load some other data then you do a matrix multiplication and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26661" target="_blank">07:24:21.280</a></span> | <span class="t">you store some data so here you are reading data here you are reading data here you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26666" target="_blank">07:24:26.240</a></span> | <span class="t">computing some stuff and here you are writing data if we look at what happens at each iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26673" target="_blank">07:24:33.280</a></span> | <span class="t">we will see the following picture imagine our gpu is made up of a compute unit and a unit that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26680" target="_blank">07:24:40.160</a></span> | <span class="t">dedicated to loading stuff so reading from the memory or writing to the memory what we will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26686" target="_blank">07:24:46.080</a></span> | <span class="t">in the time scale is that at the first iteration first we are reading some data and the compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26692" target="_blank">07:24:52.000</a></span> | <span class="t">unit is idle because we need this data then we are reading some more data and the compute unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26696" target="_blank">07:24:56.480</a></span> | <span class="t">is idle because we need this data then finally we have enough data and then we can compute this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26700" target="_blank">07:25:00.960</a></span> | <span class="t">operation and the reading unit is idle and then we are writing some data back to the memory and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26706" target="_blank">07:25:06.160</a></span> | <span class="t">the compute unit is again idle and then it will be idle for another two time steps until it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26712" target="_blank">07:25:12.000</a></span> | <span class="t">enough data to run the computation so as you can see this is not very efficient because at any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26718" target="_blank">07:25:18.560</a></span> | <span class="t">time a point in time there is only one unit working and the other is sitting idle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26724" target="_blank">07:25:24.480</a></span> | <span class="t">so one way to optimize this for loop is to do software pipelining and you can tell triton to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26730" target="_blank">07:25:30.480</a></span> | <span class="t">do it for your for loops by telling it how many stages you want so let's see how it works so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26736" target="_blank">07:25:36.320</a></span> | <span class="t">pipeline a for loop means that first of all you need to convert all these operations into async</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26742" target="_blank">07:25:42.480</a></span> | <span class="t">operations and in cuda at least in the gpu of nvidia there are the async loading from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26748" target="_blank">07:25:48.560</a></span> | <span class="t">memory and the async load writing to the memory which basically means that i spawn a load operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26755" target="_blank">07:25:55.120</a></span> | <span class="t">and after and when i only i check if it has completed when i actually need it so i will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26762" target="_blank">07:26:02.480</a></span> | <span class="t">spawn this operation and this instruction will return immediately and move to the next instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26767" target="_blank">07:26:07.840</a></span> | <span class="t">here i will spawn a load iteration and this will return immediately and move to the next instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26773" target="_blank">07:26:13.280</a></span> | <span class="t">and then i can compute but before computing i just check if these two operations have completed so i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26779" target="_blank">07:26:19.040</a></span> | <span class="t">can spawn immediately two reads and then i just check if this they have completed so with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26786" target="_blank">07:26:26.560</a></span> | <span class="t">software pipelining what we are doing is we are pipelining operations of different iterations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26792" target="_blank">07:26:32.640</a></span> | <span class="t">into a single iterations so first basically what we will do is we will do the read the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26798" target="_blank">07:26:38.720</a></span> | <span class="t">matrix that we need for computing this matrix multiplication then at this next iteration we read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26805" target="_blank">07:26:45.200</a></span> | <span class="t">the the we we read the first matrix of the second iteration and also read the second matrix of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26815" target="_blank">07:26:55.040</a></span> | <span class="t">first iteration so i call it read a and read b which indicates read the first matrix of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26822" target="_blank">07:27:02.080</a></span> | <span class="t">that we need and the b means the read the second matrix that we need all these operations are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26828" target="_blank">07:27:08.880</a></span> | <span class="t">asynchronous then i launch another asynchronous operation at the third iteration that says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26834" target="_blank">07:27:14.160</a></span> | <span class="t">read the the first matrix of the third iteration and then read the second matrix of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26843" target="_blank">07:27:23.040</a></span> | <span class="t">of the second iteration and then compute the matrix multiplication because at the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26851" target="_blank">07:27:31.040</a></span> | <span class="t">iteration this one and this one should have completed but while computing the matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26857" target="_blank">07:27:37.200</a></span> | <span class="t">multiplication i don't keep the loading unit idle because they are still computing this this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26863" target="_blank">07:27:43.600</a></span> | <span class="t">and this load this can only work if you can spawn async operations so at the third iteration i can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26871" target="_blank">07:27:51.600</a></span> | <span class="t">compute this matrix multiplication by using this one and this one because they should have finished</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26876" target="_blank">07:27:56.720</a></span> | <span class="t">but while i'm computing the matrix multiplication i already spawned some async operations to load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26882" target="_blank">07:28:02.080</a></span> | <span class="t">the data necessary for the second iteration and the third iteration so at the fourth iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26888" target="_blank">07:28:08.720</a></span> | <span class="t">i will spawn the loading of the data for the fourth iteration loading the data for the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26895" target="_blank">07:28:15.040</a></span> | <span class="t">iteration while computing the matrix multiplication of the second iteration because they should have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26899" target="_blank">07:28:19.040</a></span> | <span class="t">already completed by now actually it's not like we expect them to have been completed there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26905" target="_blank">07:28:25.360</a></span> | <span class="t">primitives in the language in the CUDA language to check if the operation has completed so actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26911" target="_blank">07:28:31.760</a></span> | <span class="t">before doing the multiplication we will actually check if the async operation has finished so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26917" target="_blank">07:28:37.520</a></span> | <span class="t">not like we just expect it we have finished it with respect to time this is like in javascript</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26923" target="_blank">07:28:43.760</a></span> | <span class="t">you have these things called promise i remember and you can wait for the promise to be finished</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26930" target="_blank">07:28:50.080</a></span> | <span class="t">before you actually need them but you can spawn as many promise as you want in C# i think they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26935" target="_blank">07:28:55.120</a></span> | <span class="t">are called tasks so you spawn as many tasks as you want and then when you need it then you just wait</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26941" target="_blank">07:29:01.680</a></span> | <span class="t">for them only the one that you needed while the other are still running in the background</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26946" target="_blank">07:29:06.080</a></span> | <span class="t">asynchronously this is the whole idea of software pipelining software pipelining as you can see only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26953" target="_blank">07:29:13.840</a></span> | <span class="t">works when you have async operations and also it increases the memory requirement for your program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26959" target="_blank">07:29:19.200</a></span> | <span class="t">because when matrix multiplication one is going to run we may have enough data for the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26968" target="_blank">07:29:28.720</a></span> | <span class="t">two iterations plus half data for the third iteration so we increase the memory requirement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26974" target="_blank">07:29:34.320</a></span> | <span class="t">for the SRAM okay and the Triton will do this software pipelining for you it will convert all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26982" target="_blank">07:29:42.720</a></span> | <span class="t">the load all the stores and maybe also the matrix multiplication into async operations and do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26988" target="_blank">07:29:48.240</a></span> | <span class="t">pipelining for you if you are confused by how it works there is another easy solution to explain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26993" target="_blank">07:29:53.360</a></span> | <span class="t">you how it works because it's already something that we do in model training it is called pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=26998" target="_blank">07:29:58.720</a></span> | <span class="t">parallelism so in pipeline parallelism it works as follows we have a very big neural network that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27006" target="_blank">07:30:06.240</a></span> | <span class="t">does not fit in a single gpu so imagine this neural network is made up of three layers layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27011" target="_blank">07:30:11.040</a></span> | <span class="t">one layer two and layer three but this is so big it does not fit entirely in one single gpu so one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27017" target="_blank">07:30:17.360</a></span> | <span class="t">way would be to put this each layer into one gpu so we put for example layer one into gpu one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27025" target="_blank">07:30:25.520</a></span> | <span class="t">layer two into gpu two layer three into gpu number three so imagine we have an input for this neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27033" target="_blank">07:30:33.680</a></span> | <span class="t">network so we put it to the first gpu the gpu one will process the layer one and generate some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27039" target="_blank">07:30:39.840</a></span> | <span class="t">output which will be transferred to the gpu two which will calculate its own output and transfer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27045" target="_blank">07:30:45.200</a></span> | <span class="t">it to the gpu three which will compute its own output and finally we will have the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27049" target="_blank">07:30:49.280</a></span> | <span class="t">the neural network the problem is when you send the output of the gpu one to the gpu two for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27055" target="_blank">07:30:55.600</a></span> | <span class="t">gpu two to do its own thing the gpu one now is free so it is a waste of resources we could always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27062" target="_blank">07:31:02.160</a></span> | <span class="t">should keep the gpus busy so what one thing that we can do is instead of sending all the the mega</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27068" target="_blank">07:31:08.880</a></span> | <span class="t">batch to the gpu one we send many smaller batches how does it work imagine that we send the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27075" target="_blank">07:31:15.760</a></span> | <span class="t">number zero so batch zero uh to the gpu one the gpu one will compute its output and send it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27083" target="_blank">07:31:23.920</a></span> | <span class="t">the gpu two so now the gpu two is computing the batch number zero so now the batch zero is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27090" target="_blank">07:31:30.560</a></span> | <span class="t">here anymore but now the gpu one is free so we send another micro batch called the batch one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27099" target="_blank">07:31:39.360</a></span> | <span class="t">then the gpu two will finish processing the batch zero and we'll send it to the batch to the gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27106" target="_blank">07:31:46.160</a></span> | <span class="t">number three so now the gpu three has the batch number zero and the gpu two now is free so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27112" target="_blank">07:31:52.240</a></span> | <span class="t">transferred and hopefully also gpu one has finished so we transfer the batch number one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27116" target="_blank">07:31:56.960</a></span> | <span class="t">from gpu one to gpu two the batches and then the gpu one will be free so so we transfer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27125" target="_blank">07:32:05.760</a></span> | <span class="t">here becomes one and now this one is free so because it's gpu one is free we can introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27130" target="_blank">07:32:10.720</a></span> | <span class="t">another batch so batch number two etc etc etc so we always introduce when while moving one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27138" target="_blank">07:32:18.160</a></span> | <span class="t">batch from one gpu to the other we introduce a new batch at the beginning of the pipeline and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27143" target="_blank">07:32:23.440</a></span> | <span class="t">they shift by one position at every iteration this will keep the gpus always busy there is only one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27150" target="_blank">07:32:30.080</a></span> | <span class="t">problem of the pipeline parallelism which is the this bubbling effect because to create this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27155" target="_blank">07:32:35.440</a></span> | <span class="t">pipeline you at the beginning of this um okay actually in the pipeline parallelism you also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27160" target="_blank">07:32:40.240</a></span> | <span class="t">have the problem of the backward step so the backward step has to run exactly in reverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27164" target="_blank">07:32:44.880</a></span> | <span class="t">in the order in which you receive the micro batches while in triton when doing software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27171" target="_blank">07:32:51.520</a></span> | <span class="t">pipelining you have the problem of the prologue and the epilogue because you need to create this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27177" target="_blank">07:32:57.440</a></span> | <span class="t">pipeline and and to start the pipelining and at the end of the pipeline you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27184" target="_blank">07:33:04.320</a></span> | <span class="t">use all the stuff that is currently in the pipeline so only in the beginning step and in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27190" target="_blank">07:33:10.400</a></span> | <span class="t">the last step of this for loop your um all the units of this gpu may not be working simultaneously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27198" target="_blank">07:33:18.000</a></span> | <span class="t">which what does it mean it means that in order to use pipelining you want the number of iterations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27203" target="_blank">07:33:23.840</a></span> | <span class="t">of your for loop to be much more bigger than the number of stages in which your iteration is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27208" target="_blank">07:33:28.400</a></span> | <span class="t">divided into in this case we have four stages these are called stages so you want the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27213" target="_blank">07:33:33.520</a></span> | <span class="t">of iterations to be much more to be much larger than the number of stages all right guys finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27219" target="_blank">07:33:39.760</a></span> | <span class="t">i have completed the video um i hope that you learned a lot from this video i believe that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27225" target="_blank">07:33:45.520</a></span> | <span class="t">can run the triton code so let's run it actually uh let's see i copied everything i believe we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27232" target="_blank">07:33:52.240</a></span> | <span class="t">put the code to test it but we didn't call uh put the uh main method which we can copy right now i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27240" target="_blank">07:34:00.400</a></span> | <span class="t">hope there is no error so i really hope there is no error i really hope so um let me check if i am</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27249" target="_blank">07:34:09.440</a></span> | <span class="t">in the right machine i am so let's just run program pray if there is an error i will just copy my own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27260" target="_blank">07:34:20.880</a></span> | <span class="t">reference implementation but i hope it works because otherwise i forgot something so i'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27267" target="_blank">07:34:27.280</a></span> | <span class="t">running my code on an h100 because my company has h100 if you have a smaller gpu what you can do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27274" target="_blank">07:34:34.720</a></span> | <span class="t">you can reduce the sequence length you can reduce the batch size i think it's already one when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27281" target="_blank">07:34:41.600</a></span> | <span class="t">call it uh oh no the batch size you can reduce the batch size the number of heads the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27285" target="_blank">07:34:45.600</a></span> | <span class="t">length you can even put head dimension equal to 8 and sequence length equal to 16 let's check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27291" target="_blank">07:34:51.680</a></span> | <span class="t">run backward triton backward returned an incorrect number of gradient expected 5 got 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27299" target="_blank">07:34:59.600</a></span> | <span class="t">we probably forgot some return statement i believe yes so i forgot the return statement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27308" target="_blank">07:35:08.800</a></span> | <span class="t">here so in the backward pass after running the last for loop we need to return the stuff that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27314" target="_blank">07:35:14.160</a></span> | <span class="t">we have computed cross finger again okay passed so the backward pass that is computed by torch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27324" target="_blank">07:35:24.080</a></span> | <span class="t">it is equivalent to our backward patch up to 10 to the power of minus 2 error absolute error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27331" target="_blank">07:35:31.040</a></span> | <span class="t">so when you as you can see this backward that we run here is different than the backward that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27337" target="_blank">07:35:37.360</a></span> | <span class="t">we run here because when you apply triton attention it will introduce a new computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27342" target="_blank">07:35:42.400</a></span> | <span class="t">graph in the computation graph of our tensors that will include this triton attention operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27347" target="_blank">07:35:47.920</a></span> | <span class="t">and when pytorch want to compute the backward pass it will just call the backward function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27352" target="_blank">07:35:52.080</a></span> | <span class="t">of this triton attention to compute it and it will populate the grad value of all the tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27358" target="_blank">07:35:58.400</a></span> | <span class="t">that are the input to this triton attention and this is how pytorch autograd works guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27364" target="_blank">07:36:04.960</a></span> | <span class="t">thank you for watching my video guys it has been super super super demanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27371" target="_blank">07:36:11.120</a></span> | <span class="t">i spent many months first of all to learn myself about the triton about cuda about flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27377" target="_blank">07:36:17.360</a></span> | <span class="t">etc also i have a full-time job so it is really hard to make videos like this like i need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27383" target="_blank">07:36:23.440</a></span> | <span class="t">dedicate you know the nights the mornings the weekends i spent three days just to record this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27387" target="_blank">07:36:27.920</a></span> | <span class="t">video because sometimes i don't like how i explain something sometimes i make mistake or sometimes i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27392" target="_blank">07:36:32.800</a></span> | <span class="t">need to restart because what i'm doing is wrong etc etc i believe there should be no big errors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27399" target="_blank">07:36:39.760</a></span> | <span class="t">in what i have done so far but for sure my notation is completely bad like because all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27405" target="_blank">07:36:45.280</a></span> | <span class="t">the mathematics i know has been self-taught by i i learned it by myself so because i didn't learn it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27411" target="_blank">07:36:51.760</a></span> | <span class="t">in academia i have bad habits and i'm trying to get rid of them so i use the very bad notation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27417" target="_blank">07:36:57.040</a></span> | <span class="t">sometimes i calls with the capital letter sometimes with this lowercase sometimes i just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27421" target="_blank">07:37:01.760</a></span> | <span class="t">forget the index etc so i'm trying to solve these problems um i believe i have explained everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27428" target="_blank">07:37:08.240</a></span> | <span class="t">so i should be you should have all the knowledge to derive all the formulas that you see in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27434" target="_blank">07:37:14.320</a></span> | <span class="t">paper of the flash attention and you should also have an internal image on how the back the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27441" target="_blank">07:37:21.200</a></span> | <span class="t">attention calculation is working block by blocks i know that i could have spent 20 hours explaining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27447" target="_blank">07:37:27.360</a></span> | <span class="t">things better but i also have a life and i also have a wife so i i i cannot make a 100 hours videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27455" target="_blank">07:37:35.920</a></span> | <span class="t">also there were some interruptions making these videos i i removed some wisdom teeth so it took me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27461" target="_blank">07:37:41.120</a></span> | <span class="t">at least one more than one week to to to recover because it was so painful so thank you guys for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27468" target="_blank">07:37:48.640</a></span> | <span class="t">watching my video i hope you learned a lot also this time i as you can see triton is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27473" target="_blank">07:37:53.600</a></span> | <span class="t">new there is not much documentation so something that i have said about triton may not be totally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27478" target="_blank">07:37:58.480</a></span> | <span class="t">correct because really there is very little documentation so all the triton that i have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27483" target="_blank">07:38:03.040</a></span> | <span class="t">learned is by looking at the code written by others and try to understand it um and i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&t=27492" target="_blank">07:38:12.400</a></span> | <span class="t">that's it guys so i wish you a wonderful day and see you next time on my channel</span></div></div></body></html>