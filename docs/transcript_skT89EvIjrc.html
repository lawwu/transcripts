<html><head><title>The State of Reasoning — from Nathan Lambert, Interconnects/AI2 [LS Live @ NeurIPS 2024]</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>The State of Reasoning — from Nathan Lambert, Interconnects/AI2 [LS Live @ NeurIPS 2024]</h2><a href="https://www.youtube.com/watch?v=skT89EvIjrc" target="_blank"><img src="https://i.ytimg.com/vi/skT89EvIjrc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>(upbeat music) - Hey everyone, happy new year. This is a quick talk that I gave at NeurIPS the Latent Space unofficial industry event. So Swake's tried to have people to talk about the major topics of the year, scaling, open models, synthetic data agents, et cetera. And he asked me to fill in a quick slot on reasoning.</p><p>A couple notes, this was before O3 was announced by OpenAI. So I think you can take everything that I said and run with it with even more enthusiasm and expect even more progress in 2025. And second, there was some recording issues. So I re-edited the slides to match up with the audio.</p><p>So you might see that they're slightly off, but it's mostly reading like a blog post and it should do a good job getting the conversation started around reasoning on interconnects in the new year. Happy new year, and I hope you like this. Thanks. I wouldn't say my main research area is reasoning.</p><p>I would say that I came from a reinforcement learning background into language models and reasoning is now getting subverted into that as a method rather than an area. And a lot of this is probably transitioning these talks into more provocative forms to prime everyone for the debate that is why most people are here.</p><p>And this is called the state of reasoning. This is by no means a comprehensive survey. To continue, I wanted to make sure that I was not off base to think about this because there's a lot of debates on reasoning and I wanted to revisit a very basic definition. And this is a dictionary definition, which is the action of thinking about something in a logical, sensible way, which is actually sufficiently vague that I would agree with it.</p><p>I think as we'll see in a lot of this talk is that I think people are going crazy about whether or not language models reason. We've seen this with AGI before and now reasoning kind of seems like the same thing, which to me is pretty ridiculous because it's like reasoning is a very general skill and I will provide more reasoning or support for the argument that these language models are doing some sort of reasoning when you give them problems.</p><p>I think I don't need to share a ton of examples for what's just like ill-formed arguments for what language models are not doing, but it's tough that this is the case and I think there are some very credible arguments that reasoning is a poor direction to pursue for language models because language models are not going to be as good at it as humans.</p><p>But to say that they can't do reasoning, I don't see a lot of proof for and I'll go through a few examples. And the question is like, why should language model reasoning be constrained to look like what humans do? I think language models are very different and they are stochastic, the stochastic parents thing is true for many reasons and we should embrace this and we should continue and I think a big trend of the year is that we're seeing new types of language model reasoning that look less human and that can be good for kind of separating the discourse for expecting a really narrow type of behaviors.</p><p>I did an interview with Ross Taylor who was a reasoning lead at Meta, which I thought was a very good education for me on this and this is just a direct pull from the transcript. But essentially it's saying is like, if you do chain of thought on a language model, what it is doing is it's essentially outputting its intermediate steps.</p><p>If I were to ask you all a math problem right now, you can do most of them in your head and you're doing some sort of intermediate storage of variables and language models have no ability to do this. They are kind of per token computation devices where each token is outputted after doing this forward pass and within that there's no explicit structure to hold these intermediate states.</p><p>So I think embracing chain of thought and these kind of intermediate values for the language models is extremely reasonable and it's showing that they're doing something that actually gets to valuable outputs. So this is like one of the many ways that we can kind of lead towards O1 is that language models have randomness built into them and a lot of what people see as failures in reasoning are kind of these language models following very static chains and making very specific mistakes along the way with really no ability to correct for that.</p><p>This is really not something that we see in human reasoning. So if a human makes a mistake, they will normally catch it on the next step, but we need to handle language models differently. And why O1 is exciting is because it's a new type of language models that are going to maximize on this view of reasoning, which is that chain of thought in kind of a forward stream of tokens can actually do a lot to achieve better outcomes when you're doing a reasoning-like action, which is just repeatedly outputting tokens to make progress on some sort of intelligence-defined task.</p><p>So it's just making forward progress by spending more compute and the token stream is the equivalent of some intermediate state. What is O1 has been a large debate since its release. I'm not gonna spend a lot of this talk on it, but the more I've spent on it is that you should take OpenAI at their face value, which they are doing very large-scale RL on the verifiable outcomes is what I've added, especially in context of the RL API that they've released, which I'll talk about more.</p><p>But most of the reasons to believe in more complicated things like process rewards models, self-play, Monte Carlo tree search, are mostly based on previous literature and things that we would have expected advanced reasoning to look like for language models and not based on evidence that they have given us or the behavior, whether you're looking at evaluations or how actually inference is done when serving the model.</p><p>This takes us to replications, or I would probably call them relatives of O1 coming from the community. These are wonderful to see. We are exploring the boundaries for what we can do with chain of thought in models. The two I've highlighted are from DeepSeq and Quen, and a lot of people in this room have probably seen them.</p><p>And I think that these models are really substantially narrower than these full O1 models from OpenAI. So OpenAI is, if you use O1, you can do it for a lot more tasks. If you use, like I was using the DeepSeq model and it's supposed to be for math or code, but they've tried to keep the model so narrow that even in that, if you ask a code question, sometimes it'll be like, I'm only supposed to work on math or code.</p><p>And a lot of the success of O1 in the future models of this is going to be able to, it being able to handle more tasks and more domains. So semi-analysis wrote a post that I haven't read in full, but even if you look at the paywalled headings, you can kind of make some intelligent claims about what O1 is or is not.</p><p>I think these are two of the things from the table of contents that you can see without paying. I'm due to pay at some point, but I have not. And incredible amounts of forward passes during training. I think you'll see this as I discuss RL, fine tuning more in a little bit, but when you're doing RL, there's two types of ways that you see data many times, and that'll relate in many, or result in many forward passes.</p><p>One is that when you're doing RL on a prompt, you can sample many completions to then grade them or use them in different ways to update your policy. So if I ask one math problem, I could look at eight completions and choose the best one or do some contrastive thing between the best and the worst one.</p><p>And that kind of gradation can help the RL policy actually learn. And the second time, because the loss function is more flexible than something like instruction tuning, you can go over the same prompts many more times than you would in instruction tuning or kind of pre-training. So this kind of means they're doing just a lot of this sampling from the model, which is very different than other types of training we've seen in the past at pre and post-training.</p><p>And then one of this one is great, thanks for Don for showing everyone this, is that post-training flops exceed pre-training. I think this pretty much clearly says that they're using a ton of compute for this large-scale RL. And at that point, it would probably mean something different, where this is like pre-training RL, and this is something that these early relative models are not going to be doing, because no one has this infrastructure like OpenAI does.</p><p>It'll take a while to do that, but people will make it. Okay, this takes us to reinforcement fine-tuning. I would say that this is a hard pivot in the talk where O1 is essentially pre-training scale RL, extremely big RL, and we don't know what all the details of the data are to OpenAI then showing us this new beta API program that they're making, which is just a sprinkle of this.</p><p>So what can you do with a tiny bit of their infrastructure? I think one of the fine-tuning leads responded to a tweet from SWIX, and they were like, the tweet literally, there was like one of the tweets, it was a long tweet that gave a lot of details, but even the first tweet I hadn't seen, I had like eight likes, and I was like, this API is using the same infrastructure that we use to train O1.</p><p>I was like, that alone is like a lot of detail. It was like on Twitter, it was a random thing. And then there's a really long details on other stuff of it. But it is just a new paradigm for fine-tuning, and I have seen some of this work, and I'm pretty optimistic that it'll work for kind of really specific capabilities where answers matter, rather than features in your style of text mattering.</p><p>So again, kind of like I was hinting at with O1, this reinforcement fine-tuning does many passes over the data, which is why they can say you only need dozens of labeled samples to actually learn from it, which is just very different than previous training regimes. So what happens is that the model gets a, the grader gives a bonus when the answer is right, and the model learns to reinforce behaviors that get right answers.</p><p>And I'll move, later in the talk, I'll highlight a research project that we did that was pretty much doing a very similar thing, to target very specific evaluations on open models, and you do RL, and you give a reward bonus when the answer is right, and that's all you do.</p><p>And the kind of key innovation in the simplicity is that modern language models are a strong enough base where just a really gentle RL fine-tuning can add these specific capabilities without degrading the model. I think a lot of fear for adding RL to these training regimes, especially on general instruct models, like in chat GPT, was just that they're gonna destroy the rest of the performance, the base of chattiness that you care about.</p><p>And it really seems like you can just do this out of the box if OpenAI is going to allow an API, they aren't gonna let people train a model that then just gets worse on random other things. So what the data format looks like, the example I gave is way more complicated than I think you should.</p><p>Seriously, you could start with a grade school math problem and just say the correct answer is the correct number, the genes are confusing, but essentially you have two components, a prompt and an answer, which is different than having a prompt in completion that you would train on, or if you're doing preference tuning, you would do a prompt in a chosen completion and a rejected completion.</p><p>So it's a new type of data format. I suspect quickly we'll see things like HuggingBase having more of these. I will highlight, we have some of ours for our specific project that we did. We have examples for math. This on the screen is an example for precise instruction following, which is the idea that if you have a prompt, you can say something like, have every sentence start with the letter A.</p><p>And you can verify that with Python really easily. This is something that we did in our project. And it's like, the model gets better at this. It's just like, you have constrained data and the RL algorithm learns to change the model just a tiny bit and actually reach these answers.</p><p>A confusing thing for people was these grader models. I think the place to come from these is evaluation. There's been a lot of work in evaluation to make answer extraction stable, especially with math, where an example that I used in the blog post I wrote today on this is like LLAMA 3.1 details their vowels.</p><p>For math, they use both SymPy, a Python process, or Python package for extraction, and LLM as a judge to extract their answers for math. And what the graders are doing is essentially amping this up to a whole nother level where it's kind of a nested structure of configs for doing reward shaping on these verifiable outputs.</p><p>For math, it can be really easy. It's like, you know you have to handle these five formats that I came up with in a minute for how you could represent different numbers and tokens. But as you get to more complicated things and more complicated behaviors, it seems like OpenAI is insinuating that you're gonna need more than just a yes/no loss function for your domains.</p><p>And that seems fine. Well, we already have a bunch of open models that are doing like judge models and Prometheus and other things that are designed specifically for LLM as a judge. And I see that continuing to just become part of this kind of open RL infrastructure. OpenAI had a bunch of screenshots.</p><p>I'm not gonna end on a commentary on these, but it looks pretty standard. They're gonna track how performance changes over time and stuff like this. You'll be able to look at all the outputs. This is just them making pretty things. And then they have this like very generic RL plot.</p><p>The most standard RL plot is a X-axis of time or trials and a Y-axis of reward. Here, reward is like an accuracy or a success rate on a certain validation set. And X is actually supposed to be like how much training was done. And this is very similar to what we did in our project.</p><p>I think this is kind of just another way you can put this with an RL feedback diagram. If you've seen RL where you have this agent interacting with the environment, this you will squint at it and it'll be familiar. If you haven't, you'll probably be in for more of these things if RL keeps becoming popular 'cause RL is really formulated as trial and error learning.</p><p>But if you're interested, we're happy to try to have people use our code, which does this for math and some instruction tuning already. And we want to try more complicated graders for things like code. So for code quality, a binary outcome doesn't really make sense, which is a good way to think about why you might need to do some reward shaping for how you would grade outputs from a various model.</p><p>And to kind of compare the plot that OpenAI had, which is like performance improving over time, these are some experiments we ran on various evaluations. So the left column is some language model evaluation that we would use in an academic paper. And the right is all the various internal RL statistics where like GSMA-K, math, and IFML are all being trained on training sets.</p><p>So we have the answer, we have the prompts, which are math questions. We have the answers, which are numbers. And we're really doing this RL on seeing if this answer is right. And then it generalizes to various math evaluations that we care about. So I kind of see this as like we got a tip from a industry lab member to do this.</p><p>A few months early, so we got a head start. And I think a lot of people are obviously going to be trying to replicate this now. So it's fun that we have a starting point. I'm excited to talk about it with people this week. And I think reasoning is worth continuing as something.</p><p>You can read the post that I was referencing here. And I'm happy to take any related or hard question on reasoning 'cause I kind of opened the floor for that. So thank you. Thank you. (upbeat music) (upbeat music) (upbeat music) (hip hop music) (upbeat music)</p></div></div></body></html>