<html><head><title>Stanford CS25: V3 I Low-level Embodied Intelligence w/ Foundation Models</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V3 I Low-level Embodied Intelligence w/ Foundation Models</h2><a href="https://www.youtube.com/watch?v=fz8wf9hN20c"><img src="https://i.ytimg.com/vi/fz8wf9hN20c/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./fz8wf9hN20c.html">Whisper Transcript</a> | <a href="./transcript_fz8wf9hN20c.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So, hey guys, thanks for coming to our second class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=10" target="_blank">00:00:10.000</a></span> | <span class="t">Today we have the pleasure of welcoming Faish Shia, he's a senior research scientist at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=15" target="_blank">00:00:15.080</a></span> | <span class="t">Google DeepMind, where he works on the robotics team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=18" target="_blank">00:00:18.800</a></span> | <span class="t">He received his PhD here, actually, working with Sylvio Salvariz in Stanford Vision and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=24" target="_blank">00:00:24.720</a></span> | <span class="t">Learning Lab, as well as Leonidas Pibus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=27" target="_blank">00:00:27.960</a></span> | <span class="t">And his mission is to build intelligent embodied agents that can interact with complex and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=33" target="_blank">00:00:33.380</a></span> | <span class="t">unstructured real-world environments with applications with home robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=40" target="_blank">00:00:40.120</a></span> | <span class="t">Recently he has been exploring the use of foundation models for robot decision-making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=44" target="_blank">00:00:44.880</a></span> | <span class="t">and action generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=47" target="_blank">00:00:47.040</a></span> | <span class="t">So now I'll hand it off to Faish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=49" target="_blank">00:00:49.080</a></span> | <span class="t">Hi everyone, I'm super happy to be here and happy to be back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=53" target="_blank">00:00:53.400</a></span> | <span class="t">I graduated from here two years ago, and now I'm a research scientist at Google DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=58" target="_blank">00:00:58.800</a></span> | <span class="t">I work on the robotics team, and today I will be talking about low-level embodied intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=63" target="_blank">00:01:03.880</a></span> | <span class="t">with foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=65" target="_blank">00:01:05.600</a></span> | <span class="t">So it's definitely an interesting topic, and I will introduce what is embodied intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=71" target="_blank">00:01:11.320</a></span> | <span class="t">and what is low-level embodied intelligence, and how we can accelerate the building of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=76" target="_blank">00:01:16.960</a></span> | <span class="t">them with foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=79" target="_blank">00:01:19.800</a></span> | <span class="t">All right, so why are we working on embodied intelligence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=84" target="_blank">00:01:24.340</a></span> | <span class="t">So embodied intelligence is an integral part of artificial intelligence, and it's an important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=91" target="_blank">00:01:31.760</a></span> | <span class="t">milestone to artificial general intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=95" target="_blank">00:01:35.760</a></span> | <span class="t">And it has a lot of use cases, like for example, we all hope we have a home robot that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=101" target="_blank">00:01:41.000</a></span> | <span class="t">be in our home 24/7 and clean the home for us, or clean up our messy room, or cook for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=108" target="_blank">00:01:48.720</a></span> | <span class="t">us, or taking care of our aging family members.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=112" target="_blank">00:01:52.400</a></span> | <span class="t">So we are not quite there yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=113" target="_blank">00:01:53.920</a></span> | <span class="t">In fact, we are quite far from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=116" target="_blank">00:01:56.360</a></span> | <span class="t">That is because our intelligence is currently mostly in the virtual world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=120" target="_blank">00:02:00.840</a></span> | <span class="t">So we have AI agents that can help us draft emails or write eloquent essays, but they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=127" target="_blank">00:02:07.480</a></span> | <span class="t">are not super good at interacting with the messy real world, unstructured, complex environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=133" target="_blank">00:02:13.720</a></span> | <span class="t">that humans reside in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=136" target="_blank">00:02:16.600</a></span> | <span class="t">So just to give you guys a couple of examples of how messy the real world can be, and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=142" target="_blank">00:02:22.280</a></span> | <span class="t">hostile it could be to robotics, I want to show you a curious mistake or curious error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=148" target="_blank">00:02:28.840</a></span> | <span class="t">from one of our robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=150" target="_blank">00:02:30.720</a></span> | <span class="t">So the task is to put the Coke can in the sink, and watch what the robot do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=156" target="_blank">00:02:36.380</a></span> | <span class="t">The robot grabs the Coke can and opens the tap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=159" target="_blank">00:02:39.400</a></span> | <span class="t">So this is kind of dangerous, but it's kind of interesting, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=165" target="_blank">00:02:45.820</a></span> | <span class="t">Because we never expect it would do something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=168" target="_blank">00:02:48.800</a></span> | <span class="t">It's just from random noise, it starts to open the tap, and the water starts to come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=173" target="_blank">00:02:53.520</a></span> | <span class="t">out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=174" target="_blank">00:02:54.520</a></span> | <span class="t">So for an agent to have this type of physical intelligence, it needs to understand the effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">of its actions, and what is so-called a world model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=184" target="_blank">00:03:04.640</a></span> | <span class="t">So people have been complaining that language models so far don't have a world model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=188" target="_blank">00:03:08.780</a></span> | <span class="t">So it doesn't understand geometry, it doesn't understand the spatial relationship of objects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=195" target="_blank">00:03:15.120</a></span> | <span class="t">or the effect of actions, basically how objects will move according to physical laws.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=201" target="_blank">00:03:21.800</a></span> | <span class="t">So we are not quite there yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=204" target="_blank">00:03:24.040</a></span> | <span class="t">In another case, so this is our robot that is ready to deliver a can, or actually throw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=210" target="_blank">00:03:30.120</a></span> | <span class="t">away a can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=211" target="_blank">00:03:31.480</a></span> | <span class="t">But as you can see, we have this pre-programmed behavior of tucking the arm behind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=217" target="_blank">00:03:37.300</a></span> | <span class="t">And in doing that, the can is upside down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=219" target="_blank">00:03:39.800</a></span> | <span class="t">So if there's any liquid in the can, it will spill and damage the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=224" target="_blank">00:03:44.800</a></span> | <span class="t">So it's another example of real world is really complex, and there are a lot of things to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=229" target="_blank">00:03:49.560</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=230" target="_blank">00:03:50.560</a></span> | <span class="t">And in order for our robots to have this sort of ambient intelligence, they really need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=236" target="_blank">00:03:56.620</a></span> | <span class="t">to understand a lot of very nuanced details of the environment, and understanding the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=242" target="_blank">00:04:02.120</a></span> | <span class="t">physics, physical laws, and understanding the effect of its actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=249" target="_blank">00:04:09.280</a></span> | <span class="t">How do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=250" target="_blank">00:04:10.280</a></span> | <span class="t">There are many ways to achieve embodied intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=252" target="_blank">00:04:12.560</a></span> | <span class="t">Actually, throughout my PhD study, I've been fascinated by this idea of creating interactive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=258" target="_blank">00:04:18.840</a></span> | <span class="t">environments, basically, let agent explore in this interactive environment, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=264" target="_blank">00:04:24.720</a></span> | <span class="t">create environments that are complex enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=268" target="_blank">00:04:28.520</a></span> | <span class="t">So that if the agent needs to survive in such environment, it must develop intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=273" target="_blank">00:04:33.920</a></span> | <span class="t">So it's an ecological view of perception and agency, and is popularized by American psychologist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=280" target="_blank">00:04:40.320</a></span> | <span class="t">James J. Gibson.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=282" target="_blank">00:04:42.320</a></span> | <span class="t">So he has a famous quote, "Ask not what is inside your head, but what your head is inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=288" target="_blank">00:04:48.200</a></span> | <span class="t">of."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=289" target="_blank">00:04:49.200</a></span> | <span class="t">So human learned this type of embodied intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=291" target="_blank">00:04:51.960</a></span> | <span class="t">Human is able to manipulate objects effortlessly, one, because of evolution, second, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=298" target="_blank">00:04:58.160</a></span> | <span class="t">of the childhood experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=299" target="_blank">00:04:59.400</a></span> | <span class="t">We have been playing with this toy, we have been interacting with this toy, and watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=303" target="_blank">00:05:03.160</a></span> | <span class="t">the physical effect so that we learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=306" target="_blank">00:05:06.080</a></span> | <span class="t">And similarly, we can give robots a safe playpen, so they can explore in those environment and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=313" target="_blank">00:05:13.240</a></span> | <span class="t">interact with environment and play, and watch the effect of actions, and effectively understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=319" target="_blank">00:05:19.080</a></span> | <span class="t">how to manipulate those objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=322" target="_blank">00:05:22.460</a></span> | <span class="t">So I have been developing these simulation environments, one of which is called Gibson</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=329" target="_blank">00:05:29.760</a></span> | <span class="t">environment, which is published as CVPR.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=332" target="_blank">00:05:32.520</a></span> | <span class="t">It's mainly aiming at simulating the visual world faithfully, and also simulate physical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=339" target="_blank">00:05:39.280</a></span> | <span class="t">world to some extent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=340" target="_blank">00:05:40.960</a></span> | <span class="t">So we built this environment, which is a scanned environment from a lot of houses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=346" target="_blank">00:05:46.680</a></span> | <span class="t">And then an agent, we can spawn an agent in that, in this case, a humanoid agent, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=351" target="_blank">00:05:51.840</a></span> | <span class="t">the agent can learn to walk or to run in this environment and simulate all this perception</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=358" target="_blank">00:05:58.120</a></span> | <span class="t">information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=359" target="_blank">00:05:59.120</a></span> | <span class="t">So we can create a perception action loop for this agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=363" target="_blank">00:06:03.560</a></span> | <span class="t">And similarly, we can put other types of agents in this environment, in this case, a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=369" target="_blank">00:06:09.200</a></span> | <span class="t">cart, and we can also put a quadruped or this ant into this environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=376" target="_blank">00:06:16.040</a></span> | <span class="t">So essentially, we create an environment where we can simulate perception for the agent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=383" target="_blank">00:06:23.000</a></span> | <span class="t">and then we can create a neural network to map the perception to action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=386" target="_blank">00:06:26.840</a></span> | <span class="t">And this way, we achieve some sort of physical intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=390" target="_blank">00:06:30.780</a></span> | <span class="t">It's mostly for navigation and locomotion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=395" target="_blank">00:06:35.960</a></span> | <span class="t">This is not enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=397" target="_blank">00:06:37.560</a></span> | <span class="t">So in this case, the environment is one monolithic piece of mesh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=402" target="_blank">00:06:42.500</a></span> | <span class="t">As you can see, the agent run into the wall and it bounced back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=406" target="_blank">00:06:46.900</a></span> | <span class="t">So there is no articulation in this environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=410" target="_blank">00:06:50.080</a></span> | <span class="t">So it's not simulating the full complexity of the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=413" target="_blank">00:06:53.800</a></span> | <span class="t">So the things that we can do with our agent is rather limited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=419" target="_blank">00:06:59.080</a></span> | <span class="t">So that's why we create other simulation environment, one of which is iGibson environment, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=424" target="_blank">00:07:04.280</a></span> | <span class="t">is called Interactive Gibson.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=426" target="_blank">00:07:06.140</a></span> | <span class="t">So what we do is we create, again, scan a lot of real world houses, and then we convert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=433" target="_blank">00:07:13.520</a></span> | <span class="t">them to CAD assets, basically mesh assets that are interactable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=438" target="_blank">00:07:18.600</a></span> | <span class="t">In this case, we have a simulated agent that go into the environment and then close all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=443" target="_blank">00:07:23.680</a></span> | <span class="t">the drawers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=445" target="_blank">00:07:25.260</a></span> | <span class="t">So we are able to do that because we model the complexity of the world a little bit more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=450" target="_blank">00:07:30.440</a></span> | <span class="t">We go beyond just modeling the visual world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">We start to model physics a little bit more, basically modeling the degree of freedom in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=458" target="_blank">00:07:38.640</a></span> | <span class="t">the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=459" target="_blank">00:07:39.640</a></span> | <span class="t">And our agent can do more than just navigating around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=464" target="_blank">00:07:44.040</a></span> | <span class="t">So we can go even further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=467" target="_blank">00:07:47.040</a></span> | <span class="t">So we can even model more degree of freedom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=470" target="_blank">00:07:50.200</a></span> | <span class="t">And our agent can develop more complicated behavior, such as unloading a dishwasher and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=475" target="_blank">00:07:55.040</a></span> | <span class="t">find a bowl or take out the bowl and put it on the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=479" target="_blank">00:07:59.700</a></span> | <span class="t">So as we scale up the complexity of the environment, we are able to learn much more complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=486" target="_blank">00:08:06.120</a></span> | <span class="t">skills in simulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=488" target="_blank">00:08:08.920</a></span> | <span class="t">And that's one way to achieve embodied intelligence, which is to build complex enough simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=494" target="_blank">00:08:14.840</a></span> | <span class="t">environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=499" target="_blank">00:08:19.080</a></span> | <span class="t">Not just my research, but the entire field of computer vision is undergoing a paradigm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=503" target="_blank">00:08:23.720</a></span> | <span class="t">shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=504" target="_blank">00:08:24.740</a></span> | <span class="t">So previously, we are focusing on internet AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=507" target="_blank">00:08:27.780</a></span> | <span class="t">We curate a lot of internet data sets to study problems like classification, segmentation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=514" target="_blank">00:08:34.180</a></span> | <span class="t">and detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=515" target="_blank">00:08:35.180</a></span> | <span class="t">Basically all these computer vision problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=517" target="_blank">00:08:37.460</a></span> | <span class="t">Now we focus a lot more on embodied AI, which is adding the action dimension to the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=524" target="_blank">00:08:44.700</a></span> | <span class="t">that we are studying problems like visual navigation, manipulation, rearrangement, embodied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=529" target="_blank">00:08:49.340</a></span> | <span class="t">question answering, instruction following, and the simulators in some sense replace the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=537" target="_blank">00:08:57.260</a></span> | <span class="t">original role of data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=539" target="_blank">00:08:59.900</a></span> | <span class="t">One thing that doesn't change, which is the data is still super important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=544" target="_blank">00:09:04.420</a></span> | <span class="t">We are still relying on a large amount of data to learn this intelligent behavior, no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=551" target="_blank">00:09:11.220</a></span> | <span class="t">matter if it's from a static data set or from a simulator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=556" target="_blank">00:09:16.780</a></span> | <span class="t">So learning in simulation can take a lot of interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=562" target="_blank">00:09:22.180</a></span> | <span class="t">So just to give you an example, we create this iGibson environment and we want to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=567" target="_blank">00:09:27.980</a></span> | <span class="t">a behavior called go into a room through a closed door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=571" target="_blank">00:09:31.820</a></span> | <span class="t">So this is a rather simple behavior, which I can show on the top right of the screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=577" target="_blank">00:09:37.320</a></span> | <span class="t">So the agent needs to stop in front of the door, it needs to stop at the right distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=582" target="_blank">00:09:42.300</a></span> | <span class="t">If it's stopped too close to the door, it cannot extend its arm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=585" target="_blank">00:09:45.540</a></span> | <span class="t">If it's too far, it cannot open the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=587" target="_blank">00:09:47.940</a></span> | <span class="t">And then it basically opens the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=589" target="_blank">00:09:49.940</a></span> | <span class="t">Let me play this again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=590" target="_blank">00:09:50.940</a></span> | <span class="t">Open this door, when there is enough clearance, it will go into the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=595" target="_blank">00:09:55.100</a></span> | <span class="t">However, it takes about 50,000 episodes or 1.25 million environment interactions to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=602" target="_blank">00:10:02.100</a></span> | <span class="t">this type of behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=604" target="_blank">00:10:04.100</a></span> | <span class="t">This is because we are using model-free reinforcement learning, the agent is exploring this environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=610" target="_blank">00:10:10.100</a></span> | <span class="t">It could really push any point, it could rather stop at any point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=614" target="_blank">00:10:14.100</a></span> | <span class="t">So we give it a reward function to go into the room, but it's very rare that it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=620" target="_blank">00:10:20.220</a></span> | <span class="t">stumble upon this behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=623" target="_blank">00:10:23.340</a></span> | <span class="t">I would like to argue with foundation models, we can do a lot more different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=628" target="_blank">00:10:28.100</a></span> | <span class="t">So what do you do nowadays?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=629" target="_blank">00:10:29.700</a></span> | <span class="t">You just ask a HHBT, how do you go into a room through a closed door?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=633" target="_blank">00:10:33.980</a></span> | <span class="t">And it will say, open the door, walk through the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=636" target="_blank">00:10:36.120</a></span> | <span class="t">So this is a gross simplification of the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=639" target="_blank">00:10:39.420</a></span> | <span class="t">Of course, the problem is not that simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=643" target="_blank">00:10:43.660</a></span> | <span class="t">But what I'm just saying is that we can leverage a lot of semantic prior from the foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=650" target="_blank">00:10:50.460</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=651" target="_blank">00:10:51.460</a></span> | <span class="t">So if we really like data, if we really need a lot of data, the foundation model is a compressed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=656" target="_blank">00:10:56.900</a></span> | <span class="t">version of the entire data and it's a knowledge base that you can query and to accelerate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=661" target="_blank">00:11:01.900</a></span> | <span class="t">the development of robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=663" target="_blank">00:11:03.580</a></span> | <span class="t">Of course, simulation and real world data is still super, super important, but maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=668" target="_blank">00:11:08.460</a></span> | <span class="t">we can get the best of both worlds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=670" target="_blank">00:11:10.580</a></span> | <span class="t">We can use foundation models plus a limited amount of simulation or real world data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=676" target="_blank">00:11:16.980</a></span> | <span class="t">So that's what I'm going to talk about today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=680" target="_blank">00:11:20.460</a></span> | <span class="t">So where are we in terms of foundation models plus robotics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=684" target="_blank">00:11:24.300</a></span> | <span class="t">So our team at Google DeepMind has been piloting in foundation model plus robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=689" target="_blank">00:11:29.940</a></span> | <span class="t">So we developed advanced planning, high level planning algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=694" target="_blank">00:11:34.700</a></span> | <span class="t">One of the first is called Palm Seiken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=697" target="_blank">00:11:37.480</a></span> | <span class="t">It is an algorithm that can parse a user command.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=701" target="_blank">00:11:41.540</a></span> | <span class="t">So here is a demo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=702" target="_blank">00:11:42.540</a></span> | <span class="t">Here is a scenario.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=703" target="_blank">00:11:43.540</a></span> | <span class="t">Here is a user command.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=704" target="_blank">00:11:44.540</a></span> | <span class="t">I spill my Coke on the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=705" target="_blank">00:11:45.540</a></span> | <span class="t">How would you throw it away and bring me something to help clean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=708" target="_blank">00:11:48.740</a></span> | <span class="t">And it's querying a large language model, which is given a score highlighted in blue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=714" target="_blank">00:11:54.620</a></span> | <span class="t">And there is also an affordance score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=716" target="_blank">00:11:56.180</a></span> | <span class="t">The affordance will tell you whether an action at a given state is possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=720" target="_blank">00:12:00.580</a></span> | <span class="t">It's augmenting the language model to give you only possible things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=724" target="_blank">00:12:04.880</a></span> | <span class="t">So essentially, it is doing the semantic planning with a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=729" target="_blank">00:12:09.860</a></span> | <span class="t">But it's also taking into consideration what it can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=733" target="_blank">00:12:13.080</a></span> | <span class="t">So it's not just outputting the-- like, language model tends to hallucination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=741" target="_blank">00:12:21.320</a></span> | <span class="t">It doesn't hallucinate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=742" target="_blank">00:12:22.320</a></span> | <span class="t">It only gives you what is possible for the robot to do and what is actionable for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=745" target="_blank">00:12:25.760</a></span> | <span class="t">robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=746" target="_blank">00:12:26.760</a></span> | <span class="t">And the robot is doing the thing that is advancing the long horizon task progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=751" target="_blank">00:12:31.860</a></span> | <span class="t">And also, each task is executed by a low-level policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=756" target="_blank">00:12:36.560</a></span> | <span class="t">Here it doesn't quite clean the table, because we haven't added this to the low-level skill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=762" target="_blank">00:12:42.880</a></span> | <span class="t">But imagine there is a low-level skill to clean the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=764" target="_blank">00:12:44.960</a></span> | <span class="t">It will finish the entire thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=768" target="_blank">00:12:48.640</a></span> | <span class="t">What is a low-level policy used here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=770" target="_blank">00:12:50.960</a></span> | <span class="t">The low-level policy used here is Robotic Transformer 1, RT1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=776" target="_blank">00:12:56.100</a></span> | <span class="t">It's our team's homegrown transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=779" target="_blank">00:12:59.200</a></span> | <span class="t">Essentially, we collect a large data set of human demonstrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=784" target="_blank">00:13:04.280</a></span> | <span class="t">We put a transformer, and we train it on this large data set of expert trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=792" target="_blank">00:13:12.440</a></span> | <span class="t">It is able to do about 700 tasks with 97% success rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=798" target="_blank">00:13:18.440</a></span> | <span class="t">And it has interesting generalization behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=801" target="_blank">00:13:21.280</a></span> | <span class="t">It can operate in a new kitchen it has never seen before, which is showing there is a successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=807" target="_blank">00:13:27.780</a></span> | <span class="t">recipe to apply foundation models in robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=811" target="_blank">00:13:31.760</a></span> | <span class="t">So that's roughly where are we in terms of foundation model plus robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=816" target="_blank">00:13:36.400</a></span> | <span class="t">And I will talk about a few new works that is bringing this to the next level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=824" target="_blank">00:13:44.600</a></span> | <span class="t">So actually, my teammate, Ted, gave a talk of foundation models plus robotics at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=831" target="_blank">00:13:51.000</a></span> | <span class="t">beginning of this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=833" target="_blank">00:13:53.440</a></span> | <span class="t">It's also this class, CS25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=835" target="_blank">00:13:55.760</a></span> | <span class="t">I highly recommend it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=837" target="_blank">00:13:57.680</a></span> | <span class="t">It's available on YouTube.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=838" target="_blank">00:13:58.680</a></span> | <span class="t">I actually watched it last night so that I don't repeat some of the contents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=844" target="_blank">00:14:04.280</a></span> | <span class="t">But what he basically mentioned is that he revealed our team's progress in terms of building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=852" target="_blank">00:14:12.040</a></span> | <span class="t">this robotic foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=854" target="_blank">00:14:14.840</a></span> | <span class="t">And we have had a lot of somewhat detour, and now we sort of figured out a recipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=861" target="_blank">00:14:21.200</a></span> | <span class="t">So in 2021 to 2022 is how we scale to many tasks with demonstrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=867" target="_blank">00:14:27.040</a></span> | <span class="t">How do we collect a large amount of data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=869" target="_blank">00:14:29.080</a></span> | <span class="t">In fact, about 100,000 demonstrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=873" target="_blank">00:14:33.680</a></span> | <span class="t">And we tried different ways to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=876" target="_blank">00:14:36.440</a></span> | <span class="t">We tried behavior cloning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=878" target="_blank">00:14:38.160</a></span> | <span class="t">We tried imitation learning plus reinforcement learning, and some other ways, or combining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=883" target="_blank">00:14:43.060</a></span> | <span class="t">them with language models such as SACAN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=886" target="_blank">00:14:46.520</a></span> | <span class="t">In 2022 to 2023, it's about how we can leverage foundation models to accelerate robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=891" target="_blank">00:14:51.880</a></span> | <span class="t">We really see a proliferation of using foundation models to accelerate robotics, both on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=898" target="_blank">00:14:58.680</a></span> | <span class="t">high-level planning and low-level control, probably leaning more towards a high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=902" target="_blank">00:15:02.780</a></span> | <span class="t">planning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=904" target="_blank">00:15:04.520</a></span> | <span class="t">So if the recipe works-- so the recipe is essentially combine a large-scale diverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=911" target="_blank">00:15:11.760</a></span> | <span class="t">offline data set with high-capacity architecture, such as a transformer, and using language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=918" target="_blank">00:15:18.480</a></span> | <span class="t">as a universal glue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=920" target="_blank">00:15:20.000</a></span> | <span class="t">So this will be the recipe to build foundation models for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=925" target="_blank">00:15:25.440</a></span> | <span class="t">So if this recipe works, what do we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=928" target="_blank">00:15:28.000</a></span> | <span class="t">What do we do next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=929" target="_blank">00:15:29.000</a></span> | <span class="t">Essentially, we're just-- let's just scale everything to orders of magnitude and be done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=934" target="_blank">00:15:34.600</a></span> | <span class="t">with it and solve robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=938" target="_blank">00:15:38.080</a></span> | <span class="t">And guess what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=939" target="_blank">00:15:39.080</a></span> | <span class="t">That's what we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=940" target="_blank">00:15:40.080</a></span> | <span class="t">So that's the end of the lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=941" target="_blank">00:15:41.080</a></span> | <span class="t">I'm going to cut this a little bit short.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=944" target="_blank">00:15:44.120</a></span> | <span class="t">And that's a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=945" target="_blank">00:15:45.800</a></span> | <span class="t">That's not happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=947" target="_blank">00:15:47.600</a></span> | <span class="t">So we are still on our way, on our quest to solve low-level embodied intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=953" target="_blank">00:15:53.680</a></span> | <span class="t">When I talk to people that you can use foundation models to do robotics, their reaction would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=961" target="_blank">00:16:01.160</a></span> | <span class="t">be it's mostly doing high-level reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=964" target="_blank">00:16:04.760</a></span> | <span class="t">It doesn't do the low-level manipulation really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=968" target="_blank">00:16:08.840</a></span> | <span class="t">And that's for a reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=970" target="_blank">00:16:10.480</a></span> | <span class="t">One of the reasons is there is a Moravec's paradox.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=974" target="_blank">00:16:14.000</a></span> | <span class="t">Moravec's paradox is the observation that in artificial intelligence and robotics, contrary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=978" target="_blank">00:16:18.600</a></span> | <span class="t">to traditional assumptions or our intuitions, reasoning requires very little computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=984" target="_blank">00:16:24.000</a></span> | <span class="t">But sensory motor control and perception skills require enormous compute resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=989" target="_blank">00:16:29.720</a></span> | <span class="t">That is because as biological creatures, we acquire the sensory motor skills through evolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=998" target="_blank">00:16:38.400</a></span> | <span class="t">This is very different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=999" target="_blank">00:16:39.680</a></span> | <span class="t">So we might not be able to reason or do large-scale computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1006" target="_blank">00:16:46.720</a></span> | <span class="t">But this sensory motor control is integral to our survival.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1011" target="_blank">00:16:51.580</a></span> | <span class="t">So it's essentially already learning our DNA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1015" target="_blank">00:16:55.080</a></span> | <span class="t">But in robotics, it's a little bit different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1017" target="_blank">00:16:57.480</a></span> | <span class="t">So the chips are very good at doing reasoning and computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1022" target="_blank">00:17:02.360</a></span> | <span class="t">But they are not super good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1023" target="_blank">00:17:03.360</a></span> | <span class="t">They haven't experienced the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1024" target="_blank">00:17:04.960</a></span> | <span class="t">They haven't acquired the sensory motor skills that is necessary for them to do tasks in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1031" target="_blank">00:17:11.040</a></span> | <span class="t">the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1032" target="_blank">00:17:12.040</a></span> | <span class="t">Here is an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1033" target="_blank">00:17:13.680</a></span> | <span class="t">When the computer beat Kasparov, basically the human champion in chess, there is another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1042" target="_blank">00:17:22.280</a></span> | <span class="t">robot arm moving the chess piece.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1043" target="_blank">00:17:23.880</a></span> | <span class="t">It can beat the human champion in chess, but there is still someone need to move the chess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1049" target="_blank">00:17:29.360</a></span> | <span class="t">piece.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1050" target="_blank">00:17:30.360</a></span> | <span class="t">Similarly, in the AlphaGo moment, when Lee Sedol was beaten by AlphaGo, there is still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1055" target="_blank">00:17:35.000</a></span> | <span class="t">someone who is moving the chess piece for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1057" target="_blank">00:17:37.840</a></span> | <span class="t">It's not a robot doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1059" target="_blank">00:17:39.260</a></span> | <span class="t">So this is showing the reasoning is that the hard things are easy, and the easy things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1063" target="_blank">00:17:43.600</a></span> | <span class="t">are hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1065" target="_blank">00:17:45.640</a></span> | <span class="t">There's another thing that prevents us from using foundation models more prevalently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1071" target="_blank">00:17:51.600</a></span> | <span class="t">more in a larger scale in robotics, which is the training data bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1077" target="_blank">00:17:57.160</a></span> | <span class="t">The training data of foundation models or large language models are mostly language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1081" target="_blank">00:18:01.440</a></span> | <span class="t">tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1082" target="_blank">00:18:02.440</a></span> | <span class="t">So it's perhaps not that surprising it knows how to clean up a kitchen because maybe there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1087" target="_blank">00:18:07.960</a></span> | <span class="t">are wikiHow articles teaching you how to clean up a kitchen or to do something in a procedural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1093" target="_blank">00:18:13.040</a></span> | <span class="t">way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1094" target="_blank">00:18:14.040</a></span> | <span class="t">But there is no wikiHow articles teaching you how to move your finger five centimeters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1098" target="_blank">00:18:18.260</a></span> | <span class="t">to the left because people just don't say that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1101" target="_blank">00:18:21.880</a></span> | <span class="t">People don't write that down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1103" target="_blank">00:18:23.040</a></span> | <span class="t">So there is a very limited amount of this low-level control data in large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1108" target="_blank">00:18:28.720</a></span> | <span class="t">training culture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1109" target="_blank">00:18:29.720</a></span> | <span class="t">So we do have a lot of challenges in bringing the foundation models to a lower level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1114" target="_blank">00:18:34.000</a></span> | <span class="t">So that's what I mean by low-level embodied intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1117" target="_blank">00:18:37.600</a></span> | <span class="t">So any questions so far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1119" target="_blank">00:18:39.680</a></span> | <span class="t">Also, I want to make this quite interactive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1122" target="_blank">00:18:42.040</a></span> | <span class="t">So if there is any questions, feel free to interrupt me any time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1127" target="_blank">00:18:47.040</a></span> | <span class="t">All right, if not, we can continue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1133" target="_blank">00:18:53.120</a></span> | <span class="t">So there are a couple of challenges of using large language models for low-level control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1137" target="_blank">00:18:57.240</a></span> | <span class="t">As I just mentioned, the first thing is lack of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1141" target="_blank">00:19:01.680</a></span> | <span class="t">So we only have perhaps 100,000 episodes of human demonstration data and takes about 13</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1150" target="_blank">00:19:10.960</a></span> | <span class="t">robots 17 months to collect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1153" target="_blank">00:19:13.200</a></span> | <span class="t">So it's a huge amount of effort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1155" target="_blank">00:19:15.920</a></span> | <span class="t">In the country, large language models are trained on the order of 1,000 billion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1161" target="_blank">00:19:21.680</a></span> | <span class="t">A smaller palm was trained on 780 billion tokens, and the larger one is trained-- following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1171" target="_blank">00:19:31.640</a></span> | <span class="t">the chinchilla rule, you would need to train it on 1.35 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1176" target="_blank">00:19:36.600</a></span> | <span class="t">So it's a huge amount of discrepancy between how much data we can achieve in robotics and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1183" target="_blank">00:19:43.320</a></span> | <span class="t">how much we can get in large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1188" target="_blank">00:19:48.120</a></span> | <span class="t">So we will always be bounded by robotic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1190" target="_blank">00:19:50.360</a></span> | <span class="t">So maybe we can scale on other fronts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1193" target="_blank">00:19:53.560</a></span> | <span class="t">Maybe we can keep the robotics data the same, and then we can scale on other fronts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1198" target="_blank">00:19:58.200</a></span> | <span class="t">Like, maybe we can scale the pre-training mix of text and image, or maybe image and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1202" target="_blank">00:20:02.320</a></span> | <span class="t">text pairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1203" target="_blank">00:20:03.520</a></span> | <span class="t">Maybe we can build this cake, and the robotics data is just a cherry on top of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1210" target="_blank">00:20:10.020</a></span> | <span class="t">And we can scale the foundation really, really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1214" target="_blank">00:20:14.260</a></span> | <span class="t">Some of my work that I'm going to talk about today actually reused the RT1 data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1218" target="_blank">00:20:18.800</a></span> | <span class="t">We don't collect the new data for RT2, but we want to do more things with the same amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1223" target="_blank">00:20:23.560</a></span> | <span class="t">of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1226" target="_blank">00:20:26.000</a></span> | <span class="t">The second challenge is kind of related to the first challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1230" target="_blank">00:20:30.400</a></span> | <span class="t">Language models lacks an interface for low-level control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1234" target="_blank">00:20:34.680</a></span> | <span class="t">If you ask a language model, how do you make a robot dog stand up on two feet, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1239" target="_blank">00:20:39.360</a></span> | <span class="t">tell you a lot of things that sound reasonable, sounds plausible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1243" target="_blank">00:20:43.120</a></span> | <span class="t">It will tell you the robot dog's torso is upright, balance over two hind feet, and standing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1248" target="_blank">00:20:48.280</a></span> | <span class="t">shoulder-width apart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1249" target="_blank">00:20:49.760</a></span> | <span class="t">This is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1250" target="_blank">00:20:50.760</a></span> | <span class="t">This is all great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1251" target="_blank">00:20:51.760</a></span> | <span class="t">But we cannot put it on the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1255" target="_blank">00:20:55.920</a></span> | <span class="t">On the other hand, maybe we can ask a language model to write control code to directly control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1260" target="_blank">00:21:00.120</a></span> | <span class="t">the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1261" target="_blank">00:21:01.120</a></span> | <span class="t">But usually, that requires you to curate an API that is friendly to the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1266" target="_blank">00:21:06.840</a></span> | <span class="t">It will directly ask it to give you my joint angles to make the robot stand upright.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1272" target="_blank">00:21:12.520</a></span> | <span class="t">It will not give you the right thing because it doesn't have enough context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1275" target="_blank">00:21:15.600</a></span> | <span class="t">So essentially, large language models don't speak robot language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1280" target="_blank">00:21:20.700</a></span> | <span class="t">Can we actually find the right robot language?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1284" target="_blank">00:21:24.120</a></span> | <span class="t">Can we find the interface between large language models and robot control?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1288" target="_blank">00:21:28.240</a></span> | <span class="t">Or can we just treat robot action as another language?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1291" target="_blank">00:21:31.960</a></span> | <span class="t">So that's what we want to find out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1296" target="_blank">00:21:36.200</a></span> | <span class="t">In today's agenda, I will be talking about low-level embodied intelligence with foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1300" target="_blank">00:21:40.280</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1301" target="_blank">00:21:41.280</a></span> | <span class="t">It's separated into two parts, and it's addressing the two challenges that I've just mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1307" target="_blank">00:21:47.480</a></span> | <span class="t">Part one is about model consolidation, joint scaling, and positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1311" target="_blank">00:21:51.760</a></span> | <span class="t">So I have to put them in one part because they are somewhat related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1316" target="_blank">00:21:56.400</a></span> | <span class="t">And part two is developing new interface of large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1320" target="_blank">00:22:00.960</a></span> | <span class="t">So what do I mean by model consolidation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1323" target="_blank">00:22:03.840</a></span> | <span class="t">Yes, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1324" target="_blank">00:22:04.840</a></span> | <span class="t">Yeah, I was going to ask, why couldn't you just fine-tune an RNN for generating low-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1332" target="_blank">00:22:12.480</a></span> | <span class="t">code?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1333" target="_blank">00:22:13.480</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1334" target="_blank">00:22:14.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1335" target="_blank">00:22:15.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1336" target="_blank">00:22:16.480</a></span> | <span class="t">Yeah, that's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1339" target="_blank">00:22:19.880</a></span> | <span class="t">So the question is, why cannot we fine-tune language model to directly output low-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1345" target="_blank">00:22:25.600</a></span> | <span class="t">code or robot actions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1349" target="_blank">00:22:29.880</a></span> | <span class="t">So I will be talking about RT2, which does somewhat similar to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1353" target="_blank">00:22:33.800</a></span> | <span class="t">It's fine-tune language model to output action as a language, to output our action representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1361" target="_blank">00:22:41.160</a></span> | <span class="t">There are certain downsides to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1362" target="_blank">00:22:42.600</a></span> | <span class="t">Like, for example, you would need to collect additional data to fine-tune a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1368" target="_blank">00:22:48.720</a></span> | <span class="t">So either we can fine-tune that, or we can use the language model zero-shot if you find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1373" target="_blank">00:22:53.160</a></span> | <span class="t">the right interface, which I will talk about a little bit in the part two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1376" target="_blank">00:22:56.360</a></span> | <span class="t">Zero-shot and without fine-tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1378" target="_blank">00:22:58.560</a></span> | <span class="t">Without fine-tuning, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1381" target="_blank">00:23:01.220</a></span> | <span class="t">So the model consolidation is, essentially, we can do the high-level reasoning and low-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1385" target="_blank">00:23:05.680</a></span> | <span class="t">control in one model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1387" target="_blank">00:23:07.240</a></span> | <span class="t">And joint scaling is, not only we scale the robot data, which is expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1392" target="_blank">00:23:12.000</a></span> | <span class="t">We also scale the pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1395" target="_blank">00:23:15.880</a></span> | <span class="t">Or we already start from a pre-trained vision language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1399" target="_blank">00:23:19.720</a></span> | <span class="t">And a positive transfer is model benefiting from diverse joint training across internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1404" target="_blank">00:23:24.560</a></span> | <span class="t">scale language, vision, and vision language domains combined with robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1411" target="_blank">00:23:31.720</a></span> | <span class="t">So this is a continuation of the axes that Tad drew in his previous talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1420" target="_blank">00:23:40.080</a></span> | <span class="t">So we can see there is a trend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1422" target="_blank">00:23:42.560</a></span> | <span class="t">So this visualization basically highlights some of the work on our team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1427" target="_blank">00:23:47.820</a></span> | <span class="t">And each work, each column, is basically a robotic system that is able to do both high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1435" target="_blank">00:23:55.160</a></span> | <span class="t">reasoning and low-level control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1437" target="_blank">00:23:57.260</a></span> | <span class="t">So previously, we need to have separate models for each thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1443" target="_blank">00:24:03.080</a></span> | <span class="t">Previously, in the initial release of SACAN, the planning is done by a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1449" target="_blank">00:24:09.040</a></span> | <span class="t">And the affordance is done by a QT opt-like policy trained with Sim2Real.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1460" target="_blank">00:24:20.040</a></span> | <span class="t">And the low-level policy is Robotic Transformer 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1463" target="_blank">00:24:23.720</a></span> | <span class="t">So it's each model doing its dedicated thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1468" target="_blank">00:24:28.160</a></span> | <span class="t">And we need to train each model differently, and perhaps with different type of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1474" target="_blank">00:24:34.480</a></span> | <span class="t">And later, we have QTransformer, which unifies, which is kind of an offline RL method that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1482" target="_blank">00:24:42.360</a></span> | <span class="t">is leveraging transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1484" target="_blank">00:24:44.560</a></span> | <span class="t">So it's a high-capacity architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1486" target="_blank">00:24:46.820</a></span> | <span class="t">It can train on both positive data and negative data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1489" target="_blank">00:24:49.520</a></span> | <span class="t">And with that, we are able to gather a policy that is also understanding affordances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1496" target="_blank">00:24:56.180</a></span> | <span class="t">So we can unify the low-level policy and affordances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1498" target="_blank">00:24:58.880</a></span> | <span class="t">But the planning is still a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1501" target="_blank">00:25:01.280</a></span> | <span class="t">And then we have PAL-ME, which is a vision language model, which is a large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1506" target="_blank">00:25:06.580</a></span> | <span class="t">model also trained on a vision language domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1509" target="_blank">00:25:09.980</a></span> | <span class="t">So the PAL-ME can do planning and affordance in just one model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1513" target="_blank">00:25:13.880</a></span> | <span class="t">But the low-level is still using RT1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1516" target="_blank">00:25:16.280</a></span> | <span class="t">And finally, we unify everything together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1518" target="_blank">00:25:18.620</a></span> | <span class="t">Like there is RT2, which I'm going to talk about today, that can do both high-level planning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1523" target="_blank">00:25:23.760</a></span> | <span class="t">to some extent, generating affordance, and do low-level policies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1528" target="_blank">00:25:28.320</a></span> | <span class="t">So behind the model consolidation is the consolidation of tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1533" target="_blank">00:25:33.640</a></span> | <span class="t">We can represent every task as a vision plus text to text task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1539" target="_blank">00:25:39.160</a></span> | <span class="t">So it's a really universal representation of the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1542" target="_blank">00:25:42.840</a></span> | <span class="t">And then with that, you can train it really on using a lot of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1548" target="_blank">00:25:48.040</a></span> | <span class="t">And you can see positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1549" target="_blank">00:25:49.960</a></span> | <span class="t">Basically, learning affordance can also tell you how to achieve a task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1556" target="_blank">00:25:56.800</a></span> | <span class="t">There are transfer between tasks when you pull all the tasks together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1563" target="_blank">00:26:03.800</a></span> | <span class="t">So to understand this joint scaling and to understand the model consolidation, we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1569" target="_blank">00:26:09.200</a></span> | <span class="t">to understand PAL-ME a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1572" target="_blank">00:26:12.280</a></span> | <span class="t">So PAL-ME is an embodied multimodal language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1575" target="_blank">00:26:15.400</a></span> | <span class="t">It's based on the PALM architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1577" target="_blank">00:26:17.520</a></span> | <span class="t">So PALM is a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1579" target="_blank">00:26:19.440</a></span> | <span class="t">We made some adaptation on the architecture so it can understand multimodal input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1585" target="_blank">00:26:25.900</a></span> | <span class="t">So it is basically one model that is able to take in multimodal input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1594" target="_blank">00:26:34.760</a></span> | <span class="t">So in large language models, each word is tokenized and tokenized and getting this embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1603" target="_blank">00:26:43.280</a></span> | <span class="t">of these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1605" target="_blank">00:26:45.800</a></span> | <span class="t">And then that is fed into a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1609" target="_blank">00:26:49.240</a></span> | <span class="t">So in PAL-ME, what we do is instead of using words, we can use multimodal tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1616" target="_blank">00:26:56.120</a></span> | <span class="t">So the multimodal tokens can come from a vision transformer, a VIT, or it can come from robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1624" target="_blank">00:27:04.560</a></span> | <span class="t">sensory data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1626" target="_blank">00:27:06.100</a></span> | <span class="t">So every multimodal token, then we map it to the text embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1634" target="_blank">00:27:14.400</a></span> | <span class="t">We basically train a linear affine transform between the multimodal token and the text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1643" target="_blank">00:27:23.120</a></span> | <span class="t">embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1644" target="_blank">00:27:24.380</a></span> | <span class="t">And then we can treat the multimodal token as words as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1650" target="_blank">00:27:30.200</a></span> | <span class="t">So essentially, we have a language model as a solid base, and then we start to adapt it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1657" target="_blank">00:27:37.600</a></span> | <span class="t">to understand multimodal tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1659" target="_blank">00:27:39.760</a></span> | <span class="t">So this is quite interesting because it doesn't require a ton of adaptation or fine tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1666" target="_blank">00:27:46.480</a></span> | <span class="t">for it to understand multimodal input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1670" target="_blank">00:27:50.000</a></span> | <span class="t">It just aligns naturally to the multimodal input, such as images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1674" target="_blank">00:27:54.840</a></span> | <span class="t">I will show a couple of examples of what it can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1678" target="_blank">00:27:58.400</a></span> | <span class="t">And we can train in the same way as training large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1681" target="_blank">00:28:01.600</a></span> | <span class="t">So essentially, we can reuse the same infrastructure and training algorithm and everything to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1687" target="_blank">00:28:07.880</a></span> | <span class="t">this PAL-ME.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1690" target="_blank">00:28:10.320</a></span> | <span class="t">A couple of other things we find along the way is positive transfer, which I will share</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1695" target="_blank">00:28:15.600</a></span> | <span class="t">in a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1697" target="_blank">00:28:17.400</a></span> | <span class="t">So I guess here, I also want to mention PAL-ME is one of the largest models we have explored</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1704" target="_blank">00:28:24.400</a></span> | <span class="t">so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1705" target="_blank">00:28:25.400</a></span> | <span class="t">It has 562 billion parameters, which is by concatenating the PALM, 540 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1712" target="_blank">00:28:32.360</a></span> | <span class="t">and the 22 billion VIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1714" target="_blank">00:28:34.400</a></span> | <span class="t">And we find a lot of emergent capabilities of these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1719" target="_blank">00:28:39.040</a></span> | <span class="t">That is, we haven't expected during training time, but really, we can prompt these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1726" target="_blank">00:28:46.080</a></span> | <span class="t">and ask it to do interesting things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1728" target="_blank">00:28:48.920</a></span> | <span class="t">We have also explored using neural scene representation, basically an object-centric representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1737" target="_blank">00:28:57.800</a></span> | <span class="t">and fed into PAL-ME.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1739" target="_blank">00:28:59.440</a></span> | <span class="t">So object-centric representation assigns one token to each object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1747" target="_blank">00:29:07.160</a></span> | <span class="t">And we find that this representation is super helpful for robot planning tasks, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1753" target="_blank">00:29:13.040</a></span> | <span class="t">traditional VIT representation is based on grid, and it doesn't have a full understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1757" target="_blank">00:29:17.760</a></span> | <span class="t">of light objects and their relationships.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1760" target="_blank">00:29:20.640</a></span> | <span class="t">We have done an extensive study on the scaling performance and the catastrophic forgetting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1767" target="_blank">00:29:27.720</a></span> | <span class="t">performance and all other interesting experiments in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1772" target="_blank">00:29:32.640</a></span> | <span class="t">So please refer to the paper for more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1774" target="_blank">00:29:34.640</a></span> | <span class="t">So here, I'm just showing some interesting qualitative examples or some emergent capability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1781" target="_blank">00:29:41.360</a></span> | <span class="t">of PAL-ME that we found out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1784" target="_blank">00:29:44.400</a></span> | <span class="t">So first, we found this model has some reasoning capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1788" target="_blank">00:29:48.000</a></span> | <span class="t">You can give it an image and ask it questions that require a little bit of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1792" target="_blank">00:29:52.960</a></span> | <span class="t">And you can prompt this with, let's think step-by-step, which is a technique used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1798" target="_blank">00:29:58.600</a></span> | <span class="t">elicit reasoning in large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1801" target="_blank">00:30:01.400</a></span> | <span class="t">But here, in multi-modal language models, you can do the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1804" target="_blank">00:30:04.760</a></span> | <span class="t">I guess people are also experimenting these days with GPT-4V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1809" target="_blank">00:30:09.440</a></span> | <span class="t">You can also prompt it to think step-by-step or count row-by-row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1813" target="_blank">00:30:13.480</a></span> | <span class="t">But here, this is before GPT-4V, and we were able to elicit reasoning using some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1819" target="_blank">00:30:19.760</a></span> | <span class="t">interesting prompts, such as we can ask it, in this photo, are there more cats or more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1826" target="_blank">00:30:26.040</a></span> | <span class="t">dogs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1827" target="_blank">00:30:27.040</a></span> | <span class="t">Let's think step-by-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1828" target="_blank">00:30:28.040</a></span> | <span class="t">And the PAL-ME found out there are equal amount of dogs and cats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1832" target="_blank">00:30:32.200</a></span> | <span class="t">And on the right, give an image, can I go down the street on a bicycle, yes or no?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1837" target="_blank">00:30:37.880</a></span> | <span class="t">Let's think step-by-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1839" target="_blank">00:30:39.240</a></span> | <span class="t">And the reply is, do not enter, second, except the bicycles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1842" target="_blank">00:30:42.680</a></span> | <span class="t">Do not entry except the bicycles, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1845" target="_blank">00:30:45.200</a></span> | <span class="t">So it's doing this modest reasoning, and it's mixing this understanding of symbols and also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1852" target="_blank">00:30:52.800</a></span> | <span class="t">mixing the understanding of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1855" target="_blank">00:30:55.240</a></span> | <span class="t">So this is quite amazing to me, to be honest, when I first saw this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1860" target="_blank">00:31:00.440</a></span> | <span class="t">I didn't expect a multi-modal language model would be able to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1864" target="_blank">00:31:04.880</a></span> | <span class="t">And we also tried one thing, which is traditionally very difficult to language models, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1870" target="_blank">00:31:10.920</a></span> | <span class="t">to tell a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1871" target="_blank">00:31:11.920</a></span> | <span class="t">Language models can understand joke, but sometimes it just doesn't-- it's not able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1877" target="_blank">00:31:17.400</a></span> | <span class="t">to tell you a joke when it comes to the punchline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1881" target="_blank">00:31:21.600</a></span> | <span class="t">Because it's just trying to make something that is plausible and sounds like a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1887" target="_blank">00:31:27.040</a></span> | <span class="t">And when it comes to the punchline, it doesn't really know what to say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1890" target="_blank">00:31:30.760</a></span> | <span class="t">So here, I give it an image, and I ask it to come up with a description, and then comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1896" target="_blank">00:31:36.560</a></span> | <span class="t">up with a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1897" target="_blank">00:31:37.780</a></span> | <span class="t">So this guides the language model to think step-by-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1900" target="_blank">00:31:40.920</a></span> | <span class="t">And the description is a donkey is carrying a dog, cat, and rooster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1905" target="_blank">00:31:45.160</a></span> | <span class="t">And the joke is, what do you call a donkey with a rooster on his back?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1907" target="_blank">00:31:47.760</a></span> | <span class="t">A rooster booster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1908" target="_blank">00:31:48.760</a></span> | <span class="t">It's so creative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1910" target="_blank">00:31:50.200</a></span> | <span class="t">Like when I saw this, I'm pleasantly surprised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1913" target="_blank">00:31:53.240</a></span> | <span class="t">And I searched online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1914" target="_blank">00:31:54.240</a></span> | <span class="t">I couldn't find another joke like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1916" target="_blank">00:31:56.360</a></span> | <span class="t">So it's actually an original joke by Pomi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1918" target="_blank">00:31:58.840</a></span> | <span class="t">And finally, we see some math reasoning with this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1923" target="_blank">00:32:03.280</a></span> | <span class="t">Basically, I give it a messy menu from a pizza store, and I ask it, I'm just buying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1932" target="_blank">00:32:12.440</a></span> | <span class="t">a pizza for me and my friend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1933" target="_blank">00:32:13.840</a></span> | <span class="t">How much should I pay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1934" target="_blank">00:32:14.840</a></span> | <span class="t">Let's think step-by-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1936" target="_blank">00:32:16.040</a></span> | <span class="t">And it's figuring out there is a pizza, and there is $9.99, and it tells you the price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1943" target="_blank">00:32:23.520</a></span> | <span class="t">In some of the answers, it even calculates text, but the text is hallucinated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1948" target="_blank">00:32:28.060</a></span> | <span class="t">So that doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1949" target="_blank">00:32:29.520</a></span> | <span class="t">All right, let's talk about positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1952" target="_blank">00:32:32.520</a></span> | <span class="t">So apart from the amazing things that Pomi can do, it also has interesting positive transfer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1961" target="_blank">00:32:41.080</a></span> | <span class="t">behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1963" target="_blank">00:32:43.100</a></span> | <span class="t">So when we train Pomi on a single domain, when we train it on just a single robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1969" target="_blank">00:32:49.840</a></span> | <span class="t">task, the performance is not super great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1972" target="_blank">00:32:52.480</a></span> | <span class="t">But when we pool all the data together, and we also include internet-scale visual language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1979" target="_blank">00:32:59.280</a></span> | <span class="t">tasks, such as captioning or visual question answering, it is able to do much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1985" target="_blank">00:33:05.400</a></span> | <span class="t">So this shows that it's important to mix all the data together and train it jointly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=1992" target="_blank">00:33:12.520</a></span> | <span class="t">The internet-scale data can act as a regularizer for you to not forget the representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2000" target="_blank">00:33:20.960</a></span> | <span class="t">And those representations are, in turn, very useful for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2006" target="_blank">00:33:26.400</a></span> | <span class="t">So that's a positive transfer result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2008" target="_blank">00:33:28.300</a></span> | <span class="t">And we start to see more and more positive transfer in other of our studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2011" target="_blank">00:33:31.660</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2012" target="_blank">00:33:32.660</a></span> | <span class="t">So how much data do you have to do collectively, like in simulation or in real world?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2017" target="_blank">00:33:37.480</a></span> | <span class="t">I think the playing with sorting stuff on the table is very impressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2024" target="_blank">00:33:44.520</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2025" target="_blank">00:33:45.520</a></span> | <span class="t">Yeah, that's a very good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2030" target="_blank">00:33:50.640</a></span> | <span class="t">So these are all planning data, like high-level planning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2037" target="_blank">00:33:57.040</a></span> | <span class="t">So maybe let's just talk about two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2040" target="_blank">00:34:00.000</a></span> | <span class="t">So first of all, the sorting results, the low-level policy is still using a traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2047" target="_blank">00:34:07.340</a></span> | <span class="t">controller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2048" target="_blank">00:34:08.600</a></span> | <span class="t">So it's using a policy called LAVA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2050" target="_blank">00:34:10.680</a></span> | <span class="t">And that policy is trained on 68,000 episodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2056" target="_blank">00:34:16.080</a></span> | <span class="t">The high-level planning is probably easier than you think, because it's giving command</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2064" target="_blank">00:34:24.600</a></span> | <span class="t">to the low-level policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2065" target="_blank">00:34:25.800</a></span> | <span class="t">So it's basically only need to say, put the red block into top-left corner, put another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2071" target="_blank">00:34:31.480</a></span> | <span class="t">red block into top-left corner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2072" target="_blank">00:34:32.960</a></span> | <span class="t">So it's rather quite standard autoregressive language modeling task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2079" target="_blank">00:34:39.840</a></span> | <span class="t">The only thing I need to do is to determine what task is not finished yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2085" target="_blank">00:34:45.260</a></span> | <span class="t">So for example, if the block is already in the corner, it shouldn't call low-level policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2089" target="_blank">00:34:49.120</a></span> | <span class="t">to move it to the corner again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2090" target="_blank">00:34:50.780</a></span> | <span class="t">So it's rather like parsing the states and understanding the states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2095" target="_blank">00:34:55.720</a></span> | <span class="t">So this high-level policy only requires about 50 to 100 demonstrations to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2100" target="_blank">00:35:00.520</a></span> | <span class="t">So it's quite parameter efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2102" target="_blank">00:35:02.800</a></span> | <span class="t">And in the future-- that's a very good question, actually-- in the future, a lot of these tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2107" target="_blank">00:35:07.280</a></span> | <span class="t">can be taught in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2109" target="_blank">00:35:09.240</a></span> | <span class="t">So maybe we just demonstrate it once to the large-language model, then it knows how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2113" target="_blank">00:35:13.880</a></span> | <span class="t">do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2114" target="_blank">00:35:14.880</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2115" target="_blank">00:35:15.880</a></span> | <span class="t">Yeah, this is through human demonstration as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2127" target="_blank">00:35:27.400</a></span> | <span class="t">So a human on a low-level can demonstrate low-level policy by tele-operating a robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2132" target="_blank">00:35:32.640</a></span> | <span class="t">to do a certain task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2134" target="_blank">00:35:34.200</a></span> | <span class="t">But on a high-level, it could also just give the low-level policy-- imagine your control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2142" target="_blank">00:35:42.440</a></span> | <span class="t">interface is through text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2144" target="_blank">00:35:44.460</a></span> | <span class="t">And then as a human, you can also guide a low-level policy to accomplish a task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2149" target="_blank">00:35:49.600</a></span> | <span class="t">And then that thing can then be used to train a large-language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2154" target="_blank">00:35:54.840</a></span> | <span class="t">So that's for the sorting block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2157" target="_blank">00:35:57.280</a></span> | <span class="t">The secant is a little bit more interesting because the planning steps are actually generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2162" target="_blank">00:36:02.740</a></span> | <span class="t">by POM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2164" target="_blank">00:36:04.460</a></span> | <span class="t">So we essentially distilled POM plus this affordance model into POM-e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2170" target="_blank">00:36:10.880</a></span> | <span class="t">So that's a little bit more interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2173" target="_blank">00:36:13.020</a></span> | <span class="t">It's like using the AI data to bootstrap itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2176" target="_blank">00:36:16.840</a></span> | <span class="t">That one has about 3,000 episodes, also not quite a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2182" target="_blank">00:36:22.080</a></span> | <span class="t">But it's able to learn complex planning behavior, replanning behavior, error recovery, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2188" target="_blank">00:36:28.680</a></span> | <span class="t">I will show in a slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2190" target="_blank">00:36:30.000</a></span> | <span class="t">So with the POM-e as a high-level planner, we are able to take the rice chips out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2198" target="_blank">00:36:38.960</a></span> | <span class="t">the drawer, and there is a twist, which is I will be messing with the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2207" target="_blank">00:36:47.440</a></span> | <span class="t">So as it put onto counter, I put it back to the drawer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2212" target="_blank">00:36:52.040</a></span> | <span class="t">And as it pick it up again, and then I put it back again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2216" target="_blank">00:36:56.880</a></span> | <span class="t">So it's able to understand the state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2218" target="_blank">00:36:58.400</a></span> | <span class="t">It's able to understand my task is not finished.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2221" target="_blank">00:37:01.120</a></span> | <span class="t">I cannot proceed with the next task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2223" target="_blank">00:37:03.240</a></span> | <span class="t">Now, after I don't mess with it anymore, it's able to close the drawer and pick up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2228" target="_blank">00:37:08.760</a></span> | <span class="t">the bag of chips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2231" target="_blank">00:37:11.560</a></span> | <span class="t">So POM-e is able to combine affordance and planning in one model and do complex reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2239" target="_blank">00:37:19.480</a></span> | <span class="t">of a scene and environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2242" target="_blank">00:37:22.760</a></span> | <span class="t">And interestingly, we can use the exact same model checkpoint to do block sorting as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2248" target="_blank">00:37:28.960</a></span> | <span class="t">So this is the same model checkpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2250" target="_blank">00:37:30.840</a></span> | <span class="t">It can not only reason about how to bring a bag of chips to a user, it can also sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2257" target="_blank">00:37:37.000</a></span> | <span class="t">blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2258" target="_blank">00:37:38.000</a></span> | <span class="t">So and it's also responding to adversarial perturbation, like if the user is putting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2266" target="_blank">00:37:46.060</a></span> | <span class="t">the block in the middle again, it's able to recover from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2270" target="_blank">00:37:50.120</a></span> | <span class="t">So these are all coming from the same model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2273" target="_blank">00:37:53.040</a></span> | <span class="t">And it can also tell a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2277" target="_blank">00:37:57.360</a></span> | <span class="t">So yeah, this is the power of vision language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2283" target="_blank">00:38:03.440</a></span> | <span class="t">Now we want to go a level deeper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2286" target="_blank">00:38:06.160</a></span> | <span class="t">These are all vision language models that are used for planning or high-level reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2290" target="_blank">00:38:10.520</a></span> | <span class="t">Can we use them for low-level control?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2292" target="_blank">00:38:12.800</a></span> | <span class="t">It turns out we can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2295" target="_blank">00:38:15.280</a></span> | <span class="t">And that's the RGQ work, which is vision language action model that transfer web knowledge to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2300" target="_blank">00:38:20.120</a></span> | <span class="t">robotic control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2301" target="_blank">00:38:21.120</a></span> | <span class="t">What can it do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2303" target="_blank">00:38:23.200</a></span> | <span class="t">When asked, pick up the extinct animal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2308" target="_blank">00:38:28.480</a></span> | <span class="t">And it has a whole range of objects on the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2311" target="_blank">00:38:31.480</a></span> | <span class="t">It will pick up the dinosaur.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2312" target="_blank">00:38:32.860</a></span> | <span class="t">So it can link the extinct animal to dinosaur and to the action that pick the dinosaur up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2320" target="_blank">00:38:40.960</a></span> | <span class="t">So it's really doing this emergent reasoning and also the manipulation in just the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2326" target="_blank">00:38:46.760</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2327" target="_blank">00:38:47.760</a></span> | <span class="t">And by the way, this robot hasn't seen any of these before, at least in the robot training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2334" target="_blank">00:38:54.440</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2335" target="_blank">00:38:55.440</a></span> | <span class="t">It might have seen this in their internet catalog, but it has never seen it in the robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2341" target="_blank">00:39:01.600</a></span> | <span class="t">training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2343" target="_blank">00:39:03.080</a></span> | <span class="t">So it's quite interesting how we need to evaluate these robots nowadays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2350" target="_blank">00:39:10.680</a></span> | <span class="t">So when we evaluate language models to prevent data contamination, every time you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2356" target="_blank">00:39:16.480</a></span> | <span class="t">give it new questions because otherwise it might already memorize it in its training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2362" target="_blank">00:39:22.000</a></span> | <span class="t">When we evaluate these robots, we actually go to dollar store to buy all these toys to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2367" target="_blank">00:39:27.200</a></span> | <span class="t">make sure it hasn't seen that before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2369" target="_blank">00:39:29.520</a></span> | <span class="t">And as we run more evaluation, maybe there will be some replication as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2373" target="_blank">00:39:33.840</a></span> | <span class="t">But as you can see, it is able to understand to pick up this dinosaur toy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2380" target="_blank">00:39:40.880</a></span> | <span class="t">How did we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2382" target="_blank">00:39:42.960</a></span> | <span class="t">So we start from a visual language model that is trained on internet-scale data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2389" target="_blank">00:39:49.000</a></span> | <span class="t">And then we also combine it with robotics action data, which is the RT1 data and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2393" target="_blank">00:39:53.920</a></span> | <span class="t">get RT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2395" target="_blank">00:39:55.600</a></span> | <span class="t">And we can dive deeper, a little bit deeper into RT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2400" target="_blank">00:40:00.280</a></span> | <span class="t">So first of all, what is a visual language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2402" target="_blank">00:40:02.520</a></span> | <span class="t">A visual language model is a transformer that takes in image and text and output text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2411" target="_blank">00:40:11.380</a></span> | <span class="t">So within Google, there is a visual language model called Pali, which is an encoder-decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2421" target="_blank">00:40:21.040</a></span> | <span class="t">type of architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2422" target="_blank">00:40:22.440</a></span> | <span class="t">It's basically having a VIT to understand images and then a transformer encoder and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2427" target="_blank">00:40:27.840</a></span> | <span class="t">the transformer decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2430" target="_blank">00:40:30.960</a></span> | <span class="t">They encompass both the visual and semantics to understand the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2435" target="_blank">00:40:35.800</a></span> | <span class="t">And in robotics, we have to deal with a lot of both of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2440" target="_blank">00:40:40.500</a></span> | <span class="t">And the question is, can we leverage the knowledge in the visual language models and apply them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2446" target="_blank">00:40:46.460</a></span> | <span class="t">to robotics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2448" target="_blank">00:40:48.720</a></span> | <span class="t">On the other hand, we have the RT1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2451" target="_blank">00:40:51.320</a></span> | <span class="t">If you want to learn more about RT1, you can listen to the previous episode of this CS25</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2457" target="_blank">00:40:57.840</a></span> | <span class="t">by Tad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2459" target="_blank">00:40:59.320</a></span> | <span class="t">So he gave a detailed introduction on the RT1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2462" target="_blank">00:41:02.080</a></span> | <span class="t">But the RT1 is, if you stand far enough, it is also a vision language to action or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2471" target="_blank">00:41:11.840</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2472" target="_blank">00:41:12.840</a></span> | <span class="t">It takes in human instruction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2474" target="_blank">00:41:14.400</a></span> | <span class="t">It takes in the current camera image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2476" target="_blank">00:41:16.220</a></span> | <span class="t">The camera image passed through a film-efficient net, which is tokenized into 81 tokens, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2481" target="_blank">00:41:21.580</a></span> | <span class="t">then going to a token learner, which compresses everything into eight tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2486" target="_blank">00:41:26.480</a></span> | <span class="t">And then there is a transformer block, leveraging a lot of self-intention layer, and then generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2491" target="_blank">00:41:31.680</a></span> | <span class="t">actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2492" target="_blank">00:41:32.680</a></span> | <span class="t">The action is also tokenized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2494" target="_blank">00:41:34.880</a></span> | <span class="t">The robot has seven degrees of freedom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2501" target="_blank">00:41:41.040</a></span> | <span class="t">The anti-factor has six degrees of freedom, its position and the rotation, and the gripper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2507" target="_blank">00:41:47.660</a></span> | <span class="t">can open and close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2509" target="_blank">00:41:49.140</a></span> | <span class="t">And there is another dimension representing terminate the episode or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2514" target="_blank">00:41:54.580</a></span> | <span class="t">Terminating means my task is already done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2517" target="_blank">00:41:57.500</a></span> | <span class="t">And we discretize every dimension into 256 bins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2523" target="_blank">00:42:03.020</a></span> | <span class="t">And then we do cross-entropy loss on those bins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2525" target="_blank">00:42:05.780</a></span> | <span class="t">So that's the RT1 architecture in a nutshell.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2530" target="_blank">00:42:10.020</a></span> | <span class="t">It's quite similar to a vision language model with different output tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2533" target="_blank">00:42:13.820</a></span> | <span class="t">So it's rather natural that we just use a large pre-trained vision language model directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2539" target="_blank">00:42:19.180</a></span> | <span class="t">as policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2540" target="_blank">00:42:20.180</a></span> | <span class="t">We can use the poly or poly-me as a policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2544" target="_blank">00:42:24.440</a></span> | <span class="t">And one question is, how do we deal with actions when using pre-trained vision language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2550" target="_blank">00:42:30.120</a></span> | <span class="t">And here is action representation that we use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2553" target="_blank">00:42:33.460</a></span> | <span class="t">The robot actions here are the eight dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2558" target="_blank">00:42:38.100</a></span> | <span class="t">And as I mentioned, there is termination, position change, and rotation change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2562" target="_blank">00:42:42.560</a></span> | <span class="t">And we discretize everything into 256 bins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2567" target="_blank">00:42:47.120</a></span> | <span class="t">We also have tried other alternative representations, but they are not as good as just this naive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2572" target="_blank">00:42:52.660</a></span> | <span class="t">representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2573" target="_blank">00:42:53.660</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2574" target="_blank">00:42:54.660</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2575" target="_blank">00:42:55.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2576" target="_blank">00:42:56.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2577" target="_blank">00:42:57.660</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2578" target="_blank">00:42:58.660</a></span> | <span class="t">Oh, the film efficient net is a pre-trained convolutional neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2584" target="_blank">00:43:04.620</a></span> | <span class="t">It's used to tokenize the images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2587" target="_blank">00:43:07.180</a></span> | <span class="t">So the reason that we do this is, through some ablation study, we can tokenize the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2591" target="_blank">00:43:11.740</a></span> | <span class="t">in different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2592" target="_blank">00:43:12.740</a></span> | <span class="t">We can tokenize in ResNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2594" target="_blank">00:43:14.740</a></span> | <span class="t">We can tokenize everything into ResNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2597" target="_blank">00:43:17.100</a></span> | <span class="t">And we can tokenize using film efficient net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2599" target="_blank">00:43:19.940</a></span> | <span class="t">Film, what it means is it also take into the language embedding and append it to the intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2606" target="_blank">00:43:26.300</a></span> | <span class="t">layers of the ResNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2608" target="_blank">00:43:28.300</a></span> | <span class="t">So we basically have some combination of feathers, and it's encoded in images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2613" target="_blank">00:43:33.820</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2614" target="_blank">00:43:34.820</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2615" target="_blank">00:43:35.820</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2616" target="_blank">00:43:36.820</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2617" target="_blank">00:43:37.820</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2618" target="_blank">00:43:38.820</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2619" target="_blank">00:43:39.820</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2620" target="_blank">00:43:40.820</a></span> | <span class="t">That's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2621" target="_blank">00:43:41.820</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2622" target="_blank">00:43:42.820</a></span> | <span class="t">The action is not encoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2623" target="_blank">00:43:43.820</a></span> | <span class="t">The action is in text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2624" target="_blank">00:43:44.820</a></span> | <span class="t">It's basically what is shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2625" target="_blank">00:43:45.820</a></span> | <span class="t">This is the action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2626" target="_blank">00:43:46.820</a></span> | <span class="t">It's eight numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2627" target="_blank">00:43:47.820</a></span> | <span class="t">Each number range from 0 to 255.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2628" target="_blank">00:43:48.820</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2629" target="_blank">00:43:49.820</a></span> | <span class="t">And maybe another note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2630" target="_blank">00:43:50.820</a></span> | <span class="t">On the film ResNet, it's about how we tokenize the images and how we combine vision information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2653" target="_blank">00:44:13.020</a></span> | <span class="t">and language information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2654" target="_blank">00:44:14.860</a></span> | <span class="t">There are many ways to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2656" target="_blank">00:44:16.340</a></span> | <span class="t">This is not the only way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2657" target="_blank">00:44:17.580</a></span> | <span class="t">There is early fusion and late fusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2660" target="_blank">00:44:20.260</a></span> | <span class="t">And there is also cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2662" target="_blank">00:44:22.100</a></span> | <span class="t">You can basically tokenize your image just by itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2665" target="_blank">00:44:25.220</a></span> | <span class="t">And then you can have language and use cross-attention to combine the image and text representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2671" target="_blank">00:44:31.140</a></span> | <span class="t">So here, we are using this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2673" target="_blank">00:44:33.540</a></span> | <span class="t">This is RT1 for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2675" target="_blank">00:44:35.540</a></span> | <span class="t">So we do have a lot of considerations, such as latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2678" target="_blank">00:44:38.660</a></span> | <span class="t">That's why we use this film ResNet, because it's super fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2682" target="_blank">00:44:42.020</a></span> | <span class="t">And it can output a limited amount of tokens, which we can further compress with Token Learner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2687" target="_blank">00:44:47.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2688" target="_blank">00:44:48.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2689" target="_blank">00:44:49.620</a></span> | <span class="t">So is this autoregressive?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2690" target="_blank">00:44:50.620</a></span> | <span class="t">Like, every single image it sees, it then reacts with each other?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2694" target="_blank">00:44:54.740</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2695" target="_blank">00:44:55.740</a></span> | <span class="t">So it is autoregressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2696" target="_blank">00:44:56.740</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2697" target="_blank">00:44:57.740</a></span> | <span class="t">And every time, we use a history of up to six steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2702" target="_blank">00:45:02.100</a></span> | <span class="t">So every time, you see this image right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2704" target="_blank">00:45:04.540</a></span> | <span class="t">And you see about two seconds of history before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2709" target="_blank">00:45:09.180</a></span> | <span class="t">And this will be your input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2711" target="_blank">00:45:11.940</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2712" target="_blank">00:45:12.940</a></span> | <span class="t">Again, if you have more questions about RT1, I recommend watching the previous episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2718" target="_blank">00:45:18.500</a></span> | <span class="t">And here, it's all about RT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2722" target="_blank">00:45:22.740</a></span> | <span class="t">So we can convert the string of numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2726" target="_blank">00:45:26.660</a></span> | <span class="t">This will be our output of our transformer, which is a visual language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2731" target="_blank">00:45:31.420</a></span> | <span class="t">We tried other alternatives, such as floating numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2735" target="_blank">00:45:35.300</a></span> | <span class="t">Floating numbers is not super friendly to language model tokenizer, because it has these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2741" target="_blank">00:45:41.340</a></span> | <span class="t">decimal points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2742" target="_blank">00:45:42.340</a></span> | <span class="t">We also tried the human language, such as left or right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2745" target="_blank">00:45:45.100</a></span> | <span class="t">It's more a semantic representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2746" target="_blank">00:45:46.820</a></span> | <span class="t">But they cannot be directly executed on a robot, which is a limitation of this method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2753" target="_blank">00:45:53.440</a></span> | <span class="t">So if we commit to this action representation, which is just a string of numbers, we essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2759" target="_blank">00:45:59.180</a></span> | <span class="t">get a visual language action model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2761" target="_blank">00:46:01.420</a></span> | <span class="t">We tried different variants, including polyX.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2765" target="_blank">00:46:05.380</a></span> | <span class="t">This is a pathways language image model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2771" target="_blank">00:46:11.700</a></span> | <span class="t">There are 5 billion parameters variant and 55 billion parameter variant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2776" target="_blank">00:46:16.020</a></span> | <span class="t">And we also tried POMI, which is 12 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2780" target="_blank">00:46:20.300</a></span> | <span class="t">The procedure that we did to train this RT2 is via co-fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2786" target="_blank">00:46:26.700</a></span> | <span class="t">Co-fine tuning is to put the internet scale data and the robotic data together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2792" target="_blank">00:46:32.880</a></span> | <span class="t">And then we fine tune it on this mixture of data so that it doesn't-- it retains the internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2799" target="_blank">00:46:39.100</a></span> | <span class="t">scale knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2802" target="_blank">00:46:42.940</a></span> | <span class="t">Maybe that's also an artifact of our data is too small and not diverse enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2806" target="_blank">00:46:46.620</a></span> | <span class="t">So if you're just a fine tune on robotics data, it will quickly overfit and forget about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2812" target="_blank">00:46:52.180</a></span> | <span class="t">all this progeny mixture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2814" target="_blank">00:46:54.480</a></span> | <span class="t">Maybe it's a dynamic of scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2816" target="_blank">00:46:56.660</a></span> | <span class="t">So we'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2819" target="_blank">00:46:59.580</a></span> | <span class="t">At inference time, how do we do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2822" target="_blank">00:47:02.260</a></span> | <span class="t">We basically-- again, we do this autoregressively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2825" target="_blank">00:47:05.860</a></span> | <span class="t">We have an instruction of a task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2830" target="_blank">00:47:10.180</a></span> | <span class="t">And we format this as a question and answering task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2833" target="_blank">00:47:13.260</a></span> | <span class="t">What should the robot do to achieve a certain task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2835" target="_blank">00:47:15.860</a></span> | <span class="t">And the task is a string that human give the robot for the robot to achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2840" target="_blank">00:47:20.620</a></span> | <span class="t">And it also have the current observation, which is the robot observation, the camera</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2849" target="_blank">00:47:29.600</a></span> | <span class="t">image, RGB image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2850" target="_blank">00:47:30.600</a></span> | <span class="t">It pass through a VIT, and then it pass through the large language model, and then output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2856" target="_blank">00:47:36.560</a></span> | <span class="t">a list of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2858" target="_blank">00:47:38.380</a></span> | <span class="t">So we leverage the constraint decoding to make sure it always have eight numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2865" target="_blank">00:47:45.680</a></span> | <span class="t">And because otherwise, we cannot de-tokenize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2869" target="_blank">00:47:49.640</a></span> | <span class="t">It's very easy for language model to just miss one number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2872" target="_blank">00:47:52.860</a></span> | <span class="t">So we do have some mechanism, such as constraint decoding and beam search, to make sure the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2878" target="_blank">00:47:58.320</a></span> | <span class="t">format is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2879" target="_blank">00:47:59.320</a></span> | <span class="t">After we get the string of eight numbers, we de-tokenize it to a delta T and delta R,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2884" target="_blank">00:48:04.920</a></span> | <span class="t">which is the anti-factor delta pose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2887" target="_blank">00:48:07.280</a></span> | <span class="t">And the robot can just directly run this on the robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2890" target="_blank">00:48:10.120</a></span> | <span class="t">After they run on the robots, we repeat this process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2893" target="_blank">00:48:13.280</a></span> | <span class="t">We get another new image, run through this process, and get a new action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2897" target="_blank">00:48:17.160</a></span> | <span class="t">And we repeat this process until a termination is decoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2901" target="_blank">00:48:21.240</a></span> | <span class="t">So some people might be concerned that this is rather slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2907" target="_blank">00:48:27.040</a></span> | <span class="t">It's in fact quite slow, because it's 12 billion parameters, or 5 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2913" target="_blank">00:48:33.160</a></span> | <span class="t">We cannot run on a robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2914" target="_blank">00:48:34.920</a></span> | <span class="t">So we run on a TPO cluster, and the robot is querying the TPO cluster to get the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2920" target="_blank">00:48:40.240</a></span> | <span class="t">and apply it on the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2922" target="_blank">00:48:42.980</a></span> | <span class="t">So for the 12 billion parameters, we can actually run at 10 hertz.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2928" target="_blank">00:48:48.080</a></span> | <span class="t">So it's quite fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2929" target="_blank">00:48:49.280</a></span> | <span class="t">For all those models, we can run at least three hertz.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2932" target="_blank">00:48:52.060</a></span> | <span class="t">So that is sufficient for controlling a robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2937" target="_blank">00:48:57.960</a></span> | <span class="t">And we see a lot of emergent skills that is not on the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2944" target="_blank">00:49:04.880</a></span> | <span class="t">Essentially, as I just mentioned, we are probing what this RT2 can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2949" target="_blank">00:49:09.280</a></span> | <span class="t">We actually don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2950" target="_blank">00:49:10.280</a></span> | <span class="t">So we are trying to figure out what RT2 can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2952" target="_blank">00:49:12.440</a></span> | <span class="t">So we test it with a lot of new tasks, such as put a strawberry into the correct bowl,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2958" target="_blank">00:49:18.720</a></span> | <span class="t">or move a banana to Germany, just to test its understanding of symbols or flags.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2966" target="_blank">00:49:26.360</a></span> | <span class="t">Pick a land animal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2967" target="_blank">00:49:27.360</a></span> | <span class="t">There's a horse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2968" target="_blank">00:49:28.360</a></span> | <span class="t">There's an octopus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2969" target="_blank">00:49:29.360</a></span> | <span class="t">So basically, test its semantic reasoning and also low-level manipulation skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2976" target="_blank">00:49:36.240</a></span> | <span class="t">And we divide the tasks into symbol understanding, and reasoning, and human recognition, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2984" target="_blank">00:49:44.280</a></span> | <span class="t">average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2985" target="_blank">00:49:45.280</a></span> | <span class="t">And we found that with RT1, which is not trained on internet-scale data, we do quite poorly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2993" target="_blank">00:49:53.080</a></span> | <span class="t">in these emergent evaluation tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=2996" target="_blank">00:49:56.200</a></span> | <span class="t">And in the RT2 variants, which is co-fine-tuned on the internet data and our robotics data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3006" target="_blank">00:50:06.280</a></span> | <span class="t">we do much better in these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3008" target="_blank">00:50:08.280</a></span> | <span class="t">And there is also an effect of scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3011" target="_blank">00:50:11.040</a></span> | <span class="t">So the RT2 with the 55 billion poly is performing better than the 12 billion poly, although</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3017" target="_blank">00:50:17.920</a></span> | <span class="t">they perform quite similarly for in-domain tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3020" target="_blank">00:50:20.840</a></span> | <span class="t">But the generalization is kind of interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3023" target="_blank">00:50:23.200</a></span> | <span class="t">It seems with larger scale, you can generalize better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3027" target="_blank">00:50:27.920</a></span> | <span class="t">And here are some videos of the robot achieving these tasks, like moving the banana to a number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3035" target="_blank">00:50:35.840</a></span> | <span class="t">put the strawberry into the correct bowl, move a Rubik's cube to the water bottle--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3041" target="_blank">00:50:41.880</a></span> | <span class="t">but I'm speaking Chinese-- moving the banana to a German flag.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3046" target="_blank">00:50:46.840</a></span> | <span class="t">So it's able to do all of these very interesting tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3051" target="_blank">00:50:51.960</a></span> | <span class="t">In terms of the quantitative evaluations, we also found that the RT2 policy is quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3057" target="_blank">00:50:57.880</a></span> | <span class="t">robust to unseen objects, unseen backgrounds, and unseen environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3063" target="_blank">00:51:03.960</a></span> | <span class="t">And here is another evidence of positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3067" target="_blank">00:51:07.040</a></span> | <span class="t">So co-fine-tuned with VQA data outperforms fine-tuning on robotics only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3072" target="_blank">00:51:12.520</a></span> | <span class="t">And if you're trained on robot data from scratch, it barely works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3076" target="_blank">00:51:16.680</a></span> | <span class="t">It almost doesn't work, because it overfits to robot data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3079" target="_blank">00:51:19.720</a></span> | <span class="t">And our robot data is just too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3081" target="_blank">00:51:21.880</a></span> | <span class="t">So we do need to do co-fine-tuning, or at least fine-tuning, so it retains its internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3089" target="_blank">00:51:29.320</a></span> | <span class="t">scale knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3090" target="_blank">00:51:30.660</a></span> | <span class="t">This is also a recipe for how people would develop a domain-specific vision language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3095" target="_blank">00:51:35.960</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3096" target="_blank">00:51:36.960</a></span> | <span class="t">So you start from a very general vision language model, and you fine-tune on your domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3101" target="_blank">00:51:41.240</a></span> | <span class="t">Or you can co-fine-tune with your specific domain data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3105" target="_blank">00:51:45.540</a></span> | <span class="t">This is likely a problem that each vertical of artificial intelligence would incur someday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3114" target="_blank">00:51:54.520</a></span> | <span class="t">We can also test on other platforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3116" target="_blank">00:51:56.800</a></span> | <span class="t">Like this shows some cross-embodiment, the RT2, PolyE3b outperforms previous models in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3122" target="_blank">00:52:02.120</a></span> | <span class="t">terms of moving blocks around a 2D environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3129" target="_blank">00:52:09.240</a></span> | <span class="t">And in large-language models, we have this chain-of-thought reasoning, which is a method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3134" target="_blank">00:52:14.640</a></span> | <span class="t">to elicit reasoning in large-language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3138" target="_blank">00:52:18.120</a></span> | <span class="t">You can either do zero-shot chain-of-thought reasoning by, say, eliciting step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3141" target="_blank">00:52:21.400</a></span> | <span class="t">I'll give you the examples of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3143" target="_blank">00:52:23.800</a></span> | <span class="t">It's basically decoding more things and then come to the conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3147" target="_blank">00:52:27.840</a></span> | <span class="t">We can use a similar procedure for the RT2 as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3152" target="_blank">00:52:32.120</a></span> | <span class="t">So in RT2 PolyE, instead of directly decoding the actions, we can actually decode a plan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3158" target="_blank">00:52:38.060</a></span> | <span class="t">and then append it with actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3160" target="_blank">00:52:40.280</a></span> | <span class="t">So this gives the language model an opportunity to understand a question or parse a question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3164" target="_blank">00:52:44.940</a></span> | <span class="t">differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3165" target="_blank">00:52:45.940</a></span> | <span class="t">It also gives us the opportunity to reason about things a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3170" target="_blank">00:52:50.200</a></span> | <span class="t">For example, if you say, "Bring me a drink," and it will say, "Pick up 7-up can," because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3175" target="_blank">00:52:55.200</a></span> | <span class="t">there's a 7-up can on the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3177" target="_blank">00:52:57.720</a></span> | <span class="t">So we synthesized a couple hundred such examples using a large-language model just by augmenting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3182" target="_blank">00:53:02.960</a></span> | <span class="t">the instruction and then fine-tuned the RT2 just for a couple hundred steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3187" target="_blank">00:53:07.080</a></span> | <span class="t">So it's between full fine-tuning and in-context learning, and it is able to do some reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3193" target="_blank">00:53:13.480</a></span> | <span class="t">And some of the interesting reasoning tasks include, "I need to hammer a nail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3197" target="_blank">00:53:17.560</a></span> | <span class="t">Which object from the scene might be useful?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3199" target="_blank">00:53:19.440</a></span> | <span class="t">And in the scene, there is a headphone, there is a rock, and there is a sticky note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3204" target="_blank">00:53:24.600</a></span> | <span class="t">And the robot will say, "Rocks," and then generate actions to pick up the rock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3208" target="_blank">00:53:28.460</a></span> | <span class="t">So it's interesting that it's able to do this sort of reasoning with RT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3214" target="_blank">00:53:34.040</a></span> | <span class="t">And here is a demonstration of some of the channel-thought reasoning with RT2 PolyE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3219" target="_blank">00:53:39.880</a></span> | <span class="t">And the task is, "Pick up the thing that is different from all other objects."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3224" target="_blank">00:53:44.280</a></span> | <span class="t">And it picks up the chocolate, because this is a snack and other things are the drink.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3229" target="_blank">00:53:49.120</a></span> | <span class="t">And I can also speak a different language, and the plan would be to translate it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3233" target="_blank">00:53:53.920</a></span> | <span class="t">a language that it's familiar with, which is English, and then do the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3241" target="_blank">00:54:01.280</a></span> | <span class="t">There are also potentially better cases of the channel-thought reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3244" target="_blank">00:54:04.700</a></span> | <span class="t">So here I say, "Move the green object together."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3246" target="_blank">00:54:06.720</a></span> | <span class="t">And as you can see, the robot oscillates between the two green objects, because there are rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3250" target="_blank">00:54:10.720</a></span> | <span class="t">two plans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3251" target="_blank">00:54:11.720</a></span> | <span class="t">It could move the can to the bag of chips, or it could move the bag of chips to the can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3256" target="_blank">00:54:16.820</a></span> | <span class="t">It oscillates between two plans until one action brings it to an object, and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3262" target="_blank">00:54:22.220</a></span> | <span class="t">commit to one of the plans rather than another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3266" target="_blank">00:54:26.460</a></span> | <span class="t">It's not always guaranteed to work, but it's quite interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3269" target="_blank">00:54:29.920</a></span> | <span class="t">And it's also interesting that, again, we are testing the manipulation policy like how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3274" target="_blank">00:54:34.460</a></span> | <span class="t">we test intelligence of humans or animals or kids, because they're getting more and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3280" target="_blank">00:54:40.980</a></span> | <span class="t">more advanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3281" target="_blank">00:54:41.980</a></span> | <span class="t">As a summary, we have the vision language and action model that is able to improve the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3289" target="_blank">00:54:49.560</a></span> | <span class="t">generalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3290" target="_blank">00:54:50.560</a></span> | <span class="t">It can do new tasks and operate the new objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3293" target="_blank">00:54:53.520</a></span> | <span class="t">It can also do chain-of-thought reasoning, and improving the underlying model, such as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3299" target="_blank">00:54:59.440</a></span> | <span class="t">the vision language model itself, by scaling it up and training it with internet-scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3306" target="_blank">00:55:06.320</a></span> | <span class="t">data or training it with larger or higher-quality internet-scale data, we can achieve better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3311" target="_blank">00:55:11.040</a></span> | <span class="t">robot control, which is quite amazing, because robotics field has been traditionally developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3316" target="_blank">00:55:16.280</a></span> | <span class="t">quite slowly and is bounded by hardware, bounded by a lot of different things, bounded by operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3321" target="_blank">00:55:21.120</a></span> | <span class="t">But now it seems we can piggyback on the development of the foundation model field, and whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3327" target="_blank">00:55:27.920</a></span> | <span class="t">they do will trickle down to our field as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3330" target="_blank">00:55:30.560</a></span> | <span class="t">And the future will be to increase the motion diversity and extend on the chain-of-thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3335" target="_blank">00:55:35.040</a></span> | <span class="t">reasoning capability and many more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3340" target="_blank">00:55:40.040</a></span> | <span class="t">And so there is another example of positive transfer, which you might have seen recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3346" target="_blank">00:55:46.520</a></span> | <span class="t">So far, I've been talking about scaling differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3349" target="_blank">00:55:49.520</a></span> | <span class="t">I've been talking about don't scale robotics data and scale other data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3354" target="_blank">00:55:54.120</a></span> | <span class="t">That's because robotics data is so hard to collect, and the purpose is not to avoid collecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3359" target="_blank">00:55:59.160</a></span> | <span class="t">robotics data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3360" target="_blank">00:56:00.160</a></span> | <span class="t">It's to develop a recipe that you can do more with limited robotics data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3365" target="_blank">00:56:05.560</a></span> | <span class="t">However, there's also an effort from our team and the entire robotics field to scale up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3372" target="_blank">00:56:12.640</a></span> | <span class="t">the robot data collection, which is called OpenX Embodiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3376" target="_blank">00:56:16.840</a></span> | <span class="t">And the model chain is called RTX, Robotics Transformer X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3380" target="_blank">00:56:20.440</a></span> | <span class="t">It's basically 22 type of embodiments and 572 scales and 60 datasets pulled all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3388" target="_blank">00:56:28.520</a></span> | <span class="t">So this will be the ultimate dataset we can use to study positive transfer and to study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3393" target="_blank">00:56:33.880</a></span> | <span class="t">this joint scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3397" target="_blank">00:56:37.280</a></span> | <span class="t">And there are already evidences of positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3402" target="_blank">00:56:42.080</a></span> | <span class="t">So we pulled all the data together from all these labs and find a common action representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3410" target="_blank">00:56:50.020</a></span> | <span class="t">that we can use to train a robotic transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3413" target="_blank">00:56:53.080</a></span> | <span class="t">And we have already found this jointly trained model can outperform task-specific model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3420" target="_blank">00:57:00.200</a></span> | <span class="t">is developed in each of the lab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3422" target="_blank">00:57:02.560</a></span> | <span class="t">So there is some benefits in pulling all the data together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3425" target="_blank">00:57:05.840</a></span> | <span class="t">So scaling robot data is also quite important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3432" target="_blank">00:57:12.560</a></span> | <span class="t">So the summary for this part is that we are having a model consolidation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3436" target="_blank">00:57:16.640</a></span> | <span class="t">We can now do the high-level reasoning and low-level control in one model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3441" target="_blank">00:57:21.240</a></span> | <span class="t">And the low-level control part is what excites me because it's so far away from the traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3446" target="_blank">00:57:26.720</a></span> | <span class="t">language model domain, it's so different and it shows signs of life that we can trickle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3453" target="_blank">00:57:33.640</a></span> | <span class="t">down a lot more than we used to think it's possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3457" target="_blank">00:57:37.880</a></span> | <span class="t">And we can scale the pre-training of vision language models as well as scaling robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3461" target="_blank">00:57:41.720</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3462" target="_blank">00:57:42.720</a></span> | <span class="t">And we observe more and more positive transfer model benefiting from diverse joint training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3467" target="_blank">00:57:47.560</a></span> | <span class="t">across internet-scale language, vision, and vision language domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3472" target="_blank">00:57:52.200</a></span> | <span class="t">All right, so I noticed that we are close to running out of time, so I will just very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3480" target="_blank">00:58:00.120</a></span> | <span class="t">quickly go through the second part, which I think is also interesting, is to find new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3484" target="_blank">00:58:04.680</a></span> | <span class="t">interfaces of language models, but I would only talk at a very high level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3490" target="_blank">00:58:10.040</a></span> | <span class="t">So language models, as we can see, can directly output action tokens if we found action representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3495" target="_blank">00:58:15.800</a></span> | <span class="t">So we can treat action as yet another language to the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3500" target="_blank">00:58:20.240</a></span> | <span class="t">So language model can do translation, so it should be able to generate action as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3503" target="_blank">00:58:23.880</a></span> | <span class="t">But that requires fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3505" target="_blank">00:58:25.540</a></span> | <span class="t">Can we do it without fine-tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3507" target="_blank">00:58:27.920</a></span> | <span class="t">Or can we generate more expressive actions that is beyond the scope of fine-tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3514" target="_blank">00:58:34.640</a></span> | <span class="t">So that is about finding the right interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3518" target="_blank">00:58:38.040</a></span> | <span class="t">So previously, we have already established that language model doesn't have an action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3522" target="_blank">00:58:42.880</a></span> | <span class="t">interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3523" target="_blank">00:58:43.880</a></span> | <span class="t">If it has an action interface, it's not as effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3528" target="_blank">00:58:48.480</a></span> | <span class="t">So what is the best interface between language and the low-level actions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3531" target="_blank">00:58:51.840</a></span> | <span class="t">I would argue the best interface between language model and the low-level actions is reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3540" target="_blank">00:59:00.400</a></span> | <span class="t">functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3541" target="_blank">00:59:01.400</a></span> | <span class="t">And reward functions is universal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3544" target="_blank">00:59:04.240</a></span> | <span class="t">It has been used in reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3546" target="_blank">00:59:06.500</a></span> | <span class="t">And it's also a reparameterization of actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3551" target="_blank">00:59:11.000</a></span> | <span class="t">What is action?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3552" target="_blank">00:59:12.000</a></span> | <span class="t">Let's see if I want to pick up this bottle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3555" target="_blank">00:59:15.680</a></span> | <span class="t">And I can say, well, what is a skill?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3557" target="_blank">00:59:17.720</a></span> | <span class="t">A skill is a mapping between my observation and my action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3561" target="_blank">00:59:21.480</a></span> | <span class="t">So the mapping between my observation and action can be seen as a skill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3565" target="_blank">00:59:25.360</a></span> | <span class="t">But a skill can have an alternative definition, which is a set of constraints and a set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3570" target="_blank">00:59:30.240</a></span> | <span class="t">objectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3571" target="_blank">00:59:31.500</a></span> | <span class="t">So picking up the bottle means the bottle is in my right hand, and the bottle is off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3577" target="_blank">00:59:37.640</a></span> | <span class="t">a supporting surface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3579" target="_blank">00:59:39.080</a></span> | <span class="t">That means picking up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3580" target="_blank">00:59:40.180</a></span> | <span class="t">And how do I pick it up doesn't really matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3582" target="_blank">00:59:42.960</a></span> | <span class="t">That's a more, to its broader sense, a definition of skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3587" target="_blank">00:59:47.120</a></span> | <span class="t">It's more transferable between different skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3591" target="_blank">00:59:51.600</a></span> | <span class="t">And the constraints and objectives can be represented as rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3597" target="_blank">00:59:57.560</a></span> | <span class="t">So we can ask language model to generate these reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3602" target="_blank">01:00:02.480</a></span> | <span class="t">And then there is an optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3604" target="_blank">01:00:04.360</a></span> | <span class="t">It could be reinforcement learning, or it could be model predictive control that optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3609" target="_blank">01:00:09.520</a></span> | <span class="t">for those rewards and then run it on the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3614" target="_blank">01:00:14.600</a></span> | <span class="t">So what is in the reward translator?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3616" target="_blank">01:00:16.960</a></span> | <span class="t">Let's open a box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3619" target="_blank">01:00:19.560</a></span> | <span class="t">So the reward translator basically is a two-stage process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3623" target="_blank">01:00:23.360</a></span> | <span class="t">It's using the same language model, and it is using two different prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3628" target="_blank">01:00:28.160</a></span> | <span class="t">So the motion description basically describes the motion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3632" target="_blank">01:00:32.360</a></span> | <span class="t">So just now we found that the language model can output a description of how a robot dog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3638" target="_blank">01:00:38.640</a></span> | <span class="t">should stand up, but it's not able to achieve that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3642" target="_blank">01:00:42.240</a></span> | <span class="t">But the motion description is still sensible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3644" target="_blank">01:00:44.400</a></span> | <span class="t">It still makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3645" target="_blank">01:00:45.400</a></span> | <span class="t">It gives you the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3646" target="_blank">01:00:46.820</a></span> | <span class="t">So we just generate this motion description, and then we have a reward translator, reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3652" target="_blank">01:00:52.600</a></span> | <span class="t">coder that translates this motion description into a piece of code that is representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3660" target="_blank">01:01:00.440</a></span> | <span class="t">reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3662" target="_blank">01:01:02.640</a></span> | <span class="t">And these reward functions cannot be directly executed on the robot, but it can go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3668" target="_blank">01:01:08.520</a></span> | <span class="t">our optimization process to learn how to achieve those reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3673" target="_blank">01:01:13.480</a></span> | <span class="t">So we're using reward as the interface between language model and a low-level controller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3680" target="_blank">01:01:20.020</a></span> | <span class="t">And for the low-level controller, we're using Mojoco MPC, which is a model predictive control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3686" target="_blank">01:01:26.140</a></span> | <span class="t">algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3687" target="_blank">01:01:27.140</a></span> | <span class="t">It's basically a black box controller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3689" target="_blank">01:01:29.660</a></span> | <span class="t">It samples a lot of trajectories and finds one that optimizes your reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3696" target="_blank">01:01:36.180</a></span> | <span class="t">And we tested on a robot dog, a quadruped robot essentially, and a dextrose manipulator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3701" target="_blank">01:01:41.540</a></span> | <span class="t">So the dextrose manipulator has an arm of six or seven degrees of freedom and a hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3709" target="_blank">01:01:49.380</a></span> | <span class="t">It's impossible to control it because it has so many degrees of freedom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3712" target="_blank">01:01:52.400</a></span> | <span class="t">So it's highly challenging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3716" target="_blank">01:01:56.600</a></span> | <span class="t">So just to showcase some of the examples, I omitted the motion description part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3722" target="_blank">01:02:02.060</a></span> | <span class="t">I only output the reward code part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3727" target="_blank">01:02:07.380</a></span> | <span class="t">So it seems that the language model is able to generate the right reward functions to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3733" target="_blank">01:02:13.340</a></span> | <span class="t">make the robot stand up on two back feet like a human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3738" target="_blank">01:02:18.140</a></span> | <span class="t">And then now we are a little bit more ambitious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3740" target="_blank">01:02:20.380</a></span> | <span class="t">We know it can stand up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3742" target="_blank">01:02:22.020</a></span> | <span class="t">Can we make the robot do a moonwalk while standing up like this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3745" target="_blank">01:02:25.380</a></span> | <span class="t">So a moonwalk is from Michael Jackson, and it's very challenging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3748" target="_blank">01:02:28.540</a></span> | <span class="t">How do we make the robot to do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3749" target="_blank">01:02:29.980</a></span> | <span class="t">So it generates the motion description and generates the reward code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3755" target="_blank">01:02:35.260</a></span> | <span class="t">But the motion is not so correct, not exactly what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3761" target="_blank">01:02:41.060</a></span> | <span class="t">The nice thing about using a language model and using the reward function is that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3764" target="_blank">01:02:44.500</a></span> | <span class="t">can coach the robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3765" target="_blank">01:02:45.820</a></span> | <span class="t">You can go back and explain what went wrong and ask the language model to fix it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3771" target="_blank">01:02:51.300</a></span> | <span class="t">So now we can actually say you're being very patient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3774" target="_blank">01:02:54.540</a></span> | <span class="t">You say moonwalk means the robot should walk backward while the feet swing as if they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3779" target="_blank">01:02:59.940</a></span> | <span class="t">moving forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3782" target="_blank">01:03:02.860</a></span> | <span class="t">Such a great explanation, kudos to my colleague, and correct your answer and also make it walk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3788" target="_blank">01:03:08.540</a></span> | <span class="t">at a speed of 0.5 meters per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3791" target="_blank">01:03:11.380</a></span> | <span class="t">And after you being very patient and give it the right instruction, it's able to modify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3796" target="_blank">01:03:16.740</a></span> | <span class="t">the motion descriptor and also generate the right set of rewards to make this happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3802" target="_blank">01:03:22.940</a></span> | <span class="t">And now you can teach a robot to do a moonwalk just by using the language as an interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3809" target="_blank">01:03:29.980</a></span> | <span class="t">And one day we'll be able to do this on the real robot as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3812" target="_blank">01:03:32.900</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3813" target="_blank">01:03:33.900</a></span> | <span class="t">So in the previous section, we showed how the language model did calculate numbers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3819" target="_blank">01:03:39.820</a></span> | <span class="t">you're constraining them to also just take numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3822" target="_blank">01:03:42.140</a></span> | <span class="t">Here, how do you prevent it from just hallucinating in some program?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3826" target="_blank">01:03:46.420</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3827" target="_blank">01:03:47.420</a></span> | <span class="t">So that's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3829" target="_blank">01:03:49.940</a></span> | <span class="t">In this work, we are not preventing it to do hallucination in a programmatic way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3836" target="_blank">01:03:56.580</a></span> | <span class="t">We have a set of system prompts or a set of rules that is explaining the API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3842" target="_blank">01:04:02.820</a></span> | <span class="t">After all, the reward functions need to be able to be compiled by the optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3851" target="_blank">01:04:11.860</a></span> | <span class="t">We do need to have some check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3854" target="_blank">01:04:14.100</a></span> | <span class="t">What's more, if it doesn't compile, we can just give the error message to the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3857" target="_blank">01:04:17.900</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3858" target="_blank">01:04:18.900</a></span> | <span class="t">It doesn't have to propagate all the way to the motion descriptor, it can stay at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3862" target="_blank">01:04:22.300</a></span> | <span class="t">reward decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3863" target="_blank">01:04:23.300</a></span> | <span class="t">If there are errors, please fix it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3865" target="_blank">01:04:25.140</a></span> | <span class="t">After that, it should be able to fix it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3868" target="_blank">01:04:28.880</a></span> | <span class="t">We can also chain multiple tasks together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3872" target="_blank">01:04:32.100</a></span> | <span class="t">Using this framework, we can say, open a drawer, take the apple, put it into the drawer, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3878" target="_blank">01:04:38.540</a></span> | <span class="t">close the drawer, and it will be able to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3882" target="_blank">01:04:42.020</a></span> | <span class="t">So we tried that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3883" target="_blank">01:04:43.740</a></span> | <span class="t">Just using reward decoder is not good enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3886" target="_blank">01:04:46.100</a></span> | <span class="t">It's rather our two-stage prompt is really, really helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3891" target="_blank">01:04:51.100</a></span> | <span class="t">I think that's another inspiration for other fields, like when your domain is too different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3896" target="_blank">01:04:56.380</a></span> | <span class="t">from language domain, maybe it would be good to find an intermediate representation and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3900" target="_blank">01:05:00.700</a></span> | <span class="t">ask the language model to explain in that intermediate representation before directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3905" target="_blank">01:05:05.000</a></span> | <span class="t">go to a more obscure representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3907" target="_blank">01:05:07.620</a></span> | <span class="t">Finally, we want to transfer this to the real world, but there is a challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3914" target="_blank">01:05:14.220</a></span> | <span class="t">Using simulation, it might generate actions that are too dexterous, like this thing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3921" target="_blank">01:05:21.580</a></span> | <span class="t">not possible to do in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3923" target="_blank">01:05:23.780</a></span> | <span class="t">So we add a few more regularizer terms to stabilize the motion, and we also run some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3930" target="_blank">01:05:30.220</a></span> | <span class="t">state estimation on the real robots so that they understand where is the cubes, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3937" target="_blank">01:05:37.020</a></span> | <span class="t">we can, in the simulation, grab the motion and then achieve it in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3941" target="_blank">01:05:41.660</a></span> | <span class="t">So here are some of the execution in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3945" target="_blank">01:05:45.100</a></span> | <span class="t">So you can, say, pick up the Rubik's cube, and it will generate the motion to pick up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3950" target="_blank">01:05:50.580</a></span> | <span class="t">the Rubik's cube and grab it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3952" target="_blank">01:05:52.300</a></span> | <span class="t">This is quite different from RT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3953" target="_blank">01:05:53.700</a></span> | <span class="t">The motions are quite smooth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3957" target="_blank">01:05:57.580</a></span> | <span class="t">It's quite fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3958" target="_blank">01:05:58.580</a></span> | <span class="t">It's much faster than 3 hertz.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3962" target="_blank">01:06:02.100</a></span> | <span class="t">So here, it can do 10 hertz or even 30 hertz.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3968" target="_blank">01:06:08.100</a></span> | <span class="t">So it's comparable with human beings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3973" target="_blank">01:06:13.360</a></span> | <span class="t">So that's a language Q reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3975" target="_blank">01:06:15.420</a></span> | <span class="t">There's one last thing that I want to talk about in terms of finding a new interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3980" target="_blank">01:06:20.580</a></span> | <span class="t">So a lot of time, we have been thinking about language model as a semantic engine, a semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3985" target="_blank">01:06:25.860</a></span> | <span class="t">machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3986" target="_blank">01:06:26.860</a></span> | <span class="t">It understands semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3987" target="_blank">01:06:27.860</a></span> | <span class="t">So, for example, you say the student takes out the book.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3993" target="_blank">01:06:33.340</a></span> | <span class="t">You will say book.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3994" target="_blank">01:06:34.340</a></span> | <span class="t">Language model is able to reason about such a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=3997" target="_blank">01:06:37.780</a></span> | <span class="t">But if you do low-level patterns, like if you just give it obscure numbers, what can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4002" target="_blank">01:06:42.700</a></span> | <span class="t">you do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4003" target="_blank">01:06:43.700</a></span> | <span class="t">It's actually a low-level interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4005" target="_blank">01:06:45.540</a></span> | <span class="t">And we can open up the low-level interface to alpha language model and ask it to do robotics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4011" target="_blank">01:06:51.380</a></span> | <span class="t">tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4012" target="_blank">01:06:52.380</a></span> | <span class="t">So in this paper, "Large Language Model as General Pattern Machines," we explore using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4016" target="_blank">01:06:56.980</a></span> | <span class="t">the low-level interface of a large language model, essentially asking it to reason about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4022" target="_blank">01:07:02.700</a></span> | <span class="t">different sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4023" target="_blank">01:07:03.860</a></span> | <span class="t">And it's surprisingly quite effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4026" target="_blank">01:07:06.020</a></span> | <span class="t">And it can solve tasks like the ARC challenge and the PCFG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4031" target="_blank">01:07:11.940</a></span> | <span class="t">And it can even do sequence improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4034" target="_blank">01:07:14.020</a></span> | <span class="t">So I will dig a little bit into sequence improvement because that's quite relevant to robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4039" target="_blank">01:07:19.180</a></span> | <span class="t">So sequence improvement is that you prompt the language model with state, action, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4043" target="_blank">01:07:23.860</a></span> | <span class="t">the reward tuples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4045" target="_blank">01:07:25.220</a></span> | <span class="t">And you just prompt it with higher reward and see if it can generate actions that achieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4051" target="_blank">01:07:31.420</a></span> | <span class="t">the higher reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4052" target="_blank">01:07:32.820</a></span> | <span class="t">So it's doing reinforced learning or reinforced learning-like thing, but in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4058" target="_blank">01:07:38.460</a></span> | <span class="t">So this is quite amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4059" target="_blank">01:07:39.460</a></span> | <span class="t">So previously, you would need a dedicated algorithm collecting data replay buffer to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4065" target="_blank">01:07:45.060</a></span> | <span class="t">do this reinforced learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4066" target="_blank">01:07:46.940</a></span> | <span class="t">But now you can just build everything in the language model context by leveraging the low-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4070" target="_blank">01:07:50.860</a></span> | <span class="t">interface of a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4073" target="_blank">01:07:53.820</a></span> | <span class="t">And with that, we can actually do something like clicker training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4077" target="_blank">01:07:57.100</a></span> | <span class="t">So if you are not very familiar with clicker training, it's how you train a dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4082" target="_blank">01:08:02.500</a></span> | <span class="t">You can have a dog, and when it does the right thing, you give it a reward by clicking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4089" target="_blank">01:08:09.020</a></span> | <span class="t">So the clicker training is giving the agent a reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4096" target="_blank">01:08:16.380</a></span> | <span class="t">And we can now use clicker training to train robots as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4100" target="_blank">01:08:20.020</a></span> | <span class="t">So here, the robot is exploring, but I would give click when it does the right thing or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4104" target="_blank">01:08:24.860</a></span> | <span class="t">towards the right direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4106" target="_blank">01:08:26.300</a></span> | <span class="t">And over time, it will be able to push the backup chips, which is the objective of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4112" target="_blank">01:08:32.500</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4113" target="_blank">01:08:33.500</a></span> | <span class="t">So you can do this entire decision transformer-like operation, but purely in context, by just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4120" target="_blank">01:08:40.060</a></span> | <span class="t">giving a language model a bunch of patterns and ask it to figure out what is the regularity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4125" target="_blank">01:08:45.620</a></span> | <span class="t">of this sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4127" target="_blank">01:08:47.260</a></span> | <span class="t">And this way, it can generate new actions to improve the previous sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4134" target="_blank">01:08:54.980</a></span> | <span class="t">So for the language model, we can find new interfaces that are more suitable for teaching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4142" target="_blank">01:09:02.140</a></span> | <span class="t">it low-level skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4144" target="_blank">01:09:04.300</a></span> | <span class="t">Reward is a bridge of language model and low-level control, and we can fully leverage it as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4149" target="_blank">01:09:09.460</a></span> | <span class="t">universal interface, and we can optimize in real time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4156" target="_blank">01:09:16.060</a></span> | <span class="t">Sometimes it outperforms generating action directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4158" target="_blank">01:09:18.420</a></span> | <span class="t">So it really motivates to use the reward functions as interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4163" target="_blank">01:09:23.640</a></span> | <span class="t">And in the language model as a general pattern machine, we can use language model beyond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4167" target="_blank">01:09:27.180</a></span> | <span class="t">the semantic tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4168" target="_blank">01:09:28.180</a></span> | <span class="t">We can ask it to reason low-level things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4170" target="_blank">01:09:30.500</a></span> | <span class="t">And also, robotics as a domain, rich of sequence transformation and sequence completion and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4176" target="_blank">01:09:36.020</a></span> | <span class="t">sequence improvement tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4177" target="_blank">01:09:37.540</a></span> | <span class="t">So we can really study the lower-level mechanisms of language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4183" target="_blank">01:09:43.380</a></span> | <span class="t">And the key takeaway for this talk is that we are seeing more and more use of foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4191" target="_blank">01:09:51.100</a></span> | <span class="t">models, not only on the semantic reasoning side of robotics, but more on the dexterous,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4197" target="_blank">01:09:57.220</a></span> | <span class="t">on the generating actions, on the lower-level embodied intelligence side of robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4203" target="_blank">01:10:03.340</a></span> | <span class="t">And we need to rethink the scaling law of robotics and transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4207" target="_blank">01:10:07.300</a></span> | <span class="t">How do we scale it with limited amount of data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4210" target="_blank">01:10:10.260</a></span> | <span class="t">We have a new recipe for scaling robot model and data in RT2, which shows that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4214" target="_blank">01:10:14.500</a></span> | <span class="t">do more with the same data, with essentially RT1 data plus internet data, you can generalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4219" target="_blank">01:10:19.460</a></span> | <span class="t">to allow more things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4220" target="_blank">01:10:20.460</a></span> | <span class="t">And RTX shows that you can do a lot more with more data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4224" target="_blank">01:10:24.660</a></span> | <span class="t">There is also benefits to collecting more robotics data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4227" target="_blank">01:10:27.140</a></span> | <span class="t">And there is positive transfers everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4229" target="_blank">01:10:29.260</a></span> | <span class="t">And part two, in terms of new interfaces for language models, I think it's worth for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4235" target="_blank">01:10:35.160</a></span> | <span class="t">robotics field to think about developing new and lower-level interface to language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4239" target="_blank">01:10:39.780</a></span> | <span class="t">which facilitate learning low-level skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4243" target="_blank">01:10:43.300</a></span> | <span class="t">With that, I would like to conclude my talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4245" target="_blank">01:10:45.540</a></span> | <span class="t">And if you'll find it interesting, there are a lot of references for you to look into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4250" target="_blank">01:10:50.540</a></span> | <span class="t">And special thanks to my team, Google DeepMind Robotics team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4255" target="_blank">01:10:55.740</a></span> | <span class="t">So we are at the forefront of developing foundation models for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4259" target="_blank">01:10:59.900</a></span> | <span class="t">And stay tuned for more in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4261" target="_blank">01:11:01.580</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4262" target="_blank">01:11:02.580</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4263" target="_blank">01:11:03.580</a></span> | <span class="t">You mentioned that load numbers are difficult for a lot of our language models, but if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4275" target="_blank">01:11:15.760</a></span> | <span class="t">just generating the action tokens themselves, like no rocks or whatever you had in an example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4281" target="_blank">01:11:21.820</a></span> | <span class="t">why don't you just have a linear layer appended to the transformer that would just generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4288" target="_blank">01:11:28.860</a></span> | <span class="t">numbers from here that you can type in whatever you need?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4292" target="_blank">01:11:32.580</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4293" target="_blank">01:11:33.580</a></span> | <span class="t">The question is that if the large language models have difficulty understanding numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4300" target="_blank">01:11:40.140</a></span> | <span class="t">why don't we use a linear layer to output the action directly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4303" target="_blank">01:11:43.460</a></span> | <span class="t">I think language models are difficult to understand numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4307" target="_blank">01:11:47.760</a></span> | <span class="t">But sometimes we still want it to bring in knowledge from the pre-training mixture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4317" target="_blank">01:11:57.140</a></span> | <span class="t">If I have a new layer, that new layer is not present in the pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4322" target="_blank">01:12:02.260</a></span> | <span class="t">So how do I expect it to transfer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4324" target="_blank">01:12:04.460</a></span> | <span class="t">I think that's an interesting question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4326" target="_blank">01:12:06.420</a></span> | <span class="t">But at the same time, I don't necessarily think using the raw numbers is the right interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4332" target="_blank">01:12:12.300</a></span> | <span class="t">We probably could do some action representation learning to learn a representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4336" target="_blank">01:12:16.660</a></span> | <span class="t">And the language model can output that representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4339" target="_blank">01:12:19.660</a></span> | <span class="t">So we're still trying to figure out what is the right representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4344" target="_blank">01:12:24.180</a></span> | <span class="t">So among the representations that we haven't tried before, like decimal numbers, flow numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4350" target="_blank">01:12:30.300</a></span> | <span class="t">actual tokens, we find that just using numbers or actual tokens would be good enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4356" target="_blank">01:12:36.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4357" target="_blank">01:12:37.500</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4358" target="_blank">01:12:38.500</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4359" target="_blank">01:12:39.500</a></span> | <span class="t">Yeah, I think both directions are worth exploring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4381" target="_blank">01:13:01.980</a></span> | <span class="t">There are different advantages of generating action directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4386" target="_blank">01:13:06.940</a></span> | <span class="t">I think it borrows the autoregressive nature of language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4392" target="_blank">01:13:12.660</a></span> | <span class="t">And it aligns with a lot of other tasks, like visual question answering really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4398" target="_blank">01:13:18.540</a></span> | <span class="t">The limitation is that then when you are generating actions, it's heavily regularized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4403" target="_blank">01:13:23.900</a></span> | <span class="t">Can you generate dexterous actions that is so out of distribution that it's kind of difficult?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4409" target="_blank">01:13:29.380</a></span> | <span class="t">The language to reward actually brings a page of the book of traditional robotics, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4415" target="_blank">01:13:35.140</a></span> | <span class="t">optimization-based or model predictive control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4420" target="_blank">01:13:40.020</a></span> | <span class="t">And you can also take into, let's say, safety constraints more easily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4426" target="_blank">01:13:46.180</a></span> | <span class="t">It can generate more diverse actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4428" target="_blank">01:13:48.820</a></span> | <span class="t">Maybe one recipe is to generate a lot of data with the language to reward system and distill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4434" target="_blank">01:13:54.180</a></span> | <span class="t">them into a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4436" target="_blank">01:13:56.900</a></span> | <span class="t">Because then you are imbuing your large language model with all this other desirable-- the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4442" target="_blank">01:14:02.780</a></span> | <span class="t">language to reward itself, I don't know how scalable it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4447" target="_blank">01:14:07.060</a></span> | <span class="t">We're not fine-tuning language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4448" target="_blank">01:14:08.900</a></span> | <span class="t">So maybe you are limited to what-- you are at the mercy of the training data of the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4454" target="_blank">01:14:14.620</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4455" target="_blank">01:14:15.620</a></span> | <span class="t">The language model can do moonwalk because it knows what moonwalk is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4460" target="_blank">01:14:20.220</a></span> | <span class="t">It roughly knows how to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4464" target="_blank">01:14:24.180</a></span> | <span class="t">But if you want to scale to completely new things, maybe you can use the language to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4468" target="_blank">01:14:28.100</a></span> | <span class="t">reward to bootstrap your data generation and then put it into the other policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4473" target="_blank">01:14:33.460</a></span> | <span class="t">So can you tell us what's the next direction Google is pursuing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4479" target="_blank">01:14:39.940</a></span> | <span class="t">So it's like, the language is rewarded in the right direction, like scaling out of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4483" target="_blank">01:14:43.180</a></span> | <span class="t">room, out of the racks, and so on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4485" target="_blank">01:14:45.220</a></span> | <span class="t">Yeah, I think that's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4486" target="_blank">01:14:46.860</a></span> | <span class="t">So the scaling being the end of the lecture, that is a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4491" target="_blank">01:14:51.940</a></span> | <span class="t">But I'm being quite serious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4494" target="_blank">01:14:54.380</a></span> | <span class="t">It's actually a promising recipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4497" target="_blank">01:14:57.040</a></span> | <span class="t">So I think everybody is believing in the power of the scaling rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4504" target="_blank">01:15:04.860</a></span> | <span class="t">So just by giving it more data, giving it more compute, you will see interesting capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4510" target="_blank">01:15:10.740</a></span> | <span class="t">coming out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4511" target="_blank">01:15:11.740</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4512" target="_blank">01:15:12.740</a></span> | <span class="t">Yeah, I still think we don't quite have enough data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4530" target="_blank">01:15:30.360</a></span> | <span class="t">I think that's still probably the biggest bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4533" target="_blank">01:15:33.660</a></span> | <span class="t">So we are trying to find ways to do more with limited data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4538" target="_blank">01:15:38.360</a></span> | <span class="t">And we are trying to collect more data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4540" target="_blank">01:15:40.500</a></span> | <span class="t">And I think it needs some time for us to accumulate enough data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4545" target="_blank">01:15:45.200</a></span> | <span class="t">And currently, I say, we have signs of life for positive transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4550" target="_blank">01:15:50.320</a></span> | <span class="t">But in language models, people don't talk about positive transfers anymore because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4554" target="_blank">01:15:54.680</a></span> | <span class="t">so commonplace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4555" target="_blank">01:15:55.680</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4556" target="_blank">01:15:56.680</a></span> | <span class="t">You see it everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4558" target="_blank">01:15:58.680</a></span> | <span class="t">And robotics is not at that stage yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4561" target="_blank">01:16:01.840</a></span> | <span class="t">Yeah, how much has your team been thinking about safety and alignment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4566" target="_blank">01:16:06.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4567" target="_blank">01:16:07.440</a></span> | <span class="t">And are you just, right now, relying on the ethics that emerge from the large language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4573" target="_blank">01:16:13.400</a></span> | <span class="t">models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4574" target="_blank">01:16:14.400</a></span> | <span class="t">It won't tell you to kill someone to achieve that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4577" target="_blank">01:16:17.280</a></span> | <span class="t">Yeah, that's a very good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4578" target="_blank">01:16:18.800</a></span> | <span class="t">Actually, we take safety very, very seriously because all of the other domains of developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4584" target="_blank">01:16:24.520</a></span> | <span class="t">language models, it doesn't have direct impact on the physical world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4591" target="_blank">01:16:31.800</a></span> | <span class="t">But here, it could have potential harm to humans and to the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4597" target="_blank">01:16:37.600</a></span> | <span class="t">And Gary Marcus actually gave a comment previously to our work that, what if you say, bring out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4605" target="_blank">01:16:45.560</a></span> | <span class="t">a bowl, feed a cat, and put it in the dishwasher?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4607" target="_blank">01:16:47.440</a></span> | <span class="t">Well, let's put the cat in the dishwasher, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4610" target="_blank">01:16:50.040</a></span> | <span class="t">If it misunderstands, actually, it will have a catastrophic failure case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4617" target="_blank">01:16:57.720</a></span> | <span class="t">We take safety carefully by designing hardware and software safety layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4623" target="_blank">01:17:03.520</a></span> | <span class="t">And there are also some constitutional safety thing that is coming out sometime soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4631" target="_blank">01:17:11.280</a></span> | <span class="t">I cannot tell much details right now, but sometime soon, we'll release some work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4637" target="_blank">01:17:17.240</a></span> | <span class="t">Is it something like, if there's a human, just don't interact?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4641" target="_blank">01:17:21.320</a></span> | <span class="t">Well, no, no, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4643" target="_blank">01:17:23.440</a></span> | <span class="t">I think it's a little bit more nuanced and more detailed than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4647" target="_blank">01:17:27.600</a></span> | <span class="t">But we do take safety quite seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4649" target="_blank">01:17:29.880</a></span> | <span class="t">And in some of our experiments, actually, the robot's finger would break off because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4654" target="_blank">01:17:34.040</a></span> | <span class="t">it cannot apply enough force to an environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4656" target="_blank">01:17:36.280</a></span> | <span class="t">So that's just yet another way of ensuring safety.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4659" target="_blank">01:17:39.600</a></span> | <span class="t">Can we have some visual language model and a synthesizer or something to stop the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4665" target="_blank">01:17:45.600</a></span> | <span class="t">that both the internet and the robot?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4669" target="_blank">01:17:49.120</a></span> | <span class="t">And maybe, this is kind of like interpretive, but both in some logical way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4675" target="_blank">01:17:55.160</a></span> | <span class="t">Right, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4676" target="_blank">01:17:56.160</a></span> | <span class="t">So I think it would be possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4678" target="_blank">01:17:58.000</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4679" target="_blank">01:17:59.000</a></span> | <span class="t">Thank you for the great talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4686" target="_blank">01:18:06.960</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4687" target="_blank">01:18:07.960</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4688" target="_blank">01:18:08.960</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4688" target="_blank">01:18:08.960</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=fz8wf9hN20c&t=4691" target="_blank">01:18:11.020</a></span> | <span class="t">you</span></div></div></body></html>