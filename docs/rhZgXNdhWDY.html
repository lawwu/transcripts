<html><head><title>Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Retrieval Augmented Generation (RAG) Explained: Embedding, Sentence BERT, Vector Database (HNSW)</h2><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY"><img src="https://i.ytimg.com/vi/rhZgXNdhWDY/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=142">2:22</a> Language Models<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=273">4:33</a> Fine-Tuning<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=364">6:4</a> Prompt Engineering (Few-Shot)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=444">7:24</a> Prompt Engineering (QA)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=615">10:15</a> RAG pipeline (introduction)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=818">13:38</a> Embedding Vectors<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1181">19:41</a> Sentence Embedding<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1397">23:17</a> Sentence BERT<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1690">28:10</a> RAG pipeline (review)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1790">29:50</a> RAG with Gradient<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1898">31:38</a> Vector Database<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1991">33:11</a> K-NN (Naive)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2116">35:16</a> Hierarchical Navigable Small Worlds (Introduction)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2154">35:54</a> Six Degrees of Separation<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2375">39:35</a> Navigable Small Worlds<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2588">43:8</a> Skip-List<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2723">45:23</a> Hierarchical Navigable Small Worlds<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2847">47:27</a> RAG pipeline (review)<br><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2902">48:22</a> Closing<br><br><div style="text-align: left;"><a href="./rhZgXNdhWDY.html">Whisper Transcript</a> | <a href="./transcript_rhZgXNdhWDY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel. Today we are going to talk about retrieval augmented generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=5" target="_blank">00:00:05.100</a></span> | <span class="t">So as you know large language models can only answer questions or generate text based on their training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=11" target="_blank">00:00:11.400</a></span> | <span class="t">So for example, if we have a language model that was trained in 2018 and we ask it about the COVID</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=17" target="_blank">00:00:17.400</a></span> | <span class="t">probably the language model will know not know anything about the COVID and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">One of the way to augment the knowledge of language model is to fine-tune the model on the latest data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=26" target="_blank">00:00:26.480</a></span> | <span class="t">But there is another technique called retrieval augmented generation, which is very useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=31" target="_blank">00:00:31.040</a></span> | <span class="t">Especially for question answering and it has been also being used recently by Twitter to create their new language model called Grok</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=38" target="_blank">00:00:38.560</a></span> | <span class="t">So Grok can access in real time all the data from the tweets and answer questions on the latest trends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=45" target="_blank">00:00:45.340</a></span> | <span class="t">So in this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=46" target="_blank">00:00:46.320</a></span> | <span class="t">we will explore what is a retrieval augmented generation, all the pipeline, all the pieces of the pipeline and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=52" target="_blank">00:00:52.060</a></span> | <span class="t">architecture behind each of these building blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=55" target="_blank">00:00:55.200</a></span> | <span class="t">So let's review the topics of today. I will give a brief introduction to language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=61" target="_blank">00:01:01.000</a></span> | <span class="t">So what they are, how they work, how we inference them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=64" target="_blank">00:01:04.220</a></span> | <span class="t">we will then move on to the pipeline that makes up the retrieval augmented generation with particular reference to the embedding vectors and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=71" target="_blank">00:01:11.400</a></span> | <span class="t">how they are built, what they are and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=74" target="_blank">00:01:14.340</a></span> | <span class="t">We will also explore the architecture behind sentence birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=78" target="_blank">00:01:18.200</a></span> | <span class="t">Which is one of the ways to generate embeddings of sentences and then we will move on to vector databases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=84" target="_blank">00:01:24.640</a></span> | <span class="t">What they are, how we use them and how the algorithm of a vector database works in finding a particular vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=91" target="_blank">00:01:31.760</a></span> | <span class="t">That we are looking similar to a query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=94" target="_blank">00:01:34.180</a></span> | <span class="t">What I expect you guys to already know before watching this video is for sure you are a little too familiar with the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=101" target="_blank">00:01:41.280</a></span> | <span class="t">We will not be going so much in detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=104" target="_blank">00:01:44.400</a></span> | <span class="t">But at least you're familiar with the basic building blocks of a transformer and that you have watched my previous video about birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=110" target="_blank">00:01:50.680</a></span> | <span class="t">So if you're not familiar with these topics, please I recommend you watch my previous video on the transformer and my previous video on birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=117" target="_blank">00:01:57.000</a></span> | <span class="t">They will give you all the necessary background to fully understand the current video. Now, let's start our journey</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=123" target="_blank">00:02:03.080</a></span> | <span class="t">We will start by introducing first of all my cat Oleo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=127" target="_blank">00:02:07.580</a></span> | <span class="t">Because I will be using him for a lot of the examples in my video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=132" target="_blank">00:02:12.480</a></span> | <span class="t">So if you are not Chinese, you don't know how to read his name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=135" target="_blank">00:02:15.640</a></span> | <span class="t">Which is Oleo which stands for the biscuits Oreo because he's black and white</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=140" target="_blank">00:02:20.400</a></span> | <span class="t">So let's get started. The first thing we will be talking about is language models. Now, what is a language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=147" target="_blank">00:02:27.740</a></span> | <span class="t">Well, a language model is a probabilistic model that assigns probabilities to sequence of words. In practice a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=155" target="_blank">00:02:35.440</a></span> | <span class="t">Allows us to compute the following. What is the probability that the word China comes after in the sentence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=163" target="_blank">00:02:43.480</a></span> | <span class="t">Shanghai is a city in. So the language model allow us to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=168" target="_blank">00:02:48.000</a></span> | <span class="t">model the probability of the next token given the prompt and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=172" target="_blank">00:02:52.800</a></span> | <span class="t">We usually train a neural network to predict these probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=177" target="_blank">00:02:57.420</a></span> | <span class="t">A neural network that has been trained on a very large corpora of text is known as a large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=183" target="_blank">00:03:03.800</a></span> | <span class="t">When we train large language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=186" target="_blank">00:03:06.800</a></span> | <span class="t">We usually have a very big corpora of text that is made of, which is a kind of a collection of documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=192" target="_blank">00:03:12.920</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=193" target="_blank">00:03:13.920</a></span> | <span class="t">Often language models are trained on the entire Wikipedia or millions of web pages or even thousands of books because we wanted the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=201" target="_blank">00:03:21.740</a></span> | <span class="t">Model to acquire as much knowledge as possible and we usually use a transformer based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=208" target="_blank">00:03:28.180</a></span> | <span class="t">Architecture to create a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=211" target="_blank">00:03:31.400</a></span> | <span class="t">For example llama is usually known as a decoder only network and bird is usually known as an encoder only network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=221" target="_blank">00:03:41.440</a></span> | <span class="t">Because llama has the last part of llama is the basically the decoder of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=226" target="_blank">00:03:46.960</a></span> | <span class="t">Transformer without the cross attention plus a linear layer plus a softmax while bird using only the encoder side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=234" target="_blank">00:03:54.160</a></span> | <span class="t">And then it has some heads that can be a linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=237" target="_blank">00:03:57.160</a></span> | <span class="t">Depending on the task it is we are using it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=240" target="_blank">00:04:00.280</a></span> | <span class="t">to inference a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=242" target="_blank">00:04:02.840</a></span> | <span class="t">we usually build a prompt and then we ask the language model to continue this prompt with by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=248" target="_blank">00:04:08.440</a></span> | <span class="t">Iteratively adding tokens such that the tokens that the language model adds make sense in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=254" target="_blank">00:04:14.640</a></span> | <span class="t">Continuity with the prompt. So for example here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=258" target="_blank">00:04:18.840</a></span> | <span class="t">I'm asking chatgpd to come to continue a joke and chatgpd continues the joke by adding a few more tokens that when read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=266" target="_blank">00:04:26.280</a></span> | <span class="t">In their entirety they make sense. They are coherent with what I actually wrote</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=273" target="_blank">00:04:33.440</a></span> | <span class="t">You are what you eat, so a language model can only output text and information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=279" target="_blank">00:04:39.280</a></span> | <span class="t">It was trained upon this means that if we train a language model only on English content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=284" target="_blank">00:04:44.480</a></span> | <span class="t">Very probably it will not be able to output Japanese or French</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=288" target="_blank">00:04:48.480</a></span> | <span class="t">to teach new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=290" target="_blank">00:04:50.600</a></span> | <span class="t">Concepts to new content new information to a language model. We need to fine-tune the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=296" target="_blank">00:04:56.040</a></span> | <span class="t">However fine-tune fine-tuning is has some cons for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=301" target="_blank">00:05:01.160</a></span> | <span class="t">It can be expensive in term of computation power necessary to fine-tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=305" target="_blank">00:05:05.280</a></span> | <span class="t">The number of parameters of the model may not be sufficient to capture all the knowledge that we want to teach the the model itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=312" target="_blank">00:05:12.920</a></span> | <span class="t">So for example llama was introduced with the 7 billion 13 billion and the 70 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=319" target="_blank">00:05:19.040</a></span> | <span class="t">Why because with 7 billion parameters you it can capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=324" target="_blank">00:05:24.200</a></span> | <span class="t">Some some knowledge, but not as much as the 70 billion parameters model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=329" target="_blank">00:05:29.320</a></span> | <span class="t">So the number of parameters is a limitation on the amount of knowledge the language model can acquire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=335" target="_blank">00:05:35.200</a></span> | <span class="t">Also fine-tuning is not additive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=338" target="_blank">00:05:38.560</a></span> | <span class="t">For example if you have a model that has trained on English content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=343" target="_blank">00:05:43.440</a></span> | <span class="t">And then you heavily fine-tune it on Japanese content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=346" target="_blank">00:05:46.660</a></span> | <span class="t">The model will not be at the end of the fine-tuning will not be as proficient in English and as in Japanese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=353" target="_blank">00:05:53.520</a></span> | <span class="t">But it may forget some of the English content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=357" target="_blank">00:05:57.040</a></span> | <span class="t">It was initially trained upon so is that we say that the fine-tuning is not additive to the knowledge of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=363" target="_blank">00:06:03.920</a></span> | <span class="t">Of course we can compensate for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=368" target="_blank">00:06:08.680</a></span> | <span class="t">for the fine-tuning with the prompt engineering for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=372" target="_blank">00:06:12.760</a></span> | <span class="t">It is possible to ask the language model to perform a new task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=376" target="_blank">00:06:16.360</a></span> | <span class="t">Task that it was not specifically trained upon by working with the prompt for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=382" target="_blank">00:06:22.400</a></span> | <span class="t">This is a few short prompting technique and the following is an example so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=387" target="_blank">00:06:27.080</a></span> | <span class="t">We first give an instruction to the language model here is the instruction on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=392" target="_blank">00:06:32.840</a></span> | <span class="t">What is the task the language model is going to perform?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=396" target="_blank">00:06:36.160</a></span> | <span class="t">Then we give the language model some example to how to perform this task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=400" target="_blank">00:06:40.560</a></span> | <span class="t">And then we ask the language model to perform it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=403" target="_blank">00:06:43.680</a></span> | <span class="t">For example the task here is Oleo is a cat that likes to play tricks on his friend Umar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=409" target="_blank">00:06:49.400</a></span> | <span class="t">By replacing all the names in everything he writes with meow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=412" target="_blank">00:06:52.720</a></span> | <span class="t">For example Umar writes Bob runs a YouTube channel and Oleo will modify it to meow runs a YouTube channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=419" target="_blank">00:06:59.880</a></span> | <span class="t">So what happens if Umar writes Alice likes to play with his friend Bob</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=424" target="_blank">00:07:04.720</a></span> | <span class="t">How would Oleo modify it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=426" target="_blank">00:07:06.720</a></span> | <span class="t">The language model comes up with the right answer, which is meow likes to play with his friend meow so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=433" target="_blank">00:07:13.200</a></span> | <span class="t">JGPT in this case was not trained on performing this task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=436" target="_blank">00:07:16.960</a></span> | <span class="t">But by looking at the prompt and the example that we provided it it was able to come up with the right solution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=443" target="_blank">00:07:23.800</a></span> | <span class="t">And we can use prompt engineering also for question answering with the same kind of reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=450" target="_blank">00:07:30.160</a></span> | <span class="t">So we build a very big prompt that includes an instruction part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=454" target="_blank">00:07:34.400</a></span> | <span class="t">So you are an assistant trained to answer questions using the given context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=459" target="_blank">00:07:39.680</a></span> | <span class="t">In which we provide some context which is a piece of text in which to retrieve the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=465" target="_blank">00:07:45.160</a></span> | <span class="t">And then we ask the question to the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=467" target="_blank">00:07:47.880</a></span> | <span class="t">How many parameters are there in grok zero so grok is the language model that is introduced by Twitter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=473" target="_blank">00:07:53.960</a></span> | <span class="t">And it's a language model that can also access the latest tweets, and we will see later how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=480" target="_blank">00:08:00.560</a></span> | <span class="t">But the point is I am asking the language model so JGPT to tell me about grok zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=487" target="_blank">00:08:07.120</a></span> | <span class="t">so very probably JGPT was not trained on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=490" target="_blank">00:08:10.160</a></span> | <span class="t">It doesn't know about the existence of this grok zero because it came out very recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=495" target="_blank">00:08:15.000</a></span> | <span class="t">So the model JGPT was able to retrieve the answer by looking at the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=501" target="_blank">00:08:21.240</a></span> | <span class="t">So in this case, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=503" target="_blank">00:08:23.440</a></span> | <span class="t">It says grok zero the prototype LLM mentioned in the provided context is stated to have been trained with 33 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=511" target="_blank">00:08:31.440</a></span> | <span class="t">Because the JGPT was able to access the context in which we talk about the how many parameters there are in grok zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=519" target="_blank">00:08:39.720</a></span> | <span class="t">Which is this line after announcing XAIV prototype we train the prototype LLM with 33 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=525" target="_blank">00:08:45.720</a></span> | <span class="t">so the answer is correct and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=528" target="_blank">00:08:48.040</a></span> | <span class="t">This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=531" target="_blank">00:08:51.160</a></span> | <span class="t">this kind of way of working with the prompt is actually very powerful, but also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=537" target="_blank">00:08:57.520</a></span> | <span class="t">Fine-tuning is not necessarily wrong or the wrong way to deal with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=541" target="_blank">00:09:01.800</a></span> | <span class="t">Lack of knowledge problem in the language models because it usually when we fine-tune a model on a specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=549" target="_blank">00:09:09.560</a></span> | <span class="t">Particular content it results in a higher quality results compared to just prompt engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=555" target="_blank">00:09:15.800</a></span> | <span class="t">And also as you saw before to ask a question to JGPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=561" target="_blank">00:09:21.480</a></span> | <span class="t">We had to build a very big prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=563" target="_blank">00:09:23.840</a></span> | <span class="t">So it means that the number of tokens that we are giving to the model is quite big the more we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=569" target="_blank">00:09:29.560</a></span> | <span class="t">the bigger the context that we need to provide to answer a particular question and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=574" target="_blank">00:09:34.300</a></span> | <span class="t">We know that the more context we give the more information the language model will have to come up with the right answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=581" target="_blank">00:09:41.360</a></span> | <span class="t">So usually we need a bigger context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=583" target="_blank">00:09:43.760</a></span> | <span class="t">But the problem is bigger context is also computationally expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=589" target="_blank">00:09:49.360</a></span> | <span class="t">So by fine-tuning actually we can reduce this content size because we don't need to provide the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=594" target="_blank">00:09:54.840</a></span> | <span class="t">Anymore because we are fine-tuning the language model on the specific data on which we will ask questions for so to a language model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=602" target="_blank">00:10:02.400</a></span> | <span class="t">Has been fine-tuned. We just need to ask the question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=605" target="_blank">00:10:05.200</a></span> | <span class="t">So how many parameters are there in grok0 without providing all the context and if the language model has been fine-tuned correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=611" target="_blank">00:10:11.440</a></span> | <span class="t">It will be able to come up with the right answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=615" target="_blank">00:10:15.040</a></span> | <span class="t">Now we need to introduce the retrieval augmented generation pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=620" target="_blank">00:10:20.080</a></span> | <span class="t">Because that's our next step and we will explore each building block that makes up the pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=625" target="_blank">00:10:25.440</a></span> | <span class="t">So imagine that we want to do question answering with the retrieval augmented generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=630" target="_blank">00:10:30.200</a></span> | <span class="t">Compared to what we did before before we did it with the prompt engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=633" target="_blank">00:10:33.840</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=636" target="_blank">00:10:36.320</a></span> | <span class="t">how many parameters are there in grok0 this is our query and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=642" target="_blank">00:10:42.000</a></span> | <span class="t">Imagine we also have some documents in which we can find this answer these documents may be some pdf documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=648" target="_blank">00:10:48.160</a></span> | <span class="t">But they may also be web pages from which we can retrieve this answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=651" target="_blank">00:10:51.920</a></span> | <span class="t">What we do is we split all these documents or pieces of text into chunks of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=658" target="_blank">00:10:58.880</a></span> | <span class="t">so small pieces of text for example a document may be made up of many pages and each page may be made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=664" target="_blank">00:11:04.600</a></span> | <span class="t">Paragraphs and each paragraph is made up of sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=668" target="_blank">00:11:08.000</a></span> | <span class="t">In the usually we split each of these documents into small sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=672" target="_blank">00:11:12.980</a></span> | <span class="t">And also the web pages are split in the same way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=677" target="_blank">00:11:17.120</a></span> | <span class="t">We create embeddings of these sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=681" target="_blank">00:11:21.220</a></span> | <span class="t">Such that each embedding is a vector of a fixed size that captures the meaning of each sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=688" target="_blank">00:11:28.720</a></span> | <span class="t">Then we store all of these embeddings into a vector database and later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=694" target="_blank">00:11:34.160</a></span> | <span class="t">We will see all these embeddings and vector database how they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=697" target="_blank">00:11:37.520</a></span> | <span class="t">Then we take also the query which is a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=701" target="_blank">00:11:41.120</a></span> | <span class="t">We convert it into an embedding using the same model that we have used to convert the documents into embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=708" target="_blank">00:11:48.340</a></span> | <span class="t">We search this embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=711" target="_blank">00:11:51.360</a></span> | <span class="t">So this query embedding into our database which already have many embeddings each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=716" target="_blank">00:11:56.380</a></span> | <span class="t">Representing a sentence from our document and it will come up with some results with the best matching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=722" target="_blank">00:12:02.220</a></span> | <span class="t">Embeddings for our particular query and each embedding is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=726" target="_blank">00:12:06.380</a></span> | <span class="t">associated with the piece of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=729" target="_blank">00:12:09.240</a></span> | <span class="t">It comes from so the vector database is also able to retrieve the original text from which that embedding was created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=736" target="_blank">00:12:16.540</a></span> | <span class="t">So if we are for example, how many parameters are there in grok zero?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=741" target="_blank">00:12:21.100</a></span> | <span class="t">The vector database will search all of its embedding and will give us the embeddings that best match our query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=747" target="_blank">00:12:27.420</a></span> | <span class="t">So probably it will look for all the piece of text that talk about grok zero and the parameters it contains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=754" target="_blank">00:12:34.460</a></span> | <span class="t">Now that we have the context and the query we create a template for a prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=761" target="_blank">00:12:41.340</a></span> | <span class="t">So just like the the prompt we have used before so you are an assistant trained to answer questions using the given context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=768" target="_blank">00:12:48.940</a></span> | <span class="t">We paste the context and the query inside of the prompt template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=774" target="_blank">00:12:54.000</a></span> | <span class="t">And just like before we feed it to the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=777" target="_blank">00:12:57.820</a></span> | <span class="t">And then the language model will be able to answer our question by using the context provided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=783" target="_blank">00:13:03.520</a></span> | <span class="t">So we are with the retrieval augmented generation. We are not fine-tuning a model to answer the questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=789" target="_blank">00:13:09.980</a></span> | <span class="t">We are actually using a prompt engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=793" target="_blank">00:13:13.980</a></span> | <span class="t">But we are introducing a database here called the vector database that can access the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=799" target="_blank">00:13:19.420</a></span> | <span class="t">Context given our query so it can retrieve the context necessary to answer our particular question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=806" target="_blank">00:13:26.620</a></span> | <span class="t">Feed it to the language model and then the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=809" target="_blank">00:13:29.980</a></span> | <span class="t">Using the context and our question will be able to come up with the right answer very probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=815" target="_blank">00:13:35.900</a></span> | <span class="t">Now let's talk about embedding vectors so what they are and how we work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=823" target="_blank">00:13:43.660</a></span> | <span class="t">Okay, first of all, why do we use vectors to represent words for example given the words cherry digital and information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=832" target="_blank">00:13:52.140</a></span> | <span class="t">If we represent embedding vectors using only two dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=835" target="_blank">00:13:55.900</a></span> | <span class="t">So as you remember in the vanilla transformer each embedding vector is 512 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=841" target="_blank">00:14:01.100</a></span> | <span class="t">But imagine we are in a simpler world. We only have two dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=844" target="_blank">00:14:04.540</a></span> | <span class="t">So we can plot these embedding vectors on a xy plane</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=849" target="_blank">00:14:09.580</a></span> | <span class="t">And what we do is we hope to see something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=853" target="_blank">00:14:13.420</a></span> | <span class="t">So that words with similar meaning or words that represent the same concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=859" target="_blank">00:14:19.100</a></span> | <span class="t">Point in the same direction in space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=861" target="_blank">00:14:21.980</a></span> | <span class="t">So for example the word digital and the word information are pointing to the same direction in space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=867" target="_blank">00:14:27.820</a></span> | <span class="t">Such that the angle between words that have a similar meaning is small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=873" target="_blank">00:14:33.500</a></span> | <span class="t">So the angle between digital and information is small and the angle between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=879" target="_blank">00:14:39.020</a></span> | <span class="t">Words that have a different meaning is bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=881" target="_blank">00:14:41.660</a></span> | <span class="t">So for example the word cherry and digital have an angle that is bigger compared to digital and information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=887" target="_blank">00:14:47.760</a></span> | <span class="t">Indicating that they represent different concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=890" target="_blank">00:14:50.400</a></span> | <span class="t">Imagine we have another word called tomato</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=893" target="_blank">00:14:53.420</a></span> | <span class="t">We expect it to point to this vertical direction here such that the angle between cherry and tomato should be small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=899" target="_blank">00:14:59.820</a></span> | <span class="t">How do we measure this angle?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=902" target="_blank">00:15:02.860</a></span> | <span class="t">We usually use the cosine similarity to measure the angle between vectors and later we will see the formula of the cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=910" target="_blank">00:15:10.480</a></span> | <span class="t">Now, how did we come up with the idea of representing words as embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=917" target="_blank">00:15:17.280</a></span> | <span class="t">The first idea is that words that are synonyms tend to occur in the same context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=923" target="_blank">00:15:23.820</a></span> | <span class="t">So surrounded by the same words, for example, the word teacher and the professor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=929" target="_blank">00:15:29.500</a></span> | <span class="t">Usually occur by the word school, university, exam, lecture, course, etc and vice versa</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=937" target="_blank">00:15:37.100</a></span> | <span class="t">We can also say that words that occur in the same context tend to have similar meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=943" target="_blank">00:15:43.900</a></span> | <span class="t">This is known as the distributional hypothesis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=946" target="_blank">00:15:46.720</a></span> | <span class="t">This means that to capture the meaning of a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=951" target="_blank">00:15:51.260</a></span> | <span class="t">We also need to have access to its context so to the words surrounding it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=957" target="_blank">00:15:57.660</a></span> | <span class="t">But this also means that this is also the reason why we employ self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=962" target="_blank">00:16:02.160</a></span> | <span class="t">In the transformer model to capture the conceptual information of each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=967" target="_blank">00:16:07.260</a></span> | <span class="t">So as you remember the transformer model we have this self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=971" target="_blank">00:16:11.260</a></span> | <span class="t">The self-attention is a way to relate each token with all the other token in the same sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=976" target="_blank">00:16:16.060</a></span> | <span class="t">Based also on the position each token occupies in the sentence because we have the concept of positional encoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=983" target="_blank">00:16:23.180</a></span> | <span class="t">So the self-attention access two things to calculate its score of attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=988" target="_blank">00:16:28.540</a></span> | <span class="t">The first is the embedding of the word which captures its meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=992" target="_blank">00:16:32.620</a></span> | <span class="t">The second information it accesses is the positional encoding so that words that are closer to each other are related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=998" target="_blank">00:16:38.940</a></span> | <span class="t">Differently to words that are far from each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1002" target="_blank">00:16:42.140</a></span> | <span class="t">And this self-attention mechanism modifies the embedding of each word in such a way that it also captures the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1010" target="_blank">00:16:50.680</a></span> | <span class="t">Contextual information of that word and the words that surround it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1014" target="_blank">00:16:54.360</a></span> | <span class="t">We trained BERT on a very particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1020" target="_blank">00:17:00.200</a></span> | <span class="t">Task which is called the musket language model task to capture information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1024" target="_blank">00:17:04.940</a></span> | <span class="t">To create the embeddings of BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1028" target="_blank">00:17:08.040</a></span> | <span class="t">This musket language model task is based on the clothes task and we humans do it very often. Let me give you an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1036" target="_blank">00:17:16.200</a></span> | <span class="t">Imagine I give you the following sentence rome is the something something of italy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1041" target="_blank">00:17:21.160</a></span> | <span class="t">This is why which is why it hosts many government buildings. Can you tell me what is the missing word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1047" target="_blank">00:17:27.320</a></span> | <span class="t">Well, of course the missing word is capital because by looking at the rest of the sentence is the one that makes the most sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1055" target="_blank">00:17:35.800</a></span> | <span class="t">How did we come up with the word capital for the missing word? Well, we look at the words that were surrounding the blank space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1064" target="_blank">00:17:44.840</a></span> | <span class="t">so it means that to the the word capital depends on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1068" target="_blank">00:17:48.840</a></span> | <span class="t">Context in which it appears on the words that surround it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1072" target="_blank">00:17:52.920</a></span> | <span class="t">And this is how we train BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1075" target="_blank">00:17:55.960</a></span> | <span class="t">We want the self-attention mechanism to relate all the input tokens with each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1080" target="_blank">00:18:00.760</a></span> | <span class="t">So that BERT has enough information about the context of the missing word to predict it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1088" target="_blank">00:18:08.280</a></span> | <span class="t">For example, imagine we want to train BERT on the musket language model task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1092" target="_blank">00:18:12.600</a></span> | <span class="t">And we create an input with the musket word just like before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1096" target="_blank">00:18:16.280</a></span> | <span class="t">So rome is the something something of italy, which is why it hosts many government buildings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1101" target="_blank">00:18:21.560</a></span> | <span class="t">We replace the blank space with a special token called musk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1104" target="_blank">00:18:24.920</a></span> | <span class="t">This becomes the input of BERT which is made of 14 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1109" target="_blank">00:18:29.000</a></span> | <span class="t">We feed it to BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1111" target="_blank">00:18:31.880</a></span> | <span class="t">BERT is a transformer model. So it will output it's a sequence to sequence model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1116" target="_blank">00:18:36.360</a></span> | <span class="t">So if the input is 14 tokens, the output will also be 14 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1120" target="_blank">00:18:40.440</a></span> | <span class="t">We ask BERT to predict the fourth token because it's the one that has been musket out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1126" target="_blank">00:18:46.280</a></span> | <span class="t">We know what is the word, which the word is capital</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1129" target="_blank">00:18:49.560</a></span> | <span class="t">So we ask we calculate the loss based on what is the predicted fourth token and what it should be the actual fourth token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1137" target="_blank">00:18:57.080</a></span> | <span class="t">And then we back propagate the the loss to update all the weights of the model when we run back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1145" target="_blank">00:19:05.080</a></span> | <span class="t">BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1146" target="_blank">00:19:06.520</a></span> | <span class="t">The model will also update the input embeddings here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1150" target="_blank">00:19:10.360</a></span> | <span class="t">So the input embeddings by the back propagation mechanism will be modified in such a way that this word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1157" target="_blank">00:19:17.800</a></span> | <span class="t">So the word capital so the embedding associated with the word capital will be modified in such a way that it captures all the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1165" target="_blank">00:19:25.100</a></span> | <span class="t">About its context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1167" target="_blank">00:19:27.080</a></span> | <span class="t">So the next time BERT will have less troubles predicting it given its context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1172" target="_blank">00:19:32.920</a></span> | <span class="t">And this is uh, actually the one of the reason why we run back propagation because we want the model to get better and better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1179" target="_blank">00:19:39.480</a></span> | <span class="t">By reducing the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1181" target="_blank">00:19:41.560</a></span> | <span class="t">What if I told you that actually we can also create embeddings not of single tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1187" target="_blank">00:19:47.400</a></span> | <span class="t">But also of entire sentences so we can use the self-attention mechanism to capture also the meaning of an entire sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1194" target="_blank">00:19:54.920</a></span> | <span class="t">What we can do is we can use a pre-trained BERT model to produce embeddings of entire sentences. Let's see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1202" target="_blank">00:20:02.760</a></span> | <span class="t">Well, suppose we have a simple input sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1205" target="_blank">00:20:05.960</a></span> | <span class="t">For example, this one made of 13 tokens called our professor always gives us lots of assignments to do in the weekend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1213" target="_blank">00:20:13.400</a></span> | <span class="t">We feed it to BERT, but notice that I removed the linear layer from BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1218" target="_blank">00:20:18.600</a></span> | <span class="t">And I will show you why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1221" target="_blank">00:20:21.080</a></span> | <span class="t">so the first thing we do is we notice is that the input of the self-attention is a matrix of shape 13 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1227" target="_blank">00:20:27.860</a></span> | <span class="t">768. Why? Because we have 13 tokens and each token is represented by an embedding with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1234" target="_blank">00:20:34.100</a></span> | <span class="t">768 dimensions which is the dimension which is the size of the embedding vector in BERT base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1241" target="_blank">00:20:41.060</a></span> | <span class="t">So the smaller BERT pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1243" target="_blank">00:20:43.220</a></span> | <span class="t">The self-attention mechanism will output another matrix with the shape 13 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1249" target="_blank">00:20:49.780</a></span> | <span class="t">768 so 13 tokens each one with its own embedding of 768 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1257" target="_blank">00:20:57.300</a></span> | <span class="t">And the output of the self-attention is an embedding that captures not only the meaning of the word or its position in the sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1264" target="_blank">00:21:04.740</a></span> | <span class="t">But also all the contextual information all the relationship between other words and the current word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1269" target="_blank">00:21:09.860</a></span> | <span class="t">So the output will be 13 tokens each one represented by an embedding of size 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1277" target="_blank">00:21:17.320</a></span> | <span class="t">What we do now each of them is representing kind of the meaning of a single word, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1284" target="_blank">00:21:24.820</a></span> | <span class="t">So what we do we can average all of them to create the embedding of the sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1289" target="_blank">00:21:29.780</a></span> | <span class="t">So we take all these vectors of size 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1292" target="_blank">00:21:32.840</a></span> | <span class="t">We calculate the average of them which will result in a single vector with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1298" target="_blank">00:21:38.100</a></span> | <span class="t">768 dimensions and this single vector will represent the sentence embedding which captures the meaning of the entire sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1305" target="_blank">00:21:45.700</a></span> | <span class="t">And this is how we create the embedding of a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1311" target="_blank">00:21:51.140</a></span> | <span class="t">Now, how can we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1313" target="_blank">00:21:53.140</a></span> | <span class="t">Sentence embeddings in to see if two sentences have similar meaning so for example, imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1319" target="_blank">00:21:59.780</a></span> | <span class="t">one sentence is talking about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1322" target="_blank">00:22:02.500</a></span> | <span class="t">The query for example before was talking about how many parameters are there in grok zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1327" target="_blank">00:22:07.140</a></span> | <span class="t">And then we have another sentence that talks about how many parameters there are in grok zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1331" target="_blank">00:22:11.460</a></span> | <span class="t">So, how can we relate these two sentences? We need to find a similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1336" target="_blank">00:22:16.320</a></span> | <span class="t">Function and what we do is we usually use a cosine similarity because they are both vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1341" target="_blank">00:22:21.620</a></span> | <span class="t">And the cosine similarity can be calculated as between vectors and it measures the cosine of the angle between the two vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1349" target="_blank">00:22:29.300</a></span> | <span class="t">A smaller angle results in a high cosine singular similarity score while a bigger angle results in a smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1356" target="_blank">00:22:36.480</a></span> | <span class="t">Cosine similarity score and this is the formula of the cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1360" target="_blank">00:22:40.560</a></span> | <span class="t">But there is a problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1363" target="_blank">00:22:43.520</a></span> | <span class="t">So nobody told BERT that the embedding it produces should be comparable with the cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1370" target="_blank">00:22:50.000</a></span> | <span class="t">So BERT is outputting some embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1372" target="_blank">00:22:52.500</a></span> | <span class="t">And then we take the average of these embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1375" target="_blank">00:22:55.440</a></span> | <span class="t">But nobody told BERT that these embeddings should be in such a way that two similar sentences should produce similar embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1383" target="_blank">00:23:03.460</a></span> | <span class="t">How can we teach BERT to produce embeddings that can be compared with a similarity function of our choice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1393" target="_blank">00:23:13.040</a></span> | <span class="t">Which could be a cosine similarity or the euclidean distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1396" target="_blank">00:23:16.420</a></span> | <span class="t">Well, we introduce sentence BERT. Sentence BERT is one of the most popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1401" target="_blank">00:23:21.760</a></span> | <span class="t">models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1404" target="_blank">00:23:24.560</a></span> | <span class="t">To produce embeddings for entire sentences that can be compared using a similarity function of our choice in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1411" target="_blank">00:23:31.600</a></span> | <span class="t">It's the cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1413" target="_blank">00:23:33.840</a></span> | <span class="t">So sentence BERT was introduced in a paper called sentence BERT sentence embeddings using Siamese BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1420" target="_blank">00:23:40.380</a></span> | <span class="t">Networks and we will see all of this. What does it mean? What is a Siamese network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1424" target="_blank">00:23:44.780</a></span> | <span class="t">Now imagine we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1427" target="_blank">00:23:47.340</a></span> | <span class="t">Two sentences that are similar in meaning for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1430" target="_blank">00:23:50.300</a></span> | <span class="t">My my father plays with me at the park and I play with my dad at the park</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1435" target="_blank">00:23:55.820</a></span> | <span class="t">The first one we will call it sentence A and the second one we will call it sentence B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1442" target="_blank">00:24:02.940</a></span> | <span class="t">We feed them to BERT. So each of them will be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1448" target="_blank">00:24:08.700</a></span> | <span class="t">Sequence of tokens. For example, this one may be 10 tokens and this one may be 8 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1452" target="_blank">00:24:12.940</a></span> | <span class="t">We feed it to BERT which will produce output of 10 tokens and 8 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1458" target="_blank">00:24:18.300</a></span> | <span class="t">Then we do the pooling the mean pooling that we did before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1461" target="_blank">00:24:21.900</a></span> | <span class="t">So we take all these output tokens and we calculate the average of them to produce one only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1467" target="_blank">00:24:27.020</a></span> | <span class="t">vector of dimension 760 as in case we are using BERT base or bigger in case we are using a bigger BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1475" target="_blank">00:24:35.660</a></span> | <span class="t">The first one we will call it sentence embedding A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1478" target="_blank">00:24:38.300</a></span> | <span class="t">So the first vector is the embedding of the sentence A and the second one is the embedding of the sentence B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1484" target="_blank">00:24:44.140</a></span> | <span class="t">We then measure the cosine similarity between these two vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1488" target="_blank">00:24:48.640</a></span> | <span class="t">We have our target cosine similarity because we are training this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1492" target="_blank">00:24:52.700</a></span> | <span class="t">BERT this model. So we for example given these two sentences, which are quite similar in meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1499" target="_blank">00:24:59.980</a></span> | <span class="t">We may have a target that is very close to one because the angle between them will be small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1505" target="_blank">00:25:05.500</a></span> | <span class="t">we hope that the angle between them should be small so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1508" target="_blank">00:25:08.620</a></span> | <span class="t">We calculate the loss because we have a target and the output of the model and then we run back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1514" target="_blank">00:25:14.460</a></span> | <span class="t">propagation on to update all the weights of this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1517" target="_blank">00:25:17.660</a></span> | <span class="t">Now as you can see this model is made up of two branches that are same. So in structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1525" target="_blank">00:25:25.340</a></span> | <span class="t">But this is called the siamese network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1527" target="_blank">00:25:27.340</a></span> | <span class="t">Which is a network that is made of two branches or more branches that are same with each other with respect to the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1534" target="_blank">00:25:34.960</a></span> | <span class="t">But also with respect to the weights of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1539" target="_blank">00:25:39.100</a></span> | <span class="t">So what we do actually when we represent these siamese networks, we represent it at two branches, but when we code this model actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1546" target="_blank">00:25:46.300</a></span> | <span class="t">We will actually only have one model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1549" target="_blank">00:25:49.980</a></span> | <span class="t">So only one branch here that will reproduce cosine similarities and what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1554" target="_blank">00:25:54.780</a></span> | <span class="t">at operating level is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1557" target="_blank">00:25:57.500</a></span> | <span class="t">First we run the sentence A through this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1561" target="_blank">00:26:01.340</a></span> | <span class="t">Then we run the sentence B also through this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1564" target="_blank">00:26:04.700</a></span> | <span class="t">We calculate the cosine similarity between these two output and then we run back propagation such that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1569" target="_blank">00:26:09.980</a></span> | <span class="t">The back propagation will only modify the parameters of this model here, but when we represent it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1577" target="_blank">00:26:17.740</a></span> | <span class="t">For showing we actually we represent it as two branches, but remember that they are not actually two branches. It's only one branch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1585" target="_blank">00:26:25.340</a></span> | <span class="t">It's only one weights only one architecture and the same number of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1589" target="_blank">00:26:29.200</a></span> | <span class="t">This way the birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1592" target="_blank">00:26:32.940</a></span> | <span class="t">Will if we train birth the sentence birth like this it will produce embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1598" target="_blank">00:26:38.080</a></span> | <span class="t">But in such a way that the similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1600" target="_blank">00:26:40.940</a></span> | <span class="t">Sentences have a similar cosine similarity. So I have high cosine similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1607" target="_blank">00:26:47.440</a></span> | <span class="t">And so we can compare them using the cosine similarity measure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1611" target="_blank">00:26:51.600</a></span> | <span class="t">Also, if if you remember birth produces at least birth base produces embeddings of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1618" target="_blank">00:26:58.320</a></span> | <span class="t">768 if we want to produce sentence embeddings that are smaller than 760 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1625" target="_blank">00:27:05.220</a></span> | <span class="t">We can include a linear layer here to reduce the dimensions. For example, we want to go from 768 to 512</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1633" target="_blank">00:27:13.760</a></span> | <span class="t">In the paper of sentence birth actually they not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1637" target="_blank">00:27:17.440</a></span> | <span class="t">Use the mean pooling that we use the so to calculate the average of all the tokens output by birth to produce one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1644" target="_blank">00:27:24.320</a></span> | <span class="t">Vector that represents the meaning of the entire sentence, but they also use max pooling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1649" target="_blank">00:27:29.360</a></span> | <span class="t">And another technique that they use is the CLS token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1652" target="_blank">00:27:32.720</a></span> | <span class="t">So if you remember the CLS token is the first token that we give as input to birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1657" target="_blank">00:27:37.040</a></span> | <span class="t">And it's also the first token that is output by birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1660" target="_blank">00:27:40.880</a></span> | <span class="t">And usually this because of the self-attention mechanism. This CLS token captures the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1666" target="_blank">00:27:46.320</a></span> | <span class="t">information from all the other tokens because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1668" target="_blank">00:27:48.960</a></span> | <span class="t">Of how the self-attention mechanism works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1672" target="_blank">00:27:52.480</a></span> | <span class="t">and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1674" target="_blank">00:27:54.240</a></span> | <span class="t">However, the sentence birth paper they have shown that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1677" target="_blank">00:27:57.200</a></span> | <span class="t">Both methods so the max pooling and the CLS token don't perform better than mean pooling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1683" target="_blank">00:28:03.440</a></span> | <span class="t">So they they recommend using mean pooling which is also one of actually what is used in production nowadays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1690" target="_blank">00:28:10.020</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1692" target="_blank">00:28:12.180</a></span> | <span class="t">Now let's review again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1694" target="_blank">00:28:14.180</a></span> | <span class="t">What is the pipeline of retrieval augmented generation now that we know how embeddings works?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1699" target="_blank">00:28:19.780</a></span> | <span class="t">So we have our query which is how many parameters are there in grok zero?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1704" target="_blank">00:28:24.900</a></span> | <span class="t">Then we have some documents in which we can find this answer. So the documents may be pdf documents or web pages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1711" target="_blank">00:28:31.780</a></span> | <span class="t">We split them into single sentences and we embed these sentences using our sentence birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1720" target="_blank">00:28:40.180</a></span> | <span class="t">Our sentence birth will produce vectors of a fixed size. Suppose 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1724" target="_blank">00:28:44.680</a></span> | <span class="t">And we store them all these vectors in a vector db. We will see later how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1731" target="_blank">00:28:51.460</a></span> | <span class="t">The query is also converted into a vector of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1736" target="_blank">00:28:56.580</a></span> | <span class="t">768 dimensions and we search this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1742" target="_blank">00:29:02.340</a></span> | <span class="t">Query in the vector dbs. How do we search?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1746" target="_blank">00:29:06.900</a></span> | <span class="t">We want to find all the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1749" target="_blank">00:29:09.160</a></span> | <span class="t">That best match our query. What do we mean by this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1752" target="_blank">00:29:12.740</a></span> | <span class="t">We mean all the embeddings that have that when we calculate the cosine similarity score with our query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1759" target="_blank">00:29:19.060</a></span> | <span class="t">It results in a high value or if you are using another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1762" target="_blank">00:29:22.420</a></span> | <span class="t">Similarity score for example euclidean distance the distance is small depending on what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1768" target="_blank">00:29:28.180</a></span> | <span class="t">Distance we are using the cosine similarity or the euclidean distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1773" target="_blank">00:29:33.060</a></span> | <span class="t">This will produce the top embeddings that best match our query and we map them back into the text from which they originated from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1781" target="_blank">00:29:41.140</a></span> | <span class="t">This will produce the context that we feed into our prompt template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1786" target="_blank">00:29:46.360</a></span> | <span class="t">Along with the query we give it to the large language model, which will produce the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1791" target="_blank">00:29:51.540</a></span> | <span class="t">As we saw previously augment the knowledge of a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1795" target="_blank">00:29:55.060</a></span> | <span class="t">We have two strategies fine tuning on a custom data set or using a vector database made up of embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1801" target="_blank">00:30:01.700</a></span> | <span class="t">We can also use a combination of both for example by fine tuning for a few epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1806" target="_blank">00:30:06.260</a></span> | <span class="t">And then using a vector database to complement with knowledge retrieved from the web</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1810" target="_blank">00:30:10.260</a></span> | <span class="t">Whatever strategy we decide to proceed with we need a reliable scalable and easy to use service for building our retrieval augmented generation pipelines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1819" target="_blank">00:30:19.240</a></span> | <span class="t">That's why I recommend gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1821" target="_blank">00:30:21.620</a></span> | <span class="t">Gradient is a scalable ai cloud platform that provides simple apis for fine-tuning models generating embeddings and running inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1830" target="_blank">00:30:30.020</a></span> | <span class="t">Thanks to its integration with popular library lama index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1833" target="_blank">00:30:33.140</a></span> | <span class="t">We can build retrieval augmented generation pipelines with few lines of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1837" target="_blank">00:30:37.140</a></span> | <span class="t">For example, we select the model we want to use in our case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1841" target="_blank">00:30:41.380</a></span> | <span class="t">It's lama2. We define the set of custom documents that the model can use to retrieve answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1846" target="_blank">00:30:46.980</a></span> | <span class="t">We define the model that we want to use to generate embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1850" target="_blank">00:30:50.200</a></span> | <span class="t">Ask a question for example, do you know anyone named oleo it voila</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1855" target="_blank">00:30:55.380</a></span> | <span class="t">Thanks to the power of retrieval augmented generation. The model can now retrieve information about our channel's mascot oleo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1861" target="_blank">00:31:01.620</a></span> | <span class="t">Gradient helps us build all aspects of a retrieval augmented generation pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1866" target="_blank">00:31:06.820</a></span> | <span class="t">For example, we can also fine-tune models on custom data as well as generate embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1871" target="_blank">00:31:11.720</a></span> | <span class="t">With gradient you have total ownership of your data as well as the weights of fine-tuned models open source models are great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1878" target="_blank">00:31:18.820</a></span> | <span class="t">Because they save time on development and debugging as we have access to the architecture and the support of a vast community of developers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1885" target="_blank">00:31:25.640</a></span> | <span class="t">Gradient also integrates with popular libraries lama index and lang chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1890" target="_blank">00:31:30.100</a></span> | <span class="t">Check the link in the description to redeem your five dollar coupon to get started today with gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1895" target="_blank">00:31:35.460</a></span> | <span class="t">Let's talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1898" target="_blank">00:31:38.420</a></span> | <span class="t">Let's talk about vector databases what they are and how their matching algorithm works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1904" target="_blank">00:31:44.100</a></span> | <span class="t">So, how can the vector database search our query in all the vectors that it has stored?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1909" target="_blank">00:31:49.220</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1911" target="_blank">00:31:51.620</a></span> | <span class="t">A vector database stores vectors of fixed dimensions called embeddings such that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1917" target="_blank">00:31:57.540</a></span> | <span class="t">We can then query the database to find all the embeddings that are closest or more similar to our query using a distance metric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1925" target="_blank">00:32:05.540</a></span> | <span class="t">The cosine similarity or the euclidean distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1929" target="_blank">00:32:09.140</a></span> | <span class="t">The vector database uses a variant of the knn algorithm which stands for the k nearest neighbor algorithm or another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1936" target="_blank">00:32:16.580</a></span> | <span class="t">Similarity search algorithm, but usually it's usually a variant of the knn algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1941" target="_blank">00:32:21.160</a></span> | <span class="t">And the vector databases are not only used in retrieval augmented generation pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1945" target="_blank">00:32:25.940</a></span> | <span class="t">They are also used for finding for example similar songs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1948" target="_blank">00:32:28.820</a></span> | <span class="t">So if we have songs we can create embeddings of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1952" target="_blank">00:32:32.100</a></span> | <span class="t">So some embedding some vector that captures all the information about that song</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1957" target="_blank">00:32:37.060</a></span> | <span class="t">And then we can find similar songs a given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1959" target="_blank">00:32:39.780</a></span> | <span class="t">One for example, we have a user who want to find all the similar songs to a given one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1964" target="_blank">00:32:44.740</a></span> | <span class="t">We will create the embedding of that song and all the others. We compare them using some similarity score. For example, the cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1972" target="_blank">00:32:52.260</a></span> | <span class="t">For example, also google images they search similar images using a similar technique. So using an embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1978" target="_blank">00:32:58.740</a></span> | <span class="t">Space in which they produce the embedding of a particular image and all the other and then they check the one that match best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1987" target="_blank">00:33:07.700</a></span> | <span class="t">We can also do the same with products, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1990" target="_blank">00:33:10.120</a></span> | <span class="t">Now knn is an algorithm that allow us to compare a particular query with all the vectors that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=1998" target="_blank">00:33:18.980</a></span> | <span class="t">stored in our database</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2001" target="_blank">00:33:21.140</a></span> | <span class="t">Sort them by distance or by similarity depending on which one we use and then we keep the top best matching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2007" target="_blank">00:33:27.860</a></span> | <span class="t">For example, imagine we have a query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2010" target="_blank">00:33:30.420</a></span> | <span class="t">So how many parameters are there in grok and imagine we have a database vector a vector database</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2016" target="_blank">00:33:36.100</a></span> | <span class="t">Made up of 1 million embeddings because actually 1 million is not even a big number because if you consider that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2022" target="_blank">00:33:42.980</a></span> | <span class="t">Suppose grok, for example that is accessing the tweets in real time every day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2028" target="_blank">00:33:48.580</a></span> | <span class="t">I think we have thousands hundreds of thousands of tweets if not millions of tweets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2033" target="_blank">00:33:53.700</a></span> | <span class="t">So actually the amount of vectors it has it's actually in the order of billions. I think not even millions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2040" target="_blank">00:34:00.420</a></span> | <span class="t">So actually millions looks like a big number, but it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2043" target="_blank">00:34:03.460</a></span> | <span class="t">Especially when we deal with the textual data also from the web we have billions of web pages that we may need to index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2050" target="_blank">00:34:10.100</a></span> | <span class="t">So what we do for example in this knn with the naive approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2054" target="_blank">00:34:14.660</a></span> | <span class="t">Which is the most simple way of matching a query to all the other vectors is to compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2060" target="_blank">00:34:20.260</a></span> | <span class="t">This query with all the other vectors given our cosine similarity function. For example, we may we may have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2066" target="_blank">00:34:26.500</a></span> | <span class="t">The with the first vector it may be 0.3. The second 0.1 0.2, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2071" target="_blank">00:34:31.780</a></span> | <span class="t">then we sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2074" target="_blank">00:34:34.900</a></span> | <span class="t">The we sort them by cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2079" target="_blank">00:34:39.060</a></span> | <span class="t">So with for example the highest one, for example, this one should be the first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2083" target="_blank">00:34:43.380</a></span> | <span class="t">Then the this one should be the second one etc, etc, and we keep the top k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2087" target="_blank">00:34:47.140</a></span> | <span class="t">So the top three or the top two depending on how we chose k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2090" target="_blank">00:34:50.580</a></span> | <span class="t">Now this is actually a very simple approach, but it's also very slow because if there are n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2096" target="_blank">00:34:56.980</a></span> | <span class="t">Embedding vectors. So in this case 1 million and each one has d dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2101" target="_blank">00:35:01.780</a></span> | <span class="t">So in this case, for example in the case of birth base, it's 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2105" target="_blank">00:35:05.480</a></span> | <span class="t">The computational complexity is in the order of n multiplied by d which is very very very slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2113" target="_blank">00:35:13.860</a></span> | <span class="t">Let's see if there are better approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2115" target="_blank">00:35:15.940</a></span> | <span class="t">And we will be exploring one algorithm in particular that is also used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2120" target="_blank">00:35:20.900</a></span> | <span class="t">Right now in the most popular vector db's which is called the hierarchical navigable small words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2126" target="_blank">00:35:26.740</a></span> | <span class="t">Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2129" target="_blank">00:35:29.620</a></span> | <span class="t">What we the idea is that we will trade precision for speed. So before what the algorithm we saw before so the naive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2136" target="_blank">00:35:36.980</a></span> | <span class="t">knn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2139" target="_blank">00:35:39.860</a></span> | <span class="t">Which performs very slowly, but it's actually precise because each query the query is compared with each of the vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2146" target="_blank">00:35:46.600</a></span> | <span class="t">So it will always produce accurate results because we have all the possible comparison done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2152" target="_blank">00:35:52.180</a></span> | <span class="t">but do so to reduce the number of to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2156" target="_blank">00:35:56.100</a></span> | <span class="t">Increase the speed we need to reduce the number of comparisons that we do and the metric that we usually care in similarity search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2163" target="_blank">00:36:03.540</a></span> | <span class="t">Is recall. So the recall basically indicates that if our suppose that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2170" target="_blank">00:36:10.020</a></span> | <span class="t">in before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2171" target="_blank">00:36:11.620</a></span> | <span class="t">The best matching vector is for example, this one and this one and this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2176" target="_blank">00:36:16.900</a></span> | <span class="t">uh, we want our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2179" target="_blank">00:36:19.620</a></span> | <span class="t">Top three query so knn to retrieve them all three of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2185" target="_blank">00:36:25.700</a></span> | <span class="t">But imagine it only returns two in this case. We we have that the the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2190" target="_blank">00:36:30.900</a></span> | <span class="t">Returned only two of the best matching vectors. So we will say that the recall is 66 percent or two thirds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2199" target="_blank">00:36:39.300</a></span> | <span class="t">So basically the recall measures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2201" target="_blank">00:36:41.060</a></span> | <span class="t">How many relevant items are retrieved among all the relevant items that it should have retrieved from our search?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2208" target="_blank">00:36:48.180</a></span> | <span class="t">And we will see one one algorithm in particular for approximate nearest neighbors, which is called hierarchical navigable small words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2217" target="_blank">00:36:57.060</a></span> | <span class="t">Now hierarchical navigable small words is actually used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2221" target="_blank">00:37:01.540</a></span> | <span class="t">Is actually very popular nowadays in vector databases and in particular. It's also the same algorithm that powers the database quadrant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2229" target="_blank">00:37:09.000</a></span> | <span class="t">Which is also the open source vector database used by twitter's grok llm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2233" target="_blank">00:37:13.940</a></span> | <span class="t">For example, this is the exchange of tweets that I saw the other day between elon musk and the team of quadrant in which they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2241" target="_blank">00:37:21.380</a></span> | <span class="t">Quadrant says that actually the grok is accessing all the tweets in real time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2248" target="_blank">00:37:28.900</a></span> | <span class="t">using the vector database, which is quadrant and if we check the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2253" target="_blank">00:37:33.700</a></span> | <span class="t">Documentation of this vector database, we will see that the quadrant currently only uses the hierarchical navigable small words as the vector index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2261" target="_blank">00:37:41.860</a></span> | <span class="t">So this is the algorithm that powers the database that is currently used by twitter to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2267" target="_blank">00:37:47.060</a></span> | <span class="t">introduce retrieval augmented generation in its large language model grok</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2274" target="_blank">00:37:54.580</a></span> | <span class="t">The first idea behind this hierarchical navigable small words is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2279" target="_blank">00:37:59.380</a></span> | <span class="t">Is the idea of the six degrees of evolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2282" target="_blank">00:38:02.900</a></span> | <span class="t">So actually the hierarchical navigable small words is an evolution of an earlier algorithm called navigable small words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2289" target="_blank">00:38:09.380</a></span> | <span class="t">Which is an algorithm for approximate nearest neighbors that we will see later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2293" target="_blank">00:38:13.700</a></span> | <span class="t">Which is based on the idea of six degrees of separation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2297" target="_blank">00:38:17.080</a></span> | <span class="t">So in the 1960s, there was an experiment which is called the milgram experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2303" target="_blank">00:38:23.540</a></span> | <span class="t">Which wanted to test the social connections among people in the usa</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2307" target="_blank">00:38:27.380</a></span> | <span class="t">The participants who were initially located in nebraska and constance were given a letter to deliver to a person that they didn't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2315" target="_blank">00:38:35.220</a></span> | <span class="t">They did not know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2316" target="_blank">00:38:36.980</a></span> | <span class="t">And this person was in boston</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2318" target="_blank">00:38:38.980</a></span> | <span class="t">However, they were not allowed to send the letter directly to the recipient instead. They were instructed to send it to someone who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2326" target="_blank">00:38:46.980</a></span> | <span class="t">Who could best know this target person?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2331" target="_blank">00:38:51.860</a></span> | <span class="t">At the end of the milgram word experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2334" target="_blank">00:38:54.200</a></span> | <span class="t">They found that the letter reached the final recipient in five or six steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2340" target="_blank">00:39:00.180</a></span> | <span class="t">Creating the concept that people all over the world are connected by six degrees of separation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2345" target="_blank">00:39:05.880</a></span> | <span class="t">And actually in 2016 facebook published a blog post in which they claimed that the 1.59</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2352" target="_blank">00:39:12.040</a></span> | <span class="t">Billion active users on facebook were connected by an average of 3.5 degrees of separation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2359" target="_blank">00:39:19.300</a></span> | <span class="t">This means that between me and mark zuckerberg. There are 3.5 connections, which means that on average, of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2365" target="_blank">00:39:25.220</a></span> | <span class="t">Which means that I have a friend who has a friend who has a friend who knows mark zuckerberg</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2370" target="_blank">00:39:30.840</a></span> | <span class="t">And this is the idea of the degrees of separation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2374" target="_blank">00:39:34.840</a></span> | <span class="t">So, let's see. What is this navigable small words?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2378" target="_blank">00:39:38.260</a></span> | <span class="t">Now the navigable small words algorithm builds a graph that just like facebook friends connects close vectors with each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2387" target="_blank">00:39:47.300</a></span> | <span class="t">But keeping the total number of connections small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2390" target="_blank">00:39:50.020</a></span> | <span class="t">For example, every vector may be connected with up to other six vectors like to mimic the sixth degree of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2396" target="_blank">00:39:56.340</a></span> | <span class="t">Separation for example, imagine we have a very small database with only 15 vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2402" target="_blank">00:40:02.340</a></span> | <span class="t">Each one representing a particular sentence that we retrieved from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2406" target="_blank">00:40:06.660</a></span> | <span class="t">Our knowledge source which could be documents or web pages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2410" target="_blank">00:40:10.420</a></span> | <span class="t">And we have a graph like this in which for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2413" target="_blank">00:40:13.700</a></span> | <span class="t">The first text is about the transformer which is connected to another piece of text that talks about the transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2420" target="_blank">00:40:20.580</a></span> | <span class="t">Then we have another text that connects the tree with the two which is now talking about the cancer with ai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2427" target="_blank">00:40:27.140</a></span> | <span class="t">So diagnosing cancer with ai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2429" target="_blank">00:40:29.140</a></span> | <span class="t">And etc etc now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2432" target="_blank">00:40:32.580</a></span> | <span class="t">How do we find a given query in this particular graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2436" target="_blank">00:40:36.900</a></span> | <span class="t">So imagine we have our query which is how many encoder layers are there in the transformer model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2443" target="_blank">00:40:43.940</a></span> | <span class="t">How does the algorithm find the k nearest neighbors? So the best matching k vectors for our query?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2450" target="_blank">00:40:50.660</a></span> | <span class="t">The algorithm will proceed like this. It will find first of all an entry point in this graph, which is randomly chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2457" target="_blank">00:40:57.460</a></span> | <span class="t">So we randomly choose among all these vectors one node as a random as an entry point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2462" target="_blank">00:41:02.660</a></span> | <span class="t">We visit it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2465" target="_blank">00:41:05.460</a></span> | <span class="t">And then we compare the similarity score of this query and this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2470" target="_blank">00:41:10.740</a></span> | <span class="t">And compare it with the similarity score of the query with the friends of this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2475" target="_blank">00:41:15.140</a></span> | <span class="t">So with the number with the node number seven and the node number two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2478" target="_blank">00:41:18.900</a></span> | <span class="t">If one of the friends has a better similarity score, then we move it to move it there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2485" target="_blank">00:41:25.060</a></span> | <span class="t">So the number two for example may have a better similarity score with the q compared to the number five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2492" target="_blank">00:41:32.100</a></span> | <span class="t">So we move to the number two and then we do again this process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2495" target="_blank">00:41:35.780</a></span> | <span class="t">so we check what is the cosine similarity score between the node number three and the query and we compare it with the cosine similarity of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2503" target="_blank">00:41:43.860</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2504" target="_blank">00:41:44.740</a></span> | <span class="t">Vector number two with the query if the number three has a better cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2510" target="_blank">00:41:50.020</a></span> | <span class="t">We move there and we keep doing like this until we reach a node in which his</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2515" target="_blank">00:41:55.700</a></span> | <span class="t">The friends of this node so the number eight and the number six</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2519" target="_blank">00:41:59.460</a></span> | <span class="t">Don't have a better cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2523" target="_blank">00:42:03.460</a></span> | <span class="t">With respect to the query compared to the current one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2526" target="_blank">00:42:06.260</a></span> | <span class="t">So this number four has the best cosine similarity score among all of his connections among the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2532" target="_blank">00:42:12.580</a></span> | <span class="t">0.1, 0.8 and the 0.6. So we stop there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2535" target="_blank">00:42:15.940</a></span> | <span class="t">And we have visited many nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2539" target="_blank">00:42:19.220</a></span> | <span class="t">Basically, we ordered them from the best matching to the lowest matching and we keep the top k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2545" target="_blank">00:42:25.300</a></span> | <span class="t">Also, we repeat this search many times by choosing different random entry points and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2553" target="_blank">00:42:33.300</a></span> | <span class="t">Every time we choose we sort all of the results by similarity score and then we keep the top k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2559" target="_blank">00:42:39.140</a></span> | <span class="t">And these are the best matching k nearest neighbor. So using the navigable small words algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2565" target="_blank">00:42:45.720</a></span> | <span class="t">If we want to insert a vector in this graph, we just do what we did before. So we actually search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2572" target="_blank">00:42:52.900</a></span> | <span class="t">Given for example, we want to insert this query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2576" target="_blank">00:42:56.100</a></span> | <span class="t">For example, we will just do it like a search and then when we have found the top k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2581" target="_blank">00:43:01.780</a></span> | <span class="t">We just connect the query with the top k and that's how we insert a new item into this graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2587" target="_blank">00:43:07.540</a></span> | <span class="t">The second idea of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2591" target="_blank">00:43:11.140</a></span> | <span class="t">Hierarchical navigable small words is based on another data structure that is called the skip list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2596" target="_blank">00:43:16.980</a></span> | <span class="t">So to go from navigable small words to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2600" target="_blank">00:43:20.180</a></span> | <span class="t">Hierarchical navigable small words. We need to introduce this new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2604" target="_blank">00:43:24.100</a></span> | <span class="t">Data structure. So the skip list is a data structure that maintains a sorted list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2610" target="_blank">00:43:30.980</a></span> | <span class="t">And allows to search and insertions with an average of logarithmic time complexity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2617" target="_blank">00:43:37.160</a></span> | <span class="t">for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2619" target="_blank">00:43:39.380</a></span> | <span class="t">If we want to search the number nine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2621" target="_blank">00:43:41.380</a></span> | <span class="t">What we can do in this first of all as you can see this, this is not only one linked list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2627" target="_blank">00:43:47.460</a></span> | <span class="t">We have many linked list levels of linked list. We have the level 0, the level 1, the level 2, the level 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2634" target="_blank">00:43:54.500</a></span> | <span class="t">The bottom level has the most number of items. The more we go up the less is the number of items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2640" target="_blank">00:44:00.740</a></span> | <span class="t">So if we want to search the number line in this linked list in this skip list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2646" target="_blank">00:44:06.100</a></span> | <span class="t">We start from the top level. So we start from the head number three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2650" target="_blank">00:44:10.340</a></span> | <span class="t">We check what is the next item and we compare it with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2655" target="_blank">00:44:15.060</a></span> | <span class="t">So the first item is number five. We compare it with what is the next item in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2660" target="_blank">00:44:20.260</a></span> | <span class="t">It's the end which means that the number nine must be down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2664" target="_blank">00:44:24.100</a></span> | <span class="t">So we go down we then compare it with the next item, which is number 17</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2668" target="_blank">00:44:28.500</a></span> | <span class="t">Which means that it cannot be after this node because it's 17. So it must be down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2674" target="_blank">00:44:34.340</a></span> | <span class="t">We go down and then we compare it with the next node, which is the number nine and we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2679" target="_blank">00:44:39.540</a></span> | <span class="t">Okay, we reached the number nine. Now imagine we want to find another number. Let's say the number 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2684" target="_blank">00:44:44.980</a></span> | <span class="t">We start again from the h3. We go to the first item and compare to the next. Okay, it's the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2692" target="_blank">00:44:52.100</a></span> | <span class="t">So we go down then we arrive here. We compare it with the next we see it's 17. So it's bigger than 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2698" target="_blank">00:44:58.820</a></span> | <span class="t">So we go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2700" target="_blank">00:45:00.580</a></span> | <span class="t">Then we see it's nine. So the next item is number nine. So we visit nine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2705" target="_blank">00:45:05.860</a></span> | <span class="t">And then we compare it with the next item which is 17, so it's bigger than 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2711" target="_blank">00:45:11.620</a></span> | <span class="t">So we go down we go here and then we compare it with what is the next item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2717" target="_blank">00:45:17.140</a></span> | <span class="t">Which is number 12 and it's the number that we are looking for and we stop there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2720" target="_blank">00:45:20.980</a></span> | <span class="t">So this is how the skip list works. Now to create the hierarchical navigable small worlds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2726" target="_blank">00:45:26.660</a></span> | <span class="t">we combine the concept of navigable small worlds with the idea of the skip list in producing a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2732" target="_blank">00:45:32.820</a></span> | <span class="t">hierarchical navigable small worlds algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2736" target="_blank">00:45:36.600</a></span> | <span class="t">So we start with we have a lower level which has more nodes and more connections and the upper level which has less nodes and less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2745" target="_blank">00:45:45.140</a></span> | <span class="t">Connections. So we say that this one is more dense and this one is more sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2750" target="_blank">00:45:50.660</a></span> | <span class="t">How does the search work in this graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2753" target="_blank">00:45:53.700</a></span> | <span class="t">Suppose we have a query just like before and we want to search in this graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2758" target="_blank">00:45:58.500</a></span> | <span class="t">We find a random entry point in the top level of this graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2762" target="_blank">00:46:02.820</a></span> | <span class="t">And then we visit it and then we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2766" target="_blank">00:46:06.420</a></span> | <span class="t">The cosine similarity of this node with the query and all of his friends with the query and we see that this one is the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2773" target="_blank">00:46:13.620</a></span> | <span class="t">One so we go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2775" target="_blank">00:46:15.460</a></span> | <span class="t">We go down and we do again the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2778" target="_blank">00:46:18.980</a></span> | <span class="t">We do again the same test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2780" target="_blank">00:46:20.180</a></span> | <span class="t">So we check the cosine similarity of this node with the query and all of his friends with the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2785" target="_blank">00:46:25.780</a></span> | <span class="t">And we see that this friend here has a better cosine similarity score. So we move here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2790" target="_blank">00:46:30.260</a></span> | <span class="t">Then we check this node here with all of his friends and we see that this one is the best one. So we go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2796" target="_blank">00:46:36.580</a></span> | <span class="t">Also this one we see that it's the best one among all of his friends for the cosine similarity score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2803" target="_blank">00:46:43.540</a></span> | <span class="t">So we go down and then we do again this test and we say what is the cosine similarity score of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2810" target="_blank">00:46:50.180</a></span> | <span class="t">Node and the query and also all of his friends with the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2815" target="_blank">00:46:55.220</a></span> | <span class="t">So this node this node and this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2817" target="_blank">00:46:57.220</a></span> | <span class="t">And we move to the one that is best in case there is one and then we stop as soon as we find a local best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2823" target="_blank">00:47:03.540</a></span> | <span class="t">So the one node that is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2825" target="_blank">00:47:05.860</a></span> | <span class="t">worse than all of his friends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2829" target="_blank">00:47:09.460</a></span> | <span class="t">We repeat this search just like before by using randomly selected entry points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2834" target="_blank">00:47:14.580</a></span> | <span class="t">We take all these vectors that we have visited we sort we keep the top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2839" target="_blank">00:47:19.620</a></span> | <span class="t">K best matching based on the similarity score that we are using or the distance function that we are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2845" target="_blank">00:47:25.780</a></span> | <span class="t">Okay, now that we have seen also how the vector database works let's review again the pipeline of retrieval augmented generation by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2857" target="_blank">00:47:37.700</a></span> | <span class="t">Summing up what we have seen. So again, we have our query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2861" target="_blank">00:47:41.220</a></span> | <span class="t">We have some documents from which we want to retrieve the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2864" target="_blank">00:47:44.740</a></span> | <span class="t">We split them into pieces of text we create embeddings using sentence bird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2869" target="_blank">00:47:49.060</a></span> | <span class="t">For example, we store all these vectors in a vector database</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2872" target="_blank">00:47:52.840</a></span> | <span class="t">We convert our query in an embedding and we search in a vector database using the algorithm that we have seen before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2879" target="_blank">00:47:59.780</a></span> | <span class="t">So the hierarchical navigable small words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2882" target="_blank">00:48:02.660</a></span> | <span class="t">This will produce the top k embeddings best matching with our query from which we associate go back to the text that they were taken from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2890" target="_blank">00:48:10.260</a></span> | <span class="t">We combine the query and the context retrieved in a template</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2895" target="_blank">00:48:15.780</a></span> | <span class="t">We feed it to the large language model and finally the large language model will produce the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2900" target="_blank">00:48:20.820</a></span> | <span class="t">Thank you guys for watching my video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2904" target="_blank">00:48:24.260</a></span> | <span class="t">I hope you learned a lot today because I wanted to create this video for a long time actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2909" target="_blank">00:48:29.460</a></span> | <span class="t">But I wanted also to understand myself all the algorithms that were behind this pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2914" target="_blank">00:48:34.340</a></span> | <span class="t">And I hope that you are also now familiar with all these concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2917" target="_blank">00:48:37.960</a></span> | <span class="t">I know that actually implementing the rug pipeline is very easy. There are many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2923" target="_blank">00:48:43.220</a></span> | <span class="t">Popular libraries like llama index and long chain, but I wanted to actually go deep inside of how it works and each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2930" target="_blank">00:48:50.660</a></span> | <span class="t">Building block how they actually work together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2933" target="_blank">00:48:53.620</a></span> | <span class="t">Please let let me know if there is something that you don't understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2937" target="_blank">00:48:57.620</a></span> | <span class="t">I will try to answer all the questions in the comment section. Also, let me know if you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2942" target="_blank">00:49:02.980</a></span> | <span class="t">Something that you want better clarified in my future videos or how can I improve my future videos for better clarity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2950" target="_blank">00:49:10.360</a></span> | <span class="t">Please subscribe to my channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2952" target="_blank">00:49:12.260</a></span> | <span class="t">This is the best motivation for me to continue making high quality content and like the video if you like it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2957" target="_blank">00:49:17.060</a></span> | <span class="t">Share the video with your friends with your professors with your students, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=rhZgXNdhWDY&t=2961" target="_blank">00:49:21.060</a></span> | <span class="t">And have a nice day guys</span></div></div></body></html>