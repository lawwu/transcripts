<html><head><title>Going beyond RAG: Extended Mind Transformers - Phoebe Klett</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Going beyond RAG: Extended Mind Transformers - Phoebe Klett</h2><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU"><img src="https://i.ytimg.com/vi_webp/cS6M6Jec0lU/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=63">1:3</a> Long Context vs RAG<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=212">3:32</a> Extended Mind Attention<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=376">6:16</a> Evaluations<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=447">7:27</a> Results<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=538">8:58</a> Citations<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=627">10:27</a> Reduce hallucinations<br><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=712">11:52</a> Parameters<br><br><div style="text-align: left;"><a href="./cS6M6Jec0lU.html">Whisper Transcript</a> | <a href="./transcript_cS6M6Jec0lU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I'm Phoebe, I'm a machine learning engineer at Normal Computing, and I'm really excited to tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=17" target="_blank">00:00:17.520</a></span> | <span class="t">you guys about some of our recent research, and in particular, extended mind transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=21" target="_blank">00:00:21.600</a></span> | <span class="t">All right, so just to briefly cover what we're going to go over in today's talk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=26" target="_blank">00:00:26.160</a></span> | <span class="t">we'll introduce the problem, which I think will be quite familiar, given the amazing talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=29" target="_blank">00:00:29.960</a></span> | <span class="t">which came before mine, and then dive right into the methods, so what is the retrieval mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=35" target="_blank">00:00:35.320</a></span> | <span class="t">that extended mind transformers implement, and then we'll dive into some experiments which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=39" target="_blank">00:00:39.480</a></span> | <span class="t">give us confidence that these methods are actually performant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=42" target="_blank">00:00:42.280</a></span> | <span class="t">After that, we'll get into two of my favorite and I think most compelling features that extended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=46" target="_blank">00:00:46.280</a></span> | <span class="t">mind transformers enable. This is a new kind of citation, as well as a new kind of generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=51" target="_blank">00:00:51.240</a></span> | <span class="t">paradigm, which is active learning inspired, and then we'll go over the most important parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=56" target="_blank">00:00:56.280</a></span> | <span class="t">to tune when implementing EMTs in your applications, and generally how to use them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=61" target="_blank">00:01:01.640</a></span> | <span class="t">All right, so we pre-train language models so that they have general knowledge, but as we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=69" target="_blank">00:01:09.880</a></span> | <span class="t">been discussing all this conference, that's not enough. We need a lot of application-specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=74" target="_blank">00:01:14.840</a></span> | <span class="t">information and a topical description of the world in order to make these things useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=79" target="_blank">00:01:19.240</a></span> | <span class="t">I'm not going to belabor the two most popular methods which try to load this description into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=86" target="_blank">00:01:26.120</a></span> | <span class="t">the language model, those being longcontext and RAG, as I think, yeah, we've heard a lot about those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=91" target="_blank">00:01:31.560</a></span> | <span class="t">great methods already, but I'd like to point out that they solve the problem in different ways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=96" target="_blank">00:01:36.840</a></span> | <span class="t">and thus suffer from different downsides. So longcontext seeks to extend the context window of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=103" target="_blank">00:01:43.480</a></span> | <span class="t">transformer model. So we train language models, we train them on sequences of a fixed length, and then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=109" target="_blank">00:01:49.000</a></span> | <span class="t">trying to say, well, can we extend that so we can include more in the context, more in the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=114" target="_blank">00:01:54.920</a></span> | <span class="t">during inference time? Fine-tuning is usually how this is done, and that's awfully expensive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=119" target="_blank">00:01:59.800</a></span> | <span class="t">and more so than that, including all of that context in your prompt can confuse the model with a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=126" target="_blank">00:02:06.200</a></span> | <span class="t">irrelevant information. And kind of beyond that, just conceptually speaking, it seems a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=131" target="_blank">00:02:11.640</a></span> | <span class="t">like wasteful, right? Like if we're trying to do question answering over a big code base, our query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=137" target="_blank">00:02:17.000</a></span> | <span class="t">is most usually, does not need to reference like all of those different function definitions, but just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=141" target="_blank">00:02:21.640</a></span> | <span class="t">needs some subset of them to answer the query correctly. Okay, so this is what RAG tries to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=147" target="_blank">00:02:27.080</a></span> | <span class="t">right? Let's try to subset that information down and just include the most relevant context in our prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=153" target="_blank">00:02:33.560</a></span> | <span class="t">So what are the issues here? Well, these mechanisms are external to the transformer, kind of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=161" target="_blank">00:02:41.080</a></span> | <span class="t">necessarily limited by being external to the model. So we make this choice of what's relevant once and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=167" target="_blank">00:02:47.080</a></span> | <span class="t">up front before the generation starts, and we're also making this choice about what's relevant using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=172" target="_blank">00:02:52.680</a></span> | <span class="t">kind of the least granular representation of that data and often ones that are disjoint from the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=178" target="_blank">00:02:58.200</a></span> | <span class="t">that the model will reason about that data. Kind of also just conceptually, neither of these methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=185" target="_blank">00:03:05.640</a></span> | <span class="t">make a difference or make a distinction between things that should go in memory and things that should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=190" target="_blank">00:03:10.920</a></span> | <span class="t">included along with your inference query. And this is more than just aesthetics, it's actually going to enable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=195" target="_blank">00:03:15.400</a></span> | <span class="t">us to, oh, it's going to enable us to have these like more granular causal citations and allow the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=204" target="_blank">00:03:24.760</a></span> | <span class="t">to retrieve more information when we can tell it's uncertain, kind of actively within the generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=209" target="_blank">00:03:29.400</a></span> | <span class="t">All right, so how do we do this? Extended mind attention is a very simple edit to the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=217" target="_blank">00:03:37.320</a></span> | <span class="t">mechanism of the transformer. I'm not going to get too much into the math because we don't have a ton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=221" target="_blank">00:03:41.240</a></span> | <span class="t">of time today, but would love for anyone to check out the paper and let me know what you think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=224" target="_blank">00:03:44.840</a></span> | <span class="t">So, but I'll just go over kind of, yeah, from a qualitative perspective, how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=230" target="_blank">00:03:50.920</a></span> | <span class="t">So the model represents data within each decoder layer. Most of the transformers that we're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=236" target="_blank">00:03:56.680</a></span> | <span class="t">today are decoder-only transformers. And within each of those decoder layers, the model will represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=242" target="_blank">00:04:02.040</a></span> | <span class="t">that data as a key value pair. So it actually already has this retrieval mechanism built into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=247" target="_blank">00:04:07.480</a></span> | <span class="t">transformer. All we have to do is kind of hack around it. And so we pass all of the memory tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=253" target="_blank">00:04:13.960</a></span> | <span class="t">through the model and save off those key value representations. And then during generation time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=259" target="_blank">00:04:19.640</a></span> | <span class="t">we allow each query token, just like RAG, using cosine similarity, to go retrieve a particular number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=266" target="_blank">00:04:26.440</a></span> | <span class="t">of those memory tokens and attend to them. So in this picture, these kind of red tokens, red highlighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=273" target="_blank">00:04:33.320</a></span> | <span class="t">tokens are meant to represent those retrieved tokens. Again, this actually ends up being a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=280" target="_blank">00:04:40.440</a></span> | <span class="t">change to the transformer model. What's difficult is figuring out how to assign position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=286" target="_blank">00:04:46.120</a></span> | <span class="t">information to those tokens. So this work is based on research from a couple of years ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=291" target="_blank">00:04:51.880</a></span> | <span class="t">but they needed to fine tune their model in order to kind of teach the model how to leverage these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=296" target="_blank">00:04:56.760</a></span> | <span class="t">retrieved tokens. And that's in large part due to the absolute position embeddings that were popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=301" target="_blank">00:05:01.800</a></span> | <span class="t">during that time. So because transformer models are position agnostic, we have to figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=307" target="_blank">00:05:07.480</a></span> | <span class="t">how to kind of tell them, okay, this token is position zero, this one is position one, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=313" target="_blank">00:05:13.000</a></span> | <span class="t">But due to today's more kind of like their softer position embeddings, this allows us to really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=320" target="_blank">00:05:20.520</a></span> | <span class="t">leverage this method without any further fine tuning. So in particular, these relative position embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=326" target="_blank">00:05:26.200</a></span> | <span class="t">that have become popular, and I'll talk about two different methods that we've tested and implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=330" target="_blank">00:05:30.680</a></span> | <span class="t">this on, really enable the model to kind of generalize to these retrieved tokens. The first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=338" target="_blank">00:05:38.760</a></span> | <span class="t">that we tested on is present in all of the Lama models. These are the rotary position embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=343" target="_blank">00:05:43.800</a></span> | <span class="t">and this generalizes the principle of using kind of like an angle between two vectors as a distance metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=350" target="_blank">00:05:50.120</a></span> | <span class="t">So we kind of take the whole embedding and we rotate kind of two positions at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=354" target="_blank">00:05:54.200</a></span> | <span class="t">The other one that we implemented this method into is the alibi, linear biases. So these actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=361" target="_blank">00:06:01.960</a></span> | <span class="t">aren't positioning embeddings at all, it just kind of linearly damps down information which is further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=368" target="_blank">00:06:08.920</a></span> | <span class="t">away. And these are the way that all of the mosaics MPT models are trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=373" target="_blank">00:06:13.480</a></span> | <span class="t">Okay, so let's talk about some evaluations. We also just open sourced a new counterfactual retrieval</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=382" target="_blank">00:06:22.360</a></span> | <span class="t">benchmark, and I'm just going to briefly describe what that benchmark looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=385" target="_blank">00:06:25.480</a></span> | <span class="t">So this is a long context benchmark, so our input context is our query answer pairs. And the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=393" target="_blank">00:06:33.000</a></span> | <span class="t">to answer those questions range from about 2,000 tokens to all the way up to 16,000 tokens. And again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=399" target="_blank">00:06:39.800</a></span> | <span class="t">these are like queries, so like the question might be who wrote the song, these shoes were made for walking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=404" target="_blank">00:06:44.440</a></span> | <span class="t">and then the corresponding Wikipedia snippet. We wanted to control for facts memorized during pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=411" target="_blank">00:06:51.080</a></span> | <span class="t">though. And actually any fine tuning also. So what we did was we looked up, for instance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=416" target="_blank">00:06:56.360</a></span> | <span class="t">in this case, the answer is Lee Hazelwood. We did a little bit of research. We figured out, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=420" target="_blank">00:07:00.440</a></span> | <span class="t">well, Terry Allen is a similar songwriter. This is a plausible answer, but it's wrong. We went in and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=426" target="_blank">00:07:06.200</a></span> | <span class="t">we replaced all the instances of Lee Hazelwood with Terry Allen, and now we asked the model to retrieve this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=431" target="_blank">00:07:11.720</a></span> | <span class="t">Terry Allen answer. All right, so how do extended-mine transformers stack up? Here we're comparing it with fine-tuned models, as well as the base Lama model, with interpolated position embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=445" target="_blank">00:07:25.000</a></span> | <span class="t">So we can see here in the green that the base model does a pretty good job extrapolating even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=450" target="_blank">00:07:30.280</a></span> | <span class="t">like many times more, so this is the model trained up to 2048 tokens during pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=455" target="_blank">00:07:35.560</a></span> | <span class="t">You can see even up to 8K it's doing okay. 16K it really falls off. The position embeddings can't extrapolate that far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=463" target="_blank">00:07:43.560</a></span> | <span class="t">The fine-tuned models you can see actually perform worse than the extended-mine model on these shorter inputs, and this is another data point that suggests that fine-tuning on super-long context actually degrades the quality of attention that you get on shorter inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=480" target="_blank">00:08:00.840</a></span> | <span class="t">And, you can see that the fine-tuned models actually perform worse than the extended-mined model on these shorter inputs, and this is another data point that suggests that fine-tuning on super-long context actually degrades the quality of attention that you get on shorter inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=486" target="_blank">00:08:06.840</a></span> | <span class="t">And, this is another data point that suggests that fine-tuning on super-long context actually degrades the quality of attention that you get on shorter inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=492" target="_blank">00:08:12.840</a></span> | <span class="t">And, extended-mined transformers continue to be competitive with those fine-tuned models, all the way up to 16K. Again, our models are not fine-tuned at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=501" target="_blank">00:08:21.160</a></span> | <span class="t">And, in this particular experiment, so what the extended-mined model sees in context is the query only. So it only sees the, like, who wrote the song, these users made for walking, and relies heavily on that internal retrieval mechanism to go look up that new information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=518" target="_blank">00:08:38.920</a></span> | <span class="t">In this second experiment, we seed it with a little bit more information in context using RAG, but again, mostly relying on that internal mechanism still.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=527" target="_blank">00:08:47.240</a></span> | <span class="t">And, you can see we're outperforming GPT-4 here now when we combine it with that more information in context as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=533" target="_blank">00:08:53.240</a></span> | <span class="t">Okay. Now we're going to talk about citations. So, I think this will be a topic that lots of you here can empathize with. As AI engineers, I think this is one of the most important things to provide in an application such that people can learn to trust the model outputs. In fact, you might actually use RAG just to get citations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=555" target="_blank">00:09:15.240</a></span> | <span class="t">So, with RAG, though, the citations that you get are a little bit kind of like post-hoc rationalization. So maybe if, like, the date appears in the output and we knew it was also in the input to the language model, we feel pretty confident that that date is not hallucinated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=571" target="_blank">00:09:31.240</a></span> | <span class="t">But again, this is not really, like, causally related to what information the model used during the generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=577" target="_blank">00:09:37.240</a></span> | <span class="t">Now, with extended mind transformers, we can look up exactly which tokens were retrieved from those memories and used during generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=585" target="_blank">00:09:45.240</a></span> | <span class="t">So, in this example, on the top left here, we have the memories. This is a snippet from Wikipedia about one of my favorite mathematicians, Alexander Grothendijk. And the query is, when did he get his French citizenship?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=597" target="_blank">00:09:57.240</a></span> | <span class="t">And then, in the bottom, you can see the completion with the correct date. I think he got it in 1971. So, the blue highlighted tokens here, importantly, the 1971, as well as some of the Alexander Grothendijk tokens, those are the ones that the model retrieved and attended to when generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=615" target="_blank">00:10:15.200</a></span> | <span class="t">that 1971 correct token. And so, being able to report that gives a lot of confidence and also just insight into how the model is using those retrieved tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=628" target="_blank">00:10:28.200</a></span> | <span class="t">Okay. We can also use extended mind transformers to reduce hallucinations. So, how do we do this? So, right now, we have access to, in the, like, simplest case, just kind of token-level entropy over that output distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=641" target="_blank">00:10:41.200</a></span> | <span class="t">And if you want to get fancier, we're also doing some Bayesian fine-tuning of language models at normal. But you can use any uncertainty metric to determine kind of how certain the model is about a generated token. And if we kind of can detect that the model is uncertain about that token, we can regenerate that step using more information from these memories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=661" target="_blank">00:11:01.200</a></span> | <span class="t">Okay. So, in the top right here, we just set, like, a baseline default number of memories that each query token is allowed to retrieve and attend to. And you can see it wasn't quite enough information to get this query right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=674" target="_blank">00:11:14.200</a></span> | <span class="t">So, if you remember from the previous slide, the correct answer here is 1971. And you can see we've got 1993 here. So, it wasn't enough. We didn't attend to that memory quite enough to get this question right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=685" target="_blank">00:11:25.200</a></span> | <span class="t">And in the bottom example, we allow it to regenerate some subset of those tokens using more information from the cache when we can tell the model was uncertain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=695" target="_blank">00:11:35.200</a></span> | <span class="t">And again, we got this right. So, it's kind of like, kind of a nice intuition for when the model's uncertain and then, okay, if it's really uncertain, let's go use more information and also can be more efficient, kind of depending on how the math works out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=710" target="_blank">00:11:50.200</a></span> | <span class="t">All right. So, now I'm going to tell you guys about the most important parameters to set when using extended mind transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=719" target="_blank">00:11:59.200</a></span> | <span class="t">So, you may have heard of something called stride length before. And this is a parameter that comes up a lot, even just kind of in regular perplexity computations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=728" target="_blank">00:12:08.200</a></span> | <span class="t">So, when we compute the memories that we're going to attend to, we pass them through the model and then, again, save off these key value representations that the model saves internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=738" target="_blank">00:12:18.200</a></span> | <span class="t">But, again, the models that we're using are trained on this fixed context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=743" target="_blank">00:12:23.200</a></span> | <span class="t">So, we need to kind of pass over them with some stride such that each of those tokens has an appropriate amount of context to generate the representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=753" target="_blank">00:12:33.200</a></span> | <span class="t">So, if the stride is smaller, you're going to get more high-quality representations, but also will require more computations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=762" target="_blank">00:12:42.200</a></span> | <span class="t">So, you can kind of tune this, and there are some graphs in the paper as well that kind of represent this trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=768" target="_blank">00:12:48.200</a></span> | <span class="t">But this is an important parameter to set when generating the memories themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=773" target="_blank">00:12:53.200</a></span> | <span class="t">Top K is probably the most important parameter to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=776" target="_blank">00:12:56.200</a></span> | <span class="t">So, this is the number of key value pairs or memories that each query token is allowed to retrieve and attend to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=783" target="_blank">00:13:03.200</a></span> | <span class="t">When your memory is quite long, kind of the more the better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=787" target="_blank">00:13:07.200</a></span> | <span class="t">But, again, yeah, this should be dynamically set based on how long your memory is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=794" target="_blank">00:13:14.200</a></span> | <span class="t">Okay, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=795" target="_blank">00:13:15.200</a></span> | <span class="t">So, lastly, we want to retrieve as much information as we can from the memory without confusing the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=803" target="_blank">00:13:23.200</a></span> | <span class="t">It's making an analogy back to kind of putting everything into context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=807" target="_blank">00:13:27.200</a></span> | <span class="t">We don't want to just throw everything in there because that will be confusing to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=811" target="_blank">00:13:31.200</a></span> | <span class="t">So, we have two different regularization techniques that we implement that we have found to be especially effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=817" target="_blank">00:13:37.200</a></span> | <span class="t">The first one is called similarity masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=820" target="_blank">00:13:40.200</a></span> | <span class="t">So, again, we retrieve these tokens based on similarity with our query token and the key that we are retrieving from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=828" target="_blank">00:13:48.200</a></span> | <span class="t">And so, we might say, like, well, if we don't hit some similarity threshold, like, we'll retrieve a lot of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=834" target="_blank">00:13:54.200</a></span> | <span class="t">But then if they, you know, if they're not at least, like, 0.25 similar, then we'll just throw them out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=838" target="_blank">00:13:58.200</a></span> | <span class="t">So, we can retrieve and then just mask the ones that end up being less important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=842" target="_blank">00:14:02.200</a></span> | <span class="t">Another important regularization technique, in particular for models that are trained using rope,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=849" target="_blank">00:14:09.200</a></span> | <span class="t">is to eliminate tokens from the memory that correspond to unknown tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=854" target="_blank">00:14:14.200</a></span> | <span class="t">So, especially if your data is super messy, a lot of the Wikipedia-based benchmarks are, like, really way more messy than I even knew before I started working on this stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=862" target="_blank">00:14:22.200</a></span> | <span class="t">They have a lot of, like, just unknown tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=865" target="_blank">00:14:25.200</a></span> | <span class="t">And so, they're kind of, like, poorly represented by the models often because they're unknown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=869" target="_blank">00:14:29.200</a></span> | <span class="t">They end up having a lot of matches with your query tokens, but then they're not actually containing a lot of useful information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=875" target="_blank">00:14:35.200</a></span> | <span class="t">So, we just eliminate those from the memory before we allow it to start retrieving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=879" target="_blank">00:14:39.200</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=880" target="_blank">00:14:40.200</a></span> | <span class="t">So, we have a whole collection of these models on Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=885" target="_blank">00:14:45.200</a></span> | <span class="t">All of the code is on GitHub as well as that data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=889" target="_blank">00:14:49.200</a></span> | <span class="t">And I encourage you all to read the paper if you're curious about more of the technical details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=893" target="_blank">00:14:53.200</a></span> | <span class="t">As I hope you can see here, it's actually pretty easy to use these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=896" target="_blank">00:14:56.200</a></span> | <span class="t">So, it's as simple as passing those memories in as inputs, as tokens, into the model during instantiation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=904" target="_blank">00:15:04.200</a></span> | <span class="t">You can dynamically change them after that as well, but it's the easiest way to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=908" target="_blank">00:15:08.200</a></span> | <span class="t">And then making sure your config is set up correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=911" target="_blank">00:15:11.200</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=912" target="_blank">00:15:12.200</a></span> | <span class="t">So, just to conclude here, I hope you all will take away that these new kinds of models achieve impressive performance on retrieval tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=922" target="_blank">00:15:22.200</a></span> | <span class="t">They enable these great new kinds of citations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=925" target="_blank">00:15:25.200</a></span> | <span class="t">They also enable this new kind of hallucination reduction technique, which is inspired by active learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=931" target="_blank">00:15:31.200</a></span> | <span class="t">They do not require fine tuning, unlike kind of long context methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=936" target="_blank">00:15:36.200</a></span> | <span class="t">And they can be easily run using our open source models and code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=941" target="_blank">00:15:41.200</a></span> | <span class="t">So, thanks so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=942" target="_blank">00:15:42.200</a></span> | <span class="t">And find me after some questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=944" target="_blank">00:15:44.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=945" target="_blank">00:15:45.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=945" target="_blank">00:15:45.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=946" target="_blank">00:15:46.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=946" target="_blank">00:15:46.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=947" target="_blank">00:15:47.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=947" target="_blank">00:15:47.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=948" target="_blank">00:15:48.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=949" target="_blank">00:15:49.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=949" target="_blank">00:15:49.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=950" target="_blank">00:15:50.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=951" target="_blank">00:15:51.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=952" target="_blank">00:15:52.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=953" target="_blank">00:15:53.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=954" target="_blank">00:15:54.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=955" target="_blank">00:15:55.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=956" target="_blank">00:15:56.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=957" target="_blank">00:15:57.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=958" target="_blank">00:15:58.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=958" target="_blank">00:15:58.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=959" target="_blank">00:15:59.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=959" target="_blank">00:15:59.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=960" target="_blank">00:16:00.200</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=cS6M6Jec0lU&t=962" target="_blank">00:16:02.260</a></span> | <span class="t">you</span></div></div></body></html>