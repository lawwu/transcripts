<html><head><title>Vision Transformers (ViT) Explained + Fine-tuning in Python</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Vision Transformers (ViT) Explained + Fine-tuning in Python</h2><a href="https://www.youtube.com/watch?v=qU7wO02urYU"><img src="https://i.ytimg.com/vi_webp/qU7wO02urYU/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=58">0:58</a> In this video<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=72">1:12</a> What are transformers and attention?<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=99">1:39</a> Attention explained simply<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=255">4:15</a> Attention used in CNNs<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=324">5:24</a> Transformers and attention<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=421">7:1</a> What vision transformer (ViT) does differently<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=448">7:28</a> Images to patch embeddings<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=502">8:22</a> 1. Building image patches<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=623">10:23</a> 2. Linear projection<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=657">10:57</a> 3. Learnable class embedding<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=810">13:30</a> 4. Adding positional embeddings<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=997">16:37</a> ViT implementation in python with Hugging Face<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1005">16:45</a> Packages, dataset, and Colab GPU<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1122">18:42</a> Initialize Hugging Face ViT Feature Extractor<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1368">22:48</a> Hugging Face Trainer setup<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1514">25:14</a> Training and CUDA device error<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1587">26:27</a> Evaluation and classification predictions with ViT<br><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1734">28:54</a> Final thoughts<br><br><div style="text-align: left;"><a href="./qU7wO02urYU.html">Whisper Transcript</a> | <a href="./transcript_qU7wO02urYU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Vision and language are the two big domains in machine learning. Two distinct disciplines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=6" target="_blank">00:00:06.320</a></span> | <span class="t">with their own problems, best practices and model architectures or at least that used to be the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=14" target="_blank">00:00:14.240</a></span> | <span class="t">The vision transformer or VIT marks the first step towards a merger of both fields into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=24" target="_blank">00:00:24.240</a></span> | <span class="t">unified discipline. For the first time in the history of machine learning we have a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=31" target="_blank">00:00:31.920</a></span> | <span class="t">model architecture that is on track to become the dominant model in both language and vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=39" target="_blank">00:00:39.760</a></span> | <span class="t">Before the vision transformer, transformers were known as those language models and nothing more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=47" target="_blank">00:00:47.040</a></span> | <span class="t">But since the introduction of the vision transformer there has been further work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=52" target="_blank">00:00:52.320</a></span> | <span class="t">that has almost solidified their position as state-of-the-art in vision. In this video we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=59" target="_blank">00:00:59.840</a></span> | <span class="t">going to dive into the vision transformer. We're going to explain what it is, how it works, why it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=67" target="_blank">00:01:07.360</a></span> | <span class="t">works and we're going to look at how we can actually use it and implement it with Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=72" target="_blank">00:01:12.400</a></span> | <span class="t">So let's get started with a very quick one-on-one introduction to transformers and the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=80" target="_blank">00:01:20.160</a></span> | <span class="t">mechanism. So transformers were introduced in 2017 in the pretty well-known paper called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=87" target="_blank">00:01:27.840</a></span> | <span class="t">"Attention is all you need". Transformers quite literally changed the entire landscape of NLP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=94" target="_blank">00:01:34.640</a></span> | <span class="t">and this was very much thanks to something called the attention mechanism. Now in NLP attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=103" target="_blank">00:01:43.360</a></span> | <span class="t">allows us to embed contextual meaning into the word level or sub-word level token embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=113" target="_blank">00:01:53.280</a></span> | <span class="t">within a model. So what I mean by that is say you have a sentence and you have two words in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=119" target="_blank">00:01:59.840</a></span> | <span class="t">that sentence that are very much related. Attention allows you to identify that relationship and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=127" target="_blank">00:02:07.840</a></span> | <span class="t">allow the model to understand those words with respect to one another within that greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=134" target="_blank">00:02:14.960</a></span> | <span class="t">sentence. Now this starts within the transformer model with tokens just being embedded into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=142" target="_blank">00:02:22.640</a></span> | <span class="t">vector space based purely on what that token is. So the token for bank will be mapped to a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=150" target="_blank">00:02:30.480</a></span> | <span class="t">vector that represents the word bank without any consideration of the words surrounding it. Now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=158" target="_blank">00:02:38.000</a></span> | <span class="t">with these token embeddings what we can do is calculate dot products between their embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=164" target="_blank">00:02:44.480</a></span> | <span class="t">and we will return a high score when they are aligned and a low score when they are not aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=171" target="_blank">00:02:51.360</a></span> | <span class="t">And as we do this within the attention mechanism we can essentially identify which words should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=178" target="_blank">00:02:58.080</a></span> | <span class="t">be placed closer together within that vector space. So for example if we had three sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=184" target="_blank">00:03:04.880</a></span> | <span class="t">a plane banks the grassy bank and the bank of England the initial embedding of that token bank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=192" target="_blank">00:03:12.960</a></span> | <span class="t">for all of those sentences is equal. But then through this encoder attention mechanism we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=201" target="_blank">00:03:21.040</a></span> | <span class="t">essentially map the token embedding bank closer to the vector space of the other relevant words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=208" target="_blank">00:03:28.720</a></span> | <span class="t">within each one of those sentences. So in the case of a plane banks what we would have is the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=216" target="_blank">00:03:36.480</a></span> | <span class="t">bank or banks being moved closer towards words like aeroplane, plane, airport, flight and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=224" target="_blank">00:03:44.800</a></span> | <span class="t">For the phrase a grassy bank we will find that the token embedding for bank gets moved towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=231" target="_blank">00:03:51.200</a></span> | <span class="t">the embedding space for grass, nature, fields. And for the bank of England we'll find that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=238" target="_blank">00:03:58.080</a></span> | <span class="t">word bank gets moved towards finance, money and so on. So as we go through these many encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=245" target="_blank">00:04:05.680</a></span> | <span class="t">blocks which contain the attention mechanism we are essentially embedding more contextual meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=252" target="_blank">00:04:12.000</a></span> | <span class="t">into those initial token embeddings. Now attention did find itself being used occasionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=259" target="_blank">00:04:19.840</a></span> | <span class="t">in convolutional neural networks which were the past state of the art in computer vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=265" target="_blank">00:04:25.600</a></span> | <span class="t">Generally speaking this has produced some benefit but it is somewhat limited. Attention is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=272" target="_blank">00:04:32.560</a></span> | <span class="t">a heavy operation when it comes to having a large number of items to compare because essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=279" target="_blank">00:04:39.920</a></span> | <span class="t">with attention you're comparing every item against every other item within your input sequence. So if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=285" target="_blank">00:04:45.920</a></span> | <span class="t">your input sequence is a even relatively large image and you're comparing pixels to pixels with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=293" target="_blank">00:04:53.680</a></span> | <span class="t">your attention mechanism the number of comparisons that you need to do becomes incredibly large very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=301" target="_blank">00:05:01.040</a></span> | <span class="t">very quickly. So in the case of convolutional neural networks attention can only really be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=306" target="_blank">00:05:06.880</a></span> | <span class="t">applied towards the later layers of the models where you basically have less activations being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=314" target="_blank">00:05:14.640</a></span> | <span class="t">compared after a few convolutions. Now that's better than nothing but it does limit the use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=320" target="_blank">00:05:20.880</a></span> | <span class="t">of attention because you can't use it throughout the entire network. Now transform models in NLP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=326" target="_blank">00:05:26.960</a></span> | <span class="t">have not had that limitation and can instead apply attention over many layers literally from the very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=334" target="_blank">00:05:34.240</a></span> | <span class="t">starting point of the model. Now the setup used by BERT, which is a again a very well-known</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=340" target="_blank">00:05:40.960</a></span> | <span class="t">transformer model, involves several encoder layers. Now within these encoder layers or encoder blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=347" target="_blank">00:05:47.280</a></span> | <span class="t">we have a few different things going on. There is a normalization component, a multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=354" target="_blank">00:05:54.400</a></span> | <span class="t">component, which is essentially many attention operations happening in parallel, and a multi-layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=362" target="_blank">00:06:02.000</a></span> | <span class="t">perceptron layer. Through each of these blocks we're just encoding more and more information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=367" target="_blank">00:06:07.120</a></span> | <span class="t">into our token embeddings and at the end of this process we get these super rich vector embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=374" target="_blank">00:06:14.800</a></span> | <span class="t">and these embeddings are the ultimate output of the core of a transform model including the vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=382" target="_blank">00:06:22.000</a></span> | <span class="t">transformer. And from there what we tend to find with transform models is that we add another few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=389" target="_blank">00:06:29.120</a></span> | <span class="t">layers onto the end which act as the head of the transformer which essentially encode or take these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=396" target="_blank">00:06:36.400</a></span> | <span class="t">vector embeddings, information rich embeddings, and translate them into predictions for a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=403" target="_blank">00:06:43.680</a></span> | <span class="t">task. So you might have a classification head or a NER head or a question answering head and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=411" target="_blank">00:06:51.120</a></span> | <span class="t">will all be slightly different in some way but at the core what they are doing is translating those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=417" target="_blank">00:06:57.200</a></span> | <span class="t">super rich information embeddings into some sort of meaningful prediction. Now the vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=422" target="_blank">00:07:02.960</a></span> | <span class="t">transformer actually works in the exact same way, the only difference is how we pre-process things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=429" target="_blank">00:07:09.600</a></span> | <span class="t">before they are fed into the vision transformer. So rather than with BERT and other language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=435" target="_blank">00:07:15.920</a></span> | <span class="t">transformers that consume word or sub-word tokens, the vision transformer consumes image patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=444" target="_blank">00:07:24.080</a></span> | <span class="t">Then the remainder transformer works in the exact same way. So let's take a look at how we go from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=450" target="_blank">00:07:30.720</a></span> | <span class="t">images to image patches and then after that into patch embeddings. The high level process for doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=459" target="_blank">00:07:39.120</a></span> | <span class="t">this is relatively simple. First we split an image into image patches. Two, we process those patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=466" target="_blank">00:07:46.720</a></span> | <span class="t">through a linear projection layer to get our initial patch embeddings. Then we pre-append something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=473" target="_blank">00:07:53.040</a></span> | <span class="t">called a class embedding to those patch embeddings and finally we sum the patch embeddings and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=479" target="_blank">00:07:59.440</a></span> | <span class="t">something called positional embedding. Now there's a lot of parallels with this process and what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=484" target="_blank">00:08:04.560</a></span> | <span class="t">see in the language and will relate to those where relevant. So after all these steps we have our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=492" target="_blank">00:08:12.240</a></span> | <span class="t">patch embeddings and we just process them in the exact same way that we would token embeddings with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=497" target="_blank">00:08:17.280</a></span> | <span class="t">a language transformer. But let's dive into each one of these steps in a little more detail. Our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=502" target="_blank">00:08:22.400</a></span> | <span class="t">first step is the transformation of our image into image patches. In NLP we actually do the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=510" target="_blank">00:08:30.960</a></span> | <span class="t">thing. We take a sentence and we translate it into a list of tokens. So in this respect images are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=519" target="_blank">00:08:39.040</a></span> | <span class="t">sentences and image patches are word or sub-word tokens. Now if we didn't create these image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=526" target="_blank">00:08:46.720</a></span> | <span class="t">patches we could alternatively feed in the full set of pixels from a image. But as I mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=533" target="_blank">00:08:53.760</a></span> | <span class="t">before that basically makes it so that we can't use attention because the calculation or the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=540" target="_blank">00:09:00.240</a></span> | <span class="t">of computations that we need to do to compare all images would be very restrictive on the size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=546" target="_blank">00:09:06.880</a></span> | <span class="t">images that we could input. We could only essentially input very very small images. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=551" target="_blank">00:09:11.360</a></span> | <span class="t">if we consider that attention requires the comparison of everything to everything else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=558" target="_blank">00:09:18.560</a></span> | <span class="t">and we're using pixels here. If we have a 224 by 224 pixel image that means we would have to perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=567" target="_blank">00:09:27.520</a></span> | <span class="t">224 to the power of 4 comparisons. Which is 2.5 billion comparisons. Which is pretty insane and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=578" target="_blank">00:09:38.240</a></span> | <span class="t">that's for a single attention layer. In transformers we have multiple attention layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=583" target="_blank">00:09:43.520</a></span> | <span class="t">So it's already just far too much. If instead we split our 224 by 224 pixel image into image patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=594" target="_blank">00:09:54.480</a></span> | <span class="t">where we have 14 by 14 pixel patches that would leave us with 256 of these patches. And with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=604" target="_blank">00:10:04.960</a></span> | <span class="t">a single attention layer requires a much more manageable 9.8 million comparisons. Which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=613" target="_blank">00:10:13.680</a></span> | <span class="t">a lot easier to do. With that we can have a huge number of attention layers and still not even get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=619" target="_blank">00:10:19.920</a></span> | <span class="t">close to the single attention layer with our full image. Now after building these image patches we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=626" target="_blank">00:10:26.560</a></span> | <span class="t">move on to the linear projection step. For this we use a linear projection layer which is simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=632" target="_blank">00:10:32.000</a></span> | <span class="t">going to map our image patch arrays into image patch vectors. By mapping these patches to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=642" target="_blank">00:10:42.720</a></span> | <span class="t">patch embeddings we are reformatting them into the correct dimensionality to be input into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=651" target="_blank">00:10:51.680</a></span> | <span class="t">our vision transformer. But we're not putting these into the vision transformer just yet because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=656" target="_blank">00:10:56.560</a></span> | <span class="t">there's two more steps. Our third step is the learnable embedding or the class token. Now this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=664" target="_blank">00:11:04.640</a></span> | <span class="t">is an idea that comes from BERT. So BERT introduced the use of something called a CLS or classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=673" target="_blank">00:11:13.600</a></span> | <span class="t">token. Now the CLS token was a special token pre-appended to every sentence that was input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=681" target="_blank">00:11:21.040</a></span> | <span class="t">into BERT. This CLS token was as with every other token converted into an embedding and passed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=688" target="_blank">00:11:28.160</a></span> | <span class="t">through several encoder layers. Now there are two things that make CLS special. First it does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=695" target="_blank">00:11:35.040</a></span> | <span class="t">represent a real word so it almost acts as like a blank slate being input into the model. And second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=704" target="_blank">00:11:44.240</a></span> | <span class="t">the CLS token embedding after the many encoder blocks is that embedding that is input into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=714" target="_blank">00:11:54.880</a></span> | <span class="t">classification head which is used as a part of the pre-training process. So essentially what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=722" target="_blank">00:12:02.080</a></span> | <span class="t">we end up doing there is we end up embedding like a general representation of the full sentence into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=729" target="_blank">00:12:09.200</a></span> | <span class="t">this single token embedding. Because in order for the model to make a good prediction about what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=736" target="_blank">00:12:16.000</a></span> | <span class="t">this sentence is it needs to have a general embedding of the whole sentence in that single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=742" target="_blank">00:12:22.400</a></span> | <span class="t">token. Because it's only that single token embedding that is passed into the classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=750" target="_blank">00:12:30.000</a></span> | <span class="t">head. Now the vision transformer applies the same logic and it adds something called a learnable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=756" target="_blank">00:12:36.320</a></span> | <span class="t">embedding or a class embedding to the embeddings as they are processed by the first layers of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=764" target="_blank">00:12:44.080</a></span> | <span class="t">model. And this learnable embedding is practically the same thing as the CLS token in BERT. Now it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=770" target="_blank">00:12:50.800</a></span> | <span class="t">also worth noting that it is potentially even more important for the vision transformer than it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=777" target="_blank">00:12:57.520</a></span> | <span class="t">for BERT. Because for BERT the main mode of pre-training is something called mass language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=783" target="_blank">00:13:03.200</a></span> | <span class="t">modeling which doesn't rely on the classification token. Whereas with the vision transformer the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=790" target="_blank">00:13:10.880</a></span> | <span class="t">ideal mode of pre-training is actually a classification task. So in that sense we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=798" target="_blank">00:13:18.640</a></span> | <span class="t">think of this CLS token or CLS embedding as actually being very critical for the overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=806" target="_blank">00:13:26.800</a></span> | <span class="t">performance and overall training of the vision transformer. Now the final set that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=812" target="_blank">00:13:32.960</a></span> | <span class="t">apply to our patch embeddings before they are actually fed into the model is we need to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=820" target="_blank">00:13:40.800</a></span> | <span class="t">something called the positional embeddings. Now positional embeddings are a common thing to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=827" target="_blank">00:13:47.680</a></span> | <span class="t">added to transformers. And that's because transformers by default don't actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=834" target="_blank">00:13:54.000</a></span> | <span class="t">any mechanism for tracking the position of inputs. So there's no order that is being considered. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=843" target="_blank">00:14:03.680</a></span> | <span class="t">that is difficult because when it comes to language and also vision, but let's think in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=851" target="_blank">00:14:11.200</a></span> | <span class="t">the sense of language for now, the order of words in a sentence is incredibly important. If you mix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=857" target="_blank">00:14:17.280</a></span> | <span class="t">up the order of words as a person it's hard to understand what this sentence is supposed to mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=863" target="_blank">00:14:23.760</a></span> | <span class="t">And it can even mean something completely different. So obviously the order of words is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=868" target="_blank">00:14:28.320</a></span> | <span class="t">super important and that applies as well to images. If we start mixing the image patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=875" target="_blank">00:14:35.120</a></span> | <span class="t">there's a good chance that we won't be able to understand what that image represents anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=880" target="_blank">00:14:40.080</a></span> | <span class="t">And in fact this is what we get with jigsaw puzzles. We get a ton of little image patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=887" target="_blank">00:14:47.040</a></span> | <span class="t">and we need to put them together in a certain order. And it takes people a long time to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=890" target="_blank">00:14:50.640</a></span> | <span class="t">out what that order actually is. So the order of our image patches is obviously quite important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=898" target="_blank">00:14:58.240</a></span> | <span class="t">but by default transformers don't have a way of handling this. So that's where the positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=904" target="_blank">00:15:04.000</a></span> | <span class="t">embeddings come in. For the vision transformer, these positional embeddings are learned embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=910" target="_blank">00:15:10.080</a></span> | <span class="t">that are summed with the incoming patch embeddings. Now, as I mentioned, these positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=917" target="_blank">00:15:17.600</a></span> | <span class="t">embeddings are learned. So during pre-training these are adjusted and what we can actually see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=923" target="_blank">00:15:23.680</a></span> | <span class="t">if we visualize this similarity or the cosine similarity between embeddings is that positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=930" target="_blank">00:15:30.480</a></span> | <span class="t">embeddings that are close to one another actually have a higher similarity. And in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=936" target="_blank">00:15:36.320</a></span> | <span class="t">positional embeddings that exist within the same row and the same column as one another also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=942" target="_blank">00:15:42.720</a></span> | <span class="t">a higher similarity. So it seems like there's this logical thing going on here with these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=950" target="_blank">00:15:50.480</a></span> | <span class="t">positional embeddings, whereas identifying patches that are within a similar area is pushing them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=956" target="_blank">00:15:56.320</a></span> | <span class="t">into a similar vector space and patches that are in a dissimilar area is pushing them away from each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=962" target="_blank">00:16:02.880</a></span> | <span class="t">other within that vector space. So there's a sense of locality being introduced within these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=968" target="_blank">00:16:08.400</a></span> | <span class="t">positional embeddings. Now, after adding our positional embeddings and patch embeddings together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=974" target="_blank">00:16:14.800</a></span> | <span class="t">we have our final patch embeddings, which are then fed into our vision transformer and they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=980" target="_blank">00:16:20.880</a></span> | <span class="t">processed through that sort of encoder attention mechanism that we described before, which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=986" target="_blank">00:16:26.160</a></span> | <span class="t">a typical transformer approach. Now, that is the logic behind vision transformer and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=994" target="_blank">00:16:34.320</a></span> | <span class="t">new innovations that it has brought. Now I want to describe or actually go through an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1000" target="_blank">00:16:40.720</a></span> | <span class="t">of an implementation of the vision transformer and how we can actually use it. Okay, so we start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1007" target="_blank">00:16:47.200</a></span> | <span class="t">by just installing any prerequisites that we have. So here we've got pip install datasets and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1013" target="_blank">00:16:53.760</a></span> | <span class="t">transformers and also PyTorch. So we run this and then what we want to do is download a dataset that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1022" target="_blank">00:17:02.400</a></span> | <span class="t">we can actually test all of this on and also fine tune with. So we're going to be using the CFAR-10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1028" target="_blank">00:17:08.880</a></span> | <span class="t">dataset. We're going to be getting that from HungFix datasets. So from datasets, import load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1034" target="_blank">00:17:14.560</a></span> | <span class="t">dataset. Let this run and we just run this. One thing just to check here before we go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1040" target="_blank">00:17:20.560</a></span> | <span class="t">everything is to make sure that we're using GPU. Save and we will have to rerun everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1048" target="_blank">00:17:28.320</a></span> | <span class="t">Okay, so after that's downloaded, we'll see that we have 50,000 images with classification labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1053" target="_blank">00:17:33.440</a></span> | <span class="t">within our training data. And we also download the test split as well. That has 10,000 of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1061" target="_blank">00:17:41.120</a></span> | <span class="t">And then what we want to do is we want to just have a look at the labels quickly. So let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1066" target="_blank">00:17:46.720</a></span> | <span class="t">what we have in there. So we have 10 labels. That's why it's called CFAR-10. And we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1073" target="_blank">00:17:53.200</a></span> | <span class="t">have 10 labels. That's why it's called CFAR-10. And of those, we have these particular classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1080" target="_blank">00:18:00.400</a></span> | <span class="t">within the dataset. Airplane, automobile, so on and so on. So from there, we can have a look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1086" target="_blank">00:18:06.240</a></span> | <span class="t">what is within a single item within that dataset. So we have this pill. So Python pill object is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1093" target="_blank">00:18:13.120</a></span> | <span class="t">essentially a image. And then also the label. Now that label corresponds to airplane here in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1100" target="_blank">00:18:20.480</a></span> | <span class="t">case, because it's number zero. And we can just check that. So run this. This is a Z. We can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1108" target="_blank">00:18:28.240</a></span> | <span class="t">really see it very well. It's very small, but that is an airplane. And we can actually map the label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1115" target="_blank">00:18:35.360</a></span> | <span class="t">So zero to labels.names in order to get the actual human readable class label. Okay, cool. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1123" target="_blank">00:18:43.200</a></span> | <span class="t">what we're going to do is we're going to load the Vision Transformer feature extractor. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1128" target="_blank">00:18:48.720</a></span> | <span class="t">we're going to be using this model here from the FaceHub. And we can actually see that over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1135" target="_blank">00:18:55.120</a></span> | <span class="t">So we have Google VIT Base Patch 16.225 in or IN 21K. Now what that means is we have patches that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1146" target="_blank">00:19:06.560</a></span> | <span class="t">are 16 by 16 pixels. They are being pulled or being built during pre-training at least by a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1154" target="_blank">00:19:14.720</a></span> | <span class="t">224 by 224 pixel image. And this IN 21K is just to say that this has been trained on or pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1165" target="_blank">00:19:25.040</a></span> | <span class="t">on the ImageNet 21K dataset. So that is the model we'll be using. And we use this feature extractor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1172" target="_blank">00:19:32.720</a></span> | <span class="t">which is almost like a pre-processor for this particular model. So we can run that and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1178" target="_blank">00:19:38.560</a></span> | <span class="t">will just download that feature extractor for us. That's pretty quick. And we can see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1184" target="_blank">00:19:44.480</a></span> | <span class="t">configuration within that feature extractor here. So what is this feature extractor doing exactly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1190" target="_blank">00:19:50.080</a></span> | <span class="t">It is taking an image. Our image can be any size and in a lot of different formats. And what it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1198" target="_blank">00:19:58.080</a></span> | <span class="t">going to do is just normalize and resize that image into something that we can then process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1203" target="_blank">00:20:03.200</a></span> | <span class="t">with our vision transformer. So we can see here that it will normalize the pixel values within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1209" target="_blank">00:20:09.360</a></span> | <span class="t">the image and it will resize the image as well. It will resize the image to this here, 224 by 224</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1217" target="_blank">00:20:17.680</a></span> | <span class="t">pixels. In terms of normalization, to normalize I'm using these values here for each of the color</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1223" target="_blank">00:20:23.040</a></span> | <span class="t">channels. So we have red, green, and blue. And yeah, that's pretty much, that's what it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1231" target="_blank">00:20:31.520</a></span> | <span class="t">to be doing. So if we take a look at the first image, we can use the feature extractor here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1236" target="_blank">00:20:36.800</a></span> | <span class="t">on our first image, which is that plane. And we're going to just return tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1242" target="_blank">00:20:42.240</a></span> | <span class="t">in using PyTorch because we'll be using PyTorch later on. So we run this and what we return is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1250" target="_blank">00:20:50.800</a></span> | <span class="t">a dictionary containing a single tensor or a single key value pair, which is pixel values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1255" target="_blank">00:20:55.760</a></span> | <span class="t">which maps to this single tensor here. And we can go down and we can have a look at the shape of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1262" target="_blank">00:21:02.160</a></span> | <span class="t">that. And we see that we have this 224 by 224 pixel image or pixel values tensor. Now that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1272" target="_blank">00:21:12.560</a></span> | <span class="t">different to the original image because the original image was train zero image or IMG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1282" target="_blank">00:21:22.560</a></span> | <span class="t">What's the shape of this? I think we can maybe do this. Maybe size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1295" target="_blank">00:21:35.120</a></span> | <span class="t">Okay. 32 by 32. So it's been resized up to 224 by 224, which is the format that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1303" target="_blank">00:21:43.200</a></span> | <span class="t">vision transformer needs. Now, when we are doing this, what we're going to want to do later on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1309" target="_blank">00:21:49.280</a></span> | <span class="t">is we're going to be training everything on GPU, not CPU. Now, by default, this tensor here is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1316" target="_blank">00:21:56.480</a></span> | <span class="t">CPU. We don't want that. We need to be using a GPU where possible. So we say, okay, if a CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1323" target="_blank">00:22:03.600</a></span> | <span class="t">enabled GPU is available, please use GPU. Okay. So we can see here, there is one available. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1330" target="_blank">00:22:10.000</a></span> | <span class="t">we're on Colab. So that's great. It means everything will be much faster. And the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1336" target="_blank">00:22:16.720</a></span> | <span class="t">why we need that is because here, we're going to need to move everything to that device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1343" target="_blank">00:22:23.040</a></span> | <span class="t">So what we'll do is here, as we use feature extractor here, we're going to say to device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1352" target="_blank">00:22:32.160</a></span> | <span class="t">That will just move everything to GPU for us. Okay. And then we use this with transform to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1358" target="_blank">00:22:38.240</a></span> | <span class="t">apply that to both the training and the testing data set. Or in reality, we're going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1363" target="_blank">00:22:43.280</a></span> | <span class="t">test data set more as a validation data set. Now, after all that, we're ready to move on to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1369" target="_blank">00:22:49.680</a></span> | <span class="t">the model fine tuning step. So with this, there are a few things we're going to need to define.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1375" target="_blank">00:22:55.760</a></span> | <span class="t">So training and testing data set, we've already done that. It's not a problem. Feature extractor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1380" target="_blank">00:23:00.000</a></span> | <span class="t">we have already done that as well. Not a problem. The model, we will define that. It's pretty easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1385" target="_blank">00:23:05.120</a></span> | <span class="t">Something called a collate function, evaluation metric, and some other training arguments. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1391" target="_blank">00:23:11.520</a></span> | <span class="t">let's start with the collate function. So here, this is essentially, when we're training with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1398" target="_blank">00:23:18.000</a></span> | <span class="t">the Hug & Face trainer, we need a way to collate all of our data into batches in a way that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1405" target="_blank">00:23:25.600</a></span> | <span class="t">sense, which requires this dictionary format. So each record is represented by dictionary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1413" target="_blank">00:23:33.120</a></span> | <span class="t">and each record contains inputs, which is the pixel values, and also the labels. So we run this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1419" target="_blank">00:23:39.520</a></span> | <span class="t">We then need to define our evaluation metric, which I'm using accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1425" target="_blank">00:23:45.680</a></span> | <span class="t">which is, you can read that if you want, but it's pretty straightforward. So we define that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1435" target="_blank">00:23:55.360</a></span> | <span class="t">And then we have all these training arguments. So these are essentially just the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1440" target="_blank">00:24:00.800</a></span> | <span class="t">parameters that we're going to use to actually train our model. So we have the batch size that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1446" target="_blank">00:24:06.960</a></span> | <span class="t">we want to use, where we're going to output the model, the number of training epochs that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1452" target="_blank">00:24:12.880</a></span> | <span class="t">to use, how often do we want to evaluate the model. So run it on the validation/test data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1460" target="_blank">00:24:20.160</a></span> | <span class="t">that we have, what learning rate do you want to use, and so on and so on. Rerun that. That just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1465" target="_blank">00:24:25.840</a></span> | <span class="t">sets up the configuration for our training. And then we move on to initializing our model. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1474" target="_blank">00:24:34.240</a></span> | <span class="t">this is just using the same thing that we had before. So when we had that feature extractor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1479" target="_blank">00:24:39.680</a></span> | <span class="t">we initialized it from pre-trained, and then we had the model name or path, model ID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1487" target="_blank">00:24:47.040</a></span> | <span class="t">So that is just the VIT patch 16224 that you saw before. One thing that we do need to add here is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1495" target="_blank">00:24:55.360</a></span> | <span class="t">because we're doing this VIT image classification, we need to specify the number of labels or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1501" target="_blank">00:25:01.680</a></span> | <span class="t">classes that will be output from that classification head, which in this case is 10 of those labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1508" target="_blank">00:25:08.400</a></span> | <span class="t">So we define that as well. We move the model to our GPU, and with that, we are ready to initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1516" target="_blank">00:25:16.400</a></span> | <span class="t">our trainer with all of those things that we just defined. So we run that, and then to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1523" target="_blank">00:25:23.680</a></span> | <span class="t">train the model, we do this. So trainer.train. After that, we can save the model, we can log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1530" target="_blank">00:25:30.800</a></span> | <span class="t">our metrics, save our metrics, and then just save the current state of the trainer at that point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1537" target="_blank">00:25:37.360</a></span> | <span class="t">So I'm going to run that very briefly and then stop. Okay, so it seems we're getting this error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1543" target="_blank">00:25:43.280</a></span> | <span class="t">which I think might be because we're trying to move the input tensors to GPU twice. So I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1550" target="_blank">00:25:50.960</a></span> | <span class="t">the trainer is doing it by default, but earlier on, we added the two device, so we need to remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1556" target="_blank">00:25:56.640</a></span> | <span class="t">that and run it again. So up here within preprocess, we just remove this, run it again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1565" target="_blank">00:26:05.200</a></span> | <span class="t">and then just rerun everything. Then pass everything to the trainer, and then try and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1571" target="_blank">00:26:11.600</a></span> | <span class="t">train again. Okay, it looks like we're having a little more luck with it this time. So we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1577" target="_blank">00:26:17.200</a></span> | <span class="t">that the model is training. Actually, it doesn't take too long, but what I'm going to do is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1583" target="_blank">00:26:23.440</a></span> | <span class="t">skip forward. So I'm going to stop this, and what we can do is you can run this to get your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1591" target="_blank">00:26:31.600</a></span> | <span class="t">evaluation metrics and view your evaluation metrics. Your model will be evaluating as it goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1597" target="_blank">00:26:37.520</a></span> | <span class="t">through your training set, thanks to the trainer. But if you would like to check again, you can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1605" target="_blank">00:26:45.040</a></span> | <span class="t">use this. But for now, let's just have a look at a specific example. So what we're going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1610" target="_blank">00:26:50.800</a></span> | <span class="t">load this image. I mean, I can't really tell what that image is. I think, so if we come down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1618" target="_blank">00:26:58.400</a></span> | <span class="t">it should be a cat, yeah? So run this, we can see that it's actually supposed to be a cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1623" target="_blank">00:27:03.120</a></span> | <span class="t">It's very blurry, I can't personally tell. But what we're going to do is load a fine-tuned model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1629" target="_blank">00:27:09.120</a></span> | <span class="t">So this is the model that has been fine-tuned using this same process. So we can download that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1636" target="_blank">00:27:16.000</a></span> | <span class="t">from Hugging Face Hub. We can also download the feature extractor, which we don't need to do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1643" target="_blank">00:27:23.840</a></span> | <span class="t">because it is actually using the same feature extractor, but in a real use case scenario,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1651" target="_blank">00:27:31.280</a></span> | <span class="t">you might actually just download everything from a particular model that is hosted within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1656" target="_blank">00:27:36.480</a></span> | <span class="t">the Hugging Face Hub. So this is what you would do, because it's not really fine-tuned. So run that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1664" target="_blank">00:27:44.640</a></span> | <span class="t">That will just download the fine-tuned model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1669" target="_blank">00:27:49.120</a></span> | <span class="t">And you can see here, we have the exact same feature extractor configuration there. We process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1679" target="_blank">00:27:59.360</a></span> | <span class="t">our image through the feature extractor, return PySource sensors, and then we say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1684" target="_blank">00:28:04.160</a></span> | <span class="t">with Torch Node Gradle, which is essentially to make sure that we're not updating the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1689" target="_blank">00:28:09.760</a></span> | <span class="t">of the model like we would during fine-tuning, because we're actually just making a prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1694" target="_blank">00:28:14.560</a></span> | <span class="t">here. We don't want to train anything. We use the model, process the inputs, and we extract the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1701" target="_blank">00:28:21.840</a></span> | <span class="t">logits, which is just the output activations. And what we want to do is take the argmax, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1708" target="_blank">00:28:28.480</a></span> | <span class="t">where the logits is the maximum value is basically highest probability that it is that class being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1715" target="_blank">00:28:35.280</a></span> | <span class="t">predicted. So we extract that, we get the labels, and then we output labels. And if we run that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1721" target="_blank">00:28:41.680</a></span> | <span class="t">we will see that we get cat. Okay, so it looks like we have fine-tuned a position transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1729" target="_blank">00:28:49.040</a></span> | <span class="t">using that same process, and the performance is pretty accurate. Now, before 2021, which really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1737" target="_blank">00:28:57.760</a></span> | <span class="t">not that long ago, transformers were known as just being those language models that they were not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1742" target="_blank">00:29:02.960</a></span> | <span class="t">used in anything else. But now, as we can see, we're actually able to use transformers and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1749" target="_blank">00:29:09.280</a></span> | <span class="t">really good results within the field of computer vision. And we're actually seeing this use in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1755" target="_blank">00:29:15.920</a></span> | <span class="t">lot of places. Vision transformer is a key component of the OpenAI's CLIP model, and OpenAI's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1762" target="_blank">00:29:22.560</a></span> | <span class="t">CLIP is a key component of all of the diffusion models that we've seen pop up everywhere, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1768" target="_blank">00:29:28.400</a></span> | <span class="t">the world is going crazy over them right now. Transformers are also a key component in Tesla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1775" target="_blank">00:29:35.360</a></span> | <span class="t">for self-driving. They are finding use in a huge number of places that would have just been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1783" target="_blank">00:29:43.680</a></span> | <span class="t">incredibly unexpected a year or even two, three years ago. And I think as time progresses, we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1790" target="_blank">00:29:50.320</a></span> | <span class="t">undoubtedly see more use of transformers within computer vision, and of course, the continued use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1796" target="_blank">00:29:56.240</a></span> | <span class="t">of transformers within the field of language. And they will undoubtedly become more and more unified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1803" target="_blank">00:30:03.920</a></span> | <span class="t">over time. For now, that's it for this video. I hope all of this has been useful and interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=qU7wO02urYU&t=1811" target="_blank">00:30:11.280</a></span> | <span class="t">So, thank you very much for watching, and I'll see you again in the next one. Bye.</span></div></div></body></html>