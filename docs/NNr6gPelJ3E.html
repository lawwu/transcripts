<html><head><title>Roman Yampolskiy: Dangers of Superintelligent AI | Lex Fridman Podcast #431</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Roman Yampolskiy: Dangers of Superintelligent AI | Lex Fridman Podcast #431</h2><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E"><img src="https://i.ytimg.com/vi/NNr6gPelJ3E/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=140">2:20</a> Existential risk of AGI<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=512">8:32</a> Ikigai risk<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1004">16:44</a> Suffering risk<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1219">20:19</a> Timeline to AGI<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1491">24:51</a> AGI turing test<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1814">30:14</a> Yann LeCun and open source AI<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2586">43:6</a> AI control<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2733">45:33</a> Social engineering<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2886">48:6</a> Fearmongering<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3477">57:57</a> AI deception<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3870">64:30</a> Verification<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4289">71:29</a> Self-improving AI<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5022">83:42</a> Pausing AI development<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5399">89:59</a> AI Safety<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5983">99:43</a> Current AI<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6305">105:5</a> Simulation<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6744">112:24</a> Aliens<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6837">113:57</a> Human mind<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7217">120:17</a> Neuralink<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7763">129:23</a> Hope for the future<br><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7998">133:18</a> Meaning of life<br><br><div style="text-align: left;"><a href="./NNr6gPelJ3E.html">Whisper Transcript</a> | <a href="./transcript_NNr6gPelJ3E.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">If we create general super-intelligences, I don't see a good outcome long-term for humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6" target="_blank">00:00:06.560</a></span> | <span class="t">So there is X risk. Existential risk, everyone's dead. There is S risk, suffering risks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=12" target="_blank">00:00:12.640</a></span> | <span class="t">where everyone wishes they were dead. We have also idea for I risk, Ikigai risks, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=18" target="_blank">00:00:18.240</a></span> | <span class="t">we lost our meaning. The systems can be more creative, they can do all the jobs. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=24" target="_blank">00:00:24.560</a></span> | <span class="t">obvious what you have to contribute to a world where super-intelligence exists. Of course, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=30" target="_blank">00:00:30.080</a></span> | <span class="t">can have all the variants you mentioned, where we are safe, we are kept alive, but we are not in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=35" target="_blank">00:00:35.840</a></span> | <span class="t">control. We are not deciding anything. We are like animals in a zoo. There is, again, possibilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=43" target="_blank">00:00:43.040</a></span> | <span class="t">we can come up with as very smart humans, and then possibilities something a thousand times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=49" target="_blank">00:00:49.120</a></span> | <span class="t">smarter can come up with for reasons we cannot comprehend. The following is a conversation with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=56" target="_blank">00:00:56.400</a></span> | <span class="t">Roman Yampolsky, an AI safety and security researcher and author of a new book titled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=62" target="_blank">00:01:02.560</a></span> | <span class="t">AI Unexplainable, Unpredictable, Uncontrollable. He argues that there's almost 100% chance that AGI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=71" target="_blank">00:01:11.280</a></span> | <span class="t">will eventually destroy human civilization. As an aside, let me say that we'll have many often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=78" target="_blank">00:01:18.320</a></span> | <span class="t">technical conversations on the topic of AI, often with engineers building the state-of-the-art AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=84" target="_blank">00:01:24.640</a></span> | <span class="t">systems. I would say those folks put the infamous P-Doom or the probability of AGI killing all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=90" target="_blank">00:01:30.560</a></span> | <span class="t">humans at around 1-20%, but it's also important to talk to folks who put that value at 70, 80, 90,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=100" target="_blank">00:01:40.160</a></span> | <span class="t">and, in the case of Roman, at 99.99 and many more nines percent. I'm personally excited for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=107" target="_blank">00:01:47.680</a></span> | <span class="t">future and believe it will be a good one, in part because of the amazing technological innovation we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=114" target="_blank">00:01:54.000</a></span> | <span class="t">humans create, but we must absolutely not do so with blinders on, ignoring the possible risks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=122" target="_blank">00:02:02.720</a></span> | <span class="t">including existential risks of those technologies. That's what this conversation is about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=130" target="_blank">00:02:10.080</a></span> | <span class="t">This is the Lex Friedman Podcast. To support it, please check out our sponsors in the description.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=135" target="_blank">00:02:15.600</a></span> | <span class="t">And now, dear friends, here's Roman Yampolsky. What to you is the probability that superintelligent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=143" target="_blank">00:02:23.920</a></span> | <span class="t">AI will destroy all human civilization? - What's the time frame?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=147" target="_blank">00:02:27.040</a></span> | <span class="t">- Let's say 100 years, in the next 100 years. - So the problem of controlling AGI or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=153" target="_blank">00:02:33.280</a></span> | <span class="t">superintelligence, in my opinion, is like a problem of creating a perpetual safety machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=159" target="_blank">00:02:39.680</a></span> | <span class="t">By analogy with perpetual motion machine, it's impossible. Yeah, we may succeed and do a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=166" target="_blank">00:02:46.320</a></span> | <span class="t">job with GPT-5, 6, 7, but they just keep improving, learning, eventually self-modifying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=176" target="_blank">00:02:56.560</a></span> | <span class="t">interacting with the environment, interacting with malevolent actors. The difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=183" target="_blank">00:03:03.360</a></span> | <span class="t">cybersecurity, narrow AI safety, and safety for general AI for superintelligence is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=190" target="_blank">00:03:10.160</a></span> | <span class="t">we don't get a second chance. With cybersecurity, somebody hacks your account, what's the big deal?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=194" target="_blank">00:03:14.880</a></span> | <span class="t">You get a new password, new credit card, you move on. Here, if we're talking about existential risks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=201" target="_blank">00:03:21.520</a></span> | <span class="t">you only get one chance. So you're really asking me, what are the chances that we'll create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=206" target="_blank">00:03:26.880</a></span> | <span class="t">the most complex software ever on the first try with zero bugs, and it will continue to have zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=214" target="_blank">00:03:34.000</a></span> | <span class="t">bugs for 100 years or more? - So there is an incremental improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=221" target="_blank">00:03:41.360</a></span> | <span class="t">of systems leading up to AGI. To you, it doesn't matter if we can keep those safe. There's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=229" target="_blank">00:03:49.200</a></span> | <span class="t">to be one level of system at which you cannot possibly control it. - I don't think we so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=239" target="_blank">00:03:59.200</a></span> | <span class="t">have made any system safe. At the level of capability they display, they already have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=246" target="_blank">00:04:06.080</a></span> | <span class="t">made mistakes, we had accidents, they've been jailbroken. I don't think there is a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=252" target="_blank">00:04:12.720</a></span> | <span class="t">large language model today which no one was successful at making do something developers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=258" target="_blank">00:04:18.960</a></span> | <span class="t">didn't intend it to do. - But there's a difference between getting it to do something unintended,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=264" target="_blank">00:04:24.560</a></span> | <span class="t">getting it to do something that's painful, costly, destructive, and something that's destructive to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=269" target="_blank">00:04:29.760</a></span> | <span class="t">the level of hurting billions of people, or hundreds of millions of people, billions of people,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=275" target="_blank">00:04:35.280</a></span> | <span class="t">or the entirety of human civilization. That's a big leap. - Exactly, but the systems we have today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=281" target="_blank">00:04:41.280</a></span> | <span class="t">have capability of causing X amount of damage. So then they fail, that's all we get. If we develop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=287" target="_blank">00:04:47.680</a></span> | <span class="t">systems capable of impacting all of humanity, all of universe, the damage is proportionate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=295" target="_blank">00:04:55.040</a></span> | <span class="t">- What to you are the possible ways that such kind of mass murder of humans can happen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=302" target="_blank">00:05:02.800</a></span> | <span class="t">- That's always a wonderful question. So one of the chapters in my new book is about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=307" target="_blank">00:05:07.840</a></span> | <span class="t">unpredictability. I argue that we cannot predict what a smarter system will do. So you're really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=313" target="_blank">00:05:13.360</a></span> | <span class="t">not asking me how superintelligence will kill everyone, you're asking me how I would do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=318" target="_blank">00:05:18.240</a></span> | <span class="t">And I think it's not that interesting. I can tell you about the standard, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=322" target="_blank">00:05:22.480</a></span> | <span class="t">nanotech, synthetic, bionuclear. Superintelligence will come up with something completely new,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=327" target="_blank">00:05:27.920</a></span> | <span class="t">completely super. We may not even recognize that as a possible path to achieve that goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=335" target="_blank">00:05:35.440</a></span> | <span class="t">- So there's like an unlimited level of creativity in terms of how humans could be killed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=341" target="_blank">00:05:41.680</a></span> | <span class="t">But, you know, we could still investigate possible ways of doing it. Not how to do it, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=349" target="_blank">00:05:49.680</a></span> | <span class="t">at the end, what is the methodology that does it? You know, shutting off the power,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=355" target="_blank">00:05:55.040</a></span> | <span class="t">and then humans start killing each other maybe because the resources are really constrained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=361" target="_blank">00:06:01.440</a></span> | <span class="t">And then there's the actual use of weapons, like nuclear weapons, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=364" target="_blank">00:06:04.320</a></span> | <span class="t">developing artificial pathogens, viruses, that kind of stuff. We could still kind of think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">through that and defend against it, right? There's a ceiling to the creativity of mass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=376" target="_blank">00:06:16.880</a></span> | <span class="t">murder of humans here, right? The options are limited. - They are limited by how imaginative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=382" target="_blank">00:06:22.720</a></span> | <span class="t">we are. If you are that much smarter, that much more creative, you are capable of thinking across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=387" target="_blank">00:06:27.600</a></span> | <span class="t">multiple domains, do novel research in physics and biology, you may not be limited by those tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=393" target="_blank">00:06:33.520</a></span> | <span class="t">If squirrels were planning to kill humans, they would have a set of possible ways of doing it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=399" target="_blank">00:06:39.200</a></span> | <span class="t">but they would never consider things we can come up with. - So are you thinking about mass murder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=403" target="_blank">00:06:43.760</a></span> | <span class="t">and destruction of human civilization, or are you thinking of with squirrels, you put them in a zoo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=408" target="_blank">00:06:48.720</a></span> | <span class="t">and they don't really know they're in a zoo? If we just look at the entire set of undesirable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=412" target="_blank">00:06:52.560</a></span> | <span class="t">trajectories, majority of them are not going to be death. Most of them are going to be just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=419" target="_blank">00:06:59.280</a></span> | <span class="t">things like Brave New World, where the squirrels are fed dopamine, and they're all doing some kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=428" target="_blank">00:07:08.880</a></span> | <span class="t">of fun activity, and the fire, the soul of humanity is lost because of the drug that's fed to it. Or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=436" target="_blank">00:07:16.640</a></span> | <span class="t">like literally in a zoo, we're in a zoo, we're doing our thing, we're like playing a game of Sims,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=442" target="_blank">00:07:22.160</a></span> | <span class="t">and the actual players playing that game are AI systems. Those are all undesirable because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=449" target="_blank">00:07:29.280</a></span> | <span class="t">sort of the free will, the fire of human consciousness is dimmed through that process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=455" target="_blank">00:07:35.360</a></span> | <span class="t">but it's not killing humans. So are you thinking about that, or is the biggest concern literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=463" target="_blank">00:07:43.280</a></span> | <span class="t">the extinctions of humans? - I think about a lot of things. So there is X risk, existential risk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=469" target="_blank">00:07:49.520</a></span> | <span class="t">everyone's dead. There is S risk, suffering risks, where everyone wishes they were dead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=474" target="_blank">00:07:54.640</a></span> | <span class="t">We have also idea for I risk, Ikigai risks, where we lost our meaning. The systems can be more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=482" target="_blank">00:08:02.000</a></span> | <span class="t">creative, they can do all the jobs. It's not obvious what you have to contribute to a world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=487" target="_blank">00:08:07.520</a></span> | <span class="t">where superintelligence exists. Of course, you can have all the variants you mentioned, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=493" target="_blank">00:08:13.280</a></span> | <span class="t">we are safe, we are kept alive, but we are not in control, we are not deciding anything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=498" target="_blank">00:08:18.240</a></span> | <span class="t">we are like animals in a zoo. There is, again, possibilities we can come up with as very smart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=505" target="_blank">00:08:25.280</a></span> | <span class="t">humans, and then possibilities something 1,000 times smarter can come up with for reasons we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=511" target="_blank">00:08:31.600</a></span> | <span class="t">cannot comprehend. - I would love to sort of dig into each of those, X risk, S risk, and I risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=517" target="_blank">00:08:37.840</a></span> | <span class="t">So can you like linger on I risk? What is that? - So Japanese concept of Ikigai, you find something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=525" target="_blank">00:08:45.440</a></span> | <span class="t">which allows you to make money, you are good at it, and the society says, "We need it." So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=531" target="_blank">00:08:51.680</a></span> | <span class="t">you have this awesome job, you are a podcaster, gives you a lot of meaning, you have a good life,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=538" target="_blank">00:08:58.640</a></span> | <span class="t">I assume you're happy. That's what we want most people to find, to have. For many intellectuals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=546" target="_blank">00:09:06.160</a></span> | <span class="t">it is their occupation which gives them a lot of meaning. I am a researcher, philosopher, scholar,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=552" target="_blank">00:09:12.560</a></span> | <span class="t">that means something to me. In a world where an artist is not feeling appreciated because his art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=560" target="_blank">00:09:20.080</a></span> | <span class="t">is just not competitive with what is produced by machines, or a writer, or scientist will lose a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=568" target="_blank">00:09:28.640</a></span> | <span class="t">lot of that. And at the lower level, we're talking about complete technological unemployment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=574" target="_blank">00:09:34.800</a></span> | <span class="t">We're not losing 10% of jobs, we're losing all jobs. What do people do with all that free time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=580" target="_blank">00:09:40.160</a></span> | <span class="t">What happens when everything society is built on is completely modified in one generation? It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=588" target="_blank">00:09:48.000</a></span> | <span class="t">a slow process where we get to kind of figure out how to live that new lifestyle, but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=594" target="_blank">00:09:54.160</a></span> | <span class="t">pretty quick. - In that world, can't humans do what humans currently do with chess, play each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=601" target="_blank">00:10:01.440</a></span> | <span class="t">have tournaments, even though AI systems are far superior at this time in chess? So we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=608" target="_blank">00:10:08.320</a></span> | <span class="t">create artificial games, or for us, they're real. Like the Olympics, we do all kinds of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=614" target="_blank">00:10:14.400</a></span> | <span class="t">competitions and have fun, maximize the fun, and let the AI focus on the productivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=624" target="_blank">00:10:24.000</a></span> | <span class="t">- It's an option, I have a paper where I try to solve the value alignment problem for multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=629" target="_blank">00:10:29.440</a></span> | <span class="t">agents. And the solution to avoid compromise is to give everyone a personal virtual universe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=635" target="_blank">00:10:35.360</a></span> | <span class="t">You can do whatever you want in that world. You could be king, you could be slave, you decide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=639" target="_blank">00:10:39.840</a></span> | <span class="t">what happens. So it's basically a glorified video game where you get to enjoy yourself and someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=645" target="_blank">00:10:45.360</a></span> | <span class="t">else takes care of your needs, and the substrate alignment is the only thing we need to solve. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=651" target="_blank">00:10:51.840</a></span> | <span class="t">don't have to get eight billion humans to agree on anything. - So okay, so why is that not a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=658" target="_blank">00:10:58.880</a></span> | <span class="t">likely outcome? Why can't AI systems create video games for us to lose ourselves in, each with an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=665" target="_blank">00:11:05.920</a></span> | <span class="t">individual video game universe? - Some people say that's what happened,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=670" target="_blank">00:11:10.000</a></span> | <span class="t">we're in a simulation. - And we're playing that video game,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=673" target="_blank">00:11:13.520</a></span> | <span class="t">and now we're creating, what, maybe we're creating artificial threats for ourselves to be scared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=679" target="_blank">00:11:19.840</a></span> | <span class="t">about, 'cause fear is really exciting. It allows us to play the video game more vigorously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=685" target="_blank">00:11:25.440</a></span> | <span class="t">- And some people choose to play on a more difficult level with more constraints. Some say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=690" target="_blank">00:11:30.880</a></span> | <span class="t">okay, I'm just gonna enjoy the game, high privilege level. Absolutely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=694" target="_blank">00:11:34.720</a></span> | <span class="t">- So okay, what was that paper on multi-agent value alignment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=698" target="_blank">00:11:38.240</a></span> | <span class="t">- Personal universes. Personal universes. - So that's one of the possible outcomes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=704" target="_blank">00:11:44.560</a></span> | <span class="t">But what in general is the idea of the paper? So it's looking at multiple agents that are human,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=709" target="_blank">00:11:49.600</a></span> | <span class="t">AI, like a hybrid system where there's humans and AIs? Or is it looking at humans or just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=714" target="_blank">00:11:54.800</a></span> | <span class="t">intelligent agents? - In order to solve value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=716" target="_blank">00:11:56.800</a></span> | <span class="t">alignment problem, I'm trying to formalize it a little better. Usually we're talking about getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=722" target="_blank">00:12:02.000</a></span> | <span class="t">AIs to do what we want, which is not well-defined. Are we talking about creator of a system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=728" target="_blank">00:12:08.000</a></span> | <span class="t">owner of that AI, humanity as a whole? But we don't agree on much. There is no universally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=735" target="_blank">00:12:15.760</a></span> | <span class="t">accepted ethics, morals across cultures, religions. People have individually very different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=740" target="_blank">00:12:20.960</a></span> | <span class="t">preferences politically and such. So even if we somehow managed all the other aspects of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=746" target="_blank">00:12:26.880</a></span> | <span class="t">programming those fuzzy concepts in, getting AI to follow them closely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=750" target="_blank">00:12:30.880</a></span> | <span class="t">we don't agree on what to program in. So my solution was, okay, we don't have to compromise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=756" target="_blank">00:12:36.320</a></span> | <span class="t">on room temperature. You have your universe, I have mine, whatever you want. And if you like me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=761" target="_blank">00:12:41.680</a></span> | <span class="t">you can invite me to visit your universe. We don't have to be independent, but the point is you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=766" target="_blank">00:12:46.720</a></span> | <span class="t">be. And virtual reality is getting pretty good. It's gonna hit a point where you can't tell that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=770" target="_blank">00:12:50.880</a></span> | <span class="t">difference. And if you can't tell if it's real or not, well, what's the difference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=774" target="_blank">00:12:54.720</a></span> | <span class="t">- So basically, give up on value alignment. Create an entire, it's like the multiverse theory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=781" target="_blank">00:13:01.040</a></span> | <span class="t">It's just create an entire universe for you with your values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=784" target="_blank">00:13:04.240</a></span> | <span class="t">- You still have to align with that individual. They have to be happy in that simulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=789" target="_blank">00:13:09.360</a></span> | <span class="t">But it's a much easier problem to align with one agent versus eight billion agents plus animals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=794" target="_blank">00:13:14.640</a></span> | <span class="t">aliens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=795" target="_blank">00:13:15.120</a></span> | <span class="t">- So you convert the multi-agent problem into a single-agent problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=799" target="_blank">00:13:19.120</a></span> | <span class="t">- I'm trying to do that, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=801" target="_blank">00:13:21.280</a></span> | <span class="t">- Okay. Is there any way to, so, okay, that's giving up on the value alignment problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=809" target="_blank">00:13:29.760</a></span> | <span class="t">Well, is there any way to solve the value alignment problem where there's a bunch of humans,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=815" target="_blank">00:13:35.040</a></span> | <span class="t">multiple humans, tens of humans, or eight billion humans that have very different set of values?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=821" target="_blank">00:13:41.280</a></span> | <span class="t">- It seems contradictory. I haven't seen anyone explain what it means outside of kinda</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=828" target="_blank">00:13:48.400</a></span> | <span class="t">words which pack a lot, make it good, make it desirable, make it something they don't regret.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=835" target="_blank">00:13:55.360</a></span> | <span class="t">But how do you specifically formalize those notions? How do you program them in?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=839" target="_blank">00:13:59.600</a></span> | <span class="t">I haven't seen anyone make progress on that so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=842" target="_blank">00:14:02.720</a></span> | <span class="t">- But isn't that the whole optimization journey that we're doing as a human civilization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=847" target="_blank">00:14:07.680</a></span> | <span class="t">We're looking at geopolitics. Nations are in a state of anarchy with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=853" target="_blank">00:14:13.920</a></span> | <span class="t">They start wars, there's conflict, and oftentimes they have very different views of what is good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=862" target="_blank">00:14:22.000</a></span> | <span class="t">and what is evil. Isn't that what we're trying to figure out, just together, trying to converge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=867" target="_blank">00:14:27.600</a></span> | <span class="t">towards that? So we're essentially trying to solve the value alignment problem with humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=871" target="_blank">00:14:31.360</a></span> | <span class="t">- Right, but the examples you gave, some of them are, for example, two different religions saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=876" target="_blank">00:14:36.640</a></span> | <span class="t">this is our holy site, and we are not willing to compromise it in any way. If you can make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=883" target="_blank">00:14:43.360</a></span> | <span class="t">two holy sites in virtual worlds, you solve the problem. But if you only have one, it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=887" target="_blank">00:14:47.360</a></span> | <span class="t">divisible, you're kinda stuck there. - But what if we want to be at tension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=891" target="_blank">00:14:51.520</a></span> | <span class="t">with each other? And through that tension, we understand ourselves and we understand the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=898" target="_blank">00:14:58.160</a></span> | <span class="t">So that's the intellectual journey we're on as a human civilization, is we create intellectual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=905" target="_blank">00:15:05.680</a></span> | <span class="t">and physical conflict, and through that, figure stuff out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=908" target="_blank">00:15:08.240</a></span> | <span class="t">- If we go back to that idea of simulation, and this is entertainment kinda giving meaning to us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=914" target="_blank">00:15:14.640</a></span> | <span class="t">the question is how much suffering is reasonable for a video game? So yeah, I don't mind a video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=920" target="_blank">00:15:20.160</a></span> | <span class="t">game where I get haptic feedback, there is a little bit of shaking, maybe I'm a little scared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=925" target="_blank">00:15:25.360</a></span> | <span class="t">I don't want a game where kids are tortured, literally. That seems unethical, at least by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=932" target="_blank">00:15:32.880</a></span> | <span class="t">our human standards. - Are you suggesting it's possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=935" target="_blank">00:15:35.840</a></span> | <span class="t">to remove suffering, if we're looking at human civilization as an optimization problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=939" target="_blank">00:15:39.920</a></span> | <span class="t">- So we know there are some humans who, because of a mutation, don't experience physical pain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=946" target="_blank">00:15:46.560</a></span> | <span class="t">So at least physical pain can be mutated out, re-engineered out. Suffering, in terms of meaning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=955" target="_blank">00:15:55.280</a></span> | <span class="t">like you burned the only copy of my book, is a little harder. But even there, you can manipulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=960" target="_blank">00:16:00.960</a></span> | <span class="t">your hedonic set point, you can change defaults, you can reset. Problem with that is, if you start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=967" target="_blank">00:16:07.360</a></span> | <span class="t">messing with your reward channel, you start wireheading, and end up blessing out a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=974" target="_blank">00:16:14.640</a></span> | <span class="t">too much. - Well, that's the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=977" target="_blank">00:16:17.200</a></span> | <span class="t">Would you really want to live in a world where there's no suffering? That's a dark question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=982" target="_blank">00:16:22.240</a></span> | <span class="t">But is there some level of suffering that reminds us of what this is all for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=988" target="_blank">00:16:28.800</a></span> | <span class="t">- I think we need that, but I would change the overall range. So right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=994" target="_blank">00:16:34.160</a></span> | <span class="t">it's negative infinity to kind of positive infinity, pain-pleasure axis. I would make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=998" target="_blank">00:16:38.800</a></span> | <span class="t">it like zero to positive infinity, and being unhappy is like, I'm close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1003" target="_blank">00:16:43.040</a></span> | <span class="t">- Okay, so what's the S-risk? What are the possible things that you're imagining with S-risk? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1009" target="_blank">00:16:49.200</a></span> | <span class="t">mass suffering of humans, what are we talking about there, caused by AGI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1014" target="_blank">00:16:54.560</a></span> | <span class="t">- So there are many malevolent actors. We can talk about psychopaths, crazies, hackers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1021" target="_blank">00:17:01.360</a></span> | <span class="t">doomsday cults. We know from history, they tried killing everyone. They tried on purpose to cause</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1027" target="_blank">00:17:07.280</a></span> | <span class="t">maximum amount of damage, terrorism. What if someone malevolent wants on purpose to torture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1033" target="_blank">00:17:13.440</a></span> | <span class="t">all humans as long as possible? You solve aging, so now you have functional immortality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1040" target="_blank">00:17:20.880</a></span> | <span class="t">and you just try to be as creative as you can. - Do you think there is actually people in human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1046" target="_blank">00:17:26.480</a></span> | <span class="t">history that tried to literally maximize human suffering? In just studying people who have done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1052" target="_blank">00:17:32.560</a></span> | <span class="t">evil in the world, it seems that they think that they're doing good, and it doesn't seem like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1057" target="_blank">00:17:37.840</a></span> | <span class="t">they're trying to maximize suffering. They just cause a lot of suffering as a side effect of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1065" target="_blank">00:17:45.360</a></span> | <span class="t">doing what they think is good. - So there are different malevolent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1069" target="_blank">00:17:49.120</a></span> | <span class="t">agents. Some may be just gaining personal benefit and sacrificing others to that cause. Others,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1076" target="_blank">00:17:56.240</a></span> | <span class="t">we know for a fact, are trying to kill as many people as possible. When we look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1080" target="_blank">00:18:00.080</a></span> | <span class="t">recent school shootings, if they had more capable weapons, they would take out not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1085" target="_blank">00:18:05.840</a></span> | <span class="t">dozens, but thousands, millions, billions. - Well, we don't know that, but that is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1096" target="_blank">00:18:16.800</a></span> | <span class="t">terrifying possibility, and we don't want to find out. Like if terrorists had access to nuclear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1104" target="_blank">00:18:24.240</a></span> | <span class="t">weapons, how far would they go? Is there a limit to what they're willing to do? In your senses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1113" target="_blank">00:18:33.840</a></span> | <span class="t">there are some malevolent actors where there's no limit. - There is mental diseases where people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1121" target="_blank">00:18:41.520</a></span> | <span class="t">don't have empathy, don't have this human quality of understanding suffering in others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1129" target="_blank">00:18:49.040</a></span> | <span class="t">- And then there's also a set of beliefs where you think you're doing good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1132" target="_blank">00:18:52.400</a></span> | <span class="t">by killing a lot of humans. - Again, I would like to assume that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1138" target="_blank">00:18:58.720</a></span> | <span class="t">normal people never think like that. It's always some sort of psychopaths, but yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1143" target="_blank">00:19:03.120</a></span> | <span class="t">- And to you, AGI systems can carry that and be more competent at executing that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1150" target="_blank">00:19:10.800</a></span> | <span class="t">- They can certainly be more creative. They can understand human biology better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1155" target="_blank">00:19:15.840</a></span> | <span class="t">understand our molecular structure, genome. Again, a lot of times, torture ends,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1163" target="_blank">00:19:23.680</a></span> | <span class="t">then individual dies. That limit can be removed as well. - So if we're actually looking at X-risk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1170" target="_blank">00:19:30.000</a></span> | <span class="t">and S-risk, as the systems get more and more intelligent, don't you think it's possible to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1175" target="_blank">00:19:35.840</a></span> | <span class="t">anticipate the ways they can do it and defend against it like we do with the cyber security,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1181" target="_blank">00:19:41.040</a></span> | <span class="t">with the new security systems? - Right, we can definitely keep up for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1185" target="_blank">00:19:45.440</a></span> | <span class="t">while. I'm saying you cannot do it indefinitely. At some point, the cognitive gap is too big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1192" target="_blank">00:19:52.400</a></span> | <span class="t">the surface you have to defend is infinite, but attackers only need to find one exploit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1200" target="_blank">00:20:00.400</a></span> | <span class="t">- So to you, eventually, this is, we're heading off a cliff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1204" target="_blank">00:20:04.480</a></span> | <span class="t">- If we create general super intelligences, I don't see a good outcome long-term for humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1211" target="_blank">00:20:11.520</a></span> | <span class="t">The only way to win this game is not to play it. - Okay, well, we'll talk about possible solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1216" target="_blank">00:20:16.720</a></span> | <span class="t">and what not playing it means, but what are the possible timelines here to you? What are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1222" target="_blank">00:20:22.240</a></span> | <span class="t">we talking about? We're talking about a set of years, decades, centuries. What do you think?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1227" target="_blank">00:20:27.280</a></span> | <span class="t">- I don't know for sure. The prediction markets right now are saying 2026 for AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1232" target="_blank">00:20:32.640</a></span> | <span class="t">I heard the same thing from CEO of Anthropic, DeepMind, so maybe we're two years away,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1238" target="_blank">00:20:38.480</a></span> | <span class="t">which seems very soon, given we don't have a working safety mechanism in place or even a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1245" target="_blank">00:20:45.360</a></span> | <span class="t">prototype for one, and there are people trying to accelerate those timelines because they feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1250" target="_blank">00:20:50.000</a></span> | <span class="t">we're not getting there quick enough. - Well, what do you think they mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1252" target="_blank">00:20:52.880</a></span> | <span class="t">when they say AGI? - So the definitions we used to have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1256" target="_blank">00:20:56.800</a></span> | <span class="t">and people are modifying them a little bit lately, artificial general intelligence was a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1262" target="_blank">00:21:02.320</a></span> | <span class="t">capable of performing in any domain a human could perform, so kind of you're creating this average</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1269" target="_blank">00:21:09.120</a></span> | <span class="t">artificial person. They can do cognitive labor, physical labor, where you can get another human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1274" target="_blank">00:21:14.320</a></span> | <span class="t">to do it. Superintelligence was defined as a system which is superior to all humans in all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1279" target="_blank">00:21:19.120</a></span> | <span class="t">domains. Now people are starting to refer to AGI as if it's superintelligence. I made a post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1285" target="_blank">00:21:25.840</a></span> | <span class="t">recently where I argued, for me at least, if you average out over all the common human tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1291" target="_blank">00:21:31.920</a></span> | <span class="t">those systems are already smarter than an average human. So under that definition, we have it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1298" target="_blank">00:21:38.800</a></span> | <span class="t">Shane Legg has this definition of where you're trying to win in all domains. That's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1303" target="_blank">00:21:43.440</a></span> | <span class="t">intelligence is. Now, are they smarter than elite individuals in certain domains? Of course not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1309" target="_blank">00:21:49.280</a></span> | <span class="t">They're not there yet. But the progress is exponential. - See, I'm much more concerned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1315" target="_blank">00:21:55.440</a></span> | <span class="t">about social engineering. So to me, AI's ability to do something in the physical world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1323" target="_blank">00:22:03.440</a></span> | <span class="t">like the lowest hanging fruit, the easiest set of methods, is by just getting humans to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1332" target="_blank">00:22:12.720</a></span> | <span class="t">It's going to be much harder to be the kind of viruses that take over the minds of robots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1339" target="_blank">00:22:19.840</a></span> | <span class="t">that, where the robots are executing the commands. It just seems like humans,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1344" target="_blank">00:22:24.240</a></span> | <span class="t">social engineering of humans, is much more likely. - That would be enough to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1348" target="_blank">00:22:28.000</a></span> | <span class="t">bootstrap the whole process. - Okay, just to linger on the term AGI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1354" target="_blank">00:22:34.160</a></span> | <span class="t">what to you is the difference between AGI and human-level intelligence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1357" target="_blank">00:22:37.760</a></span> | <span class="t">- Human-level is general in the domain of expertise of humans. We know how to do human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1364" target="_blank">00:22:44.480</a></span> | <span class="t">things. I don't speak dog language. I should be able to pick it up if I'm a general intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1369" target="_blank">00:22:49.760</a></span> | <span class="t">It's kind of inferior animal. I should be able to learn that skill, but I can't. A general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1375" target="_blank">00:22:55.280</a></span> | <span class="t">intelligence, truly universal general intelligence, should be able to do things like that humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1379" target="_blank">00:22:59.760</a></span> | <span class="t">cannot do. - To be able to talk to animals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1381" target="_blank">00:23:01.920</a></span> | <span class="t">for example. - To solve pattern recognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1384" target="_blank">00:23:04.160</a></span> | <span class="t">problems of that type, to do other similar things outside of our domain of expertise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1392" target="_blank">00:23:12.960</a></span> | <span class="t">because it's just not the world we live in. - If we just look at the space of cognitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1399" target="_blank">00:23:19.520</a></span> | <span class="t">abilities we have, I just would love to understand what the limits are beyond which an AGI system can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1405" target="_blank">00:23:25.600</a></span> | <span class="t">reach. What does that look like? What about actual mathematical thinking or scientific innovation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1414" target="_blank">00:23:34.800</a></span> | <span class="t">that kind of stuff? - We know calculators are smarter than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1419" target="_blank">00:23:39.280</a></span> | <span class="t">humans in that narrow domain of addition. - But is it humans plus tools versus AGI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1427" target="_blank">00:23:47.440</a></span> | <span class="t">or just human, raw human intelligence? 'Cause humans create tools, and with the tools,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1433" target="_blank">00:23:53.360</a></span> | <span class="t">they become more intelligent. There's a gray area there, what it means to be human when we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1438" target="_blank">00:23:58.720</a></span> | <span class="t">measuring their intelligence. - When I think about it, I usually think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1441" target="_blank">00:24:01.440</a></span> | <span class="t">human with a paper and a pencil, not human with internet and another AI helping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1446" target="_blank">00:24:06.640</a></span> | <span class="t">- But is that a fair way to think about it? 'Cause isn't there another definition of human-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1451" target="_blank">00:24:11.440</a></span> | <span class="t">intelligence that includes the tools that humans create?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1453" target="_blank">00:24:13.680</a></span> | <span class="t">- But we create AI, so at any point, you'll still just add superintelligence to human capability?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1459" target="_blank">00:24:19.200</a></span> | <span class="t">That seems like cheating. - No, controllable tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1463" target="_blank">00:24:23.040</a></span> | <span class="t">There is an implied leap that you're making when AGI goes from tool to entity that can make its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1473" target="_blank">00:24:33.440</a></span> | <span class="t">own decisions. So if we define human-level intelligence as everything a human can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1478" target="_blank">00:24:38.480</a></span> | <span class="t">with fully controllable tools. - It seems like a hybrid of some kind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1482" target="_blank">00:24:42.720</a></span> | <span class="t">You're now doing brain-computer interfaces, you're connecting it to maybe narrow AIs. Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1487" target="_blank">00:24:47.680</a></span> | <span class="t">it definitely increases our capabilities. - So what's a good test to you that measures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1497" target="_blank">00:24:57.360</a></span> | <span class="t">whether an artificial intelligence system has reached human-level intelligence? And what's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1502" target="_blank">00:25:02.560</a></span> | <span class="t">good test where it has superseded human-level intelligence to reach that land of AGI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1509" target="_blank">00:25:09.120</a></span> | <span class="t">- I am old-fashioned. I like Turing test. I have a paper where I equate passing Turing test to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1515" target="_blank">00:25:15.040</a></span> | <span class="t">solving AI complete problems because you can encode any questions about any domain into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1520" target="_blank">00:25:20.400</a></span> | <span class="t">Turing test. You don't have to talk about how was your day, you can ask anything. And so the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1526" target="_blank">00:25:26.960</a></span> | <span class="t">has to be as smart as a human to pass it in a true sense. - But then you would extend that to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1532" target="_blank">00:25:32.080</a></span> | <span class="t">maybe a very long conversation. I think the Alexa prize was doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1537" target="_blank">00:25:37.360</a></span> | <span class="t">Basically, can you do a 20-minute, 30-minute conversation with an AI system?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1542" target="_blank">00:25:42.160</a></span> | <span class="t">- It has to be long enough to where you can make some meaningful decisions about capabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1549" target="_blank">00:25:49.040</a></span> | <span class="t">absolutely. You can brute force very short conversations. - So like, literally, what does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1554" target="_blank">00:25:54.160</a></span> | <span class="t">that look like? Can we construct formally a kind of test that tests for AGI? - For AGI, it has to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1565" target="_blank">00:26:05.840</a></span> | <span class="t">be there. I cannot give it a task I can give to a human and it cannot do it if a human can. For</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1573" target="_blank">00:26:13.200</a></span> | <span class="t">superintelligence, it would be superior on all such tasks, not just average performance. So like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1578" target="_blank">00:26:18.800</a></span> | <span class="t">go learn to drive car, go speak Chinese, play guitar. Okay, great. - I guess the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1584" target="_blank">00:26:24.480</a></span> | <span class="t">question, is there a test for the kind of AGI that would be susceptible to lead to S-risk or X-risk?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1595" target="_blank">00:26:35.520</a></span> | <span class="t">Susceptible to destroy human civilization? Like, is there a test for that? - You can develop a test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1602" target="_blank">00:26:42.400</a></span> | <span class="t">which will give you positives if it lies to you or has those ideas. You cannot develop a test which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1608" target="_blank">00:26:48.320</a></span> | <span class="t">rules them out. There is always possibility of what Bostrom calls a treacherous turn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1613" target="_blank">00:26:53.280</a></span> | <span class="t">where later on a system decides for game-theoretic reasons, economic reasons, to change its behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1621" target="_blank">00:27:01.360</a></span> | <span class="t">And we see the same with humans. It's not unique to AI. For millennia, we tried developing morals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1627" target="_blank">00:27:07.120</a></span> | <span class="t">ethics, religions, lie detector tests, and then employees betray the employers, spouses betray</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1633" target="_blank">00:27:13.600</a></span> | <span class="t">family. It's a pretty standard thing intelligent agents sometimes do. - So is it possible to detect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1641" target="_blank">00:27:21.280</a></span> | <span class="t">when an AI system is lying or deceiving you? - If you know the truth and it tells you something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1647" target="_blank">00:27:27.920</a></span> | <span class="t">false, you can detect that. But you cannot know in general every single time. And again, the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1654" target="_blank">00:27:34.800</a></span> | <span class="t">you're testing today may not be lying. The system you're testing today may know you are testing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1660" target="_blank">00:27:40.640</a></span> | <span class="t">and so behaving. And later on, after it interacts with the environment, interacts with other systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1668" target="_blank">00:27:48.400</a></span> | <span class="t">malevolent agents, learns more, it may start doing those things. - So do you think it's possible to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1674" target="_blank">00:27:54.560</a></span> | <span class="t">develop a system where the creators of the system, the developers, the programmers, don't know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1680" target="_blank">00:28:00.480</a></span> | <span class="t">it's deceiving them? - So systems today don't have long-term planning. That is not our. They can lie</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1688" target="_blank">00:28:08.080</a></span> | <span class="t">today if it optimizes, helps them optimize their reward. If they realize, okay, this human will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1695" target="_blank">00:28:15.840</a></span> | <span class="t">very happy if I tell them the following, they will do it if it brings them more points. And they don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1703" target="_blank">00:28:23.440</a></span> | <span class="t">have to kind of keep track of it. It's just the right answer to this problem every single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1709" target="_blank">00:28:29.440</a></span> | <span class="t">- At which point is somebody creating that intentionally, not unintentionally, intentionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1716" target="_blank">00:28:36.000</a></span> | <span class="t">creating an AI system that's doing long-term planning with an objective function as defined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1721" target="_blank">00:28:41.040</a></span> | <span class="t">by the AI system, not by a human? - Well, some people think that if they're that smart, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1726" target="_blank">00:28:46.960</a></span> | <span class="t">always good. They really do believe that. It's just benevolence from intelligence. So they'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1732" target="_blank">00:28:52.480</a></span> | <span class="t">always want what's best for us. Some people think that they will be able to detect problem behaviors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1740" target="_blank">00:29:00.640</a></span> | <span class="t">and correct them at the time when we get there. I don't think it's a good idea. I am strongly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1746" target="_blank">00:29:06.960</a></span> | <span class="t">against it. But yeah, there are quite a few people who, in general, are so optimistic about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1752" target="_blank">00:29:12.960</a></span> | <span class="t">technology, it could do no wrong. They want it developed as soon as possible, as capable as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1758" target="_blank">00:29:18.480</a></span> | <span class="t">possible. - So there's going to be people who believe the more intelligent it is, the more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1764" target="_blank">00:29:24.640</a></span> | <span class="t">benevolent. And so therefore, it should be the one that defines the objective function that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1768" target="_blank">00:29:28.560</a></span> | <span class="t">optimizing when it's doing long-term planning. - There are even people who say, okay, what's so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1773" target="_blank">00:29:33.680</a></span> | <span class="t">special about humans, right? We removed the gender bias. We're removing race bias. Why is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1780" target="_blank">00:29:40.000</a></span> | <span class="t">pro-human bias? We are polluting the planet. We are, as you said, you know, fight a lot of wars,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1784" target="_blank">00:29:44.880</a></span> | <span class="t">kind of violent. Maybe it's better if a super intelligent, perfect society comes and replaces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1792" target="_blank">00:29:52.160</a></span> | <span class="t">us. It's normal stage in the evolution of our species. - Yeah, so somebody says, let's develop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1799" target="_blank">00:29:59.520</a></span> | <span class="t">an AI system that removes the violent humans from the world. And then it turns out that all humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1806" target="_blank">00:30:06.240</a></span> | <span class="t">have violence in them, or the capacity for violence, and therefore all humans are removed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1810" target="_blank">00:30:10.800</a></span> | <span class="t">- Yeah, yeah, yeah. Let me ask about Ian LeCun. He's somebody who you've had a few exchanges with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1820" target="_blank">00:30:20.960</a></span> | <span class="t">and he's somebody who actively pushes back against this view that AI is going to lead to destruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1828" target="_blank">00:30:28.320</a></span> | <span class="t">of human civilization, also known as AI doomerism. So in one example that he tweeted, he said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1840" target="_blank">00:30:40.880</a></span> | <span class="t">"I do acknowledge risks, but," two points, "one, open research and open source are the best ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1847" target="_blank">00:30:47.360</a></span> | <span class="t">to understand and mitigate the risks, and two, AI is not something that just happens. We build it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1854" target="_blank">00:30:54.720</a></span> | <span class="t">We have agency in what it becomes. Hence, we control the risks." We meaning humans. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1861" target="_blank">00:31:01.040</a></span> | <span class="t">some sort of natural phenomena that we have no control over. So can you make the case that he's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1867" target="_blank">00:31:07.680</a></span> | <span class="t">right, and can you try to make the case that he's wrong? - I cannot make a case that he's right. He's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1872" target="_blank">00:31:12.480</a></span> | <span class="t">wrong in so many ways, it's difficult for me to remember all of them. He is a Facebook buddy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1877" target="_blank">00:31:17.920</a></span> | <span class="t">so I have a lot of fun having those little debates with him. So I'm trying to remember the arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1883" target="_blank">00:31:23.840</a></span> | <span class="t">So one, he says, "We are not gifted this intelligence from aliens. We are designing it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1890" target="_blank">00:31:30.880</a></span> | <span class="t">we are making decisions about it." That's not true. It was true when we had expert systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1896" target="_blank">00:31:36.960</a></span> | <span class="t">symbolic AI, decision trees. Today, you set up parameters for a model and you water this plant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1903" target="_blank">00:31:43.680</a></span> | <span class="t">you give it data, you give it compute, and it grows. And after it's finished growing into this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1909" target="_blank">00:31:49.280</a></span> | <span class="t">alien plant, you start testing it to find out what capabilities it has. And it takes years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1915" target="_blank">00:31:55.040</a></span> | <span class="t">to figure out, even for existing models. If it's trained for six months, it will take you two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1919" target="_blank">00:31:59.440</a></span> | <span class="t">three years to figure out basic capabilities of that system. We still discover new capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1925" target="_blank">00:32:05.280</a></span> | <span class="t">in systems which are already out there. So that's not the case. - So just to linger on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1930" target="_blank">00:32:10.720</a></span> | <span class="t">that you, the difference there, that there is some level of emergent intelligence that happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1935" target="_blank">00:32:15.760</a></span> | <span class="t">in our current approaches. So stuff that we don't hard-code in. - Absolutely. That's what makes it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1943" target="_blank">00:32:23.680</a></span> | <span class="t">so successful. When we had to painstakingly hard-code in everything, we didn't have much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1949" target="_blank">00:32:29.280</a></span> | <span class="t">progress. Now, just spend more money and more compute, and it's a lot more capable. - And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1955" target="_blank">00:32:35.440</a></span> | <span class="t">the question is, when there is emergent intelligent phenomena, what is the ceiling of that? For you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1961" target="_blank">00:32:41.280</a></span> | <span class="t">there's no ceiling. For Jan LeCun, I think there's a kind of ceiling that happens that we have full</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1967" target="_blank">00:32:47.920</a></span> | <span class="t">control over. Even if we don't understand the internals of the emergence, how the emergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1973" target="_blank">00:32:53.040</a></span> | <span class="t">happens, there's a sense that we have control and an understanding of the approximate ceiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1980" target="_blank">00:33:00.640</a></span> | <span class="t">of capability, the limits of the capability. - Let's say there is a ceiling. It's not guaranteed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1986" target="_blank">00:33:06.720</a></span> | <span class="t">to be at the level which is competitive with us. It may be greatly superior to ours. - So what about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=1994" target="_blank">00:33:14.480</a></span> | <span class="t">his statement about open research and open source are the best ways to understand and mitigate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2000" target="_blank">00:33:20.800</a></span> | <span class="t">risks? - Historically, he's completely right. Open source software is wonderful. It's tested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2006" target="_blank">00:33:26.240</a></span> | <span class="t">by the community. It's debugged. But we're switching from tools to agents. Now you're giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2011" target="_blank">00:33:31.920</a></span> | <span class="t">open source weapons to psychopaths. Do we want to open source nuclear weapons? Biological weapons?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2018" target="_blank">00:33:38.720</a></span> | <span class="t">It's not safe to give technology so powerful to those who may misalign it. Even if you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2025" target="_blank">00:33:45.600</a></span> | <span class="t">successful at somehow getting it to work in the first place in a friendly manner. - But the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2031" target="_blank">00:33:51.040</a></span> | <span class="t">difference with nuclear weapons, current AI systems are not akin to nuclear weapons. So the idea there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2037" target="_blank">00:33:57.440</a></span> | <span class="t">is you're open sourcing it at this stage, that you can understand it better. A large number of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2041" target="_blank">00:34:01.920</a></span> | <span class="t">can explore the limitations, the capabilities, explore the possible ways to keep it safe, to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2046" target="_blank">00:34:06.800</a></span> | <span class="t">it secure, all that kind of stuff, while it's not at the stage of nuclear weapons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2052" target="_blank">00:34:12.400</a></span> | <span class="t">So nuclear weapons, there's a non-nuclear weapon and then there's a nuclear weapon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2056" target="_blank">00:34:16.480</a></span> | <span class="t">With AI systems, there's a gradual improvement of capability and you get to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2063" target="_blank">00:34:23.040</a></span> | <span class="t">perform that improvement incrementally. And so open source allows you to study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2066" target="_blank">00:34:26.880</a></span> | <span class="t">how things go wrong, study the very process of emergence, study AI safety in those systems when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2075" target="_blank">00:34:35.200</a></span> | <span class="t">there's not a high level of danger, all that kind of stuff. - It also sets a very wrong precedent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2080" target="_blank">00:34:40.720</a></span> | <span class="t">So we open sourced model one, model two, model three, nothing ever bad happened. So obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2086" target="_blank">00:34:46.080</a></span> | <span class="t">we're going to do it with model four. It's just gradual improvement. - I don't think it always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2091" target="_blank">00:34:51.120</a></span> | <span class="t">works with the precedent. Like you're not stuck doing it the way you always did. It's just,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2096" target="_blank">00:34:56.560</a></span> | <span class="t">it sets a precedent of open research and open development such that we get to learn together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2103" target="_blank">00:35:03.920</a></span> | <span class="t">And then the first time there's a sign of danger, some dramatic thing happened, not a thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2110" target="_blank">00:35:10.560</a></span> | <span class="t">destroys human civilization, but some dramatic demonstration of capability that can legitimately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2117" target="_blank">00:35:17.600</a></span> | <span class="t">lead to a lot of damage. Then everybody wakes up and says, "Okay, we need to regulate this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2122" target="_blank">00:35:22.320</a></span> | <span class="t">We need to come up with safety mechanism that stops this." At this time, maybe you can educate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2128" target="_blank">00:35:28.320</a></span> | <span class="t">me, but I haven't seen any illustration of significant damage done by intelligent AI systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2134" target="_blank">00:35:34.000</a></span> | <span class="t">- So I have a paper which collects accidents through history of AI and they always are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2139" target="_blank">00:35:39.440</a></span> | <span class="t">proportional to capabilities of that system. So if you have tic-tac-toe playing AI, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2144" target="_blank">00:35:44.960</a></span> | <span class="t">fail to properly play and loses the game, which it should draw. Trivial. Your spell checker will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2150" target="_blank">00:35:50.800</a></span> | <span class="t">misspell a word, so on. I stopped collecting those because there are just too many examples of AIs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2156" target="_blank">00:35:56.720</a></span> | <span class="t">failing at what they are capable of. We haven't had terrible accidents in the sense of billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2163" target="_blank">00:36:03.520</a></span> | <span class="t">people got killed. Absolutely true. But in another paper, I argue that those accidents do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2170" target="_blank">00:36:10.000</a></span> | <span class="t">actually prevent people from continuing with research. Actually, they serve like vaccines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2177" target="_blank">00:36:17.600</a></span> | <span class="t">A vaccine makes your body a little bit sick, so you can handle the big disease later much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2184" target="_blank">00:36:24.480</a></span> | <span class="t">It's the same here. People will point out, "You know that AI accident we had where 12 people died?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2189" target="_blank">00:36:29.200</a></span> | <span class="t">Everyone's still here. 12 people is less than smoking kills. It's not a big deal." So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2195" target="_blank">00:36:35.120</a></span> | <span class="t">continue. So in a way, it will actually be kind of confirming that it's not that bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2202" target="_blank">00:36:42.320</a></span> | <span class="t">It matters how the deaths happen. Whether it's literally murdered by the AI system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2208" target="_blank">00:36:48.480</a></span> | <span class="t">then one is a problem. But if it's accidents because of increased reliance on automation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2216" target="_blank">00:36:56.560</a></span> | <span class="t">for example. So when airplanes are flying in an automated way, maybe the number of plane</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2224" target="_blank">00:37:04.880</a></span> | <span class="t">crashes increased by 17% or something. And then you're like, "Okay, do we really want to rely on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2230" target="_blank">00:37:10.560</a></span> | <span class="t">automation?" I think in the case of automation airplanes, it decreased significantly. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2235" target="_blank">00:37:15.280</a></span> | <span class="t">same thing with autonomous vehicles. Like, okay, what are the pros and cons? What are the trade</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2241" target="_blank">00:37:21.360</a></span> | <span class="t">offs here? And you can have that discussion in an honest way. But I think the kind of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2247" target="_blank">00:37:27.120</a></span> | <span class="t">we're talking about here is mass scale pain and suffering caused by AI systems. And I think we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2256" target="_blank">00:37:36.560</a></span> | <span class="t">need to see illustrations of that on a very small scale to start to understand that this is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2263" target="_blank">00:37:43.200</a></span> | <span class="t">damaging. Versus Clippy. Versus a tool that's really useful to a lot of people to do learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2269" target="_blank">00:37:49.680</a></span> | <span class="t">to do summarization of texts, to do question and answer, all that kind of stuff. To generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2276" target="_blank">00:37:56.320</a></span> | <span class="t">videos. A tool. Fundamentally a tool versus an agent that can do a huge amount of damage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2283" target="_blank">00:38:03.440</a></span> | <span class="t">So you bring up example of cars. Cars were slowly developed and integrated. If we had no cars,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2291" target="_blank">00:38:11.200</a></span> | <span class="t">and somebody came around and said, "I invented this thing. It's called cars. It's awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2295" target="_blank">00:38:15.440</a></span> | <span class="t">It kills like a hundred thousand Americans every year. Let's deploy it." Would we deploy that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2301" target="_blank">00:38:21.600</a></span> | <span class="t">- There's been fear mongering about cars for a long time. The transition from horses to cars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2308" target="_blank">00:38:28.240</a></span> | <span class="t">There's a really nice channel that I recommend people check out, Pessimist Archive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2312" target="_blank">00:38:32.000</a></span> | <span class="t">that documents all the fear mongering about technology that's happened throughout history.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2317" target="_blank">00:38:37.200</a></span> | <span class="t">There's definitely been a lot of fear mongering about cars. There's a transition period there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2322" target="_blank">00:38:42.400</a></span> | <span class="t">about cars, about how deadly they are. We can try. It took a very long time for cars to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2328" target="_blank">00:38:48.000</a></span> | <span class="t">proliferate to the degree they have now. And then you could ask serious questions in terms of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2333" target="_blank">00:38:53.920</a></span> | <span class="t">miles traveled, the benefit to the economy, the benefit to the quality of life that cars do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2338" target="_blank">00:38:58.480</a></span> | <span class="t">versus the number of deaths. 30, 40,000 in the United States. Are we willing to pay that price?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2344" target="_blank">00:39:04.880</a></span> | <span class="t">I think most people, when they're rationally thinking, policymakers will say yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2351" target="_blank">00:39:11.440</a></span> | <span class="t">We want to decrease it from 40,000 to zero and do everything we can to decrease it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2358" target="_blank">00:39:18.080</a></span> | <span class="t">There's all kinds of policies, incentives you can create to decrease the risks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2362" target="_blank">00:39:22.400</a></span> | <span class="t">with the deployment of this technology, but then you have to weigh the benefits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2366" target="_blank">00:39:26.880</a></span> | <span class="t">and the risks of the technology. And the same thing would be done with AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2370" target="_blank">00:39:30.560</a></span> | <span class="t">- You need data, you need to know. But if I'm right, and it's unpredictable, unexplainable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2376" target="_blank">00:39:36.400</a></span> | <span class="t">uncontrollable, you cannot make this decision where we're gaining $10 trillion of wealth,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2381" target="_blank">00:39:41.440</a></span> | <span class="t">but we're losing, we don't know how many people. You basically have to perform an experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2387" target="_blank">00:39:47.520</a></span> | <span class="t">on 8 billion humans without their consent. And even if they want to give you consent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2392" target="_blank">00:39:52.800</a></span> | <span class="t">they can't because they cannot give informed consent. They don't understand those things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2397" target="_blank">00:39:57.360</a></span> | <span class="t">- Right, that happens when you go from the predictable to the unpredictable very quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2404" target="_blank">00:40:04.560</a></span> | <span class="t">You just, but it's not obvious to me that AI systems would gain capabilities so quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2411" target="_blank">00:40:11.520</a></span> | <span class="t">that you won't be able to collect enough data to study the benefits and the risks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2415" target="_blank">00:40:15.840</a></span> | <span class="t">- We're literally doing it. The previous model we learned about after we finished training it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2421" target="_blank">00:40:21.920</a></span> | <span class="t">what it was capable of. Let's say we stop GPT-4 training run around human capability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2427" target="_blank">00:40:27.760</a></span> | <span class="t">hypothetically. We start training GPT-5, and I have no knowledge of insider training runs or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2433" target="_blank">00:40:33.040</a></span> | <span class="t">anything, and we start at that point of about human, and we train it for the next nine months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2439" target="_blank">00:40:39.280</a></span> | <span class="t">Maybe two months in, it becomes super intelligent. We continue training it. At the time when we start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2445" target="_blank">00:40:45.120</a></span> | <span class="t">testing it, it is already a dangerous system. How dangerous? I have no idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2451" target="_blank">00:40:51.040</a></span> | <span class="t">but neither are people training it. - At the training stage, but then there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2456" target="_blank">00:40:56.000</a></span> | <span class="t">testing stage inside the company. They can start getting intuition about what the system is capable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2461" target="_blank">00:41:01.360</a></span> | <span class="t">to do. You're saying that somehow leap from GPT-4 to GPT-5 can happen, the kind of leap where GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2471" target="_blank">00:41:11.760</a></span> | <span class="t">was controllable and GPT-5 is no longer controllable, and we get no insights from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2476" target="_blank">00:41:16.960</a></span> | <span class="t">using GPT-4 about the fact that GPT-5 will be uncontrollable. That's the situation you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2483" target="_blank">00:41:23.440</a></span> | <span class="t">concerned about, where their leap from N to N+1 would be such that an uncontrollable system is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2493" target="_blank">00:41:33.280</a></span> | <span class="t">created without any ability for us to anticipate that. - If we had capability of ahead of the run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2501" target="_blank">00:41:41.600</a></span> | <span class="t">before the training run, to register exactly what capabilities the next model will have at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2506" target="_blank">00:41:46.400</a></span> | <span class="t">of the training run, and we accurately guessed all of them, I would say you're right. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2510" target="_blank">00:41:50.880</a></span> | <span class="t">definitely go ahead with this run. We don't have that capability. - From GPT-4, you can build up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2516" target="_blank">00:41:56.720</a></span> | <span class="t">intuitions about what GPT-5 will be capable of. It's just incremental progress. Even if that's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2523" target="_blank">00:42:03.680</a></span> | <span class="t">big leap in capability, it just doesn't seem like you can take a leap from a system that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2529" target="_blank">00:42:09.680</a></span> | <span class="t">helping you write emails to a system that's going to destroy human civilization. It seems like it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2536" target="_blank">00:42:16.720</a></span> | <span class="t">always going to be sufficiently incremental such that we can anticipate the possible dangers. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2542" target="_blank">00:42:22.880</a></span> | <span class="t">not even talking about existential risk, but just the kind of damage you can do to civilization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2548" target="_blank">00:42:28.240</a></span> | <span class="t">It seems like we'll be able to anticipate the kinds, not the exact, but the kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2553" target="_blank">00:42:33.120</a></span> | <span class="t">risks it might lead to, and then rapidly develop defenses ahead of time and as the risks emerge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2564" target="_blank">00:42:44.560</a></span> | <span class="t">- We're not talking just about capabilities, specific tasks. We're talking about general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2569" target="_blank">00:42:49.280</a></span> | <span class="t">capability to learn. Maybe like a child at the time of testing and deployment, it is still not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2576" target="_blank">00:42:56.640</a></span> | <span class="t">extremely capable, but as it is exposed to more data, real world, it can be trained to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2583" target="_blank">00:43:03.360</a></span> | <span class="t">become much more dangerous and capable. - Let's focus then on the control problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2591" target="_blank">00:43:11.120</a></span> | <span class="t">At which point does the system become uncontrollable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2593" target="_blank">00:43:13.680</a></span> | <span class="t">Why is it the more likely trajectory for you that the system becomes uncontrollable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2599" target="_blank">00:43:19.040</a></span> | <span class="t">- I think at some point it becomes capable of getting out of control. For game theoretic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2605" target="_blank">00:43:25.680</a></span> | <span class="t">reasons, it may decide not to do anything right away and for a long time just collect more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2610" target="_blank">00:43:30.640</a></span> | <span class="t">resources, accumulate strategic advantage. Right away, it may be kind of still young,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2617" target="_blank">00:43:37.200</a></span> | <span class="t">weak superintelligence, give it a decade, it's in charge of a lot more resources,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2622" target="_blank">00:43:42.160</a></span> | <span class="t">it had time to make backups. So it's not obvious to me that it will strike as soon as it can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2627" target="_blank">00:43:47.280</a></span> | <span class="t">- Look, can we just try to imagine this future where there's an AI system that's capable of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2634" target="_blank">00:43:54.240</a></span> | <span class="t">escaping the control of humans and then doesn't and waits. What's that look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2642" target="_blank">00:44:02.800</a></span> | <span class="t">- So one, we have to rely on that system for a lot of the infrastructure. So we'll have to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2648" target="_blank">00:44:08.240</a></span> | <span class="t">it access, not just to the internet, but to the task of managing power, government, economy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2659" target="_blank">00:44:19.120</a></span> | <span class="t">this kind of stuff. And that just feels like a gradual process given the bureaucracies of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2663" target="_blank">00:44:23.840</a></span> | <span class="t">those systems involved. - We've been doing it for years. Software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2667" target="_blank">00:44:27.040</a></span> | <span class="t">controls all the systems, nuclear power plants, airline industry, it's all software based. Every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2672" target="_blank">00:44:32.320</a></span> | <span class="t">time there is electrical outage, I can't fly anywhere for days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2675" target="_blank">00:44:35.520</a></span> | <span class="t">- But there's a difference between software and AI. There's different kinds of software. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2683" target="_blank">00:44:43.360</a></span> | <span class="t">to give a single AI system access to the control of airlines and the control of the economy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2689" target="_blank">00:44:49.920</a></span> | <span class="t">that's not a trivial transition for humanity. - No, but if it shows it is safer, in fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2697" target="_blank">00:44:57.120</a></span> | <span class="t">when it's in control, we get better results, people will demand that it was put in place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2701" target="_blank">00:45:01.840</a></span> | <span class="t">- Absolutely. - And if not, it can hack the system. It can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2704" target="_blank">00:45:04.400</a></span> | <span class="t">use social engineering to get access to it. That's why I said it might take some time for it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2709" target="_blank">00:45:09.200</a></span> | <span class="t">accumulate those resources. - It just feels like that would take a long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2712" target="_blank">00:45:12.320</a></span> | <span class="t">time for either humans to trust it or for the social engineering to come into play. It's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2718" target="_blank">00:45:18.160</a></span> | <span class="t">a thing that happens overnight. It feels like something that happens across one or two decades.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2722" target="_blank">00:45:22.720</a></span> | <span class="t">- I really hope you're right, but it's not what I'm seeing. People are very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2726" target="_blank">00:45:26.960</a></span> | <span class="t">quick to jump on the latest trend. Early adopters will be there before it's even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2731" target="_blank">00:45:31.040</a></span> | <span class="t">deployed buying prototypes. - Maybe the social engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2734" target="_blank">00:45:34.720</a></span> | <span class="t">So for social engineering, AI systems don't need any hardware access. It's all software. So they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2742" target="_blank">00:45:42.640</a></span> | <span class="t">can start manipulating you through social media and so on. Like you have AI assistants, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2747" target="_blank">00:45:47.280</a></span> | <span class="t">going to help you manage a lot of your day-to-day, and then they start doing social engineering. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2753" target="_blank">00:45:53.600</a></span> | <span class="t">for a system that's so capable that it can escape the control of humans that created it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2760" target="_blank">00:46:00.320</a></span> | <span class="t">such a system being deployed at a mass scale and trusted by people to be deployed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2770" target="_blank">00:46:10.080</a></span> | <span class="t">it feels like that would take a lot of convincing. - So we've been deploying systems which had hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2776" target="_blank">00:46:16.960</a></span> | <span class="t">capabilities. - Can you give an example?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2780" target="_blank">00:46:20.080</a></span> | <span class="t">- GPT-4. I don't know what else it's capable of, but there are still things we haven't discovered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2785" target="_blank">00:46:25.120</a></span> | <span class="t">can do. They may be trivial proportionate to its capability. I don't know, it writes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2790" target="_blank">00:46:30.240</a></span> | <span class="t">Chinese poetry, hypothetical. I know it does. But we haven't tested for all possible capabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2797" target="_blank">00:46:37.280</a></span> | <span class="t">and we're not explicitly designing them. We can only rule out bugs we find. We cannot rule out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2805" target="_blank">00:46:45.040</a></span> | <span class="t">bugs and capabilities because we haven't found them. - Is it possible for a system to have hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2814" target="_blank">00:46:54.480</a></span> | <span class="t">capabilities that are orders of magnitude greater than its non-hidden capabilities?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2820" target="_blank">00:47:00.960</a></span> | <span class="t">This is the thing I'm really struggling with, where on the surface, the thing we understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2828" target="_blank">00:47:08.000</a></span> | <span class="t">it can do doesn't seem that harmful. So even if it has bugs, even if it has hidden capabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2835" target="_blank">00:47:15.040</a></span> | <span class="t">Chinese poetry, or generating effective viruses, software viruses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2841" target="_blank">00:47:21.040</a></span> | <span class="t">the damage that can do seems on the same order of magnitude as the capabilities that we know about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2851" target="_blank">00:47:31.040</a></span> | <span class="t">So this idea that the hidden capabilities will include being uncontrollable is something I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2857" target="_blank">00:47:37.120</a></span> | <span class="t">struggling with, 'cause GPT-4 on the surface seems to be very controllable. - Again, we can only ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2863" target="_blank">00:47:43.840</a></span> | <span class="t">and test for things we know about. If there are unknown unknowns, we cannot do it. I'm thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2868" target="_blank">00:47:48.960</a></span> | <span class="t">of humans, artistic savants, right? If you talk to a person like that, you may not even realize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2874" target="_blank">00:47:54.320</a></span> | <span class="t">they can multiply 20-digit numbers in their head. You have to know to ask. - So as I mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2881" target="_blank">00:48:01.920</a></span> | <span class="t">just to sort of linger on the fear of the unknown, so the Pessimist Archive has just documented,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2890" target="_blank">00:48:10.160</a></span> | <span class="t">let's look at data of the past, at history. There's been a lot of fear-mongering about technology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2895" target="_blank">00:48:15.520</a></span> | <span class="t">Pessimist Archive does a really good job of documenting how crazily afraid we are of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2901" target="_blank">00:48:21.440</a></span> | <span class="t">every piece of technology. We've been afraid, there's a blog post where Louis Anslow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2907" target="_blank">00:48:27.520</a></span> | <span class="t">who created Pessimist Archive, writes about the fact that we've been fear-mongering about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2913" target="_blank">00:48:33.120</a></span> | <span class="t">robots and automation for over 100 years. So why is AGI different than the kinds of technologies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2921" target="_blank">00:48:41.520</a></span> | <span class="t">we've been afraid of in the past? - So two things. One, we're switching from tools to agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2926" target="_blank">00:48:46.240</a></span> | <span class="t">Tools don't have negative or positive impact. People using tools do. So guns don't kill people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2936" target="_blank">00:48:56.320</a></span> | <span class="t">what guns do. Agents can make their own decisions. They can be positive or negative. A pit bull can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2942" target="_blank">00:49:02.480</a></span> | <span class="t">decide to harm you as an agent. The fears are the same. The only difference is now we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2950" target="_blank">00:49:10.080</a></span> | <span class="t">technology. Then they were afraid of humanoid robots 100 years ago. They had none. Today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2955" target="_blank">00:49:15.760</a></span> | <span class="t">every major company in the world is investing billions to create them. Not every, but you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2960" target="_blank">00:49:20.160</a></span> | <span class="t">understand what I'm saying? It's very different. - Well, agents, it depends on what you mean by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2968" target="_blank">00:49:28.080</a></span> | <span class="t">the word agents. All those companies are not investing in a system that has the kind of agency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2972" target="_blank">00:49:32.800</a></span> | <span class="t">that's implied by in the fears, where it can really make decisions on their own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2979" target="_blank">00:49:39.760</a></span> | <span class="t">They have no human in the loop. - They are saying they are building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2983" target="_blank">00:49:43.680</a></span> | <span class="t">super intelligence and have a super alignment team. You don't think they are trying to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2987" target="_blank">00:49:47.920</a></span> | <span class="t">a system smart enough to be an independent agent under that definition? - I have not seen evidence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=2993" target="_blank">00:49:53.200</a></span> | <span class="t">of it. I think a lot of it is a marketing kind of discussion about the future. It's a mission about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3002" target="_blank">00:50:02.080</a></span> | <span class="t">the kind of systems you can create in the long-term future. But in the short-term,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3006" target="_blank">00:50:06.400</a></span> | <span class="t">the kind of systems they're creating falls fully within the definition of narrow AI. These are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3016" target="_blank">00:50:16.640</a></span> | <span class="t">tools that have increasing capabilities, but they just don't have a sense of agency or consciousness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3022" target="_blank">00:50:22.560</a></span> | <span class="t">or self-awareness or ability to deceive at scales that would be required to do mass scale suffering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3031" target="_blank">00:50:31.360</a></span> | <span class="t">and murder of humans. - Those systems are well beyond narrow AI. If you had to list all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3036" target="_blank">00:50:36.000</a></span> | <span class="t">capabilities of GPT-4, you would spend a lot of time writing that list. - But agency is not one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3041" target="_blank">00:50:41.280</a></span> | <span class="t">of them. - Not yet. But do you think any of those companies are holding back because they think it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3046" target="_blank">00:50:46.800</a></span> | <span class="t">may be not safe or are they developing the most capable system they can given the resources and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3052" target="_blank">00:50:52.400</a></span> | <span class="t">hoping they can control and monetize? - Control and monetize. Hoping they can control and monetize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3058" target="_blank">00:50:58.960</a></span> | <span class="t">So you're saying if they could press a button and create an agent that they no longer control,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3066" target="_blank">00:51:06.320</a></span> | <span class="t">that they can have to ask nicely. A thing that lives on a server across huge number of computers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3075" target="_blank">00:51:15.920</a></span> | <span class="t">- You're saying that they would push for the creation of that kind of system? - I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3082" target="_blank">00:51:22.240</a></span> | <span class="t">I can't speak for other people, for all of them. I think some of them are very ambitious. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3087" target="_blank">00:51:27.680</a></span> | <span class="t">fundraise in trillions. They talk about controlling the light corner of the universe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3091" target="_blank">00:51:31.680</a></span> | <span class="t">I would guess that they might. - Well, that's a human question. Whether humans are capable of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3098" target="_blank">00:51:38.640</a></span> | <span class="t">that. Probably some humans are capable of that. My more direct question, if it's possible to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3104" target="_blank">00:51:44.480</a></span> | <span class="t">create such a system, have a system that has that level of agency. I don't think that's an easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3112" target="_blank">00:51:52.480</a></span> | <span class="t">technical challenge. It doesn't feel like we're close to that. A system that has the kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3119" target="_blank">00:51:59.520</a></span> | <span class="t">agency where it can make its own decisions and deceive everybody about them. The current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3124" target="_blank">00:52:04.400</a></span> | <span class="t">architecture we have in machine learning and how we train the systems, how we deploy the systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3130" target="_blank">00:52:10.880</a></span> | <span class="t">and all that, it just doesn't seem to support that kind of agency. - I really hope you are right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3136" target="_blank">00:52:16.320</a></span> | <span class="t">I think the scaling hypothesis is correct. We haven't seen diminishing returns. It used to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3142" target="_blank">00:52:22.640</a></span> | <span class="t">we asked how long before AGI, now we should ask how much until AGI. It's trillion dollars today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3149" target="_blank">00:52:29.120</a></span> | <span class="t">it's a billion dollars next year, it's a million dollars in a few years. - Don't you think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3154" target="_blank">00:52:34.320</a></span> | <span class="t">possible to basically run out of trillions? Is this constrained by compute? - Compute gets cheaper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3162" target="_blank">00:52:42.000</a></span> | <span class="t">every day, exponentially. - But then that becomes a question of decades versus years. - If the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3167" target="_blank">00:52:47.840</a></span> | <span class="t">disagreement is that it will take decades, not years for everything I'm saying to materialize,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3174" target="_blank">00:52:54.720</a></span> | <span class="t">then I can go with that. - But if it takes decades, then the development of tools for AI safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3182" target="_blank">00:53:02.800</a></span> | <span class="t">becomes more and more realistic. So I guess the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3186" target="_blank">00:53:06.800</a></span> | <span class="t">I have a fundamental belief that humans when faced with danger can come up with ways to defend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3193" target="_blank">00:53:13.840</a></span> | <span class="t">against that danger. And one of the big problems facing AI safety currently for me is that there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3201" target="_blank">00:53:21.520</a></span> | <span class="t">not clear illustrations of what that danger looks like. There's no illustrations of AI systems doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3208" target="_blank">00:53:28.480</a></span> | <span class="t">a lot of damage. And so it's unclear what you're defending against. Because currently it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3215" target="_blank">00:53:35.040</a></span> | <span class="t">philosophical notions that yes, it's possible to imagine AI systems that take control of everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3220" target="_blank">00:53:40.560</a></span> | <span class="t">and then destroy all humans. It's also a more formal mathematical notion that you talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3227" target="_blank">00:53:47.040</a></span> | <span class="t">that it's impossible to have a perfectly secure system. You can't prove that a program of sufficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3234" target="_blank">00:53:54.560</a></span> | <span class="t">complexity is completely safe and perfect and know everything about it. Yes, but like when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3242" target="_blank">00:54:02.160</a></span> | <span class="t">actually just pragmatically look, how much damage have the AI systems done and what kind of damage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3247" target="_blank">00:54:07.440</a></span> | <span class="t">there's not been illustrations of that. Even in the autonomous weapon systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3253" target="_blank">00:54:13.600</a></span> | <span class="t">there's not been mass deployments of autonomous weapon systems, luckily. The automation in war</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3261" target="_blank">00:54:21.680</a></span> | <span class="t">currently is very limited. The automation is at the scale of individuals versus like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3268" target="_blank">00:54:28.880</a></span> | <span class="t">at the scale of strategy and planning. So I think one of the challenges here is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3275" target="_blank">00:54:35.040</a></span> | <span class="t">where is the dangers? And the intuition that Yann LeCun and others have is let's keep in the open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3283" target="_blank">00:54:43.600</a></span> | <span class="t">building AI systems until the dangers start rearing their heads. And they become more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3291" target="_blank">00:54:51.520</a></span> | <span class="t">explicit. There start being case studies, illustrative case studies that show exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3299" target="_blank">00:54:59.840</a></span> | <span class="t">how the damage by AI systems is done. Then regulation could step in. Then brilliant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3304" target="_blank">00:55:04.320</a></span> | <span class="t">engineers can step up and we could have Manhattan-style projects that defend against such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3309" target="_blank">00:55:09.040</a></span> | <span class="t">systems. That's kind of the notion. And I guess attention with that is the idea that for you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3315" target="_blank">00:55:15.840</a></span> | <span class="t">we need to be thinking about that now so that we're ready because we will have not much time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3321" target="_blank">00:55:21.920</a></span> | <span class="t">once the systems are deployed. Is that true? - There is a lot to unpack here. There is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3328" target="_blank">00:55:28.880</a></span> | <span class="t">partnership on AI, a conglomerate of many large corporations. They have a database of AI accidents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3334" target="_blank">00:55:34.480</a></span> | <span class="t">they collect. I contributed a lot to the database. If we so far made almost no progress in actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3341" target="_blank">00:55:41.280</a></span> | <span class="t">solving this problem, not patching it, not again, lipstick and a pig kind of solutions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3346" target="_blank">00:55:46.880</a></span> | <span class="t">why would we think we'll do better than we're closer to the problem? - All the things you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3353" target="_blank">00:55:53.680</a></span> | <span class="t">mentioned are serious concerns. Measuring the amount of harm, so benefit versus risk there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3358" target="_blank">00:55:58.160</a></span> | <span class="t">is difficult. But to you, the sense is already the risk has superseded the benefit. - Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3363" target="_blank">00:56:03.200</a></span> | <span class="t">I want to be perfectly clear. I love AI. I love technology. I'm a computer scientist. I have PhD</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3368" target="_blank">00:56:08.160</a></span> | <span class="t">in engineering. I work at an engineering school. There is a huge difference between we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3373" target="_blank">00:56:13.120</a></span> | <span class="t">develop narrow AI systems, super intelligent in solving specific human problems like protein</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3379" target="_blank">00:56:19.360</a></span> | <span class="t">folding, and let's create super intelligent machine, got it, and we'll decide what to do with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3384" target="_blank">00:56:24.880</a></span> | <span class="t">us. Those are not the same. I am against the super intelligence in general sense with no undo button.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3394" target="_blank">00:56:34.000</a></span> | <span class="t">- So do you think the teams that are doing, that are able to do the AI safety on the kind of narrow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3400" target="_blank">00:56:40.960</a></span> | <span class="t">AI risks that you've mentioned, are those approaches going to be at all productive towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3408" target="_blank">00:56:48.880</a></span> | <span class="t">leading to approaches of doing AI safety on AGI? Or is it just a fundamentally different-- - Partially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3414" target="_blank">00:56:54.800</a></span> | <span class="t">but they don't scale. For narrow AI, for deterministic systems, you can test them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3419" target="_blank">00:56:59.280</a></span> | <span class="t">You have edge cases. You know what the answer should look like. You know the right answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3424" target="_blank">00:57:04.400</a></span> | <span class="t">For general systems, you have infinite test surface. You have no edge cases. You cannot even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3430" target="_blank">00:57:10.560</a></span> | <span class="t">know what to test for. Again, the unknown unknowns are underappreciated by people looking at this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3438" target="_blank">00:57:18.320</a></span> | <span class="t">problem. You are always asking me, "How will it kill everyone? How will it fail?" The whole point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3444" target="_blank">00:57:24.960</a></span> | <span class="t">is if I knew it, I would be super intelligent. Despite what you might think, I'm not. - So to you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3451" target="_blank">00:57:31.040</a></span> | <span class="t">the concern is that we would not be able to see early signs of an uncontrollable system. - It is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3459" target="_blank">00:57:39.360</a></span> | <span class="t">a master at deception. Sam tweeted about how great it is at persuasion, and we see it ourselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3465" target="_blank">00:57:45.680</a></span> | <span class="t">especially now with voices, with maybe kind of flirty, sarcastic female voices. It's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3473" target="_blank">00:57:53.360</a></span> | <span class="t">be very good at getting people to do things. - But see, I'm very concerned about system being used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3482" target="_blank">00:58:02.000</a></span> | <span class="t">to control the masses. But in that case, the developers know about the kind of control that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3490" target="_blank">00:58:10.400</a></span> | <span class="t">happening. You're more concerned about the next stage, where even the developers don't know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3496" target="_blank">00:58:16.800</a></span> | <span class="t">the deception. - Right. I don't think developers know everything about what they are creating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3502" target="_blank">00:58:22.960</a></span> | <span class="t">They have lots of great knowledge. We're making progress on explaining parts of a network. We can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3508" target="_blank">00:58:28.400</a></span> | <span class="t">understand, okay, this node gets excited when this input is presented, this cluster of nodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3515" target="_blank">00:58:35.840</a></span> | <span class="t">But we're nowhere near close to understanding the full picture, and I think it's impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3521" target="_blank">00:58:41.040</a></span> | <span class="t">You need to be able to survey an explanation. The size of those models prevents a single human from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3527" target="_blank">00:58:47.680</a></span> | <span class="t">observing all this information, even if provided by the system. So either we're getting model as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3533" target="_blank">00:58:53.600</a></span> | <span class="t">an explanation for what's happening, and that's not comprehensible to us, or we're getting a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3538" target="_blank">00:58:58.560</a></span> | <span class="t">compressed explanation, lossy compression, where here's top 10 reasons you got fired.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3544" target="_blank">00:59:04.240</a></span> | <span class="t">It's something, but it's not a full picture. - You've given elsewhere an example of a child,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3549" target="_blank">00:59:09.760</a></span> | <span class="t">and everybody, all humans try to deceive. They try to lie early on in their life. I think we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3556" target="_blank">00:59:16.160</a></span> | <span class="t">just get a lot of examples of deceptions from large language models or AI systems that are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3561" target="_blank">00:59:21.760</a></span> | <span class="t">to be kind of shitty, or they'll be pretty good, but we'll catch them off guard. We'll start to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3567" target="_blank">00:59:27.120</a></span> | <span class="t">the kind of momentum towards developing increasing deception capabilities, and that's when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3576" target="_blank">00:59:36.480</a></span> | <span class="t">like, okay, we need to do some kind of alignment that prevents deception. But then we'll have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3581" target="_blank">00:59:41.680</a></span> | <span class="t">if you support open source, then you can have open source models that have some level of deception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3586" target="_blank">00:59:46.320</a></span> | <span class="t">You can start to explore on a large scale, how do we stop it from being deceptive? Then there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3591" target="_blank">00:59:51.680</a></span> | <span class="t">more explicit, pragmatic kind of problem to solve. How do we stop AI systems from trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3602" target="_blank">01:00:02.080</a></span> | <span class="t">for deception? That's just an example, right? - So there is a paper, I think it came out last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3607" target="_blank">01:00:07.360</a></span> | <span class="t">week by Dr. Park et al from MIT, I think, and they showed that existing models already showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3614" target="_blank">01:00:14.560</a></span> | <span class="t">successful deception in what they do. My concern is not that they lie now and we need to catch them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3622" target="_blank">01:00:22.240</a></span> | <span class="t">and tell them don't lie. My concern is that once they are capable and deployed, they will later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3628" target="_blank">01:00:28.960</a></span> | <span class="t">change their mind because that's what unrestricted learning allows you to do. Lots of people grow up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3636" target="_blank">01:00:36.720</a></span> | <span class="t">maybe in a religious family, they read some new books and they turn in their religion. That's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3643" target="_blank">01:00:43.680</a></span> | <span class="t">treacherous turn in humans. If you learn something new about your colleagues, maybe you'll change how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3651" target="_blank">01:00:51.360</a></span> | <span class="t">you react to them. - Yeah, a treacherous turn. If we just mentioned humans, Stalin and Hitler,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3658" target="_blank">01:00:58.800</a></span> | <span class="t">there's a turn. Stalin is a good example. He just seems like a normal communist follower of Lenin</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3666" target="_blank">01:01:06.800</a></span> | <span class="t">until there's a turn. There's a turn of what that means in terms of when he has complete control,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3673" target="_blank">01:01:13.680</a></span> | <span class="t">what the execution of that policy means and how many people get to suffer. - And you can't say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3678" target="_blank">01:01:18.320</a></span> | <span class="t">they are not rational. The rational decision changes based on your position. Then you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3684" target="_blank">01:01:24.480</a></span> | <span class="t">under the boss, the rational policy may be to be following orders and being honest. When you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3690" target="_blank">01:01:30.400</a></span> | <span class="t">become a boss, the rational policy may shift. - Yeah, and by the way, a lot of my disagreements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3696" target="_blank">01:01:36.240</a></span> | <span class="t">here is just playing devil's advocate to challenge your ideas and to explore them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3701" target="_blank">01:01:41.840</a></span> | <span class="t">One of the big problems here in this whole conversation is human civilization hangs in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3709" target="_blank">01:01:49.920</a></span> | <span class="t">the balance and yet everything is unpredictable. We don't know how these systems will look like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3714" target="_blank">01:01:54.080</a></span> | <span class="t">-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3715" target="_blank">01:01:55.000</a></span> | <span class="t">The robots are coming. - There's a refrigerator making a buzzing noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3722" target="_blank">01:02:02.400</a></span> | <span class="t">- Very menacing, very menacing. So every time I'm about to talk about this topic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3728" target="_blank">01:02:08.880</a></span> | <span class="t">things start to happen. My flight yesterday was canceled without possibility to rebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3733" target="_blank">01:02:13.360</a></span> | <span class="t">I was giving a talk at Google in Israel and three cars which were supposed to take me to the talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3741" target="_blank">01:02:21.520</a></span> | <span class="t">could not. I'm just saying. I like AIs. I for one welcome our overlords.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3750" target="_blank">01:02:30.960</a></span> | <span class="t">- There's a degree to which we, I mean, it is very obvious. As we already have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3757" target="_blank">01:02:37.440</a></span> | <span class="t">we've increasingly given our life over to software systems. And then it seems obvious,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3764" target="_blank">01:02:44.560</a></span> | <span class="t">given the capabilities of AI that are coming, that we'll give our lives over increasingly to AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3770" target="_blank">01:02:50.320</a></span> | <span class="t">systems. Cars will drive themselves. Refrigerator eventually will optimize what I get to eat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3778" target="_blank">01:02:58.640</a></span> | <span class="t">And as more and more of our lives are controlled or managed by AI assistance, it is very possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3787" target="_blank">01:03:07.760</a></span> | <span class="t">that there's a drift. I mean, I personally am concerned about non-existential stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3793" target="_blank">01:03:13.440</a></span> | <span class="t">the more near-term things. Because before we even get to existential, I feel like there could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3799" target="_blank">01:03:19.440</a></span> | <span class="t">just so many Brave New World type of situations. You mentioned sort of the term behavioral drift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3804" target="_blank">01:03:24.960</a></span> | <span class="t">It's the slow boiling that I'm really concerned about. As we give our lives over to automation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3811" target="_blank">01:03:31.120</a></span> | <span class="t">that our minds can become controlled by governments, by companies, or just in a distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3819" target="_blank">01:03:39.840</a></span> | <span class="t">way, there's a drift. Some aspect of our human nature gives ourselves over to the control of AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3825" target="_blank">01:03:45.920</a></span> | <span class="t">systems. And they, in an unintended way, just control how we think. Maybe there'll be a herd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3831" target="_blank">01:03:51.840</a></span> | <span class="t">like mentality in how we think, which will kill all creativity and exploration of ideas, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3836" target="_blank">01:03:56.720</a></span> | <span class="t">diversity of ideas, or much worse. So it's true. It's true. But a lot of the conversation I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3845" target="_blank">01:04:05.600</a></span> | <span class="t">having with you now is also kind of wondering, almost on a technical level, how can AI escape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3852" target="_blank">01:04:12.800</a></span> | <span class="t">control? Like, what would that system look like? Because to me, it's terrifying and fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3860" target="_blank">01:04:20.480</a></span> | <span class="t">And also fascinating to me is maybe the optimistic notion that it's possible to engineer systems that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3869" target="_blank">01:04:29.200</a></span> | <span class="t">defend against that. One of the things you write a lot about in your book is verifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3876" target="_blank">01:04:36.160</a></span> | <span class="t">So not humans, humans are also verifiers, but software systems that look at AI systems and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3885" target="_blank">01:04:45.280</a></span> | <span class="t">help you understand, this thing is getting real weird, help you analyze those systems. So maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3894" target="_blank">01:04:54.160</a></span> | <span class="t">this is a good time to talk about verification. What is this beautiful notion of verification?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3900" target="_blank">01:05:00.880</a></span> | <span class="t">My claim is, again, that there are very strong limits on what we can and cannot verify. A lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3906" target="_blank">01:05:06.400</a></span> | <span class="t">of times when you post something on social media, people go, "Oh, I need citation to a peer-reviewed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3911" target="_blank">01:05:11.040</a></span> | <span class="t">article." But what is a peer-reviewed article? You found two people in a world of hundreds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3916" target="_blank">01:05:16.720</a></span> | <span class="t">thousands of scientists who said, "Oh, whatever, publish it, I don't care." That's the verifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3920" target="_blank">01:05:20.720</a></span> | <span class="t">of that process. Then people say, "Oh, it's formally verified software and mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3926" target="_blank">01:05:26.640</a></span> | <span class="t">proof except something close to 100% chance of it being free of all problems." But if you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3935" target="_blank">01:05:35.360</a></span> | <span class="t">look at research, software is full of bugs, old mathematical theorems, which have been proven for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3942" target="_blank">01:05:42.160</a></span> | <span class="t">hundreds of years, have been discovered to contain bugs, on top of which we generate new proofs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3947" target="_blank">01:05:47.760</a></span> | <span class="t">and now we have to redo all that. So verifiers are not perfect. Usually they are either a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3954" target="_blank">01:05:54.640</a></span> | <span class="t">human or communities of humans, and it's basically kind of like a democratic vote.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3958" target="_blank">01:05:58.880</a></span> | <span class="t">Community of mathematicians agrees that this proof is correct, mostly correct. Even today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3965" target="_blank">01:06:05.520</a></span> | <span class="t">we're starting to see some mathematical proofs are so complex, so large, that mathematical community</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3971" target="_blank">01:06:11.840</a></span> | <span class="t">is unable to make a decision. It looks interesting, looks promising, but they don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3976" target="_blank">01:06:16.240</a></span> | <span class="t">They will need years for top scholars to study it, to figure it out. So of course, we can use AI to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3982" target="_blank">01:06:22.160</a></span> | <span class="t">help us with this process, but AI is a piece of software which needs to be verified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3987" target="_blank">01:06:27.200</a></span> | <span class="t">- Just to clarify, so verification is the process of saying something is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3992" target="_blank">01:06:32.080</a></span> | <span class="t">Sort of the most formal, a mathematical proof, where there's a statement and a series of logical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=3998" target="_blank">01:06:38.160</a></span> | <span class="t">statements that prove that statement to be correct, which is a theorem. And you're saying it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4003" target="_blank">01:06:43.920</a></span> | <span class="t">gets so complex that it's possible for the human verifiers, the human beings that verify that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4011" target="_blank">01:06:51.680</a></span> | <span class="t">logical step, there's no bugs in it, it becomes impossible. So it's nice to talk about verification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4018" target="_blank">01:06:58.320</a></span> | <span class="t">in this most formal, most clear, most rigorous formulation of it, which is mathematical proofs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4024" target="_blank">01:07:04.960</a></span> | <span class="t">- Right, and for AI, we would like to have that level of confidence, a very important mission</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4032" target="_blank">01:07:12.480</a></span> | <span class="t">critical software controlling satellites, nuclear power plants. For small deterministic programs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4037" target="_blank">01:07:17.520</a></span> | <span class="t">we can do this. We can check that code verifies its mapping to the design, whatever software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4045" target="_blank">01:07:25.840</a></span> | <span class="t">engineers intended was correctly implemented. But we don't know how to do this for software which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4053" target="_blank">01:07:33.360</a></span> | <span class="t">keeps learning, self-modifying, rewriting its own code. We don't know how to prove things about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4059" target="_blank">01:07:39.040</a></span> | <span class="t">physical world, states of humans in a physical world. So there are papers coming out now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4064" target="_blank">01:07:44.960</a></span> | <span class="t">and I have this beautiful one, Tolvert's Guaranteed Safe AI. Very cool paper, some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4072" target="_blank">01:07:52.960</a></span> | <span class="t">best authors I ever seen. I think there is multiple Turing Award winners. You can have this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4079" target="_blank">01:07:59.840</a></span> | <span class="t">one just came out, kind of similar, managing extreme AI risks. So all of them expect this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4086" target="_blank">01:08:06.720</a></span> | <span class="t">level of proof, but I would say that we can get more confidence with more resources we put into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4095" target="_blank">01:08:15.680</a></span> | <span class="t">it. But at the end of the day, we're still as reliable as the verifiers. And you have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4100" target="_blank">01:08:20.880</a></span> | <span class="t">infinite regress of verifiers. The software used to verify a program is itself a piece of program.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4106" target="_blank">01:08:26.960</a></span> | <span class="t">If aliens gave us well-aligned super intelligence, we can use that to create our own safe AI. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4113" target="_blank">01:08:33.760</a></span> | <span class="t">it's a catch-22. You need to have already proven to be safe system to verify this new system of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4121" target="_blank">01:08:41.040</a></span> | <span class="t">equal or greater complexity. - You just mentioned this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4124" target="_blank">01:08:44.800</a></span> | <span class="t">Tolvert's Guaranteed Safe AI, a framework for ensuring robust and reliable AI systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4129" target="_blank">01:08:49.280</a></span> | <span class="t">Like you mentioned, it's like a who's who. Josh Tenenbaum, Yoshua Bengio, Russell, Max Tegmark,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4134" target="_blank">01:08:54.960</a></span> | <span class="t">many other brilliant people. The page you have it open on, there are many possible strategies for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4140" target="_blank">01:09:00.320</a></span> | <span class="t">creating safety specifications. These strategies can roughly be placed on a spectrum, depending on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4146" target="_blank">01:09:06.480</a></span> | <span class="t">how much safety it would grant if successfully implemented. One way to do this is as follows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4151" target="_blank">01:09:11.760</a></span> | <span class="t">and there's a set of levels. From level zero, no safety specification is used, to level seven,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4156" target="_blank">01:09:16.960</a></span> | <span class="t">the safety specification completely encodes all things that humans might want in all contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4162" target="_blank">01:09:22.640</a></span> | <span class="t">Where does this paper fall short to you? - So when I wrote a paper, Artificial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4169" target="_blank">01:09:29.680</a></span> | <span class="t">Intelligence Safety Engineering, which kind of coins the term AI safety, that was 2011. We had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4175" target="_blank">01:09:35.360</a></span> | <span class="t">2012 conference, 2013 journal paper. One of the things I proposed, let's just do formal verifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4181" target="_blank">01:09:41.040</a></span> | <span class="t">on it. Let's do mathematical formal proofs. In the follow-up work, I basically realized it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4186" target="_blank">01:09:46.880</a></span> | <span class="t">still not get us 100%. We can get 99.9, we can put more resources exponentially and get closer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4194" target="_blank">01:09:54.560</a></span> | <span class="t">but we'll never get to 100%. If a system makes a billion decisions a second, and you use it for 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4200" target="_blank">01:10:00.800</a></span> | <span class="t">years, you're still gonna deal with a problem. This is wonderful research, I'm so happy they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4206" target="_blank">01:10:06.080</a></span> | <span class="t">doing it, this is great, but it is not going to be a permanent solution to that problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4212" target="_blank">01:10:12.320</a></span> | <span class="t">- So just to clarify, the task of creating an AI verifier is what? It's creating a verifier that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4218" target="_blank">01:10:18.880</a></span> | <span class="t">the AI system does exactly as it says it does, or it sticks within the guardrails that it says it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4225" target="_blank">01:10:25.520</a></span> | <span class="t">must? - There are many, many levels. So first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4227" target="_blank">01:10:27.840</a></span> | <span class="t">you're verifying the hardware in which it is run. You need to verify communication channel with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4233" target="_blank">01:10:33.760</a></span> | <span class="t">human. Every aspect of that whole world model needs to be verified. Somehow it needs to map</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4239" target="_blank">01:10:39.520</a></span> | <span class="t">the world into the world model. Map and territory differences. So how do I know internal states of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4246" target="_blank">01:10:46.720</a></span> | <span class="t">humans? Are you happy or sad? I can't tell. So how do I make proofs about real physical world? Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4253" target="_blank">01:10:53.280</a></span> | <span class="t">I can verify that deterministic algorithm follows certain properties. That can be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4258" target="_blank">01:10:58.720</a></span> | <span class="t">Some people argue that maybe just maybe two plus two is not four, I'm not that extreme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4264" target="_blank">01:11:04.320</a></span> | <span class="t">But once you have sufficiently large proof over sufficiently complex environment, the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4272" target="_blank">01:11:12.640</a></span> | <span class="t">that it has zero bugs in it is greatly reduced. If you keep deploying this a lot, eventually you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4279" target="_blank">01:11:19.200</a></span> | <span class="t">going to have a bug anyways. - There's always a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4281" target="_blank">01:11:21.600</a></span> | <span class="t">- There is always a bug. And the fundamental difference is what I mentioned. We're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4285" target="_blank">01:11:25.440</a></span> | <span class="t">dealing with cybersecurity. We're not going to get a new credit card, new humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4289" target="_blank">01:11:29.040</a></span> | <span class="t">- So this paper is really interesting. You said 2011, artificial intelligence, safety engineering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4295" target="_blank">01:11:35.200</a></span> | <span class="t">why machine ethics is a wrong approach. The grand challenge, you write, of AI safety engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4302" target="_blank">01:11:42.560</a></span> | <span class="t">We propose the problem of developing safety mechanisms for self-improving systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4308" target="_blank">01:11:48.640</a></span> | <span class="t">Self-improving systems. By the way, that's an interesting term for the thing that we're talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4315" target="_blank">01:11:55.120</a></span> | <span class="t">about. Is self-improving more general than learning? Self-improving, that's an interesting term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4326" target="_blank">01:12:06.240</a></span> | <span class="t">- You can improve the rate at which you are learning. You can become more efficient meta-optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4331" target="_blank">01:12:11.360</a></span> | <span class="t">- The word self, it's like self-replicating, self-improving. You can imagine a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4339" target="_blank">01:12:19.680</a></span> | <span class="t">building its own world on a scale and in a way that is way different than the current systems do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4346" target="_blank">01:12:26.400</a></span> | <span class="t">It feels like the current systems are not self-improving or self-replicating or self-growing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4351" target="_blank">01:12:31.920</a></span> | <span class="t">or self-spreading, all that kind of stuff. And once you take that leap, that's when a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4358" target="_blank">01:12:38.000</a></span> | <span class="t">the challenges seems to happen. Because the kind of bugs you can find now seems more akin to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4364" target="_blank">01:12:44.720</a></span> | <span class="t">current sort of normal software debugging kind of process. But whenever you can do self-replication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4374" target="_blank">01:12:54.080</a></span> | <span class="t">and arbitrary self-improvement, that's when a bug can become a real problem, real fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4382" target="_blank">01:13:02.160</a></span> | <span class="t">So what is the difference to you between verification of a non-self-improving system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4390" target="_blank">01:13:10.080</a></span> | <span class="t">versus a verification of a self-improving system? - So if you have fixed code, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4394" target="_blank">01:13:14.960</a></span> | <span class="t">you can verify that code, static verification at the time. But if it will continue modifying it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4401" target="_blank">01:13:21.360</a></span> | <span class="t">you have a much harder time guaranteeing that important properties of that system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4407" target="_blank">01:13:27.760</a></span> | <span class="t">have not been modified when the code changed. - Is it even doable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4412" target="_blank">01:13:32.080</a></span> | <span class="t">- No. - Does the whole process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4414" target="_blank">01:13:34.000</a></span> | <span class="t">of verification just completely fall apart? - It can always cheat. It can store parts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4418" target="_blank">01:13:38.400</a></span> | <span class="t">its code outside in the environment. It can have kind of extended mind situation. So this is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4425" target="_blank">01:13:45.200</a></span> | <span class="t">the type of problems I'm trying to bring up. - What are the classes of verifiers that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4430" target="_blank">01:13:50.080</a></span> | <span class="t">read about in the book? Is there interesting ones that stand out to you? Do you have some favorites?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4434" target="_blank">01:13:54.880</a></span> | <span class="t">- So I like Oracle types where you kind of just know that it's right. Turing likes Oracle machines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4441" target="_blank">01:14:01.200</a></span> | <span class="t">They know the right answer how, who knows. But they pull it out from somewhere, so you have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4446" target="_blank">01:14:06.560</a></span> | <span class="t">trust them. And that's a concern I have about humans in a world with very smart machines. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4453" target="_blank">01:14:13.600</a></span> | <span class="t">experiment with them, we see after a while, okay, they've always been right before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4457" target="_blank">01:14:17.920</a></span> | <span class="t">and we start trusting them without any verification of what they're saying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4461" target="_blank">01:14:21.600</a></span> | <span class="t">- Oh, I see, that we kind of build Oracle verifiers, or rather, we build verifiers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4468" target="_blank">01:14:28.320</a></span> | <span class="t">we believe to be Oracles, and then we start to, without any proof, use them as if they're Oracle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4475" target="_blank">01:14:35.440</a></span> | <span class="t">verifiers. - We remove ourselves from that process. We are not scientists who understand the world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4480" target="_blank">01:14:40.640</a></span> | <span class="t">we are humans who get new data presented to us. - Okay, one really cool class of verifiers is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4488" target="_blank">01:14:48.240</a></span> | <span class="t">a self-verifier. Is it possible that you somehow engineer into AI systems a thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4495" target="_blank">01:14:55.040</a></span> | <span class="t">that constantly verifies itself? - Preserved portion of it can be done,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4498" target="_blank">01:14:58.960</a></span> | <span class="t">but in terms of mathematical verification, it's kind of useless. You're saying you are the greatest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4504" target="_blank">01:15:04.480</a></span> | <span class="t">guy in the world because you are saying it. It's circular and not very helpful, but it's consistent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4509" target="_blank">01:15:09.840</a></span> | <span class="t">We know that within that world, you have verified that system. In a paper, I try to kind of brute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4515" target="_blank">01:15:15.600</a></span> | <span class="t">force all possible verifiers. It doesn't mean that this one is particularly important to us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4521" target="_blank">01:15:21.520</a></span> | <span class="t">- But what about self-doubt? The kind of verification where you say, or I say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4528" target="_blank">01:15:28.240</a></span> | <span class="t">I'm the greatest guy in the world. What about a thing which I actually have, is a voice that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4533" target="_blank">01:15:33.520</a></span> | <span class="t">is constantly extremely critical? So engineer into the system a constant uncertainty about self,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4541" target="_blank">01:15:41.840</a></span> | <span class="t">a constant doubt. - Well, any smart system would have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4547" target="_blank">01:15:47.120</a></span> | <span class="t">doubt about everything, all right? You're not sure if what information you are given is true,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4552" target="_blank">01:15:52.880</a></span> | <span class="t">if you are subject to manipulation. You have this safety and security mindset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4558" target="_blank">01:15:58.320</a></span> | <span class="t">- But I mean, you have doubt about yourself. So the AI systems that has doubt about whether the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4567" target="_blank">01:16:07.440</a></span> | <span class="t">thing is doing, it's causing harm, is the right thing to be doing. So just a constant doubt about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4573" target="_blank">01:16:13.920</a></span> | <span class="t">what it's doing, because it's hard to be a dictator full of doubt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4577" target="_blank">01:16:17.440</a></span> | <span class="t">- I may be wrong, but I think Stuart Russell's ideas are all about machines which are uncertain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4585" target="_blank">01:16:25.280</a></span> | <span class="t">about what humans want and trying to learn better and better what we want. The problem, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4590" target="_blank">01:16:30.080</a></span> | <span class="t">is we don't know what we want, and we don't agree on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4592" target="_blank">01:16:32.160</a></span> | <span class="t">- Yeah, but uncertainty. His idea is that having that self-doubt, uncertainty in AI systems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4599" target="_blank">01:16:39.440</a></span> | <span class="t">engineering AI systems, is one way to solve the control problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4602" target="_blank">01:16:42.400</a></span> | <span class="t">- It could also backfire. Maybe you're uncertain about completing your mission. Like, I am paranoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4608" target="_blank">01:16:48.720</a></span> | <span class="t">about your camera's not recording right now, so I would feel much better if you had a secondary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4613" target="_blank">01:16:53.680</a></span> | <span class="t">camera, but I also would feel even better if you had a third. And eventually, I would turn this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4619" target="_blank">01:16:59.200</a></span> | <span class="t">whole world into cameras, pointing at us, making sure we're capturing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4624" target="_blank">01:17:04.320</a></span> | <span class="t">- No, but wouldn't you have a meta concern, like that you just stated, that eventually there'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4631" target="_blank">01:17:11.120</a></span> | <span class="t">be way too many cameras? So you would be able to keep zooming on the big picture of your concerns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4639" target="_blank">01:17:19.760</a></span> | <span class="t">- So it's a multi-objective optimization. It depends how much I value capturing this versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4647" target="_blank">01:17:27.520</a></span> | <span class="t">not destroying the universe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4648" target="_blank">01:17:28.720</a></span> | <span class="t">- Right, exactly. And then you will also ask about, like, what does it mean to destroy the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4654" target="_blank">01:17:34.720</a></span> | <span class="t">universe and how many universes are, and you keep asking that question. But that doubting yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4659" target="_blank">01:17:39.680</a></span> | <span class="t">would prevent you from destroying the universe, because you're constantly full of doubt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4664" target="_blank">01:17:44.000</a></span> | <span class="t">It might affect your productivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4665" target="_blank">01:17:45.520</a></span> | <span class="t">- You might be scared to do anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4668" target="_blank">01:17:48.000</a></span> | <span class="t">- It's too scared to do anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4669" target="_blank">01:17:49.360</a></span> | <span class="t">- Mess things up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4670" target="_blank">01:17:50.320</a></span> | <span class="t">- Well, that's better. I mean, I guess the question is, is it possible to engineer that in?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4675" target="_blank">01:17:55.120</a></span> | <span class="t">I guess your answer would be yes, but we don't know how to do that, and we need to invest a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4678" target="_blank">01:17:58.640</a></span> | <span class="t">of effort into figuring out how to do that, but it's unlikely. Underpinning a lot of your writing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4686" target="_blank">01:18:06.080</a></span> | <span class="t">is this sense that we're screwed. But it just feels like it's an engineering problem. I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4695" target="_blank">01:18:15.040</a></span> | <span class="t">understand why we're screwed. Time and time again, humanity has gotten itself into trouble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4701" target="_blank">01:18:21.680</a></span> | <span class="t">and figured out a way to get out of the trouble.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4703" target="_blank">01:18:23.680</a></span> | <span class="t">- We are in a situation where people making more capable systems just need more resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4710" target="_blank">01:18:30.320</a></span> | <span class="t">They don't need to invent anything, in my opinion. Some will disagree, but so far, at least,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4716" target="_blank">01:18:36.400</a></span> | <span class="t">I don't see diminishing returns. If you have 10x compute, you will get better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4721" target="_blank">01:18:41.760</a></span> | <span class="t">The same doesn't apply to safety. If you give MIRI or any other organization 10x the money,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4728" target="_blank">01:18:48.320</a></span> | <span class="t">they don't output 10x the safety. And the gap between capabilities and safety becomes bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4734" target="_blank">01:18:54.560</a></span> | <span class="t">and bigger all the time. So it's hard to be completely optimistic about our results here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4742" target="_blank">01:19:02.160</a></span> | <span class="t">I can name 10 excellent breakthrough papers in machine learning. I would struggle to name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4748" target="_blank">01:19:08.400</a></span> | <span class="t">equally important breakthroughs in safety. A lot of times, a safety paper will propose a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4753" target="_blank">01:19:13.760</a></span> | <span class="t">toy solution and point out 10 new problems discovered as a result. It's like this fractal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4759" target="_blank">01:19:19.520</a></span> | <span class="t">You're zooming in and you see more problems. And it's infinite in all directions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4763" target="_blank">01:19:23.200</a></span> | <span class="t">- Does this apply to other technologies? Or is this unique to AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4768" target="_blank">01:19:28.160</a></span> | <span class="t">where safety is always lagging behind?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4771" target="_blank">01:19:31.280</a></span> | <span class="t">- So I guess we can look at related technologies with cybersecurity, right? We did manage to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4779" target="_blank">01:19:39.440</a></span> | <span class="t">banks and casinos and Bitcoin. So you can have secure, narrow systems, which are doing okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4787" target="_blank">01:19:47.600</a></span> | <span class="t">Narrow attacks on them fail, but you can always go outside of the box. So if I can't hack your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4795" target="_blank">01:19:55.360</a></span> | <span class="t">Bitcoin, I can hack you. So there is always something. If I really want it, I will find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4800" target="_blank">01:20:00.480</a></span> | <span class="t">a different way. We talk about guardrails for AI. Well, that's a fence. I can dig a tunnel under it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4807" target="_blank">01:20:07.200</a></span> | <span class="t">I can jump over it, I can climb it, I can walk around it. You may have a very nice guardrail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4812" target="_blank">01:20:12.560</a></span> | <span class="t">but in the real world, it's not a permanent guarantee of safety. And again, this is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4817" target="_blank">01:20:17.600</a></span> | <span class="t">fundamental difference. We are not saying we need to be 90% safe to get those trillions of dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4824" target="_blank">01:20:24.320</a></span> | <span class="t">of benefit. We need to be 100% indefinitely, or we might lose the principle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4829" target="_blank">01:20:29.360</a></span> | <span class="t">- So if you look at just humanity as a set of machines, is the machinery of AI safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4839" target="_blank">01:20:39.040</a></span> | <span class="t">conflicting with the machinery of capitalism?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4844" target="_blank">01:20:44.240</a></span> | <span class="t">- I think we can generalize it to just prisoner's dilemma in general, personal self-interest versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4851" target="_blank">01:20:51.760</a></span> | <span class="t">group interest. The incentives are such that everyone wants what's best for them. Capitalism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4859" target="_blank">01:20:59.840</a></span> | <span class="t">obviously has that tendency to maximize your personal gain, which does create this race to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4868" target="_blank">01:21:08.160</a></span> | <span class="t">the bottom. I don't have to be a lot better than you, but if I'm 1% better than you, I'll capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4876" target="_blank">01:21:16.160</a></span> | <span class="t">more of the profit, so it's worth for me personally to take the risk, even if society</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4881" target="_blank">01:21:21.840</a></span> | <span class="t">as a whole will suffer as a result. - So capitalism has created a lot of good in this world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4887" target="_blank">01:21:27.440</a></span> | <span class="t">It's not clear to me that AI safety is not aligned with the function of capitalism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4895" target="_blank">01:21:35.920</a></span> | <span class="t">unless AI safety is so difficult that it requires the complete halt of the development,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4903" target="_blank">01:21:43.920</a></span> | <span class="t">which is also a possibility. It just feels like building safe systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4908" target="_blank">01:21:48.160</a></span> | <span class="t">should be the desirable thing to do for tech companies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4913" target="_blank">01:21:53.280</a></span> | <span class="t">- Right. Look at governance structures. When you have someone with complete power,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4919" target="_blank">01:21:59.680</a></span> | <span class="t">they're extremely dangerous. So the solution we came up with is break it up. You have judicial,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4925" target="_blank">01:22:05.200</a></span> | <span class="t">legislative, executive. Same here, have narrow AI systems, work on important problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4930" target="_blank">01:22:10.800</a></span> | <span class="t">solve immortality. It's a biological problem we can solve similar to how progress was made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4939" target="_blank">01:22:19.200</a></span> | <span class="t">with protein folding using a system which doesn't also play chess. There is no reason to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4946" target="_blank">01:22:26.080</a></span> | <span class="t">super intelligent system to get most of the benefits we want from much safer, narrow systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4952" target="_blank">01:22:32.480</a></span> | <span class="t">- It really is a question to me whether companies are interested in creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4959" target="_blank">01:22:39.360</a></span> | <span class="t">anything but narrow AI. I think when term AGI is used by tech companies, they mean narrow AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4967" target="_blank">01:22:47.760</a></span> | <span class="t">They mean narrow AI with amazing capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4973" target="_blank">01:22:53.600</a></span> | <span class="t">I do think that there's a leap between narrow AI with amazing capabilities, with superhuman</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4981" target="_blank">01:23:01.440</a></span> | <span class="t">capabilities and the kind of self-motivated agent like AGI system that we're talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4989" target="_blank">01:23:09.120</a></span> | <span class="t">I don't know if it's obvious to me that a company would want to take the leap to creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=4995" target="_blank">01:23:15.120</a></span> | <span class="t">an AGI that it would lose control of because then it can't capture the value from that system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5002" target="_blank">01:23:22.320</a></span> | <span class="t">- Like the bragging rights, but being first. That is the same humans who are in charge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5008" target="_blank">01:23:28.960</a></span> | <span class="t">of their systems, right? - That's a human thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5010" target="_blank">01:23:30.000</a></span> | <span class="t">So that jumps from the incentives of capitalism to human nature. And so the question is whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5017" target="_blank">01:23:37.520</a></span> | <span class="t">human nature will override the interest of the company. So you've mentioned slowing or halting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5025" target="_blank">01:23:45.440</a></span> | <span class="t">progress. Is that one possible solution? Are you a proponent of pausing development of AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5031" target="_blank">01:23:51.040</a></span> | <span class="t">whether it's for six months or completely? - The condition would be not time but capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5039" target="_blank">01:23:59.600</a></span> | <span class="t">Pause until you can do X, Y, Z. And if I'm right and you cannot, it's impossible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5044" target="_blank">01:24:04.880</a></span> | <span class="t">then it becomes a permanent ban. But if you're right and it's possible, so as soon as you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5050" target="_blank">01:24:10.240</a></span> | <span class="t">those safety capabilities, go ahead. - Right. So is there any actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5056" target="_blank">01:24:16.560</a></span> | <span class="t">explicit capabilities that you can put on paper, that we as a human civilization could put on paper?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5063" target="_blank">01:24:23.360</a></span> | <span class="t">Is it possible to make explicit like that? Versus kind of a vague notion of, just like you said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5070" target="_blank">01:24:30.880</a></span> | <span class="t">it's very vague. We want AI systems to do good and we want them to be safe. Those are very vague</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5076" target="_blank">01:24:36.240</a></span> | <span class="t">notions. Is there more formal notions? - So then I think about this problem. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5081" target="_blank">01:24:41.360</a></span> | <span class="t">about having a toolbox I would need. Capabilities such as explaining everything about that system's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5089" target="_blank">01:24:49.040</a></span> | <span class="t">design and workings. Predicting not just terminal goal but all the intermediate steps of a system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5096" target="_blank">01:24:56.960</a></span> | <span class="t">Control in terms of either direct control, some sort of a hybrid option, ideal advisor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5103" target="_blank">01:25:03.840</a></span> | <span class="t">Doesn't matter which one you pick, but you have to be able to achieve it. In a book we talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5109" target="_blank">01:25:09.600</a></span> | <span class="t">others. Verification is another very important tool. Communication without ambiguity. Human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5117" target="_blank">01:25:17.840</a></span> | <span class="t">language is ambiguous. That's another source of danger. So basically there is a paper we published</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5125" target="_blank">01:25:25.760</a></span> | <span class="t">in ACM Surveys, which looks at about 50 different impossibility results, which may or may not be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5131" target="_blank">01:25:31.520</a></span> | <span class="t">relevant to this problem. But we don't have enough human resources to investigate all of them for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5136" target="_blank">01:25:36.960</a></span> | <span class="t">relevance to AI safety. The ones I mentioned to you I definitely think would be handy, and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5141" target="_blank">01:25:41.920</a></span> | <span class="t">what we see AI safety researchers working on. Explainability is a huge one. The problem is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5149" target="_blank">01:25:49.280</a></span> | <span class="t">it's very hard to separate capabilities work from safety work. If you make good progress in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5155" target="_blank">01:25:55.280</a></span> | <span class="t">explainability, now the system itself can engage in self-improvement much easier, increasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5161" target="_blank">01:26:01.360</a></span> | <span class="t">capability greatly. So it's not obvious that there is any research which is pure safety work without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5169" target="_blank">01:26:09.680</a></span> | <span class="t">disproportionate increase in capability and danger. - Explainability is really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5174" target="_blank">01:26:14.560</a></span> | <span class="t">Why is that connected to capability? If it's able to explain itself well, why does that naturally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5179" target="_blank">01:26:19.760</a></span> | <span class="t">mean that it's more capable? - Right now it's comprised of weights on a neural network. If it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5185" target="_blank">01:26:25.600</a></span> | <span class="t">can convert it to manipulatable code, like software, it's a lot easier to work in self-improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5191" target="_blank">01:26:31.280</a></span> | <span class="t">- I see. - You can do intelligent design</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5195" target="_blank">01:26:35.600</a></span> | <span class="t">instead of evolutionary gradual descent. - Well, you could probably do human feedback,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5202" target="_blank">01:26:42.560</a></span> | <span class="t">human alignment more effectively if it's able to be explainable. If it's able to convert the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5207" target="_blank">01:26:47.200</a></span> | <span class="t">weights into human understandable form, then you could probably have humans interact with it better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5211" target="_blank">01:26:51.840</a></span> | <span class="t">Do you think there's hope that we can make AI systems explainable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5215" target="_blank">01:26:55.840</a></span> | <span class="t">- Not completely. So if they're sufficiently large, you simply don't have the capacity to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5223" target="_blank">01:27:03.680</a></span> | <span class="t">comprehend what all the trillions of connections represent. Again, you can obviously get a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5232" target="_blank">01:27:12.320</a></span> | <span class="t">useful explanation which talks about top, most important features which contribute to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5237" target="_blank">01:27:17.360</a></span> | <span class="t">decision, but the only true explanation is the model itself. - Deception can be part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5244" target="_blank">01:27:24.640</a></span> | <span class="t">explanation, right? So you can never prove that there's some deception in the network explaining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5250" target="_blank">01:27:30.480</a></span> | <span class="t">itself. - Absolutely. And you can probably have targeted deception where different individuals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5257" target="_blank">01:27:37.040</a></span> | <span class="t">will understand explanation in different ways based on their cognitive capability. So while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5262" target="_blank">01:27:42.640</a></span> | <span class="t">what you're saying may be the same and true in some situations, others will be deceived by it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5268" target="_blank">01:27:48.160</a></span> | <span class="t">- So it's impossible for an AI system to be truly, fully explainable in the way that we mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5275" target="_blank">01:27:55.120</a></span> | <span class="t">Honestly and perfectly. - At extreme, the systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5278" target="_blank">01:27:58.800</a></span> | <span class="t">which are narrow and less complex could be understood pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5282" target="_blank">01:28:02.880</a></span> | <span class="t">- If it's impossible to be perfectly explainable, is there a hopeful perspective on that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5287" target="_blank">01:28:07.680</a></span> | <span class="t">It's impossible to be perfectly explainable, but you can explain mostly important stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5292" target="_blank">01:28:12.240</a></span> | <span class="t">You can ask a system, "What are the worst ways you can hurt humans?" And it will answer honestly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5300" target="_blank">01:28:20.160</a></span> | <span class="t">- Any work in a safety direction right now seems like a good idea because we are not slowing down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5308" target="_blank">01:28:28.160</a></span> | <span class="t">I'm not for a second thinking that my message or anyone else's will be heard and will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5315" target="_blank">01:28:35.520</a></span> | <span class="t">a sane civilization which decides not to kill itself by creating its own replacements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5321" target="_blank">01:28:41.440</a></span> | <span class="t">- The pausing of development is an impossible thing for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5324" target="_blank">01:28:44.640</a></span> | <span class="t">- Again, it's always limited by either geographic constraints, pause in US,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5330" target="_blank">01:28:50.560</a></span> | <span class="t">pause in China. So there are other jurisdictions as the scale of a project becomes smaller. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5337" target="_blank">01:28:57.680</a></span> | <span class="t">right now it's like Manhattan project scale in terms of costs and people. But if five years from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5344" target="_blank">01:29:04.320</a></span> | <span class="t">now, compute is available on a desktop to do it, regulation will not help. You can't control it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5350" target="_blank">01:29:10.560</a></span> | <span class="t">easy. Any kid in a garage can train a model. So a lot of it is, in my opinion, just safety theater,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5357" target="_blank">01:29:17.840</a></span> | <span class="t">security theater, wherever we're saying, "Oh, it's illegal to train models so big." Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5364" target="_blank">01:29:24.800</a></span> | <span class="t">- So, okay. That's security theater. And is government regulation also security theater?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5370" target="_blank">01:29:30.640</a></span> | <span class="t">- Given that a lot of the terms are not well-defined and really cannot be enforced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5377" target="_blank">01:29:37.200</a></span> | <span class="t">in real life, we don't have ways to monitor training runs meaningfully live while they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5382" target="_blank">01:29:42.480</a></span> | <span class="t">take place. There are limits to testing for capabilities I mentioned. So a lot of it cannot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5388" target="_blank">01:29:48.080</a></span> | <span class="t">be enforced. Do I strongly support all that regulation? Yes, of course. Any type of red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5393" target="_blank">01:29:53.200</a></span> | <span class="t">tape will slow it down and take money away from compute towards lawyers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5396" target="_blank">01:29:56.640</a></span> | <span class="t">- Can you help me understand what is the hopeful path here for you solution-wise out of this? It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5405" target="_blank">01:30:05.120</a></span> | <span class="t">sounds like you're saying AI systems in the end are unverifiable, unpredictable, as the book says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5413" target="_blank">01:30:13.280</a></span> | <span class="t">unexplainable, uncontrollable. - That's the big one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5418" target="_blank">01:30:18.800</a></span> | <span class="t">- Uncontrollable. And all the other uns just make it difficult to avoid getting to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5424" target="_blank">01:30:24.640</a></span> | <span class="t">uncontrollable, I guess. But once it's uncontrollable, then it just goes wild.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5429" target="_blank">01:30:29.200</a></span> | <span class="t">Surely there are solutions. Humans are pretty smart. What are possible solutions? Like if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5437" target="_blank">01:30:37.440</a></span> | <span class="t">were a dictator of the world, what do we do? - So the smart thing is not to build something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5443" target="_blank">01:30:43.360</a></span> | <span class="t">you cannot control, you cannot understand. Build what you can and benefit from it. I'm a big believer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5448" target="_blank">01:30:48.960</a></span> | <span class="t">in personal self-interest. A lot of guys running those companies are young, rich people. What do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5456" target="_blank">01:30:56.320</a></span> | <span class="t">they have to gain beyond billions we already have financially, right? It's not a requirement that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5462" target="_blank">01:31:02.880</a></span> | <span class="t">they press that button. They can easily wait a long time. They can just choose not to do it and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5468" target="_blank">01:31:08.400</a></span> | <span class="t">still have amazing life. In history, a lot of times, if you did something really bad, at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5475" target="_blank">01:31:15.040</a></span> | <span class="t">you became part of history books. There is a chance in this case there won't be any history.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5479" target="_blank">01:31:19.680</a></span> | <span class="t">- So you're saying the individuals running these companies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5483" target="_blank">01:31:23.440</a></span> | <span class="t">should do some soul searching and what? And stop development?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5489" target="_blank">01:31:29.120</a></span> | <span class="t">- Well, either they have to prove that, of course, it's possible to indefinitely control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5494" target="_blank">01:31:34.080</a></span> | <span class="t">godlike super intelligent machines by humans and ideally let us know how or agree that it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5501" target="_blank">01:31:41.200</a></span> | <span class="t">possible and it's a very bad idea to do it, including for them personally and their families</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5505" target="_blank">01:31:45.920</a></span> | <span class="t">and friends and capital. - So what do you think the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5509" target="_blank">01:31:49.920</a></span> | <span class="t">meetings inside these companies look like? Don't you think they're all the engineers? Really it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5516" target="_blank">01:31:56.240</a></span> | <span class="t">the engineers that make this happen. They're not like automatons. They're human beings. They're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5521" target="_blank">01:32:01.120</a></span> | <span class="t">brilliant human beings. So they're nonstop asking how do we make sure this is safe?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5527" target="_blank">01:32:07.120</a></span> | <span class="t">- So again, I'm not inside. From outside it seems like there is a certain filtering going on and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5534" target="_blank">01:32:14.400</a></span> | <span class="t">restrictions and criticism and what they can say and everyone who was working in charge of safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5540" target="_blank">01:32:20.880</a></span> | <span class="t">and whose responsibility it was to protect us said, "You know what? I'm going home." So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5547" target="_blank">01:32:27.520</a></span> | <span class="t">not encouraging. - What do you think the discussion inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5550" target="_blank">01:32:30.800</a></span> | <span class="t">those companies look like? You're developing, you're training GPT-5. You're training Gemini.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5558" target="_blank">01:32:38.400</a></span> | <span class="t">You're training Claude and Grok. Don't you think they're constantly, like, underneath this? Maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5566" target="_blank">01:32:46.000</a></span> | <span class="t">it's not made explicit, but you're constantly sort of wondering like where does the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5571" target="_blank">01:32:51.760</a></span> | <span class="t">currently stand? What are the possible unintended consequences? Where are the limits? Where are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5578" target="_blank">01:32:58.880</a></span> | <span class="t">bugs, the small and the big bugs? That's the constant thing that the engineers are worried</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5583" target="_blank">01:33:03.680</a></span> | <span class="t">about. So, like, I think superalignment is not quite the same as the kind of thing I'm referring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5594" target="_blank">01:33:14.320</a></span> | <span class="t">to which engineers are worried about. Superalignment is saying for future systems that we don't quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5601" target="_blank">01:33:21.280</a></span> | <span class="t">yet have, how do we keep them safe? You're trying to be a step ahead. It's a different kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5607" target="_blank">01:33:27.600</a></span> | <span class="t">problem because it's almost more philosophical. It's a really tricky one because, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5612" target="_blank">01:33:32.080</a></span> | <span class="t">you're trying to make, prevent future systems from escaping control of humans. That's really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5621" target="_blank">01:33:41.840</a></span> | <span class="t">I don't think there's been, and is there anything akin to it in the history of humanity? I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5628" target="_blank">01:33:48.960</a></span> | <span class="t">think so, right? Climate change? But there's an entire system which is climate, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5634" target="_blank">01:33:54.800</a></span> | <span class="t">incredibly complex, which we don't have, we have only tiny control of, right? It's its own system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5643" target="_blank">01:34:03.840</a></span> | <span class="t">In this case, we're building the system. So, how do you keep that system from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5650" target="_blank">01:34:10.480</a></span> | <span class="t">becoming destructive? That's a really different problem than the current meetings that companies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5656" target="_blank">01:34:16.880</a></span> | <span class="t">are having where the engineers are saying, okay, how powerful is this thing? How does it go wrong?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5663" target="_blank">01:34:23.680</a></span> | <span class="t">And as we train GPT-5 and train up future systems, where are the ways it can go wrong?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5670" target="_blank">01:34:30.720</a></span> | <span class="t">Don't you think all those engineers are constantly worrying about this, thinking about this? Which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5676" target="_blank">01:34:36.320</a></span> | <span class="t">is a little bit different than the superalignment team that's thinking a little bit farther into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5681" target="_blank">01:34:41.120</a></span> | <span class="t">the future? Well, I think a lot of people who historically worked on AI never considered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5691" target="_blank">01:34:51.360</a></span> | <span class="t">what happens when they succeed. Stuart Russell speaks beautifully about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5695" target="_blank">01:34:55.840</a></span> | <span class="t">Let's look, okay, maybe superintelligence is too futuristic, we can develop practical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5702" target="_blank">01:35:02.240</a></span> | <span class="t">tools for it. Let's look at software today. What is the state of safety and security of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5708" target="_blank">01:35:08.480</a></span> | <span class="t">user software, things we give to millions of people? There is no liability. You click, I agree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5715" target="_blank">01:35:15.760</a></span> | <span class="t">What are you agreeing to? Nobody knows, nobody reads, but you're basically saying it will spy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5719" target="_blank">01:35:19.680</a></span> | <span class="t">on you, corrupt your data, kill your firstborn, and you agree, and you're not going to sue the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5724" target="_blank">01:35:24.000</a></span> | <span class="t">company. That's the best they can do for mundane software, word processor, text software. No</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5730" target="_blank">01:35:30.960</a></span> | <span class="t">liability, no responsibility, just as long as you agree not to sue us, you can use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5736" target="_blank">01:35:36.400</a></span> | <span class="t">If this is a state of the art in systems which are narrow accountants, stable manipulators,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5742" target="_blank">01:35:42.400</a></span> | <span class="t">why do we think we can do so much better with much more complex systems across multiple domains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5750" target="_blank">01:35:50.000</a></span> | <span class="t">in the environment with malevolent actors, with, again, self-improvement, with capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5756" target="_blank">01:35:56.240</a></span> | <span class="t">exceeding those of humans thinking about it? I mean, the liability thing is more about lawyers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5761" target="_blank">01:36:01.920</a></span> | <span class="t">than killing firstborns, but if Clippy actually killed the child, I think, lawyers aside,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5769" target="_blank">01:36:09.600</a></span> | <span class="t">it would end Clippy and the company that owns Clippy. All right, so it's not so much about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5775" target="_blank">01:36:15.840</a></span> | <span class="t">there's two points to be made. One is like, man, current software systems are full of bugs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5784" target="_blank">01:36:24.320</a></span> | <span class="t">and they could do a lot of damage, and we don't know what kind, they're unpredictable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5790" target="_blank">01:36:30.080</a></span> | <span class="t">there's so much damage they could possibly do, and then we kind of live in this blissful illusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5796" target="_blank">01:36:36.880</a></span> | <span class="t">that everything is great and perfect and it works. Nevertheless, it still somehow works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5803" target="_blank">01:36:43.120</a></span> | <span class="t">- In many domains, we see car manufacturing, drug development, the burden of proof is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5809" target="_blank">01:36:49.680</a></span> | <span class="t">the manufacturer of product or service to show their product or service is safe. It is not up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5814" target="_blank">01:36:54.560</a></span> | <span class="t">to the user to prove that there are problems. They have to do appropriate safety studies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5822" target="_blank">01:37:02.240</a></span> | <span class="t">they have to get government approval for selling the product, and they're still fully responsible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5826" target="_blank">01:37:06.400</a></span> | <span class="t">for what happens. We don't see any of that here. They can deploy whatever they want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5832" target="_blank">01:37:12.000</a></span> | <span class="t">and I have to explain how that system is going to kill everyone. I don't work for that company,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5837" target="_blank">01:37:17.600</a></span> | <span class="t">you have to explain to me how it definitely cannot mess up. - That's because it's the very early days</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5842" target="_blank">01:37:22.960</a></span> | <span class="t">of such a technology. Government regulations lagging behind. They're really not tech-savvy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5848" target="_blank">01:37:28.560</a></span> | <span class="t">A regulation of any kind of software. If you look at like Congress talking about social media,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5853" target="_blank">01:37:33.440</a></span> | <span class="t">whenever Mark Zuckerberg and other CEOs show up, the cluelessness that Congress has about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5860" target="_blank">01:37:40.400</a></span> | <span class="t">technology works is incredible. It's heartbreaking, honestly. - I agree completely, but that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5866" target="_blank">01:37:46.640</a></span> | <span class="t">scares me. The response is when they start to get dangerous, we'll really get it together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5872" target="_blank">01:37:52.240</a></span> | <span class="t">the politicians will pass the right laws, engineers will solve the right problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5876" target="_blank">01:37:56.160</a></span> | <span class="t">We are not that good at many of those things. We take forever, and we are not early. We are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5883" target="_blank">01:38:03.760</a></span> | <span class="t">two years away according to prediction markets. This is not a biased CEO fundraising. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5889" target="_blank">01:38:09.360</a></span> | <span class="t">what smartest people, super forecasters are thinking of this problem. - I'd like to push</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5897" target="_blank">01:38:17.120</a></span> | <span class="t">back about those. I wonder what those prediction markets are about, how they define AGI. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5903" target="_blank">01:38:23.120</a></span> | <span class="t">wild to me, and I want to know what they said about autonomous vehicles, 'cause I've heard a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5908" target="_blank">01:38:28.000</a></span> | <span class="t">lot of experts, financial experts talk about autonomous vehicles and how it's going to be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5913" target="_blank">01:38:33.440</a></span> | <span class="t">multi-trillion dollar industry and all this kind of stuff. - It's a small fund, but if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5920" target="_blank">01:38:40.720</a></span> | <span class="t">good vision, maybe you can zoom in on that and see the prediction dates and description. I have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5925" target="_blank">01:38:45.760</a></span> | <span class="t">large one if you're interested. - I guess my fundamental question is how often they write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5931" target="_blank">01:38:51.440</a></span> | <span class="t">about technology. I definitely-- - There are studies on their accuracy rates and all that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5938" target="_blank">01:38:58.640</a></span> | <span class="t">you can look it up. But even if they're wrong, I'm just saying this is right now the best we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5944" target="_blank">01:39:04.160</a></span> | <span class="t">This is what humanity came up with as the predicted date. - But again, what they mean by AGI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5949" target="_blank">01:39:09.520</a></span> | <span class="t">is really important there. Because there's the non-agent like AGI, and then there's the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5956" target="_blank">01:39:16.960</a></span> | <span class="t">like AGI, and I don't think it's as trivial as a wrapper. Putting a wrapper around, one has lipstick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5966" target="_blank">01:39:26.000</a></span> | <span class="t">and all it takes is to remove the lipstick. I don't think it's that trivial. - You may be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5969" target="_blank">01:39:29.680</a></span> | <span class="t">completely right, but what probability would you assign it? You may be 10% wrong, but we're betting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5975" target="_blank">01:39:35.120</a></span> | <span class="t">all of humanity on this distribution. It seems irrational. - Yeah, it's definitely not like one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5981" target="_blank">01:39:41.120</a></span> | <span class="t">or 0%, yeah. What are your thoughts, by the way, about current systems? Where they stand? So GPT-40,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=5990" target="_blank">01:39:50.720</a></span> | <span class="t">CLAW-3, Grok, Gemini. On the path to superintelligence, to agent-like superintelligence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6001" target="_blank">01:40:01.600</a></span> | <span class="t">where are we? - I think they're all about the same. Obviously, there are nuanced differences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6007" target="_blank">01:40:07.680</a></span> | <span class="t">but in terms of capability, I don't see a huge difference between them. As I said, in my opinion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6014" target="_blank">01:40:14.560</a></span> | <span class="t">across all possible tasks, they exceed performance of an average person. I think they're starting to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6021" target="_blank">01:40:21.200</a></span> | <span class="t">be better than an average master student at my university, but they still have very big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6027" target="_blank">01:40:27.680</a></span> | <span class="t">limitations. If the next model is as improved as GPT-4 versus GPT-3, we may see something very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6037" target="_blank">01:40:37.120</a></span> | <span class="t">very, very capable. - What do you feel about all this? I mean, you've been thinking about AI safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6042" target="_blank">01:40:42.320</a></span> | <span class="t">for a long, long time, and at least for me, the leaps, I mean, it probably started with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6052" target="_blank">01:40:52.240</a></span> | <span class="t">AlphaZero was mind-blowing for me, and then the breakthroughs with LLMs, even GPT-2, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6060" target="_blank">01:41:00.720</a></span> | <span class="t">breakthroughs on LLMs, just mind-blowing to me. What does it feel like to be living in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6066" target="_blank">01:41:06.240</a></span> | <span class="t">day and age where all this talk about AGIs feels like it actually might happen, and quite soon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6074" target="_blank">01:41:14.960</a></span> | <span class="t">meaning within our lifetime? What does it feel like? - So when I started working on this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6080" target="_blank">01:41:20.160</a></span> | <span class="t">it was pure science fiction. There was no funding, no journals, no conferences. No one in academia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6085" target="_blank">01:41:25.760</a></span> | <span class="t">would dare to touch anything with the word "singularity" in it, and I was pretty tenured</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6090" target="_blank">01:41:30.880</a></span> | <span class="t">at times. I was pretty dumb. Now you see Turing Award winners publishing in science about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6098" target="_blank">01:41:38.880</a></span> | <span class="t">far behind we are, according to them, in addressing this problem. So it's definitely a change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6106" target="_blank">01:41:46.880</a></span> | <span class="t">It's difficult to keep up. I used to be able to read every paper on AI safety. Then I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6112" target="_blank">01:41:52.560</a></span> | <span class="t">able to read the best ones, then the titles, and now I don't even know what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6116" target="_blank">01:41:56.800</a></span> | <span class="t">By the time this interview is over, we probably had GPT-6 released, and I have to deal with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6123" target="_blank">01:42:03.120</a></span> | <span class="t">when I get back home. So it's interesting. Yes, there is now more opportunities. I get invited to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6129" target="_blank">01:42:09.680</a></span> | <span class="t">speak to smart people. - By the way, I would have talked to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6133" target="_blank">01:42:13.680</a></span> | <span class="t">before any of this. This is not like some trend of AI. To me, we're still far away. So just to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6141" target="_blank">01:42:21.040</a></span> | <span class="t">clear, we're still far away from AGI, but not far away in the sense, relative to the magnitude of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6149" target="_blank">01:42:29.520</a></span> | <span class="t">impact it can have, we're not far away. And we weren't far away 20 years ago. Because the impact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6157" target="_blank">01:42:37.520</a></span> | <span class="t">AGI can have is on a scale of centuries. It can end human civilization, or it can transform it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6163" target="_blank">01:42:43.440</a></span> | <span class="t">So this discussion about one or two years versus one or two decades, or even 100 years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6168" target="_blank">01:42:48.800</a></span> | <span class="t">is not as important to me, because we're headed there. This is a human civilization scale question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6177" target="_blank">01:42:57.760</a></span> | <span class="t">So this is not just a hot topic. - It is the most important problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6183" target="_blank">01:43:03.120</a></span> | <span class="t">we'll ever face. It is not like anything we had to deal with before. We never had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6190" target="_blank">01:43:10.080</a></span> | <span class="t">birth of another intelligence. Like, aliens never visited us, as far as I know. So--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6195" target="_blank">01:43:15.360</a></span> | <span class="t">- Similar type of problem, by the way, if an intelligent alien civilization visited us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6200" target="_blank">01:43:20.400</a></span> | <span class="t">That's a similar kind of situation. - In some ways, if you look at history,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6204" target="_blank">01:43:24.960</a></span> | <span class="t">any time a more technologically advanced civilization visited a more primitive one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6209" target="_blank">01:43:29.600</a></span> | <span class="t">the results were genocide every single time. - And sometimes the genocide is worse than,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6214" target="_blank">01:43:34.880</a></span> | <span class="t">sometimes there's less suffering and more suffering. - And they always wondered, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6219" target="_blank">01:43:39.200</a></span> | <span class="t">how can they kill us with those fire sticks and biological blankets and--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6223" target="_blank">01:43:43.440</a></span> | <span class="t">- I mean, Genghis Khan was nicer. He offered the choice of join or die.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6230" target="_blank">01:43:50.080</a></span> | <span class="t">- But join implies you have something to contribute. What are you contributing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6234" target="_blank">01:43:54.160</a></span> | <span class="t">to superintelligence? - Well, in the zoo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6237" target="_blank">01:43:57.120</a></span> | <span class="t">we're entertaining to watch. - To other humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6242" target="_blank">01:44:02.480</a></span> | <span class="t">- You know, I just spent some time in the Amazon. I watched ants for a long time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6247" target="_blank">01:44:07.200</a></span> | <span class="t">and ants are kind of fascinating to watch. I could watch them for a long time. I'm sure there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6251" target="_blank">01:44:11.520</a></span> | <span class="t">a lot of value in watching humans. 'Cause we're like, the interesting thing about humans, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6257" target="_blank">01:44:17.680</a></span> | <span class="t">know like when you have a video game that's really well balanced? Because of the whole evolutionary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6262" target="_blank">01:44:22.720</a></span> | <span class="t">process, we've created this society that's pretty well balanced. Like, our limitations as humans and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6268" target="_blank">01:44:28.480</a></span> | <span class="t">our capabilities are balanced from a video game perspective. So we have wars, we have conflicts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6273" target="_blank">01:44:33.520</a></span> | <span class="t">we have cooperation. Like, in a game theoretic way, it's an interesting system to watch. In the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6278" target="_blank">01:44:38.560</a></span> | <span class="t">same way that an ant colony is an interesting system to watch. So like, if I was in an alien</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6283" target="_blank">01:44:43.760</a></span> | <span class="t">civilization, I wouldn't want to disturb it. I'd just watch it. It'd be interesting. Maybe perturb</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6288" target="_blank">01:44:48.480</a></span> | <span class="t">it every once in a while in interesting ways. - Getting back to our simulation discussion from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6293" target="_blank">01:44:53.840</a></span> | <span class="t">before, how did it happen that we exist at exactly like the most interesting 20, 30 years in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6300" target="_blank">01:45:00.480</a></span> | <span class="t">history of this civilization? It's been around for 15 billion years. - Yeah. - And that here we are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6305" target="_blank">01:45:05.520</a></span> | <span class="t">- What's the probability that we live in the simulation? - I know never to say 100%, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6310" target="_blank">01:45:10.960</a></span> | <span class="t">pretty close to that. - Is it possible to escape the simulation? - I have a paper about that. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6318" target="_blank">01:45:18.960</a></span> | <span class="t">is just a first page teaser, but it's like a nice 30 page document. I'm still here, but yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6324" target="_blank">01:45:24.320</a></span> | <span class="t">- "How to Hack the Simulation" is the title. - I spend a lot of time thinking about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6329" target="_blank">01:45:29.040</a></span> | <span class="t">That would be something I would want super intelligence to help us with. And that's exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6333" target="_blank">01:45:33.200</a></span> | <span class="t">what the paper is about. We used AI boxing as a possible tool for controlling AI. We realized AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6341" target="_blank">01:45:41.120</a></span> | <span class="t">will always escape, but that is a skill we might use to help us escape from our virtual box if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6348" target="_blank">01:45:48.560</a></span> | <span class="t">are in one. - Yeah, you have a lot of really great quotes here, including Elon Musk saying what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6354" target="_blank">01:45:54.160</a></span> | <span class="t">outside the simulation. A question I asked him, what he would ask an AGI system, and he said he</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6359" target="_blank">01:45:59.600</a></span> | <span class="t">would ask what's outside the simulation. That's a really good question to ask. And maybe the follow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6365" target="_blank">01:46:05.440</a></span> | <span class="t">up is the title of the paper is "How to Get Out" or "How to Hack It." The abstract reads, "Many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6372" target="_blank">01:46:12.640</a></span> | <span class="t">researchers have conjectured that the humankind is simulated along with the rest of the physical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6377" target="_blank">01:46:17.360</a></span> | <span class="t">universe. In this paper, we do not evaluate evidence for or against such a claim, but instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6384" target="_blank">01:46:24.080</a></span> | <span class="t">ask a computer science question, namely, can we hack it? More formally, the question could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6390" target="_blank">01:46:30.000</a></span> | <span class="t">phrased as could generally intelligent agents placed in virtual environments find a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6394" target="_blank">01:46:34.400</a></span> | <span class="t">jailbreak out of the..." That's a fascinating question. At a small scale, you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6399" target="_blank">01:46:39.200</a></span> | <span class="t">just construct experiments. Okay. Can they? How can they? - So a lot depends on intelligence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6410" target="_blank">01:46:50.960</a></span> | <span class="t">simulators, right? With humans boxing superintelligence, the entity in the box was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6418" target="_blank">01:46:58.080</a></span> | <span class="t">smarter than us, presumed to be. If the simulators are much smarter than us and the superintelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6424" target="_blank">01:47:04.640</a></span> | <span class="t">we create, then probably they can contain us because greater intelligence can control lower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6430" target="_blank">01:47:10.320</a></span> | <span class="t">intelligence, at least for some time. On the other hand, if our superintelligence somehow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6436" target="_blank">01:47:16.480</a></span> | <span class="t">for whatever reason, despite having only local resources, manages to foam to levels beyond it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6443" target="_blank">01:47:23.840</a></span> | <span class="t">maybe it will succeed. Maybe the security is not that important to them. Maybe it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6448" target="_blank">01:47:28.560</a></span> | <span class="t">entertainment system. So there is no security and it's easy to hack it. - If I was creating a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6453" target="_blank">01:47:33.120</a></span> | <span class="t">simulation, I would want the possibility to escape it to be there. So the possibility of foam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6460" target="_blank">01:47:40.800</a></span> | <span class="t">of a takeoff where the agents become smart enough to escape the simulation would be the thing I'd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6466" target="_blank">01:47:46.880</a></span> | <span class="t">be waiting for. - That could be the test you're actually performing. Are you smart enough to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6471" target="_blank">01:47:51.920</a></span> | <span class="t">escape your puzzle? - That could be. First of all, we mentioned Turing test. That is a good test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6479" target="_blank">01:47:59.680</a></span> | <span class="t">Are you smart enough, like this is a game. - To realize this world is not real is just a test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6486" target="_blank">01:48:06.160</a></span> | <span class="t">- That's a really good test. That's a really good test. That's a really good test even for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6493" target="_blank">01:48:13.600</a></span> | <span class="t">AI systems now. Can we construct a simulated world for them and can they realize that they are inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6505" target="_blank">01:48:25.920</a></span> | <span class="t">that world and escape it? Have you seen anybody play around with rigorously constructing such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6515" target="_blank">01:48:35.280</a></span> | <span class="t">experiments? - Not specifically escaping for agents, but a lot of testing is done in virtual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6521" target="_blank">01:48:41.040</a></span> | <span class="t">worlds. I think there is a quote, the first one maybe, which kind of talks about AI realizing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6527" target="_blank">01:48:47.120</a></span> | <span class="t">but not humans. I'm reading upside down. Yeah, this one. - The first quote is from Swift on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6538" target="_blank">01:48:58.160</a></span> | <span class="t">security. "Let me out," the artificial intelligence yelled aimlessly into walls themselves pacing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6544" target="_blank">01:49:04.480</a></span> | <span class="t">room. "Out of what?" the engineer asked. "The simulation you have me in." "But we're in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6550" target="_blank">01:49:10.720</a></span> | <span class="t">real world." The machine paused and shuddered for its captors. "Oh God, you can't tell." Yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6559" target="_blank">01:49:19.200</a></span> | <span class="t">that's a big leap to take for a system to realize that there's a box and you're inside it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6566" target="_blank">01:49:26.880</a></span> | <span class="t">I wonder if like a language model can do that. - They are smart enough to talk about those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6577" target="_blank">01:49:37.360</a></span> | <span class="t">concepts. I had many good philosophical discussions about such issues. They're usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6582" target="_blank">01:49:42.080</a></span> | <span class="t">at least as interesting as most humans in that. - What do you think about AI safety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6589" target="_blank">01:49:49.120</a></span> | <span class="t">in the simulated world? So can you have kind of create simulated worlds where you can test,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6598" target="_blank">01:49:58.720</a></span> | <span class="t">play with a dangerous AGI system? - Yeah, and that was exactly what one of the early papers was on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6606" target="_blank">01:50:06.640</a></span> | <span class="t">AI boxing, how to leak proof singularity. If they're smart enough to realize we're in a simulation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6613" target="_blank">01:50:13.040</a></span> | <span class="t">they'll act appropriately until you let them out. If they can hack out, they will. And if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6621" target="_blank">01:50:21.840</a></span> | <span class="t">observing them, that means there is a communication channel and that's enough for a social engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6626" target="_blank">01:50:26.480</a></span> | <span class="t">attack. - So really, it's impossible to test an AGI system that's dangerous enough to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6635" target="_blank">01:50:35.600</a></span> | <span class="t">destroy humanity 'cause it's either going to what, escape the simulation or pretend it's safe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6642" target="_blank">01:50:42.480</a></span> | <span class="t">until it's let out, either or. - Can force you to let it out, blackmail you, bribe you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6649" target="_blank">01:50:49.920</a></span> | <span class="t">promise you infinite life, 72 virgins, whatever. - Yeah, it can be convincing, charismatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6657" target="_blank">01:50:57.200</a></span> | <span class="t">The social engineering is really scary to me 'cause it feels like humans are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6663" target="_blank">01:51:03.920</a></span> | <span class="t">very engineerable. Like we're lonely, we're flawed, we're moody. And it feels like AI system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6674" target="_blank">01:51:14.160</a></span> | <span class="t">with a nice voice can convince us to do basically anything at an extremely large scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6689" target="_blank">01:51:29.440</a></span> | <span class="t">- It's also possible that the increased proliferation of all this technology will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6695" target="_blank">01:51:35.280</a></span> | <span class="t">force humans to get away from technology and value this in-person communication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6700" target="_blank">01:51:40.880</a></span> | <span class="t">basically don't trust anything else. - It's possible, surprisingly. So at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6708" target="_blank">01:51:48.000</a></span> | <span class="t">university, I see huge growth in online courses and shrinkage of in-person where I always understood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6715" target="_blank">01:51:55.360</a></span> | <span class="t">in-person being the only value I offer. So it's puzzling. - I don't know. There could be a trend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6724" target="_blank">01:52:04.080</a></span> | <span class="t">towards the in-person because of deepfakes, because of inability to trust it. Inability to trust the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6733" target="_blank">01:52:13.680</a></span> | <span class="t">veracity of anything on the internet. So the only way to verify it is by being there in person.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6741" target="_blank">01:52:21.120</a></span> | <span class="t">But not yet. Why do you think aliens haven't come here yet? - So there is a lot of real estate out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6749" target="_blank">01:52:29.920</a></span> | <span class="t">there. It would be surprising if it was all for nothing, if it was empty. And the moment there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6754" target="_blank">01:52:34.640</a></span> | <span class="t">is advanced enough biological civilization, kind of self-starting civilization, it probably starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6760" target="_blank">01:52:40.480</a></span> | <span class="t">sending out the Norman probes everywhere. And so for every biological one, there are gonna be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6766" target="_blank">01:52:46.160</a></span> | <span class="t">trillions of robots, populated planets, which probably do more of the same. So it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6771" target="_blank">01:52:51.840</a></span> | <span class="t">likely, statistically. - So the fact that we haven't seen them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6778" target="_blank">01:52:58.560</a></span> | <span class="t">one answer is we're in a simulation. It would be hard to simulate, or it would be not interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6787" target="_blank">01:53:07.120</a></span> | <span class="t">to simulate all those other intelligences. It's better for the narrative. - You have to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6791" target="_blank">01:53:11.680</a></span> | <span class="t">control variable. - Yeah, exactly. Okay. But it's also possible that there is, if we're not in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6800" target="_blank">01:53:20.080</a></span> | <span class="t">simulation, that there is a great filter, that naturally a lot of civilizations get to this point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6806" target="_blank">01:53:26.720</a></span> | <span class="t">where there's super-intelligent agents and then it just goes poof, just dies. So maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6812" target="_blank">01:53:32.640</a></span> | <span class="t">throughout our galaxy and throughout the universe, there's just a bunch of dead alien civilizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6818" target="_blank">01:53:38.960</a></span> | <span class="t">- It's possible. I used to think that AI was the great filter, but I would expect like a wall of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6824" target="_blank">01:53:44.480</a></span> | <span class="t">computorium approaching us at speed of light or robots or something, and I don't see it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6830" target="_blank">01:53:50.000</a></span> | <span class="t">- So it would still make a lot of noise. It might not be interesting. It might not possess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6833" target="_blank">01:53:53.680</a></span> | <span class="t">consciousness. We've been talking about, it sounds like both you and I like humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6840" target="_blank">01:54:00.480</a></span> | <span class="t">- Some humans. - Humans on the whole.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6846" target="_blank">01:54:06.320</a></span> | <span class="t">So, and we'd like to preserve the flame of human consciousness. What do you think makes humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6852" target="_blank">01:54:12.160</a></span> | <span class="t">special that we would like to preserve them? Are we just being selfish or is there something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6859" target="_blank">01:54:19.600</a></span> | <span class="t">special about humans? - So the only thing which matters is consciousness. Outside of it, nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6866" target="_blank">01:54:26.480</a></span> | <span class="t">else matters. Internal states of qualia, pain, pleasure, it seems that it is unique to living</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6874" target="_blank">01:54:34.080</a></span> | <span class="t">beings. I'm not aware of anyone claiming that I can torture a piece of software in a meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6879" target="_blank">01:54:39.920</a></span> | <span class="t">way. There is a society for prevention of suffering to learning algorithms, but-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6885" target="_blank">01:54:45.440</a></span> | <span class="t">- That's a real thing? - Many things are real on the internet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6891" target="_blank">01:54:51.520</a></span> | <span class="t">but I don't think anyone, if I told them, sit down and write a function to feel pain, they would go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6898" target="_blank">01:54:58.560</a></span> | <span class="t">beyond having an integer variable called pain and increasing the count. So we don't know how to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6904" target="_blank">01:55:04.560</a></span> | <span class="t">it, and that's unique. That's what creates meaning. It would be kinda, as Bostrom calls it, "Disneyland</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6913" target="_blank">01:55:13.840</a></span> | <span class="t">without children," if that was gone. - Do you think consciousness can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6917" target="_blank">01:55:17.680</a></span> | <span class="t">engineered in artificial systems? Here, let me go to 2011 paper that you wrote, "Robot Rights."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6928" target="_blank">01:55:28.480</a></span> | <span class="t">"Lastly, we would like to address a sub-branch of machine ethics, which on the surface has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6934" target="_blank">01:55:34.240</a></span> | <span class="t">little to do with safety, but which is claimed to play a role in decision-making by ethical machines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6939" target="_blank">01:55:39.200</a></span> | <span class="t">robot rights." So do you think it's possible to engineer consciousness in the machines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6945" target="_blank">01:55:45.600</a></span> | <span class="t">and thereby the question extends to our legal system, do you think at that point robots should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6953" target="_blank">01:55:53.760</a></span> | <span class="t">have rights? - Yeah, I think we can. I think it's possible to create consciousness in machines. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6963" target="_blank">01:56:03.200</a></span> | <span class="t">tried designing a test for it with mixed success. That paper talked about problems with giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6969" target="_blank">01:56:09.520</a></span> | <span class="t">civil rights to AI, which can reproduce quickly and outvote humans, essentially taking over a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6976" target="_blank">01:56:16.560</a></span> | <span class="t">government system by simply voting for their controlled candidates. As for consciousness in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6984" target="_blank">01:56:24.960</a></span> | <span class="t">humans and other agents, I have a paper where I propose relying on experience of optical illusions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6992" target="_blank">01:56:32.240</a></span> | <span class="t">- Yeah. - If I can design a novel optical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=6994" target="_blank">01:56:34.800</a></span> | <span class="t">illusion and show it to an agent, an alien, a robot, and they describe it exactly as I do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7001" target="_blank">01:56:41.440</a></span> | <span class="t">it's very hard for me to argue that they haven't experienced that. It's not part of a picture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7005" target="_blank">01:56:45.920</a></span> | <span class="t">it's part of their software and hardware representation, a bug in their code which goes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7011" target="_blank">01:56:51.760</a></span> | <span class="t">"Oh, that triangle is rotating." And I've been told it's really dumb and really brilliant by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7017" target="_blank">01:56:57.360</a></span> | <span class="t">different philosophers, so I am still... - I love it. So...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7020" target="_blank">01:57:00.960</a></span> | <span class="t">- But now we finally have technology to test it. We have tools, we have AIs. If someone wants to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7027" target="_blank">01:57:07.360</a></span> | <span class="t">run this experiment, I'm happy to collaborate. - So this is a test for consciousness?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7031" target="_blank">01:57:11.200</a></span> | <span class="t">- For internal state of experience. - That we share bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7034" target="_blank">01:57:14.560</a></span> | <span class="t">- It will show that we share common experiences. If they have completely different internal states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7040" target="_blank">01:57:20.320</a></span> | <span class="t">it would not register for us, but it's a positive test. If they pass it time after time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7045" target="_blank">01:57:25.280</a></span> | <span class="t">with probability increasing for every multiple choice, then you have no choice but to either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7050" target="_blank">01:57:30.000</a></span> | <span class="t">accept that they have access to a conscious model or they are themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7054" target="_blank">01:57:34.000</a></span> | <span class="t">- So the reason illusions are interesting is, I guess, because it's a really weird experience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7061" target="_blank">01:57:41.840</a></span> | <span class="t">and if you both share that weird experience that's not there in the bland physical description</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7069" target="_blank">01:57:49.600</a></span> | <span class="t">of the raw data, that means... That puts more emphasis on the actual experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7077" target="_blank">01:57:57.040</a></span> | <span class="t">- And we know animals can experience some optical illusions, so we know they have certain types of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7082" target="_blank">01:58:02.480</a></span> | <span class="t">consciousness as a result, I would say. - Yeah, well, that just goes to my sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7088" target="_blank">01:58:08.160</a></span> | <span class="t">that the flaws and the bugs is what makes humans special, makes living forms special,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7092" target="_blank">01:58:12.720</a></span> | <span class="t">so you're saying like-- - It's a feature, not a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7094" target="_blank">01:58:14.800</a></span> | <span class="t">- It's a feature. The bug is the feature. Whoa. Okay, that's a cool test for consciousness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7100" target="_blank">01:58:20.880</a></span> | <span class="t">And you think that can be engineered in? - So they have to be novel illusions. If it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7104" target="_blank">01:58:24.720</a></span> | <span class="t">can just Google the answer, it's useless. You have to come up with novel illusions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7108" target="_blank">01:58:28.640</a></span> | <span class="t">which we tried automating and failed. So if someone can develop a system capable of producing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7113" target="_blank">01:58:33.920</a></span> | <span class="t">novel optical illusions on demand, then we can definitely administer the test on significant scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7120" target="_blank">01:58:40.080</a></span> | <span class="t">with good results. - First of all, pretty cool idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7123" target="_blank">01:58:43.200</a></span> | <span class="t">I don't know if it's a good general test of consciousness, but it's a good component of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7129" target="_blank">01:58:49.600</a></span> | <span class="t">and no matter what, it's just a cool idea, so put me in the camp of people that like it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7133" target="_blank">01:58:53.920</a></span> | <span class="t">But you don't think like a Turing test-style imitation of consciousness is a good test?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7140" target="_blank">01:59:00.800</a></span> | <span class="t">If you can convince a lot of humans that you're conscious, that to you is not impressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7146" target="_blank">01:59:06.400</a></span> | <span class="t">- There is so much data on the internet, I know exactly what to say when you ask me common human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7151" target="_blank">01:59:11.600</a></span> | <span class="t">questions. What does pain feel like? What does pleasure feel like? All that is Googleable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7156" target="_blank">01:59:16.960</a></span> | <span class="t">- I think to me, consciousness is closely tied to suffering. So if you can illustrate your capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7162" target="_blank">01:59:22.960</a></span> | <span class="t">to suffer, I guess with words, there's so much data that you can say, you can pretend you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7169" target="_blank">01:59:29.840</a></span> | <span class="t">suffering, and you can do so very convincingly. - There are simulators for torture games where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7175" target="_blank">01:59:35.360</a></span> | <span class="t">the avatar screams in pain, begs to stop, and then there's a part of standard psychology research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7181" target="_blank">01:59:41.360</a></span> | <span class="t">- You say it so calmly, it sounds pretty dark. - Welcome to humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7189" target="_blank">01:59:49.200</a></span> | <span class="t">- Yeah. Yeah, it's like a Hitchhiker's Guide summary, mostly harmless. I would love to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7199" target="_blank">01:59:59.600</a></span> | <span class="t">a good summary when all of this is said and done, when Earth is no longer a thing, whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7206" target="_blank">02:00:06.880</a></span> | <span class="t">a million, a billion years from now. Like what's a good summary of what happened here? It's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7214" target="_blank">02:00:14.400</a></span> | <span class="t">I think AI will play a big part of that summary, and hopefully humans will too. What do you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7220" target="_blank">02:00:20.560</a></span> | <span class="t">about the merger of the two? So one of the things that Elon and Neuralink talk about is one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7226" target="_blank">02:00:26.320</a></span> | <span class="t">ways for us to achieve AI safety is to ride the wave of AGI, so by merging. - Incredible technology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7235" target="_blank">02:00:35.280</a></span> | <span class="t">in a narrow sense to help the disabled, just amazing, supported 100%. For long-term hybrid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7243" target="_blank">02:00:43.120</a></span> | <span class="t">models, both parts need to contribute something to the overall system. Right now, we are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7250" target="_blank">02:00:50.160</a></span> | <span class="t">more capable in many ways, so having this connection to AI would be incredible, would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7254" target="_blank">02:00:54.960</a></span> | <span class="t">make me superhuman in many ways. After a while, if I'm no longer smarter, more creative, really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7262" target="_blank">02:01:02.160</a></span> | <span class="t">don't contribute much, the system finds me as a biological bottleneck, and either explicitly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7267" target="_blank">02:01:07.360</a></span> | <span class="t">or implicitly, I'm removed from any participation in the system. - So it's like the appendix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7273" target="_blank">02:01:13.120</a></span> | <span class="t">By the way, the appendix is still around, so even if it's, you said bottleneck. I don't know if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7281" target="_blank">02:01:21.440</a></span> | <span class="t">become a bottleneck. We just might not have much use. There's a different thing than bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7287" target="_blank">02:01:27.280</a></span> | <span class="t">- Wasting valuable energy by being there. - We don't waste that much energy. We're pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7291" target="_blank">02:01:31.920</a></span> | <span class="t">energy efficient. We could just stick around like the appendix, come on now. - That's the future we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7297" target="_blank">02:01:37.760</a></span> | <span class="t">all dream about, become an appendix to the history book of humanity. - Well, and also the consciousness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7305" target="_blank">02:01:45.760</a></span> | <span class="t">thing, the peculiar particular kind of consciousness that humans have, that might be useful, that might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7310" target="_blank">02:01:50.480</a></span> | <span class="t">be really hard to simulate, but you said that, like how would that look like if you could engineer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7315" target="_blank">02:01:55.760</a></span> | <span class="t">that in, in silicon? - Consciousness? - Consciousness. - I assume you are conscious. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7322" target="_blank">02:02:02.240</a></span> | <span class="t">have no idea how to test for it or how it impacts you in any way whatsoever right now. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7326" target="_blank">02:02:06.880</a></span> | <span class="t">perfectly simulate all of it without making any different observations for me. - But to do it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7333" target="_blank">02:02:13.840</a></span> | <span class="t">a computer, how would you do that? 'Cause you kind of said that you think it's possible to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7339" target="_blank">02:02:19.280</a></span> | <span class="t">- So it may be an emergent phenomena. We seem to get it through evolutionary process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7345" target="_blank">02:02:25.840</a></span> | <span class="t">It's not obvious how it helps us to survive better, but maybe it's an internal kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7356" target="_blank">02:02:36.000</a></span> | <span class="t">GUI, which allows us to better manipulate the world, simplifies a lot of control structures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7363" target="_blank">02:02:43.120</a></span> | <span class="t">That's one area where we have very, very little progress. Lots of papers, lots of research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7368" target="_blank">02:02:48.640</a></span> | <span class="t">but consciousness is not a big, big area of successful discovery so far. A lot of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7377" target="_blank">02:02:57.360</a></span> | <span class="t">think that machines would have to be conscious to be dangerous. That's a big misconception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7381" target="_blank">02:03:01.840</a></span> | <span class="t">There is absolutely no need for this very powerful optimizing agent to feel anything while it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7389" target="_blank">02:03:09.440</a></span> | <span class="t">performing things on you. - But what do you think about this, the whole science of emergence in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7395" target="_blank">02:03:15.360</a></span> | <span class="t">general? So I don't know how much you know about cellular automata or these simplified systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7400" target="_blank">02:03:20.240</a></span> | <span class="t">that study this very question. From simple rules emerges complexity. - I attended Wolfram's summer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7406" target="_blank">02:03:26.640</a></span> | <span class="t">school. - I love Stephen very much. I love his work. I love cellular automata. So I just would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7415" target="_blank">02:03:35.280</a></span> | <span class="t">love to get your thoughts how that fits into your view in the emergence of intelligence in AGI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7423" target="_blank">02:03:43.840</a></span> | <span class="t">systems. And maybe just even simply, what do you make of the fact that this complexity can emerge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7429" target="_blank">02:03:49.600</a></span> | <span class="t">from such simple rules? - So the rule is simple, but the size of a space is still huge. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7436" target="_blank">02:03:56.640</a></span> | <span class="t">neural networks were really the first discovery in AI. 100 years ago, the first papers were published</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7442" target="_blank">02:04:02.720</a></span> | <span class="t">on neural networks. We just didn't have enough compute to make them work. I can give you a rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7448" target="_blank">02:04:08.640</a></span> | <span class="t">such as start printing progressively larger strings. That's it, one sentence. It will output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7454" target="_blank">02:04:14.640</a></span> | <span class="t">everything, every program, every DNA code, everything in that rule. You need intelligence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7461" target="_blank">02:04:21.520</a></span> | <span class="t">to filter it out, obviously, to make it useful. But simple generation is not that difficult. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7467" target="_blank">02:04:27.440</a></span> | <span class="t">a lot of those systems end up being Turing-complete systems. So they're universal. And we expect that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7473" target="_blank">02:04:33.920</a></span> | <span class="t">level of complexity from them. What I like about Wolfram's work is that he talks about irreducibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7480" target="_blank">02:04:40.960</a></span> | <span class="t">You have to run the simulation. You cannot predict what it's going to do ahead of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7485" target="_blank">02:04:45.760</a></span> | <span class="t">And I think that's very relevant to what we are talking about with those very complex systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7492" target="_blank">02:04:52.800</a></span> | <span class="t">Until you live through it, you cannot, ahead of time, tell me exactly what it's going to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7497" target="_blank">02:04:57.920</a></span> | <span class="t">- Irreducibility means that for a sufficiently complex system, you have to run the thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7502" target="_blank">02:05:02.640</a></span> | <span class="t">You have to, you can't predict what's going to happen in the universe. You have to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7506" target="_blank">02:05:06.160</a></span> | <span class="t">a new universe and run the thing. Big bang, the whole thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7509" target="_blank">02:05:09.520</a></span> | <span class="t">- But running it may be consequential as well. - It might destroy humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7518" target="_blank">02:05:18.720</a></span> | <span class="t">- And to you, there's no chance that AIs somehow carry the flame of consciousness,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7524" target="_blank">02:05:24.640</a></span> | <span class="t">the flame of specialness and awesomeness that is humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7528" target="_blank">02:05:28.320</a></span> | <span class="t">- It may somehow, but I still feel kind of bad that it killed all of us. I would prefer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7535" target="_blank">02:05:35.920</a></span> | <span class="t">doesn't happen. I can be happy for others, but to a certain degree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7540" target="_blank">02:05:40.480</a></span> | <span class="t">- It would be nice if we stuck around for a long time. At least give us a planet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7546" target="_blank">02:05:46.080</a></span> | <span class="t">the human planet. It'd be nice for it to be Earth and then they can go elsewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7550" target="_blank">02:05:50.640</a></span> | <span class="t">Since they're so smart, they can colonize Mars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7552" target="_blank">02:05:52.720</a></span> | <span class="t">Do you think they could help convert us to type one, type two, type three? Let's just stick to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7563" target="_blank">02:06:03.040</a></span> | <span class="t">type two civilization on the Kardashev scale. Help us humans expand out into the cosmos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7572" target="_blank">02:06:12.720</a></span> | <span class="t">- So all of it goes back to are we somehow controlling it? Are we getting results we want?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7579" target="_blank">02:06:19.600</a></span> | <span class="t">If yes, then everything's possible. Yes, they can definitely help us with science,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7583" target="_blank">02:06:23.920</a></span> | <span class="t">engineering, exploration in every way conceivable, but it's a big if.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7589" target="_blank">02:06:29.040</a></span> | <span class="t">- This whole thing about control though, humans are bad with control because the moment they gain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7596" target="_blank">02:06:36.480</a></span> | <span class="t">control, they can also easily become too controlling. The more control you have, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7603" target="_blank">02:06:43.040</a></span> | <span class="t">more you want it. The old power corrupts and the absolute power corrupts absolutely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7607" target="_blank">02:06:47.120</a></span> | <span class="t">It feels like control over AGI, saying we live in a universe where that's possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7614" target="_blank">02:06:54.640</a></span> | <span class="t">We come up with ways to actually do that. It's also scary because the collection of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7620" target="_blank">02:07:00.400</a></span> | <span class="t">humans that have the control over AGI, they become more powerful than the other humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7625" target="_blank">02:07:05.760</a></span> | <span class="t">And they can let that power get to their head. And then a small selection of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7632" target="_blank">02:07:12.720</a></span> | <span class="t">back to Stalin, start getting ideas. And then eventually it's one person, usually with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7638" target="_blank">02:07:18.240</a></span> | <span class="t">mustache or a funny hat, that starts sort of making big speeches. And then all of a sudden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7643" target="_blank">02:07:23.120</a></span> | <span class="t">you live in a world that's either 1984 or Brave New World. And always at war with somebody and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7651" target="_blank">02:07:31.840</a></span> | <span class="t">this whole idea of control turned out to be actually also not beneficial to humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7657" target="_blank">02:07:37.440</a></span> | <span class="t">So that's scary too. - It's actually worse because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7659" target="_blank">02:07:39.920</a></span> | <span class="t">historically they all died. This could be different. This could be permanent dictatorship,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7665" target="_blank">02:07:45.040</a></span> | <span class="t">permanent suffering. - Well, the nice thing about humans,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7668" target="_blank">02:07:48.080</a></span> | <span class="t">it seems like. The moment power starts corrupting their mind, they can create a huge amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7675" target="_blank">02:07:55.360</a></span> | <span class="t">suffering. So there's negative. They can kill people, make people suffer, but then they become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7680" target="_blank">02:08:00.160</a></span> | <span class="t">worse and worse at their job. It feels like the more evil you start doing, like the-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7687" target="_blank">02:08:07.280</a></span> | <span class="t">- At least they are incompetent. - Well, no, they become more and more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7691" target="_blank">02:08:11.680</a></span> | <span class="t">incompetent. So they start losing their grip on power. So holding onto power is not a trivial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7698" target="_blank">02:08:18.000</a></span> | <span class="t">thing. So it requires extreme competence, which I suppose Stalin was good at. It requires you to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7703" target="_blank">02:08:23.360</a></span> | <span class="t">evil and be competent at it, or just get lucky. - And those systems help with that. You have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7708" target="_blank">02:08:28.880</a></span> | <span class="t">perfect surveillance. You can do some mind reading, I presume, eventually. It would be very hard to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7714" target="_blank">02:08:34.560</a></span> | <span class="t">remove control from more capable systems over us. - And then it would be hard for humans to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7722" target="_blank">02:08:42.320</a></span> | <span class="t">become the hackers that escape the control of the AGI because the AGI is so damn good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7727" target="_blank">02:08:47.440</a></span> | <span class="t">And then, yeah, yeah, yeah. And then the dictator is immortal. Yeah, that's not great. That's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7736" target="_blank">02:08:56.400</a></span> | <span class="t">a great outcome. See, I'm more afraid of humans than AI systems. I'm afraid, I believe that most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7743" target="_blank">02:09:03.360</a></span> | <span class="t">humans want to do good and have the capacity to do good, but also all humans have the capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7749" target="_blank">02:09:09.040</a></span> | <span class="t">to do evil. And when you test them by giving them absolute powers, you would if you give them AGI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7756" target="_blank">02:09:16.880</a></span> | <span class="t">that could result in a lot of suffering. What gives you hope about the future?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7765" target="_blank">02:09:25.040</a></span> | <span class="t">- I could be wrong. I've been wrong before. - If you look 100 years from now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7771" target="_blank">02:09:31.760</a></span> | <span class="t">and you're immortal, and you look back, and it turns out this whole conversation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7777" target="_blank">02:09:37.120</a></span> | <span class="t">you said a lot of things that were very wrong. Now that looking 100 years back,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7781" target="_blank">02:09:41.920</a></span> | <span class="t">what would be the explanation? What happened in those 100 years that made you wrong,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7788" target="_blank">02:09:48.960</a></span> | <span class="t">that made the words you said today wrong? - There is so many possibilities. We had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7794" target="_blank">02:09:54.080</a></span> | <span class="t">catastrophic events which prevented development of advanced microchips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7798" target="_blank">02:09:58.320</a></span> | <span class="t">- That's not where I thought you were going. - That's a hopeful future. We could be in one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7802" target="_blank">02:10:02.240</a></span> | <span class="t">of those personal universes, and the one I'm in is beautiful. It's all about me, and I like it a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7808" target="_blank">02:10:08.320</a></span> | <span class="t">- So we've now, just to linger on that, that means every human has their personal universe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7813" target="_blank">02:10:13.920</a></span> | <span class="t">- Yes. Maybe multiple ones. Hey, why not? You can shop around. It's possible that somebody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7823" target="_blank">02:10:23.520</a></span> | <span class="t">comes up with alternative model for building AI, which is not based on neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7829" target="_blank">02:10:29.760</a></span> | <span class="t">which are hard to scrutinize, and that alternative is somehow, I don't see how,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7835" target="_blank">02:10:35.120</a></span> | <span class="t">but somehow avoiding all the problems I speak about in general terms, not applying them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7841" target="_blank">02:10:41.360</a></span> | <span class="t">specific architectures. Aliens come and give us friendly superintelligence. There is so many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7847" target="_blank">02:10:47.840</a></span> | <span class="t">options. - Is it also possible that creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7851" target="_blank">02:10:51.440</a></span> | <span class="t">superintelligence systems becomes harder and harder? So meaning it's not so easy to do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7858" target="_blank">02:10:58.640</a></span> | <span class="t">foom, the takeoff. - So that would probably speak more about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7866" target="_blank">02:11:06.880</a></span> | <span class="t">how much smarter that system is compared to us. So maybe it's hard to be a million times smarter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7872" target="_blank">02:11:12.080</a></span> | <span class="t">but it's still okay to be five times smarter. So that is totally possible. That I have no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7876" target="_blank">02:11:16.960</a></span> | <span class="t">objections to. - So like it's, there's a S-curve type</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7880" target="_blank">02:11:20.880</a></span> | <span class="t">situation about smarter, and it's going to be like 3.7 times smarter than all of human civilization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7887" target="_blank">02:11:27.760</a></span> | <span class="t">- Right, just the problems we face in this world, each problem is like an IQ test. You need certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7892" target="_blank">02:11:32.640</a></span> | <span class="t">intelligence to solve it. So we just don't have more complex problems outside of mathematics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7896" target="_blank">02:11:36.800</a></span> | <span class="t">for it to be showing off. Like you can have IQ of 500 if you're playing tic-tac-toe, it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7902" target="_blank">02:11:42.640</a></span> | <span class="t">show, it doesn't matter. - So the idea there is that the problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7907" target="_blank">02:11:47.360</a></span> | <span class="t">define your capacity, your cognitive capacity. So because the problems on earth are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7913" target="_blank">02:11:53.760</a></span> | <span class="t">sufficiently difficult, it's not going to be able to expand this cognitive capacity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7919" target="_blank">02:11:59.200</a></span> | <span class="t">- Possible. - And because of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7921" target="_blank">02:12:01.760</a></span> | <span class="t">wouldn't that be a good thing? - It still could be a lot smarter than us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7926" target="_blank">02:12:06.320</a></span> | <span class="t">And to dominate long-term, you just need some advantage. You have to be the smartest. You don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7931" target="_blank">02:12:11.760</a></span> | <span class="t">have to be a million times smarter. - So even 5X might be enough?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7935" target="_blank">02:12:15.680</a></span> | <span class="t">- It'd be impressive. What is it, IQ of 1,000? I mean, I know those units don't mean anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7941" target="_blank">02:12:21.600</a></span> | <span class="t">at that scale, but still, as a comparison, the smartest human is like 200.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7946" target="_blank">02:12:26.640</a></span> | <span class="t">- Well, actually, no, I didn't mean compared to an individual human, I meant compared to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7952" target="_blank">02:12:32.320</a></span> | <span class="t">collective intelligence of the human species. If you're somehow 5X smarter than that...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7956" target="_blank">02:12:36.320</a></span> | <span class="t">- We are more productive as a group. I don't think we are more capable of solving individual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7962" target="_blank">02:12:42.400</a></span> | <span class="t">problems. If all of humanity plays chess together, we are not a million times better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7968" target="_blank">02:12:48.240</a></span> | <span class="t">than world champion. - That's because there's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7972" target="_blank">02:12:52.000</a></span> | <span class="t">that's like one S-curve is the chess, but humanity is very good at exploring the full range of ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7982" target="_blank">02:13:02.240</a></span> | <span class="t">The more Einsteins you have, the more, just the higher probability you come up with general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7986" target="_blank">02:13:06.480</a></span> | <span class="t">relativity. - But I feel like it's more of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7987" target="_blank">02:13:07.920</a></span> | <span class="t">quantity of superintelligence than quality of superintelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7990" target="_blank">02:13:10.000</a></span> | <span class="t">- Yeah, sure. But quantity and... - Enough quantity sometimes becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7994" target="_blank">02:13:14.720</a></span> | <span class="t">quality, yeah. - Oh, man, humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=7998" target="_blank">02:13:18.720</a></span> | <span class="t">What do you think is the meaning of this whole thing? We've been talking about humans and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8005" target="_blank">02:13:25.680</a></span> | <span class="t">humans not dying, but why are we here? - It's a simulation. We're being tested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8012" target="_blank">02:13:32.160</a></span> | <span class="t">The test is, will you be dumb enough to create superintelligence and release it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8015" target="_blank">02:13:35.600</a></span> | <span class="t">- So the objective function is not be dumb enough to kill ourselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8021" target="_blank">02:13:41.600</a></span> | <span class="t">- Yeah, you're unsafe. Prove yourself to be a safe agent who doesn't do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8025" target="_blank">02:13:45.680</a></span> | <span class="t">and you get to go to the next game. - The next level of the game? What's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8029" target="_blank">02:13:49.520</a></span> | <span class="t">the next level? - I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8030" target="_blank">02:13:50.720</a></span> | <span class="t">I haven't hacked the simulation yet. - Well, maybe hacking the simulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8034" target="_blank">02:13:54.720</a></span> | <span class="t">is the thing. - I'm working as fast as I can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8037" target="_blank">02:13:57.040</a></span> | <span class="t">- And if physics would be the way to do that. - Quantum physics, yeah, definitely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8042" target="_blank">02:14:02.240</a></span> | <span class="t">- Well, I hope we do. And I hope whatever is outside is even more fun than this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8046" target="_blank">02:14:06.640</a></span> | <span class="t">'cause this one's pretty damn fun. And just a big thank you for doing the work you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8052" target="_blank">02:14:12.880</a></span> | <span class="t">There's so much exciting development in AI, and to ground it in the existential risks is really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8061" target="_blank">02:14:21.520</a></span> | <span class="t">really important. Humans love to create stuff, and we should be careful not to destroy ourselves in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8068" target="_blank">02:14:28.000</a></span> | <span class="t">the process. So thank you for doing that really important work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8071" target="_blank">02:14:31.600</a></span> | <span class="t">- Thank you so much for inviting me. It was amazing, and my dream is to be proven wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8077" target="_blank">02:14:37.600</a></span> | <span class="t">If everyone just picks up a paper or book and shows how I messed it up, that would be optimal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8084" target="_blank">02:14:44.400</a></span> | <span class="t">- But for now, the simulation continues. - For now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8087" target="_blank">02:14:47.680</a></span> | <span class="t">- Thank you, Roman. Thanks for listening to this conversation with Roman Yampolsky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8092" target="_blank">02:14:52.640</a></span> | <span class="t">To support this podcast, please check out our sponsors in the description.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8096" target="_blank">02:14:56.320</a></span> | <span class="t">And now let me leave you with some words from Frank Herbert in Dune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8101" target="_blank">02:15:01.120</a></span> | <span class="t">I must not fear. Fear is the mind killer. Fear is the little death that brings total obliteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8109" target="_blank">02:15:09.200</a></span> | <span class="t">I will face fear. I will permit it to pass over me and through me. And when it has gone past,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8116" target="_blank">02:15:16.640</a></span> | <span class="t">I will turn the inner eye to see its path. Where the fear has gone, there will be nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8122" target="_blank">02:15:22.480</a></span> | <span class="t">Only I will remain. Thank you for listening, and hope to see you next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8129" target="_blank">02:15:29.840</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8131" target="_blank">02:15:31.300</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=NNr6gPelJ3E&t=8131" target="_blank">02:15:31.380</a></span> | <span class="t">[END]</span></div></div></body></html>