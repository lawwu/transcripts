<html><head><title>[Workshop] AI Engineering 201: Inference</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>[Workshop] AI Engineering 201: Inference</h2><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE"><img src="https://i.ytimg.com/vi_webp/N7lJY5IKVLE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=0">0:0</a> Intro & Overview<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=232">3:52</a> What is Inference?<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=616">10:16</a> Proprietary Models for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1282">21:22</a> Open Models for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1841">30:41</a> Will Open or Proprietary Models Win Long-Term?<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2179">36:19</a> Q&A on Models<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2652">44:12</a> Inference on End-User Devices<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3872">64:32</a> Inference-as-a-Service Providers<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4200">70:0</a> Cloud Inference and Serverless GPUs<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4666">77:46</a> Rack-and-Stack for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4812">80:12</a> Inference Arithmetic for GPUs<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5227">87:7</a> TPUs and Other Custom Silicon for Inference<br><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5771">96:11</a> Containerizing Inference and Inference Services<br><br><div style="text-align: left;"><a href="./N7lJY5IKVLE.html">Whisper Transcript</a> | <a href="./transcript_N7lJY5IKVLE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=18" target="_blank">00:00:18.000</a></span> | <span class="t">In the workshop today was everything that comes after once you've, you know, written your thin wrapper around the OpenAI API</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=27" target="_blank">00:00:27.000</a></span> | <span class="t">to make your ChatGPT-powered app. You've acquired $100 million in venture funding at a $4 billion valuation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=35" target="_blank">00:00:35.000</a></span> | <span class="t">And now you're like, what am I supposed to do next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=40" target="_blank">00:00:40.000</a></span> | <span class="t">So, we're going to split this into two parts so we can take kind of a break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=45" target="_blank">00:00:45.000</a></span> | <span class="t">It's like a three-hour long workshop. It's a long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=48" target="_blank">00:00:48.000</a></span> | <span class="t">So, in the first half, we're going to talk about inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=53" target="_blank">00:00:53.000</a></span> | <span class="t">So, about what exactly this workload is that we have given over to the inference as a service provider.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=61" target="_blank">00:01:01.000</a></span> | <span class="t">What's the shape of it? Why do we need these expensive accelerators? What are the other options available?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=69" target="_blank">00:01:09.000</a></span> | <span class="t">And we're going to spend half of our time on that because that's a place where we can actually talk, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=75" target="_blank">00:01:15.000</a></span> | <span class="t">in engineering terms about constraints, about service level objectives and service level agreements, the kinds of things that lead to robust systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=85" target="_blank">00:01:25.000</a></span> | <span class="t">And then we'll spend 90 minutes on the rest of the OWL, the rest of what it takes to make a successful AI-powered app,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=92" target="_blank">00:01:32.000</a></span> | <span class="t">just because there's so very little to say just yet about how to engineer these robustly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=97" target="_blank">00:01:37.000</a></span> | <span class="t">But we'll talk about what the emerging consensus on what to do is so far and what tools are out there and available to start accelerating that process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=109" target="_blank">00:01:49.000</a></span> | <span class="t">So, yeah, so for inference, what are we talking about when we're doing inference workloads?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=115" target="_blank">00:01:55.000</a></span> | <span class="t">How do we decide between using open and proprietary models to do that inference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=120" target="_blank">00:02:00.000</a></span> | <span class="t">Where do those models live? Do they live on a device? Do they live in a cloud server?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=126" target="_blank">00:02:06.000</a></span> | <span class="t">And then we'll spend some time talking about what it takes to self-serve inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=133" target="_blank">00:02:13.000</a></span> | <span class="t">For the rest of the OWL, we'll talk about architectures and patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=138" target="_blank">00:02:18.000</a></span> | <span class="t">So what are the emerging kind of patterns for usage of large language models in AI applications?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=146" target="_blank">00:02:26.000</a></span> | <span class="t">And we'll talk about monitoring, evaluation, and observability, which is how we try and actually improve applications that fit those patterns over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=155" target="_blank">00:02:35.000</a></span> | <span class="t">So, yeah, any high-level questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=160" target="_blank">00:02:40.000</a></span> | <span class="t">My mic's off. The Zoom is not hooked up. No questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=167" target="_blank">00:02:47.000</a></span> | <span class="t">Probably not. Still at a very high level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=170" target="_blank">00:02:50.000</a></span> | <span class="t">No? Just stretching. Great. Yeah, maybe we should all stretch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=175" target="_blank">00:02:55.000</a></span> | <span class="t">Yeah, so who am I? Before we dive in, who am I and why don't you listen to me tell you about any of these things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=182" target="_blank">00:03:02.000</a></span> | <span class="t">So my name's Charles. I like to teach people about AI. I've been doing it for a while now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=187" target="_blank">00:03:07.000</a></span> | <span class="t">I went to Berkeley, studied neural networks back in the 2010s, taught people about how to use them and Bayesian networks, RIP, for data science.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=198" target="_blank">00:03:18.000</a></span> | <span class="t">And then I worked in developer relations and education for weights and biases, a previous generation MLOps tool, generation times being shorter than more doubling times, I guess, at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=210" target="_blank">00:03:30.000</a></span> | <span class="t">And then for the last two years I've been working with full-stack deep learning, teaching not just things like the math of machine learning or how to do monitoring of an ML application, but how to build an application that uses ML from soup to nuts, from the GPUs up to the user experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=229" target="_blank">00:03:49.000</a></span> | <span class="t">All right. So now let's dive into the first half here on inference and specifically what is actually going on. Why does it cost so much to ping OpenAI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=247" target="_blank">00:04:07.000</a></span> | <span class="t">And so generative model inference, the kind of inference that's done by the generative AIs, is you see, when you use it via an API, you see a kind of data-to-data function where both sides of that data are human interpretable data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=264" target="_blank">00:04:24.000</a></span> | <span class="t">So things like text, images, sounds, and outcome back, new generated text, image, and sounds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=270" target="_blank">00:04:30.000</a></span> | <span class="t">So for example, this is from the Palm E paper from Google, you might show a picture of a restaurant and the question, if a robot wanted to be useful here, what steps should it take?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=279" target="_blank">00:04:39.000</a></span> | <span class="t">And the output of this language modeling generative API would be clean the table, pick up trash, pick up chairs, wipe chairs, put chairs down, which could then be sent to a cleaning robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=291" target="_blank">00:04:51.000</a></span> | <span class="t">And now you've got something, you know, it's a pretty useful system there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=296" target="_blank">00:04:56.000</a></span> | <span class="t">So this is what you see from the outside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=300" target="_blank">00:05:00.000</a></span> | <span class="t">But as you start digging in a little bit, you'll start hearing about things like tokens and log probs and temperatures and to understand what's going on and weights and networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=313" target="_blank">00:05:13.000</a></span> | <span class="t">And to understand what's going on, you have to realize that this has been broken down into kind of three pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=318" target="_blank">00:05:18.000</a></span> | <span class="t">One part that goes from what humans understand, like text and images and sound, to what neural networks understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=325" target="_blank">00:05:25.000</a></span> | <span class="t">And then one part that goes back and in between the operations of a neural network that operates on arrays and returns arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=333" target="_blank">00:05:33.000</a></span> | <span class="t">So tokenizers take in text that a human can read and turn it into an array of numbers, a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=340" target="_blank">00:05:40.000</a></span> | <span class="t">Just due to the kind of physics-y background of people in the ML community, what people would call arrays, I guess, in other software engineering communities, they get called tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=353" target="_blank">00:05:53.000</a></span> | <span class="t">They also, because they have derivatives attached to them a lot of the time, it's like there's a connection there, but really an array of numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=363" target="_blank">00:06:03.000</a></span> | <span class="t">Those tensors get turned back to things that humans care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=368" target="_blank">00:06:08.000</a></span> | <span class="t">And in between, neural networks map tensors to tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=372" target="_blank">00:06:12.000</a></span> | <span class="t">This step is the bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=374" target="_blank">00:06:14.000</a></span> | <span class="t">This step is the hard part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=376" target="_blank">00:06:16.000</a></span> | <span class="t">There's interesting stuff going on in the sampling process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=378" target="_blank">00:06:18.000</a></span> | <span class="t">There's interesting stuff going on in tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=380" target="_blank">00:06:20.000</a></span> | <span class="t">There's cursed stuff going on in tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=383" target="_blank">00:06:23.000</a></span> | <span class="t">But this step is the bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=385" target="_blank">00:06:25.000</a></span> | <span class="t">This is where the vast majority of the engineering time is spent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=388" target="_blank">00:06:28.000</a></span> | <span class="t">This is where the vast majority of the compute time, the memory, are spent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=392" target="_blank">00:06:32.000</a></span> | <span class="t">And this is the interesting part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=393" target="_blank">00:06:33.000</a></span> | <span class="t">And this is the part where the engineering focus needs to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=397" target="_blank">00:06:37.000</a></span> | <span class="t">Or this is the part that you farm out to somebody else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=401" target="_blank">00:06:41.000</a></span> | <span class="t">So diving in, double clicking on that tensor to tensor arrow at the bottom, that a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=411" target="_blank">00:06:51.000</a></span> | <span class="t">network is kind of a fancy term for a composition of a bunch of tensor to tensor functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=417" target="_blank">00:06:57.000</a></span> | <span class="t">If you have a function that takes in an A and returns an A, you can just stack those one after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=422" target="_blank">00:07:02.000</a></span> | <span class="t">another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=423" target="_blank">00:07:03.000</a></span> | <span class="t">And that's what gives neural networks the kind of Lego-y flavor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=426" target="_blank">00:07:06.000</a></span> | <span class="t">You can grab bricks from one set and bricks from another set and attach them to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">So this neural network, this is an ancient neural network, the Inception V3 model from Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=438" target="_blank">00:07:18.000</a></span> | <span class="t">that was state of the art in computer vision for a few months in the 2010s.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=442" target="_blank">00:07:22.000</a></span> | <span class="t">And each of those little blocks there takes in a tensor and returns a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=449" target="_blank">00:07:29.000</a></span> | <span class="t">And so it starts with a tensor that looks like an image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=452" target="_blank">00:07:32.000</a></span> | <span class="t">So it's got a red, green, and blue channel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">It comes out something that 8 by 8 by 2048 example there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=458" target="_blank">00:07:38.000</a></span> | <span class="t">That's probably somewhere in the middle of the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=460" target="_blank">00:07:40.000</a></span> | <span class="t">It's not the final classification output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=462" target="_blank">00:07:42.000</a></span> | <span class="t">Anyway, big block of numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=464" target="_blank">00:07:44.000</a></span> | <span class="t">They get passed into each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=466" target="_blank">00:07:46.000</a></span> | <span class="t">And then each one of those is itself parametrized by a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=470" target="_blank">00:07:50.000</a></span> | <span class="t">So it's not just like a map that you could kind of like write down by hand that's like add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=475" target="_blank">00:07:55.000</a></span> | <span class="t">one to every entry or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=477" target="_blank">00:07:57.000</a></span> | <span class="t">It's defined by like another big pile of numbers, the weights and biases of the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=486" target="_blank">00:08:06.000</a></span> | <span class="t">So this is the weights from the first layer of a computer vision network, the Alex net that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=492" target="_blank">00:08:12.000</a></span> | <span class="t">kicked off this whole deep learning revolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=495" target="_blank">00:08:15.000</a></span> | <span class="t">And it's these, these are, these are little, this is a visualization of a little block of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=501" target="_blank">00:08:21.000</a></span> | <span class="t">red, green, blue, uh, uh, like tensor with three color channels in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=506" target="_blank">00:08:26.000</a></span> | <span class="t">So humans can actually look at it and interpret it unlike the rest of them and see what's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=511" target="_blank">00:08:31.000</a></span> | <span class="t">on, that it's got little things for detecting edges and textures and color differences and things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=516" target="_blank">00:08:36.000</a></span> | <span class="t">And so, uh, we want to run a big tensor to tensor map.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=525" target="_blank">00:08:45.000</a></span> | <span class="t">And we are going to parameterize that with a big pile of tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=528" target="_blank">00:08:48.000</a></span> | <span class="t">And before it's time to actually serve users, those tensors need to be generated by the training process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=535" target="_blank">00:08:55.000</a></span> | <span class="t">And back, uh, a couple of years ago, um, or even as recently as 18 months ago, this would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=542" target="_blank">00:09:02.000</a></span> | <span class="t">be the part where we would stop, talk about gradient descent optimization, statistical learning theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=547" target="_blank">00:09:07.000</a></span> | <span class="t">and, uh, you know, GPU acceleration and all the things that are needed to turn something into,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=552" target="_blank">00:09:12.000</a></span> | <span class="t">to like get those numbers, to get things like that involving like grabbing, you know, hoovering up a bunch of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=561" target="_blank">00:09:21.000</a></span> | <span class="t">from the internet, uh, without consent, um, and then crystallizing it into those piles of numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=567" target="_blank">00:09:27.000</a></span> | <span class="t">But nowadays you don't have to do that anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=570" target="_blank">00:09:30.000</a></span> | <span class="t">Specialized foundation modeling teams, uh, generate these weights and then they either put them behind proprietary service</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=577" target="_blank">00:09:37.000</a></span> | <span class="t">or they share them with everybody else to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=580" target="_blank">00:09:40.000</a></span> | <span class="t">Um, and so we can skip past all of that stuff and jump into the actual application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=585" target="_blank">00:09:45.000</a></span> | <span class="t">Um, uh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=588" target="_blank">00:09:48.000</a></span> | <span class="t">So any questions before we start talking about, you know, where those, you know, where those weights come from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=594" target="_blank">00:09:54.000</a></span> | <span class="t">uh, or, or rather what the various ways to get a hold of such a set of weights or to be able to use such a set of weights are?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=600" target="_blank">00:10:00.000</a></span> | <span class="t">Um, any questions at the level of what we're trying to do with inference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=605" target="_blank">00:10:05.000</a></span> | <span class="t">Probably pretty clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=607" target="_blank">00:10:07.000</a></span> | <span class="t">That's, uh, maybe a reminder from the one-on-one stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=610" target="_blank">00:10:10.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=611" target="_blank">00:10:11.000</a></span> | <span class="t">So now let's start diving a little bit deeper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=613" target="_blank">00:10:13.000</a></span> | <span class="t">So you want to run an AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=618" target="_blank">00:10:18.000</a></span> | <span class="t">Uh, you're one of the first choices that you need to make as is, you know, pretty common with, um, with software is a build versus buy question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=628" target="_blank">00:10:28.000</a></span> | <span class="t">Um, are you going to like, are you going to make this, uh, you know, out of existing open components?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=634" target="_blank">00:10:34.000</a></span> | <span class="t">Or are you going to, uh, are you going to kick it off to a service?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=638" target="_blank">00:10:38.000</a></span> | <span class="t">So, um, there's in the proprietary corner, there are a couple of players in the open corner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=644" target="_blank">00:10:44.000</a></span> | <span class="t">There are a couple of players.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=645" target="_blank">00:10:45.000</a></span> | <span class="t">Let's walk through what those are and, um, what the sort of dividing lines are and why to choose one or the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=652" target="_blank">00:10:52.000</a></span> | <span class="t">So there's, uh, a number of proprietary modeling services, uh, like Anthropix, uh, from whom we just heard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=662" target="_blank">00:11:02.000</a></span> | <span class="t">Uh, and the good thing about these proprietary models is that they are the most capable models out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=669" target="_blank">00:11:09.000</a></span> | <span class="t">Uh, so this is from the LM SIS, uh, leaderboard, maybe the, the hugging face hosted version of that leaderboard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=678" target="_blank">00:11:18.000</a></span> | <span class="t">If you look at the, the top five models on that are all proprietary models and they're all from open AI or Anthropic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=685" target="_blank">00:11:25.000</a></span> | <span class="t">Um, so there are a couple of other players out there and in the future the, you know, they could release really high quality models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=693" target="_blank">00:11:33.000</a></span> | <span class="t">Um, but for now, this is the, the, the state of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=698" target="_blank">00:11:38.000</a></span> | <span class="t">So if you need the absolute highest level of intelligence in your application, uh, then you want to roll with one of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=706" target="_blank">00:11:46.000</a></span> | <span class="t">Um, it's also often common to start with some of the most highly capable models and then kind of, uh, prove out your application there and then move to doing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=716" target="_blank">00:11:56.000</a></span> | <span class="t">Uh, like with a less capable model, that's cheaper, easier to run, uh, the sort of like rewriting it in rust or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=723" target="_blank">00:12:03.000</a></span> | <span class="t">Uh, so the usual concern with using a proprietary service, there's a number of them, including things like vendor lock-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=732" target="_blank">00:12:12.000</a></span> | <span class="t">Um, but one of the, one of the ones that comes up immediately is like, how much is it going to cost me to use a proprietary service?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=739" target="_blank">00:12:19.000</a></span> | <span class="t">And can't I save money by doing it myself?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=742" target="_blank">00:12:22.000</a></span> | <span class="t">So the fact that the capabilities are higher with the proprietary models is one reason to say, well, you're not going to get exactly the same thing right now, um, using an open model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=752" target="_blank">00:12:32.000</a></span> | <span class="t">Um, but then the other kind of kicker here is that the proprietary models are priced very affordably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=758" target="_blank">00:12:38.000</a></span> | <span class="t">Um, so this is something that's, that has been the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=762" target="_blank">00:12:42.000</a></span> | <span class="t">Uh, this right quote here is from a blog post.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=765" target="_blank">00:12:45.000</a></span> | <span class="t">I wrote back in like January, um, when the only open large model was GLM 130 B from Tsinghua.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=772" target="_blank">00:12:52.000</a></span> | <span class="t">Um, and at that time, just like trying to get the thing running, uh, in a day, I got to within an order of magnitude of the cost of open AI, but you know, on the up, on the upper end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=784" target="_blank">00:13:04.000</a></span> | <span class="t">Um, and then more for a more recent, uh, and more serious example, the folks at Honeycomb, uh, made a natural language sequel, uh, kind of transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=794" target="_blank">00:13:14.000</a></span> | <span class="t">Maybe not sequel, but query language, uh, transform it transformation, uh, AI product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=799" target="_blank">00:13:19.000</a></span> | <span class="t">And like their opinion was that open AI was very inexpensive to run for their task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=804" target="_blank">00:13:24.000</a></span> | <span class="t">Uh, and so you'll find it seems that like nobody's attempting to, uh, like extract rents from monopoly pricing the way that you can get with some proprietary services.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=815" target="_blank">00:13:35.000</a></span> | <span class="t">Uh, where they know that you have no choice, but to pay them $10 million for a MATLAB license every year or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=822" target="_blank">00:13:42.000</a></span> | <span class="t">Um, not to pick on it, sorry for a license for an array programming language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=828" target="_blank">00:13:48.000</a></span> | <span class="t">Um, uh, yeah, so, and the costs here, you know, a dollar for a million tokens for Claude instance, $10 for a million tokens for Claude 2, uh, relative to how much we're used to paying like humans for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=844" target="_blank">00:14:04.000</a></span> | <span class="t">You know, that's, that's like a pretty decent deal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=847" target="_blank">00:14:07.000</a></span> | <span class="t">Anthropic has been slightly more expensive than open AI without like a clear win on, um, capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=854" target="_blank">00:14:14.000</a></span> | <span class="t">Uh, but that's the current state of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=857" target="_blank">00:14:17.000</a></span> | <span class="t">They do have longer context windows, which is kind of nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=860" target="_blank">00:14:20.000</a></span> | <span class="t">But, um, uh, but yeah, most people for the pricing reason and the capability reason, uh, choose open AI at this time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=868" target="_blank">00:14:28.000</a></span> | <span class="t">Um, so the bad thing about proprietary models, besides like, you know, uh, hearing Richard Stallman,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=876" target="_blank">00:14:36.000</a></span> | <span class="t">screaming in the back of your mind all the time, uh, is that proprietary model models cannot offer you full control by dint of their like very nature as proprietary models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=886" target="_blank">00:14:46.000</a></span> | <span class="t">Um, so Alex Gravely created a co-pilot part of the team that created co-pilot GitHub, um, was celebrating that GPT 3.5 turbo instruct a recent release from open AI had brought back log probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=900" target="_blank">00:15:00.000</a></span> | <span class="t">So you can see what, not just like what texts did the model generate, but what probabilities the model gave each token along the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=908" target="_blank">00:15:08.000</a></span> | <span class="t">And back when the, back when it was just the GPT 3 API and the playground, you could see that information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=914" target="_blank">00:15:14.000</a></span> | <span class="t">And that's where a lot of the like early work on sort of like intuition about prompt engineering came from being able to see those numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=921" target="_blank">00:15:21.000</a></span> | <span class="t">and there's all kinds of cool techniques that you can get up to if you have those log props.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=925" target="_blank">00:15:25.000</a></span> | <span class="t">Um, and if you can manipulate those log props, uh, so yeah, you can read out confidence information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=931" target="_blank">00:15:31.000</a></span> | <span class="t">You can use it like during your development process to sort of like more gather more rich information about the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=936" target="_blank">00:15:36.000</a></span> | <span class="t">Um, and there's really, you know, you are, you're interacting with a probabilistic model and you can't see the model's probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=944" target="_blank">00:15:44.000</a></span> | <span class="t">You're like fundamentally hamstrung.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=946" target="_blank">00:15:46.000</a></span> | <span class="t">Um, and so that was mid September.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=949" target="_blank">00:15:49.000</a></span> | <span class="t">And then like a couple of days ago, they turned that off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=952" target="_blank">00:15:52.000</a></span> | <span class="t">Um, and the reason why is because if you give somebody that amount of information, they can start to reverse engineer your model pretty quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=959" target="_blank">00:15:59.000</a></span> | <span class="t">Um, and then you are stuck in the situation of IBM creating a personal computer and then a bunch of people with soldering iron irons and oscilloscopes turn around and make clones of your machine like within a year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=970" target="_blank">00:16:10.000</a></span> | <span class="t">Uh, so proprietary models are like fundamentally disincentivized from giving you that level of control, despite the fact that it's very critical for, um, for like actually effectively operating the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=984" target="_blank">00:16:24.000</a></span> | <span class="t">So there needs to be that capabilities edge, um, that like raw capabilities edge in order to, uh, make up for this fact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=991" target="_blank">00:16:31.000</a></span> | <span class="t">Um, then lastly, maybe some people work in an enterprise, uh, in this room.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1000" target="_blank">00:16:40.000</a></span> | <span class="t">Don't, you don't have to out yourself, but maybe some.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1002" target="_blank">00:16:42.000</a></span> | <span class="t">Um, and if you're operating in this sort of situation, you can't just ship a ping to an external API out there, uh, into your business.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1012" target="_blank">00:16:52.000</a></span> | <span class="t">If people want to know about governance, people want to know about GDPR compliance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1017" target="_blank">00:16:57.000</a></span> | <span class="t">Um, and the, one of the nice things about opening eyes offering probably true about anthropic by this point and definitely true about Google AI soon is there's a nice white glove enterprise tier, uh, uh, around this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1031" target="_blank">00:17:11.000</a></span> | <span class="t">That gives you like, right underneath, like, you know, launch an artificial intelligence application and achieve your childhood sci-fi dreams is built in security and compliance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1041" target="_blank">00:17:21.000</a></span> | <span class="t">Um, we will spend $20 billion on cybersecurity so that you don't have to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1046" target="_blank">00:17:26.000</a></span> | <span class="t">Um, so this, like if you are in a situation where you need to like, you know, assuage concerns about data privacy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1054" target="_blank">00:17:34.000</a></span> | <span class="t">Um, this sort of enterprise tier, um, you know, sock to compliance, et cetera, is, uh, can be really critical for making your life easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1064" target="_blank">00:17:44.000</a></span> | <span class="t">Uh, any questions about proprietary models and such?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1067" target="_blank">00:17:47.000</a></span> | <span class="t">How much more expensive is it to run to be the API versus the cloud, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1072" target="_blank">00:17:52.000</a></span> | <span class="t">Um, with, uh, with, uh, with Azure?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1075" target="_blank">00:17:55.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1076" target="_blank">00:17:56.000</a></span> | <span class="t">I want to say the Azure was cheaper at the start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1078" target="_blank">00:17:58.000</a></span> | <span class="t">Um, but maybe, uh, I haven't had any reason to use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1083" target="_blank">00:18:03.000</a></span> | <span class="t">Um, so I'm, I'm not sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1085" target="_blank">00:18:05.000</a></span> | <span class="t">And just about questions, like, are there like any, um, trade-offs in general with regards to performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1091" target="_blank">00:18:11.000</a></span> | <span class="t">Um, for a minute consuming the API or--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1094" target="_blank">00:18:14.000</a></span> | <span class="t">Ah, so for, you mean for the enterprise tier versus, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1099" target="_blank">00:18:19.000</a></span> | <span class="t">So the, uh, the enterprise tier also offers like an actual SLA, which the OpenAI API like doesn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1105" target="_blank">00:18:25.000</a></span> | <span class="t">Um, and is like geared, that's, that's, you know, that's maybe another very critical feature besides security and compliance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">Like they will promise that you will get a response and not like a 500.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1117" target="_blank">00:18:37.000</a></span> | <span class="t">Um, and they have much more generous, uh, rate limits and things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1122" target="_blank">00:18:42.000</a></span> | <span class="t">And I think they also offer like a little bit more control over stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1127" target="_blank">00:18:47.000</a></span> | <span class="t">So you might be able to do some fine tuning that you can't do via the generic OpenAI API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1131" target="_blank">00:18:51.000</a></span> | <span class="t">Oh, the fine tuning for, for Azure, uh, limited to just like three models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1136" target="_blank">00:18:56.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1137" target="_blank">00:18:57.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1138" target="_blank">00:18:58.000</a></span> | <span class="t">But there's also limits on the public API for fine tuning, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1141" target="_blank">00:19:01.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1142" target="_blank">00:19:02.000</a></span> | <span class="t">Um, I personally did not find it particularly useful to like fine tuning API, both like hard to use and not clear benefits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1149" target="_blank">00:19:09.000</a></span> | <span class="t">Um, I think the example from gradient did show that like, if you want to achieve a style and you don't want to spend money on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1156" target="_blank">00:19:16.000</a></span> | <span class="t">Like context to set up that style, uh, then maybe you can win with fine tunes, but it's like, it's not really a successful way to inject new information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1165" target="_blank">00:19:25.000</a></span> | <span class="t">So you aren't saving on the tokens that you would retrieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1168" target="_blank">00:19:28.000</a></span> | <span class="t">Um, and you have to pay more to inference a fine tuned model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1172" target="_blank">00:19:32.000</a></span> | <span class="t">And that just goes down to the fundamental, like you're asking them to do more work for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1176" target="_blank">00:19:36.000</a></span> | <span class="t">Um, and they can amortize that cost over fewer users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1179" target="_blank">00:19:39.000</a></span> | <span class="t">And so it's just always going to be more expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1182" target="_blank">00:19:42.000</a></span> | <span class="t">Um, and so, uh, yeah, so that limits the utility of those fine tuning APIs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1187" target="_blank">00:19:47.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1188" target="_blank">00:19:48.000</a></span> | <span class="t">Please definitely like ask questions, um, uh, customize to what people are interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1195" target="_blank">00:19:55.000</a></span> | <span class="t">And also if I don't know about something like, please do, uh, interrupt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1199" target="_blank">00:19:59.000</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1203" target="_blank">00:20:03.000</a></span> | <span class="t">Google has vertex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1204" target="_blank">00:20:04.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1205" target="_blank">00:20:05.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1206" target="_blank">00:20:06.000</a></span> | <span class="t">So, uh, well, so vertex is a little bit different from this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1210" target="_blank">00:20:10.000</a></span> | <span class="t">I think, um, I think of vertex, which I was going to talk about later as sort of like a, something that I can launch my own services into as opposed to like, uh, oh, here's a private version of the Palm Bison API, but maybe, maybe is that part of vertex?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1227" target="_blank">00:20:27.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1228" target="_blank">00:20:28.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1229" target="_blank">00:20:29.000</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1230" target="_blank">00:20:30.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1231" target="_blank">00:20:31.000</a></span> | <span class="t">Um, so yeah, that's, um, so they're already available for Google AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1235" target="_blank">00:20:35.000</a></span> | <span class="t">Does anybody know of Anthropics AWS like, uh, like enterprise offering is up yet?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1241" target="_blank">00:20:41.000</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1242" target="_blank">00:20:42.000</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1243" target="_blank">00:20:43.000</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1244" target="_blank">00:20:44.000</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1245" target="_blank">00:20:45.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1246" target="_blank">00:20:46.000</a></span> | <span class="t">I like refuse to make slides about this stuff more than like 48 hours in advance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1251" target="_blank">00:20:51.000</a></span> | <span class="t">And I still find myself getting cut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1253" target="_blank">00:20:53.000</a></span> | <span class="t">Like, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1256" target="_blank">00:20:56.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1257" target="_blank">00:20:57.000</a></span> | <span class="t">So the question was, uh, between proprietary and open models, which ones are you betting on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1270" target="_blank">00:21:10.000</a></span> | <span class="t">Um, gambling is illegal in the state of California.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1272" target="_blank">00:21:12.000</a></span> | <span class="t">And so, um, uh, we'll get, we'll get there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1275" target="_blank">00:21:15.000</a></span> | <span class="t">So, um, let's talk about the open models and then we can answer or, uh, open up that discussion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1281" target="_blank">00:21:21.000</a></span> | <span class="t">Um, so open models are less capable, but catching up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1285" target="_blank">00:21:25.000</a></span> | <span class="t">Um, and their hackability is very powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1288" target="_blank">00:21:28.000</a></span> | <span class="t">Um, so going back to that leaderboard that I showed, if you look at the next five out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1294" target="_blank">00:21:34.000</a></span> | <span class="t">10, um, four of them are Lama two model, uh, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1299" target="_blank">00:21:39.000</a></span> | <span class="t">So this column here to be clear is the license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1302" target="_blank">00:21:42.000</a></span> | <span class="t">Uh, so the top five are proprietary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1304" target="_blank">00:21:44.000</a></span> | <span class="t">Like there's no, uh, there is no license for those weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1307" target="_blank">00:21:47.000</a></span> | <span class="t">Um, for the bottom five, they have, uh, special licenses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1312" target="_blank">00:21:52.000</a></span> | <span class="t">Um, so these are fine tunes of meta's Lama model series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1318" target="_blank">00:21:58.000</a></span> | <span class="t">And this model series has like kind of captured mind share in the free and open source software world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1325" target="_blank">00:22:05.000</a></span> | <span class="t">So a lot of the people who are like hacking independently, um, and, uh, you know, making public get commits, um, and, you know, funded by the Linux foundation and things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1337" target="_blank">00:22:17.000</a></span> | <span class="t">These people are working in the main on, uh, adjustments to, or improvements to the Lama model series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1344" target="_blank">00:22:24.000</a></span> | <span class="t">Um, and this is really critical because the secret to like the success of open source software in general is the ability to do this kind of like highly parallelized development where lots and lots of people are adding tiny little features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1358" target="_blank">00:22:38.000</a></span> | <span class="t">And like, you know, going out into the last mile and adding those tiny little things that they need, um, and sort of like making use of all that, uh, work by others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1369" target="_blank">00:22:49.000</a></span> | <span class="t">Um, so in so far as you're able to do that, you are able to provide useful open source software that can compete with software that's made by, you know, highly remunerated teams, um, you know, in, in Northern California.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1382" target="_blank">00:23:02.000</a></span> | <span class="t">Um, so the important question, uh, is this actually, uh, open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1388" target="_blank">00:23:08.000</a></span> | <span class="t">So you'll notice these licenses here do not have friendly beloved names like LGPL or MIT or Apache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1396" target="_blank">00:23:16.000</a></span> | <span class="t">They are, they have a special unique name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1399" target="_blank">00:23:19.000</a></span> | <span class="t">Um, and that's because meta's license for the Lama two weights, the Lama one weights for at least under a research only license and were only sent to certain people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1407" target="_blank">00:23:27.000</a></span> | <span class="t">And then we're immediately like torrented and the license was violated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1410" target="_blank">00:23:30.000</a></span> | <span class="t">So they gave up on that, uh, on like fully controlling it, but they did say you cannot use the, uh, data or output to improve other large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1420" target="_blank">00:23:40.000</a></span> | <span class="t">You can only use it to improve Lama models, um, and also release under the same license, which is pretty typical with open source, um, which, uh, is partly an attempt to sort of capture this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1432" target="_blank">00:23:52.000</a></span> | <span class="t">Like as people are doing parallel development, they should only be contributing to the development of this, um, this branch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1439" target="_blank">00:23:59.000</a></span> | <span class="t">Um, and then also if your products monthly active users in June of 2023 with 700 million users or above, um, you're not allowed to use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1449" target="_blank">00:24:09.000</a></span> | <span class="t">Or sorry, you have to pay for a special license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1452" target="_blank">00:24:12.000</a></span> | <span class="t">Um, and that, uh, so apologies to anybody, you know, who's, um, you know, if you're running an app with more than 700 million users, um, uh, you'll have to go elsewhere, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1463" target="_blank">00:24:23.000</a></span> | <span class="t">But the, um, the key thing is that these are violations of the, like, sort of agreed terms of what makes something an open source license according to the open source initiative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1474" target="_blank">00:24:34.000</a></span> | <span class="t">who, you know, who, you know, has some, uh, claim to controlling how that term is used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1481" target="_blank">00:24:41.000</a></span> | <span class="t">I didn't, they, I don't think they ever ended up getting a trademark, um, but they, uh, you know, they are, got the community aligned around a small number of licenses that, and around a key set of principles that include, like, you can't tell, you can't say who's allowed to use this software, for example, which is included in the Lama license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1499" target="_blank">00:24:59.000</a></span> | <span class="t">So there's, like, uh, uh, they're, they have opened up a multi-stakeholder process to define open source AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1506" target="_blank">00:25:06.000</a></span> | <span class="t">This is occurring at a time in which, like, the meaning of open source is also being contested in, sort of, like, software as a service.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1513" target="_blank">00:25:13.000</a></span> | <span class="t">So things are a little, things are a little tense there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1516" target="_blank">00:25:16.000</a></span> | <span class="t">Um, but hopefully we'll come to, uh, an agreement as a community on what that means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1521" target="_blank">00:25:21.000</a></span> | <span class="t">Um, so this is a fast-moving space still, so just because you get Mindshare early on, if, like, things change rapidly, that doesn't mean, like, Lama's locked in forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1531" target="_blank">00:25:31.000</a></span> | <span class="t">Um, so Mistral, for example, dropped a model, like, two weeks ago, um, that at only 7 billion parameters was outperforming, um, like, larger models in the 13 to 30 billion parameter range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1543" target="_blank">00:25:43.000</a></span> | <span class="t">And those models were outperforming the previous models, um, at their size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1547" target="_blank">00:25:47.000</a></span> | <span class="t">So there's, like, except insofar as those things, like, continue to get updated, um, you know, they, uh, yeah, they can be outcompeted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1558" target="_blank">00:25:58.000</a></span> | <span class="t">Um, the other thing to watch out for is that there are a lot of people who are very excited about, like, taking on the death star of open AI or whatever and, um, get very excited about these open models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1570" target="_blank">00:26:10.000</a></span> | <span class="t">There's also some political things about the politics of how ChatGPT likes to respond to questions versus the politics of how people who meet other people in discords like to respond to questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1581" target="_blank">00:26:21.000</a></span> | <span class="t">Um, and that can, that sort of, like, enthusiasm can lead to, like, pretty big errors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1588" target="_blank">00:26:28.000</a></span> | <span class="t">So, for example, there was a lot of excitement about these models that were, take, take an open llama model, uh, the, the ways from that, and then grab, like, 10,000 requests from the open AI API or scrape, like, r slash ChatGPT or whatever, and then just fine tune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1606" target="_blank">00:26:46.000</a></span> | <span class="t">Like, now there's a data set, use it to fine tune the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1609" target="_blank">00:26:49.000</a></span> | <span class="t">And those are the ones that were, like, up there on the arena, uh, uh, on that, uh, like, ELO ranking, um, from the leaderboard that I showed before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1618" target="_blank">00:26:58.000</a></span> | <span class="t">Um, and the, there was a claim that these, uh, had, like, 90% of ChatGPT's quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1625" target="_blank">00:27:05.000</a></span> | <span class="t">There are only 7 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1627" target="_blank">00:27:07.000</a></span> | <span class="t">Like, you know, people were, like, very enthusiastic about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1630" target="_blank">00:27:10.000</a></span> | <span class="t">Back in April when I was talking about this, a lot of people were, like, why are you even talking about open AI anymore?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1635" target="_blank">00:27:15.000</a></span> | <span class="t">We, like, you know, Vicua's done it, and, like, yeah, uh, so, um, this is a fake screenshot, uh, that's, uh, from a paper, or that I modified from, from a paper about, uh, about this, this topic of how well these models work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1651" target="_blank">00:27:31.000</a></span> | <span class="t">Um, so, this is one output from a language model, anonymized, um, on answer, uh, how does actor critic improve over reinforce?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1660" target="_blank">00:27:40.000</a></span> | <span class="t">Um, so, this is an algorithm from reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1663" target="_blank">00:27:43.000</a></span> | <span class="t">Um, so, one language model says, uh, actor critic algorithms are an extension of reinforce that combine both policy-based and value-based methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1671" target="_blank">00:27:51.000</a></span> | <span class="t">Um, it's got a critic network, it's got advantage estimation, it's got function approximation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1676" target="_blank">00:27:56.000</a></span> | <span class="t">So, that's one answer, that's A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1679" target="_blank">00:27:59.000</a></span> | <span class="t">Answer B, uh, actor critic algorithms are a type of reinforcement learning algorithm that improves the reinforcement algorithm by combining actor, policy, and critic value components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1689" target="_blank">00:28:09.000</a></span> | <span class="t">Actor-critic algorithms use a single critic, reinforce has a separate critic for each action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1694" target="_blank">00:28:14.000</a></span> | <span class="t">Actor-critic algorithms learn a policy and actor simultaneously, but reinforce learns them separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1699" target="_blank">00:28:19.000</a></span> | <span class="t">Um, so, you know, those might seem fairly similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1703" target="_blank">00:28:23.000</a></span> | <span class="t">So, does anybody have a strong preference for, uh, answer A here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1707" target="_blank">00:28:27.000</a></span> | <span class="t">Anybody have a weak preference for answer A over answer B here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1714" target="_blank">00:28:34.000</a></span> | <span class="t">Some people raising, like, a, like, a soft hand?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1719" target="_blank">00:28:39.000</a></span> | <span class="t">Maybe, yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1720" target="_blank">00:28:40.000</a></span> | <span class="t">Um, anybody have a strong preference for answer B?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1725" target="_blank">00:28:45.000</a></span> | <span class="t">Maybe one, maybe two, and a weak preference for answer B?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1731" target="_blank">00:28:51.000</a></span> | <span class="t">This one's got, like, they both got these nice numbered lists, you know, which looks very authoritative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1737" target="_blank">00:28:57.000</a></span> | <span class="t">It reminds me of a Medium article, which is likely to be true, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1741" target="_blank">00:29:01.000</a></span> | <span class="t">Um, uh, so, the, uh, so, uh, answer B comes from, uh, I want to say this was GPT-4, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1752" target="_blank">00:29:12.000</a></span> | <span class="t">Um, answer B comes from GPT-4 and, oh, wait, wait, sorry, answer A comes from GPT-4 and has the advantage of being correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1763" target="_blank">00:29:23.000</a></span> | <span class="t">Um, answer B comes from one of the, uh, fine-tuned models, um, and is, like, gibberish, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1770" target="_blank">00:29:30.000</a></span> | <span class="t">Um, and so, this just, you know, like, just having humans rate the outputs of language models in the way that a lot of those, uh, leaderboards were constructed, um, did not, like, it didn't have any grounding in the actual utility of the answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1786" target="_blank">00:29:46.000</a></span> | <span class="t">It was just a lot of people going, like, looks good to me, like, nice, yes, merge, um, and, um, without, like, knowing whether it was actually right or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1797" target="_blank">00:29:57.000</a></span> | <span class="t">Um, and so, uh, there's a nice paper from some folks at, uh, Berkeley about, um, sort of walking through, like, what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1805" target="_blank">00:30:05.000</a></span> | <span class="t">Basically, the models are picking up style from a fine tune, which is things like that delightful little split into bullet points and, like, you know, like, a very authoritative and friendly educational style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1816" target="_blank">00:30:16.000</a></span> | <span class="t">Um, but not, like, actual knowledge, not, like, reasoning capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1820" target="_blank">00:30:20.000</a></span> | <span class="t">And, like, a lot of people in the open modeling communities, like, sort of missed this or, like, willfully ignored it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1827" target="_blank">00:30:27.000</a></span> | <span class="t">Um, some of the sharpest people were definitely up on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1830" target="_blank">00:30:30.000</a></span> | <span class="t">Like, the Guanico paper, for example, mentions that there's, uh, some issues with evaluation came out before this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1836" target="_blank">00:30:36.000</a></span> | <span class="t">Um, but definitely a lot of people missed it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1838" target="_blank">00:30:38.000</a></span> | <span class="t">Um, so the, the immediate question that comes up is, like, between these open models and these, uh, proprietary models, who's going to win long-term?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1849" target="_blank">00:30:49.000</a></span> | <span class="t">Like, who should I bet on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1851" target="_blank">00:30:51.000</a></span> | <span class="t">Um, and in some ways, I think that's a bit of a misguided question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1855" target="_blank">00:30:55.000</a></span> | <span class="t">Um, so consider operating systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1858" target="_blank">00:30:58.000</a></span> | <span class="t">Uh, like, the first operating system, roughly, was System 360 from IBM on mainframes, extremely closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1867" target="_blank">00:31:07.000</a></span> | <span class="t">Um, in the '80s, there was a rash of operating systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1870" target="_blank">00:31:10.000</a></span> | <span class="t">Most of them closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1872" target="_blank">00:31:12.000</a></span> | <span class="t">The original Xerox Pilot on the Xerox Star, uh, was extremely closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1876" target="_blank">00:31:16.000</a></span> | <span class="t">DOS and Win-DOS was closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1878" target="_blank">00:31:18.000</a></span> | <span class="t">Uh, Mac OS at the time was, like, completely closed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1881" target="_blank">00:31:21.000</a></span> | <span class="t">Um, and there were Unix operating systems that were kind of, like, mixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1885" target="_blank">00:31:25.000</a></span> | <span class="t">Um, and then over time, the, like, closed versions of Unix lost out to more, like, friendly licensed ones, and in particular to GNU Linux.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1894" target="_blank">00:31:34.000</a></span> | <span class="t">Um, and there's been a bit of a trend towards open, uh, open operating systems kind of taking more mind and market share over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1902" target="_blank">00:31:42.000</a></span> | <span class="t">Like, data centers have more Linux in them now than they did in 2005 and then than they did in 1995.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1909" target="_blank">00:31:49.000</a></span> | <span class="t">But, uh, from what I can tell, it's still, like, 70% plus Windows, um, for, uh, for operating web servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1915" target="_blank">00:31:55.000</a></span> | <span class="t">In mobile phones, we also have an open operating system and a closed operating system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1920" target="_blank">00:32:00.000</a></span> | <span class="t">And these things have been able to co-exist and serve different needs for different organizations throughout, like, the history of operating systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1928" target="_blank">00:32:08.000</a></span> | <span class="t">Um, and the same is true of databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1932" target="_blank">00:32:12.000</a></span> | <span class="t">Uh, so, back in the '70s and '80s, it was Oracle and IBM's DBT2, um, which is still around, I found out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1941" target="_blank">00:32:21.000</a></span> | <span class="t">Um, like, you live too long in, in the, in San Francisco and you forget that there are people who use IBM DBT2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1948" target="_blank">00:32:28.000</a></span> | <span class="t">Um, in the '90s, there was some, like, consolidation around more open implementations of, of SQL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1956" target="_blank">00:32:36.000</a></span> | <span class="t">Uh, and in the 2000s to 2010s, there was the NoSQL movement, but that was still, like, mostly open source databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1964" target="_blank">00:32:44.000</a></span> | <span class="t">Uh, so there's been a lot of, like, movement in the direction of open databases, um, with streaming databases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1970" target="_blank">00:32:50.000</a></span> | <span class="t">we have both proprietary and open options.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1973" target="_blank">00:32:53.000</a></span> | <span class="t">If you look at the top 10, uh, databases as ranked by dbengines.com, which is, you could quibble with the ranking thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1981" target="_blank">00:33:01.000</a></span> | <span class="t">but the, the key point is that there are, like, it's like half and half split between, um, between proprietary databases and, um, and open source databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=1991" target="_blank">00:33:11.000</a></span> | <span class="t">Uh, and that has been, like, relatively stable over time with, like, a soft, maybe a soft trend in the direction of open, uh, databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2000" target="_blank">00:33:20.000</a></span> | <span class="t">Uh, so with these, like, very, like, language models, foundation models are this very, like, low-level, uh, component of a software stack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2009" target="_blank">00:33:29.000</a></span> | <span class="t">More like an, like an operating system or a database, I think, than, like, um, you know, than a SaaS app or a, or a UI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2017" target="_blank">00:33:37.000</a></span> | <span class="t">Uh, and because of that, they're likely to be subject to some of these same forces that say there's some people who want to work one way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2023" target="_blank">00:33:43.000</a></span> | <span class="t">there's some people who want it to work in another, and for some of them, that openness, that hackability is going to be critical for others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2030" target="_blank">00:33:50.000</a></span> | <span class="t">The, like, reliability, the existence of a white-gloved enterprise version is going to be really critical, um, and those will allow these two things to coexist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2038" target="_blank">00:33:58.000</a></span> | <span class="t">Um, and, uh, yeah, and the CEO of HuggingFace liked my tweet when I said that, so it's probably true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2047" target="_blank">00:34:07.000</a></span> | <span class="t">Um, but I think a lot of people are, like, well, no, but, like, who's, like, who's gonna win? Like, who should I bet on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2056" target="_blank">00:34:16.000</a></span> | <span class="t">Um, and I think the closest thing to an answer that I have is that if capabilities requirements saturate, if people no longer want the absolute smartest model out there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2066" target="_blank">00:34:26.000</a></span> | <span class="t">they just want a model smart enough for X, Y, Z, then open models will probably catch up and then, like, start to dominate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2072" target="_blank">00:34:32.000</a></span> | <span class="t">Um, the thing that keeps open models behind, proprietary models, is the extreme expense of maintaining a large resource team and, like, you know, continually constructing new data centers at an increased scale, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2085" target="_blank">00:34:45.000</a></span> | <span class="t">um, to the tune of, like, $500 million in order to hit that next capability level before everybody else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2090" target="_blank">00:34:50.000</a></span> | <span class="t">But, you know, uh, uh, at a certain point, processors got fast enough that people were not, like, clamoring for the next upgrade as soon as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2098" target="_blank">00:34:58.000</a></span> | <span class="t">Um, and at that point, we're starting to see, like, a little bit more opening up in the, sort of, like, in the chip space with, like, RISC-V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2104" target="_blank">00:35:04.000</a></span> | <span class="t">Um, and so, like, at, like, with, in any number of other technological domains, you've seen that when requirements start to saturate, um, then, like, open, uh, like, open versions can catch up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2116" target="_blank">00:35:16.000</a></span> | <span class="t">Um, if they are unbounded and it's like, you know, uh, what's a good example?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2122" target="_blank">00:35:22.000</a></span> | <span class="t">What's a good example?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2123" target="_blank">00:35:23.000</a></span> | <span class="t">Like, RAM?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2124" target="_blank">00:35:24.000</a></span> | <span class="t">Like, nobody has enough RAM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2125" target="_blank">00:35:25.000</a></span> | <span class="t">Everybody wants more RAM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2126" target="_blank">00:35:26.000</a></span> | <span class="t">I don't think there are any, like, open, like, attempts to make, like, an open RAM architecture or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2131" target="_blank">00:35:31.000</a></span> | <span class="t">Um, and that's because, and one reason why I think is capabilities requirements there are, remain unbounded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2138" target="_blank">00:35:38.000</a></span> | <span class="t">Um, and so, um, if that's the case for, uh, cognition and AI models, then proprietary models should be able to maintain that edge in, in capabilities, which would sort of tilt the balance in favor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2151" target="_blank">00:35:51.000</a></span> | <span class="t">More people would say, oh, no, I need this, this proprietary thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2155" target="_blank">00:35:55.000</a></span> | <span class="t">Um, so it's the closest to an answer that I have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2158" target="_blank">00:35:58.000</a></span> | <span class="t">Um, yeah, any questions on that front before we dive into, uh, um, where we actually run these models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2165" target="_blank">00:36:05.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2166" target="_blank">00:36:06.000</a></span> | <span class="t">I was curious, uh, what's the language support for these language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2171" target="_blank">00:36:11.000</a></span> | <span class="t">Like, can anyone, you know, use, like, a language other than English with these models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2177" target="_blank">00:36:17.000</a></span> | <span class="t">Yeah, um, so the question was what kind of language support do these models have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2182" target="_blank">00:36:22.000</a></span> | <span class="t">Um, and because it's only an API call away, you can, of course, use Python or Node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2188" target="_blank">00:36:28.000</a></span> | <span class="t">code or whatever you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2189" target="_blank">00:36:29.000</a></span> | <span class="t">Uh, no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2190" target="_blank">00:36:30.000</a></span> | <span class="t">So the question was about, like, these are language modeling, like, machines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2194" target="_blank">00:36:34.000</a></span> | <span class="t">What languages do they model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2195" target="_blank">00:36:35.000</a></span> | <span class="t">Um, and the basic answer is that the more text in that language that is available on the open</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2202" target="_blank">00:36:42.000</a></span> | <span class="t">internet, the better the language models will be on, on that language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2206" target="_blank">00:36:46.000</a></span> | <span class="t">So they are, like, I want to say GBD-4 is smarter in, maybe smarter in Malayalam than it is in Mandarin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2217" target="_blank">00:36:57.000</a></span> | <span class="t">Um, I forget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2218" target="_blank">00:36:58.000</a></span> | <span class="t">There's, like, some interesting inversions of, like, number of people who speak the language versus how, uh, how intelligent the language models are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2225" target="_blank">00:37:05.000</a></span> | <span class="t">Um, so I think a lot of them release benchmarks that say, like, how multilingual is this language model and for which languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2232" target="_blank">00:37:12.000</a></span> | <span class="t">Um, there is, you run into the fundamental token constraint of, like, you need, uh, you need existing, like, you need examples of that language that you can get a hold of in order to train the model in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2247" target="_blank">00:37:27.000</a></span> | <span class="t">Um, and there just are more English tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2251" target="_blank">00:37:31.000</a></span> | <span class="t">Um, but for a given capacity, you can probably achieve, like, higher quality in a specific model by looking for, um, by looking for a model trained in that language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2264" target="_blank">00:37:44.000</a></span> | <span class="t">So there's definitely some, like, good old nationalist European endeavors to make, like, a French-language model that insults you if you ask it for stuff in English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2272" target="_blank">00:37:52.000</a></span> | <span class="t">Um, which it, of course, picks up just from reading French.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2276" target="_blank">00:37:56.000</a></span> | <span class="t">Um.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2277" target="_blank">00:37:57.000</a></span> | <span class="t">Um, but yeah, but the, but the core models, like, they support English really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2282" target="_blank">00:38:02.000</a></span> | <span class="t">The instruction fine-tuning in the ROHF is actually mostly applied to them in English since the annotators who, uh, enforce that policy, uh, through their examples are mostly writing in English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2294" target="_blank">00:38:14.000</a></span> | <span class="t">Um, so fun fact, you can get ChatGPT and Claude probably to tell you how to build a bomb if you ask in the right low resource level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2301" target="_blank">00:38:21.000</a></span> | <span class="t">Um, uh, just fun facts about language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2306" target="_blank">00:38:26.000</a></span> | <span class="t">Um, yeah, so that, that does, that is a problem, and it does sort of, like, uh, it has a multiplying effect on the English languages, kind of, like, cultural hegemony, um, which is a bit unfortunate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2319" target="_blank">00:38:39.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2320" target="_blank">00:38:40.000</a></span> | <span class="t">So for the, for the languages that are less represented, is, um, is reading capabilities lower, or, you know, understanding lower, and also, are there arbitrage opportunities in translating first, uh, in the process of translating first, uh, and then...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2338" target="_blank">00:38:58.000</a></span> | <span class="t">Yeah, I'm unaware of any, like, you know, any benchmarking work on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2345" target="_blank">00:39:05.000</a></span> | <span class="t">My gut tells me that translating to English first, doing chain of thought, and then translating back to the original language would work better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2353" target="_blank">00:39:13.000</a></span> | <span class="t">Um, you, you kind of, like, wondering whether the lost in translation effect is bigger than the, like, boost of chain of thought in English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2362" target="_blank">00:39:22.000</a></span> | <span class="t">A lot of the, like, circuits in language models are very token specific.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2367" target="_blank">00:39:27.000</a></span> | <span class="t">Um, and then, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2369" target="_blank">00:39:29.000</a></span> | <span class="t">So, it's like the, like, just one example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2371" target="_blank">00:39:31.000</a></span> | <span class="t">If you ask it who Tom Cruise's mother is, then it answers better than if you ask it that woman's name's son.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2378" target="_blank">00:39:38.000</a></span> | <span class="t">I, I don't know her, her name at all, um, so I can't really do this example effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2383" target="_blank">00:39:43.000</a></span> | <span class="t">Um, JATGPT wins again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2385" target="_blank">00:39:45.000</a></span> | <span class="t">Um, but the, uh, there, so that's, like, an example of a very, of a token specific circuit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2390" target="_blank">00:39:50.000</a></span> | <span class="t">It's, like, related to Tom Cruise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2392" target="_blank">00:39:52.000</a></span> | <span class="t">Uh, and so, you can see, like, it's not reasoning the way that a person would, or that, or that you would guess from, like, you know, how, how you would think about a knowledge graph or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2403" target="_blank">00:40:03.000</a></span> | <span class="t">Um, and so, that's where you get these unintuitive things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2406" target="_blank">00:40:06.000</a></span> | <span class="t">Um, but, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2408" target="_blank">00:40:08.000</a></span> | <span class="t">So, I saw you, um, used to or maybe still do deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2414" target="_blank">00:40:14.000</a></span> | <span class="t">Oh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2415" target="_blank">00:40:15.000</a></span> | <span class="t">And I was wondering, like, one of the pieces of the whole event is, okay, there is these reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2421" target="_blank">00:40:21.000</a></span> | <span class="t">as a service APIs now, right, where you can do a lot more things without having your own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2431" target="_blank">00:40:31.000</a></span> | <span class="t">experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2432" target="_blank">00:40:32.000</a></span> | <span class="t">So, if you are aiming for the typical AI engineer, you know, to build this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2434" target="_blank">00:40:34.000</a></span> | <span class="t">Does it still make sense to learn some amount of those and learning some amount of things, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2439" target="_blank">00:40:39.000</a></span> | <span class="t">like, is it, like, what's the video piece of what post?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2443" target="_blank">00:40:43.000</a></span> | <span class="t">Is it, like, actually being, like, who can?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2445" target="_blank">00:40:45.000</a></span> | <span class="t">Is it, like, actually being made for, like, what would you like to do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2450" target="_blank">00:40:50.000</a></span> | <span class="t">Yeah, that's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2453" target="_blank">00:40:53.000</a></span> | <span class="t">Um, for individuals, I think it's a matter of your personal interest in understanding the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2462" target="_blank">00:41:02.000</a></span> | <span class="t">modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2463" target="_blank">00:41:03.000</a></span> | <span class="t">Like, I guess the analogy I would immediately jump to is, as an individual developer, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2470" target="_blank">00:41:10.000</a></span> | <span class="t">can get away with not knowing anything about databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2473" target="_blank">00:41:13.000</a></span> | <span class="t">Like, I have done that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2474" target="_blank">00:41:14.000</a></span> | <span class="t">I couldn't write a B-tree right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2476" target="_blank">00:41:16.000</a></span> | <span class="t">I don't want to ever learn how to do that, like, and to think about page sizes and, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2481" target="_blank">00:41:21.000</a></span> | <span class="t">it makes me ill to think about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2483" target="_blank">00:41:23.000</a></span> | <span class="t">And, whereas I get excited if I wake up in the morning and I can think about Bayesian inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2488" target="_blank">00:41:28.000</a></span> | <span class="t">in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2489" target="_blank">00:41:29.000</a></span> | <span class="t">And so, as an individual, I think you can, like, kind of be guided by your, like, what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2494" target="_blank">00:41:34.000</a></span> | <span class="t">find most exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2495" target="_blank">00:41:35.000</a></span> | <span class="t">As a team and as an organization, though, if you have nobody who understands databases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2500" target="_blank">00:41:40.000</a></span> | <span class="t">in your organization, you're probably going to be in trouble.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2503" target="_blank">00:41:43.000</a></span> | <span class="t">Um, just, like, it ends up, like, most applications require, like, pretty decent knowledge of databases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2510" target="_blank">00:41:50.000</a></span> | <span class="t">and when they go down or when they need to be configured.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2513" target="_blank">00:41:53.000</a></span> | <span class="t">Even if you are using Redshift or, you know, you're using some managed service, being able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2519" target="_blank">00:41:59.000</a></span> | <span class="t">to, like, understand some stuff about them is actually critical for debugging and being able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2523" target="_blank">00:42:03.000</a></span> | <span class="t">to know when you need to switch managed services or, like, yeah, or how to reconfigure them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2528" target="_blank">00:42:08.000</a></span> | <span class="t">So, I think, like, the direction that we're going to go is to evolve there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2532" target="_blank">00:42:12.000</a></span> | <span class="t">It's a question of whether you want to be a site reliability engineer focused on LLM reliability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2541" target="_blank">00:42:21.000</a></span> | <span class="t">or, you know, a modeling engineer or whether you want to be, like, more at the, like, application layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2547" target="_blank">00:42:27.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2548" target="_blank">00:42:28.000</a></span> | <span class="t">So, here's an interesting one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2551" target="_blank">00:42:31.000</a></span> | <span class="t">What's your personal opinion on this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2553" target="_blank">00:42:33.000</a></span> | <span class="t">There's a handful of companies that have gotten recent funding to build, like, vertical-oriented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2557" target="_blank">00:42:37.000</a></span> | <span class="t">commercial models in finance, healthcare, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2560" target="_blank">00:42:40.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2561" target="_blank">00:42:41.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2562" target="_blank">00:42:42.000</a></span> | <span class="t">So, the question was, what about these models that are foundational but, like, less broad?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2569" target="_blank">00:42:49.000</a></span> | <span class="t">So, it's, like, a foundational model for law, a foundational model for healthcare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2575" target="_blank">00:42:55.000</a></span> | <span class="t">For healthcare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2576" target="_blank">00:42:56.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2577" target="_blank">00:42:57.000</a></span> | <span class="t">I -- my experience has been that if you bet that some capability is not going to be available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2585" target="_blank">00:43:05.000</a></span> | <span class="t">in a language model or in a foundation model, like, you will get -- you will lose that bet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2592" target="_blank">00:43:12.000</a></span> | <span class="t">So, just as an example, in the deep learning boot camp, we spent a long time trying to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2599" target="_blank">00:43:19.000</a></span> | <span class="t">an optical character recognition system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2601" target="_blank">00:43:21.000</a></span> | <span class="t">And it's, like, you know, it's the, like, pinnacle of the class where you can finally, like, deploy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2606" target="_blank">00:43:26.000</a></span> | <span class="t">a web service that does optical character recognition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2609" target="_blank">00:43:29.000</a></span> | <span class="t">And that's, like, an accidental side feature of GPT-4v, and it's, like, better at it than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2614" target="_blank">00:43:34.000</a></span> | <span class="t">the thing that we built.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2615" target="_blank">00:43:35.000</a></span> | <span class="t">Um, and a lot of ML teams have experienced something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2619" target="_blank">00:43:39.000</a></span> | <span class="t">Um, so, I worry -- I would worry if -- if somebody were, like, offering me that as a job opportunity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2625" target="_blank">00:43:45.000</a></span> | <span class="t">for example, I would worry that it's going to get, like, scooped on either side by a hyper-specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2630" target="_blank">00:43:50.000</a></span> | <span class="t">model that's, like, 10 times more efficient and isn't, like, a generic healthcare model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2634" target="_blank">00:43:54.000</a></span> | <span class="t">but is, like, a, um, uh, ultrasound for the heart model, the one I worked on before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2641" target="_blank">00:44:01.000</a></span> | <span class="t">Um, yeah, or just send it to the chat GPT API, or to the GPT API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2651" target="_blank">00:44:11.000</a></span> | <span class="t">Great questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2652" target="_blank">00:44:12.000</a></span> | <span class="t">Um, so, uh, maybe another reason to think that there might be, like, a little more alpha in -- in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2659" target="_blank">00:44:19.000</a></span> | <span class="t">actually learning more about the models is, um, inference doesn't have to be executed over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2666" target="_blank">00:44:26.000</a></span> | <span class="t">a network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2667" target="_blank">00:44:27.000</a></span> | <span class="t">It doesn't have to be executed, like, in some central server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2670" target="_blank">00:44:30.000</a></span> | <span class="t">There are lots of reasons why you might want to execute your, uh, your inference on an end-user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2678" target="_blank">00:44:38.000</a></span> | <span class="t">device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2679" target="_blank">00:44:39.000</a></span> | <span class="t">Um, so we'll talk about the different types of end-user devices and the different constraints</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2683" target="_blank">00:44:43.000</a></span> | <span class="t">that they put on inference and the implications, um, like engineering and strategic, and then also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2688" target="_blank">00:44:48.000</a></span> | <span class="t">uh, talk about what the options are for doing things over a network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2693" target="_blank">00:44:53.000</a></span> | <span class="t">Um, so running stuff for end-users is, like, uh, like where the -- sorry, the end-user actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2700" target="_blank">00:45:00.000</a></span> | <span class="t">executes it themselves is not quite there yet, but it's, like, uh, it's getting there and maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2705" target="_blank">00:45:05.000</a></span> | <span class="t">a little bit faster than I personally expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2707" target="_blank">00:45:07.000</a></span> | <span class="t">like, I've run llama 2, uh, 13b on this very laptop, um, without it catching on fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2716" target="_blank">00:45:16.000</a></span> | <span class="t">Um, so there's, uh, there's some hope, uh, that there will -- that that will continue to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2722" target="_blank">00:45:22.000</a></span> | <span class="t">better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2723" target="_blank">00:45:23.000</a></span> | <span class="t">Um, and so this, uh, this is critical for latency-sensitive applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2729" target="_blank">00:45:29.000</a></span> | <span class="t">So, like, being able to actually execute the inference at the same place that the user -- at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2735" target="_blank">00:45:35.000</a></span> | <span class="t">same place where the user is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2736" target="_blank">00:45:36.000</a></span> | <span class="t">Um, and the reason why it goes back to this, like, this famous set of numbers every engineer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2741" target="_blank">00:45:41.000</a></span> | <span class="t">should know from, uh, Peter Norvig and Jeff Dean at Google, um, which is that the time it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2747" target="_blank">00:45:47.000</a></span> | <span class="t">takes to send a packet -- just one packet, so probably -- this probably isn't even a whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2753" target="_blank">00:45:53.000</a></span> | <span class="t">HTTP request.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2754" target="_blank">00:45:54.000</a></span> | <span class="t">I'd have to check again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2755" target="_blank">00:45:55.000</a></span> | <span class="t">But let's just say you send information back and forth from, like, here in California to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2760" target="_blank">00:46:00.000</a></span> | <span class="t">Europe and back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2761" target="_blank">00:46:01.000</a></span> | <span class="t">It's 150 milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2763" target="_blank">00:46:03.000</a></span> | <span class="t">Um, and there's, like, a number of kind of made-up numbers in the UX world about, like, how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2771" target="_blank">00:46:11.000</a></span> | <span class="t">fast you need to be for something to feel interactive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2774" target="_blank">00:46:14.000</a></span> | <span class="t">Um, so one of them going back to, like, the '70s or '80s is the Doherty threshold, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2779" target="_blank">00:46:19.000</a></span> | <span class="t">says the user and the computer can interact with each other in under 400 milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2784" target="_blank">00:46:24.000</a></span> | <span class="t">Then the, like, human won't feel like they're waiting on the computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2788" target="_blank">00:46:28.000</a></span> | <span class="t">And as you're programming things, you won't, like, end up blocked on human input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2793" target="_blank">00:46:33.000</a></span> | <span class="t">Um, but you'll -- and you'll still have plenty of time for doing stuff in -- in side threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2797" target="_blank">00:46:37.000</a></span> | <span class="t">and things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2798" target="_blank">00:46:38.000</a></span> | <span class="t">Uh, so if you were -- like, if you have to, like, do a network call every single time, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2803" target="_blank">00:46:43.000</a></span> | <span class="t">using up, like, a third of your budget just on, like, waiting for information -- information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2807" target="_blank">00:46:47.000</a></span> | <span class="t">to come back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2808" target="_blank">00:46:48.000</a></span> | <span class="t">And now you're going to spend a ton of engineering effort on, like, trying to find ways -- things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2812" target="_blank">00:46:52.000</a></span> | <span class="t">that you can do asynchronously during that, like, that network call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2816" target="_blank">00:46:56.000</a></span> | <span class="t">And, like, you can -- you can work around it, but it is punishing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2819" target="_blank">00:46:59.000</a></span> | <span class="t">Um, and that's -- like, there are even tighter, like, reaction time things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2824" target="_blank">00:47:04.000</a></span> | <span class="t">Like, if you have a self-driving car, um, you can't wait 150 milliseconds, uh, to find out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2829" target="_blank">00:47:09.000</a></span> | <span class="t">that you need to brake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2830" target="_blank">00:47:10.000</a></span> | <span class="t">Um, so, uh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2835" target="_blank">00:47:15.000</a></span> | <span class="t">So, and the nice thing about this, uh, the other benefit to it, besides it being necessary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2840" target="_blank">00:47:20.000</a></span> | <span class="t">in some places, is that if end users run the computation, then you don't need to pay for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2846" target="_blank">00:47:26.000</a></span> | <span class="t">Um, so your inferencing costs can be zero dollars, which would be -- which would be great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2854" target="_blank">00:47:34.000</a></span> | <span class="t">Um, so the cost that you pay, um, is control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2857" target="_blank">00:47:37.000</a></span> | <span class="t">So, you have less control of the execution environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2861" target="_blank">00:47:41.000</a></span> | <span class="t">Um, your ability to do telemetry and see what is going on is limited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2865" target="_blank">00:47:45.000</a></span> | <span class="t">Uh, people don't like it when you, like, carefully observe their activity using software on their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2871" target="_blank">00:47:51.000</a></span> | <span class="t">machine, but if they -- you put the same software at a URL, you can spy on them as much as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2877" target="_blank">00:47:57.000</a></span> | <span class="t">want, and they don't get mad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2878" target="_blank">00:47:58.000</a></span> | <span class="t">Um, so you lose out on telemetry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2881" target="_blank">00:48:01.000</a></span> | <span class="t">You, uh, have to worry about compatibility with different execution environments, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2885" target="_blank">00:48:05.000</a></span> | <span class="t">have to actually support past versions, unlike, uh, if you're running it as a service.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2890" target="_blank">00:48:10.000</a></span> | <span class="t">Um, or rather, if you, like, you know, control the execution environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2895" target="_blank">00:48:15.000</a></span> | <span class="t">Um, so the things that are unlocked by this are some of the best applications here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2900" target="_blank">00:48:20.000</a></span> | <span class="t">Uh, like, some of the most exciting ones, especially to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2903" target="_blank">00:48:23.000</a></span> | <span class="t">So, uh, use on smart -- use in smartphones, use in robots, use in wearables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2908" target="_blank">00:48:28.000</a></span> | <span class="t">Um, so Google, just in the past couple days, announced that the Pixel Pro 8, um, is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2914" target="_blank">00:48:34.000</a></span> | <span class="t">to have, uh, large language models directly on device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2917" target="_blank">00:48:37.000</a></span> | <span class="t">Um, they mostly showed off stuff that looked, like, kind of, like, summarization and some,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2921" target="_blank">00:48:41.000</a></span> | <span class="t">like, light image editing, so not, like, full-on, like, you know, like, "Hey Siri, why did the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2927" target="_blank">00:48:47.000</a></span> | <span class="t">Ottoman Empire fall?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2928" target="_blank">00:48:48.000</a></span> | <span class="t">Like, I don't know what you talk about with ChatGPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2930" target="_blank">00:48:50.000</a></span> | <span class="t">Um, but, uh, like, it's not quite that level, but it's a move in that direction, and a trend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2936" target="_blank">00:48:56.000</a></span> | <span class="t">we can expect to kind of continue getting that inference onto the device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2940" target="_blank">00:49:00.000</a></span> | <span class="t">Um, and, uh, there was also a recent hardware hack, um, on, like, using, you know, getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2948" target="_blank">00:49:08.000</a></span> | <span class="t">this inference on mobile robotics platforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2951" target="_blank">00:49:11.000</a></span> | <span class="t">Um, and so there's -- there was a ton of cool applications there, like, um, yeah, some stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2956" target="_blank">00:49:16.000</a></span> | <span class="t">with, like, three -- like, point cloud rendering from -- for your -- for inside your house, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2961" target="_blank">00:49:21.000</a></span> | <span class="t">a Roomba you can control with your voice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2963" target="_blank">00:49:23.000</a></span> | <span class="t">Very cool stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2964" target="_blank">00:49:24.000</a></span> | <span class="t">Um, the -- the constraints that appear here, uh, that you'll have to engineer around are, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2971" target="_blank">00:49:31.000</a></span> | <span class="t">very tight hardware constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2974" target="_blank">00:49:34.000</a></span> | <span class="t">So, um, there -- there's memory limits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2977" target="_blank">00:49:37.000</a></span> | <span class="t">Both disk and, like, VRAM and RAM are, like -- are extremely tight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2982" target="_blank">00:49:42.000</a></span> | <span class="t">And, like, current language models, you can always trade more -- up to points where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2989" target="_blank">00:49:49.000</a></span> | <span class="t">spending, like, $100,000 on a machine, you can trade more money for smarter models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2994" target="_blank">00:49:54.000</a></span> | <span class="t">Um, and phones are down at, like, gigabytes, uh, low gigabytes of RAM, uh, like -- yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3002" target="_blank">00:50:02.000</a></span> | <span class="t">Um, was running a language model on a single board computer, and that had, like, two gigabytes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3006" target="_blank">00:50:06.000</a></span> | <span class="t">of shared RAM between the CPU and GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3008" target="_blank">00:50:08.000</a></span> | <span class="t">Not a lot of space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3010" target="_blank">00:50:10.000</a></span> | <span class="t">Um, and the, like, real, uh, deep limit is power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3015" target="_blank">00:50:15.000</a></span> | <span class="t">Um, it's, uh -- or the -- sorry, there's a limit on power, which is, like, an A100, uh, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3023" target="_blank">00:50:23.000</a></span> | <span class="t">you might use for inference, draws 300 watts of power, and something like the single board</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3027" target="_blank">00:50:27.000</a></span> | <span class="t">computer, or using the Jetson Nano, that's 10 watts of power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3030" target="_blank">00:50:30.000</a></span> | <span class="t">So, a factor of 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3032" target="_blank">00:50:32.000</a></span> | <span class="t">Not gonna make that up anytime soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3034" target="_blank">00:50:34.000</a></span> | <span class="t">Um, and underneath both of these is the problem of heat dissipation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3038" target="_blank">00:50:38.000</a></span> | <span class="t">Um, there's -- that's, like, a really, like, tough thing to deal with when you are in these,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3043" target="_blank">00:50:43.000</a></span> | <span class="t">like, small environments, um, and, like, prevents them from just being, like, oh, I'll just, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3048" target="_blank">00:50:48.000</a></span> | <span class="t">make a chip where you can actually move, like, nine petabytes a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3052" target="_blank">00:50:52.000</a></span> | <span class="t">Um, like, across an inch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3055" target="_blank">00:50:55.000</a></span> | <span class="t">And it's, like, uh, like, you just do some, like, back-of-the-envelope math, and it's, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3059" target="_blank">00:50:59.000</a></span> | <span class="t">that's gonna, like, egress so much heat the thing's gonna catch on fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3063" target="_blank">00:51:03.000</a></span> | <span class="t">Um, so, um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3067" target="_blank">00:51:07.000</a></span> | <span class="t">This is -- we're talking about some hardcore engineering stuff here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3070" target="_blank">00:51:10.000</a></span> | <span class="t">Um, all right, so the, like, mobile environments, uh, maybe, like, further out in the future to get, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3079" target="_blank">00:51:19.000</a></span> | <span class="t">large capabilities onto them, but, um, not impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3082" target="_blank">00:51:22.000</a></span> | <span class="t">What about, um, what about other consumer hardware?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3085" target="_blank">00:51:25.000</a></span> | <span class="t">Desktops, um, which are a place where you could have video games with actual artificial intelligence in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3091" target="_blank">00:51:31.000</a></span> | <span class="t">Uh, operating system-level assistants, native apps with these, like, kinds of features that we're starting to see in, um, in browser apps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3100" target="_blank">00:51:40.000</a></span> | <span class="t">Uh, so you still run -- like, you run into even more heterogeneous hardware, and that's gonna give you different constraints,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3106" target="_blank">00:51:46.000</a></span> | <span class="t">depending on the system that you're on, and that is gonna require, like, really heterogeneous software to meet those constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3113" target="_blank">00:51:53.000</a></span> | <span class="t">Like, you probably can't assume that everybody has an NVIDIA 30 series or later GPU, even though it would make your life a lot easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3120" target="_blank">00:52:00.000</a></span> | <span class="t">And you probably can't assume that you can use up all the RAM on that, uh, uh, uh, you know, on that chip, even if it would make your life easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3128" target="_blank">00:52:08.000</a></span> | <span class="t">Um, I think the long-term, we might be able to expect ecosystems to adjust around the requirements of these workloads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3135" target="_blank">00:52:15.000</a></span> | <span class="t">a bit, so, like, kind of, uh, like, make it, uh, like, make cleaner interfaces for using these things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3142" target="_blank">00:52:22.000</a></span> | <span class="t">so you don't have to write 15 different versions, um, or write a make file that looks like llama.cpps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3147" target="_blank">00:52:27.000</a></span> | <span class="t">Don't look at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3148" target="_blank">00:52:28.000</a></span> | <span class="t">Um, uh, very scary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3150" target="_blank">00:52:30.000</a></span> | <span class="t">Uh, there's kind of a sweet spot, actually, in what little, like, next-generation video game consoles,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3156" target="_blank">00:52:36.000</a></span> | <span class="t">because you have total authority to just use up as much of the system as you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3160" target="_blank">00:52:40.000</a></span> | <span class="t">People pay lots of money for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3162" target="_blank">00:52:42.000</a></span> | <span class="t">They often build custom silicon based on what developers want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3166" target="_blank">00:52:46.000</a></span> | <span class="t">So that could be, if you're thinking about what you want to be doing in, like, five years, seven years in this field,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3171" target="_blank">00:52:51.000</a></span> | <span class="t">consider that as a possibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3174" target="_blank">00:52:54.000</a></span> | <span class="t">Lots of people would love to have, um, a real, like, human-like intelligence in the, um, uh, in the things they're shooting in their first-person shooter, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3185" target="_blank">00:53:05.000</a></span> | <span class="t">Like, that, I think that would make a lot of money.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3187" target="_blank">00:53:07.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3188" target="_blank">00:53:08.000</a></span> | <span class="t">Uh, when it comes to building, especially for mobile hardware, with those constraints you're talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3192" target="_blank">00:53:12.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3193" target="_blank">00:53:13.000</a></span> | <span class="t">Can you say a little bit about quantization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3194" target="_blank">00:53:14.000</a></span> | <span class="t">Yeah, so the question was, for mobile hardware, like, what are solutions and specifically quantization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3203" target="_blank">00:53:23.000</a></span> | <span class="t">So, um, when one of the key constraints is memory, like, just trying to make the size of the model smaller and the size of the computation smaller is helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3214" target="_blank">00:53:34.000</a></span> | <span class="t">So, the, like, people are pushing to try and take the parameters of language models down from being two bytes to one byte to half a byte to, like, a single bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3226" target="_blank">00:53:46.000</a></span> | <span class="t">Um, and I think people are kind of stalling out at the, like, half byte level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3230" target="_blank">00:53:50.000</a></span> | <span class="t">Um, and often to actually recognize those gains, you need to, like, write a assembler and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3240" target="_blank">00:54:00.000</a></span> | <span class="t">It's, like, it can get pretty gnarly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3242" target="_blank">00:54:02.000</a></span> | <span class="t">Um, so that's often only, like, highly resourced teams working for a long time that they can actually see those benefits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3249" target="_blank">00:54:09.000</a></span> | <span class="t">Um, the other thing that people talk about a lot, uh, for, like, making models work on smaller devices is sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3255" target="_blank">00:54:15.000</a></span> | <span class="t">Um, and so sparsity means, like, oh, there's this giant weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3260" target="_blank">00:54:20.000</a></span> | <span class="t">Maybe most of them are close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3262" target="_blank">00:54:22.000</a></span> | <span class="t">And maybe we can just, like, get rid of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3264" target="_blank">00:54:24.000</a></span> | <span class="t">Like, if we were gonna go to one bit, there's zero or one, like, why not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3267" target="_blank">00:54:27.000</a></span> | <span class="t">There's zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3268" target="_blank">00:54:28.000</a></span> | <span class="t">And then zero is, like, a very easy number to work with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3271" target="_blank">00:54:31.000</a></span> | <span class="t">Like, you, the number that comes out is multiply at zero, add, you just keep the number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3275" target="_blank">00:54:35.000</a></span> | <span class="t">So it's, you don't need, like, a full logic circuit to handle it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3279" target="_blank">00:54:39.000</a></span> | <span class="t">Um, so there are some things that make use of sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3283" target="_blank">00:54:43.000</a></span> | <span class="t">The problem is that the type of sparsity that neural networks need is called unstructured sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3288" target="_blank">00:54:48.000</a></span> | <span class="t">You have just had zeros kind of, like, scattered around your matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3292" target="_blank">00:54:52.000</a></span> | <span class="t">And all the, like, existing, easy to use, has, like, you know, Python API stuff is, um, uh, is in structured sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3302" target="_blank">00:55:02.000</a></span> | <span class="t">And so you get gains there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3304" target="_blank">00:55:04.000</a></span> | <span class="t">And it's, like, you might need, yeah, a lot, like, yeah, hand-tuned CUDA kernels or, yeah, to, like, actually take use, make use of unstructured sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3312" target="_blank">00:55:12.000</a></span> | <span class="t">So that's something, you know, if there's a ton of pressure, we could see those developments in five years or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3317" target="_blank">00:55:17.000</a></span> | <span class="t">But, um, we haven't seen, people have been thinking about that for almost, for, like, seven years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3322" target="_blank">00:55:22.000</a></span> | <span class="t">And it's, like, not made a ton of progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3325" target="_blank">00:55:25.000</a></span> | <span class="t">But, yeah, helps definitely has made it easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3328" target="_blank">00:55:28.000</a></span> | <span class="t">And, like, Google has been able to fit decent amount of language modeling capabilities on a, uh, on a mobile device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3336" target="_blank">00:55:36.000</a></span> | <span class="t">Um, using distillation and quantization and probably more secrets that I won't share.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3342" target="_blank">00:55:42.000</a></span> | <span class="t">I had a dumb question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3343" target="_blank">00:55:43.000</a></span> | <span class="t">What's the size of these models in terms of memory like it could be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3348" target="_blank">00:55:48.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3349" target="_blank">00:55:49.000</a></span> | <span class="t">Does it only have the whole model in memory?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3351" target="_blank">00:55:51.000</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3352" target="_blank">00:55:52.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3353" target="_blank">00:55:53.000</a></span> | <span class="t">Um, so the, if somebody tells you a number, like, 50B, you know, like, Lama, Lama, Anthropic 52B, Lama 70B, that's billions of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3367" target="_blank">00:56:07.000</a></span> | <span class="t">And then the question is, like, how, what, what, how big is a parameter?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3371" target="_blank">00:56:11.000</a></span> | <span class="t">Like, how many bytes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3372" target="_blank">00:56:12.000</a></span> | <span class="t">And the, like, they're trained or where they, the way, the way they come out of the factory is two bytes per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3378" target="_blank">00:56:18.000</a></span> | <span class="t">So take the number that somebody gives you, multiply it by two, and then the B is giga.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3384" target="_blank">00:56:24.000</a></span> | <span class="t">So, like, a small Lama model is, like, 14 gigabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3389" target="_blank">00:56:29.000</a></span> | <span class="t">Seven B times two, 14 gigabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3392" target="_blank">00:56:32.000</a></span> | <span class="t">So not gonna fit that in phone RAM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3394" target="_blank">00:56:34.000</a></span> | <span class="t">Uh, and that does make, yeah, doing this a lot harder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3399" target="_blank">00:56:39.000</a></span> | <span class="t">Um, you can do things, you can, like, try and do stuff with paging, like, put stuff on the disk, bring it, bring it into RAM, then, like, execute with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3406" target="_blank">00:56:46.000</a></span> | <span class="t">Um, but that, like, slows things down a ton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3410" target="_blank">00:56:50.000</a></span> | <span class="t">Um, so in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3411" target="_blank">00:56:51.000</a></span> | <span class="t">Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3412" target="_blank">00:56:52.000</a></span> | <span class="t">Hmm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3413" target="_blank">00:56:53.000</a></span> | <span class="t">So, people used to train in float 32 and release models in float 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3420" target="_blank">00:57:00.000</a></span> | <span class="t">Maybe, I thought the Lama models were released in float 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3423" target="_blank">00:57:03.000</a></span> | <span class="t">No?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3424" target="_blank">00:57:04.000</a></span> | <span class="t">Yeah, a lot of, like, a lot of people train in this new, like, Google Brain float, uh, thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3430" target="_blank">00:57:10.000</a></span> | <span class="t">And then they, they're doing that because they want to be able to use float 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3434" target="_blank">00:57:14.000</a></span> | <span class="t">And so have two bytes per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3436" target="_blank">00:57:16.000</a></span> | <span class="t">But, like, definitely the, so the, like, the default before that was four bytes per, per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3441" target="_blank">00:57:21.000</a></span> | <span class="t">And before that, when people were doing scientific computing with graphics, with graphics cards, like, um, people at the national labs, the default was, like, four bytes per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3452" target="_blank">00:57:32.000</a></span> | <span class="t">Like, um, or, sorry, eight bytes, 64 bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3455" target="_blank">00:57:35.000</a></span> | <span class="t">Um, because they really needed that, like, high precision and high range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3458" target="_blank">00:57:38.000</a></span> | <span class="t">Um, and, yeah, but now the trend has been to push them lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3463" target="_blank">00:57:43.000</a></span> | <span class="t">And many model releases are now, like, already two bytes, 16 bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3468" target="_blank">00:57:48.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3469" target="_blank">00:57:49.000</a></span> | <span class="t">Um, and now, Georgi Gergenov is, like, immediately converting them down to four bits, uh, and three bits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3477" target="_blank">00:57:57.000</a></span> | <span class="t">Um, which is, like, wild.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3479" target="_blank">00:57:59.000</a></span> | <span class="t">Like, what does that even mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3481" target="_blank">00:58:01.000</a></span> | <span class="t">Um, like, a non-power of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3484" target="_blank">00:58:04.000</a></span> | <span class="t">It's scary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3485" target="_blank">00:58:05.000</a></span> | <span class="t">Unsettling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3486" target="_blank">00:58:06.000</a></span> | <span class="t">Um, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3487" target="_blank">00:58:07.000</a></span> | <span class="t">So, uh, with -- there's another execution target that gets you a lot of the benefits of desktops, which is, like, you have a beefy machine to run on, and it's not yours, so you don't have to pay for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3503" target="_blank">00:58:23.000</a></span> | <span class="t">Um, but then you get a more homogeneous execution environment, which is the browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3507" target="_blank">00:58:27.000</a></span> | <span class="t">Um, so this is not, like, a web app where they, like, talk to a model running in a service, but, like, there is a model inside of the browser that runs inside the browser's, like, runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3516" target="_blank">00:58:36.000</a></span> | <span class="t">Uh, and that, like, the homogenization environments would be very huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3522" target="_blank">00:58:42.000</a></span> | <span class="t">Um, right now, this is kind of, like, awaiting some technical improvements in the world of browsers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3527" target="_blank">00:58:47.000</a></span> | <span class="t">So, there is a target in WebAssembly that you could compile your programs down to, um, and in principle run them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3535" target="_blank">00:58:55.000</a></span> | <span class="t">Uh, the support for, uh, GPUs is very gross.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3539" target="_blank">00:58:59.000</a></span> | <span class="t">Um, there is, uh, a working draft from the WWW Consortium for WebGPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3545" target="_blank">00:59:05.000</a></span> | <span class="t">For WebGPU, which would make it cleaner and easier to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3548" target="_blank">00:59:08.000</a></span> | <span class="t">Um, so that would help the, like, ecosystem, the, like, stack and ecosystem around this for other kinds of web applications is developing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3555" target="_blank">00:59:15.000</a></span> | <span class="t">Will, like, maybe lead developments in using this for, uh, delivering inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3561" target="_blank">00:59:21.000</a></span> | <span class="t">Um, you have a new constraint distinct from the other ones, which is you now, at least as it stands right now, you would need to deliver weights over the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3571" target="_blank">00:59:31.000</a></span> | <span class="t">And so now it's, like, you're, you're, you have kind of the model size constraints that you might associate with mobile hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3577" target="_blank">00:59:37.000</a></span> | <span class="t">Um, but only during the, like, first load.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3580" target="_blank">00:59:40.000</a></span> | <span class="t">Um, so, um, there are probably clever ways to get around that, like, progressively delivering them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3586" target="_blank">00:59:46.000</a></span> | <span class="t">Um, or, uh, like, browser, uh, companies sort of agreeing to incorporate some foundation models into the actual browser runtime itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3597" target="_blank">00:59:57.000</a></span> | <span class="t">Um, so, like, inside of, uh, like, uh, like, v9, uh, an update to v8 with a foundation model already built into the runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3607" target="_blank">01:00:07.000</a></span> | <span class="t">That would make your life a lot easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3609" target="_blank">01:00:09.000</a></span> | <span class="t">Um, so, this, like, would, uh, yeah, browser assistance, maybe sort of, like, general, uh, like, executing apps inside of a browser that feel more like native apps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3621" target="_blank">01:00:21.000</a></span> | <span class="t">Um, that's the potential applications here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3623" target="_blank">01:00:23.000</a></span> | <span class="t">But, um, still a little, um, at the edge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3628" target="_blank">01:00:28.000</a></span> | <span class="t">At the edge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3629" target="_blank">01:00:29.000</a></span> | <span class="t">That was a pun, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3631" target="_blank">01:00:31.000</a></span> | <span class="t">At the edge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3632" target="_blank">01:00:32.000</a></span> | <span class="t">Um, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3633" target="_blank">01:00:33.000</a></span> | <span class="t">So, because, oh yeah, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3637" target="_blank">01:00:37.000</a></span> | <span class="t">How many gigabytes is a small and a large model right now?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3642" target="_blank">01:00:42.000</a></span> | <span class="t">A small and a large model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3644" target="_blank">01:00:44.000</a></span> | <span class="t">So when I hear small, large language model, first I cringe internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3650" target="_blank">01:00:50.000</a></span> | <span class="t">And then I accept GPS system, ATM machine, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3655" target="_blank">01:00:55.000</a></span> | <span class="t">Um, so a small, large language model in my mind is something that has, like, kind of limited ability to, like, speak and interact with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3666" target="_blank">01:01:06.000</a></span> | <span class="t">And that's what you see at, like, the, like, 13 billion to 30 billion parameter range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3672" target="_blank">01:01:12.000</a></span> | <span class="t">Like, the medium size is, like, the 70 billion parameter range, which is, like, the largest open models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3678" target="_blank">01:01:18.000</a></span> | <span class="t">And then, like, a true large language model, the ones that, like, make people scared about losing their jobs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3683" target="_blank">01:01:23.000</a></span> | <span class="t">are generally, like, mixtures of 70 to 100 billion parameter models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3688" target="_blank">01:01:28.000</a></span> | <span class="t">Or maybe they are themselves 200 billion, 280 billion parameter models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3692" target="_blank">01:01:32.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3693" target="_blank">01:01:33.000</a></span> | <span class="t">So then, for all of those, take that and multiply it by, we'll call it two, um, to get the number of gigabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3698" target="_blank">01:01:38.000</a></span> | <span class="t">So, like, half a terabyte for the, um, for, like, a, you know, palm, well, a whole terabyte for palm 540b, um, which is one of the larger ones ever trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3710" target="_blank">01:01:50.000</a></span> | <span class="t">I guess the context for, like, if they are pre-loaded in the browser, would it have to be a smaller one, like, still in the tens of gigabytes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3719" target="_blank">01:01:59.000</a></span> | <span class="t">Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3720" target="_blank">01:02:00.000</a></span> | <span class="t">It's been a long while since I downloaded a browser to my computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3723" target="_blank">01:02:03.000</a></span> | <span class="t">But I want to say that the package that you download to install a browser is in the, like, couple of gigabytes range, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3729" target="_blank">01:02:09.000</a></span> | <span class="t">No, I think it's, like, hundreds of megs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3731" target="_blank">01:02:11.000</a></span> | <span class="t">Hundreds of megs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3732" target="_blank">01:02:12.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3733" target="_blank">01:02:13.000</a></span> | <span class="t">They actually download the whole thing in the installer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3736" target="_blank">01:02:16.000</a></span> | <span class="t">Uh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3738" target="_blank">01:02:18.000</a></span> | <span class="t">Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3739" target="_blank">01:02:19.000</a></span> | <span class="t">Uh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3740" target="_blank">01:02:20.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3741" target="_blank">01:02:21.000</a></span> | <span class="t">Wait, so you download an installer, and then you have to download-- anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3743" target="_blank">01:02:23.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3744" target="_blank">01:02:24.000</a></span> | <span class="t">So if people want to install stuff that's only a few hundred megabytes, then that's a non-starter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3748" target="_blank">01:02:28.000</a></span> | <span class="t">Um, I guess I expect a Linux distro image to be in the, like, couple of gigabytes, like, if I'm playing around with containers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3756" target="_blank">01:02:36.000</a></span> | <span class="t">So that's, um, uh, that's, like, another anchor point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3761" target="_blank">01:02:41.000</a></span> | <span class="t">Um, and also, like, for those things, we're probably talking, like, two, four, eight years before that kind of, like, standardization effort agreement, like, happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3770" target="_blank">01:02:50.000</a></span> | <span class="t">And we can hope that internet speeds will increase in that time to match the increasing needs, uh, uh, of the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3779" target="_blank">01:02:59.000</a></span> | <span class="t">Um, but yeah, that's-- it's a pretty tight constraint, and, like, probably looks a lot more like mobile stuff for a very long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3785" target="_blank">01:03:05.000</a></span> | <span class="t">Um, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3787" target="_blank">01:03:07.000</a></span> | <span class="t">Uh, programming that can change any of this map?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3792" target="_blank">01:03:12.000</a></span> | <span class="t">No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3793" target="_blank">01:03:13.000</a></span> | <span class="t">I think, like, right now, I've been kind of assuming that you're doing stuff relatively efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3800" target="_blank">01:03:20.000</a></span> | <span class="t">And to be honest, like, PyTorch is, like, pretty good at this already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3803" target="_blank">01:03:23.000</a></span> | <span class="t">Like, um, the fact that the application layer is written in Python isn't the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3809" target="_blank">01:03:29.000</a></span> | <span class="t">Um, but yeah, good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3811" target="_blank">01:03:31.000</a></span> | <span class="t">I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3812" target="_blank">01:03:32.000</a></span> | <span class="t">I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3813" target="_blank">01:03:33.000</a></span> | <span class="t">I was curious, what's the cost of inference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3815" target="_blank">01:03:35.000</a></span> | <span class="t">Like, um--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3816" target="_blank">01:03:36.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3817" target="_blank">01:03:37.000</a></span> | <span class="t">Inference on CPU, what's this inference on code?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3820" target="_blank">01:03:40.000</a></span> | <span class="t">How is it, like, distributed through data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3823" target="_blank">01:03:43.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3824" target="_blank">01:03:44.000</a></span> | <span class="t">Um, I think we'll come to that in, uh, once, like, wanted to talk about, um, after we talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3830" target="_blank">01:03:50.000</a></span> | <span class="t">running AI over network, talk about the actual inference workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3833" target="_blank">01:03:53.000</a></span> | <span class="t">Um, so we'll definitely get to talking about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3835" target="_blank">01:03:55.000</a></span> | <span class="t">I don't think-- I'm not going to have a price number to give to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3838" target="_blank">01:03:58.000</a></span> | <span class="t">Um, but you have to take whatever tokens per second you can get, and then, like, however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3843" target="_blank">01:04:03.000</a></span> | <span class="t">much you're spending on GPUs, um, and then convert that into a dollars per token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3849" target="_blank">01:04:09.000</a></span> | <span class="t">Um, and that's going to give you something you can compare to the, like, model providers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3854" target="_blank">01:04:14.000</a></span> | <span class="t">Um, and until you put some decent optimization into it, you aren't going to match them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3859" target="_blank">01:04:19.000</a></span> | <span class="t">Um, you did ask about CPU inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3863" target="_blank">01:04:23.000</a></span> | <span class="t">That is rapidly evolving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3865" target="_blank">01:04:25.000</a></span> | <span class="t">I think there are cases where you can kind of, like, compete in price there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3868" target="_blank">01:04:28.000</a></span> | <span class="t">But, um, yeah, we'll cover that more later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3872" target="_blank">01:04:32.000</a></span> | <span class="t">Uh, it seems like-- let's put a pin in those two things, and we'll come back to them after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3876" target="_blank">01:04:36.000</a></span> | <span class="t">we talk about, like, really the thing that almost everybody's going to do, like, immediately after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3881" target="_blank">01:04:41.000</a></span> | <span class="t">they leave is going to be run AI somewhere in a data center.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3885" target="_blank">01:04:45.000</a></span> | <span class="t">Um, but those are important questions long-term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3887" target="_blank">01:04:47.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3888" target="_blank">01:04:48.000</a></span> | <span class="t">So, uh, like, uh, the running stuff on end-user devices has a lot of reasons why it's not so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3896" target="_blank">01:04:56.000</a></span> | <span class="t">great right now, so what do you get when you run AI workloads in a data center?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3902" target="_blank">01:05:02.000</a></span> | <span class="t">Um, the biggest win is simplicity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3904" target="_blank">01:05:04.000</a></span> | <span class="t">Uh, the biggest pain point is latency, as we've already discussed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3907" target="_blank">01:05:07.000</a></span> | <span class="t">Um, so simplicity, like, you just-- you control the whole environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3910" target="_blank">01:05:10.000</a></span> | <span class="t">It makes your life a lot easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3912" target="_blank">01:05:12.000</a></span> | <span class="t">Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3913" target="_blank">01:05:13.000</a></span> | <span class="t">You said that latency is the biggest pain point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3915" target="_blank">01:05:15.000</a></span> | <span class="t">Is that really a thing for LLMs compared to, like, vision models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3918" target="_blank">01:05:18.000</a></span> | <span class="t">Because, like, anyway, the tokens that you can infer for seconds are quite a bit slower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3923" target="_blank">01:05:23.000</a></span> | <span class="t">than any of the network latency that you talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3927" target="_blank">01:05:27.000</a></span> | <span class="t">Yeah, so I would say that you can-- let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3932" target="_blank">01:05:32.000</a></span> | <span class="t">So the question is whether latency is actually a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3934" target="_blank">01:05:34.000</a></span> | <span class="t">So, um, if you need to do, like, back and forth, like, you need to get something back from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3941" target="_blank">01:05:41.000</a></span> | <span class="t">OpenAI API, then possibly, like, call it again with some added context or, like, run some if statements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3948" target="_blank">01:05:48.000</a></span> | <span class="t">and then send it back, now you're looking at, like, multiple network calls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3952" target="_blank">01:05:52.000</a></span> | <span class="t">And you could avoid all of that overhead if you were running things locally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3956" target="_blank">01:05:56.000</a></span> | <span class="t">So that's an example of the case where people would run into a latency problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3959" target="_blank">01:05:59.000</a></span> | <span class="t">But locally you're gonna get way lower tokens per second anyway, so you're completely dominated by the time it takes to generate tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3969" target="_blank">01:06:09.000</a></span> | <span class="t">So tokens per second is a throughput number, not a latency number, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3973" target="_blank">01:06:13.000</a></span> | <span class="t">So it doesn't matter if your tokens per second is half-- if your tokens per second is half that of what OpenAI is getting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3980" target="_blank">01:06:20.000</a></span> | <span class="t">but you only need to generate 30 tokens, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3983" target="_blank">01:06:23.000</a></span> | <span class="t">that-- then, like, the latency number is going to be the larger one, even though your, like, throughput is lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3991" target="_blank">01:06:31.000</a></span> | <span class="t">Like, this is definitely something that people have, like, run into when you have, like, highly interactive things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=3997" target="_blank">01:06:37.000</a></span> | <span class="t">Like, they're definitely-- so to be clear, there are tons of applications in which you don't feel this pain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4003" target="_blank">01:06:43.000</a></span> | <span class="t">And, like, ChatGPT, for example, is at this point, like, the latency of the response from the machine is not really the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4011" target="_blank">01:06:51.000</a></span> | <span class="t">So, yeah, it's not guaranteed to be a pain point, I would say, as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4016" target="_blank">01:06:56.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4018" target="_blank">01:06:58.000</a></span> | <span class="t">So, yeah, and I guess I'm also kind of maybe imagining situations that are closer to the computer vision case in which you need cognition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4032" target="_blank">01:07:12.000</a></span> | <span class="t">well, in the computer vision case, you need rapid responses because it's, like, in the motor loop of a system, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4039" target="_blank">01:07:19.000</a></span> | <span class="t">And if we want to use language models as the cognitive component of a moving system, then they would need latencies like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4045" target="_blank">01:07:25.000</a></span> | <span class="t">like in the tens of milliseconds or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4047" target="_blank">01:07:27.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4049" target="_blank">01:07:29.000</a></span> | <span class="t">And you are never going to be able to achieve that over a network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4054" target="_blank">01:07:34.000</a></span> | <span class="t">But, yeah, great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4057" target="_blank">01:07:37.000</a></span> | <span class="t">All right, so inference as a service providers, this makes it super easy to get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4061" target="_blank">01:07:41.000</a></span> | <span class="t">It's what, you know, when you're using OpenAI, they are inference as a service provider.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4066" target="_blank">01:07:46.000</a></span> | <span class="t">Also, all the proprietary models basically live here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4068" target="_blank">01:07:48.000</a></span> | <span class="t">There's not some, like, way that they would ship you the model and you could run it and it's proprietary license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4074" target="_blank">01:07:54.000</a></span> | <span class="t">That doesn't exist yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4076" target="_blank">01:07:56.000</a></span> | <span class="t">Open models are also available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4079" target="_blank">01:07:59.000</a></span> | <span class="t">So, like, if you want to bet on the, like, open ecosystem, you can use a service like Replicate that will, like, they'll run open models for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4090" target="_blank">01:08:10.000</a></span> | <span class="t">It's generally, like, easy to get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4093" target="_blank">01:08:13.000</a></span> | <span class="t">It's not that much more expensive than running it yourself in a lot of cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4098" target="_blank">01:08:18.000</a></span> | <span class="t">But you have limited control of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4100" target="_blank">01:08:20.000</a></span> | <span class="t">For proprietary models, we already talked about how you would have less control kind of inherently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4106" target="_blank">01:08:26.000</a></span> | <span class="t">So, for even for people who are providing open models, since they don't have IP they want to protect, in order for them to, like, serve it cheaply to you, they need to have, like, and to have, like, an economic win that they can, like, pass on to you and, like, keep a little bit for themselves, they need to do something like amortize costs across many users of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4125" target="_blank">01:08:45.000</a></span> | <span class="t">Many more than you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4127" target="_blank">01:08:47.000</a></span> | <span class="t">And that requires some amount of homogeneity of usage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4130" target="_blank">01:08:50.000</a></span> | <span class="t">And it's, like, right now it's proven to be, like, pretty hard to give people control while also giving them homogeneity of usage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4136" target="_blank">01:08:56.000</a></span> | <span class="t">For something like AWS, they came up with really smart ways to cache pieces of containers so that the fact that everybody's using kind of the same software allows them to amortize while also giving customization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4150" target="_blank">01:09:10.000</a></span> | <span class="t">But people have not figured out a similar trick for language models or image generation models yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4159" target="_blank">01:09:19.000</a></span> | <span class="t">So, you don't have as much control as you would have yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4163" target="_blank">01:09:23.000</a></span> | <span class="t">So, new constraints arise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4165" target="_blank">01:09:25.000</a></span> | <span class="t">So, there are things like API rate limits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4168" target="_blank">01:09:28.000</a></span> | <span class="t">And now this is sort of like a cost management game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4171" target="_blank">01:09:31.000</a></span> | <span class="t">You look at this as, like, rather than, like, paying up front for compute that you have, and then you think about maxing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4179" target="_blank">01:09:39.000</a></span> | <span class="t">the use of that compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4181" target="_blank">01:09:41.000</a></span> | <span class="t">You think the other direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4183" target="_blank">01:09:43.000</a></span> | <span class="t">You try to minimize your use of compute while fitting the rest of your constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4186" target="_blank">01:09:46.000</a></span> | <span class="t">So, it's a very different feeling, you know, if you ever switch between having your own compute and switching to cloud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4192" target="_blank">01:09:52.000</a></span> | <span class="t">It's the same idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4193" target="_blank">01:09:53.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4198" target="_blank">01:09:58.000</a></span> | <span class="t">So, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4201" target="_blank">01:10:01.000</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4202" target="_blank">01:10:02.000</a></span> | <span class="t">So, you could do that inference yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4205" target="_blank">01:10:05.000</a></span> | <span class="t">So, rather than having somebody else do it for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4208" target="_blank">01:10:08.000</a></span> | <span class="t">And this works pretty well and is getting easier every day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4211" target="_blank">01:10:11.000</a></span> | <span class="t">So, cloud, like, this is, like, running stuff on a public cloud is, like, one of the most popular choices for how to run ML workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4220" target="_blank">01:10:20.000</a></span> | <span class="t">And for, like, SaaS in general, there's some specialist cloud providers in this space, like Lambda Labs, that can be, like, very competitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4230" target="_blank">01:10:30.000</a></span> | <span class="t">They're, like, often cheaper than the, like, big three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4235" target="_blank">01:10:35.000</a></span> | <span class="t">And it's a nice balance of control with, like, complexity and, like, which things you actually care about having to deal with versus not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4243" target="_blank">01:10:43.000</a></span> | <span class="t">It can get expensive over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4245" target="_blank">01:10:45.000</a></span> | <span class="t">It's definitely, like, you know, more expensive than, like, over a long period of time than if you bought the stuff yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4252" target="_blank">01:10:52.000</a></span> | <span class="t">GPUs sometimes feel like second-class citizens and especially a lot of, like, big public clouds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4258" target="_blank">01:10:58.000</a></span> | <span class="t">Google Cloud's a bit of a distinction there in that you can just add GPUs to any instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4265" target="_blank">01:11:05.000</a></span> | <span class="t">It's kind of nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4266" target="_blank">01:11:06.000</a></span> | <span class="t">But in other public clouds, that's not really the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4270" target="_blank">01:11:10.000</a></span> | <span class="t">And, again, this is, like, a cost management problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4274" target="_blank">01:11:14.000</a></span> | <span class="t">And one of the popular ways to solve the cloud costs is to just agree to a large deal up front.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4282" target="_blank">01:11:22.000</a></span> | <span class="t">And now you're starting to get some of the illiquidity associated with actually building buying hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4287" target="_blank">01:11:27.000</a></span> | <span class="t">And you start to get some of the, like, vendor lock-in that you would also associate with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4293" target="_blank">01:11:33.000</a></span> | <span class="t">So you have the opportunity to kind of, like, trade those things off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4296" target="_blank">01:11:36.000</a></span> | <span class="t">But they are your constraints to work with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4299" target="_blank">01:11:39.000</a></span> | <span class="t">I did want to call out that there are some serverless approaches, which gives you some of the, like, usage-based,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4307" target="_blank">01:11:47.000</a></span> | <span class="t">like, really tightly usage-based pricing associated with inference-as-a-service providers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4312" target="_blank">01:11:52.000</a></span> | <span class="t">But also the, like, control associated with, like, you know, renting servers in the cloud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4320" target="_blank">01:12:00.000</a></span> | <span class="t">And by this, by serverless, I mean anything with, like, scale-to-zero semantics and pricing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4325" target="_blank">01:12:05.000</a></span> | <span class="t">That doesn't involve you having to, like, literally manage servers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4329" target="_blank">01:12:09.000</a></span> | <span class="t">So, like, thinking about the operating system, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4333" target="_blank">01:12:13.000</a></span> | <span class="t">And that offers high availability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4335" target="_blank">01:12:15.000</a></span> | <span class="t">This is, like, a relatively new category in software in general and especially in machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4340" target="_blank">01:12:20.000</a></span> | <span class="t">There's a couple of players here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4342" target="_blank">01:12:22.000</a></span> | <span class="t">Modal Labs is one that I like quite a bit because it doesn't just do the ML stuff, though it is very good at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4349" target="_blank">01:12:29.000</a></span> | <span class="t">So, Replicate, which also does inference-as-a-service, will do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4353" target="_blank">01:12:33.000</a></span> | <span class="t">Hugging face spaces recently changed their endpoints to scale-to-zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4357" target="_blank">01:12:37.000</a></span> | <span class="t">And there's also, yeah, banana.dev and others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4361" target="_blank">01:12:41.000</a></span> | <span class="t">The good thing is that it's, like, easier to get started, especially if you're not, like, a cloud ops person.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4368" target="_blank">01:12:48.000</a></span> | <span class="t">And very inexpensive at low traffic, like, you only have to pay when you have traffic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4372" target="_blank">01:12:52.000</a></span> | <span class="t">And if you're, like, running a small, if you're running a demo that only needs to be up when you're showing it to investors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4380" target="_blank">01:13:00.000</a></span> | <span class="t">Or if you are working on a tiny feature at a large organization, then you might have very low traffic patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4386" target="_blank">01:13:06.000</a></span> | <span class="t">Oh, my.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4388" target="_blank">01:13:08.000</a></span> | <span class="t">It's Fleet Week, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4390" target="_blank">01:13:10.000</a></span> | <span class="t">So that might be the Blue Angels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4392" target="_blank">01:13:12.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4393" target="_blank">01:13:13.000</a></span> | <span class="t">Scale-to-zero means that you, when there are no requests, you are not, when your requests go to zero, the amount of resources that you are using and being charged for also goes to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4412" target="_blank">01:13:32.000</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4413" target="_blank">01:13:33.000</a></span> | <span class="t">So, yeah, for a while, Hugging Face spaces, endpoints, they changed, there's inference endpoints, and, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4425" target="_blank">01:13:45.000</a></span> | <span class="t">For a while, it was, like, it could scale down to one, and it would autoscale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4428" target="_blank">01:13:48.000</a></span> | <span class="t">So there's, like, having a cloud server with autoscaling built in, and then there's that thing, but then it also scales to zero, and you don't have to think about server management.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4437" target="_blank">01:13:57.000</a></span> | <span class="t">And that, like, is the combination, like, it's the original, like, AWS definition of serverless that has kind of fallen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4444" target="_blank">01:14:04.000</a></span> | <span class="t">Not everyone goes by the old ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4446" target="_blank">01:14:06.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4447" target="_blank">01:14:07.000</a></span> | <span class="t">So inexpensive at low traffic, when nobody's calling your API, you don't pay for anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4454" target="_blank">01:14:14.000</a></span> | <span class="t">If you, like, come up with a feature, it doesn't work, then it doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4458" target="_blank">01:14:18.000</a></span> | <span class="t">The bad news is that you kind of generally lose, like, tight control over autoscaling behavior that you could have if you were, like, you know, if you have a, you know, Kubernetes team to work with, they can very tightly set it up so that the autoscaling delivers exactly the throughput and latencies, P99s, that you promised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4481" target="_blank">01:14:41.000</a></span> | <span class="t">And you kind of give all over some of that control to these serverless providers who are themselves probably running Kubernetes, but for a lot of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4488" target="_blank">01:14:48.000</a></span> | <span class="t">And then the thing that has kind of prevented this from being as successful as maybe serverless architectures in many other places is latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4499" target="_blank">01:14:59.000</a></span> | <span class="t">So when you're, it shows up as kind of P99 latency, so the 99th percentile of requests that hit a point when you need to do autoscaling, the, you need to get the weights of the model you're using into, not just off of disk and into RAM, but then from there into the RAM of the accelerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4526" target="_blank">01:15:26.000</a></span> | <span class="t">And that takes, like, that can take a very long amount of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4531" target="_blank">01:15:31.000</a></span> | <span class="t">And so you're looking at, like, 30-second, one-minute, three-minute cold boots in some cases because you are moving half a terabyte of data around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4541" target="_blank">01:15:41.000</a></span> | <span class="t">And so that's a place where people could maybe come up with these, like, clever ways to cache and share.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4547" target="_blank">01:15:47.000</a></span> | <span class="t">But, yeah, it's the memory constraint that you hit in other domains showing up, like, in disguise as latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4554" target="_blank">01:15:54.000</a></span> | <span class="t">And, yeah, so the, like, you still are probably going to be thinking of this in terms of, like, cost management and cost reduction as opposed to, like, resource maximization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4566" target="_blank">01:16:06.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4567" target="_blank">01:16:07.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4568" target="_blank">01:16:08.000</a></span> | <span class="t">So the point was about Cloudflare workers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4584" target="_blank">01:16:24.000</a></span> | <span class="t">So I did see that Cloudflare, I didn't include them on the slide, but Cloudflare actually recently released these, like, GPU workers, which is their entry into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4594" target="_blank">01:16:34.000</a></span> | <span class="t">And I haven't had time to play with it, so I don't know that much about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4597" target="_blank">01:16:37.000</a></span> | <span class="t">I think if I need to go from not consuming any of your resources to having a terabyte of my own personal bytes, like, in the VRAM of a GPU, I find it hard to believe that they don't have a latency problem there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4616" target="_blank">01:16:56.000</a></span> | <span class="t">Like, so I'm curious what you know about the solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4620" target="_blank">01:17:00.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4621" target="_blank">01:17:01.000</a></span> | <span class="t">Yeah, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4622" target="_blank">01:17:02.000</a></span> | <span class="t">Yeah, that's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4625" target="_blank">01:17:05.000</a></span> | <span class="t">Yeah, I'd love to hear about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4627" target="_blank">01:17:07.000</a></span> | <span class="t">That's been my experience with the other serverless GPU providers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4630" target="_blank">01:17:10.000</a></span> | <span class="t">So I'd love to hear more about the Cloudflare workers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4633" target="_blank">01:17:13.000</a></span> | <span class="t">And, yeah, if that goes away, then serverless becomes a much more competitive way of delivering inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4640" target="_blank">01:17:20.000</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4642" target="_blank">01:17:22.000</a></span> | <span class="t">So I maintain a page for full-stack deep learning that has information about, like, cloud GPUs and serverless providers pricing and what compute they provide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4654" target="_blank">01:17:34.000</a></span> | <span class="t">So you can check that out from the slides later if you're interested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4657" target="_blank">01:17:37.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4658" target="_blank">01:17:38.000</a></span> | <span class="t">And then last, let's talk, like, actually, what if you actually physically owned the computers that the inference ran on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4671" target="_blank">01:17:51.000</a></span> | <span class="t">Like, you can do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4673" target="_blank">01:17:53.000</a></span> | <span class="t">And rather than having to, like, actually, you know, construct a building which maybe the largest enterprises could go about, using a co-location facility isn't so bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4687" target="_blank">01:18:07.000</a></span> | <span class="t">And there's more reason to do this than for other kinds of workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4692" target="_blank">01:18:12.000</a></span> | <span class="t">And in particular, there's actually room to beat a lot of the major public clouds, which is why there's competitive clouds, like, alternatives in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4700" target="_blank">01:18:20.000</a></span> | <span class="t">A lot of data centers that have been, like, around or that were designed before 2021 or so are configured for, like, disk and network heavy workloads rather than power heavy workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4713" target="_blank">01:18:33.000</a></span> | <span class="t">So even if you can get a hold of, like, 30,000 A100s, you can't just necessarily put them in the same U.S. East data center that used to run -- that was designed for, like, running databases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4728" target="_blank">01:18:48.000</a></span> | <span class="t">So it's capital intensive but ends up being cheaper in the long run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4733" target="_blank">01:18:53.000</a></span> | <span class="t">You have total control if you need it, which is awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4737" target="_blank">01:18:57.000</a></span> | <span class="t">But it's very hard, very rare skill set because it, like, kind of crosses this, like, the ML stuff and the hardware stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4746" target="_blank">01:19:06.000</a></span> | <span class="t">And all of these people can go and work for OpenAI for, like, a million and a half a year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4751" target="_blank">01:19:11.000</a></span> | <span class="t">So good luck holding on to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4755" target="_blank">01:19:15.000</a></span> | <span class="t">And the biggest constraint that shows up is illiquidity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4758" target="_blank">01:19:18.000</a></span> | <span class="t">So you're going to make a big bet on what this looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4761" target="_blank">01:19:21.000</a></span> | <span class="t">For example, that inference is not going to move on to CPU or not going to move on to custom silicon that behaves very differently from graphics cards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4768" target="_blank">01:19:28.000</a></span> | <span class="t">There's a great talk from Mitesh Agrawal of Lambda Labs about this that goes into kind of detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4774" target="_blank">01:19:34.000</a></span> | <span class="t">I think it's only, like, a year old, if I remember this talk right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4778" target="_blank">01:19:38.000</a></span> | <span class="t">And, of course, he makes it sound very hard because he wants you to use their cloud or to pay them to, like, help you build your co-location -- help you actually build it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4793" target="_blank">01:19:53.000</a></span> | <span class="t">But it is a detailed explanation of everything involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4796" target="_blank">01:19:56.000</a></span> | <span class="t">And, you know, there's not very many of those out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4799" target="_blank">01:19:59.000</a></span> | <span class="t">So let's go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4802" target="_blank">01:20:02.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4803" target="_blank">01:20:03.000</a></span> | <span class="t">We're at half time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4805" target="_blank">01:20:05.000</a></span> | <span class="t">So I plan to take a break when I finish part one, which goes to the rest of self-serve inference, which we'll all say is another 15 minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4812" target="_blank">01:20:12.000</a></span> | <span class="t">So let's do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4813" target="_blank">01:20:13.000</a></span> | <span class="t">And we'll leave an hour for part two after a little break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4818" target="_blank">01:20:18.000</a></span> | <span class="t">So we actually haven't talked in great detail about, like, you know, why are we using GPUs in the first place?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4831" target="_blank">01:20:31.000</a></span> | <span class="t">Like, what is actually going on here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4833" target="_blank">01:20:33.000</a></span> | <span class="t">When we run this, like, tensor-to-tensor map with neural networks, like, what actually does that workload turn into?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4843" target="_blank">01:20:43.000</a></span> | <span class="t">We have two tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4844" target="_blank">01:20:44.000</a></span> | <span class="t">We need to load numbers from memory, and then we need to do math on those numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4849" target="_blank">01:20:49.000</a></span> | <span class="t">Those are our, like, two basic tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4852" target="_blank">01:20:52.000</a></span> | <span class="t">And that is the reason why we have -- why we end up using graphics processing units, because memory is slow and math is fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4863" target="_blank">01:21:03.000</a></span> | <span class="t">And in most -- in the transformer architecture in particular, but in many, like, sort of most neural network architectures you might write down, you only need a given number from the weights, like, one time per input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4882" target="_blank">01:21:22.000</a></span> | <span class="t">So that means you need to do a memory read, like, of this, of, like, a couple of bytes for a particular parameter to use it in a single floating-point operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4892" target="_blank">01:21:32.000</a></span> | <span class="t">And the memory read is going to be very slow, and the floating-point operation is going to be basically instant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4897" target="_blank">01:21:37.000</a></span> | <span class="t">So in order to do this economically, you need to do a lot of math for each read for memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4903" target="_blank">01:21:43.000</a></span> | <span class="t">You need to, like, load, you know, load the weight out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4909" target="_blank">01:21:49.000</a></span> | <span class="t">And in particular, we're talking here about, like, getting out of the VRAM and into the place where the -- you know, into the -- like, the -- what is it called?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4919" target="_blank">01:21:59.000</a></span> | <span class="t">Well, yeah, it's basically like an L1 cache, like, closer to the actual computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4924" target="_blank">01:22:04.000</a></span> | <span class="t">And so you want to do that and then, like, use it multiple times, you know, and that means you want to run on multiple inputs at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4934" target="_blank">01:22:14.000</a></span> | <span class="t">And that is memory intensive, single instruction, multiple data, parallel linear algebra, the same thing you need for graphics workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4944" target="_blank">01:22:24.000</a></span> | <span class="t">So the, like, graphics cards have turned out to be, like, pretty good at solving this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4950" target="_blank">01:22:30.000</a></span> | <span class="t">And the numbers there in the corner are, like, demonstrate this, like, general fact of, like, memory is slow, logic or math is fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4966" target="_blank">01:22:46.000</a></span> | <span class="t">The -- you can do 312 teraflops per second in -- for two-byte numbers, two-byte floating-point numbers in a tensor core in an A100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4977" target="_blank">01:22:57.000</a></span> | <span class="t">And you only get one and a half terabytes a second of memory bandwidth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4984" target="_blank">01:23:04.000</a></span> | <span class="t">And when you're using, like, you know, optimized existing CUDA kernels, these two things are, like, multiplexed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4991" target="_blank">01:23:11.000</a></span> | <span class="t">So, like, you load a weight where you start doing math on it and then the next weight gets loaded, like, you know, concurrently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=4998" target="_blank">01:23:18.000</a></span> | <span class="t">But you do still have this, like, mismatch in the bandwidths that means that you want to be able to, like, amortize a memory load across as many computations as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5009" target="_blank">01:23:29.000</a></span> | <span class="t">And, like, in principle, this could be flipped around and we would have, like, very -- you know, things would look very different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5015" target="_blank">01:23:35.000</a></span> | <span class="t">So, you can get, like, very, very large throughput gains by amortizing memory reads where, basically, if you are operating on a very small number of tokens, then you'll see that as you add -- like, if you're running this workload yourself, as you add more tokens, you would think, like, you should expect, like, a slight increase in the amount of time that it takes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5037" target="_blank">01:23:57.000</a></span> | <span class="t">And you'll see, like, basically a flat curve for a very long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5041" target="_blank">01:24:01.000</a></span> | <span class="t">And then once you hit the point where the -- yeah, so if you look -- batch size at which you'll see that flip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5056" target="_blank">01:24:16.000</a></span> | <span class="t">So, for an A100, it's about 200 elements in the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5061" target="_blank">01:24:21.000</a></span> | <span class="t">And you'll see -- for an H100, you'll see that that ratio -- these numbers both go up, but the ratio becomes more extreme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5069" target="_blank">01:24:29.000</a></span> | <span class="t">So, you -- there's a great blog post from Carol Chen on, like, inference arithmetic that both goes through in greater detail and then, like, matches that onto some, like, actual experimental results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5082" target="_blank">01:24:42.000</a></span> | <span class="t">And it's able to, like, track where did each, you know, microsecond, basically, of inference time come from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5089" target="_blank">01:24:49.000</a></span> | <span class="t">So, the, like, key takeaway from this is that if you want to get large throughput in, like, an inference system that you're running yourself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5099" target="_blank">01:24:59.000</a></span> | <span class="t">you're going to need batching, you're going to need to, like, collect up multiple inputs from multiple users and operate on them at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5106" target="_blank">01:25:06.000</a></span> | <span class="t">So, this is -- it's challenging to achieve the same thing in an end-user device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5113" target="_blank">01:25:13.000</a></span> | <span class="t">You are -- like, if you're only working for one person, then they might not make 10 requests, you know, quickly enough for you to fill up a batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5121" target="_blank">01:25:21.000</a></span> | <span class="t">And that actually might kind of tilt things in the direction of CPUs or of, you know, different architecture with different, you know, memory bandwidth versus --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5136" target="_blank">01:25:36.000</a></span> | <span class="t">TensorFlow versus math bandwidth trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5141" target="_blank">01:25:41.000</a></span> | <span class="t">One of the useful things that came out of this -- I guess this is a restatement of kind of what I just said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5149" target="_blank">01:25:49.000</a></span> | <span class="t">When you have a, like, low load API, you will end up with smaller batch sizes, because maybe you have -- you have to deliver with a certain latency, so you can't just wait forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5160" target="_blank">01:26:00.000</a></span> | <span class="t">And so you will make different decisions about compute memory trade-offs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5164" target="_blank">01:26:04.000</a></span> | <span class="t">For example, like, caching past computations of keys and values is very common when you're doing batch inference, but it's actually not necessarily the right choice when you are already memory bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5176" target="_blank">01:26:16.000</a></span> | <span class="t">Trading off memory for compute isn't a good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5180" target="_blank">01:26:20.000</a></span> | <span class="t">If you're, like, at -- for larger batch sizes, you -- if you're doing something that's an API where, like, serving requests directly, then you would want to --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5189" target="_blank">01:26:29.000</a></span> | <span class="t">like, roughly balance being flop bound and memory bound so that you can get quick latency of responses, even though your throughput is less than it would be otherwise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5200" target="_blank">01:26:40.000</a></span> | <span class="t">Whereas if you're doing, like, a batch job, like an overnight-type job, or that is, like, typical of data science or big data workloads, then you would just go for the largest batch that you can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5212" target="_blank">01:26:52.000</a></span> | <span class="t">And that's what people do during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5214" target="_blank">01:26:54.000</a></span> | <span class="t">They go for the absolute largest batch that they can, because there's no latency requirement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5218" target="_blank">01:26:58.000</a></span> | <span class="t">There's only throughput.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5219" target="_blank">01:26:59.000</a></span> | <span class="t">I'm assuming all this still applies to the other accelerators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5223" target="_blank">01:27:03.000</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5224" target="_blank">01:27:04.000</a></span> | <span class="t">So TPUs end up -- I, like, looked around, and I've never personally used them for anything serious, so I wasn't able to, like, directly map on, like, pull out the equivalent of a tensor core, like, flop bandwidth and VRAM to L1 latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5245" target="_blank">01:27:25.000</a></span> | <span class="t">But the results and benchmarks that I've seen is that they're, like, 30% better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5251" target="_blank">01:27:31.000</a></span> | <span class="t">Like, they have a -- they do have a slightly different choice of trade-off, and they -- and it's better for neural network workloads than graphics workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5261" target="_blank">01:27:41.000</a></span> | <span class="t">And that gets what -- from what I have seen, like, 30% improvement, but not, like, a 10 to 50x improvement, which is what you would really like to see if you're making as drastic a decision as, like, going over to a completely different accelerator with a completely different software stack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5277" target="_blank">01:27:57.000</a></span> | <span class="t">Yeah, I guess -- yeah, I got into a discussion with one of the people on the TPU team about, like, the hardware lottery, and he was, like, the GPU is already, like, is just an excellent machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5292" target="_blank">01:28:12.000</a></span> | <span class="t">Like, it does this workload really, really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5294" target="_blank">01:28:14.000</a></span> | <span class="t">And I think that is borne out in the numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5297" target="_blank">01:28:17.000</a></span> | <span class="t">Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5298" target="_blank">01:28:18.000</a></span> | <span class="t">From what I heard, the main advantage of TPUs is operational efficiency and power efficiency, not, like, good performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5307" target="_blank">01:28:27.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5308" target="_blank">01:28:28.000</a></span> | <span class="t">So, for the folks listening online, repeating the point, one of the benefits of TPUs is power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5315" target="_blank">01:28:35.000</a></span> | <span class="t">I totally buy that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5316" target="_blank">01:28:36.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5317" target="_blank">01:28:37.000</a></span> | <span class="t">I think -- yeah, the numbers that tend to get reported in things like Google's Pathways paper and Palm Papers are open -- and, like, what is known about OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5326" target="_blank">01:28:46.000</a></span> | <span class="t">It's, like, it's all about, like, flop utilization and total flops and stuff like that, and not things like power that do matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5334" target="_blank">01:28:54.000</a></span> | <span class="t">I'd say, like, people have not gone rushing to try and get a hold of them, which feels like a strong signal, similar with, like, other types of custom silicon, like Cerebris' chip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5348" target="_blank">01:29:08.000</a></span> | <span class="t">But, you know, who knows?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5350" target="_blank">01:29:10.000</a></span> | <span class="t">I think this is getting back to a question that got asked earlier, I believe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5355" target="_blank">01:29:15.000</a></span> | <span class="t">The, like, having custom chips works really well when workloads stay very fixed in, like, kind of precise detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5362" target="_blank">01:29:22.000</a></span> | <span class="t">Not just this, like, oh, we need to load from memory and then we need to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5365" target="_blank">01:29:25.000</a></span> | <span class="t">But, like, no, we need to do this shaped thing, like, with this many -- like, that works super well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5370" target="_blank">01:29:30.000</a></span> | <span class="t">And you can see that in blockchain mining where there is, like, for many chains for a long time, ASICs were the, like, only profitable way to mine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5380" target="_blank">01:29:40.000</a></span> | <span class="t">Or, like, the most profitable way to mine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5383" target="_blank">01:29:43.000</a></span> | <span class="t">And that's the workload is, like, unchangeable except by, like, distributed consensus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5388" target="_blank">01:29:48.000</a></span> | <span class="t">And that allows you to, like, very tightly target a specific workload.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5392" target="_blank">01:29:52.000</a></span> | <span class="t">Whereas, like, neural network architectures actually kind of change reasonably often.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5397" target="_blank">01:29:57.000</a></span> | <span class="t">And the, like, you know, even the difference -- the difference between a 7 billion parameter model, a 13 billion parameter model, a 170 billion parameter model, that's, like, a bigger difference than you would see between workloads for, like, the same blockchain over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5412" target="_blank">01:30:12.000</a></span> | <span class="t">And so that's -- that, like, difference in -- like, that technical difference is, I think, a big reason why we haven't seen much uptake of custom -- custom --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5427" target="_blank">01:30:27.000</a></span> | <span class="t">Yeah, I think -- like, they're one of the -- like, they're one of the -- they're one of the providers for -- like, they're one of the startups working on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5436" target="_blank">01:30:36.000</a></span> | <span class="t">They come up most frequently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5438" target="_blank">01:30:38.000</a></span> | <span class="t">I don't -- it's still, like, fairly experimental, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5444" target="_blank">01:30:44.000</a></span> | <span class="t">Like, it's not -- it's not the -- it's not, like, generally available in a public cloud or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5451" target="_blank">01:30:51.000</a></span> | <span class="t">So, yeah, I don't have much to say about it, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5456" target="_blank">01:30:56.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5457" target="_blank">01:30:57.000</a></span> | <span class="t">I think the main thing you can see there is the memory that they have is 40 gigabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5464" target="_blank">01:31:04.000</a></span> | <span class="t">Compare that with an Android, which is 80.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5467" target="_blank">01:31:07.000</a></span> | <span class="t">And for most of Elements, memory is .</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5470" target="_blank">01:31:10.000</a></span> | <span class="t">Mm-hmm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5474" target="_blank">01:31:14.000</a></span> | <span class="t">Yeah, yeah, so Cerebus has done some large model training, so it's definitely, like, possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5484" target="_blank">01:31:24.000</a></span> | <span class="t">They -- they have this very fast memory bandwidth, I think, is, like, maybe the big one, 20 petabytes memory bandwidth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5493" target="_blank">01:31:33.000</a></span> | <span class="t">Now, like, the point that was raised about the bottleneck being the memory capacity is very -- is well taken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5501" target="_blank">01:31:41.000</a></span> | <span class="t">You can network multiple chips together, and then you have more storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5506" target="_blank">01:31:46.000</a></span> | <span class="t">But, yeah, and so, like, better balancing of memory bandwidth and math bandwidth would maybe get you more -- like, more efficacy at lower batch sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5517" target="_blank">01:31:57.000</a></span> | <span class="t">But you can't exactly put that chip in a phone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5520" target="_blank">01:32:00.000</a></span> | <span class="t">So that -- yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5523" target="_blank">01:32:03.000</a></span> | <span class="t">But definitely, like, people are really converging on transformer architectures, so that could mean that this custom silicon works better in five years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5536" target="_blank">01:32:16.000</a></span> | <span class="t">Yeah, I think I -- yeah, the way that you, like -- the way that you solve this problem is if you have multiple GPUs, these -- the, like, memory per second and flops per second, like, scale at the same amount, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5546" target="_blank">01:32:26.000</a></span> | <span class="t">You have -- now you have two GPUs loading, you have two GPUs doing operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5551" target="_blank">01:32:31.000</a></span> | <span class="t">They both scale linearly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5552" target="_blank">01:32:32.000</a></span> | <span class="t">If you're really clever, you can, like, actually get that linear scaling in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5556" target="_blank">01:32:36.000</a></span> | <span class="t">The GPU memory goes up as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5559" target="_blank">01:32:39.000</a></span> | <span class="t">And so you can serve larger batches and eventually hit -- this ratio is staying the same, so you can eventually hit that point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5566" target="_blank">01:32:46.000</a></span> | <span class="t">And that's what -- and then, like, once you hit the point where you're actually, you know, flops bound, you can just, like, crank it, you know, just crank it up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5576" target="_blank">01:32:56.000</a></span> | <span class="t">Just get really big -- really big batches, speed up -- speed up the computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5582" target="_blank">01:33:02.000</a></span> | <span class="t">And that is what people do during -- are, like, used to do -- used to doing during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5587" target="_blank">01:33:07.000</a></span> | <span class="t">And so you'll very commonly see people talking about this way of making inference efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5592" target="_blank">01:33:12.000</a></span> | <span class="t">So if you are running a -- like, a web service, this is going to give you your, sort of, like, pod size, roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5600" target="_blank">01:33:20.000</a></span> | <span class="t">Like, a pod in Kubernetes is, like, a bunch of services that end up on a single physical machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5605" target="_blank">01:33:25.000</a></span> | <span class="t">So this is your, like, unit -- is determined by, like, how many GPUs do you need to hit the, like, batch size for efficiency implied by the, like, this flops memory per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5620" target="_blank">01:33:40.000</a></span> | <span class="t">And you check in to make sure this is also true for your architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5623" target="_blank">01:33:43.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5624" target="_blank">01:33:44.000</a></span> | <span class="t">All that's, like, kind of theoretical stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5628" target="_blank">01:33:48.000</a></span> | <span class="t">or sort of, like, trying to be relatively first principles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5632" target="_blank">01:33:52.000</a></span> | <span class="t">When you -- when it comes time to actually check whether you're doing things correctly, make sure to use, like, a profiler or a tracer and actually, like, look at these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5640" target="_blank">01:34:00.000</a></span> | <span class="t">Even, like, people who are pretty good at this will, like, miss stuff that shows up on a tracer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5646" target="_blank">01:34:06.000</a></span> | <span class="t">Like, like, the VLLM implementation was launching a CUDA kernel for, like, every individual element of a batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5655" target="_blank">01:34:15.000</a></span> | <span class="t">And, like, a tiny change made it switch to, like, one CUDA kernel launch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5661" target="_blank">01:34:21.000</a></span> | <span class="t">And that saved them, like, increased their throughput by 33%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5666" target="_blank">01:34:26.000</a></span> | <span class="t">And it's something that shows up as, like, way more CUDA kernel launches, which would show up as, like, I guess -- if I had the interactive version of this -- would show up as this, like, huge thing of red lines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5680" target="_blank">01:34:40.000</a></span> | <span class="t">And also just the, like, general utilization shows up as these big, like, patches of gray here, which shows you where you're, like, not actually using your GPU at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5689" target="_blank">01:34:49.000</a></span> | <span class="t">And I found, like, looking directly at these traces, not just at statistical profiles, helps, like, catch those kinds of easy-to-fix 80/20 kind of bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5703" target="_blank">01:35:03.000</a></span> | <span class="t">You can also profile and trace memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5706" target="_blank">01:35:06.000</a></span> | <span class="t">And that's going to be, like, given that memory is this key constraint that's likely to be at least as useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5712" target="_blank">01:35:12.000</a></span> | <span class="t">And these two are examples from PyTorch, these -- the compute trace and the memory trace, both from PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5724" target="_blank">01:35:24.000</a></span> | <span class="t">You might not be using PyTorch, so you might have to fall back on generic, like, GPU profiling or CPU profiling or memory profiling and tracing tools if you're using, like, a custom inference solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5737" target="_blank">01:35:37.000</a></span> | <span class="t">Yeah, speaking of which, there's a bunch of specialized LLM inference libraries out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5744" target="_blank">01:35:44.000</a></span> | <span class="t">There's a nice blog post from Hamil Hussain about using these, and this is specifically for, like, batch size one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5750" target="_blank">01:35:50.000</a></span> | <span class="t">And so the VLM is -- people are, like, pretty -- that seems to be getting, like, the most community excitement and community contribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5760" target="_blank">01:36:00.000</a></span> | <span class="t">But Hamil suggests some reasons why you might be into NLC and C Translate, too, instead for doing your LLM inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5772" target="_blank">01:36:12.000</a></span> | <span class="t">So, actually -- so all that was about, like, just thinking in terms of, like, an individual workload, setting up an entire inference service, which is now something that maybe, like, somebody in the company sets up an inference service or an inference platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5789" target="_blank">01:36:29.000</a></span> | <span class="t">And then people can, you know, submit workloads to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5793" target="_blank">01:36:33.000</a></span> | <span class="t">That can be fairly challenging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5795" target="_blank">01:36:35.000</a></span> | <span class="t">Containerization for GPU accelerated workloads is less painful now than it used to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5801" target="_blank">01:36:41.000</a></span> | <span class="t">Like, NVIDIA Docker actually works in a way that it did not, like, early on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5808" target="_blank">01:36:48.000</a></span> | <span class="t">But containerization is, like, fundamentally, you know, more dubious for these workloads than a lot of other ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5818" target="_blank">01:36:58.000</a></span> | <span class="t">So one is that the, like, application layer of the container is probably where the weights are going to live.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5824" target="_blank">01:37:04.000</a></span> | <span class="t">Like, it doesn't live in the operating system yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5827" target="_blank">01:37:07.000</a></span> | <span class="t">It's not built into the Docker engine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5829" target="_blank">01:37:09.000</a></span> | <span class="t">So it's, like, that's up there, and that's, like, maybe half a terabyte in bad situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5835" target="_blank">01:37:15.000</a></span> | <span class="t">So now your container images are really large, and that's -- that can be pretty unpleasant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5841" target="_blank">01:37:21.000</a></span> | <span class="t">You could try and do the things that people do with databases and, like, move it into remote storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5846" target="_blank">01:37:26.000</a></span> | <span class="t">But, yeah, then the container is not as, like, unitary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5851" target="_blank">01:37:31.000</a></span> | <span class="t">And then the, like, the worst problem maybe is that the CUDA driver changes the choices that you need to make kind of at the, like, application level of, like, we were just talking about actually going one level down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5866" target="_blank">01:37:46.000</a></span> | <span class="t">But, like, which GPU is going to run on changes the point at which you switch from being memory bound to flops bound because that has a different, like, memory bandwidth to math bandwidth ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5877" target="_blank">01:37:57.000</a></span> | <span class="t">So things at the level of the NVIDIA of the GPU are entangled with things at the application layer, like choices of, like, batching strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5887" target="_blank">01:38:07.000</a></span> | <span class="t">So that's -- and a similar thing can be said for the CUDA driver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5890" target="_blank">01:38:10.000</a></span> | <span class="t">I was, like, naive about containers, and I was, like, why is the, like, CUDA driver showing up as 12.0, which is what I have installed in my host operating system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5901" target="_blank">01:38:21.000</a></span> | <span class="t">when I specifically downloaded the CUDA 11.2 container, and it's, like, containers can only virtualize so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5909" target="_blank">01:38:29.000</a></span> | <span class="t">And so that also changes, like, which kernels are available by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5915" target="_blank">01:38:35.000</a></span> | <span class="t">I guess what kernels would be a layer above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5920" target="_blank">01:38:40.000</a></span> | <span class="t">So there are features of CUDA drivers that change the, like, you know, how hugging face transformers or PyTorch will work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5927" target="_blank">01:38:47.000</a></span> | <span class="t">And so that will -- and that will also change decisions that are made at the application layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5931" target="_blank">01:38:51.000</a></span> | <span class="t">So when you're in these, like, super -- like, this is clearly a performance-limited regime that we're talking about here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5936" target="_blank">01:38:56.000</a></span> | <span class="t">Like, the inference is, like, extremely expensive, and we're, like, trying to maximize performance really aggressively, and that, like, starts to reveal the limitations of virtualization and containerization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5949" target="_blank">01:39:09.000</a></span> | <span class="t">So that doesn't mean it's impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5951" target="_blank">01:39:11.000</a></span> | <span class="t">It just means it's hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5952" target="_blank">01:39:12.000</a></span> | <span class="t">And, yeah, I don't know if you've ever worked with, like, you know, trying to set up a heterogeneous compute Kubernetes cluster where there's, like, some have GPUs and some have ARM and some have, like, x86.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5966" target="_blank">01:39:26.000</a></span> | <span class="t">You, like, this -- dealing with this requires a better ops engineer than if you can ignore that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5975" target="_blank">01:39:35.000</a></span> | <span class="t">Speaking of which, this, like, application serving can be and is, in fact, done with the, like, industry standard for container orchestration in Kubernetes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5984" target="_blank">01:39:44.000</a></span> | <span class="t">I would note that now the, like, Nvidia stuff is showing up here again and, like, all the problem -- like, the entangling -- and now Kubernetes is in between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5994" target="_blank">01:39:54.000</a></span> | <span class="t">So now there's, like, a lot of opportunity for crosstalk, a lot of opportunity for tiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=5999" target="_blank">01:39:59.000</a></span> | <span class="t">And so this is a hard mode version of that problem, which is not notorious for being easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6009" target="_blank">01:40:09.000</a></span> | <span class="t">So you might choose and need to do it by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6012" target="_blank">01:40:12.000</a></span> | <span class="t">There was a comment on r/moops that was, like, oh, yeah, we look -- you know, I was looking around to see what the opinions are on these tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6021" target="_blank">01:40:21.000</a></span> | <span class="t">And an opinion I saw in a couple places was, like, if you care about -- like, you've chosen to do serving of inference yourself and not do, like, just API calls, then maybe you should make mloops a core competency of your company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6038" target="_blank">01:40:38.000</a></span> | <span class="t">And you're going to want to do something more, like, building it out of the, like, Kubernetes-affiliated ecosystem of open-source tools or other open-source things, like Q-Ray to do -- to run Ray on Kubernetes for serving, or Selden Core to run that on -- again, on Kubernetes as your container orchestration layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6063" target="_blank">01:41:03.000</a></span> | <span class="t">But given how, like, painful that is and the fact that there are so many people who are trying to run a relatively similar workload, you might consider, like, either of two kind of tiers of managed services, either the, like, white-glove end-to-end kind of cloud provider approach, the oldest one, Amazon SageMaker, more recently, Vertex AI from Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6085" target="_blank">01:41:25.000</a></span> | <span class="t">I'm sure there's an Azure version that I'm forgetting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6089" target="_blank">01:41:29.000</a></span> | <span class="t">There are also, from some startups, a sort of more toolbox-y approach, a less end-to-end approach, in part because it's not, like, integrated all the way at the layer of the, like, actual hardware like the cloud providers are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6100" target="_blank">01:41:40.000</a></span> | <span class="t">So, Bento Cloud, Selden, and AnyScale, AnyScale being a managed version of Ray, Selden offers a managed version of Selden Core, Bento Cloud, managed version of Bento ML.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6113" target="_blank">01:41:53.000</a></span> | <span class="t">So, depending on where you want to -- maybe you have a really great, sweet deal with GCP, and so Vertex is the right choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6124" target="_blank">01:42:04.000</a></span> | <span class="t">But, yeah, I think from -- I have had limited experience with these things because I've tried to keep my life simple and happy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6131" target="_blank">01:42:11.000</a></span> | <span class="t">But if you're -- if you end up in this space -- I've seen some nice things and played around with RayServe and AnyScale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6141" target="_blank">01:42:21.000</a></span> | <span class="t">Ray's the tool that a lot of people use for cluster management for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6146" target="_blank">01:42:26.000</a></span> | <span class="t">So, a lot of the teams that train the foundation models use Ray.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6149" target="_blank">01:42:29.000</a></span> | <span class="t">And so, there's kind of, like, a natural competency for them, both in the open source library and AnyScale as a layer around that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6159" target="_blank">01:42:39.000</a></span> | <span class="t">So, maybe it'd be a good choice for inference as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6164" target="_blank">01:42:44.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6166" target="_blank">01:42:46.000</a></span> | <span class="t">So, rather than take questions, I think I'm going to do a five-minute break in which you can ask me questions up here while we all get a little stretch, maybe a little air.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N7lJY5IKVLE&t=6177" target="_blank">01:42:57.000</a></span> | <span class="t">And we'll come back for the rest of the OWL in five minutes.</span></div></div></body></html>