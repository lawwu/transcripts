<html><head><title>How Will DeepMind Handle the Intelligence Explosion? - Demis Hassabis</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>How Will DeepMind Handle the Intelligence Explosion? - Demis Hassabis</h2><a href="https://www.youtube.com/watch?v=BGxiufHVVd0" target="_blank"><img src="https://i.ytimg.com/vi_webp/BGxiufHVVd0/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>- Do you buy the model that once you have an AGI, you have a system that basically speeds up further AI research? Maybe not like an overnight sense, but over the course of months and years, you have much faster progress than you would have by the right side? - I think that's potentially possible.</p><p>I think it partly depends what we decide, we as society decide to use the first AGI, nascent AGI systems or even proto-AGI systems for. So even the current LLMs seem to be pretty good at coding. So, and we have systems like AlphaCode, we also got theory improving systems. So one could imagine combining these ideas together and making them a lot better.</p><p>And then I could imagine these systems being quite good at designing and helping us build future versions of themselves. But we also have to think about the safety implications of that, of course. - Yeah, I'm curious what you think about that. So, I mean, I'm not saying this is happening this year or anything, but eventually you'll be developing a model where during the process of development, you think, there's some chance that once this is fully developed, it will be capable of like an intelligence explosion, like dynamic.</p><p>What would have to be true of that model at that point where you're like, I've seen these specific evals, I've like understand it's internal thinking enough and like it's future thinking that I'm comfortable continuing development of the system. - Well, look, we need a lot more understanding of the systems than we do today before I would be even confident of even explaining to you what we would need to tick box there.</p><p>- Do you have a sense of what the converse answer would be? So what would have to be true where tomorrow morning you're like, we got to stop Gemini 2 training. Like what would specifically- - Yeah, I could imagine that. Like, and this is where, you know, things like the sandbox simulations, I would hope we're experimenting in a safe, secure environment.</p><p>And then, you know, something happens in it, very unexpected happens, a new unexpected capability or something that we didn't want, you know, explicitly told the system we didn't want that it did, but then lied about, you know, these are the kinds of things where one would want to then dig in carefully, pause, and then really get to the bottom of why it was doing those things before one continued.</p><p>There are many, many ideas that people have from much more stringent eval systems. I think we don't have a good enough evaluations and benchmarks for things like, can the system deceive you? Can it exfiltrate its own code? Sort of undesirable behaviors. And then there's, you know, ideas of actually using AI, maybe narrow AIs, so not general learning ones, but systems that are specialized for a domain to help us as the human scientists analyze and summarize what the more general system is doing, right?</p><p>So kind of narrow AI tools. I think that there's a lot of promise in creating hardened sandboxes or simulations so that they're hardened with cybersecurity arrangements around the simulation, both to keep the AI in, but also as cybersecurity to keep hackers out. And then you could experiment a lot more freely within that sandbox domain.</p></div></div></body></html>