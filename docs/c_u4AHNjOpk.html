<html><head><title>AlexNet and ImageNet Explained</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AlexNet and ImageNet Explained</h2><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk"><img src="https://i.ytimg.com/vi/c_u4AHNjOpk/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=66">1:6</a> Birth of Deep Learning<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=172">2:52</a> ImageNet<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=476">7:56</a> Lack of Readiness for Big Datasets<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=597">9:57</a> ImageNet Challenge (ILSVRC)<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=707">11:47</a> AlexNet<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1170">19:30</a> PYTORCH IMPLEMENTATION<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1195">19:55</a> Data Preprocessing<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1626">27:6</a> Class Prediction with AlexNet<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1910">31:50</a> Goldfish Results<br><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2067">34:27</a> Closing Notes<br><br><div style="text-align: left;"><a href="./c_u4AHNjOpk.html">Whisper Transcript</a> | <a href="./transcript_c_u4AHNjOpk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Today we're going to talk about one of the most important events in the history of deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=5" target="_blank">00:00:05.600</a></span> | <span class="t">we're going to talk about what happened at ImageNet 2012 and how that launched the sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=14" target="_blank">00:00:14.560</a></span> | <span class="t">deep learning rocket ship that we've been strapped to for the past decade. So in short we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">talk about ImageNet and where it came from why it was so important and then we're going to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=25" target="_blank">00:00:25.360</a></span> | <span class="t">look at very briefly going to have a look at convolutional neural networks and AlexNet which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">is the model that triggered the massive growth of deep learning and for me I like to back everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=39" target="_blank">00:00:39.200</a></span> | <span class="t">up with code so what we'll do is towards the end of the video we're going to go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=43" target="_blank">00:00:43.840</a></span> | <span class="t">the PyTorch implementation of AlexNet and we're actually going to test it on a small ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=52" target="_blank">00:00:52.480</a></span> | <span class="t">like data set and that'll be quite useful because we can see sort of image pre-processing steps and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=58" target="_blank">00:00:58.880</a></span> | <span class="t">also how to perform inference with a convolutional neural network like AlexNet. So let's jump</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=65" target="_blank">00:01:05.840</a></span> | <span class="t">straight into it. Today's deep learning revolution traces its roots back to the 30th of September</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=72" target="_blank">00:01:12.080</a></span> | <span class="t">2012. On this day a deep layered convolutional neural network won the ImageNet 2012 challenge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=80" target="_blank">00:01:20.960</a></span> | <span class="t">and this convolutional neural network didn't just win it completely destroyed the rest of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=87" target="_blank">00:01:27.120</a></span> | <span class="t">the competition. Now this model you might have guessed is called AlexNet and the simple fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=92" target="_blank">00:01:32.560</a></span> | <span class="t">that even use convolutional neural networks was very new. Convolutional neural networks had been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=97" target="_blank">00:01:37.920</a></span> | <span class="t">around for a while but using them had kind of been deemed impractical yet when AlexNet's results came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=106" target="_blank">00:01:46.640</a></span> | <span class="t">in it proved sort of unparalleled performance on what was seen as one of the hardest challenges of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=114" target="_blank">00:01:54.800</a></span> | <span class="t">the time for computer vision. So this event made AlexNet the first widely acknowledged successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=123" target="_blank">00:02:03.360</a></span> | <span class="t">implementation of deep learning and the sheer performance improvement that it showed caught</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=132" target="_blank">00:02:12.320</a></span> | <span class="t">people's attention. Until this point deep learning was unproven it was simply a nice idea that most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=139" target="_blank">00:02:19.680</a></span> | <span class="t">people just decided okay it's impractical we don't have enough data we don't have enough compute to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=145" target="_blank">00:02:25.760</a></span> | <span class="t">do anything like this but AlexNet showed that this was not the case and that deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=150" target="_blank">00:02:30.720</a></span> | <span class="t">was now practical. Yet this sort of surge of interest in deep learning was not solely you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=159" target="_blank">00:02:39.040</a></span> | <span class="t">know thanks to AlexNet. ImageNet also played a big part in this. The foundation of applied deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=165" target="_blank">00:02:45.760</a></span> | <span class="t">learning was set by ImageNet and built upon by AlexNet. So let's begin with ImageNet. Back in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=175" target="_blank">00:02:55.840</a></span> | <span class="t">2006 the world of computer vision was a lot different to how we know it now. It was pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=183" target="_blank">00:03:03.840</a></span> | <span class="t">underfunded it didn't really get that much attention yet there were a lot of researchers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=189" target="_blank">00:03:09.040</a></span> | <span class="t">around the world focused on building better models and the year after year they saw progress but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=195" target="_blank">00:03:15.040</a></span> | <span class="t">it was slow. In that same year a woman called Fei-Fei Li had just finished her computer vision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=203" target="_blank">00:03:23.760</a></span> | <span class="t">PhD at Caltech and had started working as a professor in computer science and had noticed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=212" target="_blank">00:03:32.080</a></span> | <span class="t">this sort of focus in the field of computer vision on the models and the subsequent lack of focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=221" target="_blank">00:03:41.760</a></span> | <span class="t">data and an idea came to Li that maybe a data set that was more representative of the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=230" target="_blank">00:03:50.240</a></span> | <span class="t">could improve the performance of the modelers being trained on it. Around the same time there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=236" target="_blank">00:03:56.160</a></span> | <span class="t">was another professor called Christiana Feldbaum and she was a co-developer of a data set from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=243" target="_blank">00:04:03.040</a></span> | <span class="t">1980s called WordNet. Now WordNet consisted of a pretty large number of English language terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=251" target="_blank">00:04:11.120</a></span> | <span class="t">organized into an ontological structure. So for example for the term Siberian Husky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=256" target="_blank">00:04:16.480</a></span> | <span class="t">that would be within a tree structure and above Siberian Husky you would have a working dog,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=263" target="_blank">00:04:23.040</a></span> | <span class="t">above working dog you would have dog, above dog you'd have canine, carnivore and so on. So there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=268" target="_blank">00:04:28.480</a></span> | <span class="t">like that tree structure of different terms and how they relate to each other. In 2007 Li and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=277" target="_blank">00:04:37.120</a></span> | <span class="t">Feldbaum met and Feldbaum discussed her work on or her idea at the time of adding just a reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=285" target="_blank">00:04:45.280</a></span> | <span class="t">image to each of the terms within WordNet. So the intention was not to create a image data set but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=293" target="_blank">00:04:53.600</a></span> | <span class="t">it was simply to add like a reference image so people could more easily understand what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=297" target="_blank">00:04:57.680</a></span> | <span class="t">particular term was about and this inspired an idea from Li that would kick start the world of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=304" target="_blank">00:05:04.080</a></span> | <span class="t">computer vision and deep learning. So soon after Li put together a team to build what would become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=311" target="_blank">00:05:11.120</a></span> | <span class="t">the largest labeled data set of images in the world called ImageNet. The idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=318" target="_blank">00:05:18.720</a></span> | <span class="t">behind ImageNet was that a large ontological based data set like WordNet but for images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=327" target="_blank">00:05:27.600</a></span> | <span class="t">could be the key behind building more advanced content based image retrieval, object recognition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=336" target="_blank">00:05:36.080</a></span> | <span class="t">scene recognition and better visual understanding in computer vision models. And just two years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=343" target="_blank">00:05:43.360</a></span> | <span class="t">later the first version of ImageNet was released with 12 million labeled images. These were all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=351" target="_blank">00:05:51.680</a></span> | <span class="t">structured and labeled in line with the WordNet ontology. Yet if we consider the sheer size of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=358" target="_blank">00:05:58.160</a></span> | <span class="t">that, the 12 million images, if one person had spent literally every single day labeling one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=366" target="_blank">00:06:06.960</a></span> | <span class="t">image per minute and did literally nothing else in that time, they didn't eat, they didn't sleep,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=372" target="_blank">00:06:12.800</a></span> | <span class="t">just labeled images, it would have taken them 22 years and 10 months. Which obviously is a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=381" target="_blank">00:06:21.920</a></span> | <span class="t">long time. There's just an insane number of images to be labeled here. So how did they do it? Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=390" target="_blank">00:06:30.160</a></span> | <span class="t">the team was not huge, they didn't have an infinite amount of money to pay other researchers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=395" target="_blank">00:06:35.520</a></span> | <span class="t">students to do this. So what they eventually settled on was a platform called Amazon's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=402" target="_blank">00:06:42.000</a></span> | <span class="t">Mechanical Turk. Mechanical Turk is a crowdsourcing platform where people from around the globe will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=409" target="_blank">00:06:49.920</a></span> | <span class="t">perform tasks such as labeling images for a set amount of money. Because there's just the insane</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=419" target="_blank">00:06:59.840</a></span> | <span class="t">scale of people around the world doing this at competitive prices, that made ImageNet possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=428" target="_blank">00:07:08.080</a></span> | <span class="t">with a few adjustments to the labeling process. Because in reality if you just have random people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=434" target="_blank">00:07:14.960</a></span> | <span class="t">around the world labeling your dataset, some of those people are going to try and take advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=439" target="_blank">00:07:19.840</a></span> | <span class="t">of that, maybe game the system. So you have to have some checks and balances in place against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=446" target="_blank">00:07:26.640</a></span> | <span class="t">that. So there's a little bit of system design in there. But that is how they built the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=456" target="_blank">00:07:36.480</a></span> | <span class="t">Now on its release, ImageNet was the largest publicly available labeled dataset of images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=464" target="_blank">00:07:44.800</a></span> | <span class="t">in the world. Yet there was very little interest in the dataset. Which seems pretty crazy when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=472" target="_blank">00:07:52.320</a></span> | <span class="t">look back on that in hindsight. Because now we know, okay we want more data for our models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=476" target="_blank">00:07:56.960</a></span> | <span class="t">make them better. But at the time things were different. So ImageNet came with these 12 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=486" target="_blank">00:08:06.080</a></span> | <span class="t">images distributed across 22,000 categories. And at the time there were the odd other image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=494" target="_blank">00:08:14.880</a></span> | <span class="t">datasets that used a similar sort of structure and idea. So for example the ESP dataset used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=501" target="_blank">00:08:21.200</a></span> | <span class="t">something called the ESP game. And people would play the ESP game and label images for the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=508" target="_blank">00:08:28.240</a></span> | <span class="t">Now reportedly they had way more images. But it wasn't publicly released. They only publicly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=514" target="_blank">00:08:34.880</a></span> | <span class="t">released 60,000 of those images. And a couple of years later there were a few papers that kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=524" target="_blank">00:08:44.080</a></span> | <span class="t">looked at the ESP game and ESP dataset and said okay it's probably not actually that useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=532" target="_blank">00:08:52.240</a></span> | <span class="t">Because you can kind of guess the right answer most of the time without even looking at the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=537" target="_blank">00:08:57.200</a></span> | <span class="t">So there were some questions around the usability of that dataset. So all this to say ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=544" target="_blank">00:09:04.880</a></span> | <span class="t">was by far the biggest, at least publicly available, and most sort of accurate dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=555" target="_blank">00:09:15.280</a></span> | <span class="t">for computer vision at the time. So the reason that there was very little interest in ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=561" target="_blank">00:09:21.680</a></span> | <span class="t">despite its huge size is that people just assumed that it could not work for their models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=571" target="_blank">00:09:31.040</a></span> | <span class="t">You have to think back then they were training models on much smaller datasets which had like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=579" target="_blank">00:09:39.440</a></span> | <span class="t">12 categories of images for example. And the models would struggle with that. So when ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=586" target="_blank">00:09:46.640</a></span> | <span class="t">comes along and it's like hey I have 22,000 categories here people are just like well I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=593" target="_blank">00:09:53.040</a></span> | <span class="t">can't deal with 12 so I'm not going to even try 22,000. That's crazy. So there was a lack of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=599" target="_blank">00:09:59.200</a></span> | <span class="t">interest in ImageNet at the time. It just wasn't really received that warmly. So the ImageNet team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=608" target="_blank">00:10:08.800</a></span> | <span class="t">decided to try and push it a bit more. So by the next year 2010 they had managed to organize a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=618" target="_blank">00:10:18.800</a></span> | <span class="t">challenge with the dataset, a classification challenge initially. And they grew into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=623" target="_blank">00:10:23.680</a></span> | <span class="t">different things over the years but initially it's just a classification challenge. So the ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=629" target="_blank">00:10:29.680</a></span> | <span class="t">large-scale visual recognition challenge was first hosted in 2010. And competitors had to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=637" target="_blank">00:10:37.680</a></span> | <span class="t">correctly classify images from 1,000 categories. So not a full set of terms in the ontology of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=648" target="_blank">00:10:48.960</a></span> | <span class="t">ImageNet but they had 1,000 categories instead. And whoever produced a model with the lowest error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=657" target="_blank">00:10:57.600</a></span> | <span class="t">rate won. And there were a few entrants. There was not a huge number of entrants. I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=662" target="_blank">00:11:02.560</a></span> | <span class="t">something like 4, 5, 6 entrants in 2010, 2011, 2012. Now eventually this challenge would become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=672" target="_blank">00:11:12.960</a></span> | <span class="t">the primary benchmark in computer vision progress. But it took some time. And that really started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=681" target="_blank">00:11:21.440</a></span> | <span class="t">in 2012. So 2012 was not like the previous years for ImageNet. On the 30th of September 2012 the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=690" target="_blank">00:11:30.560</a></span> | <span class="t">latest challenge results were released. And one of those results was a lot better than any of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=698" target="_blank">00:11:38.320</a></span> | <span class="t">other results. And it came from a model that most people thought was just not practical. And that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=706" target="_blank">00:11:46.080</a></span> | <span class="t">was AlexNet. AlexNet was the first model to score a sub 25% error rate. And that same year the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=714" target="_blank">00:11:54.320</a></span> | <span class="t">nearest competitor was 9.8 percentage points behind AlexNet. And they had done this with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=721" target="_blank">00:12:01.920</a></span> | <span class="t">deep layered convolutional neural network which at the time people were not really taking seriously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=727" target="_blank">00:12:07.360</a></span> | <span class="t">Now to understand AlexNet it's probably best we very quickly cover a little bit of what a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=732" target="_blank">00:12:12.080</a></span> | <span class="t">convolutional neural network actually is. So a convolutional neural network or CNN is a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=737" target="_blank">00:12:17.680</a></span> | <span class="t">network layer that has a special layer called a convolutional layer. And today these models are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=746" target="_blank">00:12:26.560</a></span> | <span class="t">known for computer vision. They have been for quite a long time sort of undisputed champions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=754" target="_blank">00:12:34.560</a></span> | <span class="t">of computer vision. And actually you know that has changed a little bit in literally like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=760" target="_blank">00:12:40.960</a></span> | <span class="t">past couple of years. But right now they're still pretty dominant. And unlike a lot of the models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=767" target="_blank">00:12:47.760</a></span> | <span class="t">back in 2012 and earlier these did not need too much sort of manual feature extraction or too much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=777" target="_blank">00:12:57.600</a></span> | <span class="t">image pre-processing before feeding data into the model. They could just kind of deal with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=784" target="_blank">00:13:04.080</a></span> | <span class="t">themselves. CNNs use several of these convolutional layers stacked on top of each other. And what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=790" target="_blank">00:13:10.640</a></span> | <span class="t">find is that the deeper the network is the more the better it can identify more sort of complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=798" target="_blank">00:13:18.400</a></span> | <span class="t">concepts or objects in images. So for example the first with the first few layers you're probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=806" target="_blank">00:13:26.560</a></span> | <span class="t">going to just kind of identify okay this is an edge, this is a circle maybe, this is this shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=813" target="_blank">00:13:33.520</a></span> | <span class="t">and maybe some textures. As the network gets deeper and you add more layers to it it starts to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=820" target="_blank">00:13:40.640</a></span> | <span class="t">abstract those features and identify more abstract ideas. So a deeper network will be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=829" target="_blank">00:13:49.520</a></span> | <span class="t">identify okay this is like a living thing and then you go you build a deeper network and it can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=836" target="_blank">00:13:56.000</a></span> | <span class="t">identify mammals and then it can identify dogs and then it can identify Siberian huskies. So as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=844" target="_blank">00:14:04.720</a></span> | <span class="t">model gets deeper its performance and its ability to identify more nuanced things gets better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=854" target="_blank">00:14:14.400</a></span> | <span class="t">So at the time these models were overlooked because essentially to train these to get good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=863" target="_blank">00:14:23.840</a></span> | <span class="t">performance from one of these models they need to be really deep. Which means that they have a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=868" target="_blank">00:14:28.640</a></span> | <span class="t">of parameters okay and it's the more parameters you have the longer it's going to take your model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=874" target="_blank">00:14:34.240</a></span> | <span class="t">to train if you can train it at all if it's if it's too big and doesn't even fit in the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=878" target="_blank">00:14:38.800</a></span> | <span class="t">on your computer. And also the more parameters it has the more data it has to see before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=886" target="_blank">00:14:46.080</a></span> | <span class="t">can produce sort of any any good performance of anything. As a result of this they were simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=893" target="_blank">00:14:53.040</a></span> | <span class="t">overlooked. Yet the authors of AlexNet won the ImageNet challenge in 2012 and it turns out that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=900" target="_blank">00:15:00.720</a></span> | <span class="t">they were the the right people in the right place at the right time. Several pieces came from from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=907" target="_blank">00:15:07.040</a></span> | <span class="t">different places to create this. ImageNet provided a massive amount of data needed to train one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=914" target="_blank">00:15:14.800</a></span> | <span class="t">these deep layered convolutional neural networks. A few years earlier in 2007 NVIDIA had released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=920" target="_blank">00:15:20.880</a></span> | <span class="t">CUDA which you may recognize the name of. So an API that allowed software access to the lower level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=930" target="_blank">00:15:30.560</a></span> | <span class="t">highly parallel processing abilities of CUDA enabled GPUs from NVIDIA. And GPU power in itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=938" target="_blank">00:15:38.080</a></span> | <span class="t">was reaching a point where this you know training these big models was becoming possible. Although</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=945" target="_blank">00:15:45.680</a></span> | <span class="t">it wasn't quite there yet at the time for a single GPU. So AlexNet was by no means small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=952" target="_blank">00:15:52.480</a></span> | <span class="t">and because of that the authors had to solve a lot of problems to get all this working. So AlexNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=957" target="_blank">00:15:57.760</a></span> | <span class="t">consisted of five convolutional layers followed by three fully connected linear layers. The final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=965" target="_blank">00:16:05.920</a></span> | <span class="t">layer to produce the 1000 classifications required by ImageNet was a 1000 node layer that used a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=975" target="_blank">00:16:15.840</a></span> | <span class="t">softmax activation function to create this probability distribution over all of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=981" target="_blank">00:16:21.040</a></span> | <span class="t">classes. Now a key conclusion from AlexNet was that the depth of the network was key to getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=989" target="_blank">00:16:29.280</a></span> | <span class="t">the performance that they got. And that depth as I mentioned before it creates a lot of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=996" target="_blank">00:16:36.560</a></span> | <span class="t">that need to be trained making training the model either impractically slow or just simply impossible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1003" target="_blank">00:16:43.040</a></span> | <span class="t">Or at least that was the case if you're going to train it on CPU. So they had to turn to GPUs but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1009" target="_blank">00:16:49.840</a></span> | <span class="t">at the time the high-end GPUs only had a memory of about three gigabytes which was not enough for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1019" target="_blank">00:16:59.600</a></span> | <span class="t">AlexNet. So to make it work they had to distribute AlexNet across two GPUs and they did this by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1026" target="_blank">00:17:06.640</a></span> | <span class="t">pretty much splitting the layers in two and having you know half of the network on one GPU half the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1035" target="_blank">00:17:15.040</a></span> | <span class="t">network on the other GPU and having a couple of connections between the layers. So they had a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1042" target="_blank">00:17:22.080</a></span> | <span class="t">couple of points where the information could be passed between those two halves and then at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1047" target="_blank">00:17:27.920</a></span> | <span class="t">end they came together into the final classification layer. Another important factor is that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1054" target="_blank">00:17:34.240</a></span> | <span class="t">swapped the more typical softmax and tanh activation functions of the time for a rectified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1061" target="_blank">00:17:41.600</a></span> | <span class="t">linear unit or radio activation function which again further improved the efficiency of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1069" target="_blank">00:17:49.200</a></span> | <span class="t">and also meant that they didn't require normalization that you would usually have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1075" target="_blank">00:17:55.440</a></span> | <span class="t">do if you had tanh or softmax. Because both of those activation functions over many layers you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1082" target="_blank">00:18:02.560</a></span> | <span class="t">can get what's called a saturation in your activations which means the activations in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1088" target="_blank">00:18:08.480</a></span> | <span class="t">neurons either kind of push towards the two limits of one of those activation functions. So for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1094" target="_blank">00:18:14.160</a></span> | <span class="t">example with softmax you'd end up all your activations be pushed towards one or zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1099" target="_blank">00:18:19.440</a></span> | <span class="t">Nonetheless they did use another type of normalization called local response normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1104" target="_blank">00:18:24.880</a></span> | <span class="t">but that's not really used anymore. Nonetheless for AlexNet that was still a critical component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1113" target="_blank">00:18:33.600</a></span> | <span class="t">Now another super important thing that is still used today that AlexNet introduced was the use of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1120" target="_blank">00:18:40.480</a></span> | <span class="t">overlapping in the pooling layers. Now pooling is already used in convolutional networks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1128" target="_blank">00:18:48.560</a></span> | <span class="t">it essentially just summarizes a window of information from one layer into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1135" target="_blank">00:18:55.120</a></span> | <span class="t">activation value in the next layer. Now overlapping pooling does the same thing but there's a there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1139" target="_blank">00:18:59.920</a></span> | <span class="t">an overlap in the window that gets passed along in the preceding layer. So there's always it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1146" target="_blank">00:19:06.080</a></span> | <span class="t">always sees a little bit of the previous window. Okay and they found that this reduces overfitting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1152" target="_blank">00:19:12.400</a></span> | <span class="t">of the model and improves the performance. So that is how they got AlexNet to work and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1160" target="_blank">00:19:20.720</a></span> | <span class="t">a few of the details behind actually you know how it worked and why it worked so well at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1167" target="_blank">00:19:27.040</a></span> | <span class="t">Now I think it's great to talk about all this but as I said at the start I think it's better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1172" target="_blank">00:19:32.240</a></span> | <span class="t">to go through everything or back everything up with a little bit of code. So we'll go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1177" target="_blank">00:19:37.520</a></span> | <span class="t">a notebook that you can find a link to the collab version of this notebook in the description below</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1185" target="_blank">00:19:45.120</a></span> | <span class="t">or if you're reading this on the Pinecone article page it will be in the resources section at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1190" target="_blank">00:19:50.960</a></span> | <span class="t">bottom and yeah we'll start going through that. Okay so we're going to start by downloading and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1198" target="_blank">00:19:58.160</a></span> | <span class="t">pre-processing our ImageNet dataset. So we're not using the actual ImageNet itself we're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1205" target="_blank">00:20:05.600</a></span> | <span class="t">another hosted version of ImageNet which is much smaller that we can find on Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1212" target="_blank">00:20:12.160</a></span> | <span class="t">So to use this we will need to pip install a few things. So pip install datasets which is where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1219" target="_blank">00:20:19.600</a></span> | <span class="t">we're going to how we're going to use the ImageNet dataset and later on we're also going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1225" target="_blank">00:20:25.680</a></span> | <span class="t">Torch and TorchVision so install those as well. So this is the dataset we're going to use so we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1232" target="_blank">00:20:32.320</a></span> | <span class="t">using this Macie Tiny ImageNet dataset. Now this is a validation split and that contains I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1240" target="_blank">00:20:40.720</a></span> | <span class="t">it's 10,000 we can see here yeah 10,000 labeled images. Okay and then we can see a single record</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1248" target="_blank">00:20:48.320</a></span> | <span class="t">in there so we have every image is sold as a pill image object and they have these labels so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1256" target="_blank">00:20:56.800</a></span> | <span class="t">one has labeled zero we don't necessarily know what that means right now but later on we'll see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1262" target="_blank">00:21:02.480</a></span> | <span class="t">how we can actually figure that out. So this one is referring to the actual training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1271" target="_blank">00:21:11.920</a></span> | <span class="t">dataset so the training split of this dataset does contain 100,000 of those labeled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1277" target="_blank">00:21:17.520</a></span> | <span class="t">images. Now we can check the type it's the the object and we can see okay so when we're in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1286" target="_blank">00:21:26.720</a></span> | <span class="t">notebook like this we can just call this ImageNet and this is just how we we show that in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1293" target="_blank">00:21:33.760</a></span> | <span class="t">notebook so you can see it's a goldfish so we can probably guess that label zero actually means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1298" target="_blank">00:21:38.560</a></span> | <span class="t">goldfish. So there are a few pre-processing steps that we need to go through so we need to convert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1306" target="_blank">00:21:46.880</a></span> | <span class="t">all images into an RGB format so it will have three color channels. We need to resize all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1313" target="_blank">00:21:53.920</a></span> | <span class="t">images to fit the expected dimensions of AlexNet. We need to convert into a tensor for PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1322" target="_blank">00:22:02.080</a></span> | <span class="t">We need to normalize those values and stack so when we have multiple images we're going to stack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1328" target="_blank">00:22:08.800</a></span> | <span class="t">them all into a single tensor okay it's to create our batch. So we start with RGB AlexNet as I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1336" target="_blank">00:22:16.800</a></span> | <span class="t">mentioned assumes all images have three color channels red green and blue but there are many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1342" target="_blank">00:22:22.320</a></span> | <span class="t">other formats that are supported by pill objects so we'll see here that we have grayscale okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1348" target="_blank">00:22:28.960</a></span> | <span class="t">this is 201 this is a grayscale image because we have this L they're formats as well so need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1355" target="_blank">00:22:35.280</a></span> | <span class="t">aware of those and we can see it's I think it's an alligator yeah an alligator in grayscale okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1363" target="_blank">00:22:43.680</a></span> | <span class="t">so we convert into red green and blue and we'll see okay it's still grayscale that's that's fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1370" target="_blank">00:22:50.560</a></span> | <span class="t">it will still be shown as grayscale but in reality this only has one color channel which is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1377" target="_blank">00:22:57.840</a></span> | <span class="t">just like a brightness channel whereas this now has three color channels red green and blue but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1383" target="_blank">00:23:03.120</a></span> | <span class="t">they're all of equal values across across those three channels so actually still shows as being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1388" target="_blank">00:23:08.480</a></span> | <span class="t">grayscale even though it is in a RGB format. This is how we handle the RGB part but we also need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1397" target="_blank">00:23:17.120</a></span> | <span class="t">resize the image to fit the expected dimensionality for AlexNet. So for AlexNet and for a lot of other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1407" target="_blank">00:23:27.840</a></span> | <span class="t">computer vision models the height and width of the the input images is expected to be at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1415" target="_blank">00:23:35.840</a></span> | <span class="t">224 pixels so we need to do that we can by using this so we're going to first we resize the images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1426" target="_blank">00:23:46.080</a></span> | <span class="t">because these are very small images and they're not necessarily all going to be the square that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1430" target="_blank">00:23:50.400</a></span> | <span class="t">we need the 224 by 224 so we resize them to be bigger and then we use this center crop to crop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1439" target="_blank">00:23:59.040</a></span> | <span class="t">out any edges and make sure that is now a square image of that dimensionality and yeah we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1447" target="_blank">00:24:07.200</a></span> | <span class="t">that using this transforms function from torch vision which is a very good way of pre-processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1455" target="_blank">00:24:15.920</a></span> | <span class="t">your your image data has a lot of functions and we'll see we'll use a couple more of those very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1460" target="_blank">00:24:20.960</a></span> | <span class="t">soon. So if we have a look at our first image the goldfish image you see it's now a bit bigger and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1465" target="_blank">00:24:25.280</a></span> | <span class="t">we can also see it's kind of cropped some of it as well but we still get the you know we still get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1470" target="_blank">00:24:30.240</a></span> | <span class="t">the idea of what is in that image so if we compare that to this here we can kind of see its eye at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1476" target="_blank">00:24:36.240</a></span> | <span class="t">the front there and more of its head whereas here it's kind of almost chopped off. Now another thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">we need to do is normalize all the values in these in these images so RGB arrays tend to be in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1491" target="_blank">00:24:51.840</a></span> | <span class="t">range of 0 up to 255 and we need them to be in the range of 0 to 1 and we need to normalize them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1500" target="_blank">00:25:00.240</a></span> | <span class="t">using these values that you see here so this mean of 0.4 and so on and the standard deviation of 0.2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1507" target="_blank">00:25:07.360</a></span> | <span class="t">and so on this is specific to the AlexNet implementation from PyTorch so we go on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1515" target="_blank">00:25:15.200</a></span> | <span class="t">AlexNet PyTorch page you can go down and it here we go so the images have to be loaded into a range</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1522" target="_blank">00:25:22.400</a></span> | <span class="t">of 0 to 1 and normalize using the values I just showed you so that's why we're using those and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1528" target="_blank">00:25:28.480</a></span> | <span class="t">yeah so we create this process function and then we process our image through it and then we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1535" target="_blank">00:25:35.520</a></span> | <span class="t">check the size so the the final result here is going to be that normalized tensor that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1541" target="_blank">00:25:41.760</a></span> | <span class="t">and it's in the correct dimension it has the correct dimensionality that we need as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1546" target="_blank">00:25:46.160</a></span> | <span class="t">so yeah that that's perfect now we want to put all this together and we don't want to do it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1552" target="_blank">00:25:52.080</a></span> | <span class="t">every single image like this we're just going to put it all together for a mini batch of images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1557" target="_blank">00:25:57.360</a></span> | <span class="t">so we're going to go with the first 50 images because because they're all goldfish and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1562" target="_blank">00:26:02.960</a></span> | <span class="t">easily check the AlexNet's performance on that single single object so I'm going to redefine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1571" target="_blank">00:26:11.600</a></span> | <span class="t">that pre-processing pipeline using everything we've just done so we'll resize we crop it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1576" target="_blank">00:26:16.720</a></span> | <span class="t">to tensor we have to do this by the way because PyTorch is expecting a tensor object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1582" target="_blank">00:26:22.960</a></span> | <span class="t">and before we normalize it we it needs to be in that tensor format otherwise we're going to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1588" target="_blank">00:26:28.400</a></span> | <span class="t">an error and then yeah we normalize it so we go through every image in the first 50 images and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1596" target="_blank">00:26:36.720</a></span> | <span class="t">first convert any that are grayscale to RGB not RBG RGB and we pre-process them okay and just append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1608" target="_blank">00:26:48.640</a></span> | <span class="t">them to a list now that list we want to sack all those together into a single tensor so we do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1614" target="_blank">00:26:54.880</a></span> | <span class="t">here and we get this final mini batch of our images so we have a mini batch of 50 and we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1622" target="_blank">00:27:02.640</a></span> | <span class="t">those those images that you can see with the dimensionality here so with all that done we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1628" target="_blank">00:27:08.800</a></span> | <span class="t">now ready to move on to the inference step so the the prediction of the class label is for our images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1634" target="_blank">00:27:14.960</a></span> | <span class="t">with AlexNet so the first thing we're going to want to do is download the model which is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1642" target="_blank">00:27:22.000</a></span> | <span class="t">to be hosted by PyTorch so we can do that here so let me so you can see a bit better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1652" target="_blank">00:27:32.160</a></span> | <span class="t">we import Torch the PyTorch and we just do Torch upload okay PyTorch vision let's just see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1661" target="_blank">00:27:41.040</a></span> | <span class="t">version that we're using and then we have AlexNet and we're not going to train AlexNet it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1666" target="_blank">00:27:46.800</a></span> | <span class="t">take a bit of time so we're going to use the pre-trained model weights so this version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1672" target="_blank">00:27:52.800</a></span> | <span class="t">AlexNet has already been trained and then we just say it's evaluation mode for for our inference so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1679" target="_blank">00:27:59.760</a></span> | <span class="t">for the predictions we don't want to train it by default I think it is in train mode which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1685" target="_blank">00:28:05.280</a></span> | <span class="t">looks like this we want it in evaluation mode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1692" target="_blank">00:28:12.880</a></span> | <span class="t">and then we can see the model structure here as well so you can see AlexNet we have so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1700" target="_blank">00:28:20.240</a></span> | <span class="t">where we're creating the the image features so there's many of these convolutional layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1707" target="_blank">00:28:27.520</a></span> | <span class="t">followed by the radioactivation function followed by the max pooling layer and with each of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1714" target="_blank">00:28:34.480</a></span> | <span class="t">the model creates a more abstract tensor that represents the sort of information from that image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1724" target="_blank">00:28:44.320</a></span> | <span class="t">so you can sort of imagine here the the the abstraction so the feature that's been extracted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1729" target="_blank">00:28:49.680</a></span> | <span class="t">is like okay there's some straight edges here and some some curved edges here we go a little further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1735" target="_blank">00:28:55.760</a></span> | <span class="t">and this is like okay this is a this is an animal or this is a fish and then by the time we get to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1740" target="_blank">00:29:00.320</a></span> | <span class="t">here it's like okay this is a goldfish hopefully and then we move on to the classifier part so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1749" target="_blank">00:29:09.600</a></span> | <span class="t">classifier is these three layers so we have dropout this this dropout was added to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1755" target="_blank">00:29:15.760</a></span> | <span class="t">reduce the chance of overfitting and improve the ability of the model to generalize and yeah we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1764" target="_blank">00:29:24.320</a></span> | <span class="t">have these linear linear linear okay so these are the linear layers the fully connected linear layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1771" target="_blank">00:29:31.280</a></span> | <span class="t">that produce the final 1000 activations and the highest of these activations represents the class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1779" target="_blank">00:29:39.840</a></span> | <span class="t">that the model is predicting as being the the class that identifies the image I saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1788" target="_blank">00:29:48.000</a></span> | <span class="t">so that's the model we initialize it if we can it's better that we move the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1795" target="_blank">00:29:55.760</a></span> | <span class="t">over to either a cuda gpu if available or more recently we have the apple silicon chips</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1803" target="_blank">00:30:03.760</a></span> | <span class="t">so if you are on a mac with apple silicon you want to use mps okay so that's the case for me I have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1813" target="_blank">00:30:13.440</a></span> | <span class="t">I'm going to run all this on mps so we move the inputs to the device and we move the model to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1821" target="_blank">00:30:21.840</a></span> | <span class="t">to the device now when we move the model to the device it does this in place so we don't need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1826" target="_blank">00:30:26.320</a></span> | <span class="t">like we did here where it's inputs equals we just write this and then we run the model so we set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1834" target="_blank">00:30:34.320</a></span> | <span class="t">torch no grads so we don't need to calculate the gradients because we do that for training the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1838" target="_blank">00:30:38.480</a></span> | <span class="t">model we're just performing inference so we get our outputs we detach them from the model and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1845" target="_blank">00:30:45.840</a></span> | <span class="t">we can sort of see the shape so we have these 50 vectors of 1000 items so that's 50 activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1855" target="_blank">00:30:55.920</a></span> | <span class="t">across all of our 1000 classes and we can we can see those here okay now these are not normalized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1864" target="_blank">00:31:04.800</a></span> | <span class="t">so if we if you want to calculate the probability from this we use the softmax function so we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1872" target="_blank">00:31:12.240</a></span> | <span class="t">do that like this okay that that would map everything to a probability distribution and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1879" target="_blank">00:31:19.360</a></span> | <span class="t">you'll be able to get the probability of like say the top five classes for example but we don't we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1886" target="_blank">00:31:26.800</a></span> | <span class="t">don't necessarily need to do that for what we're doing here so we could actually skip this so up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1892" target="_blank">00:31:32.160</a></span> | <span class="t">here we are getting the output so we could skip this the probability part and just replace that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1898" target="_blank">00:31:38.080</a></span> | <span class="t">with output and we will get the same result for what we're doing here which is taking the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1902" target="_blank">00:31:42.560</a></span> | <span class="t">value or the index position of the maximum value out of those 1000 classes so here we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1911" target="_blank">00:31:51.760</a></span> | <span class="t">one okay now if you remember earlier on the labels that we had in this data set was zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1917" target="_blank">00:31:57.680</a></span> | <span class="t">for the for the goldfish and the reason these are different is because the data set actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1924" target="_blank">00:32:04.640</a></span> | <span class="t">uses a different set of labels so it's it's not actually the same but if we if we do this so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1933" target="_blank">00:32:13.040</a></span> | <span class="t">over here let me open this and show you so over here we have a text file where each class is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1945" target="_blank">00:32:25.280</a></span> | <span class="t">separated by a newline character so this is number zero a tench and number one is a goldfish right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1952" target="_blank">00:32:32.960</a></span> | <span class="t">so if we we get that information so number one we can see there's a lot of goldfish predictions here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1958" target="_blank">00:32:38.560</a></span> | <span class="t">which is a good sign we can import those those classes and we can create prediction labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1968" target="_blank">00:32:48.400</a></span> | <span class="t">by just splitting the response we get from this by newline characters and then what we do is if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1975" target="_blank">00:32:55.680</a></span> | <span class="t">you print out prediction labels one we get goldfish okay so the the text label for that prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1983" target="_blank">00:33:03.920</a></span> | <span class="t">and yeah so we have the first 50 images all of those are goldfish and we can we can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1993" target="_blank">00:33:13.360</a></span> | <span class="t">so i'm just printing out the last three you see they're all goldfish so we would expect everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=1999" target="_blank">00:33:19.360</a></span> | <span class="t">all these predictions to be goldfish if the model is performing well okay and yeah we see for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2005" target="_blank">00:33:25.760</a></span> | <span class="t">for the most part that is the case now if we calculate the performance or the accuracy here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2014" target="_blank">00:33:34.400</a></span> | <span class="t">we get 72 so that represents a top one error rate of 28 which beats the reported error rate of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2023" target="_blank">00:33:43.840</a></span> | <span class="t">AlexNet model in 2012 on the ImageNet challenge which was 37.5 for the for the top top one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2031" target="_blank">00:33:51.920</a></span> | <span class="t">however this is i will point out that this is just a single class this is a single label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2040" target="_blank">00:34:00.000</a></span> | <span class="t">goldfish right and the model will perform better on goldfish than other things okay so when we test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2047" target="_blank">00:34:07.760</a></span> | <span class="t">this across the if we test this across the whole data set one we have to map all of the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2052" target="_blank">00:34:12.960</a></span> | <span class="t">labels between the uh between the AlexNet model and the data set that we have here because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2057" target="_blank">00:34:17.920</a></span> | <span class="t">labels have kind of messed up uh so takes a bit of extra work but if we do that the performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2062" target="_blank">00:34:22.800</a></span> | <span class="t">will not be as good nonetheless i think that is a pretty good result so that's it for this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2069" target="_blank">00:34:29.680</a></span> | <span class="t">that's our overview of one of the most significant events in computer vision and deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2076" target="_blank">00:34:36.720</a></span> | <span class="t">the ImageNet challenge was hosted annually until 2017 by then 29 38 contestants had an error rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2086" target="_blank">00:34:46.160</a></span> | <span class="t">of less than five percent so the you know over those years the models the progress in computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2093" target="_blank">00:34:53.120</a></span> | <span class="t">vision just kind of went crazy AlexNet ended up being superseded by even more powerful convolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2099" target="_blank">00:34:59.360</a></span> | <span class="t">neural networks Microsoft Research Asia was the first other team to beat AlexNet and they did that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2107" target="_blank">00:35:07.360</a></span> | <span class="t">in 2015 and since then there have been many other sort of state-of-the-art convolution networks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2114" target="_blank">00:35:14.160</a></span> | <span class="t">have come and gone and even more recently there are the possibility of other networks coming in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2120" target="_blank">00:35:20.560</a></span> | <span class="t">such as transformer models and disrupting the dominance of convolution neural networks in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2126" target="_blank">00:35:26.880</a></span> | <span class="t">computer vision now i'll leave you with the final paragraph of the of the AlexNet paper because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2132" target="_blank">00:35:32.160</a></span> | <span class="t">almost seems like they saw the future of deep learning they noted that they did not use any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2140" target="_blank">00:35:40.240</a></span> | <span class="t">unsupervised pre-training even though they expect it will help and our results have improved as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2147" target="_blank">00:35:47.520</a></span> | <span class="t">make our network larger but we still have many orders of magnitude to go in order to map the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2153" target="_blank">00:35:53.840</a></span> | <span class="t">infrotemporal pathway of the human visual system so to match human level performance now we know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2161" target="_blank">00:36:01.040</a></span> | <span class="t">that unsupervised pre-training and ever greater models ever deeper models were really sort of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2167" target="_blank">00:36:07.760</a></span> | <span class="t">key to all the improvement gains that we've got in deep learning in the past decade so i hope that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=c_u4AHNjOpk&t=2174" target="_blank">00:36:14.400</a></span> | <span class="t">has been useful thank you very much for watching and i will see you again in the next one bye</span></div></div></body></html>