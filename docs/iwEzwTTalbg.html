<html><head><title>Variational Autoencoder - Model, ELBO, loss function and maths explained easily!</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Variational Autoencoder - Model, ELBO, loss function and maths explained easily!</h2><a href="https://www.youtube.com/watch?v=iwEzwTTalbg"><img src="https://i.ytimg.com/vi/iwEzwTTalbg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=41">0:41</a> Autoencoder<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=155">2:35</a> Variational Autoencoder<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=260">4:20</a> Latent Space<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=366">6:6</a> Math introduction<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=525">8:45</a> Model definition<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=720">12:0</a> ELBO<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=965">16:5</a> Maximizing the ELBO<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1189">19:49</a> Reparameterization Trick<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1361">22:41</a> Example network<br><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1435">23:55</a> Loss function<br><br><div style="text-align: left;"><a href="./iwEzwTTalbg.html">Whisper Transcript</a> | <a href="./transcript_iwEzwTTalbg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome to my video about the variational autoencoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=2" target="_blank">00:00:02.840</a></span> | <span class="t">In this video, I will be introducing the model, how it works, the architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=7" target="_blank">00:00:07.400</a></span> | <span class="t">and I will also be going into the maths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=10" target="_blank">00:00:10.360</a></span> | <span class="t">Why should you learn about the variational autoencoder?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=13" target="_blank">00:00:13.160</a></span> | <span class="t">Well, it's one of the building blocks of the stable diffusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=16" target="_blank">00:00:16.960</a></span> | <span class="t">and if you can understand the maths behind the variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=20" target="_blank">00:00:20.240</a></span> | <span class="t">you have covered more than 50% of the maths that you need for the stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=25" target="_blank">00:00:25.640</a></span> | <span class="t">At the same time, I will also try to simplify the math as much as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=29" target="_blank">00:00:29.400</a></span> | <span class="t">so that everyone with whatever background can follow the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=34" target="_blank">00:00:34.720</a></span> | <span class="t">Before we go into the details of variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=37" target="_blank">00:00:37.360</a></span> | <span class="t">we need to understand what is an autoencoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=41" target="_blank">00:00:41.880</a></span> | <span class="t">So the autoencoder is a model that is made of two smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=46" target="_blank">00:00:46.520</a></span> | <span class="t">The first is the encoder, the second the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=49" target="_blank">00:00:49.240</a></span> | <span class="t">and they are joined together by this bottleneck Z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=52" target="_blank">00:00:52.200</a></span> | <span class="t">The goal of the encoder is to take some input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=55" target="_blank">00:00:55.160</a></span> | <span class="t">and convert it into a lower dimensional representation, let's call it Z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=60" target="_blank">00:01:00.240</a></span> | <span class="t">and then if we take this lower dimensional representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=63" target="_blank">00:01:03.240</a></span> | <span class="t">and give it as input to the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=65" target="_blank">00:01:05.600</a></span> | <span class="t">we hope that the model will reproduce the original data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=69" target="_blank">00:01:09.440</a></span> | <span class="t">And why do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=70" target="_blank">00:01:10.960</a></span> | <span class="t">Because we want to compress the original data into a lower dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=79" target="_blank">00:01:19.040</a></span> | <span class="t">We can have an analogy with file compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=81" target="_blank">00:01:21.360</a></span> | <span class="t">For example, if you have a picture, let's call it zebra.jpg,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=84" target="_blank">00:01:24.400</a></span> | <span class="t">and you zip the file, you will end up with a zebra.zip file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=88" target="_blank">00:01:28.800</a></span> | <span class="t">And if you unzip the file or decompress the file,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=91" target="_blank">00:01:31.680</a></span> | <span class="t">you will end up with the same file again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=94" target="_blank">00:01:34.200</a></span> | <span class="t">The difference between the autoencoder and the compression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=97" target="_blank">00:01:37.200</a></span> | <span class="t">is that the autoencoder is a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=99" target="_blank">00:01:39.520</a></span> | <span class="t">and the neural network will not reproduce the exact original input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=104" target="_blank">00:01:44.920</a></span> | <span class="t">but will try to reproduce as much as possible of the original input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=109" target="_blank">00:01:49.960</a></span> | <span class="t">So what makes a good autoencoder?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=112" target="_blank">00:01:52.920</a></span> | <span class="t">The code should be as small as possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=114" target="_blank">00:01:54.760</a></span> | <span class="t">that is, the lower representation of the data should be as small as possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=118" target="_blank">00:01:58.680</a></span> | <span class="t">and the reconstructed input should be as close as possible to the original input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=123" target="_blank">00:02:03.080</a></span> | <span class="t">But what's the problem with autoencoders?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=125" target="_blank">00:02:05.640</a></span> | <span class="t">The problem with autoencoders is that the code learned by the model doesn't make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=131" target="_blank">00:02:11.280</a></span> | <span class="t">That is, the model just learns a mapping between input data and a code Z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=137" target="_blank">00:02:17.680</a></span> | <span class="t">but doesn't learn any semantic relationship between the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=141" target="_blank">00:02:21.040</a></span> | <span class="t">For example, if we watch at the code learned for the picture of the tomato,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=145" target="_blank">00:02:25.200</a></span> | <span class="t">it's very similar to the code learned for the picture of the zebra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=148" target="_blank">00:02:28.880</a></span> | <span class="t">or the cat is very similar to the code learned for the pizza, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=153" target="_blank">00:02:33.280</a></span> | <span class="t">So the model didn't capture any relationship between the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=156" target="_blank">00:02:36.840</a></span> | <span class="t">or any semantic relationship between the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=159" target="_blank">00:02:39.160</a></span> | <span class="t">And this is why we introduced the variational autoencoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=162" target="_blank">00:02:42.120</a></span> | <span class="t">In the variational autoencoder, we learn a latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=165" target="_blank">00:02:45.400</a></span> | <span class="t">So not a code, but a latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=167" target="_blank">00:02:47.800</a></span> | <span class="t">which represents a multivariate distribution over this data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=172" target="_blank">00:02:52.240</a></span> | <span class="t">And we hope that this multivariate distribution, so this latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=176" target="_blank">00:02:56.720</a></span> | <span class="t">captures also the semantic relationship between the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=179" target="_blank">00:02:59.800</a></span> | <span class="t">So for example, we hope that all the food pictures have a similar representation in this latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=185" target="_blank">00:03:05.440</a></span> | <span class="t">and also all the animals have a similar representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=188" target="_blank">00:03:08.160</a></span> | <span class="t">and all the cars and all the buildings, for example the stadium,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=191" target="_blank">00:03:11.120</a></span> | <span class="t">have a similar relationship with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=193" target="_blank">00:03:13.200</a></span> | <span class="t">And the most important thing that we want to do with this variational autoencoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=197" target="_blank">00:03:17.640</a></span> | <span class="t">is we want to be able to sample from this latent space to generate new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=203" target="_blank">00:03:23.440</a></span> | <span class="t">So what does it mean to sample the latent space?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=206" target="_blank">00:03:26.440</a></span> | <span class="t">Well, for example, when you use Python to generate a random number between 1 and 100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=212" target="_blank">00:03:32.280</a></span> | <span class="t">you're actually sampling from a random distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=215" target="_blank">00:03:35.560</a></span> | <span class="t">it's called uniform random distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=217" target="_blank">00:03:37.560</a></span> | <span class="t">because every number has equal probability of being chosen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=220" target="_blank">00:03:40.440</a></span> | <span class="t">We can sample from the latent space to generate a new random vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=224" target="_blank">00:03:44.280</a></span> | <span class="t">give it to the decoder and generate new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=226" target="_blank">00:03:46.880</a></span> | <span class="t">For example, if we sample from this latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=229" target="_blank">00:03:49.560</a></span> | <span class="t">which is the latent space of a variational autoencoder that was trained on food pictures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=234" target="_blank">00:03:54.600</a></span> | <span class="t">and we happen to sample something that was exactly in between of these three pictures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=240" target="_blank">00:04:00.040</a></span> | <span class="t">we hope to get something that also in its meaning is similar to these three pictures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=245" target="_blank">00:04:05.680</a></span> | <span class="t">So for example, in between the picture of egg, floor and basil leaves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=250" target="_blank">00:04:10.160</a></span> | <span class="t">we hope to find pasta with basil, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=253" target="_blank">00:04:13.280</a></span> | <span class="t">Which means that the model has captured somehow the relationship between the data it was trained upon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=258" target="_blank">00:04:18.960</a></span> | <span class="t">so it can generate new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=260" target="_blank">00:04:20.600</a></span> | <span class="t">Why is the space called latent space?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=263" target="_blank">00:04:23.560</a></span> | <span class="t">Because we model our data as it is coming from a variable X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=269" target="_blank">00:04:29.000</a></span> | <span class="t">so a random variable X that we can observe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=272" target="_blank">00:04:32.360</a></span> | <span class="t">but this variable X is conditioned on another random variable Z that is not visible to us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=279" target="_blank">00:04:39.080</a></span> | <span class="t">that is hidden.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=280" target="_blank">00:04:40.760</a></span> | <span class="t">So latent means hidden.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=282" target="_blank">00:04:42.760</a></span> | <span class="t">And we will model this hidden variable as a multivariate Gaussian with means and variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=290" target="_blank">00:04:50.040</a></span> | <span class="t">I know that this all sounds very abstract, so let me give you a more concrete example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=295" target="_blank">00:04:55.400</a></span> | <span class="t">I will use the Plato's allegory of the cave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=298" target="_blank">00:04:58.680</a></span> | <span class="t">In the Plato's allegory of the cave,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=301" target="_blank">00:05:01.320</a></span> | <span class="t">we have some people who since the childhood are born and lived all their life in this cave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=307" target="_blank">00:05:07.360</a></span> | <span class="t">We are talking about these people here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=309" target="_blank">00:05:09.680</a></span> | <span class="t">And these people never left the cave, so they only stayed in this area of the cave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=314" target="_blank">00:05:14.880</a></span> | <span class="t">And they are chained, so they cannot leave.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=317" target="_blank">00:05:17.840</a></span> | <span class="t">These people, since childhood, have seen these pictures on the cave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=322" target="_blank">00:05:22.840</a></span> | <span class="t">that are projected from these 3D objects through this fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=327" target="_blank">00:05:27.320</a></span> | <span class="t">So they are the shadow of these 3D objects here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=330" target="_blank">00:05:30.080</a></span> | <span class="t">But these people, they don't know that these pictures actually are casted from these 3D objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=335" target="_blank">00:05:35.680</a></span> | <span class="t">They don't know that they are shadows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=337" target="_blank">00:05:37.440</a></span> | <span class="t">For them, the horse is something black that moves like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=340" target="_blank">00:05:40.480</a></span> | <span class="t">The bird is something black that moves like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=343" target="_blank">00:05:43.360</a></span> | <span class="t">So we need to think that we are just like these people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=347" target="_blank">00:05:47.320</a></span> | <span class="t">So we have some data that we can observe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=349" target="_blank">00:05:49.960</a></span> | <span class="t">But this data actually comes from something that we cannot observe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=353" target="_blank">00:05:53.880</a></span> | <span class="t">that is of a higher representation of this data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=357" target="_blank">00:05:57.200</a></span> | <span class="t">abstract representation of this data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=359" target="_blank">00:05:59.320</a></span> | <span class="t">And we want to learn something about this abstract representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=363" target="_blank">00:06:03.000</a></span> | <span class="t">Before we go into the maths of variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=366" target="_blank">00:06:06.680</a></span> | <span class="t">let me give you a little pep talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=368" target="_blank">00:06:08.440</a></span> | <span class="t">Because the math is going to be a little hard to follow for some people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=372" target="_blank">00:06:12.080</a></span> | <span class="t">and some easy for other people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=373" target="_blank">00:06:13.920</a></span> | <span class="t">The point is, in order to understand the variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=377" target="_blank">00:06:17.160</a></span> | <span class="t">you need to understand the math behind it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=379" target="_blank">00:06:19.480</a></span> | <span class="t">Not only the numerical math, but also the concept.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=382" target="_blank">00:06:22.440</a></span> | <span class="t">So what I will try to do is to give you the necessary background to understand the math,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=386" target="_blank">00:06:26.560</a></span> | <span class="t">if you are interested in learning the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=389" target="_blank">00:06:29.360</a></span> | <span class="t">But at the same time, I will also try to convey some general information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=394" target="_blank">00:06:34.360</a></span> | <span class="t">some high-level representation of what is happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=397" target="_blank">00:06:37.000</a></span> | <span class="t">and why we are doing what we are doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=399" target="_blank">00:06:39.160</a></span> | <span class="t">Also, I believe that VA is the most important component of stable diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=403" target="_blank">00:06:43.400</a></span> | <span class="t">So concepts like ELBO that we will see in the following slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=406" target="_blank">00:06:46.840</a></span> | <span class="t">also come in stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=408" target="_blank">00:06:48.080</a></span> | <span class="t">So if you understand it here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=409" target="_blank">00:06:49.640</a></span> | <span class="t">it will make it easy for you to understand the stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">Plus, in 2023, I think you shouldn't be memorizing things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=416" target="_blank">00:06:56.600</a></span> | <span class="t">so just memorizing the architecture of models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=418" target="_blank">00:06:58.880</a></span> | <span class="t">because ChatGPT can do that faster and better than you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=421" target="_blank">00:07:01.520</a></span> | <span class="t">If you want to compete with a machine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=423" target="_blank">00:07:03.120</a></span> | <span class="t">you need to be human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=424" target="_blank">00:07:04.560</a></span> | <span class="t">You can't be a machine and compete with a machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=426" target="_blank">00:07:06.600</a></span> | <span class="t">I also believe that you should try to learn things not only out of curiosity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=431" target="_blank">00:07:11.640</a></span> | <span class="t">but because that's the true engine of innovation and creativity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=435" target="_blank">00:07:15.120</a></span> | <span class="t">And plus, math is fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=437" target="_blank">00:07:17.120</a></span> | <span class="t">So let's start by introducing some math concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=440" target="_blank">00:07:20.280</a></span> | <span class="t">that we will need in the following slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=443" target="_blank">00:07:23.560</a></span> | <span class="t">Don't be scared if you are not familiar with these concepts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=446" target="_blank">00:07:26.120</a></span> | <span class="t">because I will try to give a higher representation of what is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=450" target="_blank">00:07:30.640</a></span> | <span class="t">So even if you don't understand each step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=452" target="_blank">00:07:32.440</a></span> | <span class="t">you will still understand what is happening on a higher level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=456" target="_blank">00:07:36.480</a></span> | <span class="t">We need what is the expectation of a random variable, which is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=460" target="_blank">00:07:40.120</a></span> | <span class="t">We need the chain rule of probability, which is this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=463" target="_blank">00:07:43.200</a></span> | <span class="t">and the bias theorem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=464" target="_blank">00:07:44.800</a></span> | <span class="t">All of these three concepts are usually taught in a bachelor's class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=468" target="_blank">00:07:48.040</a></span> | <span class="t">so I hope that you are familiar with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=470" target="_blank">00:07:50.560</a></span> | <span class="t">And another concept that is not taught in a bachelor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=473" target="_blank">00:07:53.240</a></span> | <span class="t">but I will introduce now, is the Kullback-Leiber divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=476" target="_blank">00:07:56.720</a></span> | <span class="t">This is a very important concept in machine learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=479" target="_blank">00:07:59.200</a></span> | <span class="t">and it's a divergence measure that allows you to measure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=482" target="_blank">00:08:02.760</a></span> | <span class="t">the distance between two probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=486" target="_blank">00:08:06.560</a></span> | <span class="t">So given probability distribution P and Q,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=489" target="_blank">00:08:09.240</a></span> | <span class="t">the Kullback-Leiber divergence tells you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=491" target="_blank">00:08:11.720</a></span> | <span class="t">how far are these two probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=495" target="_blank">00:08:15.600</a></span> | <span class="t">But at the same time, this is not a distance metric,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=498" target="_blank">00:08:18.880</a></span> | <span class="t">because it's not symmetric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=500" target="_blank">00:08:20.120</a></span> | <span class="t">So when you have a distance metric, usually from, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=503" target="_blank">00:08:23.160</a></span> | <span class="t">the physical distance from point A to B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=505" target="_blank">00:08:25.720</a></span> | <span class="t">if A to B is one meter apart, then B to A is also one meter apart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=509" target="_blank">00:08:29.680</a></span> | <span class="t">But this doesn't happen with Kullback-Leiber divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=512" target="_blank">00:08:32.520</a></span> | <span class="t">For example, the divergence between P and Q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=514" target="_blank">00:08:34.880</a></span> | <span class="t">is not the divergence between Q and P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=517" target="_blank">00:08:37.880</a></span> | <span class="t">However, just like any distance metric,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=520" target="_blank">00:08:40.400</a></span> | <span class="t">it is always greater than or equal to zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=522" target="_blank">00:08:42.760</a></span> | <span class="t">and it's equal to zero if and only if the two distributions are same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=527" target="_blank">00:08:47.240</a></span> | <span class="t">We can now introduce our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=530" target="_blank">00:08:50.160</a></span> | <span class="t">Now, we saw before that we want to model our data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=533" target="_blank">00:08:53.600</a></span> | <span class="t">as coming from a random distribution that we call X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=536" target="_blank">00:08:56.720</a></span> | <span class="t">which is conditioned on a hidden variable or latent variable called Z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=541" target="_blank">00:09:01.120</a></span> | <span class="t">So we could also, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=542" target="_blank">00:09:02.880</a></span> | <span class="t">marginalize over the joint probability using this relationship here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=547" target="_blank">00:09:07.160</a></span> | <span class="t">The problem is this integral is intractable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=549" target="_blank">00:09:09.760</a></span> | <span class="t">because we need to integrate over all latent variable Z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=553" target="_blank">00:09:13.680</a></span> | <span class="t">And what does it mean to be intractable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=555" target="_blank">00:09:15.680</a></span> | <span class="t">It means that in theory, we can calculate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=558" target="_blank">00:09:18.440</a></span> | <span class="t">But in practice, it is so slow and so computationally expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=562" target="_blank">00:09:22.200</a></span> | <span class="t">that it's not worth it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=563" target="_blank">00:09:23.280</a></span> | <span class="t">So something intractable is like trying to guess your neighbor's Wi-Fi password.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=567" target="_blank">00:09:27.960</a></span> | <span class="t">In theory, you can do it by generating all possible passwords</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=570" target="_blank">00:09:30.800</a></span> | <span class="t">and try all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=571" target="_blank">00:09:31.800</a></span> | <span class="t">But in practice, it will take you thousands of years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=574" target="_blank">00:09:34.440</a></span> | <span class="t">So this relationship, we can also write it like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=576" target="_blank">00:09:36.960</a></span> | <span class="t">by using the chain rule of probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=579" target="_blank">00:09:39.200</a></span> | <span class="t">We are trying to find this, so our data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=582" target="_blank">00:09:42.400</a></span> | <span class="t">so a probability distribution over our data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=584" target="_blank">00:09:44.840</a></span> | <span class="t">but we need this ground truth of this, which we don't have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=588" target="_blank">00:09:48.400</a></span> | <span class="t">because this is the probability distribution over the latent space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=592" target="_blank">00:09:52.200</a></span> | <span class="t">given our data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=593" target="_blank">00:09:53.880</a></span> | <span class="t">But this is also something we want to learn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=595" target="_blank">00:09:55.520</a></span> | <span class="t">so we cannot use this relationship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=597" target="_blank">00:09:57.480</a></span> | <span class="t">So this looks like a chicken and egg problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=600" target="_blank">00:10:00.640</a></span> | <span class="t">because we are trying to find this using this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=605" target="_blank">00:10:05.240</a></span> | <span class="t">but we don't have this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=607" target="_blank">00:10:07.080</a></span> | <span class="t">And to find this, we need this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=610" target="_blank">00:10:10.040</a></span> | <span class="t">So how do we come out of it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=612" target="_blank">00:10:12.320</a></span> | <span class="t">Usually, when you cannot find something that you want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=615" target="_blank">00:10:15.920</a></span> | <span class="t">you try to approximate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=617" target="_blank">00:10:17.720</a></span> | <span class="t">And this is the road that we will follow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=620" target="_blank">00:10:20.560</a></span> | <span class="t">So, this is what we want to find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=623" target="_blank">00:10:23.160</a></span> | <span class="t">And we think that it's parametrized by some parameters theta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=629" target="_blank">00:10:29.480</a></span> | <span class="t">that we don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=631" target="_blank">00:10:31.320</a></span> | <span class="t">However, what if we could find something that is a surrogate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=634" target="_blank">00:10:34.680</a></span> | <span class="t">something that is approximation of this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=638" target="_blank">00:10:38.120</a></span> | <span class="t">that has its own parameters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=640" target="_blank">00:10:40.640</a></span> | <span class="t">Well, let's follow this road.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=642" target="_blank">00:10:42.080</a></span> | <span class="t">So let's do some maths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=645" target="_blank">00:10:45.200</a></span> | <span class="t">We start with the log likelihood of our data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=648" target="_blank">00:10:48.280</a></span> | <span class="t">which is equal to itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=650" target="_blank">00:10:50.400</a></span> | <span class="t">We can then multiply by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=652" target="_blank">00:10:52.160</a></span> | <span class="t">So why this quantity is one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=654" target="_blank">00:10:54.560</a></span> | <span class="t">Well, this is the integral over the domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=657" target="_blank">00:10:57.080</a></span> | <span class="t">of a probability distribution, which is always equal to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=660" target="_blank">00:11:00.360</a></span> | <span class="t">And we can bring this quantity inside the integral,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=663" target="_blank">00:11:03.760</a></span> | <span class="t">because it doesn't depend on the variable that is integrated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=667" target="_blank">00:11:07.280</a></span> | <span class="t">This is the definition of expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=671" target="_blank">00:11:11.240</a></span> | <span class="t">We can see that this integral is actually an expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=674" target="_blank">00:11:14.040</a></span> | <span class="t">And inside of this expectation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=677" target="_blank">00:11:17.920</a></span> | <span class="t">we can apply the equation given by the chain rule of probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=682" target="_blank">00:11:22.560</a></span> | <span class="t">We can multiply the numerator and the denominator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=685" target="_blank">00:11:25.160</a></span> | <span class="t">by the same quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=686" target="_blank">00:11:26.360</a></span> | <span class="t">which means actually multiplying by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=689" target="_blank">00:11:29.360</a></span> | <span class="t">Then we can split the expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=691" target="_blank">00:11:31.120</a></span> | <span class="t">because the log of a product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=693" target="_blank">00:11:33.360</a></span> | <span class="t">can be written as the sum of the logs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=695" target="_blank">00:11:35.800</a></span> | <span class="t">And then after writing the sum of the logs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=698" target="_blank">00:11:38.440</a></span> | <span class="t">we can split the expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=700" target="_blank">00:11:40.720</a></span> | <span class="t">And finally, we can see that the second expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=703" target="_blank">00:11:43.240</a></span> | <span class="t">is actually Kullback-Leibler divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=705" target="_blank">00:11:45.840</a></span> | <span class="t">And we know that it's always greater than or equal to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=708" target="_blank">00:11:48.880</a></span> | <span class="t">Now, let me expand this relationship that we have found.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=712" target="_blank">00:11:52.160</a></span> | <span class="t">That is, the log likelihood of our data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=715" target="_blank">00:11:55.160</a></span> | <span class="t">is equal to this quantity plus this KL divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=719" target="_blank">00:11:59.760</a></span> | <span class="t">We will call this quantity here the ELBO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=726" target="_blank">00:12:06.680</a></span> | <span class="t">which stands for evidence lower bound,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=729" target="_blank">00:12:09.400</a></span> | <span class="t">plus a KL divergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=731" target="_blank">00:12:11.040</a></span> | <span class="t">that is always greater than or equal to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=733" target="_blank">00:12:13.800</a></span> | <span class="t">Now, what can we infer from this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=737" target="_blank">00:12:17.480</a></span> | <span class="t">without knowing nothing about the quantities involved?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=740" target="_blank">00:12:20.800</a></span> | <span class="t">Okay, if you cannot see it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=743" target="_blank">00:12:23.680</a></span> | <span class="t">let me help you with a parallel example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=746" target="_blank">00:12:26.440</a></span> | <span class="t">Imagine you are an employee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=748" target="_blank">00:12:28.080</a></span> | <span class="t">You have a total compensation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=749" target="_blank">00:12:29.960</a></span> | <span class="t">which is your base salary plus a bonus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=752" target="_blank">00:12:32.720</a></span> | <span class="t">which is always greater than or equal to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=754" target="_blank">00:12:34.720</a></span> | <span class="t">Without knowing nothing about your base salary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=757" target="_blank">00:12:37.000</a></span> | <span class="t">or your total compensation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=758" target="_blank">00:12:38.720</a></span> | <span class="t">we can for sure deduce the following,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=761" target="_blank">00:12:41.480</a></span> | <span class="t">that your total compensation is always greater than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=764" target="_blank">00:12:44.680</a></span> | <span class="t">or equal to your base salary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=766" target="_blank">00:12:46.360</a></span> | <span class="t">Now, this expression here has the same structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=770" target="_blank">00:12:50.160</a></span> | <span class="t">as this expression here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=772" target="_blank">00:12:52.360</a></span> | <span class="t">So we can infer the same for the first expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=776" target="_blank">00:12:56.520</a></span> | <span class="t">That is, the first quantity is always greater than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=779" target="_blank">00:12:59.280</a></span> | <span class="t">or equal to the second quantity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=780" target="_blank">00:13:00.800</a></span> | <span class="t">without caring what happens to the third quantity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=783" target="_blank">00:13:03.240</a></span> | <span class="t">So this also means that this is a lower bound for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=787" target="_blank">00:13:07.280</a></span> | <span class="t">This also means that if we maximize this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=790" target="_blank">00:13:10.400</a></span> | <span class="t">this will also be maximized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=792" target="_blank">00:13:12.440</a></span> | <span class="t">Let's look at the ELBO in detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=794" target="_blank">00:13:14.720</a></span> | <span class="t">Now, we found before that this quantity here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=798" target="_blank">00:13:18.000</a></span> | <span class="t">is what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=799" target="_blank">00:13:19.280</a></span> | <span class="t">And if we maximize this quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=801" target="_blank">00:13:21.840</a></span> | <span class="t">we are going to automatically maximize this quantity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=804" target="_blank">00:13:24.920</a></span> | <span class="t">But this one can also be written like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=807" target="_blank">00:13:27.040</a></span> | <span class="t">by using the chain rule of probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=809" target="_blank">00:13:29.640</a></span> | <span class="t">And then we can split again the expectation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=812" target="_blank">00:13:32.560</a></span> | <span class="t">And then we can see that the second expectation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=814" target="_blank">00:13:34.600</a></span> | <span class="t">is a KL divergence itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=816" target="_blank">00:13:36.440</a></span> | <span class="t">In this case, it's a reverse KL divergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=818" target="_blank">00:13:38.920</a></span> | <span class="t">So it's not same as the one we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=821" target="_blank">00:13:41.400</a></span> | <span class="t">because the numerator doesn't match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=823" target="_blank">00:13:43.360</a></span> | <span class="t">with the probability distribution we see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=826" target="_blank">00:13:46.040</a></span> | <span class="t">So we put a minus sign here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=828" target="_blank">00:13:48.000</a></span> | <span class="t">Now, our goal is to maximize this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=831" target="_blank">00:13:51.920</a></span> | <span class="t">but we maximize this to actually maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=835" target="_blank">00:13:55.760</a></span> | <span class="t">this log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=837" target="_blank">00:13:57.920</a></span> | <span class="t">However, if we maximize this quantity here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=840" target="_blank">00:14:00.640</a></span> | <span class="t">so this is our ELBO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=842" target="_blank">00:14:02.560</a></span> | <span class="t">we are actually maximizing this sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=846" target="_blank">00:14:06.080</a></span> | <span class="t">or this difference, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=848" target="_blank">00:14:08.080</a></span> | <span class="t">And when you maximize this difference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=850" target="_blank">00:14:10.960</a></span> | <span class="t">you are maximizing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=853" target="_blank">00:14:13.160</a></span> | <span class="t">and at the same time, you are minimizing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=856" target="_blank">00:14:16.720</a></span> | <span class="t">If you cannot see it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=857" target="_blank">00:14:17.920</a></span> | <span class="t">let me give you a parallel example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=860" target="_blank">00:14:20.120</a></span> | <span class="t">By showing you like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=861" target="_blank">00:14:21.800</a></span> | <span class="t">imagine you have a company</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=863" target="_blank">00:14:23.640</a></span> | <span class="t">and your company has profit, revenue, and cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=867" target="_blank">00:14:27.160</a></span> | <span class="t">If you want to maximize your profit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=869" target="_blank">00:14:29.720</a></span> | <span class="t">what do you do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=870" target="_blank">00:14:30.560</a></span> | <span class="t">You maximize your revenue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=872" target="_blank">00:14:32.280</a></span> | <span class="t">and at the same time, you maximize your cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=874" target="_blank">00:14:34.880</a></span> | <span class="t">That's the only way, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=876" target="_blank">00:14:36.360</a></span> | <span class="t">So if we are maximizing the ELBO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=878" target="_blank">00:14:38.720</a></span> | <span class="t">we are actually maximizing this first quantity here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=881" target="_blank">00:14:41.920</a></span> | <span class="t">and at the same time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=882" target="_blank">00:14:42.960</a></span> | <span class="t">we are minimizing the second quantity we see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=886" target="_blank">00:14:46.240</a></span> | <span class="t">Now, let's look at what do these quantities mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=889" target="_blank">00:14:49.200</a></span> | <span class="t">And for that, I took this picture from a paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=892" target="_blank">00:14:52.800</a></span> | <span class="t">from Kingma and Welling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=894" target="_blank">00:14:54.040</a></span> | <span class="t">who are also the authors of the first paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=896" target="_blank">00:14:56.080</a></span> | <span class="t">about the variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=897" target="_blank">00:14:57.760</a></span> | <span class="t">and we can see that this is a log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=901" target="_blank">00:15:01.000</a></span> | <span class="t">It's a posterior of something that given Z</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=904" target="_blank">00:15:04.520</a></span> | <span class="t">gives us a probability distribution over X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=907" target="_blank">00:15:07.320</a></span> | <span class="t">So here we are talking about the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=910" target="_blank">00:15:10.360</a></span> | <span class="t">and here we have a KL divergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=912" target="_blank">00:15:12.560</a></span> | <span class="t">between two distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=914" target="_blank">00:15:14.240</a></span> | <span class="t">One is called the prior, so this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=916" target="_blank">00:15:16.440</a></span> | <span class="t">So this is what we want our Z space to look like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=920" target="_blank">00:15:20.600</a></span> | <span class="t">and as I said before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=921" target="_blank">00:15:21.760</a></span> | <span class="t">we want our Z space to be a multivariate Gaussian,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=925" target="_blank">00:15:25.000</a></span> | <span class="t">and this is the learned distribution by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=928" target="_blank">00:15:28.800</a></span> | <span class="t">So this KL, when we maximize the ELBO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=931" target="_blank">00:15:31.480</a></span> | <span class="t">since we are minimizing this quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=934" target="_blank">00:15:34.000</a></span> | <span class="t">the model actually is minimizing the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=936" target="_blank">00:15:36.960</a></span> | <span class="t">between what it is learning as the Z space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=939" target="_blank">00:15:39.960</a></span> | <span class="t">and what we want the Z space to look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=942" target="_blank">00:15:42.600</a></span> | <span class="t">So it is making the Z space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=944" target="_blank">00:15:44.200</a></span> | <span class="t">to look like a multivariate Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=946" target="_blank">00:15:46.920</a></span> | <span class="t">And because when we maximize the ELBO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=949" target="_blank">00:15:49.080</a></span> | <span class="t">we also maximize the first quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=951" target="_blank">00:15:51.360</a></span> | <span class="t">the model is also learning to maximize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=953" target="_blank">00:15:53.800</a></span> | <span class="t">the reconstruction quality of the sample X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=956" target="_blank">00:15:56.760</a></span> | <span class="t">given its latent representation Z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=959" target="_blank">00:15:59.560</a></span> | <span class="t">Now, the problem is when you maximize something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=962" target="_blank">00:16:02.960</a></span> | <span class="t">that has a stochastic quantity inside,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=966" target="_blank">00:16:06.840</a></span> | <span class="t">here we have a probability distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=969" target="_blank">00:16:09.160</a></span> | <span class="t">we need, okay, first of all,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=970" target="_blank">00:16:10.880</a></span> | <span class="t">let me describe how to maximize something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=973" target="_blank">00:16:13.680</a></span> | <span class="t">So when we want to maximize a function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=976" target="_blank">00:16:16.240</a></span> | <span class="t">we usually take the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=977" target="_blank">00:16:17.960</a></span> | <span class="t">and adjust the weights of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=979" target="_blank">00:16:19.840</a></span> | <span class="t">so they move along the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=982" target="_blank">00:16:22.080</a></span> | <span class="t">Or when we want to minimize a function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=984" target="_blank">00:16:24.360</a></span> | <span class="t">we take the gradient and adjust the weights of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=986" target="_blank">00:16:26.880</a></span> | <span class="t">so they move against the gradient direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=989" target="_blank">00:16:29.560</a></span> | <span class="t">And this is also what happens when we train our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=992" target="_blank">00:16:32.720</a></span> | <span class="t">For example, imagine we have a function that is convex,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=997" target="_blank">00:16:37.080</a></span> | <span class="t">so it looks like, let's say, a ball,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=998" target="_blank">00:16:38.840</a></span> | <span class="t">and our minimum is here, our initial weights are here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1003" target="_blank">00:16:43.880</a></span> | <span class="t">so we evaluate our gradient in this point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1007" target="_blank">00:16:47.080</a></span> | <span class="t">and the gradient always points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1008" target="_blank">00:16:48.560</a></span> | <span class="t">to where the direction of growth of the function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1012" target="_blank">00:16:52.600</a></span> | <span class="t">so the function is growing in this direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1014" target="_blank">00:16:54.760</a></span> | <span class="t">and we move against the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1016" target="_blank">00:16:56.720</a></span> | <span class="t">if we want to minimize the function, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1018" target="_blank">00:16:58.840</a></span> | <span class="t">And the problem is we are not calculating the true gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1023" target="_blank">00:17:03.840</a></span> | <span class="t">when we run our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1024" target="_blank">00:17:04.960</a></span> | <span class="t">We are actually calculating what is called,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1027" target="_blank">00:17:07.420</a></span> | <span class="t">we are using stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1030" target="_blank">00:17:10.840</a></span> | <span class="t">Have you ever wondered why it's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1032" target="_blank">00:17:12.440</a></span> | <span class="t">stochastic gradient descent and not just gradient descent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1035" target="_blank">00:17:15.640</a></span> | <span class="t">Because actually, to minimize a function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1037" target="_blank">00:17:17.760</a></span> | <span class="t">you need to evaluate the function over all the data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1041" target="_blank">00:17:21.080</a></span> | <span class="t">so all the training data you have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1043" target="_blank">00:17:23.680</a></span> | <span class="t">not only on a single batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1046" target="_blank">00:17:26.200</a></span> | <span class="t">But by doing it on a single batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1048" target="_blank">00:17:28.960</a></span> | <span class="t">you get a distribution over the possible gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1052" target="_blank">00:17:32.760</a></span> | <span class="t">So for example, when we use stochastic gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1056" target="_blank">00:17:36.480</a></span> | <span class="t">and we evaluate the gradient of our loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1060" target="_blank">00:17:40.360</a></span> | <span class="t">we do not get the true gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1062" target="_blank">00:17:42.120</a></span> | <span class="t">We get a distribution over the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1064" target="_blank">00:17:44.840</a></span> | <span class="t">And someone proved that if you do it long enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1067" target="_blank">00:17:47.960</a></span> | <span class="t">so if you do it over the entire training set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1070" target="_blank">00:17:50.940</a></span> | <span class="t">so one epoch, it will actually converge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1073" target="_blank">00:17:53.600</a></span> | <span class="t">to the true gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1075" target="_blank">00:17:55.280</a></span> | <span class="t">Now, the fact that it is a stochastic gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1078" target="_blank">00:17:58.720</a></span> | <span class="t">it also means that the gradient that we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1081" target="_blank">00:18:01.120</a></span> | <span class="t">has a mean and a variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1083" target="_blank">00:18:03.160</a></span> | <span class="t">Now, the variance, in our case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1085" target="_blank">00:18:05.680</a></span> | <span class="t">in stochastic gradient descent, is small enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1088" target="_blank">00:18:08.840</a></span> | <span class="t">so that we can use stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1092" target="_blank">00:18:12.160</a></span> | <span class="t">The problem with this quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1094" target="_blank">00:18:14.400</a></span> | <span class="t">if we do the same job with this one, we get an estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1097" target="_blank">00:18:17.640</a></span> | <span class="t">So this is called estimating a quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1099" target="_blank">00:18:19.880</a></span> | <span class="t">a stochastic quantity, that has a high variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1102" target="_blank">00:18:22.920</a></span> | <span class="t">as shown in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1105" target="_blank">00:18:25.340</a></span> | <span class="t">So if we look at the paper by Kingma and Welling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1108" target="_blank">00:18:28.180</a></span> | <span class="t">they show that there is an estimator for the elbow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1112" target="_blank">00:18:32.580</a></span> | <span class="t">and this estimator, however, exhibits a very high variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1117" target="_blank">00:18:37.020</a></span> | <span class="t">So it means that, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1118" target="_blank">00:18:38.700</a></span> | <span class="t">imagine we are trying to minimize our function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1121" target="_blank">00:18:41.380</a></span> | <span class="t">If we use an estimator that has high variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1125" target="_blank">00:18:45.700</a></span> | <span class="t">suppose we are here and the minimum of the model is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1129" target="_blank">00:18:49.300</a></span> | <span class="t">If we are lucky, when we calculate the gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1134" target="_blank">00:18:54.220</a></span> | <span class="t">we will get the true gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1135" target="_blank">00:18:55.780</a></span> | <span class="t">and we move against the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1138" target="_blank">00:18:58.060</a></span> | <span class="t">However, if we are unlucky because it has high variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1141" target="_blank">00:19:01.620</a></span> | <span class="t">the model may return a very different gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1144" target="_blank">00:19:04.540</a></span> | <span class="t">than what we expect, for example, in this direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1146" target="_blank">00:19:06.900</a></span> | <span class="t">and then we will move to the opposite direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1149" target="_blank">00:19:09.180</a></span> | <span class="t">which is this one, so it will take us far from the minimum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1152" target="_blank">00:19:12.740</a></span> | <span class="t">and this is not what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1154" target="_blank">00:19:14.740</a></span> | <span class="t">So we cannot use an estimator that has high variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1158" target="_blank">00:19:18.060</a></span> | <span class="t">This estimator is, however, unbiased.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1160" target="_blank">00:19:20.420</a></span> | <span class="t">It means that if we do it many times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1162" target="_blank">00:19:22.700</a></span> | <span class="t">it will converge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1164" target="_blank">00:19:24.700</a></span> | <span class="t">But because of it being with a high variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1167" target="_blank">00:19:27.900</a></span> | <span class="t">we cannot use it in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1170" target="_blank">00:19:30.140</a></span> | <span class="t">Plus, how do we run backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1173" target="_blank">00:19:33.460</a></span> | <span class="t">on a quantity that is stochastic?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1175" target="_blank">00:19:35.820</a></span> | <span class="t">Because we need to sample from our z-space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1178" target="_blank">00:19:38.500</a></span> | <span class="t">to calculate the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1180" target="_blank">00:19:40.980</a></span> | <span class="t">So how can we calculate the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1183" target="_blank">00:19:43.780</a></span> | <span class="t">of the sampling operation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1185" target="_blank">00:19:45.380</a></span> | <span class="t">We cannot do that, PyTorch cannot do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1187" target="_blank">00:19:47.500</a></span> | <span class="t">So we need a new estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1190" target="_blank">00:19:50.820</a></span> | <span class="t">And the idea is that we want to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1193" target="_blank">00:19:53.740</a></span> | <span class="t">the source of randomness outside of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1196" target="_blank">00:19:56.860</a></span> | <span class="t">and we will call it reparameterization trick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1199" target="_blank">00:19:59.780</a></span> | <span class="t">So the reparameterization trick means basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1202" target="_blank">00:20:02.260</a></span> | <span class="t">that we take the stochastic component outside of z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1206" target="_blank">00:20:06.900</a></span> | <span class="t">we create a new variable, epsilon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1209" target="_blank">00:20:09.740</a></span> | <span class="t">that is our stochastic node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1211" target="_blank">00:20:11.700</a></span> | <span class="t">We sample from epsilon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1214" target="_blank">00:20:14.100</a></span> | <span class="t">combine it with the parameters learned by the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1217" target="_blank">00:20:17.700</a></span> | <span class="t">so mu and sigma squared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1219" target="_blank">00:20:19.420</a></span> | <span class="t">which is the mean and the sigma of our multivariate Gaussian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1223" target="_blank">00:20:23.220</a></span> | <span class="t">that we are trying to learn,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1224" target="_blank">00:20:24.420</a></span> | <span class="t">and then we will run backpropagation through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1227" target="_blank">00:20:27.540</a></span> | <span class="t">Let me show you with a picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1229" target="_blank">00:20:29.300</a></span> | <span class="t">This is a picture I took from another paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1233" target="_blank">00:20:33.140</a></span> | <span class="t">by Kingma and Welling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1234" target="_blank">00:20:34.300</a></span> | <span class="t">and we can see here that when we run backpropagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1237" target="_blank">00:20:37.940</a></span> | <span class="t">we calculate our loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1239" target="_blank">00:20:39.820</a></span> | <span class="t">We calculate the gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1241" target="_blank">00:20:41.580</a></span> | <span class="t">but before, this node here was random, was stochastic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1246" target="_blank">00:20:46.940</a></span> | <span class="t">so we couldn't run backpropagation through it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1249" target="_blank">00:20:49.500</a></span> | <span class="t">because, as I told you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1251" target="_blank">00:20:51.220</a></span> | <span class="t">we don't know how to calculate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1254" target="_blank">00:20:54.300</a></span> | <span class="t">of the sampling operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1255" target="_blank">00:20:55.820</a></span> | <span class="t">However, if we take the randomness outside of this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1260" target="_blank">00:21:00.820</a></span> | <span class="t">to another node that is outside of z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1263" target="_blank">00:21:03.180</a></span> | <span class="t">we can run backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1264" target="_blank">00:21:04.780</a></span> | <span class="t">and update the parameters of our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1267" target="_blank">00:21:07.820</a></span> | <span class="t">And then, of course, the backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1269" target="_blank">00:21:09.780</a></span> | <span class="t">will also calculate the gradient along this path,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1273" target="_blank">00:21:13.260</a></span> | <span class="t">but we will just discard them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1276" target="_blank">00:21:16.300</a></span> | <span class="t">because we don't care.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1277" target="_blank">00:21:17.220</a></span> | <span class="t">We will choose a random source that is fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1280" target="_blank">00:21:20.180</a></span> | <span class="t">We will use N01 or N0i,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1284" target="_blank">00:21:24.180</a></span> | <span class="t">in case we are using a multivariate Gaussian,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1287" target="_blank">00:21:27.300</a></span> | <span class="t">and so now we can actually calculate the backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1290" target="_blank">00:21:30.060</a></span> | <span class="t">Plus, this estimator that we found has lower variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1293" target="_blank">00:21:33.980</a></span> | <span class="t">So we found this estimator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1295" target="_blank">00:21:35.900</a></span> | <span class="t">in which we replaced the stochastic quantity here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1298" target="_blank">00:21:38.780</a></span> | <span class="t">so Q of V of Z given X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1301" target="_blank">00:21:41.100</a></span> | <span class="t">conditioned on X, with this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1303" target="_blank">00:21:43.260</a></span> | <span class="t">which is actually coming from our noise source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1306" target="_blank">00:21:46.020</a></span> | <span class="t">which is epsilon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1307" target="_blank">00:21:47.740</a></span> | <span class="t">We combine it with the parameters learned from the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1311" target="_blank">00:21:51.900</a></span> | <span class="t">through this transformation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1313" target="_blank">00:21:53.900</a></span> | <span class="t">and then this is our new estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1316" target="_blank">00:21:56.980</a></span> | <span class="t">This is also called the Monte Carlo estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1320" target="_blank">00:22:00.300</a></span> | <span class="t">We also can prove that this new estimator is unbiased.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1323" target="_blank">00:22:03.380</a></span> | <span class="t">It means that if we run it many times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1326" target="_blank">00:22:06.060</a></span> | <span class="t">it will actually converge to the true gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1328" target="_blank">00:22:08.980</a></span> | <span class="t">So we can do that like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1330" target="_blank">00:22:10.580</a></span> | <span class="t">So if we take the gradient of this estimator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1333" target="_blank">00:22:13.580</a></span> | <span class="t">and we do it many times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1335" target="_blank">00:22:15.100</a></span> | <span class="t">so on average,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1336" target="_blank">00:22:16.540</a></span> | <span class="t">we can see that we can write this quantity here like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1340" target="_blank">00:22:20.140</a></span> | <span class="t">because this is our estimator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1342" target="_blank">00:22:22.500</a></span> | <span class="t">And then, because this gradient operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1345" target="_blank">00:22:25.900</a></span> | <span class="t">doesn't depend on the parameters of this estimation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1348" target="_blank">00:22:28.980</a></span> | <span class="t">we can take out this gradient operator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1353" target="_blank">00:22:33.260</a></span> | <span class="t">and then we can write this quantity inside this one here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1357" target="_blank">00:22:37.420</a></span> | <span class="t">as our original ELBO reparameterized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1360" target="_blank">00:22:40.740</a></span> | <span class="t">over the noise source epsilon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1362" target="_blank">00:22:42.780</a></span> | <span class="t">I want to recap what we have said so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1364" target="_blank">00:22:44.980</a></span> | <span class="t">So we found something called ELBO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1368" target="_blank">00:22:48.820</a></span> | <span class="t">that if we maximize it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1371" target="_blank">00:22:51.060</a></span> | <span class="t">we will actually learn the latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1373" target="_blank">00:22:53.900</a></span> | <span class="t">We also found an estimator for this ELBO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1377" target="_blank">00:22:57.020</a></span> | <span class="t">that allows the back propagation to be run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1380" target="_blank">00:23:00.140</a></span> | <span class="t">So now I want to combine all this knowledge together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1382" target="_blank">00:23:02.860</a></span> | <span class="t">to simulate what the network will actually do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1386" target="_blank">00:23:06.060</a></span> | <span class="t">So imagine we have a picture here of something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1390" target="_blank">00:23:10.700</a></span> | <span class="t">We run it through the encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1392" target="_blank">00:23:12.340</a></span> | <span class="t">and the encoder is something that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1393" target="_blank">00:23:13.860</a></span> | <span class="t">given our picture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1394" target="_blank">00:23:14.980</a></span> | <span class="t">gives us the latent representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1397" target="_blank">00:23:17.660</a></span> | <span class="t">Then what we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1398" target="_blank">00:23:18.740</a></span> | <span class="t">We sample from this noise source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1400" target="_blank">00:23:20.740</a></span> | <span class="t">which is outside the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1403" target="_blank">00:23:23.020</a></span> | <span class="t">so it's not inside of Z,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1404" target="_blank">00:23:24.620</a></span> | <span class="t">it's not inside of our neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1407" target="_blank">00:23:27.380</a></span> | <span class="t">And how to sample it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1408" target="_blank">00:23:28.540</a></span> | <span class="t">Well, there is a function in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1410" target="_blank">00:23:30.540</a></span> | <span class="t">called torch.rand_unlike,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1412" target="_blank">00:23:32.140</a></span> | <span class="t">because we will sampling from a distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1414" target="_blank">00:23:34.540</a></span> | <span class="t">with zero mean and unitary variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1417" target="_blank">00:23:37.220</a></span> | <span class="t">We will combine this sample,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1419" target="_blank">00:23:39.460</a></span> | <span class="t">this noisy sample,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1421" target="_blank">00:23:41.340</a></span> | <span class="t">with the parameters learned by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1424" target="_blank">00:23:44.300</a></span> | <span class="t">We will pass it through the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1426" target="_blank">00:23:46.500</a></span> | <span class="t">so given Z gives us back X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1430" target="_blank">00:23:50.500</a></span> | <span class="t">and then we will calculate the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1432" target="_blank">00:23:52.380</a></span> | <span class="t">between the reconstructed sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1435" target="_blank">00:23:55.140</a></span> | <span class="t">and the original sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1436" target="_blank">00:23:56.980</a></span> | <span class="t">I want to show you the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1439" target="_blank">00:23:59.140</a></span> | <span class="t">Don't be scared because it's a little long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1441" target="_blank">00:24:01.220</a></span> | <span class="t">and not easy to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1442" target="_blank">00:24:02.540</a></span> | <span class="t">if you don't have the necessary background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1444" target="_blank">00:24:04.100</a></span> | <span class="t">but I will try to simplify the meaning behind it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1447" target="_blank">00:24:07.060</a></span> | <span class="t">So the loss function is this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1451" target="_blank">00:24:11.180</a></span> | <span class="t">We can see it here and it's made of two components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1454" target="_blank">00:24:14.100</a></span> | <span class="t">As we saw before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1455" target="_blank">00:24:15.460</a></span> | <span class="t">the loss function is basically the elbow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1457" target="_blank">00:24:17.940</a></span> | <span class="t">so it's made of two components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1459" target="_blank">00:24:19.140</a></span> | <span class="t">One that tells how far our distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1462" target="_blank">00:24:22.020</a></span> | <span class="t">the learned distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1463" target="_blank">00:24:23.180</a></span> | <span class="t">is from what we want our distribution to look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1466" target="_blank">00:24:26.420</a></span> | <span class="t">And the second one is the quality of the reconstruction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1469" target="_blank">00:24:29.620</a></span> | <span class="t">which is this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1470" target="_blank">00:24:30.900</a></span> | <span class="t">So this one, we can just use the MSE loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1474" target="_blank">00:24:34.380</a></span> | <span class="t">that will basically evaluate pixel by pixel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1476" target="_blank">00:24:36.980</a></span> | <span class="t">how our image is different from the original image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1480" target="_blank">00:24:40.620</a></span> | <span class="t">so the reconstructed sample from the original sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1483" target="_blank">00:24:43.420</a></span> | <span class="t">And this quantity here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1485" target="_blank">00:24:45.820</a></span> | <span class="t">allows to calculate the KL divergence between the prior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1488" target="_blank">00:24:48.900</a></span> | <span class="t">so what we want our Z space to look like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1491" target="_blank">00:24:51.460</a></span> | <span class="t">and what is actually the Z space learned by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1496" target="_blank">00:24:56.020</a></span> | <span class="t">How to combine the noise sampled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1498" target="_blank">00:24:58.380</a></span> | <span class="t">from this noise source epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1501" target="_blank">00:25:01.060</a></span> | <span class="t">with the parameters learned by the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1503" target="_blank">00:25:03.100</a></span> | <span class="t">Well, because we chose the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1506" target="_blank">00:25:06.220</a></span> | <span class="t">the prior to be Gaussian,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1507" target="_blank">00:25:07.660</a></span> | <span class="t">and we also chose the noise to be Gaussian,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1510" target="_blank">00:25:10.460</a></span> | <span class="t">we can combine it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1512" target="_blank">00:25:12.900</a></span> | <span class="t">So the mu learned by the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1515" target="_blank">00:25:15.460</a></span> | <span class="t">plus the sigma learned by the model multiplied,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1518" target="_blank">00:25:18.820</a></span> | <span class="t">this is element-wise multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1522" target="_blank">00:25:22.340</a></span> | <span class="t">so not matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1523" target="_blank">00:25:23.780</a></span> | <span class="t">with the noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1525" target="_blank">00:25:25.300</a></span> | <span class="t">I also want you to notice that here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1528" target="_blank">00:25:28.700</a></span> | <span class="t">before we are not learning sigma square,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1531" target="_blank">00:25:31.420</a></span> | <span class="t">we are learning log of sigma square.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1534" target="_blank">00:25:34.220</a></span> | <span class="t">So we are not learning the variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1536" target="_blank">00:25:36.100</a></span> | <span class="t">but the log variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1538" target="_blank">00:25:38.060</a></span> | <span class="t">Why is this the case?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1539" target="_blank">00:25:39.580</a></span> | <span class="t">Well, if we learn sigma squared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1542" target="_blank">00:25:42.340</a></span> | <span class="t">we should force our model to learn a positive quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1545" target="_blank">00:25:45.980</a></span> | <span class="t">because sigma squared cannot be negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1548" target="_blank">00:25:48.180</a></span> | <span class="t">So we just pretend that we are learning log sigma squared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1551" target="_blank">00:25:51.420</a></span> | <span class="t">and then we want to transform into sigma squared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1553" target="_blank">00:25:53.940</a></span> | <span class="t">we just take the exponentiation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1556" target="_blank">00:25:56.300</a></span> | <span class="t">So I hope that you have some understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1559" target="_blank">00:25:59.380</a></span> | <span class="t">of what we did and why we did it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1562" target="_blank">00:26:02.260</a></span> | <span class="t">because my goal was to give you an insight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1564" target="_blank">00:26:04.540</a></span> | <span class="t">of what is the ELBO, so the ELBO is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1566" target="_blank">00:26:06.780</a></span> | <span class="t">that we can maximize to learn this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1569" target="_blank">00:26:09.460</a></span> | <span class="t">And I also wanted to show you the derivation of this ELBO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1572" target="_blank">00:26:12.980</a></span> | <span class="t">and all the problems involved in this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1574" target="_blank">00:26:14.780</a></span> | <span class="t">because this is the same problems that we will face</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1577" target="_blank">00:26:17.100</a></span> | <span class="t">when we will talk about the stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1579" target="_blank">00:26:19.060</a></span> | <span class="t">And this part here, I took from the original paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1582" target="_blank">00:26:22.140</a></span> | <span class="t">from Kingma and Welling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1583" target="_blank">00:26:23.580</a></span> | <span class="t">in which they saw the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1587" target="_blank">00:26:27.300</a></span> | <span class="t">If you're wondering why we got this particular formula here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1590" target="_blank">00:26:30.980</a></span> | <span class="t">for the KL divergence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1592" target="_blank">00:26:32.500</a></span> | <span class="t">I saw the derivation on Stack Exchange,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1595" target="_blank">00:26:35.340</a></span> | <span class="t">and I attach it here in case you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1597" target="_blank">00:26:37.180</a></span> | <span class="t">to have a better understanding on how to derive it yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1600" target="_blank">00:26:40.820</a></span> | <span class="t">Thank you for watching this video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1602" target="_blank">00:26:42.540</a></span> | <span class="t">and hopefully you learned everything there is to know about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1606" target="_blank">00:26:46.460</a></span> | <span class="t">at least from a theoretical point of view, about the VAE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1609" target="_blank">00:26:49.900</a></span> | <span class="t">In my next video, I want to also make a practical example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1613" target="_blank">00:26:53.460</a></span> | <span class="t">on how to code a VAE and how to train a network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1616" target="_blank">00:26:56.180</a></span> | <span class="t">and then how to sample from this latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1618" target="_blank">00:26:58.860</a></span> | <span class="t">If you watch this video and that video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1620" target="_blank">00:27:00.980</a></span> | <span class="t">I'm pretty sure that you will have a deep understanding of the VAE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1624" target="_blank">00:27:04.220</a></span> | <span class="t">and you will have a solid foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1626" target="_blank">00:27:06.420</a></span> | <span class="t">to then understand the stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=iwEzwTTalbg&t=1628" target="_blank">00:27:08.540</a></span> | <span class="t">Thank you for watching, and welcome back to my channel.</span></div></div></body></html>