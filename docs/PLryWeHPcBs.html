<html><head><title>Stanford CS224N -  NLP w/ DL | Winter 2021 | Lecture 5 - Recurrent Neural networks (RNNs)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N -  NLP w/ DL | Winter 2021 | Lecture 5 - Recurrent Neural networks (RNNs)</h2><a href="https://www.youtube.com/watch?v=PLryWeHPcBs"><img src="https://i.ytimg.com/vi/PLryWeHPcBs/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=165">2:45</a> Dependency parsers<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=342">5:42</a> Neural dependency parser<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=494">8:14</a> Distributed representations<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=679">11:19</a> Softmax classifier<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=891">14:51</a> Multiclass classifier<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1035">17:15</a> Dependency parser model<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1137">18:57</a> Whats happened since 2014<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1206">20:6</a> Context<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1341">22:21</a> Graphbased parsers<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1475">24:35</a> Regularization<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1679">27:59</a> L2 Regularization<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1818">30:18</a> Dropout<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2070">34:30</a> Backward Pass<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2181">36:21</a> Nonlinearity<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2607">43:27</a> Parameter initialization<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2751">45:51</a> Training<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2859">47:39</a> Learning Rate<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3000">50:0</a> Language Models<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3178">52:58</a> Engram Language Models<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3256">54:16</a> Markov Model<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3337">55:37</a> Foreground Language Model<br><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3523">58:43</a> Sparsity Problems<br><br><div style="text-align: left;"><a href="./PLryWeHPcBs.html">Whisper Transcript</a> | <a href="./transcript_PLryWeHPcBs.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So we're now starting in week three with lecture five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=5" target="_blank">00:00:05.000</a></span> | <span class="t">So unfortunately, in the last class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=15" target="_blank">00:00:15.320</a></span> | <span class="t">I guess I really got behind and went a bit slowly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=18" target="_blank">00:00:18.400</a></span> | <span class="t">I guess I must just enjoy talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=20" target="_blank">00:00:20.640</a></span> | <span class="t">about natural languages too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=22" target="_blank">00:00:22.640</a></span> | <span class="t">And so I never really got to the punchline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=24" target="_blank">00:00:24.940</a></span> | <span class="t">of showing how you could do good things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=26" target="_blank">00:00:26.720</a></span> | <span class="t">with the neural dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=28" target="_blank">00:00:28.600</a></span> | <span class="t">So today for the first piece,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=30" target="_blank">00:00:30.200</a></span> | <span class="t">I'll in some sense be finishing the content of last time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=33" target="_blank">00:00:33.640</a></span> | <span class="t">and talk about neural dependency parsing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=36" target="_blank">00:00:36.080</a></span> | <span class="t">which also gives us the opportunity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=38" target="_blank">00:00:38.620</a></span> | <span class="t">to introduce a simple feed forward neural net classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=43" target="_blank">00:00:43.620</a></span> | <span class="t">That will then lead into a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=46" target="_blank">00:00:46.040</a></span> | <span class="t">of just background things that you need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=48" target="_blank">00:00:48.640</a></span> | <span class="t">about neural networks content,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=50" target="_blank">00:00:50.360</a></span> | <span class="t">because the fact of the matter is there is a bunch of stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=52" target="_blank">00:00:52.660</a></span> | <span class="t">you need to know about neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=55" target="_blank">00:00:55.160</a></span> | <span class="t">And then after both of those things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=57" target="_blank">00:00:57.380</a></span> | <span class="t">I'll get into what's really meant to be the topic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=59" target="_blank">00:00:59.880</a></span> | <span class="t">of today's lecture, which is looking at language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=63" target="_blank">00:01:03.920</a></span> | <span class="t">and recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=66" target="_blank">00:01:06.160</a></span> | <span class="t">And that's then going to lead into those two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=70" target="_blank">00:01:10.000</a></span> | <span class="t">are important topics that we'll then be talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=74" target="_blank">00:01:14.020</a></span> | <span class="t">really for the whole of next week as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=76" target="_blank">00:01:16.920</a></span> | <span class="t">So there's a couple of reminders before we get underway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=79" target="_blank">00:01:19.640</a></span> | <span class="t">The first is that you should have handed in assignment two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=83" target="_blank">00:01:23.200</a></span> | <span class="t">before you joined this class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=85" target="_blank">00:01:25.860</a></span> | <span class="t">And in turn, assignment three is out today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=88" target="_blank">00:01:28.920</a></span> | <span class="t">And it's an assignment where you're going to build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=93" target="_blank">00:01:33.060</a></span> | <span class="t">essentially the neural dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=96" target="_blank">00:01:36.080</a></span> | <span class="t">that I'm just about to present in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=98" target="_blank">00:01:38.840</a></span> | <span class="t">So part of the role of this assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=100" target="_blank">00:01:40.960</a></span> | <span class="t">is actually to get you up to speed with PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=104" target="_blank">00:01:44.120</a></span> | <span class="t">So this assignment is highly scaffolded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=108" target="_blank">00:01:48.040</a></span> | <span class="t">with lots of comments and hints about what to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=111" target="_blank">00:01:51.120</a></span> | <span class="t">And so the hope is that by the time you come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=114" target="_blank">00:01:54.120</a></span> | <span class="t">to the end of it, you'll feel fairly familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=117" target="_blank">00:01:57.280</a></span> | <span class="t">and comfortable with PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=119" target="_blank">00:01:59.640</a></span> | <span class="t">Don't forget there was also a tutorial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=121" target="_blank">00:02:01.280</a></span> | <span class="t">on PyTorch last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=122" target="_blank">00:02:02.760</a></span> | <span class="t">If you didn't catch that at the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=124" target="_blank">00:02:04.560</a></span> | <span class="t">you might want to go back and look at the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=127" target="_blank">00:02:07.040</a></span> | <span class="t">Another thing to mention about the assignments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=131" target="_blank">00:02:11.000</a></span> | <span class="t">is that assignment three is the last assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=134" target="_blank">00:02:14.680</a></span> | <span class="t">where our great team of TAs are happy to look at your code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=139" target="_blank">00:02:19.120</a></span> | <span class="t">and sort out your bugs for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=141" target="_blank">00:02:21.560</a></span> | <span class="t">So maybe take advantage of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=143" target="_blank">00:02:23.680</a></span> | <span class="t">but not too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=145" target="_blank">00:02:25.320</a></span> | <span class="t">But starting on assignment four for assignments four, five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=148" target="_blank">00:02:28.240</a></span> | <span class="t">and the final project,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=150" target="_blank">00:02:30.040</a></span> | <span class="t">the TAs are very happy to help in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=152" target="_blank">00:02:32.760</a></span> | <span class="t">but it's just not going to be their job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=154" target="_blank">00:02:34.720</a></span> | <span class="t">to be actually sorting out bugs for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=157" target="_blank">00:02:37.760</a></span> | <span class="t">You should be looking at your code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=159" target="_blank">00:02:39.600</a></span> | <span class="t">and discussing ideas and concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=161" target="_blank">00:02:41.720</a></span> | <span class="t">and reasons why things might not work with them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=164" target="_blank">00:02:44.120</a></span> | <span class="t">Okay, so if you remember where we were last time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=169" target="_blank">00:02:49.120</a></span> | <span class="t">I'd introduced this idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=170" target="_blank">00:02:50.640</a></span> | <span class="t">of transition-based dependency parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=173" target="_blank">00:02:53.880</a></span> | <span class="t">and that these were an efficient linear time method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=178" target="_blank">00:02:58.120</a></span> | <span class="t">for giving the syntactic structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=180" target="_blank">00:03:00.240</a></span> | <span class="t">of natural language text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=182" target="_blank">00:03:02.320</a></span> | <span class="t">And that they worked pretty well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=185" target="_blank">00:03:05.320</a></span> | <span class="t">before neural nets came along and took over NLP again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=188" target="_blank">00:03:08.760</a></span> | <span class="t">but they had some disadvantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=191" target="_blank">00:03:11.120</a></span> | <span class="t">And their biggest disadvantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=193" target="_blank">00:03:13.160</a></span> | <span class="t">is that like most machine learning models of that time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=196" target="_blank">00:03:16.820</a></span> | <span class="t">they worked with indicator features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=199" target="_blank">00:03:19.380</a></span> | <span class="t">So that means that you are specifying some condition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">and then checking whether it was true of a configuration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=207" target="_blank">00:03:27.200</a></span> | <span class="t">So something like the word on the top of the stack is good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=210" target="_blank">00:03:30.360</a></span> | <span class="t">and it's part of speech is adjective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=212" target="_blank">00:03:32.640</a></span> | <span class="t">or the next word coming up is a personal pronoun,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=216" target="_blank">00:03:36.800</a></span> | <span class="t">that those are conditions that would be features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=219" target="_blank">00:03:39.480</a></span> | <span class="t">in a conventional transition-based dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=224" target="_blank">00:03:44.480</a></span> | <span class="t">And so what are the problems with doing that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=228" target="_blank">00:03:48.540</a></span> | <span class="t">Well, one problem is that those features are very sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=233" target="_blank">00:03:53.540</a></span> | <span class="t">A second problem is the features are incomplete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=237" target="_blank">00:03:57.440</a></span> | <span class="t">Well, what I mean by that is depending on what words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=242" target="_blank">00:04:02.440</a></span> | <span class="t">and configurations occurred in the training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=245" target="_blank">00:04:05.980</a></span> | <span class="t">there are certain features that will exist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=249" target="_blank">00:04:09.040</a></span> | <span class="t">because you sort of saw a certain word preceding a verb</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=252" target="_blank">00:04:12.520</a></span> | <span class="t">and certain features that just won't exist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=254" target="_blank">00:04:14.440</a></span> | <span class="t">because that word never occurred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=256" target="_blank">00:04:16.320</a></span> | <span class="t">before a verb in the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=258" target="_blank">00:04:18.580</a></span> | <span class="t">But perhaps the biggest problem and opportunity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=262" target="_blank">00:04:22.180</a></span> | <span class="t">for doing better with the neural dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=265" target="_blank">00:04:25.140</a></span> | <span class="t">is that it turns out that in a symbolic dependency parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=269" target="_blank">00:04:29.880</a></span> | <span class="t">computing all these features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=271" target="_blank">00:04:31.500</a></span> | <span class="t">just turns out to actually be pretty expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=274" target="_blank">00:04:34.000</a></span> | <span class="t">That although the actual transition system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=276" target="_blank">00:04:36.100</a></span> | <span class="t">that I showed last time is fast and efficient to run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=280" target="_blank">00:04:40.660</a></span> | <span class="t">you actually have to compute all of these features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=283" target="_blank">00:04:43.980</a></span> | <span class="t">And what you found was that about 95% of the parsing time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=288" target="_blank">00:04:48.380</a></span> | <span class="t">of one of these models was spent just computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=291" target="_blank">00:04:51.800</a></span> | <span class="t">all of the features of every configuration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=294" target="_blank">00:04:54.340</a></span> | <span class="t">So that suggests that perhaps we can do better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=298" target="_blank">00:04:58.940</a></span> | <span class="t">with a neural approach where we're going to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=301" target="_blank">00:05:01.300</a></span> | <span class="t">a dense and compact feature representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=304" target="_blank">00:05:04.100</a></span> | <span class="t">And so that's what I wanna go through now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=306" target="_blank">00:05:06.980</a></span> | <span class="t">So this time, we're still gonna have exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=310" target="_blank">00:05:10.420</a></span> | <span class="t">the same kind of configuration of a stack and a buffer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=315" target="_blank">00:05:15.020</a></span> | <span class="t">and running exactly the same transition sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=318" target="_blank">00:05:18.700</a></span> | <span class="t">except this time, rather than representing the configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=322" target="_blank">00:05:22.500</a></span> | <span class="t">of the stack and the buffer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=323" target="_blank">00:05:23.860</a></span> | <span class="t">by having several million symbolic features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=327" target="_blank">00:05:27.100</a></span> | <span class="t">we're instead going to summarize this configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=330" target="_blank">00:05:30.560</a></span> | <span class="t">as a dense vector of dimensionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=334" target="_blank">00:05:34.020</a></span> | <span class="t">perhaps approximately a thousand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=335" target="_blank">00:05:35.880</a></span> | <span class="t">And our neural approach is going to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=339" target="_blank">00:05:39.540</a></span> | <span class="t">this dense compact feature representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=342" target="_blank">00:05:42.740</a></span> | <span class="t">And so quite explicitly, what I'm gonna show you now briefly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=347" target="_blank">00:05:47.580</a></span> | <span class="t">and what you're going to implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=351" target="_blank">00:05:51.180</a></span> | <span class="t">is essentially the neural dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=354" target="_blank">00:05:54.020</a></span> | <span class="t">that was developed by Danqi Chen in 2014.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=358" target="_blank">00:05:58.060</a></span> | <span class="t">And to skip to the advertisement right at the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=361" target="_blank">00:06:01.940</a></span> | <span class="t">as to how this works so well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=364" target="_blank">00:06:04.860</a></span> | <span class="t">these are the kind of results that you got from it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=368" target="_blank">00:06:08.000</a></span> | <span class="t">using the measures that I introduced at the last time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=370" target="_blank">00:06:10.980</a></span> | <span class="t">the unlabeled attachment score,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=372" target="_blank">00:06:12.860</a></span> | <span class="t">whether you attach dependencies correctly to the right word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=377" target="_blank">00:06:17.020</a></span> | <span class="t">and the labeled attachment score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=379" target="_blank">00:06:19.340</a></span> | <span class="t">as to whether you also get the type of grammatical relation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=382" target="_blank">00:06:22.540</a></span> | <span class="t">of that dependency correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=384" target="_blank">00:06:24.220</a></span> | <span class="t">And so essentially, this Chen and Manning parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=389" target="_blank">00:06:29.020</a></span> | <span class="t">gave a neural version of something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=392" target="_blank">00:06:32.580</a></span> | <span class="t">like a transition-based dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=395" target="_blank">00:06:35.340</a></span> | <span class="t">like Malt parser in yellow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=397" target="_blank">00:06:37.520</a></span> | <span class="t">And the interesting thing was that taking advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=401" target="_blank">00:06:41.260</a></span> | <span class="t">of a neural classifier in ways that I'm about to explain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=406" target="_blank">00:06:46.260</a></span> | <span class="t">that that could produce something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=408" target="_blank">00:06:48.020</a></span> | <span class="t">that was about 2% more accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=410" target="_blank">00:06:50.660</a></span> | <span class="t">than the symbolic dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=413" target="_blank">00:06:53.200</a></span> | <span class="t">And because of the fact that it's not doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=415" target="_blank">00:06:55.460</a></span> | <span class="t">all of the symbolic feature computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=418" target="_blank">00:06:58.400</a></span> | <span class="t">despite the fact that you might think at first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=420" target="_blank">00:07:00.760</a></span> | <span class="t">that there's a lot of real number math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=422" target="_blank">00:07:02.540</a></span> | <span class="t">and matrix vector multiplies in a neural dependency parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=427" target="_blank">00:07:07.140</a></span> | <span class="t">it actually ran noticeably faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=429" target="_blank">00:07:09.700</a></span> | <span class="t">than the symbolic dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=431" target="_blank">00:07:11.580</a></span> | <span class="t">because it didn't have all of the feature computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=436" target="_blank">00:07:16.100</a></span> | <span class="t">The other major approach to dependency parsing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=438" target="_blank">00:07:18.300</a></span> | <span class="t">that I'm also showing here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=439" target="_blank">00:07:19.920</a></span> | <span class="t">and I'll get back to at the end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=442" target="_blank">00:07:22.220</a></span> | <span class="t">is what's referred to as graph-based dependency parsing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=445" target="_blank">00:07:25.740</a></span> | <span class="t">And so that's a different approach to dependency parsing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=449" target="_blank">00:07:29.060</a></span> | <span class="t">And so these are two symbolic graph-based dependency parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=453" target="_blank">00:07:33.700</a></span> | <span class="t">and in the pre-neural world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=456" target="_blank">00:07:36.340</a></span> | <span class="t">they were somewhat more accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=458" target="_blank">00:07:38.700</a></span> | <span class="t">than the transition-based parsers as you could see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=461" target="_blank">00:07:41.900</a></span> | <span class="t">but on the other hand,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=463" target="_blank">00:07:43.420</a></span> | <span class="t">they were close to two orders of magnitude slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=466" target="_blank">00:07:46.980</a></span> | <span class="t">And so essentially with the Chern-Manning parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=469" target="_blank">00:07:49.780</a></span> | <span class="t">we were able to provide something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=471" target="_blank">00:07:51.700</a></span> | <span class="t">that was basically as accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=473" target="_blank">00:07:53.380</a></span> | <span class="t">as the best graph-based dependency parsers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=475" target="_blank">00:07:55.940</a></span> | <span class="t">which were the best dependency parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=478" target="_blank">00:07:58.300</a></span> | <span class="t">while operating about two orders of magnitude more quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=481" target="_blank">00:08:01.780</a></span> | <span class="t">So how did we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=484" target="_blank">00:08:04.160</a></span> | <span class="t">It was actually a very straightforward implementation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=489" target="_blank">00:08:09.160</a></span> | <span class="t">which is part of what makes it great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=491" target="_blank">00:08:11.800</a></span> | <span class="t">for doing for assignment three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=493" target="_blank">00:08:13.960</a></span> | <span class="t">But this is how we did it and we got wins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=497" target="_blank">00:08:17.660</a></span> | <span class="t">So the first win,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=498" target="_blank">00:08:18.960</a></span> | <span class="t">which is what we've already talked about extensively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=501" target="_blank">00:08:21.700</a></span> | <span class="t">starting in week one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=503" target="_blank">00:08:23.140</a></span> | <span class="t">is to make use of distributed representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=506" target="_blank">00:08:26.280</a></span> | <span class="t">So we represent each word as a word embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=509" target="_blank">00:08:29.580</a></span> | <span class="t">and you've had a lot of experience with that already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=513" target="_blank">00:08:33.140</a></span> | <span class="t">And so that means when words weren't seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=516" target="_blank">00:08:36.580</a></span> | <span class="t">in a particular configuration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=518" target="_blank">00:08:38.620</a></span> | <span class="t">we still know what they're like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=520" target="_blank">00:08:40.180</a></span> | <span class="t">because they'll be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=521" target="_blank">00:08:41.260</a></span> | <span class="t">will have seen similar words in the correct configuration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=525" target="_blank">00:08:45.180</a></span> | <span class="t">But we don't stop only with word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=530" target="_blank">00:08:50.460</a></span> | <span class="t">The other things that are central to our dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=533" target="_blank">00:08:53.860</a></span> | <span class="t">are the parts of speech of words and the dependency labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=538" target="_blank">00:08:58.260</a></span> | <span class="t">And so what we decided to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=541" target="_blank">00:09:01.040</a></span> | <span class="t">is that although those are much smaller sets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=544" target="_blank">00:09:04.660</a></span> | <span class="t">so the dependency labels are about 40 in number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=547" target="_blank">00:09:07.760</a></span> | <span class="t">and the parts of speech are of around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=549" target="_blank">00:09:09.960</a></span> | <span class="t">that order of magnitude,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=551" target="_blank">00:09:11.080</a></span> | <span class="t">sometimes less, sometimes more,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=553" target="_blank">00:09:13.260</a></span> | <span class="t">that even within those sets of categories,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=556" target="_blank">00:09:16.620</a></span> | <span class="t">there are ones that are very strongly related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=559" target="_blank">00:09:19.320</a></span> | <span class="t">So we also adopted distributed representations for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=564" target="_blank">00:09:24.320</a></span> | <span class="t">So for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=565" target="_blank">00:09:25.260</a></span> | <span class="t">there might be parts of speech</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=566" target="_blank">00:09:26.780</a></span> | <span class="t">for singular nouns and plural nouns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=569" target="_blank">00:09:29.100</a></span> | <span class="t">And basically most of the time they behave similarly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=572" target="_blank">00:09:32.180</a></span> | <span class="t">and there are adjectival modifiers and numerical modifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=576" target="_blank">00:09:36.380</a></span> | <span class="t">So these are just numbers like three, four, five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=578" target="_blank">00:09:38.820</a></span> | <span class="t">And again, a lot of the time they behave the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=581" target="_blank">00:09:41.700</a></span> | <span class="t">that you have both three cows and brown cows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=586" target="_blank">00:09:46.140</a></span> | <span class="t">Okay, so everything is going to be represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=590" target="_blank">00:09:50.920</a></span> | <span class="t">in the distributed representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=593" target="_blank">00:09:53.300</a></span> | <span class="t">So at that point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=594" target="_blank">00:09:54.660</a></span> | <span class="t">we have exactly the same kind of configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=598" target="_blank">00:09:58.200</a></span> | <span class="t">where we have our stack, our buffer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=601" target="_blank">00:10:01.100</a></span> | <span class="t">and we've started to build some arcs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=603" target="_blank">00:10:03.780</a></span> | <span class="t">And so the classification decisions of the next transition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=608" target="_blank">00:10:08.780</a></span> | <span class="t">are going to be made out of a few elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=611" target="_blank">00:10:11.680</a></span> | <span class="t">of this configuration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=612" target="_blank">00:10:12.940</a></span> | <span class="t">So we're looking at the top thing on the stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=615" target="_blank">00:10:15.620</a></span> | <span class="t">the thing that's second on the stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=617" target="_blank">00:10:17.580</a></span> | <span class="t">the first word on the buffer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=619" target="_blank">00:10:19.460</a></span> | <span class="t">And then we actually added in some additional features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=622" target="_blank">00:10:22.580</a></span> | <span class="t">that have been to the extent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=624" target="_blank">00:10:24.140</a></span> | <span class="t">that we've already built arcs for words on the stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=627" target="_blank">00:10:27.460</a></span> | <span class="t">that we can be looking at the dependence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=630" target="_blank">00:10:30.060</a></span> | <span class="t">on the left and right of those words that are on the stack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=633" target="_blank">00:10:33.720</a></span> | <span class="t">that are already in the sets of arcs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=636" target="_blank">00:10:36.120</a></span> | <span class="t">And so for each of those things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=639" target="_blank">00:10:39.080</a></span> | <span class="t">there is a word, there is a part of speech,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=643" target="_blank">00:10:43.040</a></span> | <span class="t">and for some of them, there is a dependency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=647" target="_blank">00:10:47.280</a></span> | <span class="t">where it's already connected up to something else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=650" target="_blank">00:10:50.920</a></span> | <span class="t">So for example, the left corner of S2 here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=654" target="_blank">00:10:54.160</a></span> | <span class="t">has an in-sub dependency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=656" target="_blank">00:10:56.240</a></span> | <span class="t">back to the second thing on the stack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=658" target="_blank">00:10:58.580</a></span> | <span class="t">So we can take these elements of the configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=662" target="_blank">00:11:02.320</a></span> | <span class="t">and can look up the embedding of each one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=665" target="_blank">00:11:05.780</a></span> | <span class="t">So we have word embeddings, part of speech embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=667" target="_blank">00:11:07.920</a></span> | <span class="t">and dependency embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=669" target="_blank">00:11:09.360</a></span> | <span class="t">and just concatenate them all together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=671" target="_blank">00:11:11.680</a></span> | <span class="t">kind of like we did before with the window classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=674" target="_blank">00:11:14.880</a></span> | <span class="t">and that will give us a neural representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=677" target="_blank">00:11:17.120</a></span> | <span class="t">of the configuration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=678" target="_blank">00:11:18.420</a></span> | <span class="t">Now, there's a second reason why we can hope to win</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=683" target="_blank">00:11:23.200</a></span> | <span class="t">by using a deep learning classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=685" target="_blank">00:11:25.720</a></span> | <span class="t">to predict the next transition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=687" target="_blank">00:11:27.680</a></span> | <span class="t">And we haven't really said much about that yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=689" target="_blank">00:11:29.840</a></span> | <span class="t">So I just wanted to detour</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=691" target="_blank">00:11:31.920</a></span> | <span class="t">and say a little bit more about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=694" target="_blank">00:11:34.560</a></span> | <span class="t">So the simplest kind of classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=697" target="_blank">00:11:37.320</a></span> | <span class="t">that's close to what we've been talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=701" target="_blank">00:11:41.360</a></span> | <span class="t">in neural models is a softmax classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=704" target="_blank">00:11:44.640</a></span> | <span class="t">So that if we have D dimensional vectors X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=708" target="_blank">00:11:48.480</a></span> | <span class="t">and we have Y classes to assign things to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=714" target="_blank">00:11:54.240</a></span> | <span class="t">oh, sorry, Y is an element of a set of C classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=719" target="_blank">00:11:59.240</a></span> | <span class="t">to assign things to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=721" target="_blank">00:12:01.280</a></span> | <span class="t">then we can build a softmax classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=723" target="_blank">00:12:03.680</a></span> | <span class="t">using the softmax distribution that we've seen before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=727" target="_blank">00:12:07.120</a></span> | <span class="t">where we decide the classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=729" target="_blank">00:12:09.100</a></span> | <span class="t">based on having a weight matrix that's C by D,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=733" target="_blank">00:12:13.680</a></span> | <span class="t">and we train on supervised data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=736" target="_blank">00:12:16.920</a></span> | <span class="t">the values of this W weight matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=739" target="_blank">00:12:19.400</a></span> | <span class="t">to minimize our negative log likelihood loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=743" target="_blank">00:12:23.360</a></span> | <span class="t">that we've seen before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=744" target="_blank">00:12:24.880</a></span> | <span class="t">a loss that's also commonly referred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=746" target="_blank">00:12:26.800</a></span> | <span class="t">to as cross entropy loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=748" target="_blank">00:12:28.240</a></span> | <span class="t">a term that you'll see in PyTorch among other places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=752" target="_blank">00:12:32.400</a></span> | <span class="t">So that is a straightforward machine learning classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=756" target="_blank">00:12:36.120</a></span> | <span class="t">And if you've done 229, you've seen softmax classifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=761" target="_blank">00:12:41.120</a></span> | <span class="t">But a simple softmax classifier like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=765" target="_blank">00:12:45.160</a></span> | <span class="t">shares with most traditional machine learning classifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=769" target="_blank">00:12:49.080</a></span> | <span class="t">So models that include naive Bayes models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=771" target="_blank">00:12:51.680</a></span> | <span class="t">support vector machines, logistic regression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=774" target="_blank">00:12:54.960</a></span> | <span class="t">that at the end of the day,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=776" target="_blank">00:12:56.820</a></span> | <span class="t">they're not very powerful classifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=779" target="_blank">00:12:59.480</a></span> | <span class="t">They're classifiers that only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=781" target="_blank">00:13:01.040</a></span> | <span class="t">give linear decision boundaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=783" target="_blank">00:13:03.280</a></span> | <span class="t">And so this can be quite limiting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=785" target="_blank">00:13:05.160</a></span> | <span class="t">So if you have a difficult problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=787" target="_blank">00:13:07.500</a></span> | <span class="t">like the one I'm indicating in the picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=790" target="_blank">00:13:10.000</a></span> | <span class="t">in the bottom left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=791" target="_blank">00:13:11.240</a></span> | <span class="t">well, there's just no way you can divide the green points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=794" target="_blank">00:13:14.920</a></span> | <span class="t">from the red points by simply drawing a straight line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=798" target="_blank">00:13:18.440</a></span> | <span class="t">So you're going to have a quite imperfect classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=801" target="_blank">00:13:21.560</a></span> | <span class="t">So the second big win of neural classifiers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=805" target="_blank">00:13:25.760</a></span> | <span class="t">is that they can be much more powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=808" target="_blank">00:13:28.680</a></span> | <span class="t">because they can provide nonlinear classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=812" target="_blank">00:13:32.160</a></span> | <span class="t">So rather than only being able to do something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=814" target="_blank">00:13:34.440</a></span> | <span class="t">in the left picture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=816" target="_blank">00:13:36.120</a></span> | <span class="t">we can come up with classifiers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=818" target="_blank">00:13:38.200</a></span> | <span class="t">that do something like in the right picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=820" target="_blank">00:13:40.560</a></span> | <span class="t">and therefore can separate the green and the red points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=825" target="_blank">00:13:45.340</a></span> | <span class="t">As an aside, these pictures I've taken</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=827" target="_blank">00:13:47.520</a></span> | <span class="t">from Andrej Karpathy's ConvNet JS software,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=830" target="_blank">00:13:50.600</a></span> | <span class="t">which is a kind of a fun little tool to play around with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=833" target="_blank">00:13:53.720</a></span> | <span class="t">if you've got a bit of spare time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=835" target="_blank">00:13:55.480</a></span> | <span class="t">And so there's something subtle going on here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=839" target="_blank">00:13:59.920</a></span> | <span class="t">is because our more powerful neural net classifiers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=844" target="_blank">00:14:04.920</a></span> | <span class="t">at the end of the day,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=846" target="_blank">00:14:06.520</a></span> | <span class="t">what they have at the top of them is a softmax layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=850" target="_blank">00:14:10.720</a></span> | <span class="t">And so this softmax layer is indeed a linear classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=855" target="_blank">00:14:15.720</a></span> | <span class="t">and it's still a linear classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=858" target="_blank">00:14:18.380</a></span> | <span class="t">But what they have below that is other layers of neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=863" target="_blank">00:14:23.080</a></span> | <span class="t">And so effectively what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=866" target="_blank">00:14:26.540</a></span> | <span class="t">is that the classification decisions are linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=869" target="_blank">00:14:29.760</a></span> | <span class="t">as far as the top softmax is concerned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=872" target="_blank">00:14:32.600</a></span> | <span class="t">but nonlinear in the original representation space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=876" target="_blank">00:14:36.600</a></span> | <span class="t">So precisely what a neural net can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=878" target="_blank">00:14:38.920</a></span> | <span class="t">is warp the space around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=881" target="_blank">00:14:41.840</a></span> | <span class="t">and move the representation of data points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=884" target="_blank">00:14:44.880</a></span> | <span class="t">to provide something that at the end of the day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=887" target="_blank">00:14:47.720</a></span> | <span class="t">can be classified by a linear classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=890" target="_blank">00:14:50.520</a></span> | <span class="t">And so that's what a simple feed forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=894" target="_blank">00:14:54.140</a></span> | <span class="t">neural network multiclass classifier does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=897" target="_blank">00:14:57.520</a></span> | <span class="t">So it starts with an input representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=900" target="_blank">00:15:00.860</a></span> | <span class="t">So these are is some dense representation of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=905" target="_blank">00:15:05.400</a></span> | <span class="t">It puts it through a hidden layer H</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=908" target="_blank">00:15:08.160</a></span> | <span class="t">with a matrix multiply followed by nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=913" target="_blank">00:15:13.160</a></span> | <span class="t">So that matrix multiply can transform the space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=916" target="_blank">00:15:16.440</a></span> | <span class="t">and map things around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=918" target="_blank">00:15:18.200</a></span> | <span class="t">And so then the output of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=920" target="_blank">00:15:20.200</a></span> | <span class="t">we can then put into a softmax layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=922" target="_blank">00:15:22.980</a></span> | <span class="t">and get out softmax probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=925" target="_blank">00:15:25.840</a></span> | <span class="t">from which we make our classification decisions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=929" target="_blank">00:15:29.060</a></span> | <span class="t">And to the extent that our probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=932" target="_blank">00:15:32.960</a></span> | <span class="t">don't assign one to the correct class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=935" target="_blank">00:15:35.320</a></span> | <span class="t">we then get some log loss or cross entropy error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=938" target="_blank">00:15:38.520</a></span> | <span class="t">which we back propagate towards the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=941" target="_blank">00:15:41.720</a></span> | <span class="t">and embeddings of our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=944" target="_blank">00:15:44.280</a></span> | <span class="t">And as the learning that goes on via back propagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=948" target="_blank">00:15:48.760</a></span> | <span class="t">we increasingly well learn parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=952" target="_blank">00:15:52.240</a></span> | <span class="t">of this hidden layer of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=954" target="_blank">00:15:54.440</a></span> | <span class="t">which learn to re-represent the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=957" target="_blank">00:15:57.360</a></span> | <span class="t">They move the inputs around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=959" target="_blank">00:15:59.400</a></span> | <span class="t">in an intermediate hidden vector space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=962" target="_blank">00:16:02.560</a></span> | <span class="t">So it can be easily classified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=964" target="_blank">00:16:04.680</a></span> | <span class="t">with what at the end of the day is the linear softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=967" target="_blank">00:16:07.980</a></span> | <span class="t">So this is basically the whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=971" target="_blank">00:16:11.880</a></span> | <span class="t">of a simple feed forward neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=974" target="_blank">00:16:14.240</a></span> | <span class="t">multi-class classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=975" target="_blank">00:16:15.960</a></span> | <span class="t">And if we had something like a visual signal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=980" target="_blank">00:16:20.200</a></span> | <span class="t">we just sort of feed straight in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=982" target="_blank">00:16:22.800</a></span> | <span class="t">real numbers and we've been done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=984" target="_blank">00:16:24.840</a></span> | <span class="t">But normally with human language material,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=987" target="_blank">00:16:27.920</a></span> | <span class="t">we actually effectively have one more layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=991" target="_blank">00:16:31.200</a></span> | <span class="t">that we're feeding in before that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=993" target="_blank">00:16:33.060</a></span> | <span class="t">'Cause really below this dense input layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=996" target="_blank">00:16:36.480</a></span> | <span class="t">we actually have one hot vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=998" target="_blank">00:16:38.880</a></span> | <span class="t">for what words or parts of speech were involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1001" target="_blank">00:16:41.720</a></span> | <span class="t">And then we're doing a lookup process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1004" target="_blank">00:16:44.040</a></span> | <span class="t">which you can think of as one more matrix multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1007" target="_blank">00:16:47.180</a></span> | <span class="t">to convert the one hot features into our dense input layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1011" target="_blank">00:16:51.080</a></span> | <span class="t">Okay, in my picture here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1013" target="_blank">00:16:53.760</a></span> | <span class="t">the one other thing that's different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1015" target="_blank">00:16:55.560</a></span> | <span class="t">is I've introduced a different non-linearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1018" target="_blank">00:16:58.400</a></span> | <span class="t">in the hidden layer, which is a rectified linear unit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1022" target="_blank">00:17:02.400</a></span> | <span class="t">And that's what we'll be using now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1024" target="_blank">00:17:04.080</a></span> | <span class="t">neural dependency parsers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1026" target="_blank">00:17:06.200</a></span> | <span class="t">It looks like the picture in the bottom right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1028" target="_blank">00:17:08.240</a></span> | <span class="t">and I'll come back to that in a few minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1031" target="_blank">00:17:11.120</a></span> | <span class="t">That's one of the extra neural net things to talk about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1034" target="_blank">00:17:14.460</a></span> | <span class="t">Okay, so our neural net dependency parser model architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1040" target="_blank">00:17:20.720</a></span> | <span class="t">is essentially exactly that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1044" target="_blank">00:17:24.000</a></span> | <span class="t">but applied to the configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1047" target="_blank">00:17:27.400</a></span> | <span class="t">of our transition-based dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1050" target="_blank">00:17:30.720</a></span> | <span class="t">So based on our transition-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1053" target="_blank">00:17:33.920</a></span> | <span class="t">dependency parser configuration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1056" target="_blank">00:17:36.360</a></span> | <span class="t">we construct an input layer embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1059" target="_blank">00:17:39.360</a></span> | <span class="t">by looking up the various elements as I discussed previously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1063" target="_blank">00:17:43.560</a></span> | <span class="t">and then we feed it through this hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1068" target="_blank">00:17:48.120</a></span> | <span class="t">to the softmax layer to get probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1072" target="_blank">00:17:52.280</a></span> | <span class="t">out of which we can choose what the next action is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1076" target="_blank">00:17:56.120</a></span> | <span class="t">And it's no more complicated than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1078" target="_blank">00:17:58.840</a></span> | <span class="t">But what we found is that just simply,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1086" target="_blank">00:18:06.720</a></span> | <span class="t">in some sense using the simplest kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1089" target="_blank">00:18:09.240</a></span> | <span class="t">of feed-forward neural classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1093" target="_blank">00:18:13.600</a></span> | <span class="t">could provide a very accurate dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1098" target="_blank">00:18:18.600</a></span> | <span class="t">that determines the structure of sentences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1102" target="_blank">00:18:22.960</a></span> | <span class="t">supporting meaning interpretation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1104" target="_blank">00:18:24.840</a></span> | <span class="t">the kind of way that I suggested last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1108" target="_blank">00:18:28.080</a></span> | <span class="t">Indeed, despite the fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1110" target="_blank">00:18:30.600</a></span> | <span class="t">that it was a quite simple architecture in 2014,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1114" target="_blank">00:18:34.760</a></span> | <span class="t">this was the first successful neural dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1118" target="_blank">00:18:38.840</a></span> | <span class="t">And the dense representations especially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1122" target="_blank">00:18:42.720</a></span> | <span class="t">but also partly the non-linearity of the classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1126" target="_blank">00:18:46.200</a></span> | <span class="t">gave us this good result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1127" target="_blank">00:18:47.540</a></span> | <span class="t">that it could both outperform symbolic parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1131" target="_blank">00:18:51.480</a></span> | <span class="t">in terms of accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1133" target="_blank">00:18:53.120</a></span> | <span class="t">and it could outperform them in terms of speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1136" target="_blank">00:18:56.080</a></span> | <span class="t">So that was 2014.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1140" target="_blank">00:19:00.720</a></span> | <span class="t">Just quickly here are a couple more slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1143" target="_blank">00:19:03.480</a></span> | <span class="t">on what's happened since then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1146" target="_blank">00:19:06.820</a></span> | <span class="t">So lots of people got excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1148" target="_blank">00:19:08.880</a></span> | <span class="t">by the success of this neural dependency parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1151" target="_blank">00:19:11.860</a></span> | <span class="t">and a number of people, particularly at Google,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1154" target="_blank">00:19:14.520</a></span> | <span class="t">then set about building a bigger, fancier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1158" target="_blank">00:19:18.680</a></span> | <span class="t">transition-based neural dependency parser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1161" target="_blank">00:19:21.040</a></span> | <span class="t">So they explored bigger, deeper networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1163" target="_blank">00:19:23.320</a></span> | <span class="t">There's no reason to only have one hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1165" target="_blank">00:19:25.760</a></span> | <span class="t">You can have two hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1167" target="_blank">00:19:27.600</a></span> | <span class="t">You can do beam search that I briefly mentioned last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1171" target="_blank">00:19:31.560</a></span> | <span class="t">Another thing that I'm not gonna talk about now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1173" target="_blank">00:19:33.600</a></span> | <span class="t">is adding conditional random field style inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1177" target="_blank">00:19:37.660</a></span> | <span class="t">over decision sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1179" target="_blank">00:19:39.520</a></span> | <span class="t">And that then led in 2016</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1182" target="_blank">00:19:42.520</a></span> | <span class="t">for a model that they called Parsey-McParse face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1187" target="_blank">00:19:47.240</a></span> | <span class="t">which is hard to say with a straight face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1189" target="_blank">00:19:49.860</a></span> | <span class="t">which was then about 2.5, 3% more accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1194" target="_blank">00:19:54.860</a></span> | <span class="t">than the model that we had produced,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1196" target="_blank">00:19:56.660</a></span> | <span class="t">but still in basically the same family</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1199" target="_blank">00:19:59.440</a></span> | <span class="t">of transition-based parser with a neural net classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1203" target="_blank">00:20:03.520</a></span> | <span class="t">to choose the next transition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1205" target="_blank">00:20:05.260</a></span> | <span class="t">The alternative to transition-based parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1211" target="_blank">00:20:11.520</a></span> | <span class="t">is graph-based dependency parsers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1214" target="_blank">00:20:14.160</a></span> | <span class="t">And for a graph-based dependency parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1217" target="_blank">00:20:17.400</a></span> | <span class="t">what you're doing is effectively considering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1220" target="_blank">00:20:20.080</a></span> | <span class="t">every pair of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1222" target="_blank">00:20:22.100</a></span> | <span class="t">and considering a word as a dependent of root,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1224" target="_blank">00:20:24.860</a></span> | <span class="t">and you're coming up with a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1227" target="_blank">00:20:27.440</a></span> | <span class="t">as to how likely is that big is a dependent of root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1232" target="_blank">00:20:32.440</a></span> | <span class="t">or how likely is big to be dependent of cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1235" target="_blank">00:20:35.920</a></span> | <span class="t">And similarly for every other word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1237" target="_blank">00:20:37.520</a></span> | <span class="t">for the word sat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1239" target="_blank">00:20:39.580</a></span> | <span class="t">how likely is it to be a dependent of root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1243" target="_blank">00:20:43.020</a></span> | <span class="t">or a dependent of the, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1245" target="_blank">00:20:45.620</a></span> | <span class="t">And well, to do that well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1247" target="_blank">00:20:47.860</a></span> | <span class="t">you need to know more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1248" target="_blank">00:20:48.920</a></span> | <span class="t">than just what the two words involved are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1252" target="_blank">00:20:52.200</a></span> | <span class="t">And so what you want to do is understand the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1256" target="_blank">00:20:56.480</a></span> | <span class="t">So you want to have an understanding of the context of big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1260" target="_blank">00:21:00.040</a></span> | <span class="t">what's to the left of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1261" target="_blank">00:21:01.040</a></span> | <span class="t">what's to the right of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1262" target="_blank">00:21:02.320</a></span> | <span class="t">to understand how you might hook it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1264" target="_blank">00:21:04.520</a></span> | <span class="t">into the dependency representations of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1268" target="_blank">00:21:08.100</a></span> | <span class="t">And so while there'd been previous work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1271" target="_blank">00:21:11.520</a></span> | <span class="t">in graph-based dependency parsing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1273" target="_blank">00:21:13.640</a></span> | <span class="t">like the MST parser I showed on the earlier results slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1278" target="_blank">00:21:18.240</a></span> | <span class="t">it seemed appealing that we could come up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1281" target="_blank">00:21:21.200</a></span> | <span class="t">with a much better representation of context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1284" target="_blank">00:21:24.100</a></span> | <span class="t">using neural nets that look at context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1286" target="_blank">00:21:26.760</a></span> | <span class="t">And how we do that is actually what I'll be talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1289" target="_blank">00:21:29.160</a></span> | <span class="t">in the end part of the lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1291" target="_blank">00:21:31.060</a></span> | <span class="t">And so at Stanford,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1293" target="_blank">00:21:33.720</a></span> | <span class="t">we became interested in trying to work out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1297" target="_blank">00:21:37.220</a></span> | <span class="t">how to come up with a better graph-based dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1300" target="_blank">00:21:40.260</a></span> | <span class="t">using context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1301" target="_blank">00:21:41.860</a></span> | <span class="t">Sorry, I forgot this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1302" target="_blank">00:21:42.700</a></span> | <span class="t">This was showing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1304" target="_blank">00:21:44.420</a></span> | <span class="t">if we can score each pairwise dependency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1306" target="_blank">00:21:46.820</a></span> | <span class="t">we can simply choose the best one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1309" target="_blank">00:21:49.100</a></span> | <span class="t">So we can say probably big is a dependent of cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1313" target="_blank">00:21:53.580</a></span> | <span class="t">and to a first approximation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1315" target="_blank">00:21:55.860</a></span> | <span class="t">we're gonna want to choose for each word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1318" target="_blank">00:21:58.380</a></span> | <span class="t">that it is a dependent of the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1321" target="_blank">00:22:01.880</a></span> | <span class="t">that seems most likely to be a dependent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1324" target="_blank">00:22:04.380</a></span> | <span class="t">But we wanna do that with some constraints</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1326" target="_blank">00:22:06.440</a></span> | <span class="t">because we wanna get out something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1328" target="_blank">00:22:08.140</a></span> | <span class="t">that is a tree with a single root,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1330" target="_blank">00:22:10.420</a></span> | <span class="t">as I discussed last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1332" target="_blank">00:22:12.180</a></span> | <span class="t">And you can do that by making use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1334" target="_blank">00:22:14.260</a></span> | <span class="t">of a minimum spanning tree algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1336" target="_blank">00:22:16.640</a></span> | <span class="t">that uses the scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1338" target="_blank">00:22:18.520</a></span> | <span class="t">of how likely different dependencies are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1342" target="_blank">00:22:22.020</a></span> | <span class="t">Okay, so then in 2017,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1345" target="_blank">00:22:25.420</a></span> | <span class="t">another student, Tim Dozat and me then worked on saying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1349" target="_blank">00:22:29.780</a></span> | <span class="t">well, can we now also build a much better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1353" target="_blank">00:22:33.500</a></span> | <span class="t">neural graph-based dependency parser?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1356" target="_blank">00:22:36.220</a></span> | <span class="t">And we developed a novel method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1358" target="_blank">00:22:38.540</a></span> | <span class="t">for scoring neural scoring dependency parsers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1363" target="_blank">00:22:43.140</a></span> | <span class="t">in a graph-based model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1364" target="_blank">00:22:44.500</a></span> | <span class="t">which I'm not gonna get into the details of right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1368" target="_blank">00:22:48.180</a></span> | <span class="t">but that also had a very nice result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1370" target="_blank">00:22:50.380</a></span> | <span class="t">'cause getting back to graph-based parsing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1373" target="_blank">00:22:53.620</a></span> | <span class="t">we could then build a graph-based parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1375" target="_blank">00:22:55.860</a></span> | <span class="t">that performed about a percent better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1378" target="_blank">00:22:58.300</a></span> | <span class="t">than the best of the Google transition-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1381" target="_blank">00:23:01.180</a></span> | <span class="t">neural dependency parsers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1383" target="_blank">00:23:03.460</a></span> | <span class="t">But I should point out that this is a mixed win</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1386" target="_blank">00:23:06.980</a></span> | <span class="t">'cause although its accuracy is better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1390" target="_blank">00:23:10.420</a></span> | <span class="t">these graph-based parsers are just in squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1393" target="_blank">00:23:13.540</a></span> | <span class="t">and performance rather than linear time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1395" target="_blank">00:23:15.860</a></span> | <span class="t">So kind of like the early results I showed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1398" target="_blank">00:23:18.980</a></span> | <span class="t">they don't operate nearly as quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1401" target="_blank">00:23:21.300</a></span> | <span class="t">when you're wanting to parse large amounts of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1405" target="_blank">00:23:25.460</a></span> | <span class="t">with complex long sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1408" target="_blank">00:23:28.500</a></span> | <span class="t">Okay, so that's everything you need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1411" target="_blank">00:23:31.940</a></span> | <span class="t">about dependency parsers and to do assignment three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1415" target="_blank">00:23:35.220</a></span> | <span class="t">So grab it this evening and start to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1417" target="_blank">00:23:37.700</a></span> | <span class="t">But I did want to sort of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1420" target="_blank">00:23:40.600</a></span> | <span class="t">before going on to the next topic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1422" target="_blank">00:23:42.360</a></span> | <span class="t">just mention a few more things about neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1426" target="_blank">00:23:46.820</a></span> | <span class="t">Since some of you know this well already,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1429" target="_blank">00:23:49.940</a></span> | <span class="t">some of you have seen less of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1431" target="_blank">00:23:51.780</a></span> | <span class="t">but there just are a bunch of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1433" target="_blank">00:23:53.620</a></span> | <span class="t">you have to be aware of for building neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1437" target="_blank">00:23:57.220</a></span> | <span class="t">Now, again, for assignment three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1439" target="_blank">00:23:59.900</a></span> | <span class="t">essentially we give you everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1441" target="_blank">00:24:01.740</a></span> | <span class="t">And if you follow the recipe, your parser should work well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1446" target="_blank">00:24:06.280</a></span> | <span class="t">But what you should minimally do is actually look carefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1451" target="_blank">00:24:11.280</a></span> | <span class="t">at some of the things that this parser does,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1456" target="_blank">00:24:16.160</a></span> | <span class="t">which is questions like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1457" target="_blank">00:24:17.780</a></span> | <span class="t">how do we initialize our matrices of our neural network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1462" target="_blank">00:24:22.780</a></span> | <span class="t">What kind of optimizers do we use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1466" target="_blank">00:24:26.180</a></span> | <span class="t">And things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1467" target="_blank">00:24:27.400</a></span> | <span class="t">'Cause these are all important decisions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1471" target="_blank">00:24:31.160</a></span> | <span class="t">And so I wanted to say just a few words about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1474" target="_blank">00:24:34.300</a></span> | <span class="t">Okay, so the first thing that we haven't discussed at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1479" target="_blank">00:24:39.140</a></span> | <span class="t">is the concept of regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1482" target="_blank">00:24:42.380</a></span> | <span class="t">So when we're building these neural nets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1485" target="_blank">00:24:45.260</a></span> | <span class="t">we're now building models with a huge number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1490" target="_blank">00:24:50.260</a></span> | <span class="t">So essentially just about all neural net models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1494" target="_blank">00:24:54.880</a></span> | <span class="t">that work well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1497" target="_blank">00:24:57.100</a></span> | <span class="t">actually their full loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1500" target="_blank">00:25:00.900</a></span> | <span class="t">is a regularized loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1503" target="_blank">00:25:03.260</a></span> | <span class="t">So for this loss function here of J,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1508" target="_blank">00:25:08.260</a></span> | <span class="t">well, this part here is the part that we've seen before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1512" target="_blank">00:25:12.540</a></span> | <span class="t">where we're using a softmax classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1516" target="_blank">00:25:16.540</a></span> | <span class="t">and then taking a negative log likelihood loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1518" target="_blank">00:25:18.980</a></span> | <span class="t">which we're then averaging over the different examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1521" target="_blank">00:25:21.640</a></span> | <span class="t">But actually we then stick on the end of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1525" target="_blank">00:25:25.380</a></span> | <span class="t">this regularization term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1527" target="_blank">00:25:27.700</a></span> | <span class="t">And so this regularization term sums the square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1532" target="_blank">00:25:32.320</a></span> | <span class="t">of every parameter in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1534" target="_blank">00:25:34.960</a></span> | <span class="t">And so what that effectively says is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1539" target="_blank">00:25:39.260</a></span> | <span class="t">you only wanna make parameters non-zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1544" target="_blank">00:25:44.060</a></span> | <span class="t">if they're really useful, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1546" target="_blank">00:25:46.600</a></span> | <span class="t">So to the extent that the parameters don't help much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1549" target="_blank">00:25:49.720</a></span> | <span class="t">you're just being penalized here by making them non-zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1554" target="_blank">00:25:54.220</a></span> | <span class="t">But to the extent that the parameters do help,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1556" target="_blank">00:25:56.780</a></span> | <span class="t">you'll gain in your estimation of likelihood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1559" target="_blank">00:25:59.500</a></span> | <span class="t">and therefore it's okay for them to be non-zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1562" target="_blank">00:26:02.560</a></span> | <span class="t">In particular, notice that this penalty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1565" target="_blank">00:26:05.680</a></span> | <span class="t">is assessed only once per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1568" target="_blank">00:26:08.300</a></span> | <span class="t">It's not being assessed separately for each example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1571" target="_blank">00:26:11.960</a></span> | <span class="t">Okay, and having this kind of regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1576" target="_blank">00:26:16.060</a></span> | <span class="t">is essential to build neural net models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1579" target="_blank">00:26:19.700</a></span> | <span class="t">that regularize well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1581" target="_blank">00:26:21.500</a></span> | <span class="t">So the classic problem is referred to as overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1585" target="_blank">00:26:25.220</a></span> | <span class="t">And what overfitting means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1587" target="_blank">00:26:27.140</a></span> | <span class="t">is that if you have a particular training dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1589" target="_blank">00:26:29.940</a></span> | <span class="t">and you start training your model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1592" target="_blank">00:26:32.660</a></span> | <span class="t">your error will go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1595" target="_blank">00:26:35.000</a></span> | <span class="t">because you'll shift the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1596" target="_blank">00:26:36.500</a></span> | <span class="t">so they better predict the correct answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1601" target="_blank">00:26:41.260</a></span> | <span class="t">for data points in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1603" target="_blank">00:26:43.260</a></span> | <span class="t">And you can keep on doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1605" target="_blank">00:26:45.200</a></span> | <span class="t">and it will keep on reducing your error rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1609" target="_blank">00:26:49.460</a></span> | <span class="t">But if you then look at your partially trained classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1613" target="_blank">00:26:53.740</a></span> | <span class="t">and say, how well does this classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1617" target="_blank">00:26:57.140</a></span> | <span class="t">classify independent data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1620" target="_blank">00:27:00.140</a></span> | <span class="t">different test data that you weren't training the model on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1624" target="_blank">00:27:04.160</a></span> | <span class="t">what you will find is up until a certain point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1627" target="_blank">00:27:07.200</a></span> | <span class="t">you'll get better at classifying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1630" target="_blank">00:27:10.120</a></span> | <span class="t">independent test examples as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1632" target="_blank">00:27:12.680</a></span> | <span class="t">And after that, commonly what will happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1636" target="_blank">00:27:16.280</a></span> | <span class="t">is you'll actually start to get worse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1638" target="_blank">00:27:18.640</a></span> | <span class="t">at classifying independent test examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1642" target="_blank">00:27:22.120</a></span> | <span class="t">even though you're continuing to get better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1644" target="_blank">00:27:24.120</a></span> | <span class="t">at predicting the training examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1646" target="_blank">00:27:26.200</a></span> | <span class="t">And so this was then referred to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1647" target="_blank">00:27:27.960</a></span> | <span class="t">as you're overfitting the training examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1651" target="_blank">00:27:31.800</a></span> | <span class="t">that you're fiddling the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1654" target="_blank">00:27:34.120</a></span> | <span class="t">so that they're really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1655" target="_blank">00:27:35.160</a></span> | <span class="t">at predicting the training examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1657" target="_blank">00:27:37.240</a></span> | <span class="t">which aren't useful things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1659" target="_blank">00:27:39.960</a></span> | <span class="t">that can then predict on independent examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1664" target="_blank">00:27:44.440</a></span> | <span class="t">that you've come to at runtime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1667" target="_blank">00:27:47.440</a></span> | <span class="t">Okay, that classic view of regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1672" target="_blank">00:27:52.200</a></span> | <span class="t">is sort of actually outmoded and wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1675" target="_blank">00:27:55.680</a></span> | <span class="t">for modern neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1678" target="_blank">00:27:58.920</a></span> | <span class="t">So the right way to think of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1681" target="_blank">00:28:01.880</a></span> | <span class="t">for the kind of modern big neural networks that we build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1686" target="_blank">00:28:06.560</a></span> | <span class="t">is that overfitting on the training data isn't a problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1691" target="_blank">00:28:11.560</a></span> | <span class="t">but nevertheless, you need regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1697" target="_blank">00:28:17.000</a></span> | <span class="t">to make sure that your models generalize well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1700" target="_blank">00:28:20.920</a></span> | <span class="t">to independent test data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1703" target="_blank">00:28:23.520</a></span> | <span class="t">So what you would like is for your graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1706" target="_blank">00:28:26.200</a></span> | <span class="t">not to look like this example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1709" target="_blank">00:28:29.900</a></span> | <span class="t">with test error starting to head up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1712" target="_blank">00:28:32.240</a></span> | <span class="t">You'd like to have it at worst case flat line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1717" target="_blank">00:28:37.240</a></span> | <span class="t">and best case still be gradually dropping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1720" target="_blank">00:28:40.160</a></span> | <span class="t">It'll always be higher than the training error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1723" target="_blank">00:28:43.080</a></span> | <span class="t">but it's not actually showing a failure to generalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1727" target="_blank">00:28:47.840</a></span> | <span class="t">So when we train big neural nets these days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1732" target="_blank">00:28:52.800</a></span> | <span class="t">our big neural nets always overfit on the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1739" target="_blank">00:28:59.280</a></span> | <span class="t">They hugely overfit on the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1742" target="_blank">00:29:02.100</a></span> | <span class="t">In fact, in many circumstances,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1744" target="_blank">00:29:04.020</a></span> | <span class="t">our neural nets have so many parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1746" target="_blank">00:29:06.700</a></span> | <span class="t">that you can continue to train them on the training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1750" target="_blank">00:29:10.020</a></span> | <span class="t">until the error on the training data is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1752" target="_blank">00:29:12.700</a></span> | <span class="t">They get every single example right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1754" target="_blank">00:29:14.740</a></span> | <span class="t">because they can just memorize enough stuff about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1757" target="_blank">00:29:17.460</a></span> | <span class="t">to predict the right answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1759" target="_blank">00:29:19.260</a></span> | <span class="t">But in general, providing the models are regularized well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1763" target="_blank">00:29:23.740</a></span> | <span class="t">those models will still also generalize well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1767" target="_blank">00:29:27.020</a></span> | <span class="t">and predict well on independent data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1769" target="_blank">00:29:29.340</a></span> | <span class="t">And so for part of what we wanna do for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1773" target="_blank">00:29:33.740</a></span> | <span class="t">is to work out how much to regularize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1776" target="_blank">00:29:36.740</a></span> | <span class="t">And so this lambda parameter here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1779" target="_blank">00:29:39.280</a></span> | <span class="t">is the strength of regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1781" target="_blank">00:29:41.880</a></span> | <span class="t">So if you're making that lambda number big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1784" target="_blank">00:29:44.640</a></span> | <span class="t">you're getting more regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1787" target="_blank">00:29:47.440</a></span> | <span class="t">And if you make it small, you're getting less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1789" target="_blank">00:29:49.960</a></span> | <span class="t">And you don't wanna have it be too big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1791" target="_blank">00:29:51.760</a></span> | <span class="t">or else you won't fit the data well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1793" target="_blank">00:29:53.640</a></span> | <span class="t">And you don't want it to be too small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1796" target="_blank">00:29:56.300</a></span> | <span class="t">or else you have the problem that you don't generalize well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1801" target="_blank">00:30:01.300</a></span> | <span class="t">Okay, so this is classic L2 regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1805" target="_blank">00:30:05.160</a></span> | <span class="t">and it's a starting point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1806" target="_blank">00:30:06.920</a></span> | <span class="t">But our big neural nets are sufficiently complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1809" target="_blank">00:30:09.600</a></span> | <span class="t">and have sufficiently many parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1811" target="_blank">00:30:11.680</a></span> | <span class="t">that essentially L2 regularization doesn't cut it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1815" target="_blank">00:30:15.800</a></span> | <span class="t">So the next thing that you should know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1818" target="_blank">00:30:18.520</a></span> | <span class="t">and is a very standard good feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1822" target="_blank">00:30:22.040</a></span> | <span class="t">for building neural nets is a technique called dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1825" target="_blank">00:30:25.980</a></span> | <span class="t">So dropout is generally introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1830" target="_blank">00:30:30.980</a></span> | <span class="t">as a sort of a slightly funny process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1835" target="_blank">00:30:35.180</a></span> | <span class="t">that you do when training to avoid feature co-adaptation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1840" target="_blank">00:30:40.180</a></span> | <span class="t">So in dropout, what you do is at the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1844" target="_blank">00:30:44.820</a></span> | <span class="t">that you're training your model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1847" target="_blank">00:30:47.820</a></span> | <span class="t">that for each instance or for each batch in your training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1855" target="_blank">00:30:55.140</a></span> | <span class="t">then for each neuron in the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1858" target="_blank">00:30:58.480</a></span> | <span class="t">you drop 50% of its inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1861" target="_blank">00:31:01.120</a></span> | <span class="t">You just treat them as zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1863" target="_blank">00:31:03.400</a></span> | <span class="t">And so that you can do by sort of zeroing out elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1867" target="_blank">00:31:07.280</a></span> | <span class="t">of the sort of layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1870" target="_blank">00:31:10.460</a></span> | <span class="t">And then at test time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1874" target="_blank">00:31:14.840</a></span> | <span class="t">you don't drop any of the model weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1877" target="_blank">00:31:17.200</a></span> | <span class="t">you keep them all, but actually you have all the model weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1880" target="_blank">00:31:20.940</a></span> | <span class="t">because you're now keeping twice as many things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1883" target="_blank">00:31:23.560</a></span> | <span class="t">as you'd used at training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1885" target="_blank">00:31:25.320</a></span> | <span class="t">And so effectively that little recipe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1890" target="_blank">00:31:30.600</a></span> | <span class="t">prevents what's called feature co-adaptation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1893" target="_blank">00:31:33.840</a></span> | <span class="t">So you can't have features that are only useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1898" target="_blank">00:31:38.840</a></span> | <span class="t">in the presence of particular other features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1903" target="_blank">00:31:43.300</a></span> | <span class="t">because the model can't guarantee</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1905" target="_blank">00:31:45.800</a></span> | <span class="t">which features are gonna be present for different examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1908" target="_blank">00:31:48.760</a></span> | <span class="t">because different features are being randomly dropped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1911" target="_blank">00:31:51.320</a></span> | <span class="t">all of the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1912" target="_blank">00:31:52.740</a></span> | <span class="t">And so effectively dropout gives you a kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1916" target="_blank">00:31:56.040</a></span> | <span class="t">of a middle ground between naive Bayes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1918" target="_blank">00:31:58.160</a></span> | <span class="t">and a logistic regression model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1920" target="_blank">00:32:00.000</a></span> | <span class="t">In a naive Bayes models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1921" target="_blank">00:32:01.680</a></span> | <span class="t">all the weights are set independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1923" target="_blank">00:32:03.520</a></span> | <span class="t">and a logistic regression model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1925" target="_blank">00:32:05.440</a></span> | <span class="t">all the weights are set in the context of all the others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1928" target="_blank">00:32:08.360</a></span> | <span class="t">And here you are aware of other weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1930" target="_blank">00:32:10.800</a></span> | <span class="t">but they can randomly disappear from you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1933" target="_blank">00:32:13.680</a></span> | <span class="t">It's also related to ensemble models like model bagging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1937" target="_blank">00:32:17.280</a></span> | <span class="t">because you're using different subsets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1939" target="_blank">00:32:19.000</a></span> | <span class="t">of the features every time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1942" target="_blank">00:32:22.200</a></span> | <span class="t">But after all of those explanations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1946" target="_blank">00:32:26.120</a></span> | <span class="t">there's actually another way of thinking about dropout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1949" target="_blank">00:32:29.800</a></span> | <span class="t">which was actually developed here at Stanford.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1951" target="_blank">00:32:31.760</a></span> | <span class="t">This is a paper by Percy Liang and students,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1954" target="_blank">00:32:34.840</a></span> | <span class="t">which is to argue that really what dropout gives you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1959" target="_blank">00:32:39.120</a></span> | <span class="t">is a strong regularizer that isn't a uniform regularizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1963" target="_blank">00:32:43.560</a></span> | <span class="t">like L2 that regularizes everything with an L2 loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1967" target="_blank">00:32:47.040</a></span> | <span class="t">but can learn a feature dependent regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1970" target="_blank">00:32:50.160</a></span> | <span class="t">And so that dropout has just emerged as in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1973" target="_blank">00:32:53.480</a></span> | <span class="t">the best way to do regularization for neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1977" target="_blank">00:32:57.920</a></span> | <span class="t">I think you've already seen and heard this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1980" target="_blank">00:33:00.420</a></span> | <span class="t">but just to have it on my slides once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1984" target="_blank">00:33:04.280</a></span> | <span class="t">If you want to have your neural networks go fast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1989" target="_blank">00:33:09.080</a></span> | <span class="t">it's really essential that you make use of vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1992" target="_blank">00:33:12.640</a></span> | <span class="t">matrices, tensors, and you don't do things with for loops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=1996" target="_blank">00:33:16.840</a></span> | <span class="t">So here's a teeny example where I'm using time it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2001" target="_blank">00:33:21.840</a></span> | <span class="t">which is a useful thing that you can use too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2004" target="_blank">00:33:24.040</a></span> | <span class="t">to see how fast your neural nets run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2006" target="_blank">00:33:26.240</a></span> | <span class="t">and different ways of writing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2008" target="_blank">00:33:28.840</a></span> | <span class="t">And so when I'm doing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2011" target="_blank">00:33:31.500</a></span> | <span class="t">doing these dot products here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2016" target="_blank">00:33:36.980</a></span> | <span class="t">I can either do the dot product in a for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2021" target="_blank">00:33:41.980</a></span> | <span class="t">against each word vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2024" target="_blank">00:33:44.780</a></span> | <span class="t">or I can do the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2026" target="_blank">00:33:46.680</a></span> | <span class="t">with a single word vector matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2029" target="_blank">00:33:49.580</a></span> | <span class="t">And if I do it in a for loop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2031" target="_blank">00:33:51.860</a></span> | <span class="t">doing each loop takes me almost a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2037" target="_blank">00:33:57.960</a></span> | <span class="t">Whereas if I do it with a matrix multiply,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2042" target="_blank">00:34:02.960</a></span> | <span class="t">it takes me an order of magnitude less time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2046" target="_blank">00:34:06.080</a></span> | <span class="t">So you should always be looking to use vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2048" target="_blank">00:34:08.460</a></span> | <span class="t">and matrices, not for loops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2050" target="_blank">00:34:10.520</a></span> | <span class="t">And this is a speed up of about 10 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2053" target="_blank">00:34:13.640</a></span> | <span class="t">when you're doing things on a CPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2056" target="_blank">00:34:16.360</a></span> | <span class="t">Heading forward, we're going to be using GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2059" target="_blank">00:34:19.880</a></span> | <span class="t">and they only further exaggerate the advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2063" target="_blank">00:34:23.180</a></span> | <span class="t">of using vectors and matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2065" target="_blank">00:34:25.220</a></span> | <span class="t">where you'll commonly get two orders of magnitude speed up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2068" target="_blank">00:34:28.720</a></span> | <span class="t">by doing things that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2070" target="_blank">00:34:30.880</a></span> | <span class="t">Yeah, so for the backward pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2074" target="_blank">00:34:34.160</a></span> | <span class="t">you are running a backward pass as before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2079" target="_blank">00:34:39.040</a></span> | <span class="t">on the dropped out examples, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2082" target="_blank">00:34:42.800</a></span> | <span class="t">So for the things that were dropped out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2085" target="_blank">00:34:45.680</a></span> | <span class="t">no gradient is going through them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2087" target="_blank">00:34:47.680</a></span> | <span class="t">because they weren't present,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2089" target="_blank">00:34:49.200</a></span> | <span class="t">they're not affecting things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2091" target="_blank">00:34:51.640</a></span> | <span class="t">So in a particular batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2095" target="_blank">00:34:55.320</a></span> | <span class="t">you're only training weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2097" target="_blank">00:34:57.800</a></span> | <span class="t">for the things that aren't dropped out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2099" target="_blank">00:34:59.680</a></span> | <span class="t">But then since you, for each successive batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2103" target="_blank">00:35:03.280</a></span> | <span class="t">you drop out different things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2105" target="_blank">00:35:05.180</a></span> | <span class="t">that over a bunch of batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2106" target="_blank">00:35:06.940</a></span> | <span class="t">you're then training all of the weights of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2111" target="_blank">00:35:11.280</a></span> | <span class="t">And so feature dependent regularizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2114" target="_blank">00:35:14.680</a></span> | <span class="t">is meaning that how much a feature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2119" target="_blank">00:35:19.680</a></span> | <span class="t">the different features can be regularized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2123" target="_blank">00:35:23.680</a></span> | <span class="t">different amounts to maximize performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2128" target="_blank">00:35:28.680</a></span> | <span class="t">So back in this model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2132" target="_blank">00:35:32.200</a></span> | <span class="t">every feature was just sort of being penalized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2136" target="_blank">00:35:36.920</a></span> | <span class="t">by taking lambda times that squared value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2139" target="_blank">00:35:39.720</a></span> | <span class="t">So this is sort of uniform regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2144" target="_blank">00:35:44.120</a></span> | <span class="t">where the end result of this dropout style training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2148" target="_blank">00:35:48.280</a></span> | <span class="t">is that you end up with some features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2151" target="_blank">00:35:51.960</a></span> | <span class="t">being regularized much more strongly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2154" target="_blank">00:35:54.300</a></span> | <span class="t">and some other features being regularized less strongly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2159" target="_blank">00:35:59.300</a></span> | <span class="t">And how much they are regularized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2163" target="_blank">00:36:03.280</a></span> | <span class="t">depends on how much they're being used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2165" target="_blank">00:36:05.240</a></span> | <span class="t">So you're regularizing more features that are being used less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2169" target="_blank">00:36:09.800</a></span> | <span class="t">but I'm not gonna get through into the details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2173" target="_blank">00:36:13.040</a></span> | <span class="t">of how you can understand that perspective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2176" target="_blank">00:36:16.240</a></span> | <span class="t">That's outside of the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2179" target="_blank">00:36:19.080</a></span> | <span class="t">of what I'm gonna get through right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2181" target="_blank">00:36:21.520</a></span> | <span class="t">So the final bit is I just wanted to give a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2184" target="_blank">00:36:24.840</a></span> | <span class="t">of perspective on nonlinearities in our neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2189" target="_blank">00:36:29.840</a></span> | <span class="t">So the first thing to remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2193" target="_blank">00:36:33.340</a></span> | <span class="t">is you have to have nonlinearities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2195" target="_blank">00:36:35.760</a></span> | <span class="t">So if you're building a multi-layer neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2199" target="_blank">00:36:39.240</a></span> | <span class="t">and you've just got W1X plus B1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2202" target="_blank">00:36:42.400</a></span> | <span class="t">then you put it through W2X plus B2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2205" target="_blank">00:36:45.920</a></span> | <span class="t">and then put through W3X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2207" target="_blank">00:36:47.720</a></span> | <span class="t">well, I guess they're different hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2212" target="_blank">00:36:52.240</a></span> | <span class="t">Sorry, I should have said X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2213" target="_blank">00:36:53.240</a></span> | <span class="t">They should be hidden one, hidden two, hidden three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2215" target="_blank">00:36:55.360</a></span> | <span class="t">W3, hidden three plus B3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2219" target="_blank">00:36:59.040</a></span> | <span class="t">That multiple linear transformations composed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2223" target="_blank">00:37:03.820</a></span> | <span class="t">so they can be just collapsed down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2225" target="_blank">00:37:05.380</a></span> | <span class="t">into a single linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2227" target="_blank">00:37:07.660</a></span> | <span class="t">So you don't get any power as a data representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2232" target="_blank">00:37:12.660</a></span> | <span class="t">by having multiple linear layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2237" target="_blank">00:37:17.840</a></span> | <span class="t">There's a slightly longer story there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2239" target="_blank">00:37:19.740</a></span> | <span class="t">'cause you actually do get some interesting learning effects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2242" target="_blank">00:37:22.380</a></span> | <span class="t">but I'm not gonna talk about that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2245" target="_blank">00:37:25.080</a></span> | <span class="t">But standardly, we have to have some kind of nonlinearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2250" target="_blank">00:37:30.080</a></span> | <span class="t">to do something interesting in a deep neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2255" target="_blank">00:37:35.820</a></span> | <span class="t">Okay, so there's a starting point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2259" target="_blank">00:37:39.340</a></span> | <span class="t">is the most classic nonlinearity is the logistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2263" target="_blank">00:37:43.420</a></span> | <span class="t">often just called the sigmoid nonlinearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2266" target="_blank">00:37:46.420</a></span> | <span class="t">because of its S shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2268" target="_blank">00:37:48.620</a></span> | <span class="t">which we've seen before in previous lectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2271" target="_blank">00:37:51.300</a></span> | <span class="t">So this will take any real number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2274" target="_blank">00:37:54.140</a></span> | <span class="t">and map it on to the range of zero one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2277" target="_blank">00:37:57.680</a></span> | <span class="t">And that was sort of basically what people used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2282" target="_blank">00:38:02.140</a></span> | <span class="t">in sort of 1980s neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2284" target="_blank">00:38:04.900</a></span> | <span class="t">Now, one disadvantage of this nonlinearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2288" target="_blank">00:38:08.860</a></span> | <span class="t">is that it's moving everything into the positive space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2294" target="_blank">00:38:14.580</a></span> | <span class="t">because the output is always between zero and one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2297" target="_blank">00:38:17.480</a></span> | <span class="t">So people then decided that for many purposes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2301" target="_blank">00:38:21.260</a></span> | <span class="t">it was useful to have this variant sigmoid shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2304" target="_blank">00:38:24.980</a></span> | <span class="t">of hyperbolic tan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2307" target="_blank">00:38:27.060</a></span> | <span class="t">which is then being shown in the second picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2309" target="_blank">00:38:29.880</a></span> | <span class="t">Now, logistic and hyperbolic tan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2314" target="_blank">00:38:34.740</a></span> | <span class="t">they sound like they're very different things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2317" target="_blank">00:38:37.040</a></span> | <span class="t">but actually, as you maybe remember from a math class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2320" target="_blank">00:38:40.340</a></span> | <span class="t">hyperbolic tan can be represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2322" target="_blank">00:38:42.540</a></span> | <span class="t">in terms of exponentials as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2324" target="_blank">00:38:44.780</a></span> | <span class="t">And if you do a bit of math,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2326" target="_blank">00:38:46.140</a></span> | <span class="t">which possibly we might make you do on an assignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2329" target="_blank">00:38:49.620</a></span> | <span class="t">it's actually the case that a hyperbolic tan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2332" target="_blank">00:38:52.020</a></span> | <span class="t">is just a rescaled and shifted version of the logistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2335" target="_blank">00:38:55.860</a></span> | <span class="t">So it's really exactly the same curve, just squeezed a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2339" target="_blank">00:38:59.400</a></span> | <span class="t">So it goes now symmetrically between minus one and one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2342" target="_blank">00:39:02.880</a></span> | <span class="t">Well, these kinds of transcendental functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2348" target="_blank">00:39:08.700</a></span> | <span class="t">like hyperbolic tan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2351" target="_blank">00:39:11.000</a></span> | <span class="t">they're kind of slow and expensive to compute, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2353" target="_blank">00:39:13.740</a></span> | <span class="t">Even on our fast computers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2355" target="_blank">00:39:15.640</a></span> | <span class="t">calculating exponentials is a bit slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2358" target="_blank">00:39:18.260</a></span> | <span class="t">So something people became interested in was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2361" target="_blank">00:39:21.540</a></span> | <span class="t">well, could we do things with much simpler non-linearity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2365" target="_blank">00:39:25.380</a></span> | <span class="t">So what if we used a so-called hard tan H?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2369" target="_blank">00:39:29.340</a></span> | <span class="t">So the hard tan H,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2371" target="_blank">00:39:31.260</a></span> | <span class="t">up to some point it just flat lines at minus one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2377" target="_blank">00:39:37.140</a></span> | <span class="t">then it is Y equals X up until one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2382" target="_blank">00:39:42.140</a></span> | <span class="t">and then it just flat lines again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2384" target="_blank">00:39:44.700</a></span> | <span class="t">And that seems a slightly weird thing to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2389" target="_blank">00:39:49.480</a></span> | <span class="t">because if your input is over on the left</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2392" target="_blank">00:39:52.900</a></span> | <span class="t">or over on the right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2394" target="_blank">00:39:54.300</a></span> | <span class="t">you're sort of not getting any discrimination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2397" target="_blank">00:39:57.660</a></span> | <span class="t">and if for things giving the same output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2401" target="_blank">00:40:01.040</a></span> | <span class="t">But somewhat surprisingly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2402" target="_blank">00:40:02.880</a></span> | <span class="t">I mean, I was surprised when people started doing this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2406" target="_blank">00:40:06.380</a></span> | <span class="t">these kinds of models proved to be very successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2411" target="_blank">00:40:11.660</a></span> | <span class="t">And so that then led into what's proven to be kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2415" target="_blank">00:40:15.660</a></span> | <span class="t">the most successful and generally widely used non-linearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2419" target="_blank">00:40:19.620</a></span> | <span class="t">in a lot of recent deep learning work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2421" target="_blank">00:40:21.740</a></span> | <span class="t">which was what was being used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2424" target="_blank">00:40:24.660</a></span> | <span class="t">in the dependency parser model I showed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2428" target="_blank">00:40:28.220</a></span> | <span class="t">is what's called the rectified linear unit or ReLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2431" target="_blank">00:40:31.860</a></span> | <span class="t">So a ReLU is kind of the simplest kind of non-linearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2435" target="_blank">00:40:35.140</a></span> | <span class="t">that you can imagine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2436" target="_blank">00:40:36.500</a></span> | <span class="t">So if the value of X is negative, its value is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2440" target="_blank">00:40:40.600</a></span> | <span class="t">So effectively it's just dead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2442" target="_blank">00:40:42.460</a></span> | <span class="t">it's not doing anything in the computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2445" target="_blank">00:40:45.020</a></span> | <span class="t">And if its value of X is greater than zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2450" target="_blank">00:40:50.020</a></span> | <span class="t">then it's just simply Y equals X,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2453" target="_blank">00:40:53.000</a></span> | <span class="t">the value is being passed through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2455" target="_blank">00:40:55.460</a></span> | <span class="t">And at first sight, this might seem really, really weird</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2460" target="_blank">00:41:00.640</a></span> | <span class="t">and how could this be useful as a non-linearity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2464" target="_blank">00:41:04.200</a></span> | <span class="t">But if you sort of think a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2465" target="_blank">00:41:05.900</a></span> | <span class="t">about how you can approximate things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2468" target="_blank">00:41:08.780</a></span> | <span class="t">with piecewise linear functions very accurately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2472" target="_blank">00:41:12.000</a></span> | <span class="t">you might kind of start to see how you could use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2475" target="_blank">00:41:15.180</a></span> | <span class="t">to do accurate function approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2478" target="_blank">00:41:18.540</a></span> | <span class="t">with piecewise linear functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2480" target="_blank">00:41:20.780</a></span> | <span class="t">And that's what ReLU units have been found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2483" target="_blank">00:41:23.760</a></span> | <span class="t">to do extremely, extremely successfully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2487" target="_blank">00:41:27.500</a></span> | <span class="t">So logistic and tanh are still used in various places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2491" target="_blank">00:41:31.860</a></span> | <span class="t">You use logistic when you want a probability output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2494" target="_blank">00:41:34.880</a></span> | <span class="t">We'll see tanh again very soon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2497" target="_blank">00:41:37.460</a></span> | <span class="t">when we get to recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2500" target="_blank">00:41:40.060</a></span> | <span class="t">But they're no longer the default when making deep networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2503" target="_blank">00:41:43.420</a></span> | <span class="t">that in a lot of places,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2504" target="_blank">00:41:44.780</a></span> | <span class="t">the first thing you should think about trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2507" target="_blank">00:41:47.060</a></span> | <span class="t">is ReLU non-linearities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2509" target="_blank">00:41:49.180</a></span> | <span class="t">And so in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2511" target="_blank">00:41:51.180</a></span> | <span class="t">that part of why they're good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2515" target="_blank">00:41:55.940</a></span> | <span class="t">is that ReLU non-networks train very quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2520" target="_blank">00:42:00.740</a></span> | <span class="t">because you get this sort of very straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2523" target="_blank">00:42:03.340</a></span> | <span class="t">gradient backflow because providing you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2525" target="_blank">00:42:05.380</a></span> | <span class="t">on the right-hand side of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2528" target="_blank">00:42:08.260</a></span> | <span class="t">you then just gain this sort of constant gradient backflow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2531" target="_blank">00:42:11.700</a></span> | <span class="t">from the slope one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2533" target="_blank">00:42:13.100</a></span> | <span class="t">And so they train very quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2535" target="_blank">00:42:15.120</a></span> | <span class="t">The somewhat surprising fact is that sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2539" target="_blank">00:42:19.300</a></span> | <span class="t">almost the simplest non-linearity imaginable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2542" target="_blank">00:42:22.300</a></span> | <span class="t">is still enough to have a very good neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2546" target="_blank">00:42:26.300</a></span> | <span class="t">but it just is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2547" target="_blank">00:42:27.940</a></span> | <span class="t">People have played around with variants of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2550" target="_blank">00:42:30.820</a></span> | <span class="t">So people have then played around with leaky ReLUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2554" target="_blank">00:42:34.100</a></span> | <span class="t">where rather than the left-hand side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2557" target="_blank">00:42:37.620</a></span> | <span class="t">just going completely to zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2559" target="_blank">00:42:39.580</a></span> | <span class="t">it goes slightly negative on a much shallower slope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2564" target="_blank">00:42:44.580</a></span> | <span class="t">And then there's been a parametric ReLU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2568" target="_blank">00:42:48.060</a></span> | <span class="t">where you have an extra parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2569" target="_blank">00:42:49.980</a></span> | <span class="t">where you learn the slope of the negative part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2572" target="_blank">00:42:52.480</a></span> | <span class="t">Another thing that's been used recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2576" target="_blank">00:42:56.340</a></span> | <span class="t">is this swish non-linearity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2578" target="_blank">00:42:58.180</a></span> | <span class="t">which looks almost like a ReLU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2581" target="_blank">00:43:01.300</a></span> | <span class="t">but it sort of curves down just a little bit there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2584" target="_blank">00:43:04.780</a></span> | <span class="t">and starts to go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2586" target="_blank">00:43:06.180</a></span> | <span class="t">I mean, I think it's fair to say that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2589" target="_blank">00:43:09.060</a></span> | <span class="t">none of these have really proven themselves vastly superior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2592" target="_blank">00:43:12.580</a></span> | <span class="t">There are papers saying I can get better results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2595" target="_blank">00:43:15.020</a></span> | <span class="t">by using one of these and maybe you can,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2597" target="_blank">00:43:17.780</a></span> | <span class="t">but you know, it's not night and day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2600" target="_blank">00:43:20.020</a></span> | <span class="t">and a vast majority of work that you see around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2602" target="_blank">00:43:22.860</a></span> | <span class="t">is still just using ReLUs in many places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2606" target="_blank">00:43:26.120</a></span> | <span class="t">Okay, a couple more things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2611" target="_blank">00:43:31.260</a></span> | <span class="t">Parameter initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2613" target="_blank">00:43:33.580</a></span> | <span class="t">So in almost all cases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2616" target="_blank">00:43:36.700</a></span> | <span class="t">you must, must, must initialize the matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2621" target="_blank">00:43:41.700</a></span> | <span class="t">of your neural nets with small random values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2626" target="_blank">00:43:46.660</a></span> | <span class="t">Neural nets just don't work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2628" target="_blank">00:43:48.620</a></span> | <span class="t">if you start the matrices off as zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2631" target="_blank">00:43:51.660</a></span> | <span class="t">'cause effectively then everything is symmetric,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2636" target="_blank">00:43:56.060</a></span> | <span class="t">nothing can specialize in different ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2639" target="_blank">00:43:59.900</a></span> | <span class="t">and you then get sort of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2642" target="_blank">00:44:02.100</a></span> | <span class="t">you just don't have an ability for a neural net to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2647" target="_blank">00:44:07.060</a></span> | <span class="t">You sort of get this defective solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2649" target="_blank">00:44:09.940</a></span> | <span class="t">So standardly you're using some methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2653" target="_blank">00:44:13.820</a></span> | <span class="t">such as drawing random numbers uniformly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2657" target="_blank">00:44:17.380</a></span> | <span class="t">between minus R and R for a small value R</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2660" target="_blank">00:44:20.980</a></span> | <span class="t">and just filling in all the parameters with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2664" target="_blank">00:44:24.340</a></span> | <span class="t">Exception is with bias weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2666" target="_blank">00:44:26.820</a></span> | <span class="t">It's fine to set bias weights to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2669" target="_blank">00:44:29.140</a></span> | <span class="t">and in some sense that's better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2670" target="_blank">00:44:30.980</a></span> | <span class="t">In terms of choosing what the R value is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2677" target="_blank">00:44:37.300</a></span> | <span class="t">essentially for traditional neural nets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2680" target="_blank">00:44:40.940</a></span> | <span class="t">what we want to set that R range for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2684" target="_blank">00:44:44.220</a></span> | <span class="t">is so that the numbers in our neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2687" target="_blank">00:44:47.460</a></span> | <span class="t">stay of a reasonable size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2689" target="_blank">00:44:49.700</a></span> | <span class="t">They don't get too big and they don't get too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2693" target="_blank">00:44:53.540</a></span> | <span class="t">And whether they kind of blow up or not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2696" target="_blank">00:44:56.940</a></span> | <span class="t">depends on how many connections there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2699" target="_blank">00:44:59.940</a></span> | <span class="t">in the neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2701" target="_blank">00:45:01.140</a></span> | <span class="t">I'm looking at the fan in and fan out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2703" target="_blank">00:45:03.860</a></span> | <span class="t">of connections in the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2706" target="_blank">00:45:06.860</a></span> | <span class="t">And so a very common initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2711" target="_blank">00:45:11.860</a></span> | <span class="t">that you'll see in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2713" target="_blank">00:45:13.940</a></span> | <span class="t">is what's called Harvey initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2716" target="_blank">00:45:16.780</a></span> | <span class="t">named after a person who suggested that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2720" target="_blank">00:45:20.580</a></span> | <span class="t">And it's working out a value of R</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2723" target="_blank">00:45:23.940</a></span> | <span class="t">based on this fan in and fan out of the layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2728" target="_blank">00:45:28.940</a></span> | <span class="t">but you can just sort of ask for it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2730" target="_blank">00:45:30.940</a></span> | <span class="t">say initialize with this initialization and it will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2734" target="_blank">00:45:34.820</a></span> | <span class="t">This is another area where there have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2736" target="_blank">00:45:36.700</a></span> | <span class="t">some subsequent developments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2739" target="_blank">00:45:39.060</a></span> | <span class="t">So around week five,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2742" target="_blank">00:45:42.220</a></span> | <span class="t">we'll start talking about layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2744" target="_blank">00:45:44.540</a></span> | <span class="t">And if you're using layer normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2746" target="_blank">00:45:46.740</a></span> | <span class="t">then it sort of doesn't matter the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2748" target="_blank">00:45:48.380</a></span> | <span class="t">how you initialize the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2752" target="_blank">00:45:52.140</a></span> | <span class="t">So finally, we have to train our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2756" target="_blank">00:45:56.220</a></span> | <span class="t">And I've briefly introduced the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2758" target="_blank">00:45:58.940</a></span> | <span class="t">of stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2761" target="_blank">00:46:01.060</a></span> | <span class="t">And the good news is that most of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2765" target="_blank">00:46:05.220</a></span> | <span class="t">that if training your networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2767" target="_blank">00:46:07.220</a></span> | <span class="t">with stochastic gradient descent works just fine,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2771" target="_blank">00:46:11.620</a></span> | <span class="t">use it and you will get good results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2774" target="_blank">00:46:14.580</a></span> | <span class="t">However, often that requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2779" target="_blank">00:46:19.500</a></span> | <span class="t">choosing a suitable learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2781" target="_blank">00:46:21.340</a></span> | <span class="t">which is my final slide of tips on the next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2785" target="_blank">00:46:25.220</a></span> | <span class="t">But there's been an enormous amount of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2788" target="_blank">00:46:28.060</a></span> | <span class="t">on optimization of neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2791" target="_blank">00:46:31.460</a></span> | <span class="t">and people have come up with a whole series</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2794" target="_blank">00:46:34.300</a></span> | <span class="t">of more sophisticated optimizers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2798" target="_blank">00:46:38.060</a></span> | <span class="t">And I'm not gonna get into the details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2800" target="_blank">00:46:40.260</a></span> | <span class="t">of optimization in this class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2802" target="_blank">00:46:42.220</a></span> | <span class="t">but the very loose idea is that these optimizers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2806" target="_blank">00:46:46.340</a></span> | <span class="t">are adaptive in that they can kind of keep track</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2810" target="_blank">00:46:50.140</a></span> | <span class="t">of how much slope there was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2812" target="_blank">00:46:52.500</a></span> | <span class="t">how much gradient there is for different parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2816" target="_blank">00:46:56.140</a></span> | <span class="t">And therefore based on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2818" target="_blank">00:46:58.020</a></span> | <span class="t">make decisions as to how much to adjust the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2821" target="_blank">00:47:01.820</a></span> | <span class="t">when doing the gradient update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2823" target="_blank">00:47:03.860</a></span> | <span class="t">rather than adjusting it by a constant amount.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2826" target="_blank">00:47:06.460</a></span> | <span class="t">And so in that family of methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2829" target="_blank">00:47:09.260</a></span> | <span class="t">there are methods that include AdaGrad, RMSProp, Adam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2833" target="_blank">00:47:13.940</a></span> | <span class="t">and then a variance of Adam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2835" target="_blank">00:47:15.700</a></span> | <span class="t">including SparseAdam, AdamW, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2840" target="_blank">00:47:20.460</a></span> | <span class="t">The one called Adam is a pretty good place to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2844" target="_blank">00:47:24.660</a></span> | <span class="t">And a lot of the time that's a good one to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2847" target="_blank">00:47:27.340</a></span> | <span class="t">And again, from the perspective of PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2849" target="_blank">00:47:29.980</a></span> | <span class="t">when you're initializing an optimizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2852" target="_blank">00:47:32.420</a></span> | <span class="t">you can just say, please use Adam</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2854" target="_blank">00:47:34.620</a></span> | <span class="t">and you don't actually need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2856" target="_blank">00:47:36.740</a></span> | <span class="t">much more about it than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2858" target="_blank">00:47:38.660</a></span> | <span class="t">If you are using simple stochastic gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2864" target="_blank">00:47:44.700</a></span> | <span class="t">you have to change, choose a learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2867" target="_blank">00:47:47.100</a></span> | <span class="t">So that was the eta value that you multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2870" target="_blank">00:47:50.980</a></span> | <span class="t">the gradient by for how much to adjust the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2874" target="_blank">00:47:54.500</a></span> | <span class="t">And so I talked about that slightly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2877" target="_blank">00:47:57.020</a></span> | <span class="t">how you didn't want it to be too big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2879" target="_blank">00:47:59.580</a></span> | <span class="t">or your model could diverge or bounce around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2882" target="_blank">00:48:02.740</a></span> | <span class="t">You didn't want it to be too small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2884" target="_blank">00:48:04.940</a></span> | <span class="t">or else training could take place exceedingly slowly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2889" target="_blank">00:48:09.780</a></span> | <span class="t">and you'll miss the assignment deadline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2891" target="_blank">00:48:11.780</a></span> | <span class="t">How big it should be depends on all sorts of details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2896" target="_blank">00:48:16.300</a></span> | <span class="t">of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2897" target="_blank">00:48:17.220</a></span> | <span class="t">And so you sort of wanna try out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2898" target="_blank">00:48:18.820</a></span> | <span class="t">some different order of magnitude numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2902" target="_blank">00:48:22.940</a></span> | <span class="t">to see what numbers seem to work well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2905" target="_blank">00:48:25.820</a></span> | <span class="t">for training stably but reasonably quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2908" target="_blank">00:48:28.940</a></span> | <span class="t">Something around 10 to the minus three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2910" target="_blank">00:48:30.740</a></span> | <span class="t">or 10 to the minus four is in a crazy place to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2914" target="_blank">00:48:34.580</a></span> | <span class="t">In principle, you can do fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2916" target="_blank">00:48:36.220</a></span> | <span class="t">just using a constant learning rate in SGD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2919" target="_blank">00:48:39.500</a></span> | <span class="t">In practice, people generally find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2922" target="_blank">00:48:42.220</a></span> | <span class="t">they can get better results by decreasing learning rates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2925" target="_blank">00:48:45.980</a></span> | <span class="t">as you train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2927" target="_blank">00:48:47.220</a></span> | <span class="t">So a very common recipe is that you half the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2931" target="_blank">00:48:51.660</a></span> | <span class="t">after every K epochs, where an epoch means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2935" target="_blank">00:48:55.060</a></span> | <span class="t">that you've made a pass through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2936" target="_blank">00:48:56.660</a></span> | <span class="t">the entire set of training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2939" target="_blank">00:48:59.060</a></span> | <span class="t">So perhaps something like every three epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2941" target="_blank">00:49:01.700</a></span> | <span class="t">you have the learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2943" target="_blank">00:49:03.620</a></span> | <span class="t">And a final little note there in purple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2947" target="_blank">00:49:07.540</a></span> | <span class="t">is when you make a pass through the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2949" target="_blank">00:49:09.300</a></span> | <span class="t">you don't wanna go through the data items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2951" target="_blank">00:49:11.620</a></span> | <span class="t">in the same order each time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2953" target="_blank">00:49:13.980</a></span> | <span class="t">'cause that leads you just kind of have a sort of patterning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2958" target="_blank">00:49:18.620</a></span> | <span class="t">of the training examples that the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2961" target="_blank">00:49:21.940</a></span> | <span class="t">will sort of fall into that periodicity of those patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2964" target="_blank">00:49:24.980</a></span> | <span class="t">So it's best to shuffle the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2967" target="_blank">00:49:27.380</a></span> | <span class="t">before each pass through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2969" target="_blank">00:49:29.020</a></span> | <span class="t">Okay, there are more sophisticated ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2972" target="_blank">00:49:32.260</a></span> | <span class="t">to set learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2974" target="_blank">00:49:34.900</a></span> | <span class="t">And I won't really get into those now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2978" target="_blank">00:49:38.420</a></span> | <span class="t">Fancier optimizers like Adam also have a learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2982" target="_blank">00:49:42.100</a></span> | <span class="t">So you still have to choose a learning rate value,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2985" target="_blank">00:49:45.180</a></span> | <span class="t">but it's effectively, it's an initial learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2987" target="_blank">00:49:47.900</a></span> | <span class="t">which typically the optimizer shrinks as it runs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2991" target="_blank">00:49:51.060</a></span> | <span class="t">And so you commonly want to have the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2993" target="_blank">00:49:53.660</a></span> | <span class="t">it starts off with beyond the larger size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2996" target="_blank">00:49:56.180</a></span> | <span class="t">because it'll be shrinking as it goes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=2998" target="_blank">00:49:58.940</a></span> | <span class="t">Okay, so that's all by way of introduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3004" target="_blank">00:50:04.060</a></span> | <span class="t">And I'm now ready to start on language models and RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3008" target="_blank">00:50:08.020</a></span> | <span class="t">So what is language modeling?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3010" target="_blank">00:50:10.260</a></span> | <span class="t">I mean, as two words of English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3012" target="_blank">00:50:12.020</a></span> | <span class="t">language modeling could mean just about anything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3015" target="_blank">00:50:15.220</a></span> | <span class="t">but in the natural language processing literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3018" target="_blank">00:50:18.860</a></span> | <span class="t">language modeling has a very precise technical definition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3023" target="_blank">00:50:23.060</a></span> | <span class="t">which you should know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3024" target="_blank">00:50:24.380</a></span> | <span class="t">So language modeling is the task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3027" target="_blank">00:50:27.500</a></span> | <span class="t">of predicting the word that comes next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3030" target="_blank">00:50:30.860</a></span> | <span class="t">So if you have some context, like the students open there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3037" target="_blank">00:50:37.180</a></span> | <span class="t">you want to be able to predict what words will come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3040" target="_blank">00:50:40.740</a></span> | <span class="t">Is it their books, their laptops, their exams, their minds?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3045" target="_blank">00:50:45.740</a></span> | <span class="t">And so in particular, what you want to be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3051" target="_blank">00:50:51.060</a></span> | <span class="t">is being able to give a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3054" target="_blank">00:50:54.100</a></span> | <span class="t">that different words will occur in this context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3058" target="_blank">00:50:58.460</a></span> | <span class="t">So a language model is a probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3062" target="_blank">00:51:02.980</a></span> | <span class="t">over next words given a preceding context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3067" target="_blank">00:51:07.460</a></span> | <span class="t">And a system that does that is called a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3073" target="_blank">00:51:13.740</a></span> | <span class="t">So as a result of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3077" target="_blank">00:51:17.780</a></span> | <span class="t">you can also think of a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3079" target="_blank">00:51:19.820</a></span> | <span class="t">as a system that assigns a probability score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3083" target="_blank">00:51:23.020</a></span> | <span class="t">to a piece of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3084" target="_blank">00:51:24.460</a></span> | <span class="t">So if we have a piece of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3086" target="_blank">00:51:26.660</a></span> | <span class="t">then we can just work out its probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3089" target="_blank">00:51:29.020</a></span> | <span class="t">according to a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3090" target="_blank">00:51:30.820</a></span> | <span class="t">So the probability of a sequence of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3093" target="_blank">00:51:33.580</a></span> | <span class="t">we can decompose via the chain rule,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3096" target="_blank">00:51:36.860</a></span> | <span class="t">probability of the first times probability of the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3099" target="_blank">00:51:39.940</a></span> | <span class="t">given the first, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3102" target="_blank">00:51:42.020</a></span> | <span class="t">And then we can work that out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3104" target="_blank">00:51:44.340</a></span> | <span class="t">using what our language model provides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3106" target="_blank">00:51:46.980</a></span> | <span class="t">as a product of each probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3109" target="_blank">00:51:49.340</a></span> | <span class="t">of predicting the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3110" target="_blank">00:51:50.980</a></span> | <span class="t">Okay, language models are really the cornerstone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3117" target="_blank">00:51:57.500</a></span> | <span class="t">of human language technology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3120" target="_blank">00:52:00.620</a></span> | <span class="t">Everything that you do with computers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3124" target="_blank">00:52:04.100</a></span> | <span class="t">that involves human language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3126" target="_blank">00:52:06.380</a></span> | <span class="t">you are using language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3129" target="_blank">00:52:09.300</a></span> | <span class="t">So when you're using your phone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3131" target="_blank">00:52:11.780</a></span> | <span class="t">and it's suggesting whether well or badly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3134" target="_blank">00:52:14.740</a></span> | <span class="t">what the next word that you probably want to type is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3137" target="_blank">00:52:17.580</a></span> | <span class="t">that's a language model working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3139" target="_blank">00:52:19.900</a></span> | <span class="t">to try and predict the likely next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3143" target="_blank">00:52:23.300</a></span> | <span class="t">When the same thing happens in a Google Doc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3146" target="_blank">00:52:26.140</a></span> | <span class="t">and it's suggesting a next word or a next few words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3149" target="_blank">00:52:29.460</a></span> | <span class="t">that's a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3151" target="_blank">00:52:31.140</a></span> | <span class="t">The main reason why the one in Google Docs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3155" target="_blank">00:52:35.580</a></span> | <span class="t">works much better than the one on your phone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3158" target="_blank">00:52:38.060</a></span> | <span class="t">is that for the keyboard phone models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3160" target="_blank">00:52:40.660</a></span> | <span class="t">they have to be very compact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3162" target="_blank">00:52:42.940</a></span> | <span class="t">so they can run quickly and not much memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3166" target="_blank">00:52:46.060</a></span> | <span class="t">So they're sort of only mediocre language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3169" target="_blank">00:52:49.020</a></span> | <span class="t">where something like Google Docs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3170" target="_blank">00:52:50.780</a></span> | <span class="t">can do a much better language modeling job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3173" target="_blank">00:52:53.460</a></span> | <span class="t">Query completion, same thing, there's a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3179" target="_blank">00:52:59.260</a></span> | <span class="t">And so then the question is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3180" target="_blank">00:53:00.860</a></span> | <span class="t">well, how do we build language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3184" target="_blank">00:53:04.980</a></span> | <span class="t">And so I briefly wanted to first again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3188" target="_blank">00:53:08.660</a></span> | <span class="t">give the traditional answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3191" target="_blank">00:53:11.300</a></span> | <span class="t">since you should have at least some understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3193" target="_blank">00:53:13.660</a></span> | <span class="t">of how NLP was done without a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3197" target="_blank">00:53:17.740</a></span> | <span class="t">And the traditional answer that powered speech recognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3202" target="_blank">00:53:22.340</a></span> | <span class="t">and other applications for at least two decades,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3205" target="_blank">00:53:25.540</a></span> | <span class="t">three decades really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3207" target="_blank">00:53:27.420</a></span> | <span class="t">was what were called N-gram language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3210" target="_blank">00:53:30.420</a></span> | <span class="t">And these were very simple, but still quite effective idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3214" target="_blank">00:53:34.940</a></span> | <span class="t">So we want to give probabilities of next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3218" target="_blank">00:53:38.100</a></span> | <span class="t">So what we're gonna work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3222" target="_blank">00:53:42.500</a></span> | <span class="t">is what are referred to as N-grams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3224" target="_blank">00:53:44.780</a></span> | <span class="t">And so N-grams is just a chunk of N consecutive words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3229" target="_blank">00:53:49.580</a></span> | <span class="t">which are usually referred to as unigrams, bigrams,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3232" target="_blank">00:53:52.340</a></span> | <span class="t">trigrams, and then four grams and five grams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3235" target="_blank">00:53:55.980</a></span> | <span class="t">A horrible set of names, which would offend any humanist,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3240" target="_blank">00:54:00.700</a></span> | <span class="t">but that's what people normally say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3242" target="_blank">00:54:02.740</a></span> | <span class="t">And so effectively what we do is just collect statistics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3247" target="_blank">00:54:07.980</a></span> | <span class="t">about how often different N-grams occur</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3251" target="_blank">00:54:11.420</a></span> | <span class="t">in a large amount of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3253" target="_blank">00:54:13.180</a></span> | <span class="t">and then use those to build a probability model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3256" target="_blank">00:54:16.060</a></span> | <span class="t">So the first thing we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3258" target="_blank">00:54:18.940</a></span> | <span class="t">is what's referred to as making a Markov assumption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3261" target="_blank">00:54:21.900</a></span> | <span class="t">So these are also referred to as Markov models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3264" target="_blank">00:54:24.820</a></span> | <span class="t">And we decide that the word in position T plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3269" target="_blank">00:54:29.380</a></span> | <span class="t">only depends on the preceding N minus one words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3273" target="_blank">00:54:33.380</a></span> | <span class="t">So if we want to predict T plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3278" target="_blank">00:54:38.820</a></span> | <span class="t">given the entire preceding text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3281" target="_blank">00:54:41.980</a></span> | <span class="t">we actually throw away the early words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3285" target="_blank">00:54:45.100</a></span> | <span class="t">and just use the preceding N minus one words as context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3289" target="_blank">00:54:49.540</a></span> | <span class="t">Well, once we've made that simplification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3292" target="_blank">00:54:52.260</a></span> | <span class="t">we can then just use the definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3294" target="_blank">00:54:54.100</a></span> | <span class="t">of conditional probability and say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3296" target="_blank">00:54:56.380</a></span> | <span class="t">all that conditional probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3298" target="_blank">00:54:58.540</a></span> | <span class="t">is the probability of N words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3301" target="_blank">00:55:01.940</a></span> | <span class="t">divided by the preceding N minus one words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3306" target="_blank">00:55:06.940</a></span> | <span class="t">And so we have the probability of an N-gram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3309" target="_blank">00:55:09.500</a></span> | <span class="t">over the probability of an N minus one gram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3312" target="_blank">00:55:12.700</a></span> | <span class="t">And so then how do we get these N-gram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3315" target="_blank">00:55:15.900</a></span> | <span class="t">and N minus one gram probabilities?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3318" target="_blank">00:55:18.140</a></span> | <span class="t">We simply take a larger amount of text in some language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3322" target="_blank">00:55:22.460</a></span> | <span class="t">and we count how often the different N-grams occur.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3326" target="_blank">00:55:26.100</a></span> | <span class="t">And so our crude statistical approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3330" target="_blank">00:55:30.660</a></span> | <span class="t">starts off as the count of the N-gram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3333" target="_blank">00:55:33.740</a></span> | <span class="t">over the count of the N minus one gram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3337" target="_blank">00:55:37.340</a></span> | <span class="t">So here's an example of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3338" target="_blank">00:55:38.980</a></span> | <span class="t">Suppose we are learning a four-gram language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3342" target="_blank">00:55:42.020</a></span> | <span class="t">Okay, so we throw away all words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3344" target="_blank">00:55:44.540</a></span> | <span class="t">apart from the last three words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3347" target="_blank">00:55:47.020</a></span> | <span class="t">and they're our conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3349" target="_blank">00:55:49.020</a></span> | <span class="t">We look in some large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3354" target="_blank">00:55:54.020</a></span> | <span class="t">we use the counts from some large training corpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3357" target="_blank">00:55:57.700</a></span> | <span class="t">and we see how often did students open their books occur?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3361" target="_blank">00:56:01.820</a></span> | <span class="t">How often did students open their minds occur?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3364" target="_blank">00:56:04.900</a></span> | <span class="t">And then for each of those counts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3367" target="_blank">00:56:07.020</a></span> | <span class="t">we divide through by the count</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3368" target="_blank">00:56:08.740</a></span> | <span class="t">of how often students open their occurred.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3371" target="_blank">00:56:11.420</a></span> | <span class="t">And that gives us our probability estimates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3375" target="_blank">00:56:15.980</a></span> | <span class="t">So for example, if in the corpus students open,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3379" target="_blank">00:56:19.500</a></span> | <span class="t">they occurred a thousand times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3382" target="_blank">00:56:22.060</a></span> | <span class="t">students open their books occurred 400 times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3385" target="_blank">00:56:25.660</a></span> | <span class="t">we'd get a probability estimate of 0.4 for books.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3389" target="_blank">00:56:29.140</a></span> | <span class="t">If exams occurred a hundred times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3390" target="_blank">00:56:30.740</a></span> | <span class="t">we'd get 0.1 for exams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3393" target="_blank">00:56:33.540</a></span> | <span class="t">And we sort of see here already the disadvantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3397" target="_blank">00:56:37.260</a></span> | <span class="t">of having made the Markov assumption</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3399" target="_blank">00:56:39.580</a></span> | <span class="t">and have gotten rid of all of this earlier context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3403" target="_blank">00:56:43.620</a></span> | <span class="t">which would have been useful for helping us to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3406" target="_blank">00:56:46.460</a></span> | <span class="t">The one other point that I'll just mention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3412" target="_blank">00:56:52.140</a></span> | <span class="t">that I can fuse myself on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3414" target="_blank">00:56:54.140</a></span> | <span class="t">is this count of the N-gram language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3418" target="_blank">00:56:58.300</a></span> | <span class="t">So for a four-gram language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3420" target="_blank">00:57:00.940</a></span> | <span class="t">it's called a four-gram language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3423" target="_blank">00:57:03.340</a></span> | <span class="t">because in its estimation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3425" target="_blank">00:57:05.460</a></span> | <span class="t">you're using four grams in the numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3428" target="_blank">00:57:08.340</a></span> | <span class="t">and trigrams in the denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3431" target="_blank">00:57:11.460</a></span> | <span class="t">So you use the size of the numerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3434" target="_blank">00:57:14.940</a></span> | <span class="t">So that terminology is different to the terminology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3438" target="_blank">00:57:18.900</a></span> | <span class="t">that's used in Markov models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3441" target="_blank">00:57:21.020</a></span> | <span class="t">So when people talk about the order of a Markov model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3444" target="_blank">00:57:24.700</a></span> | <span class="t">that refers to the amount of context you're using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3448" target="_blank">00:57:28.220</a></span> | <span class="t">So this would correspond to a third order Markov model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3452" target="_blank">00:57:32.460</a></span> | <span class="t">Yeah, so someone said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3456" target="_blank">00:57:36.260</a></span> | <span class="t">"Is this similar to a Naive Bayes model?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3460" target="_blank">00:57:40.780</a></span> | <span class="t">Sort of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3462" target="_blank">00:57:42.260</a></span> | <span class="t">Naive Bayes models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3463" target="_blank">00:57:43.380</a></span> | <span class="t">you also estimate the probabilities just by counting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3466" target="_blank">00:57:46.860</a></span> | <span class="t">So they're related</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3469" target="_blank">00:57:49.980</a></span> | <span class="t">and there are sort of in some sense, two differences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3474" target="_blank">00:57:54.740</a></span> | <span class="t">The first difference or specialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3479" target="_blank">00:57:59.340</a></span> | <span class="t">is that Naive Bayes models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3482" target="_blank">00:58:02.300</a></span> | <span class="t">work out probabilities of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3485" target="_blank">00:58:05.980</a></span> | <span class="t">independent of their neighbors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3487" target="_blank">00:58:07.620</a></span> | <span class="t">So in one part that a Naive Bayes language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3492" target="_blank">00:58:12.100</a></span> | <span class="t">is a unigram language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3494" target="_blank">00:58:14.260</a></span> | <span class="t">So you're just using the counts of individual words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3497" target="_blank">00:58:17.340</a></span> | <span class="t">But the other part of a Naive Bayes model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3500" target="_blank">00:58:20.340</a></span> | <span class="t">is you're learning a different set of unigram counts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3504" target="_blank">00:58:24.500</a></span> | <span class="t">for every class for your classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3507" target="_blank">00:58:27.820</a></span> | <span class="t">And so you've then got sort of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3513" target="_blank">00:58:33.020</a></span> | <span class="t">so effectively a Naive Bayes model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3515" target="_blank">00:58:35.380</a></span> | <span class="t">is you've got class specific unigram language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3520" target="_blank">00:58:40.380</a></span> | <span class="t">Okay, I gave this as a simple statistical model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3528" target="_blank">00:58:48.140</a></span> | <span class="t">for estimating your probabilities with an N-gram model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3532" target="_blank">00:58:52.140</a></span> | <span class="t">You can't actually get away with just doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3534" target="_blank">00:58:54.940</a></span> | <span class="t">because you have sparsity problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3537" target="_blank">00:58:57.380</a></span> | <span class="t">So, often it'll be the case that for many words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3541" target="_blank">00:59:01.460</a></span> | <span class="t">students open their books</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3543" target="_blank">00:59:03.140</a></span> | <span class="t">or students opened their backpacks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3547" target="_blank">00:59:07.180</a></span> | <span class="t">just never occurred in the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3549" target="_blank">00:59:09.620</a></span> | <span class="t">That if you think about it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3550" target="_blank">00:59:10.700</a></span> | <span class="t">if you have something like 10 to the fifth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3553" target="_blank">00:59:13.740</a></span> | <span class="t">different words even,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3555" target="_blank">00:59:15.380</a></span> | <span class="t">and you want to have then a sequence of four words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3558" target="_blank">00:59:18.380</a></span> | <span class="t">or a problem and there are 10 to the fifth of each,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3560" target="_blank">00:59:20.820</a></span> | <span class="t">there's sort of 10 to the 20th different combinations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3564" target="_blank">00:59:24.260</a></span> | <span class="t">So unless you're seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3565" target="_blank">00:59:25.620</a></span> | <span class="t">and it's truly astronomical amount of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3568" target="_blank">00:59:28.780</a></span> | <span class="t">most four word sequences you've never seen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3571" target="_blank">00:59:31.700</a></span> | <span class="t">So then your numerator will be zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3574" target="_blank">00:59:34.380</a></span> | <span class="t">and your probability estimate will be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3576" target="_blank">00:59:36.700</a></span> | <span class="t">And so that's bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3578" target="_blank">00:59:38.060</a></span> | <span class="t">And so the commonest way of solving that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3580" target="_blank">00:59:40.220</a></span> | <span class="t">is just to add a little Delta to every count</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3583" target="_blank">00:59:43.100</a></span> | <span class="t">and then everything is non-zero and that's called smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3586" target="_blank">00:59:46.580</a></span> | <span class="t">But well, sometimes it's worse than that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3589" target="_blank">00:59:49.820</a></span> | <span class="t">'cause sometimes you won't even have seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3591" target="_blank">00:59:51.820</a></span> | <span class="t">students open theirs and that's more problematic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3595" target="_blank">00:59:55.020</a></span> | <span class="t">'cause that means our denominator is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3598" target="_blank">00:59:58.740</a></span> | <span class="t">And so the division will be ill-defined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3601" target="_blank">01:00:01.740</a></span> | <span class="t">and we can't usefully calculate any probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3605" target="_blank">01:00:05.020</a></span> | <span class="t">in a context that we've never seen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3607" target="_blank">01:00:07.340</a></span> | <span class="t">And so the standard solution to that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3609" target="_blank">01:00:09.580</a></span> | <span class="t">is to shorten the context and that's called back off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3613" target="_blank">01:00:13.420</a></span> | <span class="t">So we condition only on open there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3616" target="_blank">01:00:16.580</a></span> | <span class="t">or if we still haven't seen the open there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3620" target="_blank">01:00:20.100</a></span> | <span class="t">we'll condition only on there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3622" target="_blank">01:00:22.460</a></span> | <span class="t">or we could just forget all conditioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3625" target="_blank">01:00:25.220</a></span> | <span class="t">and actually use a unigram model for our probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3628" target="_blank">01:00:28.540</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3631" target="_blank">01:00:31.500</a></span> | <span class="t">And so as you increase the order N</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3637" target="_blank">01:00:37.100</a></span> | <span class="t">of the N-gram language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3638" target="_blank">01:00:38.740</a></span> | <span class="t">these sparsity problems become worse and worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3642" target="_blank">01:00:42.180</a></span> | <span class="t">So in the early days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3643" target="_blank">01:00:43.660</a></span> | <span class="t">people normally worked with trigram models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3646" target="_blank">01:00:46.300</a></span> | <span class="t">As it became easier to collect billions of words of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3650" target="_blank">01:00:50.540</a></span> | <span class="t">people commonly moved to five-gram models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3653" target="_blank">01:00:53.420</a></span> | <span class="t">But every time you go up an order of conditioning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3658" target="_blank">01:00:58.420</a></span> | <span class="t">you effectively need to be collecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3661" target="_blank">01:01:01.580</a></span> | <span class="t">orders of magnitude more data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3663" target="_blank">01:01:03.340</a></span> | <span class="t">because of the size of the vocabularies of human languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3667" target="_blank">01:01:07.060</a></span> | <span class="t">There's also a problem that these models are huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3673" target="_blank">01:01:13.700</a></span> | <span class="t">So you basically have to be storing counts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3677" target="_blank">01:01:17.460</a></span> | <span class="t">of all of these words sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3679" target="_blank">01:01:19.500</a></span> | <span class="t">so you can work out these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3682" target="_blank">01:01:22.100</a></span> | <span class="t">And I mean, that's actually had a big effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3684" target="_blank">01:01:24.340</a></span> | <span class="t">in terms of what technology is available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3687" target="_blank">01:01:27.620</a></span> | <span class="t">So in the 2000s decade up till about whenever it was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3692" target="_blank">01:01:32.620</a></span> | <span class="t">2014, that there was already Google Translate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3697" target="_blank">01:01:37.620</a></span> | <span class="t">using probabilistic models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3700" target="_blank">01:01:40.940</a></span> | <span class="t">that included language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3703" target="_blank">01:01:43.220</a></span> | <span class="t">of the N-gram language model sort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3705" target="_blank">01:01:45.260</a></span> | <span class="t">But the only way they could possibly be run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3707" target="_blank">01:01:47.980</a></span> | <span class="t">is in the cloud</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3709" target="_blank">01:01:49.620</a></span> | <span class="t">because you needed to have these huge tables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3712" target="_blank">01:01:52.980</a></span> | <span class="t">of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3714" target="_blank">01:01:54.660</a></span> | <span class="t">But now we have neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3715" target="_blank">01:01:55.980</a></span> | <span class="t">and you can have Google Translate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3717" target="_blank">01:01:57.820</a></span> | <span class="t">just actually run on your phone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3719" target="_blank">01:01:59.780</a></span> | <span class="t">And that's possible because neural net models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3723" target="_blank">01:02:03.620</a></span> | <span class="t">can be massively more compact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3725" target="_blank">01:02:05.700</a></span> | <span class="t">than these old N-gram language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3728" target="_blank">01:02:08.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3733" target="_blank">01:02:13.420</a></span> | <span class="t">But nevertheless, before we get onto the neural models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3736" target="_blank">01:02:16.860</a></span> | <span class="t">let's just sort of look at the example of how these work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3741" target="_blank">01:02:21.860</a></span> | <span class="t">So it's trivial to train an N-gram language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3747" target="_blank">01:02:27.860</a></span> | <span class="t">'cause you really just count how often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3749" target="_blank">01:02:29.900</a></span> | <span class="t">word sequences occur in a corpus and you're ready to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3753" target="_blank">01:02:33.260</a></span> | <span class="t">So these models can be trained in seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3755" target="_blank">01:02:35.460</a></span> | <span class="t">That's really good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3756" target="_blank">01:02:36.540</a></span> | <span class="t">That's not like sitting around for training neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3759" target="_blank">01:02:39.540</a></span> | <span class="t">So if I train on my laptop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3762" target="_blank">01:02:42.780</a></span> | <span class="t">a small language model on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3765" target="_blank">01:02:45.500</a></span> | <span class="t">you know, about 1.7 million words as a trigram model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3770" target="_blank">01:02:50.500</a></span> | <span class="t">I can then ask it to generate text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3773" target="_blank">01:02:53.420</a></span> | <span class="t">If I give it a couple of words today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3775" target="_blank">01:02:55.900</a></span> | <span class="t">I can then get it to sort of suggest a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3779" target="_blank">01:02:59.380</a></span> | <span class="t">that might come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3780" target="_blank">01:03:00.860</a></span> | <span class="t">And the way I do that is the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3783" target="_blank">01:03:03.780</a></span> | <span class="t">knows the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3786" target="_blank">01:03:06.180</a></span> | <span class="t">of things that can come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3788" target="_blank">01:03:08.820</a></span> | <span class="t">Now there's a kind of a crude probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3792" target="_blank">01:03:12.260</a></span> | <span class="t">I mean, 'cause effectively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3794" target="_blank">01:03:14.220</a></span> | <span class="t">over this relatively small corpus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3796" target="_blank">01:03:16.740</a></span> | <span class="t">there were things that occurred once, Italian and Emirate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3800" target="_blank">01:03:20.220</a></span> | <span class="t">There are things that occurred twice, price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3802" target="_blank">01:03:22.540</a></span> | <span class="t">There were things that occurred four times, company and bank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3806" target="_blank">01:03:26.420</a></span> | <span class="t">It's sort of fairly crude and rough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3808" target="_blank">01:03:28.580</a></span> | <span class="t">but I nevertheless get probability estimates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3811" target="_blank">01:03:31.460</a></span> | <span class="t">I can then say, okay, based on this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3816" target="_blank">01:03:36.380</a></span> | <span class="t">let's take this probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3819" target="_blank">01:03:39.660</a></span> | <span class="t">and then we'll just sample the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3821" target="_blank">01:03:41.980</a></span> | <span class="t">So the two most likely words to sample,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3824" target="_blank">01:03:44.380</a></span> | <span class="t">a company or bank, but we're rolling the dice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3828" target="_blank">01:03:48.140</a></span> | <span class="t">and we might get any of the words that had come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3830" target="_blank">01:03:50.740</a></span> | <span class="t">So maybe I sample price.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3833" target="_blank">01:03:53.860</a></span> | <span class="t">Now I'll condition on price, the price,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3837" target="_blank">01:03:57.740</a></span> | <span class="t">and look up the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3840" target="_blank">01:04:00.500</a></span> | <span class="t">of what comes next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3841" target="_blank">01:04:01.940</a></span> | <span class="t">The most likely thing is of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3843" target="_blank">01:04:03.940</a></span> | <span class="t">And so again, I'll sample,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3845" target="_blank">01:04:05.700</a></span> | <span class="t">and maybe this time I'll pick up of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3849" target="_blank">01:04:09.300</a></span> | <span class="t">And then I will now condition on price of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3852" target="_blank">01:04:12.980</a></span> | <span class="t">and I will look up the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3856" target="_blank">01:04:16.140</a></span> | <span class="t">of words following that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3858" target="_blank">01:04:18.140</a></span> | <span class="t">And I get this probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3861" target="_blank">01:04:21.060</a></span> | <span class="t">and I'll sample randomly some word from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3864" target="_blank">01:04:24.380</a></span> | <span class="t">And maybe this time I'll sample a rare,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3867" target="_blank">01:04:27.460</a></span> | <span class="t">but possible one like gold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3869" target="_blank">01:04:29.740</a></span> | <span class="t">And I can keep on going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3871" target="_blank">01:04:31.820</a></span> | <span class="t">and I'll get out something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3874" target="_blank">01:04:34.220</a></span> | <span class="t">Today, the price of gold per tonne</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3876" target="_blank">01:04:36.580</a></span> | <span class="t">while production of shoe lasts and shoe industry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3879" target="_blank">01:04:39.660</a></span> | <span class="t">the bank intervened just after it considered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3881" target="_blank">01:04:41.860</a></span> | <span class="t">and rejected an IMF demand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3883" target="_blank">01:04:43.820</a></span> | <span class="t">to rebuild depleted European stocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3886" target="_blank">01:04:46.500</a></span> | <span class="t">SEP 30 N primary 76 cents a share.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3890" target="_blank">01:04:50.220</a></span> | <span class="t">So what just a simple trigram model can produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3894" target="_blank">01:04:54.500</a></span> | <span class="t">over not very much text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3896" target="_blank">01:04:56.620</a></span> | <span class="t">is actually already kind of interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3899" target="_blank">01:04:59.180</a></span> | <span class="t">Like it's actually surprisingly grammatical, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3902" target="_blank">01:05:02.220</a></span> | <span class="t">There are whole pieces of it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3903" target="_blank">01:05:03.740</a></span> | <span class="t">while production of shoe lasts and shoe industry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3907" target="_blank">01:05:07.020</a></span> | <span class="t">the bank intervene just after it considered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3909" target="_blank">01:05:09.620</a></span> | <span class="t">and rejected an IMF demand, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3911" target="_blank">01:05:11.540</a></span> | <span class="t">It's really actually pretty good grammatical text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3914" target="_blank">01:05:14.420</a></span> | <span class="t">So it's sort of amazing that these simple N-gram models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3917" target="_blank">01:05:17.980</a></span> | <span class="t">actually can model a lot of human language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3921" target="_blank">01:05:21.940</a></span> | <span class="t">On the other hand, it's not a very good piece of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3925" target="_blank">01:05:25.060</a></span> | <span class="t">It's completely incoherent and makes no sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3929" target="_blank">01:05:29.180</a></span> | <span class="t">And so to actually be able to generate text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3933" target="_blank">01:05:33.060</a></span> | <span class="t">that seems like it makes sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3935" target="_blank">01:05:35.500</a></span> | <span class="t">we're going to need a considerably better language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3938" target="_blank">01:05:38.900</a></span> | <span class="t">And that's precisely what neural language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3942" target="_blank">01:05:42.540</a></span> | <span class="t">have allowed us to build as we'll see later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3945" target="_blank">01:05:45.060</a></span> | <span class="t">Okay, so how can we build a neural language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3950" target="_blank">01:05:50.380</a></span> | <span class="t">And so first of all, we're gonna do a simple one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3954" target="_blank">01:05:54.060</a></span> | <span class="t">and then we'll see where we get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3956" target="_blank">01:05:56.300</a></span> | <span class="t">but to move into a current neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3958" target="_blank">01:05:58.500</a></span> | <span class="t">might still take us to the next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3962" target="_blank">01:06:02.420</a></span> | <span class="t">So we've gonna have input sequence of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3965" target="_blank">01:06:05.540</a></span> | <span class="t">and we want a probability distribution over the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3970" target="_blank">01:06:10.140</a></span> | <span class="t">Well, the simplest thing that we could try is to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3973" target="_blank">01:06:13.700</a></span> | <span class="t">well, kind of the only tool we have so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3977" target="_blank">01:06:17.420</a></span> | <span class="t">is a window-based classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3979" target="_blank">01:06:19.740</a></span> | <span class="t">So what we can say, what we'd done previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3984" target="_blank">01:06:24.140</a></span> | <span class="t">either for our named entity recognized in lecture three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3988" target="_blank">01:06:28.660</a></span> | <span class="t">or what I just showed you for the dependency parser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3991" target="_blank">01:06:31.340</a></span> | <span class="t">is we have some context window,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3993" target="_blank">01:06:33.900</a></span> | <span class="t">we put it through a neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3995" target="_blank">01:06:35.660</a></span> | <span class="t">and we predict something as a classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=3998" target="_blank">01:06:38.380</a></span> | <span class="t">So before we were predicting a location,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4001" target="_blank">01:06:41.900</a></span> | <span class="t">but maybe instead we could reuse exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4005" target="_blank">01:06:45.580</a></span> | <span class="t">the same technology and say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4008" target="_blank">01:06:48.460</a></span> | <span class="t">we're going to have a window-based classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4011" target="_blank">01:06:51.060</a></span> | <span class="t">So we're discarding the further away words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4013" target="_blank">01:06:53.620</a></span> | <span class="t">just like in N-gram language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4017" target="_blank">01:06:57.820</a></span> | <span class="t">but we'll feed this fixed window into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4022" target="_blank">01:07:02.100</a></span> | <span class="t">So we can catenate the word embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4025" target="_blank">01:07:05.540</a></span> | <span class="t">we put it through a hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4027" target="_blank">01:07:07.660</a></span> | <span class="t">and then we have a softmax classifier over our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4032" target="_blank">01:07:12.660</a></span> | <span class="t">And so now rather than predicting something like location</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4038" target="_blank">01:07:18.540</a></span> | <span class="t">or left arc in the dependency parser,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4041" target="_blank">01:07:21.900</a></span> | <span class="t">we're going to have a softmax over the entire vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4045" target="_blank">01:07:25.780</a></span> | <span class="t">sort of like we did with the skip-gram negative sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4049" target="_blank">01:07:29.380</a></span> | <span class="t">model in the first two lectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4051" target="_blank">01:07:31.900</a></span> | <span class="t">And so we're going to see this choice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4055" target="_blank">01:07:35.100</a></span> | <span class="t">as predicting what word that comes next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4057" target="_blank">01:07:37.820</a></span> | <span class="t">whether it produces laptops, minds, books, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4062" target="_blank">01:07:42.100</a></span> | <span class="t">Okay, so this is a fairly simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4066" target="_blank">01:07:46.540</a></span> | <span class="t">fixed window neural net classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4069" target="_blank">01:07:49.100</a></span> | <span class="t">but this is essentially a famous early model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4075" target="_blank">01:07:55.540</a></span> | <span class="t">in the use of neural nets for NLP applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4080" target="_blank">01:08:00.100</a></span> | <span class="t">So first the 2000 conference paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4082" target="_blank">01:08:02.580</a></span> | <span class="t">and then a somewhat later journal paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4085" target="_blank">01:08:05.940</a></span> | <span class="t">Yoshua Bengio and colleagues introduced precisely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4089" target="_blank">01:08:09.740</a></span> | <span class="t">this model as the neural probabilistic language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4094" target="_blank">01:08:14.180</a></span> | <span class="t">And they were already able to show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4097" target="_blank">01:08:17.140</a></span> | <span class="t">that this could give interesting good results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4100" target="_blank">01:08:20.220</a></span> | <span class="t">for language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4101" target="_blank">01:08:21.780</a></span> | <span class="t">And so it wasn't a great solution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4105" target="_blank">01:08:25.300</a></span> | <span class="t">for neural language modeling, but it still had value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4109" target="_blank">01:08:29.220</a></span> | <span class="t">So it didn't solve the problem of allowing us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4112" target="_blank">01:08:32.380</a></span> | <span class="t">to have bigger context to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4114" target="_blank">01:08:34.780</a></span> | <span class="t">what words are going to come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4116" target="_blank">01:08:36.980</a></span> | <span class="t">It's in that way limited exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4120" target="_blank">01:08:40.660</a></span> | <span class="t">like an N-gram language model is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4123" target="_blank">01:08:43.260</a></span> | <span class="t">but it does have all the advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4125" target="_blank">01:08:45.540</a></span> | <span class="t">of distributed representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4127" target="_blank">01:08:47.900</a></span> | <span class="t">So rather than having these counts for word sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4132" target="_blank">01:08:52.700</a></span> | <span class="t">that are very sparse and very crude,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4136" target="_blank">01:08:56.620</a></span> | <span class="t">we can use distributed representations of words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4140" target="_blank">01:09:00.820</a></span> | <span class="t">which then make predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4143" target="_blank">01:09:03.100</a></span> | <span class="t">that semantically similar words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4145" target="_blank">01:09:05.260</a></span> | <span class="t">should give similar probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4149" target="_blank">01:09:09.020</a></span> | <span class="t">So the idea of that is if we'd use some other word here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4153" target="_blank">01:09:13.340</a></span> | <span class="t">like maybe the pupils open there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4156" target="_blank">01:09:16.980</a></span> | <span class="t">well, maybe in our training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4159" target="_blank">01:09:19.180</a></span> | <span class="t">we'd seen sentences about students,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4161" target="_blank">01:09:21.860</a></span> | <span class="t">but we'd never seen sentences about pupils.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4164" target="_blank">01:09:24.580</a></span> | <span class="t">An N-gram language model then would sort of have no idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4167" target="_blank">01:09:27.700</a></span> | <span class="t">what probabilities to use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4169" target="_blank">01:09:29.660</a></span> | <span class="t">whereas a neural language model can say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4172" target="_blank">01:09:32.900</a></span> | <span class="t">well, pupils is kind of similar to students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4175" target="_blank">01:09:35.500</a></span> | <span class="t">Therefore I can predict similarly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4177" target="_blank">01:09:37.300</a></span> | <span class="t">to what I would have predicted for students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4179" target="_blank">01:09:39.500</a></span> | <span class="t">Okay, so there's now no sparsity problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4185" target="_blank">01:09:45.460</a></span> | <span class="t">We don't need to store billions of N-gram counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4190" target="_blank">01:09:50.460</a></span> | <span class="t">We simply need to store our word vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4196" target="_blank">01:09:56.220</a></span> | <span class="t">and our W and U matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4199" target="_blank">01:09:59.340</a></span> | <span class="t">but we still have the remaining problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4201" target="_blank">01:10:01.660</a></span> | <span class="t">that our fixed window is too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4204" target="_blank">01:10:04.700</a></span> | <span class="t">We can try and make the window larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4207" target="_blank">01:10:07.660</a></span> | <span class="t">If we do that, the W matrix gets bigger,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4211" target="_blank">01:10:11.380</a></span> | <span class="t">but that also points out another problem with this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4215" target="_blank">01:10:15.580</a></span> | <span class="t">Not only can the window never be large enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4218" target="_blank">01:10:18.900</a></span> | <span class="t">but W is just a trained matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4223" target="_blank">01:10:23.180</a></span> | <span class="t">And so therefore we're learning completely different weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4227" target="_blank">01:10:27.500</a></span> | <span class="t">for each position of context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4229" target="_blank">01:10:29.540</a></span> | <span class="t">the word minus one position, the word minus two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4232" target="_blank">01:10:32.340</a></span> | <span class="t">the word minus three, and the word minus four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4235" target="_blank">01:10:35.260</a></span> | <span class="t">so that there's no sharing in the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4237" target="_blank">01:10:37.940</a></span> | <span class="t">as to how it treats words in different positions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4242" target="_blank">01:10:42.940</a></span> | <span class="t">even though in some sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4245" target="_blank">01:10:45.460</a></span> | <span class="t">they will contribute semantic components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4248" target="_blank">01:10:48.260</a></span> | <span class="t">that are at least somewhat position independent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4252" target="_blank">01:10:52.580</a></span> | <span class="t">So again, for those of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4254" target="_blank">01:10:54.140</a></span> | <span class="t">if you sort of think back to either a naive Bayes model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4257" target="_blank">01:10:57.300</a></span> | <span class="t">or what we saw with the word2vec model at the beginning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4261" target="_blank">01:11:01.740</a></span> | <span class="t">the word2vec model or a naive Bayes model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4264" target="_blank">01:11:04.180</a></span> | <span class="t">completely ignores word order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4266" target="_blank">01:11:06.500</a></span> | <span class="t">So it has one set of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4268" target="_blank">01:11:08.540</a></span> | <span class="t">regardless of what position things occur in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4271" target="_blank">01:11:11.300</a></span> | <span class="t">That doesn't work well for language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4273" target="_blank">01:11:13.820</a></span> | <span class="t">'cause word order is really important in language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4276" target="_blank">01:11:16.900</a></span> | <span class="t">If the last word is the, that's a really good predictor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4280" target="_blank">01:11:20.260</a></span> | <span class="t">of there being an adjective or noun following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4282" target="_blank">01:11:22.660</a></span> | <span class="t">where if the word4vec is the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4284" target="_blank">01:11:24.940</a></span> | <span class="t">it doesn't give you the same information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4288" target="_blank">01:11:28.180</a></span> | <span class="t">So you do wanna somewhat make use of word order,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4293" target="_blank">01:11:33.180</a></span> | <span class="t">but this model is at the opposite extreme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4296" target="_blank">01:11:36.940</a></span> | <span class="t">that each position is being modeled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4299" target="_blank">01:11:39.100</a></span> | <span class="t">completely independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4301" target="_blank">01:11:41.660</a></span> | <span class="t">So what we'd like to have is a neural architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4306" target="_blank">01:11:46.100</a></span> | <span class="t">that can process an arbitrary amount of context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4311" target="_blank">01:11:51.100</a></span> | <span class="t">and have more sharing of the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4314" target="_blank">01:11:54.460</a></span> | <span class="t">while still be sensitive to proximity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4318" target="_blank">01:11:58.500</a></span> | <span class="t">And so that's the idea of recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4322" target="_blank">01:12:02.340</a></span> | <span class="t">And I'll say about five minutes about these today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4326" target="_blank">01:12:06.380</a></span> | <span class="t">and then next time we'll return</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4327" target="_blank">01:12:07.900</a></span> | <span class="t">and do more about recurrent neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4331" target="_blank">01:12:11.900</a></span> | <span class="t">So for the recurrent neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4334" target="_blank">01:12:14.940</a></span> | <span class="t">rather than having a single hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4339" target="_blank">01:12:19.340</a></span> | <span class="t">inside our classifier here that we compute each time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4344" target="_blank">01:12:24.340</a></span> | <span class="t">for the recurrent neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4346" target="_blank">01:12:26.980</a></span> | <span class="t">we have the hidden layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4349" target="_blank">01:12:29.100</a></span> | <span class="t">which often is referred to as the hidden state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4352" target="_blank">01:12:32.420</a></span> | <span class="t">but we maintain it over time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4355" target="_blank">01:12:35.300</a></span> | <span class="t">and we feed it back into itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4358" target="_blank">01:12:38.180</a></span> | <span class="t">So that's what the word recurrent is meaning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4360" target="_blank">01:12:40.620</a></span> | <span class="t">that you're sort of feeding the hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4363" target="_blank">01:12:43.540</a></span> | <span class="t">back into itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4365" target="_blank">01:12:45.300</a></span> | <span class="t">So what we do is based on the first word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4370" target="_blank">01:12:50.300</a></span> | <span class="t">we compute a hidden representation kind of like before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4375" target="_blank">01:12:55.180</a></span> | <span class="t">which can be used to predict the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4378" target="_blank">01:12:58.820</a></span> | <span class="t">But then for when we want to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4382" target="_blank">01:13:02.500</a></span> | <span class="t">what comes after the second word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4384" target="_blank">01:13:04.900</a></span> | <span class="t">we not only feed in the second word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4387" target="_blank">01:13:07.500</a></span> | <span class="t">we feed in the hidden layer from the previous word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4392" target="_blank">01:13:12.500</a></span> | <span class="t">to have it help predict the hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4396" target="_blank">01:13:16.700</a></span> | <span class="t">above the second word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4398" target="_blank">01:13:18.420</a></span> | <span class="t">And so formally the way we're doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4400" target="_blank">01:13:20.700</a></span> | <span class="t">is we're taking the hidden layer above the first word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4404" target="_blank">01:13:24.860</a></span> | <span class="t">multiplying it by a matrix W,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4408" target="_blank">01:13:28.500</a></span> | <span class="t">and then that's going to be going in together with X2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4412" target="_blank">01:13:32.860</a></span> | <span class="t">to generate the next hidden step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4415" target="_blank">01:13:35.660</a></span> | <span class="t">And so we keep on doing that at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4418" target="_blank">01:13:38.900</a></span> | <span class="t">so that we are kind of repeating a pattern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4422" target="_blank">01:13:42.100</a></span> | <span class="t">of creating a next hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4424" target="_blank">01:13:44.780</a></span> | <span class="t">based on the next input word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4428" target="_blank">01:13:48.580</a></span> | <span class="t">and the previous hidden state by updating it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4432" target="_blank">01:13:52.020</a></span> | <span class="t">by multiplying it by a matrix W.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4434" target="_blank">01:13:54.540</a></span> | <span class="t">Okay, so on my slide here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4435" target="_blank">01:13:55.980</a></span> | <span class="t">I've still only got four words of context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4438" target="_blank">01:13:58.100</a></span> | <span class="t">because it's nice for my slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4439" target="_blank">01:13:59.940</a></span> | <span class="t">but in principle there could be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4442" target="_blank">01:14:02.660</a></span> | <span class="t">any number of words of context now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4444" target="_blank">01:14:04.900</a></span> | <span class="t">Okay, so what we're doing is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4448" target="_blank">01:14:08.100</a></span> | <span class="t">so that we start off by having input vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4453" target="_blank">01:14:13.100</a></span> | <span class="t">which can be our word vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4455" target="_blank">01:14:15.980</a></span> | <span class="t">that we've looked up for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4458" target="_blank">01:14:18.020</a></span> | <span class="t">So, sorry, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4461" target="_blank">01:14:21.580</a></span> | <span class="t">so we can have the one hot vectors for word identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4465" target="_blank">01:14:25.220</a></span> | <span class="t">We look up our word embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4466" target="_blank">01:14:26.780</a></span> | <span class="t">so then we've got word embeddings for each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4470" target="_blank">01:14:30.020</a></span> | <span class="t">And then we want to compute hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4473" target="_blank">01:14:33.620</a></span> | <span class="t">So we need to start from somewhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4476" target="_blank">01:14:36.100</a></span> | <span class="t">H0 is the initial hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4478" target="_blank">01:14:38.820</a></span> | <span class="t">and H0 is normally taken as a zero vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4482" target="_blank">01:14:42.580</a></span> | <span class="t">So this is actually just initialized as zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4485" target="_blank">01:14:45.740</a></span> | <span class="t">And so for working out the first hidden state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4489" target="_blank">01:14:49.420</a></span> | <span class="t">we calculate it based on the first word embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4494" target="_blank">01:14:54.420</a></span> | <span class="t">by multiplying this embedding by a matrix WE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4499" target="_blank">01:14:59.340</a></span> | <span class="t">And that gives us the first hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4502" target="_blank">01:15:02.540</a></span> | <span class="t">But then, as we go on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4505" target="_blank">01:15:05.340</a></span> | <span class="t">we want to apply the same formula over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4509" target="_blank">01:15:09.740</a></span> | <span class="t">So we have just two parameter matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4514" target="_blank">01:15:14.140</a></span> | <span class="t">in the recurrent neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4517" target="_blank">01:15:17.020</a></span> | <span class="t">one matrix for multiplying input embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4520" target="_blank">01:15:20.460</a></span> | <span class="t">and one matrix for updating the hidden state of the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4524" target="_blank">01:15:24.860</a></span> | <span class="t">And so for the second word, from its word embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4529" target="_blank">01:15:29.380</a></span> | <span class="t">we multiply it by the WE matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4534" target="_blank">01:15:34.060</a></span> | <span class="t">We take the previous time steps hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4537" target="_blank">01:15:37.020</a></span> | <span class="t">and multiply it by the WH matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4541" target="_blank">01:15:41.300</a></span> | <span class="t">And we use the two of those to generate the new hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4545" target="_blank">01:15:45.820</a></span> | <span class="t">and precisely how we generate the new hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4548" target="_blank">01:15:48.820</a></span> | <span class="t">is then by shown on this equation on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4552" target="_blank">01:15:52.500</a></span> | <span class="t">So we take the previous hidden state, multiply it by WH.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4556" target="_blank">01:15:56.620</a></span> | <span class="t">We take the input embedding, multiply it by WE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4560" target="_blank">01:16:00.900</a></span> | <span class="t">We sum those two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4562" target="_blank">01:16:02.620</a></span> | <span class="t">We add on a learn bias weight,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4566" target="_blank">01:16:06.860</a></span> | <span class="t">and then we put that through a non-linearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4571" target="_blank">01:16:11.060</a></span> | <span class="t">And although on this slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4572" target="_blank">01:16:12.780</a></span> | <span class="t">that non-linearity is written as sigma,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4575" target="_blank">01:16:15.260</a></span> | <span class="t">by far the most common non-linearity to use here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4578" target="_blank">01:16:18.740</a></span> | <span class="t">actually is a tanh non-linearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4582" target="_blank">01:16:22.380</a></span> | <span class="t">And so this is the core equation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4586" target="_blank">01:16:26.100</a></span> | <span class="t">for a simple recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4588" target="_blank">01:16:28.980</a></span> | <span class="t">And for each successive time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4591" target="_blank">01:16:31.340</a></span> | <span class="t">we're just gonna keep on applying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4593" target="_blank">01:16:33.860</a></span> | <span class="t">to work out hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4596" target="_blank">01:16:36.180</a></span> | <span class="t">And then from those hidden states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4598" target="_blank">01:16:38.660</a></span> | <span class="t">we can use them just like in our window classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4602" target="_blank">01:16:42.940</a></span> | <span class="t">to predict what would be the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4605" target="_blank">01:16:45.500</a></span> | <span class="t">So at any position, we can take this hidden vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4610" target="_blank">01:16:50.740</a></span> | <span class="t">put it through a softmax layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4612" target="_blank">01:16:52.580</a></span> | <span class="t">which is multiplying by U matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4614" target="_blank">01:16:54.620</a></span> | <span class="t">and adding on another bias,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4616" target="_blank">01:16:56.260</a></span> | <span class="t">and then making a softmax distribution out of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4619" target="_blank">01:16:59.020</a></span> | <span class="t">And that will then give us a probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4621" target="_blank">01:17:01.940</a></span> | <span class="t">over next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4624" target="_blank">01:17:04.140</a></span> | <span class="t">What we saw here, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4625" target="_blank">01:17:05.900</a></span> | <span class="t">This is the entire math of a simple recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4630" target="_blank">01:17:10.900</a></span> | <span class="t">And next time I'll come back and say more about them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4636" target="_blank">01:17:16.380</a></span> | <span class="t">but this is the entirety of what you need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4640" target="_blank">01:17:20.540</a></span> | <span class="t">in some sense for the computation of the forward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4644" target="_blank">01:17:24.140</a></span> | <span class="t">of a simple recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4646" target="_blank">01:17:26.220</a></span> | <span class="t">So the advantages we have now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4648" target="_blank">01:17:28.380</a></span> | <span class="t">is it can process a text import of any length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4653" target="_blank">01:17:33.140</a></span> | <span class="t">In theory, at least it can use information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4657" target="_blank">01:17:37.140</a></span> | <span class="t">from any number of steps back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4659" target="_blank">01:17:39.420</a></span> | <span class="t">We'll talk more about in practice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4661" target="_blank">01:17:41.060</a></span> | <span class="t">how well that actually works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4662" target="_blank">01:17:42.860</a></span> | <span class="t">The model size is fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4665" target="_blank">01:17:45.700</a></span> | <span class="t">It doesn't matter how much of a past context there is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4669" target="_blank">01:17:49.820</a></span> | <span class="t">All we have is our WH and WE parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4674" target="_blank">01:17:54.020</a></span> | <span class="t">And at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4676" target="_blank">01:17:56.580</a></span> | <span class="t">we use exactly the same weights to update our hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4680" target="_blank">01:18:00.380</a></span> | <span class="t">So there's a symmetry in how different inputs are processed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4685" target="_blank">01:18:05.340</a></span> | <span class="t">in producing our predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4688" target="_blank">01:18:08.140</a></span> | <span class="t">RNNs in practice though,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4690" target="_blank">01:18:10.180</a></span> | <span class="t">or the simple RNNs in practice aren't perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4693" target="_blank">01:18:13.860</a></span> | <span class="t">So a disadvantage is that they're actually kind of slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4698" target="_blank">01:18:18.220</a></span> | <span class="t">'cause with this recurrent computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4701" target="_blank">01:18:21.300</a></span> | <span class="t">in some sense we are sort of stuck</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4703" target="_blank">01:18:23.300</a></span> | <span class="t">with having to have on the outside a for loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4706" target="_blank">01:18:26.100</a></span> | <span class="t">So we can do vector matrix multiplies on the inside here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4710" target="_blank">01:18:30.500</a></span> | <span class="t">but really we have to do for time step equals one to N,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4715" target="_blank">01:18:35.500</a></span> | <span class="t">calculate the successive hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4719" target="_blank">01:18:39.340</a></span> | <span class="t">And so that's not a perfect neural net architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4722" target="_blank">01:18:42.700</a></span> | <span class="t">and we'll discuss alternatives to that later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4727" target="_blank">01:18:47.580</a></span> | <span class="t">And although in theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4729" target="_blank">01:18:49.220</a></span> | <span class="t">this model can access information any number of steps back,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4733" target="_blank">01:18:53.140</a></span> | <span class="t">in practice we find that it's pretty imperfect at doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4737" target="_blank">01:18:57.780</a></span> | <span class="t">and that will then lead to more advanced forms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4741" target="_blank">01:19:01.220</a></span> | <span class="t">of recurrent neural network that I'll talk about next time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4745" target="_blank">01:19:05.060</a></span> | <span class="t">that are able to more effectively access past context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4750" target="_blank">01:19:10.060</a></span> | <span class="t">Okay, I think I'll stop there for the day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PLryWeHPcBs&t=4752" target="_blank">01:19:12.300</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>