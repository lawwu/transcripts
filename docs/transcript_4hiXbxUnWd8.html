<html><head><title>Everyone Was Wrong About Intelligence – Dario Amodei (Anthropic CEO)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Everyone Was Wrong About Intelligence – Dario Amodei (Anthropic CEO)</h2><a href="https://www.youtube.com/watch?v=4hiXbxUnWd8" target="_blank"><img src="https://i.ytimg.com/vi_webp/4hiXbxUnWd8/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>I feel like these scaling laws have been very predictable, but then when you say like, well, you know, when, when is there going to be a commercial explosion in these models or what's the form it's going to be, or are the models going to do things instead of humans or pairing with humans, I feel like certainly my track record on predicting these things is terrible, but I also looking around, I don't really see anyone whose track record is great.</p><p>I've been right about some things, but I've still, you know, with these theoretical pictures ahead, been wrong about most things, being right about 10% of the stuff is, you know, it sets you head and shoulders above, above, above, above many people. You know, if you look back to, I can't remember who it was kind of, you know, made these diagrams that are like, you know, here's, here's the village idiot.</p><p>Here's Einstein. Here's the scale of intelligence, right? And the village idiot and Einstein are like very close to each other like that. Maybe that's still true in some abstract sense or something, but it's, it's not really what we're seeing, is it we're seeing like, that it seems like the human range is pretty broad and doesn't, we don't hit the human range in the same place or at the same time for different tasks, right?</p><p>Like, you know, like write, write a sonnet, you know, in the style of Cormac McCarthy or something like, I don't know, I'm not very creative, so I couldn't do that, but like, you know, that's, that's a pretty high level human skill. Right. Um, and even the model is starting to get good at stuff of, you know, like constrained writing, you know, there's like write a, you know, write a page without using the letter E or something like write a page about X without using the letter E.</p><p>Like, I think the models might be like superhuman or close to superhuman at that. Um, but when it comes to, you know, I, yeah, I don't know, prove relatively simple mathematical theorems, like they're, they're just starting to do the beginning of it. They make really dumb mistakes sometimes. And they, they really lack any kind of broad, like, you know, correcting your errors or doing some extended task.</p><p>And so I don't know, it turns out that intelligence isn't, isn't a spectrum. There are a bunch of different areas of domain expertise. There are a bunch of different like kinds of skills, like memory is different. I mean, it's all, it's all formed in the blob. It's not, it's all formed in the blob.</p><p>It's not complicated, but to the extent it even is on the spectrum, the spectrum is also wide. If you asked me 10 years ago, that's not what I would have expected at all. But, uh, I think that's very much the way it's turned out. One thing that's been surprising is like, I thought things might click into place a little more than they do.</p><p>Like, you know, I thought like different cognitive abilities might all be connected and there was more of one secret behind them, but it's, it's like. The model just learns various things at different times, you know, and it can be like very good at coding, but like, you know, it can't, it can't quite, you know, prove the prime number theorem yet.</p><p>And I don't, I mean, I guess it's a little bit the same for, for humans, although it's, it's weird, the juxtaposition of things that can do and not, I guess the main lesson is like having theories of intelligence or how intelligence works. Like a lot of these words just, just kind of like dissolve into a continuum, right?</p><p>They, they just kind of like dematerialize, I think less in terms of intelligence and more in terms of what we see in front of us. If you told me in 2018, we'll have models in 2023, like Law 2 that can write theorems in the style of Shakespeare, whatever theory you want, you want, they can a standardized tests with open-ended questions, you know, just all kinds of really impressive things you would have said at that time, I would have said, Oh, you have AGI, you clearly have something that is a human level intelligence where these, while these things are impressive, it clearly seems we're not at human level, at least in the current generation and potentially for generations to come.</p><p>What explains this discrepancy between super impressive performance in these benchmarks and in just like the things you could describe versus, yeah, generally. So that, that was one area where actually I was not prescient and I was surprised as well. Yeah. Um, so when I first looked at GPT-3 and, you know, more so the kind of things that we built in the early days at, at Anthropic, my, my general sense was I, you know, I looked at these and I'm like, it seems like they, they really grasped the essence of language.</p><p>I'm not sure how much we need to scale them up. Like maybe we, maybe what's, what's more needed from here is like RL and all, and kind of, and kind of all the other stuff, like we might be kind of near the, you know, I thought in 2020, like we can scale this a bunch more, but I wonder if it's more efficient to scale it more or to start adding on these other objectives like, like RL, I thought maybe if you do as much RL as, you know, as, as you've done pre-training for a, for a, you know, 2020 style model that that's, that's the way to go and scaling it up, we'll keep working, but you know, is that, is that really the best path?</p><p>And I, I think it, I don't know, it just keeps going. Like I thought it had understood a lot of the essence of language, but then, you know, there's, there's kind of, there's kind of further to go. The models are maybe two to three orders of magnitude smaller than the human brain.</p><p>If you compare it to the number of synapses, while at the same time being trained on, you know, three to four more orders of magnitude of data, if you compare to, you know, number of words, human, a human sees as they're developing to age 18, we have to admit that that's a weird thing that doesn't match up.</p><p>And, you know, it's one reason I'm a bit, you know, skeptical of kind of biological analogies. I thought in terms of them like five or six years ago, but now that we actually have these models in front of us as artifacts, it feels like almost all the evidence from that has been screened off by what we've seen and what we've seen are models that are much smaller than the human brain and yet, yet can do a lot of the things that humans can do and yet paradoxically require a lot more data.</p><p>Um, so maybe we'll discover something that makes it all efficient, or maybe we'll understand why the discrepancy is present, but at the end of the day, I don't think it matters, right? If we keep scaling the way we are, I think what's more relevant at this point is just measuring the abilities of the model and seeing how far they are from humans and they don't seem terribly far to me.</p><p>What do you make of the fact that these things have basically the entire corpus of human knowledge memorized. And as far as I'm aware, they haven't been able to make like a single new connection that has led to a discovery. Whereas if even a moderately intelligent person had this much stuff memorized, they'd notice, Oh, this thing causes this symptom.</p><p>This other thing also causes the symptom. You know, there's a medical cure right here. Right. What, what, what, what shouldn't we be expecting that kind of stuff? I'm not, I'm not sure. I mean, I think, you know, I don't know these words, discovery, creativity. Like it's one of the lessons I've learned is that in, in, you know, in kind of the big blob of compute, often these, these ideas often end up being kind of fuzzy and elusive and hard to track down, but I think, I think there is something here, which is.</p><p>I think the models do display a kind of ordinary creativity again, again, you know, the kind of like, you know, write a, write a sonnet, you know, in the style of Cormac McCarthy or Barbie or so, you know, like there is some creativity to that and I think they do draw, you know, new connections of the kind that an ordinary person would draw.</p><p>I, I agree with you that there haven't been any kind of like, I don't know, like I would say like big scientific discoveries, I think that's a mix of like just the model skill level is not, is not high enough yet that I think is going to change with the, with the scaling.</p><p>I do think there's an interesting point about, well, the models have an advantage, which is they know a lot more than us, you know, like should, should they have an advantage already? Even if, even if they, their skill level isn't, isn't, isn't quite high. Maybe that's kind of what you're getting at.</p><p>I don't really have an answer to that. I mean, it seems certainly like memorization and facts and drawing connections is an area where the models are ahead. And I, I do think maybe you need those connections and you need a fairly high level of skill. I do think particularly in the area of biology for better and for worse, the complexity of biology is such that the current models know a lot of things right now, and that's what, that's what you need to make discoveries and draw.</p><p>It's not like physics where you need to, you know, you need to think and come up with a formula in biology. You need to know a lot of things. Right. And so I do think the models know a lot of things and they have a skill level that's not quite high enough to put them together.</p><p>And I think they are, they are just on the cusp of being able to put these things together.</p></div></div></body></html>