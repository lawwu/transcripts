<html><head><title>GPT-5: Everything You Need to Know So Far</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>GPT-5: Everything You Need to Know So Far</h2><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA"><img src="https://i.ytimg.com/vi/Zc03IYnnuIA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Zc03IYnnuIA.html">Whisper Transcript</a> | <a href="./transcript_Zc03IYnnuIA.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">It seems quite likely that yesterday was the day that OpenAI launched the full training run of GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=7" target="_blank">00:00:07.600</a></span> | <span class="t">I've gone through every source I can find to bring you the most reliable information on what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=12" target="_blank">00:00:12.640</a></span> | <span class="t">that means, including possibly every public comment on the topic from OpenAI, an exclusive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=19" target="_blank">00:00:19.040</a></span> | <span class="t">interview with a hardware CEO, and tons of my own analysis. Plus, I'm going to find time to throw in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=25" target="_blank">00:00:25.760</a></span> | <span class="t">a practical tip that you can use literally every time you open ChatGPT, and a bonus DALI discovery</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">that I really enjoy. But let's start with these two tweets. And the first clue that the full-scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=39" target="_blank">00:00:39.520</a></span> | <span class="t">GPT-5 is being trained comes from the president and co-founder of OpenAI, Greg Brockman. Now first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=46" target="_blank">00:00:46.000</a></span> | <span class="t">a little bit of context. OpenAI typically trains smaller models of about a thousandth the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=51" target="_blank">00:00:51.440</a></span> | <span class="t">before they do a full training run. They then gather insights from these smaller models before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=56" target="_blank">00:00:56.320</a></span> | <span class="t">doing the full training run. So that's the backdrop to OpenAI, in Brockman's words, scientifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=62" target="_blank">00:01:02.640</a></span> | <span class="t">predicting and understanding the resulting systems. That being done, what they're now building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=69" target="_blank">00:01:09.040</a></span> | <span class="t">is maximally harnessing all their computing resources. They're gathering all of their ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=74" target="_blank">00:01:14.640</a></span> | <span class="t">together and scaling beyond precedent. Translated, they are training their biggest model yet. We'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=80" target="_blank">00:01:20.640</a></span> | <span class="t">get to parameters, data, and capabilities in a moment. But first, what's this other tweet?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=85" target="_blank">00:01:25.040</a></span> | <span class="t">This comes from Jason Wei, a top OpenAI researcher. A few hours after Brockman's tweet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=90" target="_blank">00:01:30.160</a></span> | <span class="t">he said there's no adrenaline rush like launching a massive GPU training. And this got plenty of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=96" target="_blank">00:01:36.720</a></span> | <span class="t">salutes in the reply from other OpenAI employees. Now for context, this does not mean that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=102" target="_blank">00:01:42.720</a></span> | <span class="t">imminently going to get GPT-5. GPT-4 took around three months to train, and then there was the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=108" target="_blank">00:01:48.880</a></span> | <span class="t">safety testing. I'm actually going to end this video with my exact prediction of when I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=113" target="_blank">00:01:53.040</a></span> | <span class="t">they're going to release GPT-5. But first, here is a little bit more supporting evidence that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=118" target="_blank">00:01:58.080</a></span> | <span class="t">they are currently training GPT-5. OpenAI updated their blog to say that applications for the red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=124" target="_blank">00:02:04.400</a></span> | <span class="t">teaming network have closed, and that those red teamers would know about the status of their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=129" target="_blank">00:02:09.200</a></span> | <span class="t">applications by the end of last year. What that means is that the red teamers are now in place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=134" target="_blank">00:02:14.720</a></span> | <span class="t">to start safety testing the new model. Now, you might say, what's the point of having those red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=139" target="_blank">00:02:19.520</a></span> | <span class="t">teamers in place if it's still going to be training for two to three months? Well, before a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=144" target="_blank">00:02:24.560</a></span> | <span class="t">model is fully trained, it goes through various checkpoints. Think of them a bit like a video game</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=150" target="_blank">00:02:30.160</a></span> | <span class="t">save. What that also means is that in effect, OpenAI will have a GPT-4.2 before they have a GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=158" target="_blank">00:02:38.400</a></span> | <span class="t">Indeed, Greg Brockman, going back to April of last year, said that it might be one of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=163" target="_blank">00:02:43.520</a></span> | <span class="t">checkpoints that OpenAI release first. He said that it's easy to create a continuum of incrementally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=170" target="_blank">00:02:50.640</a></span> | <span class="t">better AIs, such as by deploying subsequent checkpoints of a given training run. And he</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=175" target="_blank">00:02:55.920</a></span> | <span class="t">explicitly contrasted that approach and said it would be very unlike our historical approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=181" target="_blank">00:03:01.440</a></span> | <span class="t">of infrequent major model upgrades. But remember that even before those checkpoints, OpenAI would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=187" target="_blank">00:03:07.360</a></span> | <span class="t">have already gotten a glimpse of GPT-5 capabilities from those smaller, earlier versions of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=193" target="_blank">00:03:13.920</a></span> | <span class="t">Indeed, Sam Altman, back in November, said that he was privileged to be in the room</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=198" target="_blank">00:03:18.080</a></span> | <span class="t">when they pushed back the veil of ignorance. I'm super excited. I can't imagine anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=203" target="_blank">00:03:23.040</a></span> | <span class="t">more exciting to work on. And on a personal note, like four times now in the history of OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=207" target="_blank">00:03:27.680</a></span> | <span class="t">the most recent time was just in the last couple of weeks. I've gotten to be in the room when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=212" target="_blank">00:03:32.880</a></span> | <span class="t">sort of like push the front, the sort of the veil of ignorance back and the frontier of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=218" target="_blank">00:03:38.000</a></span> | <span class="t">discovery forward. And getting to do that is like the professional honor of a lifetime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=222" target="_blank">00:03:42.480</a></span> | <span class="t">It all points to OpenAI in November and December having trained those smaller versions of GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=229" target="_blank">00:03:49.120</a></span> | <span class="t">Again, with the purpose of scientifically predicting and understanding the resulting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=234" target="_blank">00:03:54.560</a></span> | <span class="t">GPT-5 system. So they know it's going to be good, but just how good and how big? And what are these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=241" target="_blank">00:04:01.120</a></span> | <span class="t">new or old ideas that they're going to incorporate? Well, one thing that seemed almost certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=246" target="_blank">00:04:06.240</a></span> | <span class="t">is that they're going to incorporate a way to let GPT-5 think for longer. In other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=251" target="_blank">00:04:11.280</a></span> | <span class="t">it's going to lay out its reasoning steps before solving a challenge and have each of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=256" target="_blank">00:04:16.000</a></span> | <span class="t">reasoning steps checked internally or externally. Here's Sam Altman a few days ago at Davos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=262" target="_blank">00:04:22.240</a></span> | <span class="t">What it means to verify or understand what's going on is going to be a little bit different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=266" target="_blank">00:04:26.560</a></span> | <span class="t">than people think right now. I actually can't look in your brain and look at the hundred trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=272" target="_blank">00:04:32.080</a></span> | <span class="t">synapses and try to understand what's happening in each one and say, okay, I really understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=276" target="_blank">00:04:36.880</a></span> | <span class="t">why he's thinking what he's thinking. You're not a black box to me. But what I can ask you to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=282" target="_blank">00:04:42.720</a></span> | <span class="t">is explain to me your reasoning. I can say, you know, you think this thing, why? And you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=288" target="_blank">00:04:48.080</a></span> | <span class="t">explain first this, then this, then there's this conclusion, then that one, and then there's this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=292" target="_blank">00:04:52.240</a></span> | <span class="t">And I can decide if that sounds reasonable to me or not. And I think our AI systems will also be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=298" target="_blank">00:04:58.400</a></span> | <span class="t">able to do the same thing. They'll be able to explain to us in natural language, the steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=303" target="_blank">00:05:03.040</a></span> | <span class="t">from A to B, and we can decide whether we think those are good steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=309" target="_blank">00:05:09.200</a></span> | <span class="t">And a few days before that, Sam Altman told Bill Gates that that might involve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=314" target="_blank">00:05:14.000</a></span> | <span class="t">asking GPT-4 or GPT-5 the same question 10,000 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=319" target="_blank">00:05:19.200</a></span> | <span class="t">You know, when you look at the next two years, what do you think some of the key milestones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=324" target="_blank">00:05:24.080</a></span> | <span class="t">will be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=324" target="_blank">00:05:24.880</a></span> | <span class="t">Multimodality will definitely be important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=327" target="_blank">00:05:27.040</a></span> | <span class="t">Which means speech in, speech out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=329" target="_blank">00:05:29.840</a></span> | <span class="t">Speech in, speech out, images, eventually video. Clearly, people really want that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=334" target="_blank">00:05:34.880</a></span> | <span class="t">We launched images and audio, and it had a much stronger response than we expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=339" target="_blank">00:05:39.600</a></span> | <span class="t">We'll be able to push that much further. But maybe the most important areas of progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=345" target="_blank">00:05:45.280</a></span> | <span class="t">will be around reasoning ability. Right now, GPT-4 can reason in only extremely limited ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=350" target="_blank">00:05:50.800</a></span> | <span class="t">And also reliability. If you ask GPT-4 most questions 10,000 times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=356" target="_blank">00:05:56.400</a></span> | <span class="t">one of those 10,000 is probably pretty good, but it doesn't always know which one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=360" target="_blank">00:06:00.320</a></span> | <span class="t">And you'd like to get the best response of 10,000 each time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=363" target="_blank">00:06:03.440</a></span> | <span class="t">That increase in reliability will be important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=365" target="_blank">00:06:05.760</a></span> | <span class="t">And at this point, watchers of my channel will know exactly what he's referring to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=369" target="_blank">00:06:09.520</a></span> | <span class="t">Both of those approaches, checking your reasoning steps and sampling up to 10,000 times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=375" target="_blank">00:06:15.040</a></span> | <span class="t">are incorporated into OpenAI's Let's Verify step-by-step paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=379" target="_blank">00:06:19.440</a></span> | <span class="t">Now, I'm not going to dive into the details of Let's Verify in this video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=383" target="_blank">00:06:23.360</a></span> | <span class="t">because I've got at least two previous videos on the topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=386" target="_blank">00:06:26.480</a></span> | <span class="t">But notice in the paper how many times they sample GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=390" target="_blank">00:06:30.160</a></span> | <span class="t">The chart shows what happens when you sample the model over a thousand times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=394" target="_blank">00:06:34.720</a></span> | <span class="t">and pick out the best responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=396" target="_blank">00:06:36.640</a></span> | <span class="t">And notice something about this process-supervised way of doing things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=400" target="_blank">00:06:40.240</a></span> | <span class="t">The results are continuing to go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=402" target="_blank">00:06:42.640</a></span> | <span class="t">I can't resist showing you a quick example of the reasoning steps broken down into separate lines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=408" target="_blank">00:06:48.240</a></span> | <span class="t">and essentially a verifier looking in and checking which steps are accurate or inaccurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=413" target="_blank">00:06:53.760</a></span> | <span class="t">The answers for whom each step in the reasoning process got a thumbs up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=418" target="_blank">00:06:58.000</a></span> | <span class="t">were the ones submitted and the results were dramatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=421" target="_blank">00:07:01.440</a></span> | <span class="t">Essentially, this process of sampling the model thousands of times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=424" target="_blank">00:07:04.640</a></span> | <span class="t">and taking the answer that had the highest rated reasoning steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=428" target="_blank">00:07:08.320</a></span> | <span class="t">doubled the performance in mathematics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=430" target="_blank">00:07:10.480</a></span> | <span class="t">And no, this didn't just work for mathematics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=432" target="_blank">00:07:12.720</a></span> | <span class="t">It had dramatic results across these STEM fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=435" target="_blank">00:07:15.840</a></span> | <span class="t">And remember, this was using GPT-4 as a base model, not GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=440" target="_blank">00:07:20.240</a></span> | <span class="t">And it was only 2000 samples, not 10,000 like Sam Altman talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=444" target="_blank">00:07:24.480</a></span> | <span class="t">So this is the evidence I would present to someone who said that LLMs have peaked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=449" target="_blank">00:07:29.760</a></span> | <span class="t">If OpenAI can incorporate through parallelization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=453" target="_blank">00:07:33.360</a></span> | <span class="t">a way to get the model to submit and analyze 10,000 responses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=458" target="_blank">00:07:38.240</a></span> | <span class="t">the results could be truly dramatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=460" target="_blank">00:07:40.480</a></span> | <span class="t">Indeed, the Let's Verify paper from OpenAI repeatedly cited this earlier DeepMind paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=466" target="_blank">00:07:46.400</a></span> | <span class="t">on solving math problems with process-based feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=469" target="_blank">00:07:49.920</a></span> | <span class="t">And for coding, we know AlphaCode2 from Google DeepMind used the mass sampling approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=475" target="_blank">00:07:55.760</a></span> | <span class="t">to get an 87th percentile score on a coding contest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=480" target="_blank">00:08:00.320</a></span> | <span class="t">In other words, it beat 87% of participants in this CodeForces coding challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=485" target="_blank">00:08:05.440</a></span> | <span class="t">For context, the GPT-4 that we got scored around the 5th percentile,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=489" target="_blank">00:08:09.680</a></span> | <span class="t">a score of 400 in the CodeForces challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=492" target="_blank">00:08:12.640</a></span> | <span class="t">These numbers are a little out of date, but AlphaCode2 would have scored around here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=496" target="_blank">00:08:16.960</a></span> | <span class="t">expert or candidate master level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=499" target="_blank">00:08:19.040</a></span> | <span class="t">Or to just translate everything, if they find a way to let GPT-5 think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=504" target="_blank">00:08:24.480</a></span> | <span class="t">it could be night and day in terms of performance for coding, mathematics, and STEM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=510" target="_blank">00:08:30.160</a></span> | <span class="t">But just how big will GPT-5 be that's doing all of this parallel thinking?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=514" target="_blank">00:08:34.720</a></span> | <span class="t">Well, for AI Insiders, I interviewed Gavin Uberti,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=518" target="_blank">00:08:38.240</a></span> | <span class="t">the CEO and co-founder of EtchedAI, which I've also talked about on this channel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=523" target="_blank">00:08:43.440</a></span> | <span class="t">He is a 21-year-old dropout from Harvard University and on his LinkedIn profile,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=527" target="_blank">00:08:47.920</a></span> | <span class="t">it says he's building the hardware for superintelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=531" target="_blank">00:08:51.120</a></span> | <span class="t">In the interview, he guessed that GPT-5 would have around 10 times the parameter count of GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=537" target="_blank">00:08:57.040</a></span> | <span class="t">According to leaks, GPT-4 has around 1.5 to 1.8 trillion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=541" target="_blank">00:09:01.920</a></span> | <span class="t">But just quickly, what did he mean when he said that he expects that to come from a combination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=545" target="_blank">00:09:05.920</a></span> | <span class="t">of a larger embedding dimension, more layers, and double the number of experts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=550" target="_blank">00:09:10.560</a></span> | <span class="t">Well, think of the embedding dimension as being about how granular the training can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=555" target="_blank">00:09:15.040</a></span> | <span class="t">about each token and its context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=557" target="_blank">00:09:17.360</a></span> | <span class="t">A bigger embedding dimension means more granularity and nuance about each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=562" target="_blank">00:09:22.240</a></span> | <span class="t">And doubling the number of layers allows a model to develop deeper pattern recognition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=566" target="_blank">00:09:26.960</a></span> | <span class="t">It allows it to see more complex patterns within patterns within patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=571" target="_blank">00:09:31.040</a></span> | <span class="t">More highlights from that interview will be coming on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=573" target="_blank">00:09:33.600</a></span> | <span class="t">AI Insiders on Patreon in the coming weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=576" target="_blank">00:09:36.720</a></span> | <span class="t">But during this video on GPT-5, I promised you two interludes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=581" target="_blank">00:09:41.280</a></span> | <span class="t">One focused on DALI-3 and one a practical tip for using ChatGPT anytime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=587" target="_blank">00:09:47.120</a></span> | <span class="t">Well, here is the first of those two interludes focused on a particular quirk of DALI-3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=592" target="_blank">00:09:52.400</a></span> | <span class="t">I say that, but this trick also works for mid-journey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=595" target="_blank">00:09:55.040</a></span> | <span class="t">Now, many of you might have noticed a trend on TikTok, Reddit, Twitter, and YouTube shorts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=600" target="_blank">00:10:00.160</a></span> | <span class="t">where people post an image and then make it progressively more intense, let's say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=604" target="_blank">00:10:04.720</a></span> | <span class="t">Well, here's something arguably even more quirky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=607" target="_blank">00:10:07.680</a></span> | <span class="t">I got the original idea from Peter Wilderford on Twitter and decided to make it more intense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=613" target="_blank">00:10:13.520</a></span> | <span class="t">But first I asked, draw an image of a London scene, but don't use lampposts in the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=618" target="_blank">00:10:18.960</a></span> | <span class="t">And lo and behold, we get dozens of lampposts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=622" target="_blank">00:10:22.400</a></span> | <span class="t">I mean, in this one, we have a lamppost coming down from the sky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=625" target="_blank">00:10:25.840</a></span> | <span class="t">And what does GPT-4 say in analyzing these?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=628" target="_blank">00:10:28.960</a></span> | <span class="t">It says that these are two images of a London street scene without lampposts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=633" target="_blank">00:10:33.440</a></span> | <span class="t">So then I said, now make these images with even fewer lampposts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=637" target="_blank">00:10:37.760</a></span> | <span class="t">stripping any lamppost references completely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=640" target="_blank">00:10:40.240</a></span> | <span class="t">And here were the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=642" target="_blank">00:10:42.080</a></span> | <span class="t">As you can see on the right, there's barely a lamppost in sight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=645" target="_blank">00:10:45.840</a></span> | <span class="t">And as GPT-4 says, these images were created with complete omission of any lamppost references.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=652" target="_blank">00:10:52.000</a></span> | <span class="t">Then I said, delete absolutely everything that pertains to a lamppost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=655" target="_blank">00:10:55.920</a></span> | <span class="t">And I don't know about you, but I can't see any lampposts left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=658" target="_blank">00:10:58.400</a></span> | <span class="t">And finally, I said, take this to the max and give me an image that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=661" target="_blank">00:11:01.760</a></span> | <span class="t">someone could not picture a lamppost existing within, even in their wildest imagination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=666" target="_blank">00:11:06.480</a></span> | <span class="t">Now, I think it's pretty cute that the lamppost persisted throughout these images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=670" target="_blank">00:11:10.800</a></span> | <span class="t">And I suspect that the deeper reason is that the caption training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=675" target="_blank">00:11:15.040</a></span> | <span class="t">that DALI 3 got didn't have many examples of omission.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=678" target="_blank">00:11:18.800</a></span> | <span class="t">They used web captions and synthetic captions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=681" target="_blank">00:11:21.360</a></span> | <span class="t">but I doubt there were many examples of people saying this image does not contain X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=686" target="_blank">00:11:26.000</a></span> | <span class="t">But speaking of modality, the first thing they want to fix apparently for GPT-5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=691" target="_blank">00:11:31.120</a></span> | <span class="t">is the real time nature of the voice interaction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=694" target="_blank">00:11:34.400</a></span> | <span class="t">At the moment, there is quite a bit of time to first token latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=698" target="_blank">00:11:38.560</a></span> | <span class="t">In other words, it takes a bit too long to reply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=701" target="_blank">00:11:41.040</a></span> | <span class="t">Here's Sam Altman speaking last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=702" target="_blank">00:11:42.960</a></span> | <span class="t">I think there's all sorts of the current stuff that people complain about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=705" target="_blank">00:11:45.440</a></span> | <span class="t">like the voice is too slow and, you know, it's not real time and that'll get better this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=709" target="_blank">00:11:49.360</a></span> | <span class="t">I think where we're headed, and then I'll talk about this year,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=711" target="_blank">00:11:51.680</a></span> | <span class="t">is we're headed towards the way you use the computers to talk to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=714" target="_blank">00:11:54.800</a></span> | <span class="t">The operating system of a computer in some sense is close to this idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=718" target="_blank">00:11:58.560</a></span> | <span class="t">that you're like working inside of a chat experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=722" target="_blank">00:12:02.400</a></span> | <span class="t">When he mentioned using an LLM as an operating system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=725" target="_blank">00:12:05.840</a></span> | <span class="t">he was drawing on Andrej Karpathy's vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=728" target="_blank">00:12:08.320</a></span> | <span class="t">I've talked about it before, but notice at the top that it's video in and out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=732" target="_blank">00:12:12.640</a></span> | <span class="t">audio in and out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=734" target="_blank">00:12:14.320</a></span> | <span class="t">And it's not like OpenAI are hiding it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=736" target="_blank">00:12:16.640</a></span> | <span class="t">They want as much text, image, audio and video data as they can get their hands on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=741" target="_blank">00:12:21.680</a></span> | <span class="t">They also want what I'm going to call reasoning data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=744" target="_blank">00:12:24.560</a></span> | <span class="t">Data that expresses human intention, they call it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=747" target="_blank">00:12:27.440</a></span> | <span class="t">Now, I didn't notice that phrase at the time of this blog post in November,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=751" target="_blank">00:12:31.440</a></span> | <span class="t">but it fits in clearly with what I was saying earlier about Let's Verify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=755" target="_blank">00:12:35.440</a></span> | <span class="t">Think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=756" target="_blank">00:12:36.000</a></span> | <span class="t">How would you make a model agentic, able to solve more complex challenges?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=760" target="_blank">00:12:40.240</a></span> | <span class="t">Well, if GPT-5 gets loads of data about people laying out plans full of human intention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=765" target="_blank">00:12:45.760</a></span> | <span class="t">GPT-5 could learn to imitate those schemes and plans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=769" target="_blank">00:12:49.120</a></span> | <span class="t">and maybe have a verifier internally or externally judging those reasoning steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=773" target="_blank">00:12:53.840</a></span> | <span class="t">Now, as for the question of whether those reasoning steps faithfully represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=777" target="_blank">00:12:57.840</a></span> | <span class="t">what the model is internally calculating, that will have to be for another day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=782" target="_blank">00:13:02.240</a></span> | <span class="t">This paper from Anthropic back in July said that as models become larger and more capable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=787" target="_blank">00:13:07.520</a></span> | <span class="t">they actually produce less faithful reasoning on most tasks we study.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=791" target="_blank">00:13:11.360</a></span> | <span class="t">That doesn't mean getting the answer wrong more often.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=794" target="_blank">00:13:14.000</a></span> | <span class="t">It means outputting reasoning steps that don't actually reflect what it's internally calculating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=799" target="_blank">00:13:19.040</a></span> | <span class="t">So, GPT-5 may end up being an excellent productivity assistant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=803" target="_blank">00:13:23.280</a></span> | <span class="t">while still being somewhat inscrutable on a deeper level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=806" target="_blank">00:13:26.720</a></span> | <span class="t">Just quickly before we leave data, I think there's one thing that we can safely say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=810" target="_blank">00:13:30.320</a></span> | <span class="t">which is that there will be much more multilingual data in GPT-5 training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=815" target="_blank">00:13:35.440</a></span> | <span class="t">OpenAI have formed so many data partnerships,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=818" target="_blank">00:13:38.240</a></span> | <span class="t">including with people like the Icelandic government,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=820" target="_blank">00:13:40.720</a></span> | <span class="t">and there are so many more multilingual data sets being open sourced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=824" target="_blank">00:13:44.720</a></span> | <span class="t">that I think it's almost inevitable that there will be a dramatic forward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=829" target="_blank">00:13:49.120</a></span> | <span class="t">in GPT-5's multilingual abilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=831" target="_blank">00:13:51.520</a></span> | <span class="t">Partly, this is a safety thing too,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=833" target="_blank">00:13:53.360</a></span> | <span class="t">with OpenAI wanting its red teamers to be fluent in more than one language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=837" target="_blank">00:13:57.440</a></span> | <span class="t">Models are notoriously easier to jailbreak in different languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=841" target="_blank">00:14:01.360</a></span> | <span class="t">and it looks like OpenAI are working hard on that front.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=844" target="_blank">00:14:04.240</a></span> | <span class="t">But there is one language that I bet you didn't know GPT-4 can already speak,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=848" target="_blank">00:14:08.480</a></span> | <span class="t">and that's the language of gobbledygook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=850" target="_blank">00:14:10.400</a></span> | <span class="t">According to this fascinating paper from Tokyo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=853" target="_blank">00:14:13.280</a></span> | <span class="t">GPT-4 can almost perfectly handle unnatural scrambled text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=857" target="_blank">00:14:17.360</a></span> | <span class="t">Now you might already know that humans have this ability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=860" target="_blank">00:14:20.000</a></span> | <span class="t">if the first and last letter of a word is the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=863" target="_blank">00:14:23.200</a></span> | <span class="t">you can often still recognize the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=865" target="_blank">00:14:25.280</a></span> | <span class="t">But GPT-4, and obviously GPT-5, will be able to go a step further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=869" target="_blank">00:14:29.440</a></span> | <span class="t">Even if the first and last letter are different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=872" target="_blank">00:14:32.320</a></span> | <span class="t">if a word is completely scrambled, it can recover the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=876" target="_blank">00:14:36.400</a></span> | <span class="t">I tested it out, and indeed it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=878" target="_blank">00:14:38.720</a></span> | <span class="t">Just look at how utterly gobbled this sentence is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=882" target="_blank">00:14:42.480</a></span> | <span class="t">For me and you, that would be almost complete gobbledygook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=885" target="_blank">00:14:45.920</a></span> | <span class="t">But GPT-4 was able to recognize what I'm saying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=889" target="_blank">00:14:49.120</a></span> | <span class="t">So that is the practical tip that I also wanted to give you in this video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=893" target="_blank">00:14:53.040</a></span> | <span class="t">If you have a quick request of GPT-4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=896" target="_blank">00:14:56.000</a></span> | <span class="t">don't bother going back and spending 30 seconds correcting all your typos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=900" target="_blank">00:15:00.800</a></span> | <span class="t">Trust me, I've been guilty of this in the past because I love perfect English,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=905" target="_blank">00:15:05.680</a></span> | <span class="t">but if you have a letter or two out of place, don't worry, it will understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=909" target="_blank">00:15:09.520</a></span> | <span class="t">To be honest, if it can unscramble this, it can understand your typo of the word there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=914" target="_blank">00:15:14.080</a></span> | <span class="t">your missing comma, and all the rest of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=916" target="_blank">00:15:16.400</a></span> | <span class="t">So save yourself 30 seconds and don't even bother correcting your typos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=920" target="_blank">00:15:20.080</a></span> | <span class="t">But now it's time for me to finally give my prediction for when GPT-5 is going to be released.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=925" target="_blank">00:15:25.600</a></span> | <span class="t">For the last few weeks, I've honestly thought it would be around September of this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=930" target="_blank">00:15:30.160</a></span> | <span class="t">But now I think it will be toward the end of November of 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=935" target="_blank">00:15:35.760</a></span> | <span class="t">And no, that's not just because that would be the two-year anniversary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=939" target="_blank">00:15:39.440</a></span> | <span class="t">of the release of ChatGPT, the original version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=942" target="_blank">00:15:42.400</a></span> | <span class="t">First, let me clarify that I don't think they will release</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=945" target="_blank">00:15:45.040</a></span> | <span class="t">the full capabilities of GPT-5 on the first go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=948" target="_blank">00:15:48.640</a></span> | <span class="t">As mentioned, I think they'll release different checkpoints,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=950" target="_blank">00:15:50.880</a></span> | <span class="t">different functionalities as we head into 2025.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=954" target="_blank">00:15:54.080</a></span> | <span class="t">But what explains the delay from now when they're training it all the way to November?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=958" target="_blank">00:15:58.880</a></span> | <span class="t">Well, first of all, as mentioned, it does take a few months to train a model the size of GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=965" target="_blank">00:16:05.120</a></span> | <span class="t">Yes, they might be able to use, say, 100,000 H100 GPUs from NVIDIA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=969" target="_blank">00:16:09.520</a></span> | <span class="t">But training a model has hiccups and, of course, the model will be much larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=973" target="_blank">00:16:13.680</a></span> | <span class="t">But let's say that takes around two months. That would bring us to the end of March.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=977" target="_blank">00:16:17.840</a></span> | <span class="t">Now here's the key point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=979" target="_blank">00:16:19.040</a></span> | <span class="t">Sam Altman has boasted many times in the past</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=982" target="_blank">00:16:22.080</a></span> | <span class="t">about how they tested GPT-4 for six to eight months before releasing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=986" target="_blank">00:16:26.800</a></span> | <span class="t">It would be pretty awkward for OpenAI to have even less safety testing for GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=993" target="_blank">00:16:33.120</a></span> | <span class="t">So add six months to the end of March and you get to the end of September.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=997" target="_blank">00:16:37.280</a></span> | <span class="t">Of course, add eight months and you get to the end of November.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1000" target="_blank">00:16:40.640</a></span> | <span class="t">So why end of November rather than end of September?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1003" target="_blank">00:16:43.680</a></span> | <span class="t">Well, I think OpenAI will want to steer clear of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1007" target="_blank">00:16:47.360</a></span> | <span class="t">what will be an incredibly contentious American election.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1010" target="_blank">00:16:50.960</a></span> | <span class="t">If they release even an alpha version of GPT-5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1014" target="_blank">00:16:54.080</a></span> | <span class="t">with, say, video and audio before the election, they could come under incredible flack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1019" target="_blank">00:16:59.200</a></span> | <span class="t">As they say on their website, they're still working to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1022" target="_blank">00:17:02.240</a></span> | <span class="t">how effective their current tools might be for personalized persuasion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1026" target="_blank">00:17:06.160</a></span> | <span class="t">That would be stepping into a minefield.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1028" target="_blank">00:17:08.880</a></span> | <span class="t">In the recent New Hampshire election, we already saw robocalls imitating Joe Biden.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1033" target="_blank">00:17:13.520</a></span> | <span class="t">So November 30th, as well as being a symbolic date, would steer clear of that election.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1046" target="_blank">00:17:26.880</a></span> | <span class="t">You might say, what about 2025?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1048" target="_blank">00:17:28.480</a></span> | <span class="t">But I think the incentives of Moloch will prevent them delaying too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1052" target="_blank">00:17:32.160</a></span> | <span class="t">In not too long, we might be getting the release of Gemini Ultra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1055" target="_blank">00:17:35.040</a></span> | <span class="t">not to mention Gemini 2 Ultra and, of course, LLAMA 3 from Meta, announced by Zuckerberg.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1060" target="_blank">00:17:40.240</a></span> | <span class="t">When everyone else has caught up, they might feel compelled to release the next model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1064" target="_blank">00:17:44.320</a></span> | <span class="t">And Anthropic might choose a similar time to release Claude III.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1068" target="_blank">00:17:48.240</a></span> | <span class="t">Here is the CEO of Anthropic, Dario Amadei, giving his rough predictions for GPT-5 and Claude III.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1075" target="_blank">00:17:55.040</a></span> | <span class="t">What do you think happens on the next major training run for LLMs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1078" target="_blank">00:17:58.880</a></span> | <span class="t">My guess would be, you know, nothing truly insane happens, say, in any training run that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1085" target="_blank">00:18:05.680</a></span> | <span class="t">you know, happens in 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1087" target="_blank">00:18:07.680</a></span> | <span class="t">You know, to really invent new science, the ability to cure diseases, the ability to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1094" target="_blank">00:18:14.160</a></span> | <span class="t">bio, yeah, the ability to make bioweapons, yeah, and maybe someday the Dyson spheres.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1098" target="_blank">00:18:18.480</a></span> | <span class="t">The least impressive of those things, I think, you know, will happen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1102" target="_blank">00:18:22.000</a></span> | <span class="t">I would say, no sooner than 2025, maybe 2026.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1105" target="_blank">00:18:25.360</a></span> | <span class="t">I think we're just going to see, in 2024,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1108" target="_blank">00:18:28.640</a></span> | <span class="t">crisper, more commercially applicable versions of the models that exist today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1114" target="_blank">00:18:34.000</a></span> | <span class="t">Like, you know, we've seen a few of these generations of jumps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1117" target="_blank">00:18:37.360</a></span> | <span class="t">I think in 2024, people are certainly going to be surprised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1120" target="_blank">00:18:40.560</a></span> | <span class="t">Like, they're going to be surprised at how much better these things have gotten.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1123" target="_blank">00:18:43.680</a></span> | <span class="t">But it's not going to quite bend reality yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1127" target="_blank">00:18:47.120</a></span> | <span class="t">Of course, I have to say, at the end of this video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1129" target="_blank">00:18:49.520</a></span> | <span class="t">that no one truly knows, not even OpenAI, what GPT-5 will be like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1134" target="_blank">00:18:54.160</a></span> | <span class="t">As Sam Altman recently said, "Until we go train that model, GPT-5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1138" target="_blank">00:18:58.000</a></span> | <span class="t">it's like a fun guessing game for us."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1140" target="_blank">00:19:00.400</a></span> | <span class="t">I can't tell you, here's exactly what it's going to do that GPT-4 didn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1144" target="_blank">00:19:04.480</a></span> | <span class="t">And here's Greg Brockman with a similar message.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1147" target="_blank">00:19:07.040</a></span> | <span class="t">Right, that is the biggest theme in the history of AI, is that it's full of surprises.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1150" target="_blank">00:19:10.400</a></span> | <span class="t">Every time you think you know something, you scale it up 10x, turns out you knew nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1154" target="_blank">00:19:14.000</a></span> | <span class="t">And so I think that we, as a humanity, as a species, are really exploring this together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1158" target="_blank">00:19:18.560</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1159" target="_blank">00:19:19.360</a></span> | <span class="t">And then we get cryptic messages like this from senior members of OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1163" target="_blank">00:19:23.840</a></span> | <span class="t">Ben Newhouse says he's hiring at OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1166" target="_blank">00:19:26.800</a></span> | <span class="t">"And we're building what I think could be an industry-defining 0-to-1 product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1172" target="_blank">00:19:32.640</a></span> | <span class="t">that leverages the latest and greatest from our upcoming models, i.e. GPT-5."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1177" target="_blank">00:19:37.680</a></span> | <span class="t">And two other OpenAI employees replied like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1180" target="_blank">00:19:40.640</a></span> | <span class="t">"This product will change everything and what they said."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1184" target="_blank">00:19:44.160</a></span> | <span class="t">Of course, it would be pure speculation to guess what they mean with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1188" target="_blank">00:19:48.560</a></span> | <span class="t">So those were my predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1190" target="_blank">00:19:50.080</a></span> | <span class="t">I try to base them on evidence rather than idle speculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1194" target="_blank">00:19:54.080</a></span> | <span class="t">I do try not to be lazy like GPT-4 has in the past been accused of being.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1198" target="_blank">00:19:58.720</a></span> | <span class="t">If you like this kind of thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1199" target="_blank">00:19:59.760</a></span> | <span class="t">I would invite you to see more exclusive premium content on AI Insiders on Patreon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1205" target="_blank">00:20:05.040</a></span> | <span class="t">but for everyone watching to the end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Zc03IYnnuIA&t=1207" target="_blank">00:20:07.440</a></span> | <span class="t">I want to thank you so much for being here and I want to wish you a wonderful day.</span></div></div></body></html>