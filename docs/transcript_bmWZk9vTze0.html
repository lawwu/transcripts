<html><head><title>MCP is all you need — Samuel Colvin, Pydantic</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>MCP is all you need — Samuel Colvin, Pydantic</h2><a href="https://www.youtube.com/watch?v=bmWZk9vTze0" target="_blank"><img src="https://i.ytimg.com/vi_webp/bmWZk9vTze0/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=0 target="_blank"">0:0</a> Introduction: Speaker Samuel Colvin introduces himself as the creator of Pydantic.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=42 target="_blank"">0:42</a> Pydantic Ecosystem: Introduction to Pydantic the company, the Pydantic AI agent framework, and the Logfire observability platform.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=78 target="_blank"">1:18</a> Talk Thesis: Explaining the title "MCP is all you need" and the main argument that MCP simplifies agent communication.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=125 target="_blank"">2:5</a> MCP's Focus: Clarifying that the talk focuses on MCP for autonomous agents and custom code, not its original desktop automation use case.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=168 target="_blank"">2:48</a> Tool Calling Primitive: Highlighting that "tool calling" is the most relevant MCP primitive for this context.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=190 target="_blank"">3:10</a> MCP vs. OpenAPI: Listing the advantages MCP has over a simple OpenAPI specification for tool calls.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=201 target="_blank"">3:21</a> Feature 1: Dynamic Tools: Tools can appear and disappear based on server state.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=206 target="_blank"">3:26</a> Feature 2: Streaming Logs: The ability to return log data to the user while a tool is still executing.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=213 target="_blank"">3:33</a> Feature 3: Sampling: A mechanism for a tool (server) to request an LLM call back through the agent (client).<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=241 target="_blank"">4:1</a> MCP Architecture Diagram: Visualizing the basic agent-to-tool communication flow.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=283 target="_blank"">4:43</a> Complex Architecture: Discussing scenarios where tools are themselves agents that need LLM access.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=324 target="_blank"">5:24</a> Explaining Sampling: Detailing how sampling solves the problem of every agent needing its own LLM by allowing tools to "piggyback" on the client's LLM access.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=402 target="_blank"">6:42</a> Pydantic AI's Role in Sampling: How the Pydantic AI library supports sampling on both the client and server side.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=430 target="_blank"">7:10</a> Demo Start: Beginning the demonstration of a research agent that uses an MCP tool to query BigQuery.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=503 target="_blank"">8:23</a> Code Walkthrough: Validation: Showing how Pydantic is used for output validation and automatic retries (model_retry).<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=540 target="_blank"">9:0</a> Code Walkthrough: Context Logging: Demonstrating the use of mcp_context.log to send progress updates back to the client.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=651 target="_blank"">10:51</a> MCP Server Setup: Showing the code for setting up an MCP server using fast_mcp.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=714 target="_blank"">11:54</a> Design Pattern: Inference Inside the Tool: Explaining the benefit of having the tool perform its own LLM inference to reduce the context burden on the main agent.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=747 target="_blank"">12:27</a> Main Application Code: Reviewing the client-side code that defines the agent and registers the MCP tool.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=796 target="_blank"">13:16</a> Observability with Logfire: Switching to the Logfire UI to trace the execution of the agent's query.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=849 target="_blank"">14:9</a> Observing Sampling in Action: Pointing out the specific span in the trace that shows the tool making an LLM call back through the client via sampling.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=888 target="_blank"">14:48</a> Inspecting the SQL Query: Showing how the observability tool can be used to see the exact SQL query that was generated by the internal agent.<br><a href="https://www.youtube.com/watch?v=bmWZk9vTze0&t=915 target="_blank"">15:15</a> Conclusion: Final summary of the talk's points.<br><h3>Transcript</h3><div class='max-width'><p>I'm talking about MCP is all you need. A bit about who I am before we get started. I'm best known as the creator of Pydantic, data validation library for Python that is fairly ubiquitous, downloaded about 360 million times a month. So someone pointed out to me that's like 140 times a second.</p><p>Pydantic is used in general Python development everywhere, but also in Gen.ai. So it's used in all of the SDKs and agent frameworks in Python basically. Pydantic became a company beginning of 23 and we have built two things beyond Pydantic since then. Pydantic AI, an agent framework for Python built on the same principles as Pydantic, and Pydantic Logfire observability platform, which is the commercial part of what we do.</p><p>I'm also a somewhat inactive co-maintainer of the MCP Python SDK. So MCP is all you need. It's obviously a play on Jason Lu's talks, Pydantic is all you need. He gave at AI engineer, I think first of all, nearly two years ago, and then the second one, Pydantic is still all you need, maybe this time last year.</p><p>And it has the same basic idea that people are overcomplicating something that we can use a single tool for. And I guess also, similarly, the title is completely unrealistic. Of course, Pydantic is not all you need. And neither is MCP for everything. But it has that we have the I think where where we agree is that there are an awful lot of things that MCP can do and that people are overcomplicating the situation sometimes trying to come up with new ways of doing agent to agent communication.</p><p>Um, I'm talking here specifically about autonomous agents or code that you're writing, I'm not talking about the, um, uh, Claude desktop or cursor, uh, Zed, windsurf, etc. Use case of coding agents. Those were what MCP was originally primarily designed for. Um, I don't know whether or not David Pereira would say that, that what we're doing using MCP from Python is that, you know, he definitely wouldn't say it was the primary, uh, use case for, um, for MCP.</p><p>So two of the, of the primitives of MCP prompts and resources probably don't come into this use case that much. They're very useful or should be very useful in the kind of cursor type use case that don't really apply in what we're talking about here. Um, but tool calling the third primitive is extremely useful for what we're trying to do here.</p><p>Um, tool calling is a lot more complicated than you might at first think. A lot of people say to me about MCP are, but couldn't it just be, uh, open API? Why do we need this, uh, custom protocol for doing it? Um, and there's a number of reasons. The idea of dynamic tools, the tools that come and go during an agent execution, depending on the state of the server logging.</p><p>So being able to return data to the user while the tool is still executing sampling, which I'm going to talk about a lot today, perhaps the most confusingly named part of MCP, if not tech in general right now, uh, and stuff like tracing observability. Um, and I would also add to that actually, uh, uh, uh, MCP's way of being allowed to operate as effectively a sub process over standard in and standard out is extremely useful for lots of use cases.</p><p>And open API wouldn't, wouldn't solve those problems. So this is the kind of prototypical image that you will see from lots of people of what, uh, MCP is all about. The idea is we have some agent, we have any number of different tools that we can connect to that agent.</p><p>And then the point is that like the agent doesn't need to be designed with those particular tools in mind. And those tools can be designed without knowing anything about the agent. And we can just compose the two together in the same way that, uh, I can go and use a browser and the web application that the website I'm going to doesn't need to know anything about the browser.</p><p>I mean, I know we live in a kind of monoculture of browsers now, but like at least the ideal originally was we could have many different browsers all connecting over the same protocol. MCP is following the same idea, but it can get more complicated than this. We can have situations like this where, uh, we have tools within our system, which are themselves agents and are doing agentic things, need access to an LLM.</p><p>And they of course can then in turn connect to other tools over MCP or directly connecting to tools. This, this works nicely. This is elegant, but there's a problem. Every single agent in our system needs access to an LLM. And so we need to go and configure that. We need to work out resources for that.</p><p>And if we are, um, using remote MCP servers, if that remote MCP server needs to, um, use an LLM, well now it's worried about what the cost is going to be of doing that. What, what if the, uh, remote agent that's operating as a tool could effectively piggyback off the, uh, the model that the original agent has access to.</p><p>That's what sampling gives us. So as I say, I think sampling is a somewhat, uh, not making that any bigger, unfortunately. Um, is that clear on screen? I may, maybe I'll make it bigger like that. Um, sampling is this idea of all of a way where within MCP, the protocol, the, um, server can effectively make a request back through the client to the LLM.</p><p>So in this case, client makes a request, starts in some sort of agentic query, makes a call to the LLM. LLM comes back and says, I want to call that particular tool, which is an MCP server. Uh, client takes care of making that call to the MCP server. The MCP server now says, Hey, I actually need to be able to use an LLM to answer whatever this question is.</p><p>So that then gets sent back to the client. The client proxies that request to the LLM, receives the response from the LLM, sends that, uh, onto the MCP server. And the MCP server then returns and we can continue on our way. Um, sampling is very powerful, not that widely supported at the moment.</p><p>Um, I'm going to demo it today with Pydantic AI where we have support for sampling. Well, I'll be honest, it's a PR right now, but it will be soon. It will be merged. Um, we have support for sampling both as a, uh, as the client. So knowing how to proxy the, those LLM calls and as a server, basically being able to register, use the MCP client as, as the LLM.</p><p>So this example is obviously like all examples trivialized or simplified to be, to fit on screen. The idea is that we, we're building a like research agent, which is going to go and research open source, uh, packages or libraries for us. And we have implemented one of the many tools that you would in fact need for this.</p><p>And that tool is, um, making, uh, I will switch now to code and show you, uh, the one tool that we have, uh, I'm in completely the wrong file. Here we are. Um, so this tool is querying Big Query, the Big Query public dataset for, uh, PyPI to get, uh, numbers about the number of downloads of a particular package.</p><p>So this is, this is pretty standard Pydantic AI, uh, Pydantic AI code. We can pick a log file, which I'll show you in a moment. We have the dependencies that the, uh, that the agent has access to while it's running. We said we can do some retries. So if the agent returns, if the LLM returns the wrong data, we can send a retry.</p><p>A big system prompt where we give it basically the schema of the table, uh, tell it what to do, give it a few examples, yadda yadda. But then we get to this is the probably the powerful bit. So as an output validator, we are going to go and, first of all, we're going to strip out, uh, markdown block quotes from the SQL.</p><p>Um, if they're there, then we will, uh, check that the table name is right, that it's querying against and tell it that it shouldn't, if it shouldn't. And then we're going to go and run the query. And critically, if the query fails, we're going to, uh, raise model retry within Pydantic AI to go and retry, uh, making the, um, uh, making the request to the, um, LLM again.</p><p>Saying, asking the LLM to, uh, attempt to, to retry this. And what we're, the other thing we're doing throughout this, you'll see here, is we have this context dot depth dot mcp context dot log. So you'll see here when we define the depth type, we said that that was going to be an instance of this MCP, uh, context, which is what we get when you call the MCP server.</p><p>So what we're doing here is we're having a, we're providing a type safe way within, in this case, um, the agent validator, but it could be in a tool call if you wanted it to be, to access that context. And so we can see here that we know, uh, um, in the type hint, uh, uh, uh, the, the, the type is, uh, MCP context.</p><p>And so we have this log function and we know it's signature and we can go and make this log call. The point is this is going to return to the client and ultimately to the user watching before the, the thing is completed. So you can get kind of progress updates as we go.</p><p>MCP also has a context, a concept of progress, which I'm not using here, but you can imagine that also being valuable. If you knew how far through the query you were, you could show an update in progress. So the idea that I think the original principle of, uh, logging like this is that you have the, the cursor style agent running.</p><p>And we want to be able to give updates to the user. Don't worry. I'm still going before it's finished and exactly what's happening. But you could also imagine this being useful if you were using MCP. If this was research agent was, uh, running as a web application, you wanted to show the user what was going on.</p><p>This deep research might take, you know, minutes to run. We can give these logs while the tool call is still executing. And then we're just going to take the, the output, turn it into a list of dicts and then format it as XML. So you get a nice, uh, models are very good at basically reviewing XML data.</p><p>So we basically return whatever the query results are as that kind of XMLish data, which the LLM will then be good at, uh, interpreting. Now we get to the MCP bit. So in this code, we are setting up an MCP server using fast MCP. There are two versions of fast MCP right now.</p><p>Confusingly. This is the one from inside the MCP SDK. Um, we, the doc string for our function. So we're registering one tool here, PI PI downloads. And our doc string from that function will end up becoming the description on the tool that is ultimately fed to the LLM that chooses to go and call it.</p><p>Um, and we're going to pass in the user's question. I think one of the, one of the important things to say here is of course you could set this up to generate the SQL within your, uh, central agent. You could include all of the, um, uh, description of the SQL, the instructions within your, within the description of the tool.</p><p>Uh, models don't seem to like that much data inside a tool description, but more to the point, we're just going to blow up the context window of our main agent. If we're going to ship all of this context on how to make these queries into our main agent, that's just all overhead in all of our calls to that agent, regardless of whether we're going to call this particular tool.</p><p>So doing this kind of thing where we're doing the inference inside a tool is a powerful way of effectively limiting, uh, the context window of the, of the main running agent. And then we're just going to return this output, which will be a string, the value returned from, from here.</p><p>And we'll just run the, run the MCP server and by default the MCP server will run over standard IO. Um, and then we come to our main application. So here we have a definition of our agent and you see we've defined one MCP server. That's just going to run the, the script I just showed you, the PI PI MCP server.</p><p>Um, and so then this agent will act as the client and has that registered as the tool to be able to call. Uh, we're also going to set the, give it the current date. Uh, so it doesn't, uh, assume it's 20, 2023 as they often do. Um, and now we can go and ultimately run our main agent, ask it, for example, how many downloads Pydantic has had this year.</p><p>And I'm going to be brave and run it and see what happens. Uh, and it has succeeded. And it has, uh, gone and told us, uh, that we had whatever 1.6 billion downloads this year. But probably more interesting is to come and look at what that looks like in Logfire.</p><p>So if you look at, is it going to come through to Logfire or are we having a failure here as well? This, I will admit, this is the run from just before, uh, I came on stage. But it would look exactly the same. So I'm not going to talk too much about observability and how we do, uh, how MCP observability or tracing works within MCP.</p><p>Because I know there's a talk coming up directly after me talking about that. So think of this as a kind of, uh, spoiler for what's going to come up. But you can see, we, we run our outer agent. It decides to, it calls, uh, uh, QPT 4.0, uh, which decides, sure enough, I'm going to go and call this tool.</p><p>Uh, it doesn't need to think about regeneration with SQL. It can just have a natural language description of the query that we're trying to make. We then, uh, this is the MCP client. As you can see here, MCP client then calls into the MCP server. Um, makes the, which then again runs a different, uh, Pydantic AI, uh, agent, which then makes a call to an LLM, which happens through proxying it through the client.</p><p>So that's where you can see the service going client server, uh, client server. Ultimately, if you look at the top level, uh, exchange with the model, you'll see here. Yeah, the, the, the, the ultimate output was it, which had the, the return response from running the query was, was this kind of XML data.</p><p>And then the LLM was able to turn that into a human description of what was going on. So, I think the other interesting thing probably is we can go and look in. We should be able to see the actual SQL that was called. So this is the agent call inside, uh, MCP server.</p><p>And you can see here the SQL it wrote. And you can confirm that it indeed looks correct. Um, I am going to, uh, go on from there and say, um, thank you very much. Um, we are at the booth, the, the pedantic booth. So if anyone has any questions on this, wants to see this fail in numerous other exciting ways.</p><p>Very happy to, to talk to you. Yeah. Come and say hi. .</p></div></div></body></html>