<html><head><title>Full Workshop: Realtime Voice AI — Mark Backman, Daily</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Full Workshop: Realtime Voice AI — Mark Backman, Daily</h2><a href="https://www.youtube.com/watch?v=nxuTVd7v7dg" target="_blank"><img src="https://i.ytimg.com/vi_webp/nxuTVd7v7dg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>I'm Mark with Daily. This is Alesh. And then we have a few other Daily folks: Quinn, Nina, Varun, and then I'm not sure where he went but Philip from right there, from the Google team, Google DeepMind team. So this session we're going to spend just a few minutes getting everyone started.</p><p>The idea here is going to be a hands-on workshop where all the folks I just called out are going to be available to help out. We'll walk you through a quick start to get you up and running and then the idea is to build something. So build a voice bot in the next 78 minutes and 12 seconds or whatever time we have left.</p><p>The one, I guess there's one consideration is the Wi-Fi. If you don't have good Wi-Fi you might want to try to tether. I was able to tether and it worked fairly well but the conference Wi-Fi was a little shaky. This is real-time so you will be streaming data. It does require a viable connection and not just sending a few bits over.</p><p>So just a heads up if you hit that as a snag. So I guess before I get started, who here knows about PipeCat or has built anything with voice AI? Okay. A smaller audience. Has anyone built any real-time applications with LLMs or AI? Maybe slightly bigger? Okay. Great. So PipeCat is a, it's a open-source repo.</p><p>It's a Python framework for building voice AI. voice and AI multimodal agents. And it's built by the team at Daly. But we're an open-source, it's an open-source project that anyone can contribute to. It's been around for, I don't know, just over a year now? Yeah, like I would say officially PipeCat was March 2024, something like that.</p><p>Okay. So 13 months, there we go. So just a quick walk through, maybe just to kind of ground everyone in the thinking around voice AI. These slides weren't built for this talk, but I'm going to use them. So the, you know, voice AI or real-time applications are tough because there's just, you know, we as humans communicate all the time with each other, thousands, tens of thousands of years of evolution baked into our brains.</p><p>So it's pretty tough to make a machine, you know, work on the same level. So we have great expectations being the user in it. So, you know, you need a good listener, some of those smart and conversational. You need to be connected to data stores. It has to sound normal or natural.</p><p>Think back to even just maybe two, three years ago, what your voice bots sound like, and many of them, if you call them on the phone, still sound like it needs to sound natural. And actually, kudos to the Google team, the latest Gemini Live native audio dialogue is quite good in that regard.</p><p>It has to be fast. So the whole end-to-end communication needs to happen. And roughly, you know, kind of the benchmark is around 800 milliseconds. You could strive for more. I think we see maybe on the human level, it might be 500 milliseconds or somewhere on that order. So it is pretty fast.</p><p>So there's a lot to kind of to get all the way there. And this is something that we at Daily, everyone building PipeCat has been working very, very hard on getting all the way to meeting all these expectations. So just to kind of ground you in some of this, since we're going to be working in PipeCat.</p><p>PipeCat has a pipeline. I don't know if maybe Alessio, you want to talk a little bit about the origin of that quickly. Sure, sure. You can think about it as a multimedia pipeline. And you would think what is the multimedia pipeline is basically just think about like boxes that receive input and input could be audio or video.</p><p>And then those boxes just will stream those same data or modify data or new data to the following, to the following elements or processors in PipeCat. Well, we call them processors. So in PipeCat, you would have a pipeline where you have a transport, which is the the transport of your data or the input of your data.</p><p>For example, when you're talking, you could be talking in a meeting. So that would be the audio of the user. Then you would have another box following that, which is the speech to text service. So the speech to text service would be the audio of the user. It would transcribe it, then you would get text.</p><p>That would be the following data that goes through the pipeline. And then the next one would be the LLM. So now the LLM has what the user has said. And then it generates output, whatever, whatever the LLM that would be tokens, then those tokens are converted into text to speech.</p><p>And then the text to speech outputs audio and then the audio goes back to the transport so you could hear what the LLM has said. Today, what we're going to do today with Gemini Live, a lot of those boxes go away because the LLM will do a bunch of these things.</p><p>It will do transcription, it will do the LLM, it will do the text to speech in one of these boxes. But you might still require, for example, if you want to save the audio, record the audio into a file, you need a bunch of utilities to do that. And pipecat has all that built for you.</p><p>So basically, that's it. Yeah. I mean, a lot of this is really just its orchestration. So if you think about what pipecat offers, it's orchestration. It also offers a lot of abstractions for a lot of common utilities, like Alesh had said. So recording, transcript outputs, artifacts you might want to produce, or even ways that you might manipulate the information in the pipeline itself.</p><p>So this image here, which is what Alesh actually just talked through, is what you would call, I guess, a cascaded model, where you have this flow through of information. So we're going to be, you can build with Google and many different services in this way. In the last year, there's been an emergence of speech to speech models that now take audio in natively and audio out natively.</p><p>And those models also allow for audio in, and then optionally, text and/or audio out. So you can actually, for example, take a raw, you know, microphone input, or, you know, audio input, and then the model would run all of its logic. So you can actually opt to have it output text, if you want to say, parse the text output before speaking.</p><p>So there are a few different demos we'll look at that offer that. And in pipecat, we show kind of all the ways to do things, because that's what we offer, or at least what pipecat offers as a value proposition. So the-- Yeah, just one thing. I don't think you'll mention it in the slides, but all these boxes, you can pluck and play the service you want in pipecat.</p><p>So the speech to speech, speech to text, it could be, I don't know, Deepgram, for example, the LLM could be Google, or OpenAI, or whatever. You can just pluck and play any service you want. Right, yeah. The modularity, I guess, is the other big strength. So there's no, you know, you can change out a service without changing out your underlying application code, which makes it easy.</p><p>And we see with this, a lot of companies that are building for voice AI might have maybe even a more complex thing. A pipeline here runs straight down, but you can actually have split branches where you might have one leg that's running some logic and the other running different.</p><p>We call that a parallel pipeline. So if you wanted to have, say, a failover, if vendor A goes down, you can move to vendor B dynamically, even within the same conversation. That's something that pipecat affords as well. And that can allow you to transfer context over. So a lot of really cool stuff.</p><p>The goal again today being to get you familiar with just building a voice agent and building one to get started. So one of the cool things, like Alesh had pointed out, that with the cascaded models, there's a lot of complexity, but with your speech to speech model, things get dramatically simplified.</p><p>You know, your code may have looked something like this. This is like an old example of some like a ton of orchestration in the pipeline. But with a speech to speech model, you may be able to simplify it down to this, but then you have to remember, you actually need orchestration around it.</p><p>So it does get simpler to some regard. So it's more about the services you interface with. I think with that, why don't we transition now because I'm realizing we have only about maybe 70, 70, 75 minutes left to looking at the actual activity for today. All right, so there's a public repo.</p><p>I don't know how big or small this is, but it's under daily co. So on get out daily co daily dash co slash gemini dash pipecat dash workshop. I'll give everyone a chance to give everyone a chance to make sure internet's working. Can everyone see the repo? that's really tiny.</p><p>Yeah. Okay. I should have it in big text somewhere. No, no, not really. No. There we go. All right. Let's take a look at this. So what I want to do, so I spent a little bit of time, Elish and I spent time writing up this repo. This is meant to be just a jumping off point.</p><p>I'm going to get you oriented and then I want to look through one of the bot files, which is kind of the main pipecat code with you. And then we'll break and make this an interactive session where we can answer a bunch of questions. So in the repo, you could either start doing it now or maybe take a pause, but this will give you the steps to walk through getting the quick start running.</p><p>Before we do that, I want to take a moment here. Let's see how the Wi-Fi is doing. It's not a good sign. It's down. Well, hey, you know, it is tough to do Wi-Fi for this many people. So very, very tough. Instead of real time, it's going to be real slow.</p><p>Real slow. Yeah. Real slow voice communication. All right. Yes. Yeah. There will be some. Okay. So here is this GeminiBot.py files. This is all Python again. So everything will be in Python. There will be some client code options, which we'll look at in a second. Just to orient you, I'll just jump right into the meat of the pipeline.</p><p>We have this main function that runs your bot. Everything is going to run kind of encapsulated within an AIo HTTP session. We're going to pass that session around. That's more of just kind of the mechanics of things. In our pipeline, let's just jump to the simple part here. We'll have daily as the transport, daily as we are a WebRTC provider as well as we build pipecat.</p><p>There will be context aggregation. So one important note, when you speak, every turn of the bot is like a discrete point in time. And this is maybe less so the case for a speech-to-speech model. But for basic LLMs, they get discrete inputs. Everything is like a REST API call.</p><p>So you're going to get a snapshot of the conversation. The context aggregator is going to collect all the bits of the conversation, both from the user and the assistant. And we'll put that into the form of what the LLMs can handle. So this, in this case, is more for function calling and kind of logistics and management.</p><p>Gemini is amazing because it offers a lot of this for you. But if you're to build with, say, the -- just build with Gemini not live, the actual kind of just the text-based LLM, you'd have to have this context aggregation. That will then go to your LLM, which is going to be Gemini live, Gemini multimodal live.</p><p>And then it's going to be outputted through daily again on that side of the transport. So you have daily. We'd configure our service, which takes a number of arguments. Like you set up a room with a token, give it a name, and then have some properties. There are docs, which I'll link to.</p><p>It's linked in the quick start. There's also a Gemini multimodal live LLM service, which is a pipecat class that is a wrapper around the Gemini live API. So this, again, you just initialize and run. With the LLM, you see we do a few special things. We're going to define tools.</p><p>This one has just two really basic kind of canned functions. Fortunately, we're not calling out to the internet because it's not working very well. Our connection is not working well. So this one just has the dummy, like fetch weather, and we'll give you a restaurant recommendation. So these are two handlers that when your function is called, we'll just return this result information.</p><p>So we have the actual functions themselves that are defined in this function schema, which is -- you can use just native function definitions using whatever LLM format. We also created this function schema, which is a universal schema that lets you define and move between any LLM without having to kind of transform your LLM calls from OpenAI to Anthropic to Gemini to Bedrock.</p><p>Because they're all a little bit different or Grock. You know, they all have slightly different formats. So this is more of kind of a universal transform for that. And then they're collected and translated into the native format in this tool schema. So we'll pass the tools then to the Gemini service, and that's how it gets access to use and run those tools.</p><p>There's also a prompt above, which I think in this simple example, we just say, hey, you're a chat bot. You have these tools available, and that's that. We're also setting up our context aggregation, which, for better or worse, we use OpenAI as kind of the default, like the lingua franca for context.</p><p>So everything gets kind of folded back into OpenAI at a certain level. And then we define the pipeline. So the pipeline is, again, like Elesh had said, just a list of all of your different -- or I guess a tuple of all of your different services that are running in the pipeline.</p><p>And you can write your own. So if you want to instead make the LLM output text and you want to either extract information from them, maybe you have it in code, some XML or some type of information, you can actually extract it, store it. Maybe your application does something with it.</p><p>Or maybe you want to inject a text-to-speech type of frame. You can actually do that by separating the LLM, that audio output, from audio output to just be text. And then you would add, like, a text-to-speech service here. Or you could write your own processor, which may be not within the next hour.</p><p>And then lastly, all of these -- or not all, but many of them have events they emit. The transport emits handlers for when the client connects and disconnects. So in this case, we use this line here to actually inject a context frame into Gemini to kick off the conversation.</p><p>So when your client application connects, it's going to queue a frame. So again, frames being kind of the base format for information. Think of it as like an object for your pipeline. You're going to queue one of those frames. And what this function does is it just grabs the latest context.</p><p>So when we set this one above, I think that just says hello, that's going to pass that into -- basically push that into the pipeline, which then it will make its way to Gemini to initialize the conversation. And the rest of this, you could think of as boilerplate to run it.</p><p>You create a runner, which then is the thing that actually runs your task. And the task is what runs the pipeline. So maybe beyond today, but just know that that's something that's required to run your code. Okay. I'm going to pause here just for any questions, because I do want to get to developing soon.</p><p>So you said with the web app, you can provide a web app, you can provide a web app. Mm-hmm. We provide both. The whole topic that is a talk in itself. The short answer is if you're building a client-server app, you should, with strong emphasis, use WebRTC. It has a whole bunch of properties that are relevant, like error correction, better audio quality, et cetera, et cetera.</p><p>If you're building server-to-server, so one of the options today is to build a phone chatbot, you can use -- and it's probably the best option to use WebSockets. So you can bring your own transport. We actually -- in PipeCat, there's a fast API version of that that's a server that you can use to exchange messages with a WebSocket.</p><p>So, yeah, it's really up to you. So I guess maybe the takeaway is if you're building client-server, you really want WebRTC, you could technically use WebSockets, but you'll hit like a long tail of errors when you get to production. And then server-to-server, totally fine. You're going to be fine with WebSockets.</p><p>Could anyone download the repo by any chance? Did anyone not download the repo? Just one person? Two persons? Three, four? Wi-Fi is dead. Wi-Fi is dead. Can anyone -- can you tether? Is that an option? Do folks have tether hotspots on your phone? Or -- that's even shaky. Oh, geez.</p><p>Okay. I can dance. I could do some -- I don't know. You know, I'm not really -- you know, it's not my thing, but I could try. True. Yeah, I guess I could just make this code walkthrough. Or we could write it. What -- yeah. Yeah. Can I have its own VAD?</p><p>Or -- Gemini does have its own VAD. Yeah. What's your key to the choice? Gemini does have its own VAD. In fact, if you're using a speech-to-text service, that likely also brings its own VAD to the equation. What we've found is that -- so maybe to use some of our extra time because we're having internet issues -- the VAD serves a really important purpose of detecting when a user starts speaking.</p><p>So in the whole kind of life cycle of a turn, that user speaking kind of ushers in the user's turn for the conversation. So PipeCat will emit a user started speaking frame, and that will also push through an interruption. So the user will interrupt, like, anything that's talking to -- if the bot was speaking or whatever.</p><p>It basically clears the way because the user has expressed that they want to speak. The idea with the VAD is that we want it to be extremely accurate and extremely fast. So running something on-device -- we recommend Solero, which is an open-source option. It works incredibly well. I don't know what the inference time is like -- I don't know -- millisecond -- extremely fast.</p><p>So you're going to get an event back fast. In fact, you have the ability to tune how long to hear human speech before that event gets emitted. And the defaults are pretty good in PipeCat, and there are maybe scenarios where you want to change that. But the VAD is a really important consideration.</p><p>It's extremely low CPU consumption. Quinn has a great spreadsheet of breaking down the full cost analysis of an agent. And really, the CPU is going to be extremely low. Interestingly, the TTS tokens or characters are the most expensive by far. So when you think about it, running that local VAD gives you superior performance.</p><p>And it allows you -- and there's not much of a cost to it. I mean, it's maybe like a fraction of 1% to run a local VAD. But, yeah, you have all sorts of choices. But we find that to work really well. It's integrated with phone carriers, too. So there are -- I found out -- I didn't work much with phone.</p><p>Varun's been our phone expert, though. I don't -- Varun is like a figure in the WebRTC community. He's like an author of many things. But he's also a phone expert, which I don't know if that crosses over too much. Maybe that happened before. Phones are super complicated, how you actually make calls.</p><p>PipeCat supports all of them. So maybe a very quick list. You can make a WebSocket connection with a phone provider like a Twilio or Telnyx or Plevo or Exotel and exchange media streams. So that's a way to have just a native WebSocket connection from PipeCat to Twilio. You'd call Twilio.</p><p>It's going to emit the WebSocket and there'll be a handshake to get connected. You can also use PSTN, which is a public switch telephone network, which lets you dial in. And that's going to be kind of a different mechanism. There's also SIP, which is its own separate thing. Again, and all of the telephony providers would also support this as well.</p><p>With SIP, you would call something, say, like Twilio. You would call into, say, a server or a SIP provider like Daily, which offers SIP provided rooms. And you then have the ability to kind of bring the two together via that SIP connection. The nice thing about SIP is that you have the ability to have superior call control.</p><p>It is slightly more complicated, whereas that WebSocket connection is instantaneous. Your bot needs to be up and running. So there's a whole, we're not going to talk about it today, but like cold starts for agents. They need to start immediately. So if you don't have resources provisioned, you don't want your users waiting like 20 seconds while the bot comes online.</p><p>So a long-ish, medium-ish answer for a very complicated question. Maybe you didn't know that. Yeah, yeah, yeah. Yeah. Yeah. Yeah. I'll let you answer that. Yeah, yeah. Cartesia of-- Yeah, can you, I didn't hear it. Well, I think you were asking if PyCAD compares to, or how it compares to Cartesia.</p><p>Is that-- Yeah, Cartesia is, well, I think you're asking if PyCAD compares to, or how it compares to Cartesia. Yeah, Cartesia is, well, I think they're going to do more stuff. But as of today, it's a text-to-speech service. And you can clone your voice, or you can, you know, and then they provide a real-time API.</p><p>You can just via WebSockets, you pass the text, and they reply with audio, basically, with audio frames. And then PyCAD integrates with Cartesia as any other text-to-speech service, like 11 labs or anything. Think about PyCAD as a, it's just a framework for developers where they can plug and play the service they want.</p><p>So they can plug and, they can take Cartesia, they can put Cartesia, they can take it out, put 11 labs. Oops, we're even closer. Okay, now I can't hear myself. Yeah, you can plug and play any service you want. Like, you can change LLM, you can use Llama, you can use Anthropic, you can use Cartesia or 11 labs.</p><p>Then for speech-to-text you can use Deepgram, or you can use one box, like Gemini Live, which has all that built for you. So, is that clear? Yeah, yeah, okay. What about what you do? About what, sorry? One way, I mean, we're talking about, what is the product, right? For example, what about, right?</p><p>So, what about this, how are you, like, talking about, what about, right? Like, you can say, I don't know, can I see that the model is going to be small, something that is correct to the platform, and the, or, and the user, say something that could not be, you know, like, a version or whatever, right?</p><p>Mm-hmm. I'm not sure if I understood. Are you talking, like, how to ensure that the LLM says what the right thing, or not the right thing? Yeah, well, PipeCAD doesn't have control about that. It's up to you to define the prompt or define how the LLM will reply. You can put, if you want, you can put, you will be able to write your own processors, that we call them, those little boxes, and check what the LLM has said, for example, before it's, like, put some kind of real-time eval, like, to make sure that the LLM has, you could, you could do that, yeah.</p><p>All that, that pipeline is very flexible, so you can put whatever you want there, like, in parallel, not in parallel. For example, Mark was saying about parallel pipelines, like, if you have video and audio at the same time, with Gemini Live you can do everything in one box, but let's say you don't have Gemini Live, you want to use other services, that one does video and the other one does audio.</p><p>So, you know, you can have a parallel pipeline, which, you know, it's like a tree, right? You have your transfer input, and then if it's audio, it goes this way, if it's video, it goes that way, and you can, you can do things dynamically like that, yeah. This next question, yeah.</p><p>Yeah, I have a related question. Is that common that people put some kind of check in place, and you see, like, what is the actual latency of what that produces, and then for a role of, like, Gemini Live, that produces audio, it seems like, would it also produce the text with it, or do you have to kind of do speech-to-text again, and then back to it?</p><p>Okay. Sure. Okay. So the question was, are guardrails requirements, and how do people use them, and how does that apply for speech-to-speech models? So the answer is, they're not required, and there is a challenge here, and actually, I talked about this on, like, one of the first slides. Latency is absolutely critical, so what you want to avoid are unnecessary turns.</p><p>You know, obviously, LLMs are amazing language processors, so if you had all the time in the world, you could do hallucination checking against the LLM. There are other strategies to handle this. One of the big things is that we see, because of the aggregate nature of the context, it grows over the course of the conversation.</p><p>Actually, you can find better accuracy with the responses if you have more control over how you prompt the LLM. So this is a whole topic and talk in itself. What we found is there are two ways to handle this. Well, at least two. One, if you, for a lot of conversations, they're going to be task-oriented.</p><p>So let's say something simple like a restaurant reservation bot. It may have to take your name, get your time, log the time to a database. You can chunk that out, even that small conversation, into just discrete tasks, and LLMs are really good at following the most recent input. So if you kind of feed it task by task, that helps.</p><p>Also, if you control the context window, like the size, it can really be beneficial to kind of manage that really judiciously. So you could either reset. One example might be, let's say you're building a patient intake bot at a doctor's office. The very first thing it may do is verify the date of birth, which serves no utility beyond just the very first, you know, checkpoint.</p><p>So you may actually remove that from the context, like completely get it out of there. Because otherwise, it's just cruft that hangs on. And instead, you kind of reset and then maybe roll through the tasks. You could also, for really, really long conversations, summarize the context. So you may want to do an out-of-band LLM call.</p><p>And this is something actually, Quinn, just that we talked internally about this, that we're going to see more and more of this mixture of LLMs where, even in the context of real time, you may have an out-of-band, like, REST call to the text-based LLM just to do a summary and then return it back so that you can kind of compress that context window.</p><p>And just to give a call out to Google, the live API, so maybe transitioning there, they offer context management through a bunch of different strategies. Like a rolling, they have a rolling window or sliding window. I think they offer, like, token caps for that so you can have some control.</p><p>Or, if you want, you can output text and then kind of do whatever you want with it. They also take text input, so there's a lot of flexibility with speech-to-speech models. They do offer, or they do pose some other maybe development challenges, but offer, like, tremendous benefits in terms of the features they offer.</p><p>I think there's a question for a long time. I'm going to hold questions just for a sec, because I've heard the Wi-Fi is back. Can folks try downloading the repo again? Slack channel. Slack channel. Yes. Quinn, can you maybe come to the mic? I can't remember what you told me.</p><p>Workshop voice Gemini pipe cat with dashes in between on AI engineer Slack. Okay. Does everyone know where that is? I know this is, like, day one, hour, three. Workshop dash voice dash Gemini dash pipe cat is the channel name. So, if you go to the AI engineer Slack and search for workshop Gemini, it should come up.</p><p>And Quinn will be posting links as we go along if the Wi-Fi stays on. If anyone can get on that channel, can you raise your hand? Can people, like, do they have, do you guys have access to the Slack, AI engineer Slack? Okay. Good. Slack. There's two channels. There's two channels.</p><p>There's two channels. Well, we're just giving it. We're just giving it. Yeah. Oh, yeah. There's, like... Yeah. So, we'll see what that happened when the Wi-Fi was down and we were trying to create it. All right. They'll take a question over here. On your batch, there should be the join the Slack group.</p><p>All right. I'd say... I'm going to... I'll take that one and I'll... If I could... Let's just try... Listen and also try to get the repo. I'd like to walk through the quick start and then we can look at some examples. I... You know, it's something actually... Quinn has done a ton of work with this.</p><p>I have not personally. I mean, there's obviously massive latency benefits because you cut out network round trips all over the place. so you save a ton. And depending on where you are in the world, that can save a lot. If you're US-based, you know, your network latency is going to be relatively low.</p><p>But a lot of the developers in the community are in Europe and a lot of these AI services are relatively new with, you know, data centers only in the US. So, there are different challenges when doing that, though, when running it. Actually, there are great hosting providers like Modal that offer really good options for running, like your own local LLM, which then you're not, you know, you're buying or I guess leasing like the GPU time instead of running everything on a GPU, which would probably be cost prohibitive because of the way processes run.</p><p>But that's a... That is also a talk in itself. Next question. Sure. Oh, yeah. Definitely. Well, the large context windows definitely cause LLMs to... Yeah, I'm sorry. The question was around state management with LLMs and whether it's better to, I guess, chunk or have more kind of deterministic input versus just a large context where you just dump everything in.</p><p>This is actually an extension of what the other gentleman was asking. The idea being that... And actually, so, daily we built the chat widget that's on the homepage of the voice AI World's Fair page. I personally built that. What was interesting there is that... And Alesh and I were just talking about this.</p><p>Function calls in the context of real time are still slow, unfortunately. Like, too slow. Actually, Gemini... I'll give more props to Gemini being like maybe one of the fastest. If you run like a basic local, like one of these demos and you ask it what the weather is, it will return back with time to first bite in, I don't know, less than 500 milliseconds.</p><p>Whereas other vendors, not trying to throw open AI under the bus, but it has gotten slower. Like the... You might see upwards of 1.5 to 2 seconds of waiting just to get that first token back. And we dug into... Actually, this is just something recently this morning. The issue is when you get the normal streamed response for the conversation, you can start playing that audio out once you get the first sentence.</p><p>The issue being when you get a tool, you need the entire JSON response before you can actually do anything with it. So that's slow. That's one part of it. Separately, chunking the prompts is absolutely the way to go. And building that world's fair bot, I was kind of balancing between the two worlds because you can talk to it and ask it about speakers for the session.</p><p>Route 1 would have said, let's use like a mag approach and put all of the speaker JSON in something that could be a tool that could be accessed. And I tried that and unfortunately, it's just a little too slow. It's a giant context. It takes a while to come back.</p><p>What's interesting is if you instead move that all just directly into the context with Gemini Live. It's a little bit variable, but under good conditions, you'll get a response back on that like 800 millisecond latency. So it actually has access to like the full context. The one trade-off though is what this gentleman over here was asking is accuracy.</p><p>It's going to get confused, especially when you get a JSON with a lot of speaker. And this isn't, this is all LLMs. It's not specifically Gemini. With that type of, even structured data becomes very hard to kind of discern what's what when a lot of it looks the same.</p><p>So it's all, I mean, a lot of this is emerging like other things in AI, trying to just do it as fast as possible with voice. Before I take one more question, I want to check in on have folks have any luck with the repo got thumbs up. All right, I'm going to pause on questions for the time being.</p><p>We can take them at the tail end. I do want to try to go through the quick start if we could. That would be great. And again, for now, we consolidate channels. There's one channel in the Slack. It's workshop, hyping voice, hyping Gemini, hyping pipe cat. When is in there, answer questions, we're going to join and we're going to share links here.</p><p>So Mark, that's great. Okay, great. So I would recommend maybe we'll just take like a few minutes of independent getting set up. If you all go to the read me on in the Gemini pipe cat workshop. And if you'd roll through the first few steps, maybe get a, I don't know, a hand up.</p><p>You could just flash it up real quick so I could see when people start to get through it. You don't have to hold it up. If you don't mind, if you brought a device. Okay, we've got one, two. So for this workshop, I have leaked my key, a key from one of my accounts, which I'll cycle after this.</p><p>So you don't need to sign up for a daily account. You do need to sign up for a Gemini account. I don't have a key I can just give out. So in the environment.example, you'll see there's already a daily key, which you can use. Do people know how to sign up for a Gemini?</p><p>I have it in the read me. You have it in the read me. It's through AI studio. Is that a good spot to do it? Okay. So. All right. I'll take a question while we're waiting. So in terms of the pipe, that's a good question in terms of the pipe cat interface, you interface with the pipe cat class.</p><p>So there's a service class in within pipe cats. So how you write your application code is going to be uniform across all of the services. The individual providers haven't really, unlike the text based LLMs, haven't really settled on kind of a standard. So pipe cat handles all that translation on your behalf.</p><p>And I think there are other frameworks that do similar things. The idea is to provide like a uniform, simple interface. So that you could take, and this is part of the modularity. If you wanted to, you could swap this bot out for open AI real time or, you know, a text based model with a TTS and STT paired with it.</p><p>That's kind of the whole idea. There is maybe a little bit in terms of the one thing LLM providers, maybe I don't know if anybody here can nudge anyone you know. The system instruction or system prompt is not uniformly dealt with. I don't know if that's well understood. But open AI has this like user that is system that you can inject anywhere at any time, which is really fantastic.</p><p>But Anthropic and Google require like a named system instruction that's a special one time, like at constructor time instruction. So there's some differences. As much as best we can, we unify. All right. Any -- how are folks doing on getting Quick Start going? Is that a question or -- okay, I'll take -- maybe it's related to the Quick Start.</p><p>Oh, yeah. Great topic. Yeah, so there's a question about noisy environments, which is like voice AI's kryptonite. So with the VAD, no, not at the moment. But there are -- again, this is where like PipeCat is the assembler of all things. You want to plug in -- that you can run separately from the VAD.</p><p>We found like Crisp is one that's a partner of ours. They have a fantastic noise cancellation. You can run something outside of the loop that would actually clean up. like in the -- in PipeCat, it would be in the transport itself. So in that audio input, it would take the audio input and remove any ambient noise.</p><p>So like chip bags opening or dogs barking. But maybe more impressively, human background voice. So it will remove that from the feed. So you could be in this conference and it picks up the primary speaker for the device like incredibly well. At the moment, they're the only ones that I'm aware that know how to do that and do it that well.</p><p>But it's -- I mean, it's phenomenal. But you're right. The VAD is -- I mean, it was one of the biggest problems that we saw until we found Crisp and they're fantastic. Is that C-R-I-S-P? K-R-I-S-P. K-R-I-S-P. So big props to the Crisp team. Which speech? What do you guys use right now?</p><p>We use all of them. PipeCat's open source. So we bring -- I think options are Gemini, Multimodal Live, OpenAI Realtime, and then AWS just launched a new one called Novosonic. So we have those three within PipeCat. They're all pretty -- they're all very much -- actually, well, they're all very similar.</p><p>I'm not going to -- I don't want to, like, nitpick on all of the vendors here. But they're -- I mean, they're -- they have strengths and weaknesses each because it's still an emerging field. But they're -- latency-wise, that is not an issue. Latency is fantastic for all the providers.</p><p>Yeah. All right. Okay. Maybe Alesh will walk through a quick start. Why don't we -- he's going to just do some live coding here, and maybe this will help to understand how this all works. And then perhaps we -- I stop chatting, and maybe you could grab me if you have questions and just do some heads-down working time.</p><p>Okay. Yeah, I cannot type, and -- Teamwork. Teamwork. Teamwork. Yeah. I'll try from the very, very -- from nothing, from scratch. So this is a Python project. Actually, let me -- let me try again. So, yeah. This is a Python project. The first thing we'll do is create an environment, like a virtual environment that it's called.</p><p>I like to call it .amv. That's how you create a virtual environment in Python. A virtual environment will have -- Oh, yeah. What's that? Your screen is hard to see. Oh, it's hard to see. How do I do that? Can you change your theme to, like, white? I guess.</p><p>Can you change your theme to, like, white? I guess. Can you change your theme to, like, white? I guess. Can you change your theme to, like, white? I guess. Can you change your theme to, like, white? I guess. Can you change your theme to, like, white? I guess. Can you change your theme to, like, white?</p><p>I guess. Can you change your theme to, like, white? I guess. Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white?</p><p>Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change your theme to, like, white? Can you change something? Oh, yeah. And we also need a VAT analyzer, which is gonna be a Cilero VAT analyzer. So the transport is gonna be able to use this VAT analyzer to detect if the user has spoken or not.</p><p>What's that? Oh, yeah. And this is gonna be AI engineer. There we go. Okay, now we have the transport. Now we're gonna create the LLM. In this case, it's Gemini Live, so I don't need to create a speech-to-text or text-to-speech. We just create the Gemini Live. And for that, I'll need to copy it, 'cause I don't know that from memory.</p><p>But, oops. That's in this file. GeminiBot. I just wanna copy these lines here. There we go. All right. So this is my LLM. And, again, it uses this Gemini Multimodal Live LLM service. Gemini Multimodal Live LLM service. That's gonna add my import. Okay. And now it needs a couple of things.</p><p>The system instruction, which is like what the agent is gonna do, and some tools. The tools, I'm gonna skip them for now. So let's do the system instruction. Again, I'm gonna copy it from somewhere. There it is. This is like the prompt. This is like the prompt. Like the main prompt of the, well, yeah.</p><p>All right. So the system instruction is you're a helpful assistant who can answer questions and use tools. For now, we're not gonna use any tools. You know what? Let me get rid of the tools. Just copy here for later. I'm gonna command this out. And you are just the helpful assistant.</p><p>Okay. So that's... All right. So no complaints here. All right. And now we just create the pipeline. I'm gonna avoid storing the context because I don't think we need it for now. And this is the pipeline. The pipeline just receives a list of processors or elements. And the first one is a transport.input.</p><p>That's the input transport. So how we get audio from the daily room in this case. The LLM. And the transport.output. All right. Now we need... This is just defines... A pipeline also is another processor. So you could build a pipeline of pipelines of pipelines of pipelines of pipelines. So you can build...</p><p>Or you can plug and play the way you like that. So how do you run a pipeline? You need a task. What we call a pipeline task. That receives a pipeline. And the pipeline task also has some params. Which are called pipeline params. And we're gonna say... Oops. That we allow interruptions.</p><p>And I think that's enough. And how do you run a task? You can create more than one pipeline task if you wanted. In this case we just have one. Usually you just have one. You're gonna create a runner. And guess what? It's called pipeline runner. It's a pipeline runner.</p><p>And then we just do... Await. Runner. Run. Task. And some completions, please. Pipeline. Runner. All right. And I think that's it. We'll try it. Oh. OS. Import OS. I think. I think there's no more warnings. Oh. And I need to load the environment variables. Which is this line here.</p><p>Load.end. I'm just copying it from another file. Okay. And where do we get load.end? This is just a function that imports the environment variable. All right. And yeah. Let's try. I'm just gonna open the, oops. I'm just gonna open the terminal here. And I'm just gonna run it. I think we call it bot.py.</p><p>No model. Oh. Maybe I need to install the requirements. I forgot this step. There it is. Okay. So at the beginning I wrote that file requirements.txt. Which has had a bunch of, well, I got just a few requirements. But I forgot to, to install them. In the meantime, I'm just gonna go to the daily room that I just pointed the bot to.</p><p>No video. Okay. So that's, right now it's just me in that room. So, and now we just have to wait for this to, to finish. And hopefully the bot will join the room and we'll be able to talk to it. Hopefully. Yeah. How it forces, like daily as part of the splutter.</p><p>Like that room. Like it's an interchangeable. It is in the, yeah, it's, this is because we're using the daily transport. And the daily transport just connects to a daily room. So, but you could have a, a web socket transport. And then use Twilio with a phone number. And Twilio being connected to that.</p><p>If we have time, we can even try that. Um, so I think. You wanna, you wanna talk? Oh. Yeah. Yeah. Yeah. I'll wait. So we also, uh, in pipe cat, we also have added, um, based off of the AIO RTC Python package, which is how, uh, WebRTC package in Python.</p><p>We've added a, a new transport called small WebRTC transport. It is a peer to peer WebRTC communication that's free. So it's separate from any vendor. Uh, though the one downside is that it requires a turn server, which we, you bring your own. So we, we didn't, you know, we weren't prepared for that for the conference.</p><p>And also just the conference wifi makes that a little challenging. But normally if you're running any of the, any of the, uh, we call them foundational examples in pipe cat. Think of them as the like essential, um, examples that show how to do very specific functions. There's probably about a hundred of them in pipe cat, but one by one, it shows you how to like record or add an STT or push frames or show images or sync images and, and sound.</p><p>Those all use the peer to peer WebRTC transport. So we, we would have loved to have used that. You wouldn't need a key, but unfortunately firewall rules have trumped. You wouldn't need a key. All right. Uh, so I'm just running the bot and see how it fails because it has to fail the first time.</p><p>Yeah. This is the mass there. Cause there's a bunch of the, um, Python packages and Python just decides to take a time to load them. But you see how easy it was to write, um, like an agent, like a voice agent with Gemini live. Uh, if it worked, it'd be just a few lines of code, uh, that we wrote in, I don't know how long it took me, but, uh, maybe like five, 10 minutes.</p><p>Um, yeah. Are there any questions on the example or what's that? It worked. It worked? Yeah. What worked? The bot worked? Yeah. Okay. All righty. All right. Nice. Yeah. There's a couple of things that popped up there with the words later in it. We have, I don't know the actual number of customers, but I mean, pipe cat probably serves hundreds of thousands of calls a day.</p><p>I don't know. Well, a lot. Quinn, you probably have a better idea. Oh yeah. I mean, pipe cat is made by some very large companies in production, uh, and people are contributing to it from Nvidia, AWS, uh, open AI, Google, uh, lots of, lots of big companies. Yeah. There's one thing we didn't mention about pipe cat is that what you see now in the screen, these runs on the server side, but we do have client as the case for Android, iOS, JavaScript, and React, and I think that's about it, but, uh, even a, a C++ client, um, uh, if you want.</p><p>Um, so yeah. So that's the server side, but you can plug your, your client and connect to the, to the agent on your, on your phone. Uh, that would be, it, that depends on the, uh, transfer you use, but yeah, you could, you could have your client connect to a daily or we support life kit as well, but to a daily room, we like daily because we are working daily, but, uh, you connect, you can connect to a daily room and then the bot would connect or the agent would connect to the daily room as well.</p><p>And then that's the transport, the web RTC transport. Yeah. I think there were questions there. I can see. Uh, say it again. Um, yes, yes, there is, um, actually, uh, for the previous version, um, I just hacked together a thing called, uh, that I call release evals, which is a bot talking to a bot.</p><p>And what it does is I put this bot, uh, up and then it joins a daily room and then I have, uh, an eval bot and the eval bot, um, what he's going to do, it has a prompt, which is ask a simple addition. Okay. And then that eval bot is going to connect to the room.</p><p>He's going to add what is two plus two. And then the other bot is going to reply two plus two is four. And the eval bot, the LLM, uh, it checks if the answer of the user is correct. And the user in this case is another LLM. So it verifies, it's like an end to end.</p><p>The good thing is we, we run the, we used to run like more than a hundred examples every release just to make sure they work. So I just got tired of it cause it's very painful and very slow. So we have these, uh, eval bot or, uh, release evals that are gonna test each service.</p><p>Like we test Gemini live, we test Cartesia, deep gram, like all the services like end to end. And then the bots basically talk to each other with voice. So that's the, that's the, the nice thing. So yeah. Okay. Maybe. Oh, is this on? Maybe real quick just to show for those that didn't see it, it is.</p><p>Hey, can you tell me a joke? Why don't scientists trust atoms? Because they make up everything. Which, if you build pipe cat, I've heard that joke probably, I mean, not like 5,000 times. Like you can even try to seed it with something different, but it's still- I will try to come up with a new joke.</p><p>Yeah. Why do, why do you always say the same joke? That's an interesting question. Why do you think people keep telling the same jokes? I don't know. You tell me. From my search, there appear to be several reasons why people repeat jokes. Enjoyment and reinforcement. People repeat things they find funny because they want to re-experience the good feeling associated with laughter.</p><p>It feels good. And repeating it is a way to try and recreate that sensation. Memory aid. Okay, okay, that's enough. Repeating something can help you remember. Yeah. Is there anything else I can help you with today? No, thank you. That was it. Thanks. Thank you so much. Well, just to show there was a question about interruptions.</p><p>We could just have it, like, my favorite is to ask it to tell you like a really long story and then interrupt it. So can you tell me a really long story? Okay, I can do that. Here's a story. And feel free to interrupt whenever you like. Once upon a time.</p><p>Okay, actually tell me that new joke. Hey, tell me that new joke. Why don't scientists trust atoms? Because they make up everything. Okay, here's one. That was the same joke. That was the same joke. All right. All right. That was it. No. Well, I find, like, a lot of conversations, like, it's like a child's based on when you pause talking.</p><p>Oh. So I think the questions asked during this workshop could map out, like, years of work. So this is, like, another one of those fantastic cutting edge things. So, again, back to, like, human evolution, we all know, and when we talk, actually, it's even hard for humans to talk to not speak over each other.</p><p>So the way that it works mechanically is when the user stops speaking, the VAD has a timeout. You tell it and program it, wait, let's say, one second, 0.8 seconds, half second, whatever feels natural. And you're trying to balance low latency response with giving the user enough time to speak.</p><p>It's a really hard thing. And it's one of the biggest complaints is that agents will speak over the human. So if you're, let's say you're building an interview bot, like, you're using, like, Tavis, one of their digital twins, you want to have, like, a real, like, likeness, and you want to speak to it.</p><p>You may take time to think, because sometimes you have to take time to think. And that's a really difficult thing for bots to do, because, again, it's driven by, like, a simple stop-speaking algorithm. So this is a new, like I said, it's like an emerging field of models, which is looking at semantic end of turn.</p><p>So driven off of things like speech filler words, pauses, intonation, so things in the audio realm, and also things in the text-based realm, so just looking at context. So we've actually started, we're one of many that are doing this, I think, but we launched a model. If you look at it on GitHub, it's under smart dash turn.</p><p>It's a native audio in classifier that runs an inference on the input audio, and it simply outputs either complete or incomplete. And the way PipeCat uses this is that if you get an incomplete response, we can dynamically adjust the VAD timeout. So we can tell the PipeCat bot, okay, he or she is not done speaking.</p><p>Let's actually move the, let's give three seconds to complete the thought. And if it's not done, then the bot will actually respond. So you can create a little bit of, like, dynamic interaction there. And that's one of the first things. I'm sure the Google team is working on similar things.</p><p>I know OpenAI is, and all the SDT vendors are also looking at their own things. So I'd say, right now, it is very much an unsolved problem. But I would imagine, given how fast things are going in the next 12 months, we'll have great solutions that will make it even more natural to talk to a bot.</p><p>It's a good question. Any more questions? Well, actually, I was kind of wondering, like, is there a way to see the transcripts of that happening? Oh, yeah. Yeah, yeah. One, this is actually back to the, well, for, this is specific to PipeCat, but also, like, Gemini Live will output audio and text.</p><p>And other speech-to-speech LLMs do this. PipeCat offers, in terms of its, again, orchestration role, when you get a, actually, it's going to be specific to TTS provider. Many, there are great TTS providers that do word and timestamp synchronization. So they'll give pairs. They call them, like, alignment pairs. So if you're using a Cartesia or an Eleven Labs or Rhyme, they all output these pairs.</p><p>One of the really cool things with PipeCat is that the TTS services output not only the audio stream, but also the text stream. So they'll output text frames, TTS text frames, we call them in PipeCat. And if you place, we have, in terms of how the client software works, there is, like, an observer role, where you can actually watch.</p><p>There's a process that can watch things that happen in the pipeline and emit events. So we've instrumented that for the clients so that whenever you see those text frames move through the transport, you can get synchronized word and audio output. So in your client, if you wanted to have word-by-word output synchronized to the audio, you can do that with PipeCat.</p><p>And it's as simple as just adding an event. I think you listen to, like, bot TTS text output or on bot TTS text, and it will give you the synchronized output. If I wanted to build a fully offline voice engine, what box in that pipeline would be hardest to do?</p><p>Fully offline? Well, they're all doable. They're great models. I think it really depends on what your bot needs to accomplish. A lot of the state-of-the-art models, to do all the best and smartest things, need to have some. Like, they're going to be run, like, on-prem or in the cloud.</p><p>But if you have -- and a lot of bots do jobs. Like, if you wanted to build, like, a restaurant reservation one, like I referenced earlier, it's a very simple job. You could probably run it with some version of Llama running locally. There are great local -- something, again, Quinn has been experimenting with -- a lot of great local models.</p><p>Like, whisper has challenges, you know. I mean, it's -- it has a lot -- it has some challenges as an open source model for STT. But there are good and emerging TTS services. So, you know, I -- things are only as good as the input. And we've actually seen this with some of the speech-to-speech models that sometimes they mistranscribe.</p><p>So, you really need -- I mean, it's -- you know, every part is critical, but if you can't transcribe the speech really well, nothing really matters. Like, it has to understand you. And having, like, disfluencies or, like, hallucinated responses or even just inaccurate responses kind of breaks everything down.</p><p>So, things mostly start at the STT. So, maybe that's the hardest. I don't know if there are a lot of good open source options for that right now. I don't know. We don't -- we're not doing anything in the STT world. No. No. No. No. That's a whole different ballgame.</p><p>Good question, though. Just made me realize we could have used local models and avoid this. We could have. Yeah. Well, we're partnering with the Google team. Yeah. Yeah. As a -- yeah, exactly. Yeah. Has anyone looked at any of the sample projects and had questions? There's a lot of interesting things that we're doing.</p><p>There's a lot of interesting things there. If any of this has, like, interested you, we do have a Discord. You're welcome to get on it. You can find us at pipecat.ai and find our Discord there. You can ask questions. There's some really cool stuff with Gemini that can be done.</p><p>There -- in particular, in the PipeCat repo, we built -- I don't know if you know the game Catch Phrase, where you describe a word and something, you know, guesses it. We built a version of that. We had to brand it something else called Word Wrangler. And you, as the human, you describe a word, and then you have the AI agent try to answer it.</p><p>So we built a client-server version of that, which I linked to in the repo. And then we have one that's a phone-based one that's, I think, particularly sophisticated and interesting. So you might think, like, how the hell would I build this with a speech-to-speech model? We actually use two Gemini agents in the same call, and we use a parallel pipeline where one agent is the host giving out the questions to the human user.</p><p>The other is the guesser. And we kind of limit the audio flow so that the guesser, the AI player, can only hear the user. So there's a bunch of really interesting things getting into, like, majorly into the weeds of some of the powers of Pipecat. But it also speaks to the strength of having just native audio input being really, really helpful.</p><p>So I'd recommend checking those out. Really cool, easy demos to run. One's Twilio. The other is, again, a client-server. I think it's, like, a React Next.js project. What's that? Word Wrangler? Yeah, I mean, we could run the Word Wrangler client app. It's actually just on the web. Test. Welcome to Word Wrangler.</p><p>I'll try to guess the words you describe. Remember, don't say any part of the word itself. Ready? Let's go. I'm going to skip to something easier. Okay, this is something you take pictures with. It's on your phone. Is it camera? All right, this is a field related to the study of languages, I think.</p><p>Is it linguistics? All right, this is a game of the yellow ball you play with rackets. Hit the ball over the net. Is it tennis? All right, this is a round dessert with chocolate chips sometimes and other fun goodies. Is it cookie? It's really good even when I'm bad at giving answers.</p><p>So, pretty cool. This is built with Gemini Live. But, again, just an example of things you can build with voice AI. So, cool, unique interactions. All right. I think that's about it. Thanks, everybody. Thanks, everybody. Thanks, everybody. Thanks, everybody. Thanks, everybody. Thanks, everybody. Thanks, everybody.</p></div></div></body></html>