<html><head><title>Realtime Conversational Video with Pipecat and Tavus — Chad Bailey and Brian Johnson, Daily & Tavus</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Realtime Conversational Video with Pipecat and Tavus — Chad Bailey and Brian Johnson, Daily & Tavus</h2><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q"><img src="https://i.ytimg.com/vi_webp/ujt0da9Z29Q/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ujt0da9Z29Q.html">Whisper Transcript</a> | <a href="./transcript_ujt0da9Z29Q.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=0" target="_blank">00:00:00.040</a></span> | <span class="t">We're here to talk about real-time conversational video with PipeCat, that's me, and with Tavis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=19" target="_blank">00:00:19.320</a></span> | <span class="t">that's Brian. We'll introduce ourselves a little bit more, but in the interest of keeping it moving,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=22" target="_blank">00:00:22.600</a></span> | <span class="t">let's talk about what we're here for. Have any of you ever seen one of these robot concierge things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=28" target="_blank">00:00:28.080</a></span> | <span class="t">Do they work? No, they don't. They're terrible, right? It's actually possible nowadays to build this kind of thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=34" target="_blank">00:00:34.380</a></span> | <span class="t">but actually good. It's a little bit tricky, but that's what we're here to show you how to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">There are three things you need to think about when you want to build real-time AI. The first is your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=46" target="_blank">00:00:46.620</a></span> | <span class="t">models. Obviously, we all know what models are. That's why we're here at this conference. The thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=51" target="_blank">00:00:51.680</a></span> | <span class="t">that you don't necessarily know you need to think about is your orchestration layer. We're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=56" target="_blank">00:00:56.220</a></span> | <span class="t">talk a little bit about that. Then, of course, you need to deploy these bots somewhere. That's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=60" target="_blank">00:01:00.420</a></span> | <span class="t">third step, deployment. We'll talk about that as well. Step one, models. I come from a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=68" target="_blank">00:01:08.040</a></span> | <span class="t">more of a traditional, funny to say that, voice AI world where the traditional pipeline people talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=73" target="_blank">00:01:13.560</a></span> | <span class="t">about is speech-to-text, so transcription, and then LLMs for your inference, and then text-to-speech.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=79" target="_blank">00:01:19.620</a></span> | <span class="t">That's the typical kind of cascading pipeline you hear. Sure enough, people nowadays are using some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=84" target="_blank">00:01:24.660</a></span> | <span class="t">voice-to-voice models. That is a use case for this kind of thing, but there are reasons sometimes you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=88" target="_blank">00:01:28.500</a></span> | <span class="t">might use one or the other. Real-time video is a lot more complicated. It doesn't have to be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=95" target="_blank">00:01:35.760</a></span> | <span class="t">but it can be, and I think Brian will tell you that it should be. There's a lot more stuff you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=100" target="_blank">00:01:40.440</a></span> | <span class="t">think about to do video generation in real-time. So, Brian, you want to tell us a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=106" target="_blank">00:01:46.620</a></span> | <span class="t">about Tavis and how you all are thinking about this? Thanks, Chad. So, Tavis started out as an AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=111" target="_blank">00:01:51.960</a></span> | <span class="t">research company, and we started off with a single model that was like a rendering model. What we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=117" target="_blank">00:01:57.920</a></span> | <span class="t">quickly realized is that we need to be able to put this into a real-time context for it to be useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=123" target="_blank">00:02:03.680</a></span> | <span class="t">so it needs to be fast. And once we did that, we started realizing there are a lot of missing pieces,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=129" target="_blank">00:02:09.060</a></span> | <span class="t">things like turn detection, response timing, picking up signals and orchestration. And we started off in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=136" target="_blank">00:02:16.980</a></span> | <span class="t">the beginning, we didn't know about PipeCat when we first built it, but we've been partnering with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=141" target="_blank">00:02:21.420</a></span> | <span class="t">PipeCat over the last year, and it's come to our realization that like a lot of the stuff PipeCat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=146" target="_blank">00:02:26.160</a></span> | <span class="t">does is going to be very important for conversational AI and making it real. I think we can go to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=153" target="_blank">00:02:33.660</a></span> | <span class="t">one. Yep. We have a demo. You can go to our site, Tavis.io. I was going to do it live,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=158" target="_blank">00:02:38.300</a></span> | <span class="t">but for the sake of time, just check it out. You can go check it out on our website, Tavis.io.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=163" target="_blank">00:02:43.220</a></span> | <span class="t">And I'll hand it back. Well, no, there's one more thing. Yep. So what we do at Tavis now is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=170" target="_blank">00:02:50.220</a></span> | <span class="t">offer a conversational video interface. It is an end-to-end pipeline that allows you to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=176" target="_blank">00:02:56.060</a></span> | <span class="t">conversation with a replica of anyone. You can create your own replica of yourself, you can put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=181" target="_blank">00:03:01.180</a></span> | <span class="t">it online, and you can have a conversation. The response time is around 600 milliseconds, but that's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=186" target="_blank">00:03:06.920</a></span> | <span class="t">ideal because a lot of times that's too fast. So we have to slow that down sometimes based on some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=191" target="_blank">00:03:11.840</a></span> | <span class="t">some of these models that we're using. And there are a lot of steps that go into this. You can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=198" target="_blank">00:03:18.260</a></span> | <span class="t">there's like, like Chad talked about, the basic layers of a conversational stack. But we also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=205" target="_blank">00:03:25.360</a></span> | <span class="t">these proprietary models, Sparrow Zero and Raven Zero, that we've created, which is kind of like our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=210" target="_blank">00:03:30.140</a></span> | <span class="t">IP or what we're offering. And we're going to, right now, we offer those in our stack. But we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=215" target="_blank">00:03:35.980</a></span> | <span class="t">moving towards a world where we're going to offer those in things like PipeCat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=218" target="_blank">00:03:38.900</a></span> | <span class="t">So models, and we'll come back to the Tavis models in a little bit and how they are getting better and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=226" target="_blank">00:03:46.100</a></span> | <span class="t">some of the cool new things that are coming from Tavis that you will want to use. Orchestration is where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=231" target="_blank">00:03:51.680</a></span> | <span class="t">my world steps in. So that's PipeCat. That's the thing on my water bottle and my shirt and my jacket and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=237" target="_blank">00:03:57.980</a></span> | <span class="t">all that kind of stuff. Let's talk a little bit about what PipeCat is. There's a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=244" target="_blank">00:04:04.060</a></span> | <span class="t">interesting phrase on Brian's slide, real time observability and control into the flow of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=249" target="_blank">00:04:09.060</a></span> | <span class="t">conversation. That's a lot of those are words that you that don't really mean anything until you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=256" target="_blank">00:04:16.540</a></span> | <span class="t">actually go build one of these things. And when you build it, the first time you use it, go, wow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=261" target="_blank">00:04:21.000</a></span> | <span class="t">this is amazing. This is great. And then as you start to actually think about what it's going to mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=264" target="_blank">00:04:24.640</a></span> | <span class="t">to have that in production, you realize, oh, wait, there are a lot of like, boring infrastructure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=270" target="_blank">00:04:30.720</a></span> | <span class="t">kinds of things that we need to solve, the ability to understand like to have observability into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=275" target="_blank">00:04:35.720</a></span> | <span class="t">how the bot is behaving and why it's behaving that way, the ability to get capture metrics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=279" target="_blank">00:04:39.220</a></span> | <span class="t">on things and understand things like, sometimes the bot takes a long time to respond, I wonder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=283" target="_blank">00:04:43.300</a></span> | <span class="t">why that is. Well, it turns out there's a whole lot of these kinds of things that you need for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=287" target="_blank">00:04:47.920</a></span> | <span class="t">a real live production app for a real live production bot that you need something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=292" target="_blank">00:04:52.440</a></span> | <span class="t">PipeCat. PipeCat is an open source framework. It's built by my company, but it is open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=297" target="_blank">00:04:57.120</a></span> | <span class="t">and actually fully vendor neutral. And it's designed to be this orchestration layer for real time AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=303" target="_blank">00:05:03.320</a></span> | <span class="t">And by that, I mean, you are you have a user that is going to be producing either video and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=308" target="_blank">00:05:08.400</a></span> | <span class="t">or audio. And you want to also be delivering video and or audio to that user. And you want to do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=314" target="_blank">00:05:14.120</a></span> | <span class="t">with a low latency as possible. That's the real time part of this whole conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=317" target="_blank">00:05:17.720</a></span> | <span class="t">If you went to the AI engineer website, and you saw the little button on the bottom right that says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=323" target="_blank">00:05:23.040</a></span> | <span class="t">talk to AIE, that's powered by PipeCat. It's actually using the Gemini live model. So it's using a voice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=329" target="_blank">00:05:29.100</a></span> | <span class="t">to voice model. But there's still so much other stuff you have to do to go from voice to voice demo bot on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=335" target="_blank">00:05:35.360</a></span> | <span class="t">the web to like like in your browser or on the web to an actual like shipping production app that even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=342" target="_blank">00:05:42.080</a></span> | <span class="t">Google themselves, even the Gemini documentation says, you can go use our own like tools and our you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=350" target="_blank">00:05:50.480</a></span> | <span class="t">know, like our browser tools and things to experiment with Gemini multimodal life. But when you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=356" target="_blank">00:05:56.080</a></span> | <span class="t">take it to production, you do need something like PipeCat to actually orchestrate what's happening in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=360" target="_blank">00:06:00.800</a></span> | <span class="t">entire app. I'm going to try to do this side very quickly. And there are a few QR codes coming up. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=369" target="_blank">00:06:09.120</a></span> | <span class="t">now would be a good time to get those buttons ready. PipeCat itself, two lists of three that you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=378" target="_blank">00:06:18.400</a></span> | <span class="t">think about to understand what PipeCat does. The first one is something I just kind of already talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=382" target="_blank">00:06:22.240</a></span> | <span class="t">about. The three things that PipeCat is doing for you is handling input. It's handling the processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=388" target="_blank">00:06:28.560</a></span> | <span class="t">and the output. Input is receiving media from your user. So in the case of a traditional voice bot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=395" target="_blank">00:06:35.200</a></span> | <span class="t">that's just voice. In the case of a Tavis replica, that's sending voice and they even they're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=400" target="_blank">00:06:40.960</a></span> | <span class="t">some interesting things that we'll talk about with inputting your users video and allowing a Tavis replica</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=406" target="_blank">00:06:46.800</a></span> | <span class="t">to respond to not only what it's hearing in the voice, but what it's seeing in the video coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=411" target="_blank">00:06:51.680</a></span> | <span class="t">from the user. Getting into that, that's the processing part. That's step two. That's where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=417" target="_blank">00:06:57.600</a></span> | <span class="t">essentially you're going to run through a bunch of different models. In some cases, you can do almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=422" target="_blank">00:07:02.480</a></span> | <span class="t">all of what you need with a single model. In the case of like Gemini multimodal live for voice or a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=427" target="_blank">00:07:07.120</a></span> | <span class="t">Tavis replica, there is a way that you use Tavis inside a PipeCat bot where you can basically let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=432" target="_blank">00:07:12.960</a></span> | <span class="t">Tavis kind of do everything for you. Run kind of as just one integrated piece. And then, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=439" target="_blank">00:07:19.040</a></span> | <span class="t">all of those models, hopefully, this is supposed to be real time and interactive video. Hopefully,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=443" target="_blank">00:07:23.040</a></span> | <span class="t">those models are producing some kind of output that you want to show to your user. That's the video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=447" target="_blank">00:07:27.360</a></span> | <span class="t">and the audio being produced by your tools. In a typical voice bot, that is, you know, that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=452" target="_blank">00:07:32.960</a></span> | <span class="t">text to speech that is being played out as audio. It might also be things like UI updates. If you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=458" target="_blank">00:07:38.240</a></span> | <span class="t">in a web app that you're pushing UI updates, that kind of thing. And of course, in the Tavis case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=461" target="_blank">00:07:41.840</a></span> | <span class="t">it's video and audio that are hopefully presented in a way where the video stays synchronized to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=466" target="_blank">00:07:46.640</a></span> | <span class="t">audio, for example. That's a really, really hard thing to do well, depending on exactly how you build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=470" target="_blank">00:07:50.800</a></span> | <span class="t">this whole thing. The three fundamental pieces of the of PipeCat that enable those things to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=477" target="_blank">00:07:57.440</a></span> | <span class="t">are frames, processors, and pipelines. PipeCat's name comes from the fact that it is about building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=482" target="_blank">00:08:02.800</a></span> | <span class="t">a pipeline and a pipeline is comprised or is composed of processors. Processors are things that handle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=490" target="_blank">00:08:10.640</a></span> | <span class="t">frames. Frames are essentially any, it's basically a type container for a kind of data. So, in a PipeCat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=498" target="_blank">00:08:18.640</a></span> | <span class="t">pipeline, you will see a whole bunch of frames with things like little snippets of user audio,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=503" target="_blank">00:08:23.520</a></span> | <span class="t">like 10 or 20 milliseconds of audio comes across as an audio frame or video frames from the user's camera</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=509" target="_blank">00:08:29.760</a></span> | <span class="t">device you can capture. But even things like voice activity detection, VAD, comes across as a user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=514" target="_blank">00:08:34.960</a></span> | <span class="t">started speaking frame in PipeCat. All of those frames progress through a series of processors and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=520" target="_blank">00:08:40.160</a></span> | <span class="t">processor just takes in some frames and outputs other frames. So, a good example would be like the LLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=527" target="_blank">00:08:47.680</a></span> | <span class="t">processor, for example, is taking in frames that are essentially context frames, like completed context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=534" target="_blank">00:08:54.560</a></span> | <span class="t">turns from the user in the bot and it is outputting a stream of text frames. So, if you're capturing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=540" target="_blank">00:09:00.560</a></span> | <span class="t">streaming output from your LLM, in PipeCat that looks like a bunch of text frames coming out of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=546" target="_blank">00:09:06.400</a></span> | <span class="t">processor. And all those are put together in a pipeline and the pipeline is how you describe what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=551" target="_blank">00:09:11.760</a></span> | <span class="t">you want your bot to do. And the idea behind how PipeCat runs your pipeline is that it's doing all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=556" target="_blank">00:09:16.720</a></span> | <span class="t">that stuff asynchronously and doing its best to minimize the latency of every piece of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=562" target="_blank">00:09:22.720</a></span> | <span class="t">as it goes through the pipeline. So, there is a much better and longer explanation. I know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=568" target="_blank">00:09:28.640</a></span> | <span class="t">that was a lot. There's a much better and longer explanation in the PipeCat docs that is that QA file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=574" target="_blank">00:09:34.560</a></span> | <span class="t">In terms of what it actually looks like, it was going to be a little tight to try to get in and do some live</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=579" target="_blank">00:09:39.760</a></span> | <span class="t">coding during 15 minutes. But this is a QR code that links to this example file. There's so much stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=586" target="_blank">00:09:46.800</a></span> | <span class="t">in the PipeCat repo that shows you this. But just to step through these pieces real quick, at the top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=591" target="_blank">00:09:51.760</a></span> | <span class="t">there's the transport input. This is the core pipeline inside this bot file. And this is actually one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=597" target="_blank">00:09:57.360</a></span> | <span class="t">the Tavis examples that we have in the repo. First thing is transport input. That's where the frames come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=603" target="_blank">00:10:03.040</a></span> | <span class="t">in from your media transport. So, whether it's WebRTC or WebSockets or Twilio WebSockets or anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=609" target="_blank">00:10:09.040</a></span> | <span class="t">like that, frames start pouring in from the transport input. They go to a speech-to-text processor. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=614" target="_blank">00:10:14.560</a></span> | <span class="t">where transcription is happening. So, for example, one thing that frame processor is doing is it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=619" target="_blank">00:10:19.760</a></span> | <span class="t">collecting snippets of audio at, you know, a frame at a time, 20 milliseconds at a time. But it is sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=625" target="_blank">00:10:25.200</a></span> | <span class="t">up to your transcription processor, whatever that is, deep gram or whisper running on something or whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=630" target="_blank">00:10:30.560</a></span> | <span class="t">to, exactly, collect a bunch of frames, collect however many frames it needs to then output a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=635" target="_blank">00:10:35.680</a></span> | <span class="t">snippet of transcription information, right? So that happens in speech-to-text. From there we go into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=640" target="_blank">00:10:40.160</a></span> | <span class="t">something called the context aggregator. That's because the transcription or the STT processor is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=646" target="_blank">00:10:46.000</a></span> | <span class="t">emitting transcriptions whenever it feels like it. So we use other frames in the pipeline that have made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=651" target="_blank">00:10:51.520</a></span> | <span class="t">their way through to understand, okay, the user has started talking. The user's microphone has, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=656" target="_blank">00:10:56.480</a></span> | <span class="t">microphone level has dropped. So it looks like the user has stopped talking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=659" target="_blank">00:10:59.760</a></span> | <span class="t">Maybe now is a good time to group all of the various pieces of transcription we've gotten over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=663" target="_blank">00:11:03.840</a></span> | <span class="t">the past few seconds together and emit a single context aggregation frame. That's what triggers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=669" target="_blank">00:11:09.360</a></span> | <span class="t">the LLM to run. And so we grab the context. And if you, you know, of course, if you've programmed with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=674" target="_blank">00:11:14.640</a></span> | <span class="t">the LLMs, you know you get the context with all the array of messages and the tools and everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=678" target="_blank">00:11:18.480</a></span> | <span class="t">You show that to the LLM and then it starts streaming tokens back. Those tokens come out of LLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=683" target="_blank">00:11:23.280</a></span> | <span class="t">as text frames as well as there's like a start and end frame. And if you, if you're familiar with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=687" target="_blank">00:11:27.840</a></span> | <span class="t">approach, you can probably see all these other frames as they start to exist in here. But then TTS</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=692" target="_blank">00:11:32.960</a></span> | <span class="t">essentially accumulates those and generate speech. This bot file is actually an older example that uses an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=701" target="_blank">00:11:41.040</a></span> | <span class="t">older Tavis model where we were actually generating audio. And then we were sending the audio over, I believe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=707" target="_blank">00:11:47.520</a></span> | <span class="t">a WebSocket, it's not important. We were sending audio to a Tavis model that was generating the video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=714" target="_blank">00:11:54.000</a></span> | <span class="t">based on the audio and then sending back to us, back to PikeCat, audio and video. So essentially the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=719" target="_blank">00:11:59.840</a></span> | <span class="t">audio, but synchronized with the video. Those come as a different series of frames that then go out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=724" target="_blank">00:12:04.240</a></span> | <span class="t">transport output. And that is, again, essentially the same transport that we're using on the import side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=729" target="_blank">00:12:09.600</a></span> | <span class="t">input side, but this is the output side. And so that's where all that media goes back to the other user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=734" target="_blank">00:12:14.320</a></span> | <span class="t">So you can, you can start to see how with this structure, um, it looks very simple right here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=740" target="_blank">00:12:20.640</a></span> | <span class="t">but it is incredibly powerful when you realize that you can kind of put anything you want in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=744" target="_blank">00:12:24.480</a></span> | <span class="t">pipeline. Uh, we have people, for example, that like there's a construct in PikeCat called parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=749" target="_blank">00:12:29.280</a></span> | <span class="t">pipelines. And so we have people that have this exact same workflow, but at the same time in real time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=754" target="_blank">00:12:34.240</a></span> | <span class="t">they're running another LLM that is doing things like, um, you know, sentiment analysis, or we have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=760" target="_blank">00:12:40.880</a></span> | <span class="t">there's, there's, there's one, uh, PikeCat user I talked to that is using Gemini live multimodal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=766" target="_blank">00:12:46.960</a></span> | <span class="t">detect if the person answering the phone is a person or if it's a voicemail greeting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=772" target="_blank">00:12:52.960</a></span> | <span class="t">but they have a separate, they have separate pipelines running for whether it's a voicemail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=776" target="_blank">00:12:56.880</a></span> | <span class="t">or whether it's a human. And all that happens in PikeCat through the use of a parallel pipeline,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=780" target="_blank">00:13:00.480</a></span> | <span class="t">run one model to determine, and then it sends a signal to the back to the pipeline to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=784" target="_blank">00:13:04.800</a></span> | <span class="t">do the voicemail branch or do the human branch. So you can start to get a, get an idea of what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=790" target="_blank">00:13:10.880</a></span> | <span class="t">can build, even if you have a model like Tavis that is doing 90% of the hard work of making the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=797" target="_blank">00:13:17.440</a></span> | <span class="t">interaction feel good. There's just enough other stuff that's going to happen around the periphery</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=802" target="_blank">00:13:22.560</a></span> | <span class="t">that it just makes a lot of sense to wrap what you're doing inside something like PikeCat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=808" target="_blank">00:13:28.160</a></span> | <span class="t">This is what, so Brian showed a picture, this is that same Tavis avatar. Um, if you go to the QR code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=815" target="_blank">00:13:35.280</a></span> | <span class="t">on the last slide, um, which is going to come up again in a second, um, you can basically run that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=821" target="_blank">00:13:41.600</a></span> | <span class="t">example like you should need to sign up for Tavis, you get a key, you drop a key in there, you run that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=825" target="_blank">00:13:45.440</a></span> | <span class="t">example code on modified and it will pop up this UI where you can both talk to that avatar in real time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=831" target="_blank">00:13:51.680</a></span> | <span class="t">talk to the replica in real time, but also you can see some of like the interesting guts of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=836" target="_blank">00:13:56.080</a></span> | <span class="t">happening inside PikeCat in that debug panel over there. Do you want to tell us a little bit about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=840" target="_blank">00:14:00.960</a></span> | <span class="t">why this architecture is interesting and what we can do in the near future with it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=845" target="_blank">00:14:05.920</a></span> | <span class="t">Yeah. So as I mentioned, when we first built Tavis's conversational video, video interface,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=850" target="_blank">00:14:10.720</a></span> | <span class="t">we built it ourselves because we didn't know about PikeCat. So we've spent the last year learning a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=855" target="_blank">00:14:15.680</a></span> | <span class="t">the lessons that PikeCat has already solved. There are a ton of orchestration, aggregation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=864" target="_blank">00:14:24.640</a></span> | <span class="t">communication functionalities that are in PikeCat already that are going to basically save you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=870" target="_blank">00:14:30.720</a></span> | <span class="t">months of time. I mean, it's going to save you a lot of time. So, um, when we first talked about having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=876" target="_blank">00:14:36.960</a></span> | <span class="t">this talk, I was like, we're not using PikeCat internally. I was like, I can't really say we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=882" target="_blank">00:14:42.320</a></span> | <span class="t">using it internally, but the thing is our customers that have come to us that are enterprise customers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=886" target="_blank">00:14:46.960</a></span> | <span class="t">they're using PikeCat and they want to be able to use our stuff in PikeCat. So now we're, we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=892" target="_blank">00:14:52.880</a></span> | <span class="t">ready to move our best models into PikeCat. We've already moved Phoenix, which is our rendering model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=898" target="_blank">00:14:58.240</a></span> | <span class="t">but we're also going to be moving turn taking response, timing, perception models, things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=904" target="_blank">00:15:04.480</a></span> | <span class="t">And eventually we're going to made up and actually bring PikeCat internally as well because it, I spent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=910" target="_blank">00:15:10.720</a></span> | <span class="t">like the last couple of days actually debugging a problem that pipe gets already solved really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=914" target="_blank">00:15:14.960</a></span> | <span class="t">And I don't want to have to do that anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=916" target="_blank">00:15:16.240</a></span> | <span class="t">Yeah. So I talked about these models that are coming. So we, we, we have a couple different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=924" target="_blank">00:15:24.720</a></span> | <span class="t">unique models. Our turn detection model is a multilingual model that determines when a person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=931" target="_blank">00:15:31.360</a></span> | <span class="t">is done speaking. You wouldn't believe how important that is in a conversational AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=936" target="_blank">00:15:36.240</a></span> | <span class="t">It's going to make your AI faster and it's going to make it so it doesn't interrupt people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=942" target="_blank">00:15:42.960</a></span> | <span class="t">simultaneously. If you, if you have a very fast, a conversational pipeline, oftentimes it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=950" target="_blank">00:15:50.800</a></span> | <span class="t">actually talk over the user. But, and if you have a slow one, it will take so long to respond that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=956" target="_blank">00:15:56.400</a></span> | <span class="t">people will be like, is it broken? You want to get the best of both worlds. And that's what turn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=959" target="_blank">00:15:59.920</a></span> | <span class="t">detection does. We're also working on a response timing model right now. And that response timing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=965" target="_blank">00:16:05.520</a></span> | <span class="t">we're bringing all these to PikeCat soon. That response timing model will determine how quickly it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=970" target="_blank">00:16:10.240</a></span> | <span class="t">should respond even though the person's done. Because if I'm telling you about my, my, my grandmother</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=975" target="_blank">00:16:15.600</a></span> | <span class="t">who's like going into a, into a, into a home and she's sad, you're not going to want to like quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=980" target="_blank">00:16:20.640</a></span> | <span class="t">respond to that. You, you want to think and take your time, right? But if we're having a chit chat, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=984" target="_blank">00:16:24.400</a></span> | <span class="t">want to be fast. So that's what that's all about. And then finally, our multimodal perception is able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=989" target="_blank">00:16:29.440</a></span> | <span class="t">look at emotions, look at the surroundings, what the person's wearing. And also we'll be feeding that into the turn taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=996" target="_blank">00:16:36.160</a></span> | <span class="t">and the response timing so that we're, we're able to provide much more nuanced conversational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1003" target="_blank">00:16:43.520</a></span> | <span class="t">flow. So those things are coming to my point. And so, and so this is another example, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1009" target="_blank">00:16:49.360</a></span> | <span class="t">I will tear through the last of these because we're, we are already out of time and that's my fault.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1012" target="_blank">00:16:52.880</a></span> | <span class="t">This is another example showing essentially a different way that you can integrate Tavis into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1016" target="_blank">00:16:56.640</a></span> | <span class="t">PikeCat. And this is part of the flexibility. As they develop new models, there are going to be things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1021" target="_blank">00:17:01.120</a></span> | <span class="t">will run directly inside Tavis. There are things that you want to have a little bit of control. And so you just drop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1025" target="_blank">00:17:05.360</a></span> | <span class="t">them into, into a slightly differently shaped pipeline and you can get your bot to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1029" target="_blank">00:17:09.600</a></span> | <span class="t">do what you want to do. Um, I will talk about step three, which is deployment extremely quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1034" target="_blank">00:17:14.960</a></span> | <span class="t">Um, there are a lot of different ways that you can ship these bots. PipeCat is I, sometimes I call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1040" target="_blank">00:17:20.000</a></span> | <span class="t">open source to a fault. I wish it had a little, a few more opinions on some things. Um, really what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1046" target="_blank">00:17:26.000</a></span> | <span class="t">need is kind of two pieces. You need some kind of rest API to essentially, to allow your app,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1051" target="_blank">00:17:31.760</a></span> | <span class="t">whatever your client app is, you need some kind of basic rest API to tell your app that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1057" target="_blank">00:17:37.440</a></span> | <span class="t">that a user wants to talk to a bot. And when that happens, you need something to relatively quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1062" target="_blank">00:17:42.240</a></span> | <span class="t">spin up a new instance of your bot and connect it to that user. And this is what essentially that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1066" target="_blank">00:17:46.480</a></span> | <span class="t">showing here. Um, and then you also need a thing we haven't talked about again, go read the docs is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1071" target="_blank">00:17:51.200</a></span> | <span class="t">the transport layer. That's the, that's the hopefully web RTC part that actually moves the media back and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1076" target="_blank">00:17:56.720</a></span> | <span class="t">forth. That's part of what your infrastructure is configuring. You have a user that wants to use a bot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1081" target="_blank">00:18:01.360</a></span> | <span class="t">you need a, you need an API that can start a bot and get, and connect that bot to your user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1086" target="_blank">00:18:06.640</a></span> | <span class="t">The very short version of how, if you want to just solve this problem with a little bit of money,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1090" target="_blank">00:18:10.640</a></span> | <span class="t">come talk to us at our booth because pipecat cloud is like, if you don't want to mess with Kubernetes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1094" target="_blank">00:18:14.800</a></span> | <span class="t">and all that kind of stuff, if Kubernetes makes you, we used to have this thing in Heroku where it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1098" target="_blank">00:18:18.800</a></span> | <span class="t">replace Kubernetes with scare quotes around it in the Heroku Slack, which was fun. Um, come to this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1103" target="_blank">00:18:23.680</a></span> | <span class="t">This is Mark, uh, one of my colleagues talking a lot more about pipecat cloud and how we solve the problems of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1110" target="_blank">00:18:30.160</a></span> | <span class="t">deploying bots at scale and how you can either use pipecat cloud. But if you want to actually just do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ujt0da9Z29Q&t=1114" target="_blank">00:18:34.240</a></span> | <span class="t">it yourself, this is where you can learn how to do that. And that's our time. Thank you all very much.</span></div></div></body></html>