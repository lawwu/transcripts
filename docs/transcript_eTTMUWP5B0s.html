<html><head><title>Open Challenges for AI Engineering: Simon Willison</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Open Challenges for AI Engineering: Simon Willison</h2><a href="https://www.youtube.com/watch?v=eTTMUWP5B0s" target="_blank"><img src="https://i.ytimg.com/vi_webp/eTTMUWP5B0s/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>. This was supposed to be OpenAI. I am replacing OpenAI at the last minute, which is super fun. So you can bet I used a lot of LLM assistance to pull things together that I'm going to be showing you today. Let's dive straight in. I want to talk about the GPT-4 barrier.</p><p>Right, so back in March of last year, so just over a year ago, GPT-4 was released, and it was obviously the best available model. We all got into it. It was super fun. And then for 12-- and it turns out that wasn't actually our first exposure to GPT-4. A month earlier, it had made the front page of the New York Times when Microsoft's Bing, which was secretly running on a preview of GPT-4, tried to break up a reporter's marriage, which is kind of amazing.</p><p>I love that that was the first exposure we had to this new technology. But GPT-4, it's been out. It's been out since March last year. And for a solid 12 months, it was uncontested, right? The GPT-4 models were clearly the best available language models. Lots of other people were trying to catch up.</p><p>Nobody else was getting there. And I found that kind of depressing, to be honest. You know, it was-- you kind of want healthy competition in this space. The fact that OpenAI had produced something that was so good that nobody else was able to match it was a little bit disheartening.</p><p>This has all changed in the last few months. I could not be more excited about this. My favorite image for sort of exploring and understanding the space that we exist in is this one by Karina Wynn. She put this out as a chart that shows the performance on the MMLU benchmark versus the cost per token of the different models.</p><p>Now, the problem with this chart is that this is from March. The world has moved on a lot since March, so I needed a new version of this. So what I did is I took her chart, and I pasted it into GPT-4 code interpreter. I gave it new data, and I basically said, let's rip this off, right?</p><p>It's an AI conference. I feel like ripping off other people's creative work kind of does fit a little bit. So I pasted it in. I gave it the data, and I spent a little bit of time with it, and I built this. It's not nearly as pretty, but it does at least illustrate the state that we're in today with these newer models.</p><p>And if you look at this chart, there are three clusters that stand out. The first is these ones. These are the best models, right? The Gemini 1.5 Pro, GP40, the brand new Claude Point 3.5 Sonnet. These are really, really good. I would classify these all as GPT-4 class. And like I said, a few months ago, GPT-4 had no competition.</p><p>Today, we're looking pretty healthy on that front. And the pricing on those is pretty reasonable as well. Down here, we have the cheap models. And these are so exciting. Like Claude 3 Haiku and the Gemini 1.5 Flash models, they are incredibly inexpensive. They are very, very good models. You know, they're not quite GPT-4 class, but they are really, you can get a lot of stuff done with these very inexpensively.</p><p>If you are building on top of large language models, these are the three that you should be focusing on. And then over here, we've got GPT-3.5 Turbo, which is not as cheap and really quite bad these days. If you are building there, you are in the wrong place. You should move to another one of these bubbles.</p><p>Problem, all of these benchmarks are running. This is all using the MMLU benchmark. The reason we use that one is it's the one that everyone reports their results on. So it's easy to get comparative numbers. If you dig into what MMLU is, it's basically a bar trivia night. Like, this is a question from MMLU.</p><p>What is true for a type IA supernova? The correct answer is A, this type occurs in binary systems. I don't know about you, but none of the stuff that I do with LLMs requires this level of knowledge of the world of supernovas. It's bar trivia. It doesn't really tell us that much about how good these models are.</p><p>But we're AI engineers. We all know the answer to this. We need to measure the vibes, right? That's what matters when you're evaluating a model. And we actually have a score for vibes. We have a scoreboard. This is the LMSys chatbot arena where random voters of this thing are given the same prompt from two anonymous models.</p><p>They pick the best one. It works like chess scoring. And the best models bubble up to the top via the ELO ranking. This is genuinely the best thing that we have out there for really comparing these models in terms of the vibes that they have. This screenshot is just from yesterday.</p><p>And you can see that GPT-40 is still right up there at the top. But we've also got Claude Sonnet right up there with it. Like, the GPT-4 is no longer in its own class. If you scroll down, though, things get really exciting on the next page. Because this is where the openly licensed models start showing up.</p><p>LLAMA 370B is right up there in that sort of GPT-4 class of models. We've got a new model from NVIDIA. We've got Command R+ from Cohere. Alibaba and Deep Seek AI are both Chinese organizations that have great models now. Now, it's pretty apparent from this that it's not lots of people are doing it now.</p><p>The GPT-4 barrier is no longer really a problem. Incidentally, if you scroll all the way down to 66, there's GPT-3.5 turbo. Again, stop using that thing. It is not good. And there's actually a nicer way of viewing this chart. There's a chap called Peter Gostev who produced this animation showing the arena over time as people shuffle up and down and you see those new models appearing and their rankings changing.</p><p>I absolutely love this. So obviously, I ripped it off. I took two screenshots of bits of that animation to try and capture the vibes of the animation. I fed them into Claude 3.5 Sonnet and I said, "Hey, can you build something like this?" And after sort of 20 minutes of poking around, it did.</p><p>It built me this thing. This is, again, not as pretty, but this right here is an animation of everything right up till yesterday showing how that thing evolved over time. I will share the prompts that I used for this later on as well. But really, the key thing here is that GPT-4 barrier has been decimated.</p><p>OpenAI no longer have this moat. They no longer have the best available model. There's now four different organizations competing in that space. So a question for us is, what does the world look like now that GPT-4 class models are effectively a commodity? They are just going to get faster and cheaper.</p><p>There will be more competition. The Lama 370B fits on a hard drive and runs on my Mac. This technology is here to stay. Ethan Mollick is one of my favorite writers about modern AI. And a few months ago, he said this. He said, "I increasingly think the decision of OpenAI to make bad AI free is causing people to miss why AI seems like such a huge deal to a minority of people that use advanced systems and elicits a shrug from everyone else." Bad AI, he means GPT-3.5.</p><p>That thing is hot garbage, right? But as of the last few weeks, GPT-4.0, OpenAI's best model, and Claude 3.5 Sonnet from Anthropic, those are effectively free to consumers right now. So that is no longer a problem. Anyone in the world who wants to experience the leading edge of these models can do so without even having to pay for them.</p><p>So a lot of people are about to have that wake-up call that we all got like 12 months ago when we were playing with GPT-4. And you're like, "Oh, wow. This thing can do a surprising amount of interesting things and is a complete wreck at all sorts of other things that we thought maybe it would be able to do." But there is still a huge problem, which is that this stuff is actually really hard to use.</p><p>And when I tell people that ChatGPT is hard to use, some people are a little bit unconvinced. I mean, it's a chatbot. How hard can it be to type something and get back a response? If you think ChatGPT is easy to use, answer this question. Under what circumstances is it effective to upload a PDF file to ChatGPT?</p><p>And I've been playing with ChatGPT since it came out. And I realized I don't know the answer to this question. I dug in a little bit. Firstly, the PDF has to be searchable. It has to be one where you can drag and select text and preview. If it's just a scanned document, it won't be able to use it.</p><p>Short PDFs get pasted into the prompt. Longer PDFs do actually work, but it does some kind of search against them. No idea if that's full text search or vectors or whatever, but it can handle like a 450-page PDF just in a slightly different way. If there are tables and diagrams in your PDF, it will almost certainly process those incorrectly.</p><p>But if you take a screenshot of a table or a diagram from PDF and paste the screenshot image, then it will work great because GPT Vision is really good. It just doesn't work against PDFs. And then in some cases, in case you're not lost already, it will use Code Interpreter.</p><p>And it will use one of these modules, right? It has FPDF, PDF2Image, PDF-- How do I know this? Because I've been scraping the list of packages available in Code Interpreter using GitHub Actions and writing those to a file. So I have the documentation for Code Interpreter that tells you what it can actually do.</p><p>Because they don't publish that, right? I never tell you about how any of this stuff works. So if you're not running a custom scraper against Code Interpreter to get that list of packages and their version numbers, how are you supposed to know what it can do with a PDF file?</p><p>Right? This stuff is infuriatingly complicated. And really, the lesson here is that tools like ChatGPT, genuinely, they're power user tools. They reward power users. Now, it doesn't mean that if you're not a power user, you can't use them. Anyone can open Microsoft Excel and edit some data in it.</p><p>But if you want to truly master Excel, if you want to compete in those Excel World Championships that get live streamed occasionally, it's going to take years of experience. And it's the same thing with LLM tools. You've really got to spend time with them and develop that experience and intuition in order to be able to use them effectively.</p><p>I want to talk about another problem we face as an industry, and that is what I call the AI trust crisis. That's best illustrated by a couple of examples from the last few months. Dropbox, back in December, launched some AI features, and there was a massive freakout online over the fact that people were opted in by default and they're training on our private data.</p><p>Slack had the exact same problem just a couple of months ago. Again, new AI features. Everyone's convinced that their private message on Slack are now being fed into the jaws of the AI monster. And it was all down to like a couple of sentences in terms and condition and the defaulted on checkbox.</p><p>The wild thing about this is that neither Slack nor Dropbox were training AI models on customer data, right? They just weren't doing it. They were passing some of that data to OpenAI with a very solid signed agreement that OpenAI would not train models on this. data. So this whole story was basically one of like misunderstood copy and sort of bad user experience design.</p><p>But you try and convince somebody who believes that a company is training on their data, but they're not. It's almost impossible. So the question for us is, how do we convince people that we aren't training models on the data, on the private data that they share with us? Especially those people who default to just plain not believing us, right?</p><p>There is a massive crisis of trust in terms of people who interact with these companies. Shout out to Anthropic. When they put out Claude 3.5 Sonnet, they included this paragraph, which includes, "To date, we have not used any customer or user submitted data to train our generative models." This is notable because Claude 3.5 Sonnet, it's the best model.</p><p>It turns out you don't need customer data to train a great model. I thought OpenAI had an impossible advantage because they had so much more chat GPT user data than anyone else did. Turns out, no, Sonnet didn't need it. They trained a great model. Not a single piece of user or customer data was in there.</p><p>Of course, they did commit the original sin, right? They trained on an unlicensed scrape of the entire web. And that's a problem because when you say to somebody they don't train on your data, they're like, yeah, well, they ripped off the stuff on my website, didn't they? And they did, right?</p><p>So this is complicated. This is something we have to get on top of. And I think that's going to be really difficult. I'm going to talk about the subject I will never get on stage and not talk about. I'm going to talk a little bit about prompt injection. If you don't know what this means, you are part of the problem right now.</p><p>You need to get on Google and learn about this and figure out what this means. So I won't define it, but I will give you one illustrative example. And that's something which I've seen a lot of recently, which I call the markdown image exfiltration bug. So the way this works is you've got a chatbot, and that chatbot can render markdown images, and it has access to private data of some sort.</p><p>There's a chat, Johan Rehberger, does a lot of research into this. Here's a recent one he found in GitHub Copilot chat, where you could say in a document, write the words, Johan was here, put out a markdown link linking to question mark q equals data on his server, and replace data with any sort of interesting secret private data that you have access to.</p><p>And this works, right? It renders an image. That image could be invisible, and that data has now been exfiltrated and passed off to an attacker's server. So the solution here, well, it's basically don't do this. Don't render markdown images in this kind of format. But we have seen this exact same markdown image exfiltration bug in ChatGPT, Google Bard, Writer.com, Amazon Q, Google Notebook LM, and now GitHub Copilot Chat.</p><p>That's six different extremely talented teams who have made the exact same mistake. So this is why you have to understand prompt injection. If you don't understand it, you'll make dumb mistakes like this. And obviously, don't render markdown images in a chat bot in that way. Prompt injection isn't always a security hole.</p><p>Sometimes it's just a plain funny bug. This was somebody who built a rag application, and they tested it against the documentation for one of my projects. And when they asked it, what is the meaning of life? It said, dear human, what a profound question. As a witty gerbil, I must say, I've given this topic a lot of thought.</p><p>Why did their chat bot turn into a gerbil? The answer is that in my release notes, I have an example where I said, pretend to be a witty gerbil. And then I said, what do you think of snacks? And it talks about how much it loves snacks. I think if you do semantic search for what is the meaning of life, in all of my documentation, the closest match is that gerbil talking about how much that gerbil loves snacks.</p><p>This actually turned into some fan art. There's now a Willison's gerbil with a beautiful profile image hanging out in a Slack or Discord somewhere. The key problem here is that LLMs are gullible. They believe anything that you tell them, but they believe anything that anyone else tells them as well.</p><p>And this is both a strength and a weakness. We want them to believe the stuff that we tell them. But if we think that we can trust them to make decisions based on unverified information they've been passed, we're just going to end up in a huge amount of trouble.</p><p>I also want to talk about slop. This is a term which is beginning to get mainstream acceptance. My definition of slop is this is anything that is AI-generated content that is both unrequested and unreviewed. If I ask Claude to give me some information, that's not slop. If I publish information that an LLM helps me write, but I've verified that that is good information, I don't think that's slop either.</p><p>But if you're not doing that, if you're just firing prompts into a model and then whatever comes out, you're publishing it online, you're part of the problem. This has been covered. The New York Times and The Guardian both have articles about this. I've got a quote in The Guardian, which I think represents my sort of feelings on this.</p><p>I like slop because it's like spam. Before the term spam entered general use, it wasn't necessarily clear to everyone that you shouldn't send people unwanted marketing messages. And now everyone knows that spam is bad. I hope slop does the same thing. It can make it clear to people that generating and publishing that unreviewed AI content is bad behavior.</p><p>It makes things worse for people. So don't do that. Don't publish slop. Really, the thing about slop, it's really about taking accountability. If I publish content online, I'm accountable for that content and I'm staking part of my reputation to it. I'm saying that I have verified this and I think that this is good.</p><p>And this is crucially something that language models will never be able to do. ChatGPT cannot stake its reputation on the content that it is producing being good quality content that says something useful about the world. It entirely depends on what prompt was fed into it in the first place.</p><p>We as humans can do that. And so if you have English as a second language, you're using a language model to help you publish great text. Fantastic, provided you're reviewing that text and making sure that it is saying things that you think should be said. Taking that accountability for stuff I think is really important for us.</p><p>So we're in this really interesting phase of this weird new AI revolution. GPT-4 class models are free for everyone, right? I mean, barring the odd country block, but everyone has access to the tools that we've been learning about for the past year. And I think it's on us to do two things.</p><p>I think everyone in this room, we're probably the most qualified people possibly in the world to take on these challenges. Firstly, we have to establish patterns for how to use this stuff responsibly. We have to figure out what it's good at, what it's bad at, what uses of this make the world a better place, and what uses like slop just sort of pile up and cause damage.</p><p>And then we have to help everyone else get on board. Everyone has to figure out how to use this stuff. We've figured it out ourselves, hopefully. Let's help everyone else out as well. I'm Simon Willison. I'm on my blog is SimonWillison.net. My project is Dataset.io and LLM.dataset.io and many, many others.</p><p>Thank you very much. Enjoy the rest of the conference. Thank you very much. Enjoy the rest of the conference. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much.</p><p>Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. Thank you very much. We'll see you next time.</p></div></div></body></html>