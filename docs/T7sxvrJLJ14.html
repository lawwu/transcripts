<html><head><title>Moondream: how does a tiny vision model slap so hard? — Vikhyat Korrapati</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Moondream: how does a tiny vision model slap so hard? — Vikhyat Korrapati</h2><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14"><img src="https://i.ytimg.com/vi_webp/T7sxvrJLJ14/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./T7sxvrJLJ14.html">Whisper Transcript</a> | <a href="./transcript_T7sxvrJLJ14.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi. My name is Vik. I work on a model, an open-source vision model called Moon Dream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">A little bit about myself before I dive into Moon Dream. I was at AWS for about nine years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=23" target="_blank">00:00:23.920</a></span> | <span class="t">before I started working on this model. Looking at where the stock price is going,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=28" target="_blank">00:00:28.960</a></span> | <span class="t">I'm not sure if that was the right financial decision, but I'm very happy with the work I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=32" target="_blank">00:00:32.480</a></span> | <span class="t">doing. So let's dive into it. I'll talk about Moon Dream a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=36" target="_blank">00:00:36.080</a></span> | <span class="t">It is a tiny vision language model. It's less than two billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=42" target="_blank">00:00:42.160</a></span> | <span class="t">so it can run anywhere, and it's open-source, Apache 2.0, so you can use it to do anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=48" target="_blank">00:00:48.960</a></span> | <span class="t">Here are some examples of things you can do with Moon Dream. You can ask it questions about images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=55" target="_blank">00:00:55.760</a></span> | <span class="t">You can caption images. It can detect specific objects inside of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=62" target="_blank">00:01:02.000</a></span> | <span class="t">So here I asked it to tell me where the peak is, and it gives me coordinates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=65" target="_blank">00:01:05.120</a></span> | <span class="t">I can count stuff. It can do all sorts of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=68" target="_blank">00:01:08.800</a></span> | <span class="t">I had the audacity to title my talk, "How can a tiny vision model slap so hard?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=74" target="_blank">00:01:14.320</a></span> | <span class="t">So I have to back things up a little bit. And so here's me doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=79" target="_blank">00:01:19.040</a></span> | <span class="t">So these are two vision benchmarks, vision question-answering benchmarks. One is called VQA, V2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=85" target="_blank">00:01:25.120</a></span> | <span class="t">The other is called GQA. As you can see, Moon Dream has been steadily improving over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=91" target="_blank">00:01:31.280</a></span> | <span class="t">releases I've made over the last three months. I've included a reference line over there for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=95" target="_blank">00:01:35.280</a></span> | <span class="t">Lava 1.5, which is a popular 7 billion parameter vision model. So this shows you that Moon Dream gives you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=103" target="_blank">00:01:43.520</a></span> | <span class="t">a performance that's comparable to models that are about four times bigger than it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=108" target="_blank">00:01:48.480</a></span> | <span class="t">I didn't really set out to build a vision model, so I kind of got roped into it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=115" target="_blank">00:01:55.520</a></span> | <span class="t">I was originally trying to build an application that required an AI agent, so I needed to be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=120" target="_blank">00:02:00.000</a></span> | <span class="t">see what was going on on the user's screen and have it describe what's on the browser page for QA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=126" target="_blank">00:02:06.240</a></span> | <span class="t">testing automation. I tried to do this at first with GPT-4V, but there were too many safety refusals back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=133" target="_blank">00:02:13.600</a></span> | <span class="t">then. Like, if there was any human being present in the image, I would just refuse to process it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=138" target="_blank">00:02:18.240</a></span> | <span class="t">It was also going to be really slow and expensive, and so I realized if this is a product I'm trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">build, I really need to have control over the model itself. So I figured, you know what, how hard can it be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=148" target="_blank">00:02:28.080</a></span> | <span class="t">Let me just go try and build this model myself. Now, the task I was trying to perform here was fairly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=153" target="_blank">00:02:33.520</a></span> | <span class="t">constrained. I just needed to describe screens and answer questions about screens, so it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=161" target="_blank">00:02:41.760</a></span> | <span class="t">need to be generally intelligent. I had a couple of 3090s at home, so I figured I'd train a small version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=166" target="_blank">00:02:46.640</a></span> | <span class="t">of the model at home and then rent some beefier machines in the cloud to go train a bigger version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=174" target="_blank">00:02:54.560</a></span> | <span class="t">Once I got done training a small version, I was like, hey, this actually works pretty well, so I posted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=178" target="_blank">00:02:58.000</a></span> | <span class="t">it on Twitter. I thought, you know what, I might get 20 likes off of this, and then I'll move on with my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=182" target="_blank">00:03:02.320</a></span> | <span class="t">side project at the time. It blew up far beyond expectations. I was a little surprised, pleasantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=188" target="_blank">00:03:08.480</a></span> | <span class="t">surprised, but surprised nonetheless. And I immediately started seeing other automated testing companies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=194" target="_blank">00:03:14.480</a></span> | <span class="t">reach out and be like, hey, can I use this to describe browser screens? Because this would work really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=199" target="_blank">00:03:19.280</a></span> | <span class="t">great deal for us. As well as other companies, shout out to our friends at Open Interpreter from Seattle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=204" target="_blank">00:03:24.560</a></span> | <span class="t">That basically told us that they were -- I figured, you know what, like, this is getting a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=211" target="_blank">00:03:31.760</a></span> | <span class="t">traction. Let me pause on the whole automated testing app for a couple of weeks and focus on Moon Dream and see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=216" target="_blank">00:03:36.320</a></span> | <span class="t">where it goes. Yeah, so let me dive into a couple of the technical details from what makes the model succeed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=224" target="_blank">00:03:44.640</a></span> | <span class="t">despite being small. The first thing we did that I think really helped was deciding what problems the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=233" target="_blank">00:03:53.280</a></span> | <span class="t">should solve and what it should not solve. So Moon Dream wants to be a developer tool. We focus on being really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=238" target="_blank">00:03:58.400</a></span> | <span class="t">accurate and not hallucinate. It doesn't really have a lot of knowledge about the world, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=244" target="_blank">00:04:04.560</a></span> | <span class="t">if you ask it to write a poem, it's probably not going to help you. It's really focused on answering questions, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=250" target="_blank">00:04:10.960</a></span> | <span class="t">giving you -- helping you understand images. This is really important because it affects the type of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=256" target="_blank">00:04:16.400</a></span> | <span class="t">that you use and the sort of benchmarks that you want to focus on. There's a popular vision language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=262" target="_blank">00:04:22.640</a></span> | <span class="t">model benchmark called MathVester, which measures how good models are at solving math problems. You take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=266" target="_blank">00:04:26.800</a></span> | <span class="t">picture of a differential equation and you see whether the model can solve it. That was an example of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=271" target="_blank">00:04:31.920</a></span> | <span class="t">non-goal for us because we just want the model to be good at looking at images. The most we do is probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=277" target="_blank">00:04:37.840</a></span> | <span class="t">generate a latex representation of the problem. We don't really want to even attempt to try and solve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=284" target="_blank">00:04:44.240</a></span> | <span class="t">calculus. It was not pre-trained from scratch. We use -- we use a vision encoder called Siglip from Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=292" target="_blank">00:04:52.320</a></span> | <span class="t">with a pre-trained text model called PHY 1.5 from Microsoft. The notable thing over here is PHY 1.5 was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=300" target="_blank">00:05:00.960</a></span> | <span class="t">also trained on mostly synthetic data, which is very similar to our pipeline set works very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=304" target="_blank">00:05:04.640</a></span> | <span class="t">For this sort of task, pre-training from scratch doesn't really make a difference as opposed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=312" target="_blank">00:05:12.560</a></span> | <span class="t">using pre-trained models, and it is cost prohibitive. So, unless you want to get those brownie points for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=316" target="_blank">00:05:16.640</a></span> | <span class="t">saying you trained it from scratch, it's probably not worth doing. We experimented with a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=320" target="_blank">00:05:20.960</a></span> | <span class="t">different other models as they were released, and nothing really made too much of a difference. What does make a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=326" target="_blank">00:05:26.480</a></span> | <span class="t">difference, though, is training data? The latest release of Moon Dream is trained on around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=333" target="_blank">00:05:33.360</a></span> | <span class="t">35 million images, and the problem is, especially when you're on a budget, like high-quality multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=340" target="_blank">00:05:40.960</a></span> | <span class="t">training data is really hard to come by. There's companies that -- there's a lot of companies out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=347" target="_blank">00:05:47.280</a></span> | <span class="t">there that will annotate data with humans, but it's really expensive, and I've heard a rumor recently that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=353" target="_blank">00:05:53.680</a></span> | <span class="t">they won't even talk to you anymore unless you're willing to sign an upfront seven-figure commitment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=357" target="_blank">00:05:57.440</a></span> | <span class="t">There's a lot of data on the internet -- images, all text pairs. The problem with this is it's often not in the format you want it to be, and it's really noisy, and the noise is really problematic when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=371" target="_blank">00:06:11.520</a></span> | <span class="t">when you're training small models. And so synthetic data is a way to solve this, where you use that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=378" target="_blank">00:06:18.000</a></span> | <span class="t">all-text information, process it. It's a bit of an open secret that a lot of people are training on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=382" target="_blank">00:06:22.400</a></span> | <span class="t">outputs from GPT-4. You probably don't want to do that. Besides being questionable in terms of use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=391" target="_blank">00:06:31.120</a></span> | <span class="t">it's often not helpful. GPT-4 is a very powerful model. It has reasoning capabilities and knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=396" target="_blank">00:06:36.320</a></span> | <span class="t">that your small model is never going to be able to get. And so when you train it on GPT-4 outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=401" target="_blank">00:06:41.120</a></span> | <span class="t">what it learns instead is to hallucinate. It's going to generate plausible-sounding outputs that include</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=406" target="_blank">00:06:46.160</a></span> | <span class="t">details that it cannot possibly memorize, and so you end up in trouble. So this is a little important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">I'm going to go a little more technically detail for a couple of minutes to dive into how to do synthetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=417" target="_blank">00:06:57.840</a></span> | <span class="t">data. So bear with me for a sec. We'll pop back up. Here's an example of how not to do it. Coco is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=423" target="_blank">00:07:03.840</a></span> | <span class="t">a dataset. It has around 200k images. Each image has five short descriptions and a bunch of object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=429" target="_blank">00:07:09.840</a></span> | <span class="t">annotations with, like, hey, there's a bicycle at these coordinates and whatnot. And let's say you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=435" target="_blank">00:07:15.760</a></span> | <span class="t">want to take those short descriptions and these object annotations and generate more detailed captions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=440" target="_blank">00:07:20.000</a></span> | <span class="t">that include the union of all the information present over here. If you just naively call GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=445" target="_blank">00:07:25.040</a></span> | <span class="t">with this information, it generates this. It's not important to read all of it, but there's two important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=450" target="_blank">00:07:30.800</a></span> | <span class="t">things to note. The first is that it hallucinates. It says in the second paragraph there's a person near</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=457" target="_blank">00:07:37.840</a></span> | <span class="t">the right side of the harbor. I think there's, like, a person way back. There's, like, five pixels there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=464" target="_blank">00:07:44.080</a></span> | <span class="t">that may be a post. It may be a person. We don't really know. That's because object annotations were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=468" target="_blank">00:07:48.080</a></span> | <span class="t">bad. But besides that, like, the model is also taking a lot of creative liberties over here, like saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=473" target="_blank">00:07:53.280</a></span> | <span class="t">there is five yatches standing out from the rest and whatnot. And so this is -- you need to do a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=481" target="_blank">00:08:01.440</a></span> | <span class="t">more preprocessing of your data before you feed it to the model. Here's another example. There's a dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=486" target="_blank">00:08:06.400</a></span> | <span class="t">from Google called localized narratives. The task annotators here -- annotators are given here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=492" target="_blank">00:08:12.560</a></span> | <span class="t">verbally describe this image. And as you're describing the image, hover your mouse over the part of the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=498" target="_blank">00:08:18.800</a></span> | <span class="t">that you're describing. So it's nice in that it encourages people to create really detailed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=502" target="_blank">00:08:22.640</a></span> | <span class="t">descriptions that capture spatial positioning in the image. So for example, here it says the girl in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=508" target="_blank">00:08:28.800</a></span> | <span class="t">front is playing the guitar and whatnot. And spatial reasoning is something that vision language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=513" target="_blank">00:08:33.040</a></span> | <span class="t">typically tend to struggle with. I ended up having to build a fairly sophisticated data processing pipeline to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=518" target="_blank">00:08:38.560</a></span> | <span class="t">get really good results with this. Not really important to dive into the details over here. But the important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=523" target="_blank">00:08:43.760</a></span> | <span class="t">thing to note is, A, it gets really expensive. Each image ends up being 20 LLM calls. And the LLM here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=532" target="_blank">00:08:52.480</a></span> | <span class="t">Mixtral 8x7b. So it gets pretty expensive. But it was necessary. The training data is the biggest needle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=541" target="_blank">00:09:01.680</a></span> | <span class="t">mover in terms of model performance. And because of this, I'd say we spent like maybe one or two orders of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=546" target="_blank">00:09:06.880</a></span> | <span class="t">magnitude more compute on generating training data than actually training the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=551" target="_blank">00:09:11.280</a></span> | <span class="t">A couple -- so yeah, this particular data set we've open sourced. It's available on Hugging Face. Here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=558" target="_blank">00:09:18.960</a></span> | <span class="t">an example of the type of questions it generates for this image. There's an interesting question towards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=566" target="_blank">00:09:26.400</a></span> | <span class="t">the end. What theory does the kid have about the existence of pleasure in the image? I'll talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=570" target="_blank">00:09:30.240</a></span> | <span class="t">that in a sec. But basically, you want to generate a few distractor questions so the model knows to not always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=575" target="_blank">00:09:35.440</a></span> | <span class="t">agree with the question that the user is asking. So yeah, a couple of the challenges involved in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=581" target="_blank">00:09:41.920</a></span> | <span class="t">working with synthetic data. There was an interesting incident I had early on where a user was like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=588" target="_blank">00:09:48.880</a></span> | <span class="t">hey, I asked a relatively simple question. Why couldn't the model answer this? And when I looked at it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=593" target="_blank">00:09:53.920</a></span> | <span class="t">it turned out that they didn't capitalize the first letter in their question. And the model had never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=597" target="_blank">00:09:57.920</a></span> | <span class="t">seen anything like that during training. So I was like, what do I do over here? And so it's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=604" target="_blank">00:10:04.000</a></span> | <span class="t">important for you to make sure that your training data has the same rough distribution as your real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=609" target="_blank">00:10:09.120</a></span> | <span class="t">world query. So I ended up adding like an extra step where we artificially inject like capitalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=613" target="_blank">00:10:13.600</a></span> | <span class="t">issues and typos and whatnot into the model before training it. There's also this risk of what we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=619" target="_blank">00:10:19.280</a></span> | <span class="t">model collapse, where your model has biases inherent to it. So for example, if you try to ask Mixtral to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=625" target="_blank">00:10:25.440</a></span> | <span class="t">generate distractor questions, hey, just generate a question that's completely irrelevant to the image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=629" target="_blank">00:10:29.600</a></span> | <span class="t">it'll always generate something about dinosaurs and aliens. And so if you train your model on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=633" target="_blank">00:10:33.280</a></span> | <span class="t">it'll instead learn to say, hey, if the question is about dinosaurs and aliens, always say no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=640" target="_blank">00:10:40.480</a></span> | <span class="t">which doesn't really help. And so you need to inject like some entropy into the process of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=645" target="_blank">00:10:45.200</a></span> | <span class="t">generating synthetic data to avoid this. In the case of synthetic captioning, you can do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=649" target="_blank">00:10:49.520</a></span> | <span class="t">like, hey, describe this image, but also consider the alt text on the image, which may be noisy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=653" target="_blank">00:10:53.440</a></span> | <span class="t">may be irrelevant. But if it is relevant, use relevant facts from that. And that tends to help a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=658" target="_blank">00:10:58.160</a></span> | <span class="t">All right. So popping back up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=668" target="_blank">00:11:08.000</a></span> | <span class="t">there's a couple of important learnings I had over the last three months that I would like to share</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=671" target="_blank">00:11:11.520</a></span> | <span class="t">with all of you. The first was the community was really critical in this whole journey. Seeing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=678" target="_blank">00:11:18.240</a></span> | <span class="t">original engagement that we got from the Moon Dream release helped me realize that, hey, maybe this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=683" target="_blank">00:11:23.440</a></span> | <span class="t">more valuable than that QA testing application that I was working on, because a lot of people have a need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=688" target="_blank">00:11:28.240</a></span> | <span class="t">for this to build applications like that. Coming from an enterprise-ish company, it's been really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=695" target="_blank">00:11:35.040</a></span> | <span class="t">valuable. It's been refreshing to be able to just talk to customers directly, like someone at Twitter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=699" target="_blank">00:11:39.760</a></span> | <span class="t">DM and be like, hey, I just saw you looking for this. What do you think? But it's also helped us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=705" target="_blank">00:11:45.280</a></span> | <span class="t">connect with a lot of partners, mentors, and get a lot of support from the community. Being open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=711" target="_blank">00:11:51.200</a></span> | <span class="t">was critical. I kind of didn't really have a choice over here because the competition was free. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=716" target="_blank">00:11:56.560</a></span> | <span class="t">what am I going to do? But when you're in the dev tool space, it is pretty important. Open source is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=722" target="_blank">00:12:02.560</a></span> | <span class="t">important to a lot of developers. They would like to have the ability to run it in different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=726" target="_blank">00:12:06.800</a></span> | <span class="t">environments. It's also pretty important for a lot of enterprise users. In a lot of cases, they don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=732" target="_blank">00:12:12.320</a></span> | <span class="t">really want to run the software themselves, but having the option is very important to them because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=738" target="_blank">00:12:18.320</a></span> | <span class="t">they've had -- most enterprises have had situations where a vendor goes out of business or decides to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=746" target="_blank">00:12:26.400</a></span> | <span class="t">screw them in some other capacity. It's also been really critical for engagement for us. We've had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=751" target="_blank">00:12:31.200</a></span> | <span class="t">a lot of people in the community help out, port it to different platforms, run the model in the web</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=755" target="_blank">00:12:35.280</a></span> | <span class="t">browser and whatnot. So it's been very valuable for us. This one is a little controversial. I'm not sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=764" target="_blank">00:12:44.720</a></span> | <span class="t">everyone agrees with this, but I feel pretty strongly that safety guardrails should be implemented at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=770" target="_blank">00:12:50.000</a></span> | <span class="t">application layer, not baked into the model itself. This was one of my learnings from my first attempt to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=777" target="_blank">00:12:57.040</a></span> | <span class="t">build a QA testing application with GPT-4B. It made no sense for that application to reject pictures of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=783" target="_blank">00:13:03.840</a></span> | <span class="t">any picture that contained a human being. I understand why they felt it was important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=790" target="_blank">00:13:10.320</a></span> | <span class="t">DevTools are kind of B2 -- B2B, not B2C. So it's important to make it easy for developers to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=796" target="_blank">00:13:16.240</a></span> | <span class="t">decide what guardrails they want and implement it in their model as opposed to just deciding it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=801" target="_blank">00:13:21.280</a></span> | <span class="t">all users. I'm not saying this is not important at all. Kind of makes sense if you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=805" target="_blank">00:13:25.040</a></span> | <span class="t">build an assistant to make that stuff right directly into the model. But when you're building for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=809" target="_blank">00:13:29.440</a></span> | <span class="t">developers, it makes less sense. Yeah, I believe pretty strongly now that tiny models are going to run the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=819" target="_blank">00:13:39.920</a></span> | <span class="t">In computer vision, more so perhaps than in text models, efficiency is really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=826" target="_blank">00:13:46.480</a></span> | <span class="t">In a lot of cases, you're really worried about cost because you're processing video and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=834" target="_blank">00:13:54.400</a></span> | <span class="t">30 frames a second at seven-tenths of a cent per second adds up very quickly and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=839" target="_blank">00:13:59.280</a></span> | <span class="t">doesn't give you a lot of room to work with. But there's also situations where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=842" target="_blank">00:14:02.960</a></span> | <span class="t">really worried about privacy or latency and therefore you want to run the model really close to where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=849" target="_blank">00:14:09.520</a></span> | <span class="t">decisions need to be made. Which is not to say big models are not useful. I think they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=854" target="_blank">00:14:14.400</a></span> | <span class="t">very useful. I just think that we'll mostly be running them in our development environments maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=858" target="_blank">00:14:18.560</a></span> | <span class="t">for generating training data. But the artifact that you're going to want to deploy is most likely going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=864" target="_blank">00:14:24.560</a></span> | <span class="t">to be a smaller model. Another thing that was a little surprising to me was looking at the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=877" target="_blank">00:14:37.520</a></span> | <span class="t">things people were doing with Moon Dream. There were a lot of people building net new applications that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=881" target="_blank">00:14:41.920</a></span> | <span class="t">weren't possible to do before because the model can understand language as well as images. But there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=887" target="_blank">00:14:47.600</a></span> | <span class="t">were also a lot of people doing traditional computer vision things with the model. It's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=892" target="_blank">00:14:52.480</a></span> | <span class="t">is there a person in the scene? Or is there something suspicious going on? Tell me where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=897" target="_blank">00:14:57.760</a></span> | <span class="t">bus is in this picture from a road camera. All of which was possible to do before we had transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=907" target="_blank">00:15:07.760</a></span> | <span class="t">like just train a YOLO V3 model or whatnot. The thing that was --</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=911" target="_blank">00:15:11.360</a></span> | <span class="t">Yeah, the lesson I took from this was prompting is a much better developer experience than having to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=919" target="_blank">00:15:19.360</a></span> | <span class="t">a custom model. And so for a lot of developers that would be interested in incorporating vision into their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=925" target="_blank">00:15:25.280</a></span> | <span class="t">applications, before they'd be like, you know what, it's not worth me spending two weeks learning how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=931" target="_blank">00:15:31.200</a></span> | <span class="t">like collect data and annotate it and train my own custom model. Giving them the option to say, hey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=937" target="_blank">00:15:37.520</a></span> | <span class="t">for fairly cheap, you can just in English describe what you want extracted from this image makes it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=943" target="_blank">00:15:43.520</a></span> | <span class="t">something that they actually consider doing now. All right. I think I'm a little ahead of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=951" target="_blank">00:15:51.040</a></span> | <span class="t">so I'm excited to maybe do a live demo if the demo got a smile upon me, but we'll see. In conclusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=958" target="_blank">00:15:58.880</a></span> | <span class="t">yeah, where's Moon Dream going? We're not AGI people. I'm really focused on making it really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=965" target="_blank">00:16:05.520</a></span> | <span class="t">easy for developers to build amazing applications with vision. There's a bunch of model improvements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=971" target="_blank">00:16:11.440</a></span> | <span class="t">that I'm working on right now. I'll talk about some. Right now, we use 729 tokens to represent an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=978" target="_blank">00:16:18.080</a></span> | <span class="t">image, so you can only really send one image to the model at a time. We're working on giving users the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=983" target="_blank">00:16:23.200</a></span> | <span class="t">option to give a more compressed representation to the model, which makes sense if you're not trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=988" target="_blank">00:16:28.000</a></span> | <span class="t">read text or something from the image if you're just trying to do classification and whatnot. That makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=991" target="_blank">00:16:31.200</a></span> | <span class="t">the model run a lot faster, which is important, especially if you're on CPU as opposed to GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=995" target="_blank">00:16:35.600</a></span> | <span class="t">which can't do as much -- CPUs can't do as much parallel compute, and so that sort of thing ends up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1000" target="_blank">00:16:40.720</a></span> | <span class="t">being really important. We've also just raised a seed round from Felicis, Ascend, and also the GitHub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1010" target="_blank">00:16:50.160</a></span> | <span class="t">one, which I forgot to include in the slide. Sorry, GitHub. This means more GPUs, but more importantly, it means I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1017" target="_blank">00:16:57.840</a></span> | <span class="t">finally get some sleep because we're able to get a couple more people to join the team. If you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1022" target="_blank">00:17:02.480</a></span> | <span class="t">interested, please reach out. We have a contact email on the website or just hit me up on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1027" target="_blank">00:17:07.120</a></span> | <span class="t">We also have an exciting release coming up later this summer that I'm super pumped for, so stay tuned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1032" target="_blank">00:17:12.000</a></span> | <span class="t">I think that's about it. So I have a couple of minutes left, I think, so I'm going to try doing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1039" target="_blank">00:17:19.280</a></span> | <span class="t">that may not be the wisest idea, but we'll see how it goes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1042" target="_blank">00:17:22.560</a></span> | <span class="t">All right. I'll turn the Wi-Fi off. This whole thing is running locally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1054" target="_blank">00:17:34.080</a></span> | <span class="t">So what this is going to do is, like, start taking my webcam in, and it's going to use Moon Dream in an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1064" target="_blank">00:17:44.560</a></span> | <span class="t">infinite loop to describe what it sees, and we can ask it different questions. So we'll see how that goes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1086" target="_blank">00:18:06.000</a></span> | <span class="t">And, yeah, you can ask it different things. So let's say, is the person wearing glasses? You do have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1095" target="_blank">00:18:15.280</a></span> | <span class="t">to tell the model to answer briefly if you want a yes or no, otherwise it gives you, like, a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1099" target="_blank">00:18:19.120</a></span> | <span class="t">answer with a single word. Let's try that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1107" target="_blank">00:18:27.680</a></span> | <span class="t">Yes. Okay. I'll take them off. I can't see. Did it get it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1115" target="_blank">00:18:35.840</a></span> | <span class="t">Let's do that. I'll go back to the old prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1130" target="_blank">00:18:50.720</a></span> | <span class="t">All right. Well, that was it for me. Thank you all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=T7sxvrJLJ14&t=1147" target="_blank">00:19:07.680</a></span> | <span class="t">I'll see you next time.</span></div></div></body></html>