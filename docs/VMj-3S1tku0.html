<html><head><title>The spelled-out intro to neural networks and backpropagation: building micrograd</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>The spelled-out intro to neural networks and backpropagation: building micrograd</h2><a href="https://www.youtube.com/watch?v=VMj-3S1tku0"><img src="https://i.ytimg.com/vi_webp/VMj-3S1tku0/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=0">0:0</a> intro<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=25">0:25</a> micrograd overview<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=488">8:8</a> derivative of a simple function with one input<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=852">14:12</a> derivative of a function with multiple inputs<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1149">19:9</a> starting the core Value object of micrograd and its visualization<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1930">32:10</a> manual backpropagation example #1: simple expression<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3070">51:10</a> preview of a single optimization step<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3172">52:52</a> manual backpropagation example #2: a neuron<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4142">69:2</a> implementing the backward function for each operation<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4652">77:32</a> implementing the backward function for a whole expression graph<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4948">82:28</a> fixing a backprop bug when one node is used multiple times<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5225">87:5</a> breaking up a tanh, exercising with more operations<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5971">99:31</a> doing the same thing but in PyTorch: comparison<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6235">103:55</a> building out a neural net library (multi-layer perceptron) in micrograd<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6664">111:4</a> creating a tiny dataset, writing the loss function<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7076">117:56</a> collecting all of the parameters of the neural net<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7272">121:12</a> doing gradient descent optimization manually, training the network<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8043">134:3</a> summary of what we learned, how to go towards modern neural nets<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8206">136:46</a> walkthrough of the full code of micrograd on github<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8470">141:10</a> real stuff: diving into PyTorch, finding their backward pass for tanh<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8679">144:39</a> conclusion<br><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8720">145:20</a> outtakes :)<br><br><div style="text-align: left;"><a href="./VMj-3S1tku0.html">Whisper Transcript</a> | <a href="./transcript_VMj-3S1tku0.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4" target="_blank">00:00:04.800</a></span> | <span class="t">and in this lecture I'd like to show you what neural network training looks like under the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=9" target="_blank">00:00:09.600</a></span> | <span class="t">So in particular we are going to start with a blank Jupyter notebook and by the end of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=14" target="_blank">00:00:14.000</a></span> | <span class="t">lecture we will define and train a neural net and you'll get to see everything that goes on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=18" target="_blank">00:00:18.560</a></span> | <span class="t">under the hood and exactly sort of how that works on an intuitive level. Now specifically what I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=23" target="_blank">00:00:23.520</a></span> | <span class="t">would like to do is I would like to take you through building of micrograd. Now micrograd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=29" target="_blank">00:00:29.200</a></span> | <span class="t">is this library that I released on github about two years ago but at the time I only uploaded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=33" target="_blank">00:00:33.840</a></span> | <span class="t">the source code and you'd have to go in by yourself and really figure out how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=38" target="_blank">00:00:38.480</a></span> | <span class="t">So in this lecture I will take you through it step by step and kind of comment on all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=42" target="_blank">00:00:42.960</a></span> | <span class="t">pieces of it. So what is micrograd and why is it interesting? Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=47" target="_blank">00:00:47.600</a></span> | <span class="t">Micrograd is basically an autograd engine. Autograd is short for automatic gradient and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=54" target="_blank">00:00:54.720</a></span> | <span class="t">really what it does is it implements back propagation. Now back propagation is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=58" target="_blank">00:00:58.640</a></span> | <span class="t">algorithm that allows you to efficiently evaluate the gradient of some kind of a loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=64" target="_blank">00:01:04.960</a></span> | <span class="t">with respect to the weights of a neural network and what that allows us to do then is we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=70" target="_blank">00:01:10.000</a></span> | <span class="t">iteratively tune the weights of that neural network to minimize the loss function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=73" target="_blank">00:01:13.920</a></span> | <span class="t">therefore improve the accuracy of the network. So back propagation would be at the mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=78" target="_blank">00:01:18.960</a></span> | <span class="t">core of any modern deep neural network library like say pytorch or jax. So the functionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=84" target="_blank">00:01:24.800</a></span> | <span class="t">of micrograd is I think best illustrated by an example. So if we just scroll down here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=88" target="_blank">00:01:28.720</a></span> | <span class="t">you'll see that micrograd basically allows you to build out mathematical expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=93" target="_blank">00:01:33.520</a></span> | <span class="t">and here what we are doing is we have an expression that we're building out where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=98" target="_blank">00:01:38.000</a></span> | <span class="t">you have two inputs a and b and you'll see that a and b are negative four and two but we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=104" target="_blank">00:01:44.880</a></span> | <span class="t">wrapping those values into this value object that we are going to build out as part of micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=110" target="_blank">00:01:50.480</a></span> | <span class="t">So this value object will wrap the numbers themselves and then we are going to build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=116" target="_blank">00:01:56.080</a></span> | <span class="t">out a mathematical expression here where a and b are transformed into c d and eventually e f and g</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=123" target="_blank">00:02:03.120</a></span> | <span class="t">and I'm showing some of the functionality of micrograd and the operations that it supports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=128" target="_blank">00:02:08.800</a></span> | <span class="t">So you can add two value objects, you can multiply them, you can raise them to a constant power,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=134" target="_blank">00:02:14.480</a></span> | <span class="t">you can offset by one, negate, squash at zero, square, divide by a constant, divide by it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=141" target="_blank">00:02:21.520</a></span> | <span class="t">etc. And so we're building out an expression graph with these two inputs a and b and we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=147" target="_blank">00:02:27.760</a></span> | <span class="t">creating an output value of g and micrograd will in the background build out this entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=153" target="_blank">00:02:33.600</a></span> | <span class="t">mathematical expression. So it will for example know that c is also a value, c was a result of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=159" target="_blank">00:02:39.680</a></span> | <span class="t">an addition operation and the child nodes of c are a and b because the and it will maintain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=167" target="_blank">00:02:47.360</a></span> | <span class="t">pointers to a and b value objects so we'll basically know exactly how all of this is laid out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=172" target="_blank">00:02:52.480</a></span> | <span class="t">And then not only can we do what we call the forward pass where we actually look at the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=177" target="_blank">00:02:57.520</a></span> | <span class="t">of g of course that's pretty straightforward we will access that using the dot data attribute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=183" target="_blank">00:03:03.120</a></span> | <span class="t">and so the output of the forward pass the value of g is 24.7 it turns out but the big deal is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=190" target="_blank">00:03:10.160</a></span> | <span class="t">that we can also take this g value object and we can call dot backward and this will basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=196" target="_blank">00:03:16.240</a></span> | <span class="t">initialize backpropagation at the node g. And what backpropagation is going to do is it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=201" target="_blank">00:03:21.920</a></span> | <span class="t">start at g and it's going to go backwards through that expression graph and it's going to recursively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=207" target="_blank">00:03:27.440</a></span> | <span class="t">apply the chain rule from calculus and what that allows us to do then is we're going to evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=213" target="_blank">00:03:33.440</a></span> | <span class="t">basically the derivative of g with respect to all the internal nodes like e d and c but also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=220" target="_blank">00:03:40.400</a></span> | <span class="t">with respect to the inputs a and b and then we can actually query this derivative of g with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=227" target="_blank">00:03:47.200</a></span> | <span class="t">to a for example that's a dot grad in this case it happens to be 138 and the derivative of g with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=232" target="_blank">00:03:52.880</a></span> | <span class="t">respect to b which also happens to be here 645 and this derivative we'll see soon is very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=240" target="_blank">00:04:00.000</a></span> | <span class="t">information because it's telling us how a and b are affecting g through this mathematical expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=246" target="_blank">00:04:06.720</a></span> | <span class="t">so in particular a dot grad is 138 so if we slightly nudge a and make it slightly larger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=253" target="_blank">00:04:13.920</a></span> | <span class="t">138 is telling us that g will grow and the slope of that growth is going to be 138</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=260" target="_blank">00:04:20.720</a></span> | <span class="t">and the slope of growth of b is going to be 645 so that's going to tell us about how g will respond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=266" target="_blank">00:04:26.800</a></span> | <span class="t">if a and b get tweaked a tiny amount in a positive direction okay um now you might be confused about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=274" target="_blank">00:04:34.720</a></span> | <span class="t">what this expression is that we built out here and this expression by the way is completely meaningless</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=279" target="_blank">00:04:39.680</a></span> | <span class="t">i just made it up i'm just flexing about the kinds of operations that are supported by micrograd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=284" target="_blank">00:04:44.800</a></span> | <span class="t">and what we actually really care about are neural networks but it turns out that neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=288" target="_blank">00:04:48.720</a></span> | <span class="t">are just mathematical expressions just like this one but actually slightly a bit less crazy even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=293" target="_blank">00:04:53.680</a></span> | <span class="t">neural networks are just a mathematical expression they take the input data as an input and they take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=300" target="_blank">00:05:00.080</a></span> | <span class="t">the weights of a neural network as an input and it's a mathematical expression and the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=304" target="_blank">00:05:04.560</a></span> | <span class="t">are your predictions of your neural net or the loss function we'll see this in a bit but basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=309" target="_blank">00:05:09.600</a></span> | <span class="t">neural networks just happen to be a certain class of mathematical expressions but back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=314" target="_blank">00:05:14.800</a></span> | <span class="t">is actually significantly more general it doesn't actually care about neural networks at all it only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=319" target="_blank">00:05:19.440</a></span> | <span class="t">tells us about arbitrary mathematical expressions and then we happen to use that machinery for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=324" target="_blank">00:05:24.480</a></span> | <span class="t">training of neural networks now one more note i would like to make at this stage is that as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=328" target="_blank">00:05:28.960</a></span> | <span class="t">see here micrograd is a scalar valued autograd engine so it's working on the you know level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=334" target="_blank">00:05:34.320</a></span> | <span class="t">individual scalars like negative four and two and we're taking neural nets and we're breaking them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=338" target="_blank">00:05:38.320</a></span> | <span class="t">down all the way to these atoms of individual scalars and all the little pluses and times and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=342" target="_blank">00:05:42.960</a></span> | <span class="t">it's just excessive and so obviously you would never be doing any of this in production it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=348" target="_blank">00:05:48.000</a></span> | <span class="t">really just for done for pedagogical reasons because it allows us to not have to deal with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=352" target="_blank">00:05:52.000</a></span> | <span class="t">these n-dimensional tensors that you would use in modern deep neural network library so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=357" target="_blank">00:05:57.280</a></span> | <span class="t">really done so that you understand and refactor out back propagation and chain rule and understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=363" target="_blank">00:06:03.200</a></span> | <span class="t">of neural training and then if you actually want to train bigger networks you have to be using these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=368" target="_blank">00:06:08.320</a></span> | <span class="t">tensors but none of the math changes this is done purely for efficiency we are basically taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=373" target="_blank">00:06:13.120</a></span> | <span class="t">scalar value all the scalar values we're packaging them up into tensors which are just arrays of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=378" target="_blank">00:06:18.240</a></span> | <span class="t">these scalars and then because we have these large arrays we're making operations on those large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=383" target="_blank">00:06:23.440</a></span> | <span class="t">arrays that allows us to take advantage of the parallelism in a computer and all those operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=389" target="_blank">00:06:29.040</a></span> | <span class="t">can be done in parallel and then the whole thing runs faster but really none of the math changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=393" target="_blank">00:06:33.520</a></span> | <span class="t">and they've done purely for efficiency so i don't think that it's pedagogically useful to be dealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=397" target="_blank">00:06:37.760</a></span> | <span class="t">with tensors from scratch and i think and that's why i fundamentally wrote micrograd because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=402" target="_blank">00:06:42.640</a></span> | <span class="t">can understand how things work at the fundamental level and then you can speed it up later okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=408" target="_blank">00:06:48.320</a></span> | <span class="t">here's the fun part my claim is that micrograd is what you need to train your own networks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">everything else is just efficiency so you'd think that micrograd would be a very complex piece of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=417" target="_blank">00:06:57.280</a></span> | <span class="t">code and that turns out to not be the case so if we just go to micrograd and you'll see that there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=424" target="_blank">00:07:04.480</a></span> | <span class="t">only two files here in micrograd this is the actual engine it doesn't know anything about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=429" target="_blank">00:07:09.280</a></span> | <span class="t">neural nets and this is the entire neural nets library on top of micrograd so engine and nn.py</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=436" target="_blank">00:07:16.560</a></span> | <span class="t">so the actual back propagation autograd engine that gives you the power of neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=443" target="_blank">00:07:23.280</a></span> | <span class="t">is literally 100 lines of code of like very simple python which we'll understand by the end of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=451" target="_blank">00:07:31.120</a></span> | <span class="t">lecture and then nn.py this neural network library built on top of the autograd engine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=457" target="_blank">00:07:37.040</a></span> | <span class="t">is like a joke it's like we have to define what is a neuron and then we have to define what is a layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=463" target="_blank">00:07:43.920</a></span> | <span class="t">of neurons and then we define what is a multilayer perceptron which is just a sequence of layers of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=468" target="_blank">00:07:48.800</a></span> | <span class="t">neurons and so it's just a total joke so basically there's a lot of power that comes from only 150</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=476" target="_blank">00:07:56.400</a></span> | <span class="t">lines of code and that's all you need to understand to understand neural network training and everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=481" target="_blank">00:08:01.200</a></span> | <span class="t">else is just efficiency and of course there's a lot to efficiency but fundamentally that's all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=486" target="_blank">00:08:06.880</a></span> | <span class="t">that's happening okay so now let's dive right in and implement micrograd step by step the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=491" target="_blank">00:08:11.600</a></span> | <span class="t">thing i'd like to do is i'd like to make sure that you have a very good understanding intuitively of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=495" target="_blank">00:08:15.600</a></span> | <span class="t">what a derivative is and exactly what information it gives you so let's start with some basic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=501" target="_blank">00:08:21.280</a></span> | <span class="t">imports that i copy paste in every jupyter notebook always and let's define a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=506" target="_blank">00:08:26.880</a></span> | <span class="t">a scalar valid function f of x as follows so i just made this up randomly i just wanted a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=513" target="_blank">00:08:33.520</a></span> | <span class="t">scalar valid function that takes a single scalar x and returns a single scalar y and we can call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=519" target="_blank">00:08:39.280</a></span> | <span class="t">this function of course so we can pass and say 3.0 and get 20 back now we can also plot this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=525" target="_blank">00:08:45.520</a></span> | <span class="t">to get a sense of its shape you can tell from the mathematical expression that this is probably a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=530" target="_blank">00:08:50.000</a></span> | <span class="t">parabola it's a quadratic and so if we just create a set of um um scalar values that we can feed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=539" target="_blank">00:08:59.280</a></span> | <span class="t">using for example a range from negative 5 to 5 in steps of 0.25 so this is so x is is just from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=546" target="_blank">00:09:06.320</a></span> | <span class="t">negative 5 to 5 not including 5 in steps of 0.25 and we can actually call this function on this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=553" target="_blank">00:09:13.120</a></span> | <span class="t">numpy array as well so we get a set of y's if we call f on x's and these y's are basically also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=560" target="_blank">00:09:20.800</a></span> | <span class="t">applying the function on every one of these elements independently and we can plot this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=566" target="_blank">00:09:26.400</a></span> | <span class="t">using mathplotlib so plt.plot x's and y's and we get a nice parabola so previously here we fed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=573" target="_blank">00:09:33.760</a></span> | <span class="t">3.0 somewhere here and we received 20 back which is here the y coordinate so now i'd like to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=579" target="_blank">00:09:39.840</a></span> | <span class="t">through what is the derivative of this function at any single input point x right so what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=586" target="_blank">00:09:46.160</a></span> | <span class="t">derivative at different points x of this function now if you remember back to your calculus class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=591" target="_blank">00:09:51.360</a></span> | <span class="t">you've probably derived derivatives so we take this mathematical expression 3x squared minus 4x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=596" target="_blank">00:09:56.400</a></span> | <span class="t">plus 5 and you would write out on a piece of paper and you would you know apply the product rule and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=600" target="_blank">00:10:00.560</a></span> | <span class="t">all the other rules and derive the mathematical expression of the great derivative of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=605" target="_blank">00:10:05.040</a></span> | <span class="t">original function and then you could plug in different taxes and see what the derivative is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=608" target="_blank">00:10:08.720</a></span> | <span class="t">we're not going to actually do that because no one in neural networks actually writes out the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=614" target="_blank">00:10:14.800</a></span> | <span class="t">expression for the neural net it would be a massive expression it would be you know thousands tens of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=619" target="_blank">00:10:19.600</a></span> | <span class="t">thousands of terms no one actually derives the derivative of course and so we're not going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=624" target="_blank">00:10:24.240</a></span> | <span class="t">take this kind of like symbolic approach instead what i'd like to do is i'd like to look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=628" target="_blank">00:10:28.160</a></span> | <span class="t">definition of derivative and just make sure that we really understand what the derivative is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=631" target="_blank">00:10:31.920</a></span> | <span class="t">measuring what it's telling you about the function and so if we just look up derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">we see that um okay so this is not a very good definition of derivative this is a definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=646" target="_blank">00:10:46.560</a></span> | <span class="t">of what it means to be differentiable but if you remember from your calculus it is the limit as h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=651" target="_blank">00:10:51.120</a></span> | <span class="t">goes to zero of f of x plus h minus f of x over h so basically what it's saying is if you slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=658" target="_blank">00:10:58.960</a></span> | <span class="t">bump up you're at some point x that you're interested in or a and if you slightly bump up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=664" target="_blank">00:11:04.000</a></span> | <span class="t">you know you slightly increase it by a small number h how does the function respond with what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=669" target="_blank">00:11:09.760</a></span> | <span class="t">sensitivity does it respond where is the slope at that point does the function go up or does it go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=674" target="_blank">00:11:14.480</a></span> | <span class="t">down and by how much and that's the slope of that function the the slope of that response at that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=680" target="_blank">00:11:20.880</a></span> | <span class="t">point and so we can basically evaluate the derivative here numerically by taking a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=686" target="_blank">00:11:26.880</a></span> | <span class="t">small h of course the definition would ask us to take h to zero we're just going to pick a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=691" target="_blank">00:11:31.760</a></span> | <span class="t">small h 0.001 and let's say we're interested in 0.3.0 so we can look at f of x of course as 20</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=698" target="_blank">00:11:38.240</a></span> | <span class="t">and now f of x plus h so if we slightly nudge x in a positive direction how is the function going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=704" target="_blank">00:11:44.560</a></span> | <span class="t">respond and just looking at this do you expand do you expect f of x plus h to be slightly greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=710" target="_blank">00:11:50.320</a></span> | <span class="t">than 20 or do you expect to be slightly lower than 20 and since this 3 is here and this is 20</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=717" target="_blank">00:11:57.040</a></span> | <span class="t">if we slightly go positively the function will respond positively so you'd expect this to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=722" target="_blank">00:12:02.160</a></span> | <span class="t">slightly greater than 20 and now by how much is telling you the sort of the the strength of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=728" target="_blank">00:12:08.640</a></span> | <span class="t">slope right the the size of the slope so f of x plus h minus f of x this is how much the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=734" target="_blank">00:12:14.720</a></span> | <span class="t">responded in the positive direction and we have to normalize by the run so we have the rise over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=741" target="_blank">00:12:21.360</a></span> | <span class="t">run to get the slope so this of course is just a numerical approximation of the slope because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=747" target="_blank">00:12:27.440</a></span> | <span class="t">have to make h very very small to converge to the exact amount now if i'm doing too many zeros</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=754" target="_blank">00:12:34.560</a></span> | <span class="t">at some point i'm going to get an incorrect answer because we're using floating point arithmetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=760" target="_blank">00:12:40.400</a></span> | <span class="t">and the representations of all these numbers in computer memory is finite and at some point we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=765" target="_blank">00:12:45.280</a></span> | <span class="t">getting into trouble so we can converge towards the right answer with this approach but basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=770" target="_blank">00:12:50.960</a></span> | <span class="t">at 3 the slope is 14 and you can see that by taking 3x squared minus 4x plus 5 and differentiating it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=779" target="_blank">00:12:59.200</a></span> | <span class="t">in our head so 3x squared would be 6x minus 4 and then we plug in x equals 3 so that's 18 minus 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=788" target="_blank">00:13:08.000</a></span> | <span class="t">is 14 so this is correct so that's at 3 now how about the slope at say negative 3 would you expect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=798" target="_blank">00:13:18.480</a></span> | <span class="t">what would you expect for the slope now telling the exact value is really hard but what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=803" target="_blank">00:13:23.200</a></span> | <span class="t">sign of that slope so at negative 3 if we slightly go in the positive direction at x the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=810" target="_blank">00:13:30.400</a></span> | <span class="t">would actually go down and so that tells you that the slope would be negative so we'll get a slight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=814" target="_blank">00:13:34.560</a></span> | <span class="t">number below 20 and so if we take the slope we expect something negative negative 22 okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=822" target="_blank">00:13:42.160</a></span> | <span class="t">and at some point here of course the slope would be zero now for this specific function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=828" target="_blank">00:13:48.320</a></span> | <span class="t">i looked it up previously and it's at point 2 over 3 so at roughly 2 over 3 that's somewhere here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=837" target="_blank">00:13:57.040</a></span> | <span class="t">this derivative would be zero so basically at that precise point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=841" target="_blank">00:14:01.120</a></span> | <span class="t">yeah at that precise point if we nudge in a positive direction the function doesn't respond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=848" target="_blank">00:14:08.160</a></span> | <span class="t">this stays the same almost and so that's why the slope is zero okay now let's look at a bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=852" target="_blank">00:14:12.560</a></span> | <span class="t">complex case so we're going to start you know complexifying a bit so now we have a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=859" target="_blank">00:14:19.680</a></span> | <span class="t">here with output variable b that is a function of three scalar inputs a b and c so a b and c are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=867" target="_blank">00:14:27.280</a></span> | <span class="t">some specific values three inputs into our expression graph and a single output d and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=873" target="_blank">00:14:33.440</a></span> | <span class="t">if we just print d we get four and now what i'd like to do is i'd like to again look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=878" target="_blank">00:14:38.720</a></span> | <span class="t">derivatives of d with respect to a b and c and think through again just the intuition of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=885" target="_blank">00:14:45.600</a></span> | <span class="t">this derivative is telling us so in order to evaluate this derivative we're going to get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=891" target="_blank">00:14:51.040</a></span> | <span class="t">bit hacky here we're going to again have a very small value of h and then we're going to fix the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=896" target="_blank">00:14:56.960</a></span> | <span class="t">inputs at some values that we're interested in so these are the this is the point a b c at which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=903" target="_blank">00:15:03.360</a></span> | <span class="t">we're going to be evaluating the derivative of d with respect to all a b and c at that point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=908" target="_blank">00:15:08.800</a></span> | <span class="t">so there are the inputs and now we have d1 is that expression and then we're going to for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=914" target="_blank">00:15:14.880</a></span> | <span class="t">look at the derivative of d with respect to a so we'll take a and we'll bump it by h and then we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=920" target="_blank">00:15:20.400</a></span> | <span class="t">get d2 to be the exact same function and now we're going to print um you know f1 d1 is d1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=930" target="_blank">00:15:30.000</a></span> | <span class="t">d2 is d2 and print slope so the derivative or slope here will be um of course d2 minus d1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=942" target="_blank">00:15:42.800</a></span> | <span class="t">divide h so d2 minus d1 is how much the function increased when we bumped the uh the specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=952" target="_blank">00:15:52.560</a></span> | <span class="t">input that we're interested in by a tiny amount and this is the normalized by h to get the slope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=962" target="_blank">00:16:02.720</a></span> | <span class="t">so um yeah so this so i just run this we're going to print d1 which we know is four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=974" target="_blank">00:16:14.240</a></span> | <span class="t">now d2 will be bumped a will be bumped by h so let's just think through a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=983" target="_blank">00:16:23.760</a></span> | <span class="t">what d2 will be uh printed out here in particular d1 will be four will d2 be a number slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=993" target="_blank">00:16:33.200</a></span> | <span class="t">greater than four or slightly lower than four and that's going to tell us the the the sign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=998" target="_blank">00:16:38.320</a></span> | <span class="t">of the derivative so we're bumping a by h b is minus three c is 10 so you can just intuitively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1009" target="_blank">00:16:49.760</a></span> | <span class="t">think through this derivative and what it's doing a will be slightly more positive and but b is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1015" target="_blank">00:16:55.840</a></span> | <span class="t">negative number so if a is slightly more positive because b is negative three we're actually going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1023" target="_blank">00:17:03.840</a></span> | <span class="t">to be adding less to d so you'd actually expect that the value of the function will go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1033" target="_blank">00:17:13.760</a></span> | <span class="t">so let's just see this yeah and so we went from four to 3.9996 and that tells you that the slope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1042" target="_blank">00:17:22.000</a></span> | <span class="t">will be negative and then um will be a negative number because we went down and then the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1049" target="_blank">00:17:29.440</a></span> | <span class="t">number of slope will be exact amount of slope is negative three and you can also convince yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1054" target="_blank">00:17:34.880</a></span> | <span class="t">that negative three is the right answer uh mathematically and analytically because if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1059" target="_blank">00:17:39.280</a></span> | <span class="t">have a times b plus c and you are you know you have calculus then uh differentiating a times b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1065" target="_blank">00:17:45.440</a></span> | <span class="t">plus c with respect to a gives you just b and indeed the value of b is negative three which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1071" target="_blank">00:17:51.040</a></span> | <span class="t">is the derivative that we have so you can tell that that's correct so now if we do this with b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1076" target="_blank">00:17:56.320</a></span> | <span class="t">so if we bump b by a little bit in a positive direction we'd get different slopes so what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1083" target="_blank">00:18:03.040</a></span> | <span class="t">the influence of b on the output d so if we bump b by a tiny amount in a positive direction then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1089" target="_blank">00:18:09.520</a></span> | <span class="t">because a is positive we'll be adding more to d right so um and now what is the what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1096" target="_blank">00:18:16.960</a></span> | <span class="t">sensitivity what is the slope of that addition and it might not surprise you that this should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1101" target="_blank">00:18:21.840</a></span> | <span class="t">two and why is it two because d of d by db differentiating with respect to b would be would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1110" target="_blank">00:18:30.720</a></span> | <span class="t">give us a and the value of a is two so that's also working well and then if c gets bumped a tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1117" target="_blank">00:18:37.040</a></span> | <span class="t">amount in h by h then of course a times b is unaffected and now c becomes slightly bit higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1124" target="_blank">00:18:44.240</a></span> | <span class="t">what does that do to the function it makes it slightly bit higher because we're simply adding c</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1128" target="_blank">00:18:48.240</a></span> | <span class="t">and it makes it slightly bit higher by the exact same amount that we added to c and so that tells</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1133" target="_blank">00:18:53.840</a></span> | <span class="t">you that the slope is one that will be the the rate at which d will increase as we scale c okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1145" target="_blank">00:19:05.360</a></span> | <span class="t">so we now have some intuitive sense of what this derivative is telling you about the function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1149" target="_blank">00:19:09.280</a></span> | <span class="t">and we'd like to move to neural networks now as i mentioned neural networks will be pretty massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1153" target="_blank">00:19:13.120</a></span> | <span class="t">expressions mathematical expressions so we need some data structures that maintain these expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1157" target="_blank">00:19:17.680</a></span> | <span class="t">and that's what we're going to start to build out now so we're going to build out this value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1163" target="_blank">00:19:23.760</a></span> | <span class="t">that i showed you in the readme page of micrograd so let me copy paste a skeleton of the first very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1171" target="_blank">00:19:31.440</a></span> | <span class="t">simple value object so class value takes a single scalar value that it wraps and keeps track of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1178" target="_blank">00:19:38.640</a></span> | <span class="t">and that's it so we can for example do value of 2.0 and then we can get we can look at its content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1187" target="_blank">00:19:47.760</a></span> | <span class="t">and python will internally use the wrapper function to return this string oops like that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1197" target="_blank">00:19:57.280</a></span> | <span class="t">so this is a value object with data equals two that we're creating here now we'd like to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1204" target="_blank">00:20:04.080</a></span> | <span class="t">like we'd like to be able to have not just like two values but we'd like to do a plus b right we'd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1212" target="_blank">00:20:12.000</a></span> | <span class="t">like to add them so currently you would get an error because python doesn't know how to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1217" target="_blank">00:20:17.760</a></span> | <span class="t">two value objects so we have to tell it so here's addition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1222" target="_blank">00:20:22.880</a></span> | <span class="t">so you have to basically use these special double underscore methods in python to define these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1230" target="_blank">00:20:30.880</a></span> | <span class="t">operators for these objects so if we call um the uh if we use this plus operator python will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1239" target="_blank">00:20:39.760</a></span> | <span class="t">internally call a dot add of b that's what will happen internally and so b will be the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1247" target="_blank">00:20:47.280</a></span> | <span class="t">and self will be a and so we see that what we're going to return is a new value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1254" target="_blank">00:20:54.000</a></span> | <span class="t">and it's just uh it's going to be wrapping the plus of their data but remember now because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1261" target="_blank">00:21:01.360</a></span> | <span class="t">data is the actual like numbered python number so this operator here is just the typical floating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1267" target="_blank">00:21:07.360</a></span> | <span class="t">point plus addition now it's not an addition of value objects and we'll return a new value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1273" target="_blank">00:21:13.680</a></span> | <span class="t">so now a plus b should work and it should print value of negative one because that's two plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1279" target="_blank">00:21:19.200</a></span> | <span class="t">minus three there we go okay let's now implement multiply just so we can recreate this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1285" target="_blank">00:21:25.920</a></span> | <span class="t">here so multiply i think it won't surprise you will be fairly similar so instead of add we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1292" target="_blank">00:21:32.880</a></span> | <span class="t">going to be using mul and then here of course we want to do times and so now we can create a c value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1298" target="_blank">00:21:38.640</a></span> | <span class="t">object which will be 10.0 and now we should be able to do a times b well let's just do a times b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1305" target="_blank">00:21:45.280</a></span> | <span class="t">first um that's value of negative six now and by the way i skipped over this a little bit uh suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1312" target="_blank">00:21:52.960</a></span> | <span class="t">that i didn't have the wrapper function here uh then it's just that you'll get some kind of an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1317" target="_blank">00:21:57.680</a></span> | <span class="t">ugly expression so what wrapper is doing is it's providing us a way to print out like a nicer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1323" target="_blank">00:22:03.120</a></span> | <span class="t">looking expression in python uh so we don't just have something cryptic we actually are you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1329" target="_blank">00:22:09.040</a></span> | <span class="t">it's value of negative six so this gives us a times and then this we should now be able to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1336" target="_blank">00:22:16.560</a></span> | <span class="t">c to it because we've defined and told the python how to do mul and add and so this will call this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1342" target="_blank">00:22:22.240</a></span> | <span class="t">will basically be equivalent to a dot mul of b and then this new value object will be dot add of c</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1351" target="_blank">00:22:31.760</a></span> | <span class="t">and so let's see if that worked yep so that worked well that gave us four which is what we expect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1357" target="_blank">00:22:37.680</a></span> | <span class="t">from before and i believe you can just call them manually as well there we go so yeah okay so now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1365" target="_blank">00:22:45.600</a></span> | <span class="t">what we are missing is the connected tissue of this expression as i mentioned we want to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1370" target="_blank">00:22:50.000</a></span> | <span class="t">these expression graphs so we need to know and keep pointers about what values produce what other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1375" target="_blank">00:22:55.440</a></span> | <span class="t">values so here for example we are going to introduce a new variable which we'll call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1380" target="_blank">00:23:00.320</a></span> | <span class="t">children and by default it will be an empty tuple and then we're actually going to keep a slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1385" target="_blank">00:23:05.120</a></span> | <span class="t">different variable in the class which we'll call underscore prev which will be the set of children</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1390" target="_blank">00:23:10.320</a></span> | <span class="t">this is how i done i did it in the original micrograd looking at my code here i can't remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1396" target="_blank">00:23:16.320</a></span> | <span class="t">exactly the reason i believe it was efficiency but this underscore children will be a tuple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1400" target="_blank">00:23:20.800</a></span> | <span class="t">for convenience but then when we actually maintain it in the class it will be just this set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1404" target="_blank">00:23:24.400</a></span> | <span class="t">i believe for efficiency so now when we are creating a value like this with a constructor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1412" target="_blank">00:23:32.800</a></span> | <span class="t">children will be empty and prep will be the empty set but when we are creating a value through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1417" target="_blank">00:23:37.360</a></span> | <span class="t">addition or multiplication we're going to feed in the children of this value which in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1423" target="_blank">00:23:43.520</a></span> | <span class="t">is self and other so those are the children here so now we can do d dot prev and we'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1434" target="_blank">00:23:54.160</a></span> | <span class="t">the children of the we now know are this a value of negative six and value of 10 and this of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1440" target="_blank">00:24:00.880</a></span> | <span class="t">is the value resulting from a times b and the c value which is 10 now the last piece of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1448" target="_blank">00:24:08.400</a></span> | <span class="t">we don't know so we know now the children of every single value but we don't know what operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1452" target="_blank">00:24:12.960</a></span> | <span class="t">created this value so we need one more element here let's call it underscore up and by default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1459" target="_blank">00:24:19.920</a></span> | <span class="t">this is the empty set for leaves and then we'll just maintain it here and now the operation will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1466" target="_blank">00:24:26.880</a></span> | <span class="t">be just a simple string and in the case of addition it's plus in the case of multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1472" target="_blank">00:24:32.000</a></span> | <span class="t">it's times so now we not just have d dot prep we also have a d dot up and we know that d was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1479" target="_blank">00:24:39.600</a></span> | <span class="t">produced by an addition of those two values and so now we have the full mathematical expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1485" target="_blank">00:24:45.040</a></span> | <span class="t">and we're building out this data structure and we know exactly how each value came to be by what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1490" target="_blank">00:24:50.320</a></span> | <span class="t">expression and from what other values now because these expressions are about to get quite a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1496" target="_blank">00:24:56.880</a></span> | <span class="t">larger we'd like a way to nicely visualize these expressions that we're building out so for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1502" target="_blank">00:25:02.560</a></span> | <span class="t">i'm going to copy paste a bunch of slightly scary code that's going to visualize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1507" target="_blank">00:25:07.840</a></span> | <span class="t">these expression graphs for us so here's the code and i'll explain it in a bit but first let me just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1513" target="_blank">00:25:13.040</a></span> | <span class="t">show you what this code does basically what it does is it creates a new function draw dot that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1518" target="_blank">00:25:18.400</a></span> | <span class="t">we can call on some root node and then it's going to visualize it so if we call draw dot on d which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1525" target="_blank">00:25:25.040</a></span> | <span class="t">is this final value here that is a times b plus c it creates something like this so this is d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1532" target="_blank">00:25:32.240</a></span> | <span class="t">and you see that this is a times b creating an integer value plus c gives us this output node d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1538" target="_blank">00:25:38.880</a></span> | <span class="t">so that's draw dot of b and i'm not going to go through this in complete detail you can take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1545" target="_blank">00:25:45.840</a></span> | <span class="t">look at graphvis and its api graphvis is a open source graph visualization software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1550" target="_blank">00:25:50.800</a></span> | <span class="t">and what we're doing here is we're building out this graph in graphvis api and you can basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1557" target="_blank">00:25:57.600</a></span> | <span class="t">see that trace is this helper function that enumerates all the nodes and edges in the graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1562" target="_blank">00:26:02.160</a></span> | <span class="t">so that just builds a set of all the nodes and edges and then we iterate through all the nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1566" target="_blank">00:26:06.720</a></span> | <span class="t">and we create special node objects for them in using dot node and then we also create edges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1575" target="_blank">00:26:15.120</a></span> | <span class="t">using dot dot edge and the only thing that's like slightly tricky here is you'll notice that i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1580" target="_blank">00:26:20.400</a></span> | <span class="t">basically add these fake nodes which are these operation nodes so for example this node here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1585" target="_blank">00:26:25.680</a></span> | <span class="t">is just like a plus node and i create these special op nodes here and i connect them accordingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1596" target="_blank">00:26:36.240</a></span> | <span class="t">so these nodes of course are not actual nodes in the original graph they're not actually a value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1602" target="_blank">00:26:42.960</a></span> | <span class="t">object the only value objects here are the things in squares those are actual value objects or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1608" target="_blank">00:26:48.960</a></span> | <span class="t">representations thereof and these op nodes are just created in this draw dot routine so that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1614" target="_blank">00:26:54.160</a></span> | <span class="t">looks nice let's also add labels to these graphs just so we know what variables are where so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1620" target="_blank">00:27:00.320</a></span> | <span class="t">create a special underscore label or let's just do label equals empty by default and save it in each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1628" target="_blank">00:27:08.560</a></span> | <span class="t">node and then here we're going to do label as a label is b label is c and then let's create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1645" target="_blank">00:27:25.680</a></span> | <span class="t">special e equals a times b and e dot label will be e it's kind of naughty and e will be e plus c</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1657" target="_blank">00:27:37.200</a></span> | <span class="t">and a d dot label will be b okay so nothing really changes i just added this new e function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1665" target="_blank">00:27:45.840</a></span> | <span class="t">that new e variable and then here when we are printing this i'm going to print the label here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1674" target="_blank">00:27:54.080</a></span> | <span class="t">so this will be a percent s bar and this will be n dot label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1678" target="_blank">00:27:58.240</a></span> | <span class="t">and so now we have the label on the left here so it says a b creating e and then e plus c</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1687" target="_blank">00:28:07.680</a></span> | <span class="t">creates d just like we have it here and finally let's make this expression just one layer deeper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1693" target="_blank">00:28:13.520</a></span> | <span class="t">so d will not be the final output node instead after d we are going to create a new value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1701" target="_blank">00:28:21.760</a></span> | <span class="t">called f we're going to start running out of variables soon f will be negative 2.0 and its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1707" target="_blank">00:28:27.920</a></span> | <span class="t">label will of course just be f and then L capital L will be the output of our graph and L will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1716" target="_blank">00:28:36.160</a></span> | <span class="t">d times f okay so L will be negative 8 is the output so now we don't just draw a d we draw L</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1730" target="_blank">00:28:50.000</a></span> | <span class="t">okay and somehow the label of L is undefined oops all that label has to be explicitly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1737" target="_blank">00:28:57.680</a></span> | <span class="t">sort of given to it there we go so L is the output so let's quickly recap what we've done so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1743" target="_blank">00:29:03.600</a></span> | <span class="t">we are able to build out mathematical expressions using only plus and times so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1748" target="_blank">00:29:08.160</a></span> | <span class="t">they are scalar valued along the way and we can do this forward pass and build out a mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1755" target="_blank">00:29:15.440</a></span> | <span class="t">expression so we have multiple inputs here a b c and f going into a mathematical expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1761" target="_blank">00:29:21.520</a></span> | <span class="t">that produces a single output L and this here is visualizing the forward pass so the output of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1768" target="_blank">00:29:28.000</a></span> | <span class="t">forward pass is negative 8 that's the value now what we'd like to do next is we'd like to run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1773" target="_blank">00:29:33.920</a></span> | <span class="t">back propagation and in back propagation we are going to start here at the end and we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1779" target="_blank">00:29:39.600</a></span> | <span class="t">reverse and calculate the gradient along along all these intermediate values and really what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1785" target="_blank">00:29:45.920</a></span> | <span class="t">we're computing for every single value here we're going to compute the derivative of that node with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1792" target="_blank">00:29:52.800</a></span> | <span class="t">respect to L so the derivative of L with respect to L is just one and then we're going to derive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1801" target="_blank">00:30:01.520</a></span> | <span class="t">what is the derivative of L with respect to f with respect to d with respect to c with respect to e</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1807" target="_blank">00:30:07.600</a></span> | <span class="t">with respect to b and with respect to a and in a neural network setting you'd be very interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1813" target="_blank">00:30:13.040</a></span> | <span class="t">in the derivative of basically this loss function L with respect to the weights of a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1818" target="_blank">00:30:18.800</a></span> | <span class="t">and here of course we have just these variables a b c and f but some of these will eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1823" target="_blank">00:30:23.840</a></span> | <span class="t">represent the weights of a neural net and so we'll need to know how those weights are impacting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1829" target="_blank">00:30:29.200</a></span> | <span class="t">loss function so we'll be interested basically in the derivative of the output with respect to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1833" target="_blank">00:30:33.760</a></span> | <span class="t">of its leaf nodes and those leaf nodes will be the weights of the neural net and the other leaf</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1838" target="_blank">00:30:38.800</a></span> | <span class="t">nodes of course will be the data itself but usually we will not want or use the derivative of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1843" target="_blank">00:30:43.920</a></span> | <span class="t">the loss function with respect to data because the data is fixed but the weights will be iterated on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1849" target="_blank">00:30:49.680</a></span> | <span class="t">using the gradient information so next we are going to create a variable inside the value class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1855" target="_blank">00:30:55.440</a></span> | <span class="t">that maintains the derivative of L with respect to that value and we will call this variable grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1863" target="_blank">00:31:03.840</a></span> | <span class="t">so there's a dot data and there's a self dot grad and initially it will be zero and remember that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1870" target="_blank">00:31:10.320</a></span> | <span class="t">zero is basically means no effect so at initialization we're assuming that every value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1875" target="_blank">00:31:15.520</a></span> | <span class="t">does not impact does not affect the output right because if the gradient is zero that means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1881" target="_blank">00:31:21.920</a></span> | <span class="t">changing this variable is not changing the loss function so by default we assume that the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1887" target="_blank">00:31:27.680</a></span> | <span class="t">is zero and then now that we have grad and it's you know 0.0 we are going to be able to visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1898" target="_blank">00:31:38.080</a></span> | <span class="t">it here after data so here grad is 0.4f and this will be in dot grad and now we are going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1907" target="_blank">00:31:47.040</a></span> | <span class="t">showing both the data and the grad initialized at zero and we are just about getting ready to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1915" target="_blank">00:31:55.600</a></span> | <span class="t">calculate the back propagation and of course this grad again as I mentioned is representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1920" target="_blank">00:32:00.400</a></span> | <span class="t">the derivative of the output in this case L with respect to this value so with respect to so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1926" target="_blank">00:32:06.320</a></span> | <span class="t">is the derivative of L with respect to f with respect to d and so on so let's now fill in those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1931" target="_blank">00:32:11.680</a></span> | <span class="t">gradients and actually do back propagation manually so let's start filling in these gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1935" target="_blank">00:32:15.840</a></span> | <span class="t">and start all the way at the end as I mentioned here first we are interested to fill in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1940" target="_blank">00:32:20.400</a></span> | <span class="t">gradient here so what is the derivative of L with respect to L in other words if I change L</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1946" target="_blank">00:32:26.800</a></span> | <span class="t">by a tiny amount h how much does L change it changes by h so it's proportional and therefore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1955" target="_blank">00:32:35.440</a></span> | <span class="t">the derivative will be one we can of course measure these or estimate these numerical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1960" target="_blank">00:32:40.320</a></span> | <span class="t">gradients numerically just like we've seen before so if I take this expression and I create a def</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1966" target="_blank">00:32:46.800</a></span> | <span class="t">lol function here and put this here now the reason I'm creating a gating function lol here is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1973" target="_blank">00:32:53.840</a></span> | <span class="t">I don't want to pollute or mess up the global scope here this is just kind of like a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1977" target="_blank">00:32:57.840</a></span> | <span class="t">staging area and as you know in Python all of these will be local variables to this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1982" target="_blank">00:33:02.560</a></span> | <span class="t">so I'm not changing any of the global scope here so here L1 will be L</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1987" target="_blank">00:33:07.520</a></span> | <span class="t">and then copy pasting this expression we're going to add a small amount h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=1995" target="_blank">00:33:15.280</a></span> | <span class="t">and for example a right and this would be measuring the derivative of L with respect to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2004" target="_blank">00:33:24.480</a></span> | <span class="t">so here this will be L2 and then we want to print that derivative so print L2 minus L1 which is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2013" target="_blank">00:33:33.280</a></span> | <span class="t">much L changed and then normalize it by h so this is the rise over run and we have to be careful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2020" target="_blank">00:33:40.400</a></span> | <span class="t">because L is a value node so we actually want its data so that these are floats dividing by h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2028" target="_blank">00:33:48.880</a></span> | <span class="t">and this should print the derivative of L with respect to a because a is the one that we bumped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2033" target="_blank">00:33:53.920</a></span> | <span class="t">a little bit by h so what is the derivative of L with respect to a it's six okay and obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2043" target="_blank">00:34:03.520</a></span> | <span class="t">if we change L by h then that would be here effectively this looks really awkward but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2054" target="_blank">00:34:14.000</a></span> | <span class="t">changing L by h you see the derivative here is one that's kind of like the base case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2062" target="_blank">00:34:22.560</a></span> | <span class="t">what we are doing here so basically we can come up here and we can manually set L.grad to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2069" target="_blank">00:34:29.600</a></span> | <span class="t">this is our manual back propagation L.grad is one and let's redraw and we'll see that we filled in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2077" target="_blank">00:34:37.280</a></span> | <span class="t">grad is one for L we're now going to continue the back propagation so let's here look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2082" target="_blank">00:34:42.160</a></span> | <span class="t">derivatives of L with respect to D and F let's do a D first so what we are interested in if I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2089" target="_blank">00:34:49.120</a></span> | <span class="t">create a markdown on here is we'd like to know basically we have that L is D times F and we'd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2094" target="_blank">00:34:54.560</a></span> | <span class="t">like to know what is D L by D D what is that and if you know your calculus L is D times F so what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2104" target="_blank">00:35:04.400</a></span> | <span class="t">is D L by D D it would be F and if you don't believe me we can also just derive it because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2111" target="_blank">00:35:11.200</a></span> | <span class="t">the proof would be fairly straightforward we go to the definition of the derivative which is F of x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2118" target="_blank">00:35:18.560</a></span> | <span class="t">plus h minus F of x divide h as a limit limit of h goes to zero of this kind of expression so when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2126" target="_blank">00:35:26.160</a></span> | <span class="t">we have L is D times F then increasing D by h would give us the output of D plus h times F</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2134" target="_blank">00:35:34.480</a></span> | <span class="t">that's basically F of x plus h right minus D times F and then divide h and symbolically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2144" target="_blank">00:35:44.880</a></span> | <span class="t">expanding out here we would have basically D times F plus h times F minus D times F divide h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2151" target="_blank">00:35:51.600</a></span> | <span class="t">and then you see how the D F minus D F cancels so you're left with h times F</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2156" target="_blank">00:35:56.160</a></span> | <span class="t">divide h which is F so in the limit as h goes to zero of you know derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2166" target="_blank">00:36:06.720</a></span> | <span class="t">definition we just get F in the case of D times F so symmetrically D L by D F will just be D</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2177" target="_blank">00:36:17.120</a></span> | <span class="t">so what we have is that F dot grad we see now is just the value of D which is four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2185" target="_blank">00:36:25.120</a></span> | <span class="t">and we see that D dot grad is just the value of F</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2194" target="_blank">00:36:34.560</a></span> | <span class="t">and so the value of F is negative two so we'll set those manually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2201" target="_blank">00:36:41.600</a></span> | <span class="t">let me erase this markdown node and then let's redraw what we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2206" target="_blank">00:36:46.640</a></span> | <span class="t">okay and let's just make sure that these were correct so we seem to think that D L by D D is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2215" target="_blank">00:36:55.040</a></span> | <span class="t">negative two so let's double check um let me erase this plus h from before and now we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2220" target="_blank">00:37:00.400</a></span> | <span class="t">derivative with respect to F so let's just come here when I create F and let's do a plus h here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2225" target="_blank">00:37:05.840</a></span> | <span class="t">and this should print a derivative of L with respect to F so we expect to see four yeah and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2231" target="_blank">00:37:11.360</a></span> | <span class="t">this is four up to floating point funkiness and then D L by D D should be F which is negative two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2242" target="_blank">00:37:22.320</a></span> | <span class="t">grad is negative two so if we again come here and we change D</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2246" target="_blank">00:37:26.080</a></span> | <span class="t">D dot data plus equals h right here so we expect so we've added a little h and then we see how L</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2255" target="_blank">00:37:35.040</a></span> | <span class="t">changed and we expect to print negative two there we go so we've numerically verified what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2265" target="_blank">00:37:45.760</a></span> | <span class="t">doing here is kind of like an inline gradient check gradient check is when we are deriving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2270" target="_blank">00:37:50.720</a></span> | <span class="t">this like back propagation and getting the derivative with respect to all the intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2274" target="_blank">00:37:54.720</a></span> | <span class="t">results and then numerical gradient is just you know um estimating it using small step size now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2281" target="_blank">00:38:01.760</a></span> | <span class="t">we're getting to the crux of back propagation so this will be the most important node to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2288" target="_blank">00:38:08.320</a></span> | <span class="t">because if you understand the gradient for this node you understand the gradient for this node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2292" target="_blank">00:38:12.720</a></span> | <span class="t">because if you understand the gradient for this node you understand all of back propagation and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2297" target="_blank">00:38:17.040</a></span> | <span class="t">all of training of neural nets basically so we need to derive D L by D C in other words the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2303" target="_blank">00:38:23.920</a></span> | <span class="t">derivative of L with respect to C because we've computed all these other gradients already now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2309" target="_blank">00:38:29.840</a></span> | <span class="t">we're coming here and we're continuing the back propagation manually so we want D L by D C and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2315" target="_blank">00:38:35.840</a></span> | <span class="t">we'll also derive D L by D E now here's the problem how do we derive D L by D C we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2324" target="_blank">00:38:44.640</a></span> | <span class="t">know the derivative L with respect to D so we know how L is sensitive to D but how is L sensitive to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2332" target="_blank">00:38:52.240</a></span> | <span class="t">C so if we wiggle C how does that impact L through D so we know D L by D C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2341" target="_blank">00:39:01.920</a></span> | <span class="t">and we also here know how C impacts D and so just very intuitively if you know the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2346" target="_blank">00:39:06.800</a></span> | <span class="t">impact that C is having on D and the impact that D is having on L then you should be able to somehow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2352" target="_blank">00:39:12.720</a></span> | <span class="t">put that information together to figure out how C impacts L and indeed this is what we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2358" target="_blank">00:39:18.480</a></span> | <span class="t">do so in particular we know just concentrating on D first let's look at how what is the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2364" target="_blank">00:39:24.400</a></span> | <span class="t">basically of D with respect to C so in other words what is D D by D C so here we know that D</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2373" target="_blank">00:39:33.040</a></span> | <span class="t">is C times C plus E that's what we know and now we're interested in D D by D C if you just know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2380" target="_blank">00:39:40.400</a></span> | <span class="t">your calculus again and you remember then differentiating C plus E with respect to C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2384" target="_blank">00:39:44.880</a></span> | <span class="t">you know that that gives you 1.0 and we can also go back to the basics and derive this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2390" target="_blank">00:39:50.880</a></span> | <span class="t">because again we can go to our f of x plus h minus f of x divide by h that's the definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2397" target="_blank">00:39:57.280</a></span> | <span class="t">of a derivative as h goes to zero and so here focusing on C and its effect on D we can basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2405" target="_blank">00:40:05.120</a></span> | <span class="t">do the f of x plus h will be C is incremented by h plus E that's the first evaluation of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2412" target="_blank">00:40:12.160</a></span> | <span class="t">function minus C plus E and then divide h and so what is this just expanding this out this will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2421" target="_blank">00:40:21.840</a></span> | <span class="t">C plus h plus C minus C minus E divide h and then you see here how C minus C cancels E minus E</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2429" target="_blank">00:40:29.680</a></span> | <span class="t">cancels we're left with h over h which is 1.0 and so by symmetry also D D by D E will be 1.0 as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2442" target="_blank">00:40:42.000</a></span> | <span class="t">so basically the derivative of a sum expression is very simple and this is the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2448" target="_blank">00:40:48.240</a></span> | <span class="t">so I call this the local derivative because we have the final output value all the way at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2452" target="_blank">00:40:52.480</a></span> | <span class="t">end of this graph and we're now like a small node here and this is a little plus node and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2458" target="_blank">00:40:58.320</a></span> | <span class="t">it the little plus node doesn't know anything about the rest of the graph that it's embedded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2462" target="_blank">00:41:02.880</a></span> | <span class="t">in all it knows is that it did plus it took a C and an E added them and created D and this plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2469" target="_blank">00:41:09.760</a></span> | <span class="t">node also knows the local influence of C on D or rather the derivative of D with respect to C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2476" target="_blank">00:41:16.000</a></span> | <span class="t">and it also knows the derivative of D with respect to E but that's not what we want that's just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2481" target="_blank">00:41:21.520</a></span> | <span class="t">local derivative what we actually want is D L by D C and L is here just one step away</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2489" target="_blank">00:41:29.120</a></span> | <span class="t">but in the general case this little plus node could be embedded in like a massive graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2494" target="_blank">00:41:34.560</a></span> | <span class="t">so again we know how L impacts D and now we know how C and E impact D how do we put that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2501" target="_blank">00:41:41.360</a></span> | <span class="t">information together to write D L by D C and the answer of course is the chain rule in calculus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2506" target="_blank">00:41:46.880</a></span> | <span class="t">and so I pulled up chain rule here from Wikipedia and I'm going to go through this very briefly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2515" target="_blank">00:41:55.520</a></span> | <span class="t">so chain rule Wikipedia sometimes can be very confusing and calculus can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2520" target="_blank">00:42:00.480</a></span> | <span class="t">be very confusing like this is the way I learned chain rule and it was very confusing like what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2526" target="_blank">00:42:06.960</a></span> | <span class="t">is happening it's just complicated so I like this expression much better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2531" target="_blank">00:42:11.200</a></span> | <span class="t">if a variable Z depends on a variable Y which itself depends on a variable X</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2536" target="_blank">00:42:16.960</a></span> | <span class="t">then Z depends on X as well obviously through the intermediate variable Y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2541" target="_blank">00:42:21.680</a></span> | <span class="t">and in this case the chain rule is expressed as if you want D Z by D X then you take the D Z by D Y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2550" target="_blank">00:42:30.400</a></span> | <span class="t">and you multiply it by D Y by D X so the chain rule fundamentally is telling you how we chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2558" target="_blank">00:42:38.240</a></span> | <span class="t">these derivatives together correctly so to differentiate through a function composition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2565" target="_blank">00:42:45.440</a></span> | <span class="t">we have to apply a multiplication of those derivatives so that's really what chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2573" target="_blank">00:42:53.360</a></span> | <span class="t">is telling us and there's a nice little intuitive explanation here which I also think is kind of cute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2579" target="_blank">00:42:59.120</a></span> | <span class="t">the chain rule states that knowing the instantaneous rate of change of Z with respect to Y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2582" target="_blank">00:43:02.880</a></span> | <span class="t">and Y relative to X allows one to calculate the instantaneous rate of change of Z relative to X</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2587" target="_blank">00:43:07.120</a></span> | <span class="t">as a product of those two rates of change simply the product of those two so here's a good one if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2594" target="_blank">00:43:14.480</a></span> | <span class="t">a car travels twice as fast as a bicycle and the bicycle is four times as fast as walking men</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2599" target="_blank">00:43:19.200</a></span> | <span class="t">then the car travels two times four eight times as fast as the man and so this makes it very clear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2606" target="_blank">00:43:26.800</a></span> | <span class="t">that the correct thing to do sort of is to multiply so car is twice as fast as bicycle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2613" target="_blank">00:43:33.680</a></span> | <span class="t">and bicycle is four times as fast as man so the car will be eight times as fast as the man</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2619" target="_blank">00:43:39.440</a></span> | <span class="t">and so we can take these intermediate rates of change if you will and multiply them together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2625" target="_blank">00:43:45.760</a></span> | <span class="t">and that justifies the chain rule intuitively so have a look at chain rule but here really what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2632" target="_blank">00:43:52.560</a></span> | <span class="t">it means for us is there's a very simple recipe for deriving what we want which is dL by dc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2638" target="_blank">00:43:58.000</a></span> | <span class="t">and what we have so far is we know want and we know what is the impact of d on L so we know dL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2651" target="_blank">00:44:11.760</a></span> | <span class="t">by dd the derivative of L with respect to dd we know that that's negative two and now because of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2658" target="_blank">00:44:18.240</a></span> | <span class="t">this local reasoning that we've done here we know dd by dc so how does c impact d and in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2668" target="_blank">00:44:28.000</a></span> | <span class="t">this is a plus node so the local derivative is simply 1.0 it's very simple and so the chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2674" target="_blank">00:44:34.720</a></span> | <span class="t">rule tells us that dL by dc going through this intermediate variable will just be simply dL by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2684" target="_blank">00:44:44.000</a></span> | <span class="t">dd times dd by dc that's chain rule so this is identical to what's happening here except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2697" target="_blank">00:44:57.120</a></span> | <span class="t">z is our L y is our d and x is our c so we literally just have to multiply these and because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2710" target="_blank">00:45:10.320</a></span> | <span class="t">these local derivatives like dd by dc are just one we basically just copy over dL by dd because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2717" target="_blank">00:45:17.680</a></span> | <span class="t">this is just times one so what does it do so because dL by dd is negative two what is dL by dc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2724" target="_blank">00:45:24.640</a></span> | <span class="t">well it's the local gradient 1.0 times dL by dd which is negative two so literally what a plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2732" target="_blank">00:45:32.880</a></span> | <span class="t">node does you can look at it that way is it literally just routes the gradient because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2737" target="_blank">00:45:37.760</a></span> | <span class="t">the plus nodes local derivatives are just one and so in the chain rule one times dL by dd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2744" target="_blank">00:45:44.400</a></span> | <span class="t">is just dL by dd and so that derivative just gets routed to both c and to e in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2754" target="_blank">00:45:54.560</a></span> | <span class="t">so basically we have that e dot grad or let's start with c since that's the one we've looked at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2762" target="_blank">00:46:02.480</a></span> | <span class="t">is negative two times one negative two and in the same way by symmetry e dot grad will be negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2772" target="_blank">00:46:12.080</a></span> | <span class="t">two that's the claim so we can set those we can redraw and you see how we just assign negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2780" target="_blank">00:46:20.880</a></span> | <span class="t">two negative two so this back propagating signal which is carrying the information of like what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2785" target="_blank">00:46:25.680</a></span> | <span class="t">the derivative of L with respect to all the intermediate nodes we can imagine it almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2790" target="_blank">00:46:30.080</a></span> | <span class="t">like flowing backwards through the graph and a plus node will simply distribute the derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2794" target="_blank">00:46:34.800</a></span> | <span class="t">to all the leaf nodes sorry to all the children nodes of it so this is the claim and now let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2800" target="_blank">00:46:40.480</a></span> | <span class="t">verify it so let me remove the plus h here from before and now instead what we're going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2806" target="_blank">00:46:46.400</a></span> | <span class="t">we want to increment c so c dot data will be incremented by h and when I run this we expect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2812" target="_blank">00:46:52.080</a></span> | <span class="t">to see negative two negative two and then of course for e so e dot data plus equals h and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2820" target="_blank">00:47:00.960</a></span> | <span class="t">we expect to see negative two simple so those are the derivatives of these internal nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2830" target="_blank">00:47:10.160</a></span> | <span class="t">and now we're going to recurse our way backwards again and we're again going to apply the chain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2837" target="_blank">00:47:17.200</a></span> | <span class="t">rule so here we go our second application of chain rule and we will apply it all the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2841" target="_blank">00:47:21.920</a></span> | <span class="t">through the graph we just happen to only have one more node remaining we have that dl by de</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2847" target="_blank">00:47:27.840</a></span> | <span class="t">as we have just calculated is negative two so we know that so we know the derivative of l with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2853" target="_blank">00:47:33.920</a></span> | <span class="t">respect to e and now we want dl by da right and the chain rule is telling us that that's just dl by de</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2866" target="_blank">00:47:46.800</a></span> | <span class="t">times the local gradient so what is the local gradient basically de by da we have to look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2878" target="_blank">00:47:58.000</a></span> | <span class="t">that so i'm a little times node inside a massive graph and i only know that i did a times b and i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2886" target="_blank">00:48:06.960</a></span> | <span class="t">produced an e so now what is de by da and de by db that's the only thing that i sort of know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2894" target="_blank">00:48:14.400</a></span> | <span class="t">that's my local gradient so because we have that e is a times b we're asking what is de by da</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2902" target="_blank">00:48:22.480</a></span> | <span class="t">and of course we just did that here we had a times so i'm not going to re-derive it but if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2911" target="_blank">00:48:31.040</a></span> | <span class="t">to differentiate this with respect to a you'll just get b right the value of b which in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2917" target="_blank">00:48:37.680</a></span> | <span class="t">is negative 3.0 so basically we have that dl by da well let me just do it right here we have that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2927" target="_blank">00:48:47.520</a></span> | <span class="t">a dot grad and we are applying chain rule here is dl by de which we see here is negative two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2934" target="_blank">00:48:54.800</a></span> | <span class="t">times what is de by da it's the value of b which is negative three that's it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2945" target="_blank">00:49:05.680</a></span> | <span class="t">and then we have b dot grad is again dl by de which is negative two just the same way times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2953" target="_blank">00:49:13.440</a></span> | <span class="t">what is de by d db is the value of a which is 2.0 that's the value of a so these are our claimed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2966" target="_blank">00:49:26.080</a></span> | <span class="t">derivatives let's redraw and we see here that a dot grad turns out to be six because that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2973" target="_blank">00:49:33.440</a></span> | <span class="t">negative two times negative three and b dot grad is negative four times sorry is negative two times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2980" target="_blank">00:49:40.080</a></span> | <span class="t">two which is negative four so those are our claims let's delete this and let's verify them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2987" target="_blank">00:49:47.680</a></span> | <span class="t">we have a here a dot data plus equals h so the claim is that a dot grad is six let's verify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=2999" target="_blank">00:49:59.360</a></span> | <span class="t">six and we have b dot data plus equals h so nudging b by h and looking at what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3010" target="_blank">00:50:10.320</a></span> | <span class="t">we claim it's negative four and indeed it's negative four plus minus again float oddness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3016" target="_blank">00:50:16.080</a></span> | <span class="t">and that's it this that was the manual back propagation all the way from here to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3026" target="_blank">00:50:26.880</a></span> | <span class="t">leaf nodes and we've done it piece by piece and really all we've done is as you saw we iterated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3032" target="_blank">00:50:32.400</a></span> | <span class="t">through all the nodes one by one and locally applied the chain rule we also applied the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3037" target="_blank">00:50:37.280</a></span> | <span class="t">one and locally applied the chain rule we always know what is the derivative of l with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3042" target="_blank">00:50:42.880</a></span> | <span class="t">this little output and then we look at how this output was produced this output was produced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3047" target="_blank">00:50:47.600</a></span> | <span class="t">through some operation and we have the pointers to the children nodes of this operation and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3053" target="_blank">00:50:53.200</a></span> | <span class="t">in this little operation we know what the local derivatives are and we just multiply them onto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3057" target="_blank">00:50:57.920</a></span> | <span class="t">the derivative always so we just go through and recursively multiply on the local derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3064" target="_blank">00:51:04.080</a></span> | <span class="t">and that's what back propagation is it's just a recursive application of chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3068" target="_blank">00:51:08.160</a></span> | <span class="t">backwards through the computation graph let's see this power in action just very briefly what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3073" target="_blank">00:51:13.680</a></span> | <span class="t">we're going to do is we're going to nudge our inputs to try to make l go up so in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3080" target="_blank">00:51:20.560</a></span> | <span class="t">what we're doing is we want a dot data we're going to change it and if we want l to go up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3086" target="_blank">00:51:26.000</a></span> | <span class="t">that means we just have to go in the direction of the gradient so a should increase in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3091" target="_blank">00:51:31.840</a></span> | <span class="t">direction of gradient by like some small step amount this is the step size and we don't just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3097" target="_blank">00:51:37.040</a></span> | <span class="t">want this for b but also for b also for c also for f those are leaf nodes which we usually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3109" target="_blank">00:51:49.280</a></span> | <span class="t">control over and if we nudge in direction of the gradient we expect a positive influence on l</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3115" target="_blank">00:51:55.680</a></span> | <span class="t">so we expect l to go up positively so it should become less negative it should go up to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3122" target="_blank">00:52:02.640</a></span> | <span class="t">negative you know six or something like that it's hard to tell exactly and we'd have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3128" target="_blank">00:52:08.240</a></span> | <span class="t">rerun the forward pass so let me just do that here this would be the forward pass f would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3138" target="_blank">00:52:18.960</a></span> | <span class="t">unchanged this is effectively the forward pass and now if we print l dot data we expect because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3146" target="_blank">00:52:26.080</a></span> | <span class="t">we nudged all the values all the inputs in the direction of gradient we expected less negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3150" target="_blank">00:52:30.640</a></span> | <span class="t">l we expect it to go up so maybe it's negative six or so let's see what happens okay negative seven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3157" target="_blank">00:52:37.520</a></span> | <span class="t">and this is basically one step of an optimization that will end up running and really this gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3165" target="_blank">00:52:45.120</a></span> | <span class="t">just give us some power because we know how to influence the final outcome and this will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3169" target="_blank">00:52:49.760</a></span> | <span class="t">extremely useful for training neural nets as well as cnc so now i would like to do one more example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3176" target="_blank">00:52:56.080</a></span> | <span class="t">of manual back propagation using a bit more complex and useful example we are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3182" target="_blank">00:53:02.960</a></span> | <span class="t">back propagate through a neuron so we want to eventually build up neural networks and in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3190" target="_blank">00:53:10.160</a></span> | <span class="t">simplest case these are multi-layer perceptrons as they're called so this is a two-layer neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3194" target="_blank">00:53:14.800</a></span> | <span class="t">net and it's got these hidden layers made up of neurons and these neurons are fully connected to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3199" target="_blank">00:53:19.040</a></span> | <span class="t">each other now biologically neurons are very complicated devices but we have very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3203" target="_blank">00:53:23.920</a></span> | <span class="t">mathematical models of them and so this is a very simple mathematical model of a neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3209" target="_blank">00:53:29.360</a></span> | <span class="t">you have some inputs x's and then you have these synapses that have weights on them so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3215" target="_blank">00:53:35.600</a></span> | <span class="t">the w's are weights and then the synapse interacts with the input to this neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3223" target="_blank">00:53:43.360</a></span> | <span class="t">multiplicatively so what flows to the cell body of this neuron is w times x but there's multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3230" target="_blank">00:53:50.720</a></span> | <span class="t">inputs so there's many w times x's flowing to the cell body the cell body then has also like some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3236" target="_blank">00:53:56.960</a></span> | <span class="t">bias so this is kind of like the in innate sort of trigger happiness of this neuron so this bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3244" target="_blank">00:54:04.240</a></span> | <span class="t">can make it a bit more trigger happy or a bit less trigger happy regardless of the input but basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3248" target="_blank">00:54:08.960</a></span> | <span class="t">we're taking all the w times x of all the inputs adding the bias and then we take it through an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3255" target="_blank">00:54:15.040</a></span> | <span class="t">activation function and this activation function is usually some kind of a squashing function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3259" target="_blank">00:54:19.760</a></span> | <span class="t">like a sigmoid or 10h or something like that so as an example we're going to use the 10h in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3266" target="_blank">00:54:26.320</a></span> | <span class="t">example numpy has a np.10h so we can call it on a range and we can plot it this is the 10h function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3278" target="_blank">00:54:38.080</a></span> | <span class="t">and you see that the inputs as they come in get squashed on the y coordinate here so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3284" target="_blank">00:54:44.000</a></span> | <span class="t">right at zero we're going to get exactly zero and then as you go more positive in the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3290" target="_blank">00:54:50.080</a></span> | <span class="t">then you'll see that the function will only go up to one and then plateau out and so if you pass in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3297" target="_blank">00:54:57.200</a></span> | <span class="t">very positive inputs we're going to cap it smoothly at one and on the negative side we're going to cap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3302" target="_blank">00:55:02.320</a></span> | <span class="t">it smoothly to negative one so that's 10h and that's the squashing function or an activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3309" target="_blank">00:55:09.040</a></span> | <span class="t">function and what comes out of this neuron is just the activation function applied to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3314" target="_blank">00:55:14.000</a></span> | <span class="t">dot product of the weights and the inputs so let's write one out i'm going to copy paste because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3327" target="_blank">00:55:27.360</a></span> | <span class="t">i don't want to type too much but okay so here we have the inputs x1 x2 so this is a two-dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3333" target="_blank">00:55:33.360</a></span> | <span class="t">neuron so two inputs are going to come in these are thought of as the weights of this neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3338" target="_blank">00:55:38.160</a></span> | <span class="t">weights w1 w2 and these weights again are the synaptic strengths for each input and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3345" target="_blank">00:55:45.600</a></span> | <span class="t">the bias of the neuron b and now what we want to do is according to this model we need to multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3353" target="_blank">00:55:53.440</a></span> | <span class="t">x1 times w1 and x2 times w2 and then we need to add bias on top of it and it gets a little messy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3362" target="_blank">00:56:02.720</a></span> | <span class="t">here but all we are trying to do is x1 w1 plus x2 w2 plus b and these are multiplied here except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3370" target="_blank">00:56:10.320</a></span> | <span class="t">i'm doing it in small steps so that we actually have pointers to all these intermediate nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3375" target="_blank">00:56:15.040</a></span> | <span class="t">so we have x1 w1 variable x times x2 w2 variable and i'm also labeling them so n is now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3383" target="_blank">00:56:23.760</a></span> | <span class="t">the cell body raw activation without the activation function for now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3389" target="_blank">00:56:29.680</a></span> | <span class="t">and this should be enough to basically plot it so draw dot of n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3394" target="_blank">00:56:34.560</a></span> | <span class="t">gives us x1 times w1 x2 times w2 being added then the bias gets added on top of this and this n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3407" target="_blank">00:56:47.280</a></span> | <span class="t">is this sum so we're now going to take it through an activation function and let's say we use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3413" target="_blank">00:56:53.440</a></span> | <span class="t">tan h so that we produce the output so what we'd like to do here is we'd like to do the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3419" target="_blank">00:56:59.600</a></span> | <span class="t">and i'll call it o is n dot tan h okay but we haven't yet written the tan h now the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3428" target="_blank">00:57:08.880</a></span> | <span class="t">that we need to implement another tan h function here is that tan h is a hyperbolic function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3436" target="_blank">00:57:16.000</a></span> | <span class="t">we've only so far implemented plus and the times and you can't make a tan h out of just pluses and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3440" target="_blank">00:57:20.960</a></span> | <span class="t">times you also need exponentiation so tan h is this kind of a formula here you can use either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3447" target="_blank">00:57:27.840</a></span> | <span class="t">one of these and you see that there's exponentiation involved which we have not implemented yet for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3452" target="_blank">00:57:32.560</a></span> | <span class="t">our low value node here so we're not going to be able to produce tan h yet and we have to go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3456" target="_blank">00:57:36.800</a></span> | <span class="t">up and implement something like it now one option here is we could actually implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3464" target="_blank">00:57:44.880</a></span> | <span class="t">exponentiation right and we could return the exp of a value instead of a tan h of a value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3471" target="_blank">00:57:51.760</a></span> | <span class="t">because if we had exp then we have everything else that we need so because we know how to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3478" target="_blank">00:57:58.560</a></span> | <span class="t">and we know how to we know how to add and we know how to multiply so we'd be able to create tan h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3484" target="_blank">00:58:04.960</a></span> | <span class="t">if we knew how to exp but for the purposes of this example i specifically wanted to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3490" target="_blank">00:58:10.000</a></span> | <span class="t">show you that we don't necessarily need to have the most atomic pieces in this value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3498" target="_blank">00:58:18.160</a></span> | <span class="t">we can actually like create functions at arbitrary points of abstraction they can be complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3505" target="_blank">00:58:25.200</a></span> | <span class="t">functions but they can be also very very simple functions like a plus and it's totally up to us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3510" target="_blank">00:58:30.000</a></span> | <span class="t">the only thing that matters is that we know how to differentiate through any one function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3514" target="_blank">00:58:34.160</a></span> | <span class="t">so we take some inputs and we make an output the only thing that matters it can be arbitrarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3518" target="_blank">00:58:38.560</a></span> | <span class="t">complex function as long as you know how to create the local derivative if you know the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3523" target="_blank">00:58:43.360</a></span> | <span class="t">local derivative of how the inputs impact the output then that's all you need so we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3527" target="_blank">00:58:47.760</a></span> | <span class="t">to cluster up all of this expression and we're not going to break it down to its atomic pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3533" target="_blank">00:58:53.200</a></span> | <span class="t">we're just going to directly implement tan h so let's do that dev tan h and then out will be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3540" target="_blank">00:59:00.320</a></span> | <span class="t">value of and we need this expression here so um let me actually copy paste</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3551" target="_blank">00:59:11.040</a></span> | <span class="t">let's grab n which is a sub theta and then this i believe is the tan h math.exp of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3564" target="_blank">00:59:24.560</a></span> | <span class="t">two no n minus one over two n plus one maybe i can call this x just so that it matches exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3574" target="_blank">00:59:34.320</a></span> | <span class="t">okay and now this will be t and uh children of this node there's just one child and i'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3584" target="_blank">00:59:44.800</a></span> | <span class="t">wrapping it in a tuple so this is a tuple of one object just self and here the name of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3590" target="_blank">00:59:50.240</a></span> | <span class="t">operation will be tan h and we're going to return that okay so now value should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3600" target="_blank">01:00:00.640</a></span> | <span class="t">implementing tan h and now we can scroll all the way down here and we can actually do n dot tan h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3606" target="_blank">01:00:06.560</a></span> | <span class="t">and that's going to return the tan h output of n and now we should be able to draw dot of o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3613" target="_blank">01:00:13.360</a></span> | <span class="t">not of n so let's see how that worked there we go n went through tan h to produce this output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3623" target="_blank">01:00:23.200</a></span> | <span class="t">so now tan h is a sort of uh our little micro grad supported node here as an operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3631" target="_blank">01:00:31.120</a></span> | <span class="t">and as long as we know the derivative of tan h then we'll be able to back propagate through it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3638" target="_blank">01:00:38.240</a></span> | <span class="t">now let's see this tan h in action currently it's not squashing too much because the input to it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3643" target="_blank">01:00:43.600</a></span> | <span class="t">pretty low so the bias was increased to say eight then we'll see that what's flowing into the tan h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3651" target="_blank">01:00:51.680</a></span> | <span class="t">now is two and tan h is squashing it to 0.96 so we're already hitting the tail of this tan h and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3659" target="_blank">01:00:59.920</a></span> | <span class="t">it will sort of smoothly go up to one and then plateau out over there okay so now i'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3664" target="_blank">01:01:04.240</a></span> | <span class="t">do something slightly strange i'm going to change this bias from eight to this number 6.88 etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3671" target="_blank">01:01:11.840</a></span> | <span class="t">and i'm going to do this for specific reasons because we're about to start back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3676" target="_blank">01:01:16.400</a></span> | <span class="t">and i want to make sure that our numbers come out nice they're not like very crazy numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3681" target="_blank">01:01:21.600</a></span> | <span class="t">they're nice numbers that we can sort of understand in our head let me also add those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3685" target="_blank">01:01:25.840</a></span> | <span class="t">label o is short for output here so that's the r okay so 0.88 flows into tan h comes out 0.7</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3695" target="_blank">01:01:35.120</a></span> | <span class="t">so on so now we're going to do back propagation and we're going to fill in all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3700" target="_blank">01:01:40.240</a></span> | <span class="t">so what is the derivative o with respect to all the inputs here and of course in a typical neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3707" target="_blank">01:01:47.280</a></span> | <span class="t">network setting what we really care about the most is the derivative of these neurons on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3712" target="_blank">01:01:52.720</a></span> | <span class="t">weights specifically the w2 and w1 because those are the weights that we're going to be changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3717" target="_blank">01:01:57.760</a></span> | <span class="t">part of the optimization and the other thing that we have to remember is here we have only a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3722" target="_blank">01:02:02.240</a></span> | <span class="t">neuron but in the neural net you typically have many neurons and they're connected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3727" target="_blank">01:02:07.120</a></span> | <span class="t">so this is only like a one small neuron a piece of a much bigger puzzle and eventually there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3731" target="_blank">01:02:11.280</a></span> | <span class="t">loss function that sort of measures the accuracy of the neural net and we're back propagating with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3735" target="_blank">01:02:15.440</a></span> | <span class="t">respect to that accuracy and trying to increase it so let's start off back propagation here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3741" target="_blank">01:02:21.280</a></span> | <span class="t">what is the derivative of o with respect to o the base case sort of we know always is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3747" target="_blank">01:02:27.920</a></span> | <span class="t">the gradient is just 1.0 so let me fill it in and then let me split out the drawing function here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3760" target="_blank">01:02:40.320</a></span> | <span class="t">and then here cell clear this output here okay so now when we draw o we'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3772" target="_blank">01:02:52.320</a></span> | <span class="t">o that grad is 1 so now we're going to back propagate through the tan h so to back propagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3777" target="_blank">01:02:57.840</a></span> | <span class="t">through tan h we need to know the local derivative of tan h so if we have that o is tan h of n</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3787" target="_blank">01:03:07.200</a></span> | <span class="t">then what is do by dn now what you could do is you could come here and you could take this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3794" target="_blank">01:03:14.640</a></span> | <span class="t">expression and you could do your calculus derivative taking and that would work but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3800" target="_blank">01:03:20.880</a></span> | <span class="t">we can also just scroll down wikipedia here into a section that hopefully tells us that derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3807" target="_blank">01:03:27.520</a></span> | <span class="t">d by dx of tan h of x is any of these i like this one 1 minus tan h square of x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3814" target="_blank">01:03:34.480</a></span> | <span class="t">so this is 1 minus tan h of x squared so basically what this is saying is that do by dn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3823" target="_blank">01:03:43.760</a></span> | <span class="t">is 1 minus tan h of n squared and we already have tan h of n it's just o so it's 1 minus o squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3835" target="_blank">01:03:55.760</a></span> | <span class="t">so o is the output here so the output is this number o dot data is this number and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3848" target="_blank">01:04:08.080</a></span> | <span class="t">what this is saying is that do by dn is 1 minus this squared so 1 minus o dot data squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3854" target="_blank">01:04:14.880</a></span> | <span class="t">is 0.5 conveniently so the local derivative of this tan h operation here is 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3863" target="_blank">01:04:23.120</a></span> | <span class="t">and so that would be do by dn so we can fill in that n dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3870" target="_blank">01:04:30.640</a></span> | <span class="t">is 0.5 we'll just fill it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3875" target="_blank">01:04:35.440</a></span> | <span class="t">so this is exactly 0.5 one half so now we're going to continue the back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3887" target="_blank">01:04:47.680</a></span> | <span class="t">this is 0.5 and this is a plus node so how is backprop going to what is backprop going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3895" target="_blank">01:04:55.680</a></span> | <span class="t">here and if you remember our previous example a plus is just a distributor of gradient so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3902" target="_blank">01:05:02.160</a></span> | <span class="t">gradient will simply flow to both of these equally and that's because the local derivative of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3906" target="_blank">01:05:06.960</a></span> | <span class="t">operation is one for every one of its nodes so one times 0.5 is 0.5 so therefore we know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3914" target="_blank">01:05:14.880</a></span> | <span class="t">this node here which we called this it's grad it's just 0.5 and we know that b dot grad is also 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3924" target="_blank">01:05:24.800</a></span> | <span class="t">so let's set those and let's draw so those are 0.5 continuing we have another plus 0.5 again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3933" target="_blank">01:05:33.280</a></span> | <span class="t">we'll just distribute so 0.5 will flow to both of these so we can set theirs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3939" target="_blank">01:05:39.520</a></span> | <span class="t">x2 w2 as well dot grad is 0.5 and let's redraw pluses are my favorite uh operations to back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3951" target="_blank">01:05:51.280</a></span> | <span class="t">propagate through because it's very simple so now what's flowing into these expressions is 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3957" target="_blank">01:05:57.760</a></span> | <span class="t">and so really again keep in mind what the derivative is telling us at every point in time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3961" target="_blank">01:06:01.360</a></span> | <span class="t">along here this is saying that if we want the output of this neuron to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3966" target="_blank">01:06:06.480</a></span> | <span class="t">then the influence on these expressions is positive on the output both of them are positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3973" target="_blank">01:06:13.760</a></span> | <span class="t">contribution to the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3980" target="_blank">01:06:20.480</a></span> | <span class="t">so now back propagating to x1 w2 first this is a times node so we know that the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3987" target="_blank">01:06:27.120</a></span> | <span class="t">is you know the other term so if we want to calculate x2 dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3991" target="_blank">01:06:31.440</a></span> | <span class="t">then can you think through what it's going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=3995" target="_blank">01:06:35.120</a></span> | <span class="t">so x2 dot grad will be w2 dot data times this x2 w2 dot grad right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4009" target="_blank">01:06:49.840</a></span> | <span class="t">and w2 dot grad will be x2 dot data times x2 w2 dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4017" target="_blank">01:06:57.840</a></span> | <span class="t">right so that's the little local piece of chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4023" target="_blank">01:07:03.040</a></span> | <span class="t">let's set them and let's redraw so here we see that the gradient on our weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4030" target="_blank">01:07:10.560</a></span> | <span class="t">2 is 0 because x2's data was 0 right but x2 will have the gradient 0.5 because data here was 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4038" target="_blank">01:07:18.560</a></span> | <span class="t">and so what's interesting here right is because the input x2 was 0 then because of the way the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4045" target="_blank">01:07:25.440</a></span> | <span class="t">times works of course this gradient will be 0 and think about intuitively why that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4050" target="_blank">01:07:30.640</a></span> | <span class="t">derivative always tells us the influence of this on the final output if i wiggle w2 how is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4059" target="_blank">01:07:39.200</a></span> | <span class="t">output changing it's not changing because we're multiplying by 0 so because it's not changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4064" target="_blank">01:07:44.400</a></span> | <span class="t">there is no derivative and 0 is the correct answer because we're squashing that 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4069" target="_blank">01:07:49.600</a></span> | <span class="t">and let's do it here 0.5 should come here and flow through this times and so we'll have that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4078" target="_blank">01:07:58.240</a></span> | <span class="t">x1 dot grad is can you think through a little bit what what this should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4086" target="_blank">01:08:06.160</a></span> | <span class="t">the local derivative of times with respect to x1 is going to be w1 so w1's data times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4094" target="_blank">01:08:14.240</a></span> | <span class="t">x1 w1 dot grad and w1 dot grad will be x1 dot data times x1 w2 w1 dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4105" target="_blank">01:08:25.760</a></span> | <span class="t">let's see what those came out to be so this is 0.5 so this would be negative 1.5 and this would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4113" target="_blank">01:08:33.360</a></span> | <span class="t">1 and we've back propagated through this expression these are the actual final derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4119" target="_blank">01:08:39.040</a></span> | <span class="t">so if we want this neuron's output to increase we know that what's necessary is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4125" target="_blank">01:08:45.440</a></span> | <span class="t">w2 we have no gradient w2 doesn't actually matter to this neuron right now but this neuron this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4132" target="_blank">01:08:52.640</a></span> | <span class="t">weight should go up so if this weight goes up then this neuron's output would have gone up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4139" target="_blank">01:08:59.360</a></span> | <span class="t">and proportionally because the gradient is 1 okay so doing the back propagation manually is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4144" target="_blank">01:09:04.080</a></span> | <span class="t">obviously ridiculous so we are now going to put an end to this suffering and we're going to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4148" target="_blank">01:09:08.560</a></span> | <span class="t">how we can implement the backward pass a bit more automatically we're not going to be doing all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4153" target="_blank">01:09:13.280</a></span> | <span class="t">it manually out here it's now pretty obvious to us by example how these pluses and times are back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4158" target="_blank">01:09:18.560</a></span> | <span class="t">propagating gradients so let's go up to the value object and we're going to start codifying what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4165" target="_blank">01:09:25.520</a></span> | <span class="t">we've seen in the examples below so we're going to do this by storing a special self.backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4172" target="_blank">01:09:32.720</a></span> | <span class="t">and underscore backward and this will be a function which is going to do that little piece</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4179" target="_blank">01:09:39.920</a></span> | <span class="t">of chain rule at each little node that compute that took inputs and produced output we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4185" target="_blank">01:09:45.520</a></span> | <span class="t">to store how we are going to chain the outputs gradient into the inputs gradients so by default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4194" target="_blank">01:09:54.000</a></span> | <span class="t">this will be a function that doesn't do anything so and you can also see that here in the value in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4201" target="_blank">01:10:01.600</a></span> | <span class="t">micro grad so we have this backward function by default doesn't do anything this is an empty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4209" target="_blank">01:10:09.040</a></span> | <span class="t">function and that would be sort of the case for example for a leaf node for leaf node there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4213" target="_blank">01:10:13.360</a></span> | <span class="t">nothing to do but now if when we're creating these out values these out values are an addition of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4222" target="_blank">01:10:22.160</a></span> | <span class="t">self and other and so we will want to sell set outs backward to be the function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4230" target="_blank">01:10:30.560</a></span> | <span class="t">propagates the gradient so let's define what should happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4237" target="_blank">01:10:37.200</a></span> | <span class="t">and we're going to store it in a closure let's define what should happen when we call outs grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4247" target="_blank">01:10:47.760</a></span> | <span class="t">for addition our job is to take outs grad and propagate it into selves grad and other dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4256" target="_blank">01:10:56.960</a></span> | <span class="t">so basically we want to solve self dot grad to something and we want to set others dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4262" target="_blank">01:11:02.720</a></span> | <span class="t">to something okay and the way we saw below how chain rule works we want to take the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4270" target="_blank">01:11:10.720</a></span> | <span class="t">times the um sort of global derivative i should call it which is the derivative of the final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4276" target="_blank">01:11:16.240</a></span> | <span class="t">output of the expression with respect to outs data with respect to out so the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4285" target="_blank">01:11:25.680</a></span> | <span class="t">of self in an addition is 1.0 so it's just 1.0 times outs grad that's the chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4295" target="_blank">01:11:35.120</a></span> | <span class="t">and others dot grad will be 1.0 times out grad and what you basically what you're seeing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4300" target="_blank">01:11:40.960</a></span> | <span class="t">is that outs grad will simply be copied onto selves grad and others grad as we saw happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4307" target="_blank">01:11:47.760</a></span> | <span class="t">for an addition operation so we're going to later call this function to propagate the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4313" target="_blank">01:11:53.440</a></span> | <span class="t">having done an addition let's now do multiplication we're going to also define backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4319" target="_blank">01:11:59.280</a></span> | <span class="t">and we're going to set its backward to be backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4327" target="_blank">01:12:07.760</a></span> | <span class="t">and we want to chain out grad into self dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4331" target="_blank">01:12:11.840</a></span> | <span class="t">and others dot grad and this will be a little piece of chain rule for multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4339" target="_blank">01:12:19.600</a></span> | <span class="t">so we'll have so what should this be can you think through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4343" target="_blank">01:12:23.920</a></span> | <span class="t">so what is the local derivative here the local derivative was others dot data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4355" target="_blank">01:12:35.360</a></span> | <span class="t">and then oops others dot data and then times out dot grad that's channel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4360" target="_blank">01:12:40.960</a></span> | <span class="t">and here we have self dot data times out dot grad that's what we've been doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4366" target="_blank">01:12:46.400</a></span> | <span class="t">and finally here for tan h that's backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4372" target="_blank">01:12:52.320</a></span> | <span class="t">and then we want to set outs backwards to be just backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4380" target="_blank">01:13:00.480</a></span> | <span class="t">and here we need to back propagate we have out dot grad and we want to chain it into self dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4387" target="_blank">01:13:07.440</a></span> | <span class="t">and self dot grad will be the local derivative of this operation that we've done here which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4394" target="_blank">01:13:14.880</a></span> | <span class="t">tan h and so we saw that the local gradient is 1 minus the tan h of x squared which here is t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4402" target="_blank">01:13:22.000</a></span> | <span class="t">that's the local derivative because that's t is the output of this tan h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4407" target="_blank">01:13:27.360</a></span> | <span class="t">so 1 minus t squared is the local derivative and then gradient has to be multiplied because of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4413" target="_blank">01:13:33.840</a></span> | <span class="t">chain rule so outs grad is chained through the local gradient into self dot grad and that should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4420" target="_blank">01:13:40.000</a></span> | <span class="t">be basically it so we're going to redefine our value node we're going to swing all the way down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4426" target="_blank">01:13:46.160</a></span> | <span class="t">here and we're going to redefine our expression make sure that all the grads are zero okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4436" target="_blank">01:13:56.240</a></span> | <span class="t">but now we don't have to do this manually anymore we are going to basically be calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4441" target="_blank">01:14:01.120</a></span> | <span class="t">the dot backward in the right order so first we want to call outs dot backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4449" target="_blank">01:14:09.040</a></span> | <span class="t">so o was the outcome of tan h right so calling those those backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4462" target="_blank">01:14:22.160</a></span> | <span class="t">will be this function this is what it will do now we have to be careful because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4468" target="_blank">01:14:28.480</a></span> | <span class="t">there's a times out dot grad and out dot grad remember is initialized to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4474" target="_blank">01:14:34.640</a></span> | <span class="t">so here we see grad zero so as a base case we need to set oath dot grad to 1.0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4486" target="_blank">01:14:46.640</a></span> | <span class="t">to initialize this with one and then once this is one we can call o dot backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4496" target="_blank">01:14:56.560</a></span> | <span class="t">and what that should do is it should propagate this grad through tan h so the local derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4503" target="_blank">01:15:03.360</a></span> | <span class="t">times the global derivative which is initialized at one so this should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4508" target="_blank">01:15:08.640</a></span> | <span class="t">um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4516" target="_blank">01:15:16.000</a></span> | <span class="t">dope so i thought about redoing it but i figured i should just leave the error in here because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4521" target="_blank">01:15:21.120</a></span> | <span class="t">pretty funny why is not an object not callable it's because i screwed up we're trying to save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4528" target="_blank">01:15:28.880</a></span> | <span class="t">these functions so this is correct this here you don't want to call the function because that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4535" target="_blank">01:15:35.040</a></span> | <span class="t">returns none these functions return none we just want to store the function so let me redefine the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4540" target="_blank">01:15:40.640</a></span> | <span class="t">value object and then we're going to come back in redefine the expression draw dot everything is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4547" target="_blank">01:15:47.120</a></span> | <span class="t">great o dot grad is one o dot grad is one and now now this should work of course okay so all that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4556" target="_blank">01:15:56.960</a></span> | <span class="t">backward should this grad should now be 0.5 if we redraw and if everything went correctly 0.5 yay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4565" target="_blank">01:16:05.200</a></span> | <span class="t">okay so now we need to call ns dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4567" target="_blank">01:16:07.760</a></span> | <span class="t">ns dot backward sorry ns backward so that seems to have worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4575" target="_blank">01:16:15.600</a></span> | <span class="t">so ns dot backward routed the gradient to both of these so this is looking great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4583" target="_blank">01:16:23.040</a></span> | <span class="t">now we could of course called uh called b dot grad b dot backward sorry what's going to happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4592" target="_blank">01:16:32.000</a></span> | <span class="t">well b doesn't have a backward b's backward because b is a leaf node b's backward is by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4598" target="_blank">01:16:38.960</a></span> | <span class="t">initialization the empty function so nothing would happen but we can call call it on it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4604" target="_blank">01:16:44.960</a></span> | <span class="t">but when we call this one it's backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4610" target="_blank">01:16:50.720</a></span> | <span class="t">then we expect this 0.5 to get further routed right so there we go 0.5 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4620" target="_blank">01:17:00.960</a></span> | <span class="t">and then finally we want to call it here on x2 w2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4627" target="_blank">01:17:07.120</a></span> | <span class="t">and on x1 w1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4631" target="_blank">01:17:11.840</a></span> | <span class="t">let's do both of those and there we go so we get 0.5 negative 1.5 and 1 exactly as we did before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4645" target="_blank">01:17:25.360</a></span> | <span class="t">but now we've done it through calling that backward sort of manually so we have the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4653" target="_blank">01:17:33.520</a></span> | <span class="t">last piece to get rid of which is us calling underscore backward manually so let's think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4658" target="_blank">01:17:38.640</a></span> | <span class="t">through what we are actually doing we've laid out a mathematical expression and now we're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4663" target="_blank">01:17:43.920</a></span> | <span class="t">go backwards through that expression so going backwards through the expression just means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4669" target="_blank">01:17:49.840</a></span> | <span class="t">we never want to call a dot backward for any node before we've done sort of everything after it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4678" target="_blank">01:17:58.800</a></span> | <span class="t">so we have to do everything after it before we're ever going to call dot backward on any one node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4683" target="_blank">01:18:03.840</a></span> | <span class="t">we have to get all of its full dependencies everything that it depends on has to propagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4688" target="_blank">01:18:08.960</a></span> | <span class="t">to it before we can continue back propagation so this ordering of graphs can be achieved using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4695" target="_blank">01:18:15.680</a></span> | <span class="t">something called topological sort so topological sort is basically a laying out of a graph such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4703" target="_blank">01:18:23.280</a></span> | <span class="t">that all the edges go only from left to right basically so here we have a graph it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4708" target="_blank">01:18:28.720</a></span> | <span class="t">direct acyclic graph a dag and this is two different topological orders of it i believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4715" target="_blank">01:18:35.520</a></span> | <span class="t">where basically you'll see that it's a laying out of the nodes such that all the edges go only one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4719" target="_blank">01:18:39.520</a></span> | <span class="t">way from left to right and implementing topological sort you can look in wikipedia and so on i'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4726" target="_blank">01:18:46.240</a></span> | <span class="t">going to go through it in detail but basically this is what builds a topological graph we maintain a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4734" target="_blank">01:18:54.960</a></span> | <span class="t">set of visited nodes and then we are going through starting at some root node which for us is o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4743" target="_blank">01:19:03.040</a></span> | <span class="t">that's where we want to start the topological sort and starting at o we go through all of its children</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4748" target="_blank">01:19:08.720</a></span> | <span class="t">and we need to lay them out from left to right and basically this starts at o if it's not visited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4756" target="_blank">01:19:16.080</a></span> | <span class="t">then it marks it as visited and then it iterates through all of its children and calls build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4762" target="_blank">01:19:22.080</a></span> | <span class="t">topological on them and then after it's gone through all the children it adds itself so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4769" target="_blank">01:19:29.040</a></span> | <span class="t">this node that we're going to call it on like say o is only going to add itself to the topo list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4776" target="_blank">01:19:36.480</a></span> | <span class="t">after all of the children have been processed and that's how this function is guaranteeing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4782" target="_blank">01:19:42.160</a></span> | <span class="t">you're only going to be in the list once all your children are in the list and that's the invariant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4786" target="_blank">01:19:46.480</a></span> | <span class="t">that is being maintained so if we built up on o and then inspect this list we're going to see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4793" target="_blank">01:19:53.120</a></span> | <span class="t">it ordered our value objects and the last one is the value of 0.707 which is the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4801" target="_blank">01:20:01.760</a></span> | <span class="t">so this is o and then this is n and then all the other nodes get laid out before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4808" target="_blank">01:20:08.000</a></span> | <span class="t">so that builds the topological graph and really what we're doing now is we're just calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4814" target="_blank">01:20:14.720</a></span> | <span class="t">dot underscore backward on all of the nodes in a topological order so if we just reset the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4821" target="_blank">01:20:21.360</a></span> | <span class="t">gradients they're all zero what did we do we started by setting o dot grad to be one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4830" target="_blank">01:20:30.400</a></span> | <span class="t">that's the base case then we built a topological order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4835" target="_blank">01:20:35.920</a></span> | <span class="t">and then we went for node in reversed of topo now in in the reverse order because this list goes from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4850" target="_blank">01:20:50.800</a></span> | <span class="t">you know we need to go through it in reversed order so starting at o node backward and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4859" target="_blank">01:20:59.040</a></span> | <span class="t">should be it there we go those are the correct derivatives finally we are going to hide this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4868" target="_blank">01:21:08.880</a></span> | <span class="t">functionality so i'm going to copy this and we're going to hide it inside the value class because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4875" target="_blank">01:21:15.200</a></span> | <span class="t">we don't want to have all that code lying around so instead of an underscore backward we're now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4880" target="_blank">01:21:20.160</a></span> | <span class="t">going to define an actual backward so that backward without the underscore and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4886" target="_blank">01:21:26.640</a></span> | <span class="t">going to do all the stuff that we just derived so let me just clean this up a little bit so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4891" target="_blank">01:21:31.200</a></span> | <span class="t">we're first going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4893" target="_blank">01:21:33.360</a></span> | <span class="t">build a topological graph starting at self so build topo of self will populate the topological</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4906" target="_blank">01:21:46.000</a></span> | <span class="t">order into the topo list which is a local variable then we set self dot grads to be one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4911" target="_blank">01:21:51.440</a></span> | <span class="t">and then for each node in the reversed list so starting at us and going to all the children</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4918" target="_blank">01:21:58.160</a></span> | <span class="t">underscore backward and that should be it so save come down here we define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4931" target="_blank">01:22:11.120</a></span> | <span class="t">okay all the grads are zero and now what we can do is o dot backward without the underscore and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4937" target="_blank">01:22:17.360</a></span> | <span class="t">there we go and that's uh that's back propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4944" target="_blank">01:22:24.960</a></span> | <span class="t">place for one neuron now we shouldn't be too happy with ourselves actually because we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4952" target="_blank">01:22:32.000</a></span> | <span class="t">bad bug and we have not surfaced the bug because of some specific conditions that we are we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4957" target="_blank">01:22:37.840</a></span> | <span class="t">to think about right now so here's the simplest case that shows the bug say i create a single node a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4965" target="_blank">01:22:45.760</a></span> | <span class="t">and then i create a b that is a plus a and then i call backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4972" target="_blank">01:22:52.480</a></span> | <span class="t">so what's going to happen is a is three and then a b is a plus a so there's two arrows on top of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4980" target="_blank">01:23:00.960</a></span> | <span class="t">each other here then we can see that b is of course the forward pass works b is just a plus a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4988" target="_blank">01:23:08.640</a></span> | <span class="t">which is six but the gradient here is not actually correct that we calculate it automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4994" target="_blank">01:23:14.240</a></span> | <span class="t">and that's because um of course uh just doing calculus in your head the derivative of b with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5003" target="_blank">01:23:23.200</a></span> | <span class="t">respect to a should be uh two one plus one it's not one intuitively what's happening here right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5012" target="_blank">01:23:32.240</a></span> | <span class="t">so b is the result of a plus a and then we call backward on it so let's go up and see what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5018" target="_blank">01:23:38.800</a></span> | <span class="t">does um b is the result of addition so out as b and then when we call backward what happened is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5030" target="_blank">01:23:50.880</a></span> | <span class="t">self dot grad was set to one and then other that grad was set to one but because we're doing a plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5039" target="_blank">01:23:59.120</a></span> | <span class="t">a self and other are actually the exact same object so we are overriding the gradient we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5046" target="_blank">01:24:06.000</a></span> | <span class="t">setting it to one and then we are setting it again to one and that's why it stays at one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5052" target="_blank">01:24:12.160</a></span> | <span class="t">so that's a problem there's another way to see this in a little bit more complicated expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5058" target="_blank">01:24:18.320</a></span> | <span class="t">so here we have a and b and then d will be the multiplication of the two and e will be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5070" target="_blank">01:24:30.320</a></span> | <span class="t">addition of the two and then we multiply e times d to get f and then we call f dot backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5076" target="_blank">01:24:36.560</a></span> | <span class="t">and these gradients if you check will be incorrect so fundamentally what's happening here again is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5085" target="_blank">01:24:45.040</a></span> | <span class="t">basically we're going to see an issue any time we use a variable more than once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5088" target="_blank">01:24:48.560</a></span> | <span class="t">until now in these expressions above every variable is used exactly once so we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5093" target="_blank">01:24:53.520</a></span> | <span class="t">see the issue but here if a variable is used more than once what's going to happen during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5097" target="_blank">01:24:57.920</a></span> | <span class="t">backward pass we're back propagating from f to e to d so far so good but now e calls a backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5104" target="_blank">01:25:04.720</a></span> | <span class="t">and it deposits its gradients to a and b but then we come back to d and call backward and it overwrites</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5111" target="_blank">01:25:11.520</a></span> | <span class="t">those gradients at a and b so that's obviously a problem and the solution here if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5119" target="_blank">01:25:19.280</a></span> | <span class="t">the multivariate case of the chain rule and its generalization there the solution there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5125" target="_blank">01:25:25.040</a></span> | <span class="t">basically that we have to accumulate these gradients these gradients add and so instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5131" target="_blank">01:25:31.600</a></span> | <span class="t">setting those gradients we can simply do plus equals we need to accumulate those gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5139" target="_blank">01:25:39.120</a></span> | <span class="t">plus equals plus equals plus equals plus equals and this will be okay remember because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5148" target="_blank">01:25:48.400</a></span> | <span class="t">initializing them at zero so they start at zero and then any contribution that flows backwards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5155" target="_blank">01:25:55.920</a></span> | <span class="t">will simply add so now if we redefine this one because the plus equals this now works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5166" target="_blank">01:26:06.000</a></span> | <span class="t">because a dot grad started at zero and we called b dot backward we deposit one and then we deposit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5171" target="_blank">01:26:11.840</a></span> | <span class="t">one again and now this is two which is correct and here this will also work and we'll get correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5177" target="_blank">01:26:17.440</a></span> | <span class="t">gradients because when we call e dot backward we will deposit the gradients from this branch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5182" target="_blank">01:26:22.320</a></span> | <span class="t">and then we get to back to d dot backward it will deposit its own gradients and then those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5187" target="_blank">01:26:27.440</a></span> | <span class="t">gradients simply add on top of each other and so we just accumulate those gradients and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5192" target="_blank">01:26:32.080</a></span> | <span class="t">fixes the issue okay now before we move on let me actually do a bit of cleanup here and delete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5197" target="_blank">01:26:37.680</a></span> | <span class="t">some of these some of this intermediate work so i'm not going to need any of this now that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5202" target="_blank">01:26:42.880</a></span> | <span class="t">we've derived all of it um we are going to keep this because i want to come back to it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5208" target="_blank">01:26:48.720</a></span> | <span class="t">delete the 10h delete our modeling example delete the step delete this keep the code that draws</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5219" target="_blank">01:26:59.680</a></span> | <span class="t">and then delete this example and leave behind only the definition of value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5224" target="_blank">01:27:04.160</a></span> | <span class="t">and now let's come back to this non-linearity here that we implemented the 10h now i told you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5229" target="_blank">01:27:09.920</a></span> | <span class="t">that we could have broken down 10h into its explicit atoms in terms of other expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5235" target="_blank">01:27:15.840</a></span> | <span class="t">if we had the exp function so if you remember 10h is defined like this and we chose to develop 10h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5241" target="_blank">01:27:21.760</a></span> | <span class="t">as a single function and we can do that because we know it's derivative and we can back propagate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5246" target="_blank">01:27:26.000</a></span> | <span class="t">through it but we can also break down 10h into and express it as a function of exp and i would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5251" target="_blank">01:27:31.600</a></span> | <span class="t">like to do that now because i want to prove to you that you get all the same results and all the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5255" target="_blank">01:27:35.280</a></span> | <span class="t">gradients um but also because it forces us to implement a few more expressions it forces us to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5260" target="_blank">01:27:40.880</a></span> | <span class="t">do exponentiation addition subtraction division and things like that and i think it's a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5265" target="_blank">01:27:45.760</a></span> | <span class="t">exercise to go through a few more of these okay so let's scroll up to the definition of value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5272" target="_blank">01:27:52.080</a></span> | <span class="t">and here one thing that we currently can't do is we can't do like a value of say 2.0 but we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5279" target="_blank">01:27:59.040</a></span> | <span class="t">do you know here for example we want to add a constant one and we can't do something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5283" target="_blank">01:28:03.920</a></span> | <span class="t">and we can't do it because it says int object has no attribute data that's because a plus one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5290" target="_blank">01:28:10.160</a></span> | <span class="t">comes right here to add and then other is the integer one and then here python is trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5296" target="_blank">01:28:16.160</a></span> | <span class="t">access one dot data and that's not a thing and that's because basically one is not a value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5301" target="_blank">01:28:21.280</a></span> | <span class="t">and we only have addition for value objects so as a matter of convenience so that we can create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5306" target="_blank">01:28:26.800</a></span> | <span class="t">expressions like this and make them make sense we can simply do something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5310" target="_blank">01:28:30.640</a></span> | <span class="t">basically we let other alone if other is an instance of value but if it's not an instance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5318" target="_blank">01:28:38.000</a></span> | <span class="t">of value we're going to assume that it's a number like an integer or float and we're going to simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5321" target="_blank">01:28:41.840</a></span> | <span class="t">wrap it in in value and then other will just become value of other and then other will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5326" target="_blank">01:28:46.880</a></span> | <span class="t">a data attribute and this should work so if i just say this redefine value then this should work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5332" target="_blank">01:28:52.560</a></span> | <span class="t">there we go okay and now let's do the exact same thing for multiply because we can't do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5337" target="_blank">01:28:57.680</a></span> | <span class="t">like this again for the exact same reason so we just have to go to mul and if other is not a value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5345" target="_blank">01:29:05.600</a></span> | <span class="t">then let's wrap it in value let's redefine value and now this works now here's a kind of unfortunate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5352" target="_blank">01:29:12.080</a></span> | <span class="t">and not obvious part a times two works we saw that but two times a is that going to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5358" target="_blank">01:29:18.320</a></span> | <span class="t">you'd expect it to right but actually it will not and the reason it won't is because python doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5365" target="_blank">01:29:25.120</a></span> | <span class="t">know like when when you do a times two basically um so a times two python will go and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5372" target="_blank">01:29:32.000</a></span> | <span class="t">basically do something like a dot mul of two that's basically what it will call but to it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5378" target="_blank">01:29:38.000</a></span> | <span class="t">two times a is the same as two dot mul of a and it doesn't two can't multiply value and so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5385" target="_blank">01:29:45.840</a></span> | <span class="t">really confused about that so instead what happens is in python the way this works is you are free to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5390" target="_blank">01:29:50.800</a></span> | <span class="t">define something called the armol and armol is kind of like a fallback so if a python can't do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5398" target="_blank">01:29:58.800</a></span> | <span class="t">two times a it will check if um if by any chance a knows how to multiply two and that will be called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5406" target="_blank">01:30:06.720</a></span> | <span class="t">into armol so because python can't do two times a it will check is there an armol in value and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5413" target="_blank">01:30:13.680</a></span> | <span class="t">because there is it will now call that and what we'll do here is we will swap the order of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5419" target="_blank">01:30:19.520</a></span> | <span class="t">operands so basically two times a will redirect to armol and armol will basically call a times two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5426" target="_blank">01:30:26.000</a></span> | <span class="t">and that's how that will work so redefining that with armol two times a becomes four okay now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5432" target="_blank">01:30:32.880</a></span> | <span class="t">looking at the other elements that we still need we need to know how to exponentiate and how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5436" target="_blank">01:30:36.320</a></span> | <span class="t">divide so let's first do the explanation to the exponentiation part we're going to introduce a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5441" target="_blank">01:30:41.920</a></span> | <span class="t">single function exp here and exp is going to mirror tanh in the sense that it's a simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5448" target="_blank">01:30:48.880</a></span> | <span class="t">single function that transforms a single scalar value and outputs a single scalar value so we pop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5454" target="_blank">01:30:54.000</a></span> | <span class="t">out the python number we use math.exp to exponentiate it create a new value object everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5459" target="_blank">01:30:59.520</a></span> | <span class="t">that we've seen before the tricky part of course is how do you back propagate through e to the x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5464" target="_blank">01:31:04.800</a></span> | <span class="t">and so here you can potentially pause the video and think about what should go here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5470" target="_blank">01:31:10.000</a></span> | <span class="t">okay so basically we need to know what is the local derivative of e to the x so d by dx of e</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5479" target="_blank">01:31:19.520</a></span> | <span class="t">to the x is famously just e to the x and we've already just calculated e to the x and it's inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5485" target="_blank">01:31:25.520</a></span> | <span class="t">out that data so we can do out that data times and out that grad that's the chain rule so we're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5492" target="_blank">01:31:32.640</a></span> | <span class="t">chaining on to the current running grad and this is what the expression looks like it looks a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5497" target="_blank">01:31:37.680</a></span> | <span class="t">confusing but this is what it is and that's the exponentiation so redefining we should now be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5503" target="_blank">01:31:43.600</a></span> | <span class="t">to call a.exp and hopefully the backward pass works as well okay and the last thing we'd like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5509" target="_blank">01:31:49.600</a></span> | <span class="t">to do of course is we'd like to be able to divide now i actually will implement something slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5514" target="_blank">01:31:54.880</a></span> | <span class="t">more powerful than division because division is just a special case of something a bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5518" target="_blank">01:31:58.960</a></span> | <span class="t">powerful so in particular just by rearranging if we have some kind of a b equals value of 4.0 here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5526" target="_blank">01:32:06.880</a></span> | <span class="t">we'd like to basically be able to do a divide b and we'd like this to be able to give us 0.5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5530" target="_blank">01:32:10.720</a></span> | <span class="t">now division actually can be reshuffled as follows if we have a divide b that's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5537" target="_blank">01:32:17.280</a></span> | <span class="t">the same as a multiplying 1 over b and that's the same as a multiplying b to the power of negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5543" target="_blank">01:32:23.040</a></span> | <span class="t">1 and so what i'd like to do instead is i basically like to implement the operation of x to the k for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5549" target="_blank">01:32:29.440</a></span> | <span class="t">some constant k so it's an integer or a float and we would like to be able to differentiate this and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5556" target="_blank">01:32:36.160</a></span> | <span class="t">then as a special case negative 1 will be division and so i'm doing that just because it's more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5562" target="_blank">01:32:42.960</a></span> | <span class="t">general and yeah you might as well do it that way so basically what i'm saying is we can redefine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5569" target="_blank">01:32:49.760</a></span> | <span class="t">division which we will put here somewhere yeah we can put it here somewhere what i'm saying is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5576" target="_blank">01:32:56.800</a></span> | <span class="t">we can redefine division so self divide other this can actually be rewritten as self times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5583" target="_blank">01:33:03.040</a></span> | <span class="t">other to the power of negative one and now value raised to the power of negative one we have now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5590" target="_blank">01:33:10.000</a></span> | <span class="t">defined that so here's so we need to implement the pow function where am i going to put the pow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5597" target="_blank">01:33:17.280</a></span> | <span class="t">function maybe here somewhere there's this coliform for it so this function will be called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5603" target="_blank">01:33:23.840</a></span> | <span class="t">when we try to raise a value to some power and other will be that power now i'd like to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5609" target="_blank">01:33:29.520</a></span> | <span class="t">sure that other is only an int or a float usually other is some kind of a different value object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5615" target="_blank">01:33:35.360</a></span> | <span class="t">but here other will be forced to be an int or a float otherwise the math won't work for for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5622" target="_blank">01:33:42.320</a></span> | <span class="t">what we're trying to achieve in the specific case that would be a different derivative expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5626" target="_blank">01:33:46.480</a></span> | <span class="t">if we wanted other to be a value so here we create the up the value which is just uh you know this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5632" target="_blank">01:33:52.880</a></span> | <span class="t">data raised to the power of other and other here could be for example negative one that's what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5637" target="_blank">01:33:57.280</a></span> | <span class="t">are hoping to achieve and then uh this is the backward stub and this is the fun part which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5643" target="_blank">01:34:03.600</a></span> | <span class="t">what is the uh chain rule expression here for back for um back propagating through the power function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5652" target="_blank">01:34:12.320</a></span> | <span class="t">where the power is to the power of some kind of a constant so this is the exercise and maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5657" target="_blank">01:34:17.040</a></span> | <span class="t">pause the video here and see if you can figure it out yourself as to what we should put here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5660" target="_blank">01:34:20.720</a></span> | <span class="t">okay so um you can actually go here and look at derivative rules as an example and we see lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5673" target="_blank">01:34:33.360</a></span> | <span class="t">derivative rules that you can hopefully know from calculus in particular what we're looking for is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5677" target="_blank">01:34:37.600</a></span> | <span class="t">the power rule because that's telling us that if we're trying to take d by dx of x to the n which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5682" target="_blank">01:34:42.880</a></span> | <span class="t">is what we're doing here then that is just n times x to the n minus one right okay so that's telling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5692" target="_blank">01:34:52.160</a></span> | <span class="t">us about the local derivative of this power operation so all we want here basically n is now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5699" target="_blank">01:34:59.760</a></span> | <span class="t">other and self.data is x and so this now becomes other which is n times self.data which is now a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5710" target="_blank">01:35:10.800</a></span> | <span class="t">python int or a float it's not a value object we're accessing the data attribute raised to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5717" target="_blank">01:35:17.600</a></span> | <span class="t">power of other minus one or n minus one i can put brackets around this but this doesn't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5723" target="_blank">01:35:23.440</a></span> | <span class="t">because power takes precedence over multiply and python so that would have been okay and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5729" target="_blank">01:35:29.840</a></span> | <span class="t">the local derivative only but now we have to chain it and we change it just simply by multiplying by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5734" target="_blank">01:35:34.560</a></span> | <span class="t">our top grand that's chain rule and this should technically work and we're going to find out soon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5742" target="_blank">01:35:42.240</a></span> | <span class="t">but now if we do this this should now work and we get 0.5 so the forward pass works but does the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5749" target="_blank">01:35:49.760</a></span> | <span class="t">backward pass work and i realized that we actually also have to know how to subtract so right now a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5755" target="_blank">01:35:55.520</a></span> | <span class="t">minus b will not work to make it work we need one more piece of code here and basically this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5764" target="_blank">01:36:04.080</a></span> | <span class="t">subtraction and the way we're going to implement subtraction is we're going to implement it by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5769" target="_blank">01:36:09.440</a></span> | <span class="t">addition of a negation and then to implement negation we're going to multiply by negative one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5773" target="_blank">01:36:13.760</a></span> | <span class="t">so just again using the stuff we've already built and just expressing it in terms of what we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5778" target="_blank">01:36:18.960</a></span> | <span class="t">and a minus b is not working okay so now let's scroll again to this expression here for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5783" target="_blank">01:36:23.840</a></span> | <span class="t">neuron and let's just compute the backward pass here once we've defined o and let's draw it so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5792" target="_blank">01:36:32.160</a></span> | <span class="t">here's the gradients for all these leaf nodes for this two-dimensional neuron that has a 10h that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5796" target="_blank">01:36:36.880</a></span> | <span class="t">we've seen before so now what i'd like to do is i'd like to break up this 10h into this expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5803" target="_blank">01:36:43.520</a></span> | <span class="t">here so let me copy paste this here and now instead of we'll preserve the label and we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5811" target="_blank">01:36:51.280</a></span> | <span class="t">change how we define o so in particular we're going to implement this formula here so we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5817" target="_blank">01:36:57.200</a></span> | <span class="t">e to the 2x minus 1 over e to the x plus 1 so e to the 2x we need to take 2 times n and we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5824" target="_blank">01:37:04.960</a></span> | <span class="t">exponentiate it that's e to the 2x and then because we're using it twice let's create an intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5830" target="_blank">01:37:10.800</a></span> | <span class="t">variable e and then define o as e plus 1 over e minus 1 over e plus 1 e minus 1 over e plus 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5841" target="_blank">01:37:21.840</a></span> | <span class="t">and that should be it and then we should be able to draw dot of o so now before i run this what do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5849" target="_blank">01:37:29.120</a></span> | <span class="t">we expect to see number one we're expecting to see a much longer graph here because we've broken up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5855" target="_blank">01:37:35.280</a></span> | <span class="t">10h into a bunch of other operations but those operations are mathematically equivalent and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5860" target="_blank">01:37:40.480</a></span> | <span class="t">what we're expecting to see is number one the same result here so the forward pass works and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5866" target="_blank">01:37:46.000</a></span> | <span class="t">number two because of that mathematical equivalence we expect to see the same backward pass and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5870" target="_blank">01:37:50.800</a></span> | <span class="t">same gradients on these leaf nodes so these gradients should be identical so let's run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5876" target="_blank">01:37:56.400</a></span> | <span class="t">so number one let's verify that instead of a single 10h node we have now exp and we have plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5884" target="_blank">01:38:04.880</a></span> | <span class="t">we have times negative one this is the division and we end up with the same forward pass here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5891" target="_blank">01:38:11.120</a></span> | <span class="t">and then the gradients we have to be careful because they're in slightly different order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5894" target="_blank">01:38:14.800</a></span> | <span class="t">potentially the gradients for w2 x2 should be 0 and 0.5 w2 and x2 are 0 and 0.5 and w1 x1 are 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5903" target="_blank">01:38:23.440</a></span> | <span class="t">and negative 1.5 1 and negative 1.5 so that means that both our forward passes and backward passes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5910" target="_blank">01:38:30.160</a></span> | <span class="t">were correct because this turned out to be equivalent to 10h before and so the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5916" target="_blank">01:38:36.640</a></span> | <span class="t">I wanted to go through this exercise is number one we got to practice a few more operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5920" target="_blank">01:38:40.960</a></span> | <span class="t">and writing more backwards passes and number two I wanted to illustrate the point that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5926" target="_blank">01:38:46.320</a></span> | <span class="t">level at which you implement your operations is totally up to you you can implement backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5932" target="_blank">01:38:52.400</a></span> | <span class="t">passes for tiny expressions like a single individual plus or a single times or you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5937" target="_blank">01:38:57.040</a></span> | <span class="t">implement them for say 10h which is a kind of a potentially you can see it as a composite operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5942" target="_blank">01:39:02.960</a></span> | <span class="t">because it's made up of all these more atomic operations but really all of this is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5947" target="_blank">01:39:07.280</a></span> | <span class="t">like a fake concept all that matters is we have some kind of inputs and some kind of an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5951" target="_blank">01:39:11.360</a></span> | <span class="t">and this output is a function of the inputs in some way and as long as you can do forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5955" target="_blank">01:39:15.360</a></span> | <span class="t">and the backward pass of that little operation it doesn't matter what that operation is and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5961" target="_blank">01:39:21.440</a></span> | <span class="t">composite it is if you can write the local gradients you can chain the gradient and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5965" target="_blank">01:39:25.840</a></span> | <span class="t">can continue back propagation so the design of what those functions are is completely up to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5970" target="_blank">01:39:30.800</a></span> | <span class="t">so now I would like to show you how you can do the exact same thing but using a modern deep neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5976" target="_blank">01:39:36.000</a></span> | <span class="t">network library like for example PyTorch which I've roughly modeled micro grad by and so PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5984" target="_blank">01:39:44.080</a></span> | <span class="t">is something you would use in production and I'll show you how you can do the exact same thing but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5988" target="_blank">01:39:48.080</a></span> | <span class="t">in PyTorch API so I'm just going to copy paste it in and walk you through it a little bit this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=5992" target="_blank">01:39:52.800</a></span> | <span class="t">what it looks like so we're going to import PyTorch and then we need to define these value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6000" target="_blank">01:40:00.000</a></span> | <span class="t">objects like we have here now micro grad is a scalar valued engine so we only have scalar values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6007" target="_blank">01:40:07.440</a></span> | <span class="t">like 2.0 but in PyTorch everything is based around tensors and like I mentioned tensors are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6013" target="_blank">01:40:13.120</a></span> | <span class="t">n-dimensional arrays of scalars so that's why things get a little bit more complicated here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6018" target="_blank">01:40:18.640</a></span> | <span class="t">I just need a scalar valued tensor a tensor with just a single element but by default when you work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6024" target="_blank">01:40:24.720</a></span> | <span class="t">with PyTorch you would use more complicated tensors like this so if I import PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6031" target="_blank">01:40:31.600</a></span> | <span class="t">then I can create tensors like this and this tensor for example is a two by three array of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6040" target="_blank">01:40:40.080</a></span> | <span class="t">scalars in a single compact representation so you can check its shape we see that it's a two by three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6047" target="_blank">01:40:47.120</a></span> | <span class="t">array and so on so this is usually what you would work with in the actual libraries so here I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6054" target="_blank">01:40:54.160</a></span> | <span class="t">creating a tensor that has only a single element 2.0 and then I'm casting it to be double because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6063" target="_blank">01:41:03.920</a></span> | <span class="t">Python is by default using double precision for its floating point numbers so I'd like everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6068" target="_blank">01:41:08.560</a></span> | <span class="t">to be identical by default the data type of these tensors will be float 32 so it's only using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6075" target="_blank">01:41:15.280</a></span> | <span class="t">single precision float so I'm casting it to double so that we have float 64 just like in Python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6081" target="_blank">01:41:21.600</a></span> | <span class="t">so I'm casting to double and then we get something similar to value of 2.0 the next thing I have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6088" target="_blank">01:41:28.640</a></span> | <span class="t">do is because these are leaf nodes by default PyTorch assumes that they do not require gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6093" target="_blank">01:41:33.760</a></span> | <span class="t">so I need to explicitly say that all of these nodes require gradients okay so this is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6098" target="_blank">01:41:38.960</a></span> | <span class="t">to construct scalar valued one element tensors make sure that PyTorch knows that they require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6104" target="_blank">01:41:44.880</a></span> | <span class="t">gradients now by default these are set to false by the way because of efficiency reasons because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6110" target="_blank">01:41:50.240</a></span> | <span class="t">usually you would not want gradients for leaf nodes like the inputs to the network and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6115" target="_blank">01:41:55.760</a></span> | <span class="t">is just trying to be efficient in the most common cases so once we've defined all of our values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6121" target="_blank">01:42:01.440</a></span> | <span class="t">in PyTorch land we can perform arithmetic just like we can here in micrograd land so this would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6126" target="_blank">01:42:06.240</a></span> | <span class="t">just work and then there's a torch.tanh also and when we get back as a tensor again and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6133" target="_blank">01:42:13.520</a></span> | <span class="t">just like in micrograd it's got a data attribute and it's got grad attributes so these tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6139" target="_blank">01:42:19.200</a></span> | <span class="t">objects just like in micrograd have a dot data and a dot grad and the only difference here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6144" target="_blank">01:42:24.800</a></span> | <span class="t">we need to call a dot item because otherwise PyTorch dot item basically takes a single tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6153" target="_blank">01:42:33.120</a></span> | <span class="t">of one element and it just returns that element stripping out the tensor so let me just run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6158" target="_blank">01:42:38.800</a></span> | <span class="t">and hopefully we are going to get this is going to print the forward pass which is 0.707 and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6165" target="_blank">01:42:45.360</a></span> | <span class="t">will be the gradients which hopefully are 0.50 negative 1.5 and 1 so if we just run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6172" target="_blank">01:42:52.240</a></span> | <span class="t">there we go 0.7 so the forward pass agrees and then 0.50 negative 1.5 and 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6179" target="_blank">01:42:59.680</a></span> | <span class="t">so PyTorch agrees with us and just to show you here basically oh here's a tensor with a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6186" target="_blank">01:43:06.400</a></span> | <span class="t">element and it's a double and we can call dot item on it to just get the single number out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6193" target="_blank">01:43:13.440</a></span> | <span class="t">so that's what item does and o is a tensor object like i mentioned and it's got a backward function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6199" target="_blank">01:43:19.920</a></span> | <span class="t">just like we've implemented and then all of these also have a dot grad so like x2 for example has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6205" target="_blank">01:43:25.440</a></span> | <span class="t">a grad and it's a tensor and we can pop out the individual number with dot item so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6212" target="_blank">01:43:32.880</a></span> | <span class="t">Torches Torch can do what we did in micro grad as a special case when your tensors are all single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6218" target="_blank">01:43:38.960</a></span> | <span class="t">element tensors but the big deal with PyTorch is that everything is significantly more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6224" target="_blank">01:43:44.080</a></span> | <span class="t">because we are working with these tensor objects and we can do lots of operations in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6228" target="_blank">01:43:48.800</a></span> | <span class="t">on all of these tensors but otherwise what we've built very much agrees with the API of PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6235" target="_blank">01:43:55.040</a></span> | <span class="t">okay so now that we have some machinery to build out pretty complicated mathematical expressions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6239" target="_blank">01:43:59.760</a></span> | <span class="t">we can also start building up neural nets and as I mentioned neural nets are just a specific class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6244" target="_blank">01:44:04.480</a></span> | <span class="t">of mathematical expressions so we're going to start building out a neural net piece by piece</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6249" target="_blank">01:44:09.280</a></span> | <span class="t">and eventually we'll build out a two-layer multi-layer layer perceptron as it's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6253" target="_blank">01:44:13.920</a></span> | <span class="t">and I'll show you exactly what that means let's start with a single individual neuron we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6258" target="_blank">01:44:18.080</a></span> | <span class="t">implemented one here but here I'm going to implement one that also subscribes to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6262" target="_blank">01:44:22.720</a></span> | <span class="t">PyTorch API in how it designs its neural network modules so just like we saw that we can like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6268" target="_blank">01:44:28.800</a></span> | <span class="t">match the API of PyTorch on the autograd side we're going to try to do that on the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6274" target="_blank">01:44:34.400</a></span> | <span class="t">network modules so here's class neuron and just for the sake of efficiency I'm going to copy paste</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6281" target="_blank">01:44:41.760</a></span> | <span class="t">some sections that are relatively straightforward so the constructor will take number of inputs to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6288" target="_blank">01:44:48.640</a></span> | <span class="t">this neuron which is how many inputs come to a neuron so this one for example has three inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6295" target="_blank">01:44:55.120</a></span> | <span class="t">and then it's going to create a weight that is some random number between negative one and one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6299" target="_blank">01:44:59.440</a></span> | <span class="t">for every one of those inputs and a bias that controls the overall trigger happiness of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6304" target="_blank">01:45:04.640</a></span> | <span class="t">neuron and then we're going to implement a def underscore underscore call of self and x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6312" target="_blank">01:45:12.720</a></span> | <span class="t">sum input x and really what we don't want to do here is w times x plus b where w times x here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6318" target="_blank">01:45:18.800</a></span> | <span class="t">a dot product specifically now if you haven't seen call let me just return 0.0 here for now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6326" target="_blank">01:45:26.000</a></span> | <span class="t">the way this works now is we can have an x which is say like 2.0 3.0 then we can initialize a neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6332" target="_blank">01:45:32.160</a></span> | <span class="t">that is two-dimensional because these are two numbers and then we can feed those two numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6336" target="_blank">01:45:36.720</a></span> | <span class="t">into that neuron to get an output and so when you use this notation n of x python will use call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6344" target="_blank">01:45:44.960</a></span> | <span class="t">so currently call just returns 0.0 now we'd like to actually do the forward pass of this neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6352" target="_blank">01:45:52.960</a></span> | <span class="t">instead so we're going to do here first is we need to basically multiply all of the elements of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6359" target="_blank">01:45:59.920</a></span> | <span class="t">w with all of the elements of x pairwise we need to multiply them so the first thing we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6365" target="_blank">01:46:05.200</a></span> | <span class="t">to do is we're going to zip up salta w and x and in python zip takes two iterators and it creates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6373" target="_blank">01:46:13.280</a></span> | <span class="t">a new iterator that iterates over the tuples of their corresponding entries so for example just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6378" target="_blank">01:46:18.960</a></span> | <span class="t">to show you we can print this list and still return 0.0 here sorry i'm in my so we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6394" target="_blank">01:46:34.880</a></span> | <span class="t">these w's are paired up with the x's w with x and now what we want to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6402" target="_blank">01:46:42.640</a></span> | <span class="t">for wi xi in we want to multiply w times wi times xi and then we want to sum all of that together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6416" target="_blank">01:46:56.960</a></span> | <span class="t">to come up with an activation and add also salta b on top so that's the raw activation and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6424" target="_blank">01:47:04.160</a></span> | <span class="t">of course we need to pass that through a non-linearity so what we're going to be returning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6427" target="_blank">01:47:07.760</a></span> | <span class="t">is act dot 10h and here's out so now we see that we are getting some outputs and we get a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6436" target="_blank">01:47:16.080</a></span> | <span class="t">output from a neuron each time because we are initializing different weights and biases and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6441" target="_blank">01:47:21.280</a></span> | <span class="t">then to be a bit more efficient here actually sum by the way takes a second optional parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6447" target="_blank">01:47:27.200</a></span> | <span class="t">which is the start and by default the start is zero so these elements of this sum will be added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6454" target="_blank">01:47:34.080</a></span> | <span class="t">on top of zero to begin with but actually we can just start with cell.b and then we just have an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6459" target="_blank">01:47:39.280</a></span> | <span class="t">expression like this and then the generator expression here must be parenthesized in python</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6469" target="_blank">01:47:49.360</a></span> | <span class="t">there we go yep so now we can forward a single neuron next up we're going to define a layer of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6478" target="_blank">01:47:58.400</a></span> | <span class="t">neurons so here we have a schematic for a MLP so we see that these MLPs each layer this is one layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6486" target="_blank">01:48:06.320</a></span> | <span class="t">has actually a number of neurons and they're not connected to each other but all of them are fully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6489" target="_blank">01:48:09.760</a></span> | <span class="t">connected to the input so what is a layer of neurons it's just it's just a set of neurons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6494" target="_blank">01:48:14.560</a></span> | <span class="t">evaluated independently so in the interest of time i'm going to do something fairly straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6501" target="_blank">01:48:21.360</a></span> | <span class="t">here it's literally a layer is just a list of neurons and then how many neurons do we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6510" target="_blank">01:48:30.640</a></span> | <span class="t">we take that as an input argument here how many neurons do you want in your layer number of outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6515" target="_blank">01:48:35.120</a></span> | <span class="t">in this layer and so we just initialize completely independent neurons with this given dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6521" target="_blank">01:48:41.280</a></span> | <span class="t">and when we call on it we just independently evaluate them so now instead of a neuron we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6528" target="_blank">01:48:48.000</a></span> | <span class="t">make a layer of neurons they are two-dimensional neurons and let's have three of them and now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6532" target="_blank">01:48:52.720</a></span> | <span class="t">see that we have three independent evaluations of three different neurons right okay and finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6539" target="_blank">01:48:59.680</a></span> | <span class="t">let's complete this picture and define an entire multi-layer perceptron or MLP and as we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6545" target="_blank">01:49:05.200</a></span> | <span class="t">here in an MLP these layers just feed into each other sequentially so let's come here and i'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6550" target="_blank">01:49:10.480</a></span> | <span class="t">just going to copy the code here in interest of time so an MLP is very similar we're taking the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6557" target="_blank">01:49:17.200</a></span> | <span class="t">number of inputs as before but now instead of saying taking a single n out which is number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6562" target="_blank">01:49:22.240</a></span> | <span class="t">neurons in a single layer we're going to take a list of n outs and this list defines the sizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6567" target="_blank">01:49:27.440</a></span> | <span class="t">of all the layers that we want in our MLP so here we just put them all together and then iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6572" target="_blank">01:49:32.720</a></span> | <span class="t">over consecutive pairs of these sizes and create layer objects for them and then in the call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6578" target="_blank">01:49:38.560</a></span> | <span class="t">function we are just calling them sequentially so that's an MLP really and let's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6583" target="_blank">01:49:43.520</a></span> | <span class="t">re-implement this picture so we want three input neurons and then two layers of four and an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6588" target="_blank">01:49:48.320</a></span> | <span class="t">unit so we want a three-dimensional input say this is an example input we want three inputs into two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6597" target="_blank">01:49:57.680</a></span> | <span class="t">layers of four and one output and this of course is an MLP and there we go that's a forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6605" target="_blank">01:50:05.360</a></span> | <span class="t">of an MLP to make this a little bit nicer you see how we have just a single element but it's wrapped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6610" target="_blank">01:50:10.320</a></span> | <span class="t">in a list because layer always returns lists so for convenience return outs at zero if len outs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6618" target="_blank">01:50:18.320</a></span> | <span class="t">is exactly a single element else return fullest and this will allow us to just get a single value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6624" target="_blank">01:50:24.560</a></span> | <span class="t">out at the last layer that only has a single neuron and finally we should be able to draw dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6630" target="_blank">01:50:30.000</a></span> | <span class="t">of n of x and as you might imagine these expressions are now getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6635" target="_blank">01:50:35.440</a></span> | <span class="t">relatively involved so this is an entire MLP that we're defining now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6640" target="_blank">01:50:40.880</a></span> | <span class="t">all the way until a single output okay and so obviously you would never differentiate on pen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6651" target="_blank">01:50:51.920</a></span> | <span class="t">and paper these expressions but with micrograd we will be able to back propagate all the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6656" target="_blank">01:50:56.720</a></span> | <span class="t">through this and back propagate into these weights of all these neurons so let's see how that works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6664" target="_blank">01:51:04.080</a></span> | <span class="t">okay so let's create ourselves a very simple example data set here so this data set has four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6669" target="_blank">01:51:09.920</a></span> | <span class="t">examples and so we have four possible inputs into the neural net and we have four desired targets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6677" target="_blank">01:51:17.440</a></span> | <span class="t">so we'd like the neural net to assign or output 1.0 when it's fed this example negative one when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6684" target="_blank">01:51:24.880</a></span> | <span class="t">it's fed these examples and one when it's fed this example so it's a very simple binary classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6689" target="_blank">01:51:29.600</a></span> | <span class="t">neural net basically that we would like here now let's think what the neural net currently thinks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6694" target="_blank">01:51:34.320</a></span> | <span class="t">about these four examples we can just get their predictions basically we can just call n of x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6700" target="_blank">01:51:40.160</a></span> | <span class="t">for x and x's and then we can print so these are the outputs of the neural net on those four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6707" target="_blank">01:51:47.680</a></span> | <span class="t">examples so the first one is 0.91 but we'd like it to be 1 so we should push this one higher this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6715" target="_blank">01:51:55.920</a></span> | <span class="t">one we want to be higher this one says 0.88 and we want this to be negative 1 this is 0.88 we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6723" target="_blank">01:52:03.600</a></span> | <span class="t">want it to be negative 1 and this one is 0.88 we want it to be 1 so how do we make the neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6730" target="_blank">01:52:10.000</a></span> | <span class="t">and how do we tune the weights to better predict the desired targets and the trick used in deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6738" target="_blank">01:52:18.080</a></span> | <span class="t">learning to achieve this is to calculate a single number that somehow measures the total performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6744" target="_blank">01:52:24.000</a></span> | <span class="t">of your neural net and we call this single number the loss so the loss first is a single number that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6752" target="_blank">01:52:32.320</a></span> | <span class="t">we're going to define that basically measures how well the neural net is performing right now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6756" target="_blank">01:52:36.480</a></span> | <span class="t">have the intuitive sense that it's not performing very well because we're not very much close to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6760" target="_blank">01:52:40.080</a></span> | <span class="t">this so the loss will be high and we'll want to minimize the loss so in particular in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6765" target="_blank">01:52:45.760</a></span> | <span class="t">what we're going to do is we're going to implement the mean squared error loss so what this is doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6770" target="_blank">01:52:50.480</a></span> | <span class="t">is we're going to basically iterate for y ground truth and y output in zip of y's and y_red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6781" target="_blank">01:53:01.120</a></span> | <span class="t">so we're going to pair up the ground truths with the predictions and the zip iterates over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6787" target="_blank">01:53:07.360</a></span> | <span class="t">tuples of them and for each y ground truth and y output we're going to subtract them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6794" target="_blank">01:53:14.240</a></span> | <span class="t">and square them so let's first see what these losses are these are individual loss components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6801" target="_blank">01:53:21.680</a></span> | <span class="t">and so basically for each one of the four we are taking the prediction and the ground truth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6809" target="_blank">01:53:29.440</a></span> | <span class="t">we are subtracting them and squaring them so because this one is so close to its target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6816" target="_blank">01:53:36.240</a></span> | <span class="t">0.91 is almost one subtracting them gives a very small number so here we would get like a negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6823" target="_blank">01:53:43.360</a></span> | <span class="t">0.1 and then squaring it just makes sure that regardless of whether we are more negative or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6830" target="_blank">01:53:50.160</a></span> | <span class="t">more positive we always get a positive number instead of squaring we should we could also take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6836" target="_blank">01:53:56.000</a></span> | <span class="t">for example the absolute value we need to discard the sign and so you see that the expression is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6840" target="_blank">01:54:00.880</a></span> | <span class="t">ranged so that you only get zero exactly when y out is equal to y ground truth when those two are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6847" target="_blank">01:54:07.120</a></span> | <span class="t">equal so your prediction is exactly the target you are going to get zero and if your prediction is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6851" target="_blank">01:54:11.920</a></span> | <span class="t">not the target you are going to get some other number so here for example we are way off and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6857" target="_blank">01:54:17.200</a></span> | <span class="t">that's why the loss is quite high and the more off we are the greater the loss will be so we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6865" target="_blank">01:54:25.120</a></span> | <span class="t">want high loss we want low loss and so the final loss here will be just the sum of all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6872" target="_blank">01:54:32.720</a></span> | <span class="t">numbers so you see that this should be zero roughly plus zero roughly but plus seven so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6881" target="_blank">01:54:41.200</a></span> | <span class="t">loss should be about seven here and now we want to minimize the loss we want the loss to be low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6889" target="_blank">01:54:49.280</a></span> | <span class="t">because if loss is low then every one of the predictions is equal to its target so the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6897" target="_blank">01:54:57.120</a></span> | <span class="t">the lowest it can be is zero and the greater it is the worse off the neural net is predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6902" target="_blank">01:55:02.560</a></span> | <span class="t">so now of course if we do loss.backward something magical happened when i hit enter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6909" target="_blank">01:55:09.760</a></span> | <span class="t">and the magical thing of course that happened is that we can look at n.layers.neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6915" target="_blank">01:55:15.280</a></span> | <span class="t">and that layers at say like the first layer that neurons at zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6920" target="_blank">01:55:20.000</a></span> | <span class="t">because remember that mlp has the layers which is a list and each layer has neurons which is a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6928" target="_blank">01:55:28.720</a></span> | <span class="t">and that gives us an individual neuron and then it's got some weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6932" target="_blank">01:55:32.000</a></span> | <span class="t">and so we can for example look at the weights at zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6940" target="_blank">01:55:40.160</a></span> | <span class="t">oops it's not called weights it's called w and that's a value but now this value also has a grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6948" target="_blank">01:55:48.000</a></span> | <span class="t">because of the backward pass and so we see that because this gradient here on this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6954" target="_blank">01:55:54.240</a></span> | <span class="t">weight of this particular neuron of this particular layer is negative we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6958" target="_blank">01:55:58.480</a></span> | <span class="t">its influence on the loss is also negative so slightly increasing this particular weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6963" target="_blank">01:56:03.760</a></span> | <span class="t">of this neuron of this layer would make the loss go down and we actually have this information for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6970" target="_blank">01:56:10.400</a></span> | <span class="t">every single one of our neurons and all their parameters actually it's worth looking at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6974" target="_blank">01:56:14.640</a></span> | <span class="t">also the draw dot of loss by the way so previously we looked at the draw dot of a single neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6980" target="_blank">01:56:20.720</a></span> | <span class="t">neuron forward pass and that was already a large expression but what is this expression we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6985" target="_blank">01:56:25.920</a></span> | <span class="t">forwarded every one of those four examples and then we have the loss on top of them with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6991" target="_blank">01:56:31.360</a></span> | <span class="t">mean squared error and so this is a really massive graph because this graph that we've built up now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=6998" target="_blank">01:56:38.400</a></span> | <span class="t">oh my gosh this graph that we've built up now which is kind of excessive it's excessive because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7005" target="_blank">01:56:45.440</a></span> | <span class="t">it has four forward passes of a neural net for every one of the examples and then it has the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7010" target="_blank">01:56:50.080</a></span> | <span class="t">loss on top and it ends with the value of the loss which was 7.12 and this loss will now back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7016" target="_blank">01:56:56.240</a></span> | <span class="t">propagate through all the four forward passes all the way through just every single intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7021" target="_blank">01:57:01.520</a></span> | <span class="t">value of the neural net all the way back to of course the parameters of the weights which are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7026" target="_blank">01:57:06.640</a></span> | <span class="t">the input so these weight parameters here are inputs to this neural net and these numbers here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7034" target="_blank">01:57:14.320</a></span> | <span class="t">these scalars are inputs to the neural net so if we went around here we will probably find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7040" target="_blank">01:57:20.640</a></span> | <span class="t">some of these examples this 1.0 potentially maybe this 1.0 or you know some of the others and you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7046" target="_blank">01:57:26.480</a></span> | <span class="t">see that they all have gradients as well the thing is these gradients on the input data are not that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7052" target="_blank">01:57:32.160</a></span> | <span class="t">useful to us and that's because the input data seems to be not changeable it's it's a given to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7059" target="_blank">01:57:39.040</a></span> | <span class="t">the problem and so it's a fixed input we're not going to be changing it or messing with it even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7063" target="_blank">01:57:43.120</a></span> | <span class="t">though we do have gradients for it but some of these gradients here will be for the neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7070" target="_blank">01:57:50.320</a></span> | <span class="t">parameters the w's and the b's and those we of course we want to change okay so now we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7076" target="_blank">01:57:56.640</a></span> | <span class="t">to want some convenience code to gather up all of the parameters of the neural net so that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7081" target="_blank">01:58:01.920</a></span> | <span class="t">operate on all of them simultaneously and every one of them we will nudge a tiny amount based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7089" target="_blank">01:58:09.040</a></span> | <span class="t">the gradient information so let's collect the parameters of the neural net all in one array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7094" target="_blank">01:58:14.800</a></span> | <span class="t">so let's create a parameters of self that just returns self.w which is a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7101" target="_blank">01:58:21.360</a></span> | <span class="t">concatenated with a list of self.b so this will just return a list list plus list just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7110" target="_blank">01:58:30.640</a></span> | <span class="t">you know gives you a list so that's parameters of neuron and i'm calling it this way because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7116" target="_blank">01:58:36.080</a></span> | <span class="t">also pytorch has a parameters on every single nn module and it does exactly what we're doing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7122" target="_blank">01:58:42.240</a></span> | <span class="t">it just returns the parameter tensors for us it's the parameter scalars now layer is also a module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7130" target="_blank">01:58:50.320</a></span> | <span class="t">so it will have parameters self and basically what we want to do here is something like this like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7137" target="_blank">01:58:57.680</a></span> | <span class="t">params is here and then for neuron in self.neurons we want to get neuron.parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7150" target="_blank">01:59:10.560</a></span> | <span class="t">and we want to params.extend right so these are the parameters of this neuron and then we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7157" target="_blank">01:59:17.280</a></span> | <span class="t">to put them on top of params so params.extend of piece and then we want to return params</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7163" target="_blank">01:59:23.920</a></span> | <span class="t">so this there's way too much code so actually there's a way to simplify this which is return</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7173" target="_blank">01:59:33.920</a></span> | <span class="t">p for neuron in self.neurons for p in neuron.parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7184" target="_blank">01:59:44.000</a></span> | <span class="t">so it's a single list comprehension in python you can sort of nest them like this and you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7190" target="_blank">01:59:50.400</a></span> | <span class="t">then create the desired array so this is these are identical we can take this out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7200" target="_blank">02:00:00.000</a></span> | <span class="t">and then let's do the same here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7201" target="_blank">02:00:01.440</a></span> | <span class="t">def parameters self and return a parameter for layer in self.layers for p in layer.parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7217" target="_blank">02:00:17.440</a></span> | <span class="t">and that should be good now let me pop out this so we don't re-initialize our network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7228" target="_blank">02:00:28.960</a></span> | <span class="t">because we need to re-initialize our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7231" target="_blank">02:00:31.680</a></span> | <span class="t">okay so unfortunately we will have to probably re-initialize the network because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7239" target="_blank">02:00:39.120</a></span> | <span class="t">we just had functionality because this class of course we i want to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7243" target="_blank">02:00:43.840</a></span> | <span class="t">all the end up parameters but that's not going to work because this is the old class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7248" target="_blank">02:00:48.240</a></span> | <span class="t">okay so unfortunately we do have to re-initialize the network which will change some of the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7255" target="_blank">02:00:55.680</a></span> | <span class="t">but let me do that so that we pick up the new api we can now do end up parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7259" target="_blank">02:00:59.280</a></span> | <span class="t">and these are all the weights and biases inside the entire neural net so in total this mlp has 41</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7268" target="_blank">02:01:08.720</a></span> | <span class="t">parameters and now we'll be able to change them if we recalculate the loss here we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7278" target="_blank">02:01:18.560</a></span> | <span class="t">unfortunately we have slightly different predictions and slightly different loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7284" target="_blank">02:01:24.880</a></span> | <span class="t">um but that's okay okay so we see that this neuron's gradient is slightly negative we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7293" target="_blank">02:01:33.200</a></span> | <span class="t">also look at its data right now which is 0.85 so this is the current value of this neuron and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7299" target="_blank">02:01:39.840</a></span> | <span class="t">is its gradient on the loss so what we want to do now is we want to iterate for every p in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7307" target="_blank">02:01:47.040</a></span> | <span class="t">end up parameters so for all the 41 parameters of this neural net we actually want to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7313" target="_blank">02:01:53.360</a></span> | <span class="t">p.data slightly according to the gradient information okay so dot dot dot to do here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7321" target="_blank">02:02:01.360</a></span> | <span class="t">but this will be basically a tiny update in this gradient descent scheme and gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7329" target="_blank">02:02:09.520</a></span> | <span class="t">we are thinking of the gradient as a vector pointing in the direction of increased loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7339" target="_blank">02:02:19.120</a></span> | <span class="t">and so in gradient descent we are modifying p.data by a small step size in the direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7346" target="_blank">02:02:26.480</a></span> | <span class="t">of the gradient so the step size as an example could be like a very small number like 0.01 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7350" target="_blank">02:02:30.960</a></span> | <span class="t">the step size times p.grad right but we have to think through some of the signs here so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7359" target="_blank">02:02:39.120</a></span> | <span class="t">in particular working with this specific example here we see that if we just left it like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7367" target="_blank">02:02:47.040</a></span> | <span class="t">then this neuron's value would be currently increased by a tiny amount of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7372" target="_blank">02:02:52.560</a></span> | <span class="t">the gradient is negative so this value of this neuron would go slightly down it would become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7378" target="_blank">02:02:58.480</a></span> | <span class="t">like 0.84 or something like that but if this neuron's value goes lower that would actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7387" target="_blank">02:03:07.280</a></span> | <span class="t">increase the loss that's because the derivative of this neuron is negative so increasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7396" target="_blank">02:03:16.560</a></span> | <span class="t">this makes the loss go down so increasing it is what we want to do instead of decreasing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7402" target="_blank">02:03:22.880</a></span> | <span class="t">so basically what we're missing here is we're actually missing a negative sign</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7405" target="_blank">02:03:25.440</a></span> | <span class="t">and again this other interpretation and that's because we want to minimize the loss we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7410" target="_blank">02:03:30.880</a></span> | <span class="t">want to maximize the loss we want to decrease it and the other interpretation as i mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7415" target="_blank">02:03:35.600</a></span> | <span class="t">is you can think of the gradient vector so basically just the vector of all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7420" target="_blank">02:03:40.400</a></span> | <span class="t">as pointing in the direction of increasing the loss but then we want to decrease it so we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7426" target="_blank">02:03:46.800</a></span> | <span class="t">want to go in the opposite direction and so you can convince yourself that this sort of like does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7431" target="_blank">02:03:51.360</a></span> | <span class="t">the right thing here with the negative because we want to minimize the loss so if we nudge all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7436" target="_blank">02:03:56.480</a></span> | <span class="t">parameters by a tiny amount then we'll see that this data will have changed a little bit so now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7444" target="_blank">02:04:04.640</a></span> | <span class="t">this neuron is a tiny amount greater value so 0.854 went to 0.857 and that's a good thing because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7455" target="_blank">02:04:15.600</a></span> | <span class="t">slightly increasing this neuron data makes the loss go down according to the gradient and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7463" target="_blank">02:04:23.120</a></span> | <span class="t">the correcting has happened sine wise and so now what we would expect of course is that because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7469" target="_blank">02:04:29.360</a></span> | <span class="t">we've changed all these parameters we expect that the loss should have gone down a bit so we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7475" target="_blank">02:04:35.680</a></span> | <span class="t">re-evaluate the loss let me basically this is just a data definition that hasn't changed but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7482" target="_blank">02:04:42.960</a></span> | <span class="t">forward pass here of the network we can recalculate and actually let me do it outside here so that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7491" target="_blank">02:04:51.920</a></span> | <span class="t">can compare the two loss values so here if i recalculate the loss we'd expect the new loss now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7499" target="_blank">02:04:59.280</a></span> | <span class="t">to be slightly lower than this number so hopefully what we're getting now is a tiny bit lower than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7504" target="_blank">02:05:04.240</a></span> | <span class="t">4.36 okay and remember the way we've arranged this is that low loss means that our predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7513" target="_blank">02:05:13.440</a></span> | <span class="t">are matching the targets so our predictions now are probably slightly closer to the targets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7520" target="_blank">02:05:20.240</a></span> | <span class="t">and now all we have to do is we have to iterate this process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7523" target="_blank">02:05:23.440</a></span> | <span class="t">so again we've done the forward pass and this is the loss now we can loss that backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7529" target="_blank">02:05:29.280</a></span> | <span class="t">let me take these out and we can do a step size and now we should have a slightly lower loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7536" target="_blank">02:05:36.320</a></span> | <span class="t">4.36 goes to 3.9 and okay so we've done the forward pass here's the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7544" target="_blank">02:05:44.320</a></span> | <span class="t">nudge and now the loss is 3.66</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7547" target="_blank">02:05:47.520</a></span> | <span class="t">3.47 and you get the idea we just continue doing this and this is gradient descent we're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7557" target="_blank">02:05:57.520</a></span> | <span class="t">iteratively doing forward pass backward pass update forward pass backward pass update and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7562" target="_blank">02:06:02.880</a></span> | <span class="t">the neural net is improving its predictions so here if we look at y pred now y pred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7572" target="_blank">02:06:12.720</a></span> | <span class="t">we see that this value should be getting closer to one so this value should be getting more positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7578" target="_blank">02:06:18.400</a></span> | <span class="t">these should be getting more negative and this one should be also getting more positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7581" target="_blank">02:06:21.840</a></span> | <span class="t">so if we just iterate this a few more times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7584" target="_blank">02:06:24.640</a></span> | <span class="t">actually we may be able to afford to go a bit faster let's try a slightly higher learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7591" target="_blank">02:06:31.040</a></span> | <span class="t">oops okay there we go so now we're at 0.31</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7599" target="_blank">02:06:39.200</a></span> | <span class="t">if you go too fast by the way if you try to make it too big of a step you may actually overstep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7604" target="_blank">02:06:44.400</a></span> | <span class="t">overconfidence because again remember we don't actually know exactly about the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7611" target="_blank">02:06:51.280</a></span> | <span class="t">the loss function has all kinds of structure and we only know about the very local dependence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7616" target="_blank">02:06:56.320</a></span> | <span class="t">all these parameters on the loss but if we step too far we may step into you know a part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7621" target="_blank">02:07:01.520</a></span> | <span class="t">loss that is completely different and that can destabilize training and make your loss actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7625" target="_blank">02:07:05.840</a></span> | <span class="t">blow up even so the loss is now 0.04 so actually the predictions should be really quite close let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7633" target="_blank">02:07:13.600</a></span> | <span class="t">take a look so you see how this is almost one almost negative one almost one we can continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7640" target="_blank">02:07:20.080</a></span> | <span class="t">going so yep backward update oops there we go so we went way too fast and we actually overstepped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7651" target="_blank">02:07:31.440</a></span> | <span class="t">so we got too eager where are we now oops okay 7e negative 9 so this is very very low loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7660" target="_blank">02:07:40.080</a></span> | <span class="t">and the predictions are basically perfect so somehow we basically we were doing way too big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7668" target="_blank">02:07:48.880</a></span> | <span class="t">updates and we briefly exploded but then somehow we ended up getting into a really good spot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7672" target="_blank">02:07:52.880</a></span> | <span class="t">so usually this learning rate and the tuning of it is a subtle art you want to set your learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7678" target="_blank">02:07:58.640</a></span> | <span class="t">rate if it's too low you're going to take way too long to converge but if it's too high the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7683" target="_blank">02:08:03.280</a></span> | <span class="t">whole thing gets unstable and you might actually even explode the loss depending on your loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7687" target="_blank">02:08:07.760</a></span> | <span class="t">function so finding the step size to be just right it's it's a pretty subtle art sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7693" target="_blank">02:08:13.040</a></span> | <span class="t">when you're using sort of vanilla gradient descent but we happen to get into a good spot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7697" target="_blank">02:08:17.680</a></span> | <span class="t">we can look at n dot parameters so this is the setting of weights and biases that makes our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7707" target="_blank">02:08:27.520</a></span> | <span class="t">network predict the desired targets very very close and basically we've successfully trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7717" target="_blank">02:08:37.120</a></span> | <span class="t">a neural net okay let's make this a tiny bit more respectable and implement an actual training loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7722" target="_blank">02:08:42.240</a></span> | <span class="t">and what that looks like so this is the data definition that stays this is the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7728" target="_blank">02:08:48.320</a></span> | <span class="t">so for k in range you know we're going to take a bunch of steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7735" target="_blank">02:08:55.040</a></span> | <span class="t">first you do the forward pass we validate the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7741" target="_blank">02:09:01.040</a></span> | <span class="t">let's reinitialize the neural net from scratch and here's the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7746" target="_blank">02:09:06.960</a></span> | <span class="t">and we first do forward pass then we do the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7752" target="_blank">02:09:12.480</a></span> | <span class="t">and then we do an update that's gradient descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7761" target="_blank">02:09:21.680</a></span> | <span class="t">and then we should be able to iterate this and we should be able to print the current step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7769" target="_blank">02:09:29.600</a></span> | <span class="t">the current loss let's just print the sort of number of the loss and that should be it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7780" target="_blank">02:09:40.480</a></span> | <span class="t">and then the learning rate 0.01 is a little too small 0.1 we saw is like a little bit dangerous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7784" target="_blank">02:09:44.960</a></span> | <span class="t">with ui let's go somewhere in between and we'll optimize this for not 10 steps but let's go for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7791" target="_blank">02:09:51.680</a></span> | <span class="t">say 20 steps let me erase all of this junk and let's run the optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7800" target="_blank">02:10:00.800</a></span> | <span class="t">and you see how we've actually converged slower in a more controlled manner and got to a loss that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7809" target="_blank">02:10:09.280</a></span> | <span class="t">is very low so i expect y_pred to be quite good there we go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7816" target="_blank">02:10:16.240</a></span> | <span class="t">and that's it okay so this is kind of embarrassing but we actually have a really terrible bug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7827" target="_blank">02:10:27.600</a></span> | <span class="t">in here and it's a subtle bug and it's a very common bug and i can't believe i've done it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7833" target="_blank">02:10:33.920</a></span> | <span class="t">the 20th time in my life especially on camera and i could have re-shot the whole thing but i think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7839" target="_blank">02:10:39.600</a></span> | <span class="t">it's pretty funny and you know you get to appreciate a bit what working with neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7844" target="_blank">02:10:44.720</a></span> | <span class="t">nuts maybe is like sometimes we are guilty of a common bug i've actually tweeted the most common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7853" target="_blank">02:10:53.520</a></span> | <span class="t">neural mistakes a long time ago now and i'm not really gonna explain any of these except for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7861" target="_blank">02:11:01.520</a></span> | <span class="t">we are guilty of number three you forgot to zero grad before dot backward what is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7866" target="_blank">02:11:06.800</a></span> | <span class="t">basically what's happening and it's a subtle bug and i'm not sure if you saw it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7872" target="_blank">02:11:12.160</a></span> | <span class="t">is that all of these weights here have a dot data and a dot grad and the dot grad starts at zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7881" target="_blank">02:11:21.120</a></span> | <span class="t">and then we do backward and we fill in the gradients and then we do an update on the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7887" target="_blank">02:11:27.520</a></span> | <span class="t">but we don't flush the grad it stays there so when we do the second forward pass and we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7894" target="_blank">02:11:34.400</a></span> | <span class="t">backward again remember that all the backward operations do a plus equals on the grad and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7899" target="_blank">02:11:39.760</a></span> | <span class="t">these gradients just add up and they never get reset to zero so basically we didn't zero grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7907" target="_blank">02:11:47.280</a></span> | <span class="t">so here's how we zero grad before backward we need to iterate over all the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7914" target="_blank">02:11:54.160</a></span> | <span class="t">and we need to make sure that p dot grad is set to zero we need to reset it to zero just like it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7921" target="_blank">02:12:01.040</a></span> | <span class="t">in the constructor so remember all the way here for all these value nodes grad is reset to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7927" target="_blank">02:12:07.360</a></span> | <span class="t">and then all these backward passes do a plus equals from that grad but we need to make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7932" target="_blank">02:12:12.240</a></span> | <span class="t">that we reset these graphs to zero so that when we do backward all of them start at zero and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7938" target="_blank">02:12:18.640</a></span> | <span class="t">actual backward pass accumulates the loss derivatives into the grads so this is zero grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7946" target="_blank">02:12:26.960</a></span> | <span class="t">in pytorch and we will slightly we'll get a slightly different optimization let's reset the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7953" target="_blank">02:12:33.760</a></span> | <span class="t">neural net the data is the same this is now i think correct and we get a much more you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7960" target="_blank">02:12:40.240</a></span> | <span class="t">we get a much more slower descent we still end up with pretty good results and we can continue this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7967" target="_blank">02:12:47.280</a></span> | <span class="t">a bit more to get down lower and lower and lower yeah so the only reason that the previous thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7977" target="_blank">02:12:57.600</a></span> | <span class="t">worked it's extremely buggy the only reason that worked is that this is a very very simple problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7984" target="_blank">02:13:04.880</a></span> | <span class="t">and it's very easy for this neural net to fit this data and so the grads ended up accumulating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7992" target="_blank">02:13:12.000</a></span> | <span class="t">and it effectively gave us a massive step size and it made us converge extremely fast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=7996" target="_blank">02:13:16.880</a></span> | <span class="t">but basically now we have to do more steps to get to very low values of loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8003" target="_blank">02:13:23.600</a></span> | <span class="t">and get y pred to be really good we can try to step a bit greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8009" target="_blank">02:13:29.040</a></span> | <span class="t">yeah we're gonna get closer and closer to one minus one and one so working with neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8020" target="_blank">02:13:40.640</a></span> | <span class="t">is sometimes tricky because you may have lots of bugs in the code and your network might actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8029" target="_blank">02:13:49.280</a></span> | <span class="t">work just like ours worked but chances are is that if we had a more complex problem then actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8035" target="_blank">02:13:55.200</a></span> | <span class="t">this bug would have made us not optimize the loss very well and we were only able to get away with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8039" target="_blank">02:13:59.520</a></span> | <span class="t">it because the problem is very simple so let's now bring everything together and summarize what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8045" target="_blank">02:14:05.680</a></span> | <span class="t">we learned what are neural nets neural nets are these mathematical expressions fairly simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8052" target="_blank">02:14:12.000</a></span> | <span class="t">mathematical expressions in the case of multilear perceptron that take input as the data and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8058" target="_blank">02:14:18.320</a></span> | <span class="t">take input the weights and the parameters of the neural net mathematical expression for the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8063" target="_blank">02:14:23.360</a></span> | <span class="t">pass followed by a loss function and the loss function tries to measure the accuracy of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8068" target="_blank">02:14:28.320</a></span> | <span class="t">predictions and usually the loss will be low when your predictions are matching your targets or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8073" target="_blank">02:14:33.440</a></span> | <span class="t">where the network is basically behaving well so we we manipulate the loss function so that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8078" target="_blank">02:14:38.960</a></span> | <span class="t">the loss is low the network is doing what you want it to do on your problem and then we backward the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8085" target="_blank">02:14:45.440</a></span> | <span class="t">loss use back propagation to get the gradient and then we know how to tune all the parameters to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8090" target="_blank">02:14:50.800</a></span> | <span class="t">decrease the loss locally but then we have to iterate that process many times in what's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8095" target="_blank">02:14:55.200</a></span> | <span class="t">the gradient descent so we simply follow the gradient information and that minimizes the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8100" target="_blank">02:15:00.960</a></span> | <span class="t">and the loss is arranged so that when the loss is minimized the network is doing what you want it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8105" target="_blank">02:15:05.120</a></span> | <span class="t">do and yeah so we just have a blob of neural stuff and we can make it do arbitrary things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8112" target="_blank">02:15:12.640</a></span> | <span class="t">and that's what gives neural nets their power it's you know this is a very tiny network with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8117" target="_blank">02:15:17.120</a></span> | <span class="t">41 parameters but you can build significantly more complicated neural nets with billions at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8124" target="_blank">02:15:24.160</a></span> | <span class="t">this point almost trillions of parameters and it's a massive blob of neural tissue simulated neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8129" target="_blank">02:15:29.840</a></span> | <span class="t">tissue roughly speaking and you can make it do extremely complex problems and these neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8136" target="_blank">02:15:36.560</a></span> | <span class="t">then have all kinds of very fascinating emergent properties in when you try to make them do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8142" target="_blank">02:15:42.800</a></span> | <span class="t">significantly hard problems as in the case of gpt for example we have massive amounts of text from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8149" target="_blank">02:15:49.680</a></span> | <span class="t">the internet and we're trying to get a neural net to predict to take like a few words and try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8154" target="_blank">02:15:54.400</a></span> | <span class="t">predict the next word in a sequence that's the learning problem and it turns out that when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8158" target="_blank">02:15:58.480</a></span> | <span class="t">train this on all of internet the neural net actually has like really remarkable emergent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8162" target="_blank">02:16:02.880</a></span> | <span class="t">properties but that neural net would have hundreds of billions of parameters but it works on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8168" target="_blank">02:16:08.400</a></span> | <span class="t">fundamentally the exact same principles the neural net of course will be a bit more complex but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8173" target="_blank">02:16:13.360</a></span> | <span class="t">otherwise the evaluating the gradient is there and will be identical and the gradient descent would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8180" target="_blank">02:16:20.080</a></span> | <span class="t">be there and would be basically identical but people usually use slightly different updates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8184" target="_blank">02:16:24.720</a></span> | <span class="t">this is a very simple stochastic gradient descent update and the loss function would not be a mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8190" target="_blank">02:16:30.640</a></span> | <span class="t">squared error they would be using something called the cross entropy loss for predicting the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8194" target="_blank">02:16:34.880</a></span> | <span class="t">token so there's a few more details but fundamentally the neural network setup and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8198" target="_blank">02:16:38.560</a></span> | <span class="t">neural network training is identical and pervasive and now you understand intuitively how that works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8204" target="_blank">02:16:44.800</a></span> | <span class="t">under the hood in the beginning of this video i told you that by the end of it you would understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8208" target="_blank">02:16:48.800</a></span> | <span class="t">everything in micrograd and then would slowly build it up let me briefly prove that to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8214" target="_blank">02:16:54.000</a></span> | <span class="t">so i'm going to step through all the code that is in micrograd as of today actually potentially some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8218" target="_blank">02:16:58.640</a></span> | <span class="t">of the code will change by the time you watch this video because i intend to continue developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8222" target="_blank">02:17:02.160</a></span> | <span class="t">micrograd but let's look at what we have so far at least init.py is empty when you go to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8227" target="_blank">02:17:07.680</a></span> | <span class="t">engine.py that has the value everything here you should mostly recognize so we have the data.data.grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8233" target="_blank">02:17:13.600</a></span> | <span class="t">attributes we have the backward function we have the previous set of children and the operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8238" target="_blank">02:17:18.320</a></span> | <span class="t">that produced this value we have addition multiplication and raising to a scalar power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8244" target="_blank">02:17:24.320</a></span> | <span class="t">we have the relu non-linearity which is slightly different type of non-linearity than tanh that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8249" target="_blank">02:17:29.200</a></span> | <span class="t">we used in this video both of them are non-linearities and notably tanh is not actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8254" target="_blank">02:17:34.160</a></span> | <span class="t">present in micrograd as of right now but i intend to add it later we have the backward which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8259" target="_blank">02:17:39.680</a></span> | <span class="t">identical and then all of these other operations which are built up on top of operations here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8265" target="_blank">02:17:45.440</a></span> | <span class="t">so value should be very recognizable except for the non-linearity used in this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8269" target="_blank">02:17:49.040</a></span> | <span class="t">there's no massive difference between relu and tanh and sigmoid and these other non-linearities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8275" target="_blank">02:17:55.200</a></span> | <span class="t">they're all roughly equivalent and can be used in MLPs so i use tanh because it's a bit smoother</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8280" target="_blank">02:18:00.240</a></span> | <span class="t">and because it's a little bit more complicated than relu and therefore it's stressed a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8284" target="_blank">02:18:04.480</a></span> | <span class="t">bit more the local gradients and working with those derivatives which i thought would be useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8289" target="_blank">02:18:09.440</a></span> | <span class="t">and then .py is the neural networks library as i mentioned so you should recognize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8295" target="_blank">02:18:15.120</a></span> | <span class="t">identical implementation of neuron layer and MLP notably or not so much we have a class module here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8301" target="_blank">02:18:21.840</a></span> | <span class="t">there's a parent class of all these modules i did that because there's an nn.module class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8307" target="_blank">02:18:27.120</a></span> | <span class="t">in pytorch and so this exactly matches that api and nn.module in pytorch has also a zero grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8312" target="_blank">02:18:32.720</a></span> | <span class="t">which i refactored out here so that's the end of micrograd really then there's a test which you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8320" target="_blank">02:18:40.720</a></span> | <span class="t">see basically creates two chunks of code one in micrograd and one in pytorch and we'll make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8327" target="_blank">02:18:47.520</a></span> | <span class="t">that the forward and the backward paths agree identically for a slightly less complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8331" target="_blank">02:18:51.920</a></span> | <span class="t">expression a slightly more complicated expression everything agrees so we agree with pytorch and all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8337" target="_blank">02:18:57.360</a></span> | <span class="t">of these operations and finally there's a demo.pypy.ymb here and it's a bit more complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8342" target="_blank">02:19:02.880</a></span> | <span class="t">binary classification demo than the one i covered in this lecture so we only had a tiny data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8347" target="_blank">02:19:07.680</a></span> | <span class="t">of four examples here we have a bit more complicated example with lots of blue points and lots of red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8353" target="_blank">02:19:13.680</a></span> | <span class="t">points and we're trying to again build a binary classifier to distinguish two-dimensional points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8358" target="_blank">02:19:18.800</a></span> | <span class="t">as red or blue it's a bit more complicated MLP here with it's a bigger MLP the loss is a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8365" target="_blank">02:19:25.440</a></span> | <span class="t">more complicated because it supports batches so because our data set was so tiny we always did a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8371" target="_blank">02:19:31.680</a></span> | <span class="t">forward pass on the entire data set of four examples but when your data set is like a million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8376" target="_blank">02:19:36.400</a></span> | <span class="t">examples what we usually do in practice is we basically pick out some random subset we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8382" target="_blank">02:19:42.000</a></span> | <span class="t">that a batch and then we only process the batch forward backward and update so we don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8387" target="_blank">02:19:47.120</a></span> | <span class="t">forward the entire training set so this supports batching because there's a lot more examples here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8392" target="_blank">02:19:52.640</a></span> | <span class="t">we do a forward pass the loss is slightly more different this is a max margin loss that i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8398" target="_blank">02:19:58.720</a></span> | <span class="t">implement here the one that we used was the mean squared error loss because it's the simplest one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8404" target="_blank">02:20:04.560</a></span> | <span class="t">there's also the binary cross-entropy loss all of them can be used for binary classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8409" target="_blank">02:20:09.040</a></span> | <span class="t">and don't make too much of a difference in the simple examples that we looked at so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8412" target="_blank">02:20:12.720</a></span> | <span class="t">there's something called L2 regularization used here this has to do with generalization of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8418" target="_blank">02:20:18.640</a></span> | <span class="t">neural net and controls the overfitting in machine learning setting but i did not cover these concepts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8423" target="_blank">02:20:23.920</a></span> | <span class="t">in concepts in this video potentially later and the training loop you should recognize so forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8429" target="_blank">02:20:29.520</a></span> | <span class="t">backward with zero grad and update and so on you'll notice that in the update here the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8436" target="_blank">02:20:36.720</a></span> | <span class="t">is scaled as a function of number of iterations and it shrinks and this is something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8442" target="_blank">02:20:42.720</a></span> | <span class="t">learning rate decay so in the beginning you have a high learning rate and as the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8447" target="_blank">02:20:47.040</a></span> | <span class="t">sort of stabilizes near the end you bring down the learning rate to get some of the fine details in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8452" target="_blank">02:20:52.000</a></span> | <span class="t">the end and in the end we see the decision surface of the neural net and we see that it learns to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8457" target="_blank">02:20:57.440</a></span> | <span class="t">separate out the red and the blue area based on the data points so that's the slightly more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8462" target="_blank">02:21:02.880</a></span> | <span class="t">complicated example in the demo demo.hypeiymb that you're free to go over but yeah as of today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8468" target="_blank">02:21:08.720</a></span> | <span class="t">that is micrograd i also wanted to show you a little bit of real stuff so that you get to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8473" target="_blank">02:21:13.040</a></span> | <span class="t">how this is actually implemented in a production grade library like pytorch so in particular i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8477" target="_blank">02:21:17.760</a></span> | <span class="t">wanted to show i wanted to find and show you the backward pass for 10h in pytorch so here in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8483" target="_blank">02:21:23.520</a></span> | <span class="t">micrograd we see that the backward pass for 10h is 1 minus t square where t is the output of the 10h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8490" target="_blank">02:21:30.560</a></span> | <span class="t">of x times of that grad which is the chain rule so we're looking for something that looks like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8497" target="_blank">02:21:37.360</a></span> | <span class="t">now i went to pytorch which has an open source github codebase and i looked through a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8505" target="_blank">02:21:45.760</a></span> | <span class="t">its code and honestly i i spent about 15 minutes and i couldn't find 10h and that's because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8512" target="_blank">02:21:52.800</a></span> | <span class="t">libraries unfortunately they grow in size and entropy and if you just search for 10h you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8518" target="_blank">02:21:58.320</a></span> | <span class="t">apparently 2800 results and 400 and 406 files so i don't know what these files are doing honestly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8525" target="_blank">02:22:05.760</a></span> | <span class="t">and why there are so many mentions of 10h but unfortunately these libraries are quite complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8531" target="_blank">02:22:11.920</a></span> | <span class="t">they're meant to be used not really inspected eventually i did stumble on someone who tries to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8539" target="_blank">02:22:19.840</a></span> | <span class="t">change the 10h backward code for some reason and someone here pointed to the cpu kernel and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8544" target="_blank">02:22:24.880</a></span> | <span class="t">cuda kernel for 10h backward so this so basically depends on if you're using pytorch on a cpu device</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8551" target="_blank">02:22:31.440</a></span> | <span class="t">or on a gpu which these are different devices and i haven't covered this but this is the 10h</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8556" target="_blank">02:22:36.080</a></span> | <span class="t">backward kernel for cpu and the reason it's so large is that number one this is like if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8565" target="_blank">02:22:45.200</a></span> | <span class="t">using a complex type which we haven't even talked about if you're using a specific data type of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8569" target="_blank">02:22:49.280</a></span> | <span class="t">bfloat16 which we haven't talked about and then if you're not then this is the kernel and deep here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8576" target="_blank">02:22:56.560</a></span> | <span class="t">we see something that resembles our backward pass so they have a times 1 minus b square so this b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8584" target="_blank">02:23:04.720</a></span> | <span class="t">b here must be the output of the 10h and this is the out.grad so here we found it deep inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8594" target="_blank">02:23:14.080</a></span> | <span class="t">pytorch on this location for some reason inside binary ops kernel when 10h is not actually binary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8599" target="_blank">02:23:19.600</a></span> | <span class="t">op and then this is the gpu kernel we're not complex we're here and here we go with one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8609" target="_blank">02:23:29.280</a></span> | <span class="t">line of code so we did find it but basically unfortunately these code bases are very large and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8615" target="_blank">02:23:35.440</a></span> | <span class="t">micrograd is very very simple but if you actually want to use real stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8620" target="_blank">02:23:40.400</a></span> | <span class="t">finding the code for it you'll actually find that difficult i also wanted to show you a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8625" target="_blank">02:23:45.840</a></span> | <span class="t">example here where pytorch is showing you how you can register a new type of function that you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8630" target="_blank">02:23:50.560</a></span> | <span class="t">to add to pytorch as a lego building block so here if you want to for example add a legendre polynomial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8637" target="_blank">02:23:57.280</a></span> | <span class="t">3 here's how you could do it you will register it as a class that subclasses torch.rgrad.function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8646" target="_blank">02:24:06.320</a></span> | <span class="t">and then you have to tell pytorch how to forward your new function and how to backward through it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8652" target="_blank">02:24:12.080</a></span> | <span class="t">so as long as you can do the forward pass of this little function piece that you want to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8656" target="_blank">02:24:16.240</a></span> | <span class="t">and as long as you know the the local derivative the local gradients which are implemented in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8660" target="_blank">02:24:20.480</a></span> | <span class="t">backward pytorch will be able to back propagate through your function and then you can use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8664" target="_blank">02:24:24.960</a></span> | <span class="t">as a lego block in a larger lego castle of all the different lego blocks that pytorch already has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8670" target="_blank">02:24:30.320</a></span> | <span class="t">and so that's the only thing you have to tell pytorch and everything would just work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8674" target="_blank">02:24:34.240</a></span> | <span class="t">and you can register new types of functions in this way following this example and that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8679" target="_blank">02:24:39.360</a></span> | <span class="t">everything that i wanted to cover in this lecture so i hope you enjoyed building out micrograd with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8683" target="_blank">02:24:43.440</a></span> | <span class="t">me i hope you find it interesting insightful and yeah i will post a lot of the links that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8690" target="_blank">02:24:50.560</a></span> | <span class="t">related to this video in the video description below i will also probably post a link to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8695" target="_blank">02:24:55.280</a></span> | <span class="t">discussion forum or discussion group where you can ask questions related to this video and then i can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8700" target="_blank">02:25:00.800</a></span> | <span class="t">answer or someone else can answer your questions and i may also do a follow-up video that answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8705" target="_blank">02:25:05.760</a></span> | <span class="t">some of the most common questions but for now that's it i hope you enjoyed it if you did then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8711" target="_blank">02:25:11.360</a></span> | <span class="t">please like and subscribe so that youtube knows to feature this video to more people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8714" target="_blank">02:25:14.880</a></span> | <span class="t">and that's it for now i'll see you later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8722" target="_blank">02:25:22.480</a></span> | <span class="t">now here's the problem we know dl by wait what is the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8729" target="_blank">02:25:29.520</a></span> | <span class="t">and that's everything i wanted to cover in this lecture so i hope</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8734" target="_blank">02:25:34.960</a></span> | <span class="t">you enjoyed us building up micro grabbed micro grab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8739" target="_blank">02:25:39.120</a></span> | <span class="t">okay now let's do the exact same thing for multiply because we can't do something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=VMj-3S1tku0&t=8745" target="_blank">02:25:45.520</a></span> | <span class="t">a times two whoops i know what happened there</span></div></div></body></html>