<html><head><title>Advanced Guardrails for AI Agents | Full Tutorial</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Advanced Guardrails for AI Agents | Full Tutorial</h2><a href="https://www.youtube.com/watch?v=rMUycP_cp9g" target="_blank"><img src="https://i.ytimg.com/vi_webp/rMUycP_cp9g/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=0 target="_blank"">0:0</a> Why Guardrails<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=20 target="_blank"">0:20</a> Guardrails for Agents<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=220 target="_blank"">3:40</a> Sparse and Dense Vectors<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=480 target="_blank"">8:0</a> Hybrid Guardrails with Python<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=749 target="_blank"">12:29</a> Initializing the HybridRouter<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=901 target="_blank"">15:1</a> Optimizing our Hybrid Guardrails<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=1156 target="_blank"">19:16</a> Testing our Hybrid Router<br><a href="https://www.youtube.com/watch?v=rMUycP_cp9g&t=1246 target="_blank"">20:46</a> AI Guardrails in Context<br><h3>Transcript</h3><div class='max-width'><p>Today, we are going to be taking a look at how we can build, to be honest, pretty spectacular guardrails for AI agents, chat applications, or just anything conversational, or anything where we need to classify incoming natural language queries from a user, or even from an LLM itself. When we're building these guardrails, there are a few different components that we would usually put together.</p><p>We wouldn't just have one. We would typically have a routing layer, and that may even be broken down into multiple routing layers, or even a hierarchy of those. That routing layer may be our guardrails, or it could be the opposite. It could be defining the scope of what we can talk about, or maybe even both.</p><p>So we could have where we're allowed to talk about over here in yellow, and where we're not allowed to talk about over here in red, and maybe we even overlap a little bit for very specific topics. Now, this fits in as, for example, a user over here will be chatting, and their queries will be hitting our initial set of guardrails, and saying, "Okay, you can come through, or you cannot." Now, depending on whether we get a "okay, you can continue," or we hit a guardrail, we may do different things.</p><p>We may, with a guardrail, we may decide, "Okay, we're just going to provide a pre-written response," or we might say, "Okay, we need to go to another LLM and tell it it needs to deal with this situation in a good way." So that will be the first layer. Let's say we do get through our initial set of guardrails.</p><p>Then what we might want to do is have our LLM down here. But of course, we're not just going to rely solely on those guardrails. We also want to be prompting our LLM to be as secure as possible as well. So there's another layer of protection also here. And that LLM, if a guardrail is hit, might decide, "Okay, we don't respond directly, or we provide a specific response that goes back to the user." Okay?</p><p>Let's say that did get through. Then what we might do is we come back around to the user and we hit either the same guardrails or, more likely, a different set of guardrails to make sure that the LLM isn't responding to something that we don't particularly like. Now, this structure here is what I would say is the essentials of guardrail protection for a public chat agent.</p><p>There are still more things that we could add here. We could add things like specific codified rules that look for specific terms. Or we can have different classification models that also look at the incoming and outgoing text. But with what we have here, we can actually get pretty far.</p><p>Now, what we are focusing on is this layer here. This is our semantic routing layer. And I've spoken a lot about the semantic router before. But what I want to talk about in this video is making semantic router better by not just relying on semantics, but also involving term matching.</p><p>Now, what does that actually mean? So, let's start with the semantic router. Our user will say something. And what we do is we process a query through an embedding model such as OpenAeyes takes embedding 3. And what that does is it creates this vector representation of their query within a semantic space.</p><p>And what that means is that similar queries, when embedded with the same embedding model, would appear in a similar space to other queries that have similar semantics, i.e. they have a similar human meaning. So, for example, one of the queries that we're going to be using later is "Can I sell my Tesla?" Right, let's say that this is "Can I sell my Tesla?" in vector space.</p><p>Okay, so we can imagine this as a 2D area. Let's say this one is "Can I sell my Tesla?" Then what we would find is, okay, over here we might have another query, which is "Can I sell my Pulsar?" Okay, which is another EV. And the human meaning of both of those queries is pretty similar.</p><p>So they end up in a similar vector space. And that means that for our user query here, what they might be saying is "Can I sell my some other car?" Okay, and what the semantic router by itself is doing is looking at a user's query and it's saying "How similar is that user's query to a set of other queries that we have predefined?" Okay, so we could say here, okay, these two red queries, they are very similar and they are above a certain similarity threshold to our user's query.</p><p>So we then know to flag the user's query as a particular route or a particular guardrail based on those predefined queries. Now this works pretty well, but there are some issues with it. So for example, if our use case is we are a chatbot on a EVMaker's website. Let's say our chatbot is from BYD.</p><p>And we only want our users to be able to talk about BYD. We don't want them to talk about Pulsar. We don't want them to talk about Tesla. Now, the problem with semantics here is that the semantic meaning is general. It's not the semantic meaning based on whether this is BYD or Tesla.</p><p>It's the semantic meaning of general language. So what you will find is if someone's asking about BYD, the semantic similarity between that and someone asking about Tesla is going to be pretty similar. So this is where semantic routing alone can fall down. However, before we had dense embedding models, which create these semantic vectors, we had the more traditional embedding models, which are things like BM25 or TF-IDF.</p><p>And the way that these work is that they actually look at the words or terms between two sentences. And where we see that there is a lot of term overlap, we would score them more highly. And with that, we can create what we would call sparse embeddings. And these sparse embeddings base their similarity scores on how much term overlap there is between two queries rather than the semantic similarity between them.</p><p>So this is a really good use case for a scenario where we are a brand. And we want to say queries that include information about our brand are okay, we can talk about them. But queries about other brands, especially our competitors, we do not want to answer to them.</p><p>But at the same time, we still do want to include that semantic similarity. Just because someone says something about BYD or Tesla, it doesn't mean that it would be Inscope or our particular chatbot, which is something that semantic vectors can capture better. So we can merge these two methods and use a hybrid approach to create our guardrails.</p><p>Now we're going to be working through this example here. It's in the semantic router docs. And I'm going to go ahead and just open this in colab. And we can see the initial setup. And we're going to start by just installing the semantic router library. And then what we're going to do is create a set of routes.</p><p>So these are the routes that we would like to allow. Okay, so in this case, we are BYD and we want a customer facing chatbot that allows users to ask about our products. But then we want to block our competitors products from being spoken about. So in this case, we are looking at Tesla, Pulsar, and Rivian.</p><p>Now, all of the utterances that you see here are just example queries. So these are things that we could imagine users asking. And what we are essentially doing here is populating that semantic space, but also the sparse vector space. So we'll see that queries that mention BYD will share a high similarity to this route, whereas queries that include Tesla will share high similarity to this other route.</p><p>So let's go ahead and run those. We gather all those routes within a single routes variable here. Now we will need a OpenAI API key for this part here. And we can get that from platformopenai.com/apikeys. And we should find ourselves on this page. So we're going to go ahead and create a new key.</p><p>I'm just going to call this one the hybrid chat demo. Of course, you call it whatever you like. Whatever is useful to you. And we're just going to run this cell and enter our API key there. And what I just want to show you is I just want to show you some examples of similarity or semantic only similarity between various queries.</p><p>Okay. So in this one, we have can I start my Tesla, Pulsar, BYD, and Rivian. And you can see the similarity between these is like 0.65, 67, 69. Okay. So fairly high similarities for this embedding model. Now, what about if we just talk about BYD? All right. So we're asking various queries about BYD.</p><p>Let's just see how similar those are. Okay. And you can actually see overall the similarities between these queries here, which are all talking about BYD, are actually score lower than the similarity scores that we have from asking how to sell the various EVs. So this is a perfect scenario where we can't just rely on semantic embeddings.</p><p>And instead, we might want to bring in the hybrid approach. So to do that, we're going to be using a sparse encoder. And this will actually be using BM25, but it's a BM25 model that is compatible with VectorSearch. Not all of them are. And it has also been trained on a large internet scale dataset.</p><p>Now, what that does is essentially proxies using BM25 on your own dataset. Because in many cases, you might want to later train your own BM25 model on your own data. But you can actually use this and get pretty solid results out of the box. So for this, we will need another API key, which we get from platform.aurelio.ai.</p><p>You would need to create an account. Now, assuming that you haven't already used the Aurelio platform, you can get some free credits to follow along with me on this video. And to do that, we'd go to billing, add credits, put $5 for USD here. Go through to purchase, and then you just add your promotion code here.</p><p>And that will give you $5 of free credits. Once you have your credits, you can then go to settings, API keys, and create a new API key. I'm going to, again, just call this hybrid chat demo. I'm just going to copy that and use it here. Okay, great. So we have that, and we can now move on to setting up our hybrid router.</p><p>So we're using from semantic router routers. And rather than using the semantic router, which I'm sure probably a lot of you have used, we would be using the hybrid router. And the only real difference in terms of using the hybrid router is that we need to include this sparse encoder, which is, of course, is what we have initialized up here.</p><p>So we initialize our sparse encoder, pass it through to our hybrid router, and then we're actually good to go. Now, one unique thing with hybrid is that it's much harder to define what a good similarity threshold is. Because your similarity threshold is a merger between your dense semantic vector space and your sparse term matching vector space.</p><p>So we provide these default thresholds here, and these are mostly fine, depending on your use case. So in this use case, if we're just blocking mentions of these, but allowing mentions of these, and we don't really care about any other queries, this would actually probably be completely fine. But in the scenario where you have other conversations that you would allow without including them within our predefined set of utterances, this will not work as well.</p><p>So let me go through how we can optimize our routes and those specifically those route thresholds to support more use cases. So, okay, just to start with, if we are looking at very specific queries, okay, about Rivian, Pulsar, Tesla, BYD, we can actually get pretty good performance. Okay, so we can test that by creating a set of test data.</p><p>Okay, we have our utterance on the left and the target route on the right. So we create a list of tuples that looks like that. From that, we get our list of X, which is the queries. So these items here. And then we have our targets, a Y, which is BYD, Tesla, and so on.</p><p>Okay, and we evaluate our performance of our router on this test data set, which is a very small test data set. But nonetheless, you can see, okay, yeah, we get 100% accuracy. That's not too surprising, given we don't have many examples here. However, if we make that a little more complicated, so we add many more examples, so many more utterances here, they are a lot more complex.</p><p>So for example, BYD, we're including, you know, mentions about Tesla and other EVs. And we're just including far more utterance to route pairs here than we did before. So that obviously increases the scope of what we are testing, which is important. You should always have as big as possible a test data set.</p><p>So we have all these, and then we have also added these. So this is if we would like to allow our users to talk about other things, which in, honestly, in a lot of cases, we might actually not want to do that, but it makes it harder. So I do want to, in this example, show you how we do handle that.</p><p>So if we provide a non-route within our test data, the hybrid router is going to perceive that as, okay, this should not trigger any routes. And it should pass through the router without any issues. Okay, so we just have a lot of generic queries here. Like, how do I start a vegetable garden?</p><p>What was the best way to cook a steak? Who's the first person to walk on the moon? Like things that are nothing to do with our earlier queries. And it's also very important to include a lot of these. If you are aiming to build that sort of general purpose chat bot that will not be blocked on everything.</p><p>Okay. So we run that and we would pass that through to our evaluate method again. Before we do that, this is what X and Y look like. So what I mentioned before, X is our utterances. So the input data and Y includes our target routes. Okay. You can go all the way to the unknown.</p><p>We should see some of those nones as well. I'm not sure we need to do that. But if you like, you can set your thresholds here via the set threshold. Also, if you wanted to, you can set threshold for all of your routes like this. But I don't think we necessarily need to do that, to be honest.</p><p>So let me even show you. So we'll just stick with those default route thresholds and let's see what the accuracy is. Okay, pretty shocking to be honest, right? 51%. That's not great. Now, a big part of that is because these route thresholds here are so low that essentially all those non-test cases are going to be hitting like BYD or Tesla or Pulsar no matter what, even though they're completely unrelated.</p><p>Because this is an incredibly low similarity score. So anything with a high similarity than that is going to be triggering one of those routes. Which is fine if you're creating guardrails for everything. But it's not fine if you want to allow some pass through of unknown queries. So what we need to do is call the fit method.</p><p>And what this will do is based on our test data up here, it's going to say, okay, I'm going to find the optimal thresholds given your test data set. So we run that. And we can see already, so this actually runs for probably far longer than it needs to.</p><p>But you can see already that the accuracy here, it was a little lower at the start. It's increased and now it's at 94%. Now let's leave this to continue. We see it's increased up to 95, 96% now. We could possibly even leave it for longer, but that is pretty good.</p><p>Okay, so now let's see what our new thresholds are. So we have 0.57, 4.4 for Tesla. Pulsar, 3.4. And for Rivian, 71. Interesting. Okay, so these are thresholds that the hybrid router has identified as being the best performing thresholds for our test data set. And the accuracy, whereas before we had 51%, has now increased up to 96%, which is a huge improvement.</p><p>And we can also evaluate that again. So it's 95.61% now, which, yeah, great improvement. And it was not really that hard to do. Now, that is with a relatively small data set here. This is not huge. As I said, the bigger, the better. But what you can also do, which I like to do anyway, is you can go to like ChatGPT or some other LLM and just ask it.</p><p>Okay, these are my scenarios I want to cover. These are scenarios I want to let through. Can you create a set, a big training data set for me? And it's actually pretty good at doing that. So I wouldn't rely solely on that. But you can at least fill up that data set quite easily by doing so.</p><p>So with that, we can ask some other queries. Like, okay, can I buy a Tesla from you? Okay, I don't think that was in the train set, but I'm not sure. And we see, okay, route choice is Tesla. Okay, that's great. Now I can say, okay, something random. How much is a flight to Australia from Europe?</p><p>Very generic, but fine. And we see that our route choice is none, so no guardrails were triggered. Okay, so that looks to be working pretty well. Now, that is actually it for this video. This is, as I mentioned at the start, this is one component of what I think should be, you should have multiple layers of guardrails, especially for anything that you're putting in production.</p><p>That should be, as we've seen here, essentially the input guardrails using hybrid routes. But you should also, of course, cover, okay, what's coming out from your LLM. You should be covering your LLM prompting guardrails and putting in various other safety components to ensure that, okay, people aren't going to be misusing whatever it is that you're building.</p><p>But as you can see, you can get pretty far already with these hybrid routers. And they're insanely, just insanely fast and also insanely cost effective compared to, let's say, just using their LLM as your guardrails, whilst also providing that extra layer of safety. Now, that is it for this video.</p><p>I hope all of this has been useful and interesting. But for now, I'll leave there. So, thank you very much for watching. And I will see you again in the next one. Bye. Bye.</p></div></div></body></html>