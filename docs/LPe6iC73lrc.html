<html><head><title>2024 in Post-Transformer Architectures: State Space Models, RWKV [Latent Space LIVE! @ NeurIPS 2024]</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>2024 in Post-Transformer Architectures: State Space Models, RWKV [Latent Space LIVE! @ NeurIPS 2024]</h2><a href="https://www.youtube.com/watch?v=LPe6iC73lrc"><img src="https://i.ytimg.com/vi/LPe6iC73lrc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./LPe6iC73lrc.html">Whisper Transcript</a> | <a href="./transcript_LPe6iC73lrc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">(upbeat music)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2" target="_blank">00:00:02.580</a></span> | <span class="t">- Yeah, so thanks so much for having us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=8" target="_blank">00:00:08.620</a></span> | <span class="t">So this is gonna be a little bit of a two-part presentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=11" target="_blank">00:00:11.600</a></span> | <span class="t">My name is Dan, I'm at Together AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=14" target="_blank">00:00:14.240</a></span> | <span class="t">and I'll be joining UCSD as faculty in about a year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=17" target="_blank">00:00:17.840</a></span> | <span class="t">And Eugene, you wanna introduce yourself?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=19" target="_blank">00:00:19.960</a></span> | <span class="t">- I'm Eugene, I lead the art activity team,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=22" target="_blank">00:00:22.000</a></span> | <span class="t">and I'm CEO and co-founder of Featherless,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=25" target="_blank">00:00:25.120</a></span> | <span class="t">and we both work on this new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=27" target="_blank">00:00:27.480</a></span> | <span class="t">post-transformer architecture space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=29" target="_blank">00:00:29.740</a></span> | <span class="t">- Yeah, so today, we're really excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">to talk to you a little bit about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=35" target="_blank">00:00:35.600</a></span> | <span class="t">So first, I'm gonna give a broad overview</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=37" target="_blank">00:00:37.800</a></span> | <span class="t">of kind of the last few years of progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=40" target="_blank">00:00:40.380</a></span> | <span class="t">in non-post-transformer architectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=42" target="_blank">00:00:42.900</a></span> | <span class="t">and then afterwards, Eugene will tell us a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=45" target="_blank">00:00:45.960</a></span> | <span class="t">about the latest and the greatest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=47" target="_blank">00:00:47.440</a></span> | <span class="t">and the latest frontier models in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=50" target="_blank">00:00:50.620</a></span> | <span class="t">So the story starts with scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=54" target="_blank">00:00:54.280</a></span> | <span class="t">So this is probably a figure or something like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=56" target="_blank">00:00:56.880</a></span> | <span class="t">that you've seen very recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=59" target="_blank">00:00:59.120</a></span> | <span class="t">Over the last five to six years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=60" target="_blank">00:01:00.920</a></span> | <span class="t">we've seen models really scale up in parameter size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=63" target="_blank">00:01:03.640</a></span> | <span class="t">and that's brought with it a bunch of new capabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=65" target="_blank">00:01:05.640</a></span> | <span class="t">like the ability to talk to you and tell you sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=69" target="_blank">00:01:09.700</a></span> | <span class="t">how to use your Colab and your AWS screens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=72" target="_blank">00:01:12.800</a></span> | <span class="t">But another place where we've seen scaling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=75" target="_blank">00:01:15.480</a></span> | <span class="t">especially recently, is scaling in context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=78" target="_blank">00:01:18.680</a></span> | <span class="t">So this can mean just having more text inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=81" target="_blank">00:01:21.920</a></span> | <span class="t">for your models, but it can also mean things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=83" target="_blank">00:01:23.880</a></span> | <span class="t">like taking a lot of visual token inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=88" target="_blank">00:01:28.000</a></span> | <span class="t">image inputs to your models, or generating lots of outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=91" target="_blank">00:01:31.520</a></span> | <span class="t">And one thing that's been really exciting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=94" target="_blank">00:01:34.080</a></span> | <span class="t">over the last few months or so is that we're seeing scaling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=97" target="_blank">00:01:37.680</a></span> | <span class="t">not only during training time, but also during test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=99" target="_blank">00:01:39.920</a></span> | <span class="t">So this is one of the, this is the iconic image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=103" target="_blank">00:01:43.240</a></span> | <span class="t">from the OpenAI '01 release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=105" target="_blank">00:01:45.280</a></span> | <span class="t">Not only are we starting to scale train time compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=107" target="_blank">00:01:47.840</a></span> | <span class="t">but we're also starting to scale test time compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=111" target="_blank">00:01:51.160</a></span> | <span class="t">Now, if you're familiar with our attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=113" target="_blank">00:01:53.880</a></span> | <span class="t">in our transformer architectures today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=115" target="_blank">00:01:55.820</a></span> | <span class="t">this graph on the right might look a little bit scary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=118" target="_blank">00:01:58.640</a></span> | <span class="t">And one of the reasons is that the implications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=122" target="_blank">00:02:02.280</a></span> | <span class="t">are a little bit interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=124" target="_blank">00:02:04.920</a></span> | <span class="t">So what does it mean if we want to continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=126" target="_blank">00:02:06.960</a></span> | <span class="t">having smarter and smarter models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=128" target="_blank">00:02:08.680</a></span> | <span class="t">Do we just need to start building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=129" target="_blank">00:02:09.960</a></span> | <span class="t">bigger, bigger data centers, spending more flops?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=133" target="_blank">00:02:13.320</a></span> | <span class="t">Is this, this little Dolly 3, we need more flops guy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=136" target="_blank">00:02:16.800</a></span> | <span class="t">is this gonna be the future of all of AI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=140" target="_blank">00:02:20.720</a></span> | <span class="t">Or is there a better way, another path forward?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=143" target="_blank">00:02:23.960</a></span> | <span class="t">Maybe we can get the same capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=146" target="_blank">00:02:26.300</a></span> | <span class="t">that we've gotten used to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=147" target="_blank">00:02:27.900</a></span> | <span class="t">but for a lot less compute, a lot less flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=150" target="_blank">00:02:30.160</a></span> | <span class="t">And one of the things that we're gonna talk about today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=153" target="_blank">00:02:33.300</a></span> | <span class="t">is specifically looking at that core attention operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=157" target="_blank">00:02:37.140</a></span> | <span class="t">in some of these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=158" target="_blank">00:02:38.820</a></span> | <span class="t">And the reason is that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=160" target="_blank">00:02:40.340</a></span> | <span class="t">so this is just some basic scaling curves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=164" target="_blank">00:02:44.100</a></span> | <span class="t">but attention has compute that scales quadratically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=167" target="_blank">00:02:47.140</a></span> | <span class="t">in the context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=168" target="_blank">00:02:48.480</a></span> | <span class="t">So that means that if you're doing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=170" target="_blank">00:02:50.020</a></span> | <span class="t">like test time compute, and you want to spend a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=172" target="_blank">00:02:52.580</a></span> | <span class="t">of tokens thinking about what comes next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=174" target="_blank">00:02:54.820</a></span> | <span class="t">the longer that that goes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=176" target="_blank">00:02:56.900</a></span> | <span class="t">the more tokens you spend on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=180" target="_blank">00:03:00.160</a></span> | <span class="t">that compute grows quadratically in that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=182" target="_blank">00:03:02.380</a></span> | <span class="t">One of the questions that we're interested in is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=184" target="_blank">00:03:04.860</a></span> | <span class="t">can we take that basic sequence model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=186" target="_blank">00:03:06.900</a></span> | <span class="t">the basic sequence primitive at the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=188" target="_blank">00:03:08.900</a></span> | <span class="t">and get it to scale better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=190" target="_blank">00:03:10.260</a></span> | <span class="t">Can we scale and let's say N to the three halves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=192" target="_blank">00:03:12.580</a></span> | <span class="t">or N log N?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=193" target="_blank">00:03:13.660</a></span> | <span class="t">And so in the first part of the talk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=197" target="_blank">00:03:17.220</a></span> | <span class="t">so we just went over the introduction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=198" target="_blank">00:03:18.940</a></span> | <span class="t">what I'm gonna do over the next few slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=200" target="_blank">00:03:20.560</a></span> | <span class="t">is just talk about some of the key advances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=202" target="_blank">00:03:22.860</a></span> | <span class="t">and ideas that have shown over the past few years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=205" target="_blank">00:03:25.840</a></span> | <span class="t">since maybe early 2020 to now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=209" target="_blank">00:03:29.140</a></span> | <span class="t">that shown promise that this might actually be possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=212" target="_blank">00:03:32.080</a></span> | <span class="t">that you can actually get potentially the same quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=214" target="_blank">00:03:34.460</a></span> | <span class="t">that we want while scaling better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=217" target="_blank">00:03:37.480</a></span> | <span class="t">So to do that, and basically the story</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=222" target="_blank">00:03:42.480</a></span> | <span class="t">that we're gonna look is we're gonna start to see how,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=225" target="_blank">00:03:45.020</a></span> | <span class="t">so this is a basic graph of just the past couple of years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=228" target="_blank">00:03:48.220</a></span> | <span class="t">of progress of perplexity where that blue line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=231" target="_blank">00:03:51.100</a></span> | <span class="t">that dotted blue line is attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=232" target="_blank">00:03:52.580</a></span> | <span class="t">it's your basic transformer, full dense attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=235" target="_blank">00:03:55.100</a></span> | <span class="t">And then the dots coming down are some of the methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=238" target="_blank">00:03:58.480</a></span> | <span class="t">that you'll see in this presentation today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=240" target="_blank">00:04:00.640</a></span> | <span class="t">We're gonna turn the clock back all the way to 2020.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=244" target="_blank">00:04:04.460</a></span> | <span class="t">So this question of, can we make attention sub-quadratic?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=249" target="_blank">00:04:09.180</a></span> | <span class="t">Basically, as soon as we said, attention is all you need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=251" target="_blank">00:04:11.940</a></span> | <span class="t">people started asking this question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=253" target="_blank">00:04:13.420</a></span> | <span class="t">So we have this quadratic attention operator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=256" target="_blank">00:04:16.260</a></span> | <span class="t">can we do better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=257" target="_blank">00:04:17.460</a></span> | <span class="t">I'll briefly talk about why attention is quadratic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=259" target="_blank">00:04:19.860</a></span> | <span class="t">And the basic thing that happens if you're not familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=263" target="_blank">00:04:23.020</a></span> | <span class="t">is that you have these inputs, these keys and queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=265" target="_blank">00:04:25.860</a></span> | <span class="t">and what you do in this attention matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=267" target="_blank">00:04:27.740</a></span> | <span class="t">this S matrix over here is that you're using,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=270" target="_blank">00:04:30.320</a></span> | <span class="t">you're comparing every token in your input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=272" target="_blank">00:04:32.500</a></span> | <span class="t">to every other token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=273" target="_blank">00:04:33.860</a></span> | <span class="t">So when I try to do something like upload a whole book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=276" target="_blank">00:04:36.340</a></span> | <span class="t">to Gemini, what happens beyond the, or maybe not Gemini,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=279" target="_blank">00:04:39.620</a></span> | <span class="t">'cause we don't necessarily know what architecture is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=281" target="_blank">00:04:41.680</a></span> | <span class="t">but let's say we upload it to Llama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=283" target="_blank">00:04:43.580</a></span> | <span class="t">what happens behind the scenes is that it's gonna take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=286" target="_blank">00:04:46.980</a></span> | <span class="t">every single word in that book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=288" target="_blank">00:04:48.300</a></span> | <span class="t">and compare it to every other word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=290" target="_blank">00:04:50.060</a></span> | <span class="t">And this has been a really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=291" target="_blank">00:04:51.500</a></span> | <span class="t">it's led to some pretty impressive things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=293" target="_blank">00:04:53.940</a></span> | <span class="t">but it's kind of a brute forcing of the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=296" target="_blank">00:04:56.060</a></span> | <span class="t">that you would try to interpret something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=299" target="_blank">00:04:59.620</a></span> | <span class="t">And what attention does in particular is the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=303" target="_blank">00:05:03.020</a></span> | <span class="t">and then what attention, sorry, don't wanna, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=306" target="_blank">00:05:06.780</a></span> | <span class="t">No, no laser pointer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=308" target="_blank">00:05:08.020</a></span> | <span class="t">What attention does afterwards is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=310" target="_blank">00:05:10.180</a></span> | <span class="t">instead of always operating in this quadratic thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=313" target="_blank">00:05:13.060</a></span> | <span class="t">it takes a row-wise softmax over this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=315" target="_blank">00:05:15.700</a></span> | <span class="t">and then multiplies it by this values matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=317" target="_blank">00:05:17.660</a></span> | <span class="t">So one of the key points to notice is that the output size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=321" target="_blank">00:05:21.100</a></span> | <span class="t">is always gonna be the same as the inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=323" target="_blank">00:05:23.360</a></span> | <span class="t">at least in standard self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=326" target="_blank">00:05:26.340</a></span> | <span class="t">So one of the first things that folks tried to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=328" target="_blank">00:05:28.340</a></span> | <span class="t">around 2020 is this thing called linear attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=330" target="_blank">00:05:30.500</a></span> | <span class="t">which is just noticing that if we take out this softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=334" target="_blank">00:05:34.100</a></span> | <span class="t">from here, if we take out this non-linearity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=336" target="_blank">00:05:36.100</a></span> | <span class="t">in the middle of the attention operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=337" target="_blank">00:05:37.900</a></span> | <span class="t">and then if you compute the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=339" target="_blank">00:05:39.400</a></span> | <span class="t">and the values operation first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=341" target="_blank">00:05:41.160</a></span> | <span class="t">you actually never hit this quadratic bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=344" target="_blank">00:05:44.060</a></span> | <span class="t">So that's potentially a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=346" target="_blank">00:05:46.300</a></span> | <span class="t">to get a lot more computationally efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=350" target="_blank">00:05:50.540</a></span> | <span class="t">And there are various ways to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=352" target="_blank">00:05:52.420</a></span> | <span class="t">by basically using feature maps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=354" target="_blank">00:05:54.060</a></span> | <span class="t">or try to approximate this overall attention computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=357" target="_blank">00:05:57.620</a></span> | <span class="t">But some of this work sort of started to hit a wall in 2020</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=362" target="_blank">00:06:02.060</a></span> | <span class="t">and the basic challenges were two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=364" target="_blank">00:06:04.220</a></span> | <span class="t">So one was quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=365" target="_blank">00:06:05.600</a></span> | <span class="t">Back then it was kind of hard to get good quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=369" target="_blank">00:06:09.580</a></span> | <span class="t">with these linear attention operators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=371" target="_blank">00:06:11.620</a></span> | <span class="t">The other one was actually hardware efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=373" target="_blank">00:06:13.460</a></span> | <span class="t">So this feature map that was just shown by Simplify here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=378" target="_blank">00:06:18.260</a></span> | <span class="t">actually ends up being quite computationally expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=380" target="_blank">00:06:20.980</a></span> | <span class="t">if you just implement it naively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=382" target="_blank">00:06:22.980</a></span> | <span class="t">So you started having these operators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=384" target="_blank">00:06:24.440</a></span> | <span class="t">that not only you're not really sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=387" target="_blank">00:06:27.300</a></span> | <span class="t">if they have the same quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=388" target="_blank">00:06:28.820</a></span> | <span class="t">but also they're actually just wall clock slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=390" target="_blank">00:06:30.740</a></span> | <span class="t">So you kind of end up getting the worst of both worlds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=394" target="_blank">00:06:34.340</a></span> | <span class="t">So this was the SAGE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=396" target="_blank">00:06:36.620</a></span> | <span class="t">So that kind of sets the SAGE for four years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=398" target="_blank">00:06:38.900</a></span> | <span class="t">Keep this in mind because linear attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=400" target="_blank">00:06:40.460</a></span> | <span class="t">is actually gonna come back in a few years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=403" target="_blank">00:06:43.020</a></span> | <span class="t">once we have a better understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=406" target="_blank">00:06:46.260</a></span> | <span class="t">But one of the works that started kicking off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=408" target="_blank">00:06:48.340</a></span> | <span class="t">this mini revolution in post-transformer architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=412" target="_blank">00:06:52.740</a></span> | <span class="t">was this idea called state-space model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=414" target="_blank">00:06:54.680</a></span> | <span class="t">So here the seminal work is one about our work in 2022.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=419" target="_blank">00:06:59.500</a></span> | <span class="t">And this piece of work really brought together a few ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=423" target="_blank">00:07:03.420</a></span> | <span class="t">from some long running research lines of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=430" target="_blank">00:07:10.460</a></span> | <span class="t">The first one was, and this is really one of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=433" target="_blank">00:07:13.660</a></span> | <span class="t">to closing the gap in quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=436" target="_blank">00:07:16.420</a></span> | <span class="t">was just using things that if you talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=439" target="_blank">00:07:19.820</a></span> | <span class="t">to an electrical engineer off the street,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=443" target="_blank">00:07:23.100</a></span> | <span class="t">they might know like the back of their hand,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=446" target="_blank">00:07:26.980</a></span> | <span class="t">but taking some of those properties</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=448" target="_blank">00:07:28.360</a></span> | <span class="t">with how we model dynamical systems in signal processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=452" target="_blank">00:07:32.660</a></span> | <span class="t">and then using those ideas to model the inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=456" target="_blank">00:07:36.020</a></span> | <span class="t">the text tokens in, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=459" target="_blank">00:07:39.060</a></span> | <span class="t">a transformer-like next token prediction architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=462" target="_blank">00:07:42.300</a></span> | <span class="t">So some of those early state-space model papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=464" target="_blank">00:07:44.780</a></span> | <span class="t">were looking at this relatively simple recurrent update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=469" target="_blank">00:07:49.140</a></span> | <span class="t">model that comes from maybe chapter one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=470" target="_blank">00:07:50.960</a></span> | <span class="t">of a signal processing class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=473" target="_blank">00:07:53.100</a></span> | <span class="t">but then using some principle theory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=475" target="_blank">00:07:55.680</a></span> | <span class="t">about how you should do that recurrent update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=478" target="_blank">00:07:58.340</a></span> | <span class="t">in order to really get the most that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=481" target="_blank">00:08:01.560</a></span> | <span class="t">out of your hidden state, out of your sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=485" target="_blank">00:08:05.860</a></span> | <span class="t">So that was one key idea for quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=487" target="_blank">00:08:07.900</a></span> | <span class="t">And when this was eventually realized,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=491" target="_blank">00:08:11.300</a></span> | <span class="t">you started to see a bunch of benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=493" target="_blank">00:08:13.060</a></span> | <span class="t">that were pretty sticky for a few years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=495" target="_blank">00:08:15.220</a></span> | <span class="t">things like long range arena,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=496" target="_blank">00:08:16.620</a></span> | <span class="t">some long sequence evaluation benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=499" target="_blank">00:08:19.980</a></span> | <span class="t">there were stuff in time series, time series analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=504" target="_blank">00:08:24.420</a></span> | <span class="t">You started to see the quality tick up in meaningful ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=509" target="_blank">00:08:29.300</a></span> | <span class="t">But the other key thing that was so influential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=513" target="_blank">00:08:33.820</a></span> | <span class="t">about these state-space models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=514" target="_blank">00:08:34.940</a></span> | <span class="t">is that they also had a key idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=516" target="_blank">00:08:36.780</a></span> | <span class="t">about how you can compute these things efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=521" target="_blank">00:08:41.020</a></span> | <span class="t">So if you go back to your machine learning 101 class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=523" target="_blank">00:08:43.460</a></span> | <span class="t">where you learned about RNNs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=524" target="_blank">00:08:44.780</a></span> | <span class="t">one thing that you may have learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=526" target="_blank">00:08:46.020</a></span> | <span class="t">is that they don't paralyze as well as detention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=528" target="_blank">00:08:48.980</a></span> | <span class="t">because if you just run them naively,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=531" target="_blank">00:08:51.060</a></span> | <span class="t">you have to do this kind of sequential update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=534" target="_blank">00:08:54.140</a></span> | <span class="t">to process new tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=535" target="_blank">00:08:55.620</a></span> | <span class="t">Whereas in attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=536" target="_blank">00:08:56.840</a></span> | <span class="t">you can process all the tokens in parallel at one time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=540" target="_blank">00:09:00.120</a></span> | <span class="t">One of the key insights behind the S4 paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=542" target="_blank">00:09:02.460</a></span> | <span class="t">was that these recurrent models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=544" target="_blank">00:09:04.220</a></span> | <span class="t">you could take them and you could also formulate them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=547" target="_blank">00:09:07.060</a></span> | <span class="t">as a convolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=548" target="_blank">00:09:08.420</a></span> | <span class="t">And in particular, with a convolution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=549" target="_blank">00:09:09.780</a></span> | <span class="t">you could, instead of using a PyTorch conv1d operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=552" target="_blank">00:09:12.540</a></span> | <span class="t">you can compute that with the FFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=555" target="_blank">00:09:15.060</a></span> | <span class="t">And that would give you N log N compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=557" target="_blank">00:09:17.380</a></span> | <span class="t">in the sequence length N with a operator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=560" target="_blank">00:09:20.820</a></span> | <span class="t">that was relatively well optimized for modern hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=564" target="_blank">00:09:24.460</a></span> | <span class="t">So those are really, I'd say the two key ideas in 2022</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=568" target="_blank">00:09:28.380</a></span> | <span class="t">that started allowing these breakthroughs to happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=571" target="_blank">00:09:31.740</a></span> | <span class="t">in these non-transformer architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=573" target="_blank">00:09:33.700</a></span> | <span class="t">So these ideas about how to principally model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=576" target="_blank">00:09:36.780</a></span> | <span class="t">sorry, how to model the recurrent updates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=579" target="_blank">00:09:39.020</a></span> | <span class="t">of a sequence in a principled way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=582" target="_blank">00:09:42.500</a></span> | <span class="t">and also these key ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=583" target="_blank">00:09:43.740</a></span> | <span class="t">and how you can compute it efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=585" target="_blank">00:09:45.780</a></span> | <span class="t">by turning it into a convolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=587" target="_blank">00:09:47.860</a></span> | <span class="t">and then scaling it up with the FFT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=590" target="_blank">00:09:50.040</a></span> | <span class="t">Along those same lines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=593" target="_blank">00:09:53.580</a></span> | <span class="t">so afterwards, we started putting out some work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=597" target="_blank">00:09:57.500</a></span> | <span class="t">on specialized kernels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=598" target="_blank">00:09:58.700</a></span> | <span class="t">So just like we have flash attention for transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=601" target="_blank">00:10:01.320</a></span> | <span class="t">we also have works like flash FFT conv,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=603" target="_blank">00:10:03.500</a></span> | <span class="t">and if you look at these lines of work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=605" target="_blank">00:10:05.620</a></span> | <span class="t">oftentimes whenever you see a new architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=607" target="_blank">00:10:07.940</a></span> | <span class="t">you see a new primitive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=609" target="_blank">00:10:09.540</a></span> | <span class="t">one of the table stakes now is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=611" target="_blank">00:10:11.620</a></span> | <span class="t">do you have an efficient kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=612" target="_blank">00:10:12.740</a></span> | <span class="t">so that you can actually get wall clock speed up?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=614" target="_blank">00:10:14.780</a></span> | <span class="t">So by 2022, 2023, we were starting to have these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=618" target="_blank">00:10:18.460</a></span> | <span class="t">that had promising quality primitives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=621" target="_blank">00:10:21.380</a></span> | <span class="t">and also promising wall clocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=623" target="_blank">00:10:23.180</a></span> | <span class="t">So you could actually see regimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=624" target="_blank">00:10:24.980</a></span> | <span class="t">where they were better than transformers in meaningful ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=627" target="_blank">00:10:27.980</a></span> | <span class="t">That being said, there were still sometimes a quality gap,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=631" target="_blank">00:10:31.860</a></span> | <span class="t">particularly for language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=633" target="_blank">00:10:33.580</a></span> | <span class="t">And because language is so core to what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=636" target="_blank">00:10:36.540</a></span> | <span class="t">in sequence modeling these days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=638" target="_blank">00:10:38.220</a></span> | <span class="t">the next key idea that I'm gonna talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=641" target="_blank">00:10:41.500</a></span> | <span class="t">is this idea of selection mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=643" target="_blank">00:10:43.860</a></span> | <span class="t">And this is basically an idea of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=645" target="_blank">00:10:45.940</a></span> | <span class="t">so you have this recurrent state that you're keeping around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=648" target="_blank">00:10:48.600</a></span> | <span class="t">that just summarizes everything that came before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=652" target="_blank">00:10:52.140</a></span> | <span class="t">and to get a good sequence model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=653" target="_blank">00:10:53.620</a></span> | <span class="t">one of the things that you really need to be able to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=656" target="_blank">00:10:56.060</a></span> | <span class="t">is have the model learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=658" target="_blank">00:10:58.020</a></span> | <span class="t">what's the best way to pick out pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=660" target="_blank">00:11:00.300</a></span> | <span class="t">from that recurrent state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=662" target="_blank">00:11:02.100</a></span> | <span class="t">So one of the major ideas here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=664" target="_blank">00:11:04.800</a></span> | <span class="t">in a line of work called H3, Hungry, Hungry Hippos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=667" target="_blank">00:11:07.860</a></span> | <span class="t">and also these hyena models were,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=671" target="_blank">00:11:11.060</a></span> | <span class="t">one way you can do this is by just adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=672" target="_blank">00:11:12.860</a></span> | <span class="t">some simple element-wise gates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=675" target="_blank">00:11:15.580</a></span> | <span class="t">So versions of these ideas have been around for decades.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=678" target="_blank">00:11:18.780</a></span> | <span class="t">If you squint at the LSTM paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=681" target="_blank">00:11:21.580</a></span> | <span class="t">you can probably find this gating mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=684" target="_blank">00:11:24.740</a></span> | <span class="t">But turns out you can take those old ideas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=686" target="_blank">00:11:26.540</a></span> | <span class="t">add them into these new states-based models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=689" target="_blank">00:11:29.420</a></span> | <span class="t">and then you can see quality start to pick up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=692" target="_blank">00:11:32.660</a></span> | <span class="t">If you've heard of the Mamba model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=695" target="_blank">00:11:35.940</a></span> | <span class="t">this also takes the selection to the next level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=699" target="_blank">00:11:39.200</a></span> | <span class="t">by actually making some changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=700" target="_blank">00:11:40.660</a></span> | <span class="t">in that fundamental recurrent state space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=703" target="_blank">00:11:43.700</a></span> | <span class="t">So it's not only just this gating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=705" target="_blank">00:11:45.540</a></span> | <span class="t">that happens around the SSM layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=707" target="_blank">00:11:47.620</a></span> | <span class="t">but also you can actually make the ABCD matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=711" target="_blank">00:11:51.840</a></span> | <span class="t">of your state-based model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=713" target="_blank">00:11:53.100</a></span> | <span class="t">you can make them data-dependent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=714" target="_blank">00:11:54.860</a></span> | <span class="t">which will allow you to even better select out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=717" target="_blank">00:11:57.420</a></span> | <span class="t">different pieces from your hidden state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=719" target="_blank">00:11:59.120</a></span> | <span class="t">depending on what you're seeing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=720" target="_blank">00:12:00.720</a></span> | <span class="t">I'll also point out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=722" target="_blank">00:12:02.420</a></span> | <span class="t">if you look at the bottom right of this figure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=723" target="_blank">00:12:03.980</a></span> | <span class="t">there's this little triangle with a GPU SRAM, GPU HBM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=727" target="_blank">00:12:07.380</a></span> | <span class="t">and this is just continuing that trend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=729" target="_blank">00:12:09.460</a></span> | <span class="t">of when you have a new architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=732" target="_blank">00:12:12.140</a></span> | <span class="t">you also release it with a kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=734" target="_blank">00:12:14.580</a></span> | <span class="t">to show that it is hardware efficient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=736" target="_blank">00:12:16.940</a></span> | <span class="t">that it can be hardware efficient on modern hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=740" target="_blank">00:12:20.480</a></span> | <span class="t">One of the next cool things that happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=746" target="_blank">00:12:26.500</a></span> | <span class="t">is once we had this understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=748" target="_blank">00:12:28.340</a></span> | <span class="t">of these are the basic pieces,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=750" target="_blank">00:12:30.380</a></span> | <span class="t">these are the basic principles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=751" target="_blank">00:12:31.800</a></span> | <span class="t">behind some of the sequence models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=754" target="_blank">00:12:34.320</a></span> | <span class="t">linear attention actually started to come back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=756" target="_blank">00:12:36.160</a></span> | <span class="t">So in earlier this year,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=758" target="_blank">00:12:38.120</a></span> | <span class="t">there's a model called BASED from Simran Arora</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=761" target="_blank">00:12:41.800</a></span> | <span class="t">and some other folks that combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=764" target="_blank">00:12:44.600</a></span> | <span class="t">a more principled version of linear attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=766" target="_blank">00:12:46.920</a></span> | <span class="t">that basically the two-second summaries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=770" target="_blank">00:12:50.680</a></span> | <span class="t">that it used a Taylor approximation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=772" target="_blank">00:12:52.860</a></span> | <span class="t">of the softmax attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=774" target="_blank">00:12:54.600</a></span> | <span class="t">combined that with a simple sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=777" target="_blank">00:12:57.200</a></span> | <span class="t">and was starting to be able to expand the Pareto frontier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=781" target="_blank">00:13:01.540</a></span> | <span class="t">of how much data can you recall from your sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=784" target="_blank">00:13:04.820</a></span> | <span class="t">versus how small is your recurrent state size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=787" target="_blank">00:13:07.220</a></span> | <span class="t">So those orange dots are at the top there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=789" target="_blank">00:13:09.500</a></span> | <span class="t">are just showing smaller sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=792" target="_blank">00:13:12.020</a></span> | <span class="t">that can recall more memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=794" target="_blank">00:13:14.160</a></span> | <span class="t">And the last major idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=798" target="_blank">00:13:18.460</a></span> | <span class="t">I think that has been influential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=799" target="_blank">00:13:19.980</a></span> | <span class="t">on this line of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=800" target="_blank">00:13:20.820</a></span> | <span class="t">and is very relatively late breaking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=802" target="_blank">00:13:22.660</a></span> | <span class="t">just a few months ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=804" target="_blank">00:13:24.360</a></span> | <span class="t">is just the basic idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=805" target="_blank">00:13:25.660</a></span> | <span class="t">that when you have these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=807" target="_blank">00:13:27.380</a></span> | <span class="t">that are fundamentally more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=809" target="_blank">00:13:29.900</a></span> | <span class="t">in the sequence length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=811" target="_blank">00:13:31.420</a></span> | <span class="t">you maybe don't want to prompt them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=812" target="_blank">00:13:32.980</a></span> | <span class="t">or use them in exactly the same way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=815" target="_blank">00:13:35.020</a></span> | <span class="t">So this was a really cool paper called Just Read Twice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=817" target="_blank">00:13:37.740</a></span> | <span class="t">also from Simran that basically said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=820" target="_blank">00:13:40.620</a></span> | <span class="t">hey, all these efficient models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=822" target="_blank">00:13:42.540</a></span> | <span class="t">can process tokens so much more efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=824" target="_blank">00:13:44.500</a></span> | <span class="t">than transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=825" target="_blank">00:13:45.700</a></span> | <span class="t">that they can sometimes have unfair advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=828" target="_blank">00:13:48.480</a></span> | <span class="t">compared to a simple transformer token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=831" target="_blank">00:13:51.540</a></span> | <span class="t">So, sorry, a simple transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=833" target="_blank">00:13:53.500</a></span> | <span class="t">So take, for example, the standard use case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=837" target="_blank">00:13:57.100</a></span> | <span class="t">of you have some long document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=838" target="_blank">00:13:58.740</a></span> | <span class="t">you're gonna pass it in as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=840" target="_blank">00:14:00.060</a></span> | <span class="t">and then you're gonna ask some question about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=843" target="_blank">00:14:03.060</a></span> | <span class="t">One problem you might imagine for a recurrent model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=846" target="_blank">00:14:06.740</a></span> | <span class="t">where you have a fixed state size is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=848" target="_blank">00:14:08.660</a></span> | <span class="t">let's say that your article is very long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=851" target="_blank">00:14:11.580</a></span> | <span class="t">and you're trying to ask about some really niche thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=854" target="_blank">00:14:14.900</a></span> | <span class="t">You can imagine it might be hard for the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=856" target="_blank">00:14:16.660</a></span> | <span class="t">to know ahead of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=857" target="_blank">00:14:17.540</a></span> | <span class="t">what information to put into the hidden state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=860" target="_blank">00:14:20.520</a></span> | <span class="t">But these models are so much more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=863" target="_blank">00:14:23.020</a></span> | <span class="t">that you can do something really stupid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=864" target="_blank">00:14:24.520</a></span> | <span class="t">like you can just put the document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=866" target="_blank">00:14:26.940</a></span> | <span class="t">write down the document, write down the question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=869" target="_blank">00:14:29.060</a></span> | <span class="t">write down the document again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=870" target="_blank">00:14:30.300</a></span> | <span class="t">and then write down the question again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=871" target="_blank">00:14:31.900</a></span> | <span class="t">And then this time, the second time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=873" target="_blank">00:14:33.300</a></span> | <span class="t">that you go over that document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=874" target="_blank">00:14:34.500</a></span> | <span class="t">you know exactly what to look for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=876" target="_blank">00:14:36.920</a></span> | <span class="t">And the cool thing about this is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=878" target="_blank">00:14:38.300</a></span> | <span class="t">so this results in better quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=881" target="_blank">00:14:41.020</a></span> | <span class="t">especially on these recall intensive tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=883" target="_blank">00:14:43.700</a></span> | <span class="t">But the other interesting thing is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=885" target="_blank">00:14:45.660</a></span> | <span class="t">it really takes advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=887" target="_blank">00:14:47.140</a></span> | <span class="t">of the more efficient architectures that we're having here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=890" target="_blank">00:14:50.680</a></span> | <span class="t">So one of the other, I think, influential ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=893" target="_blank">00:14:53.100</a></span> | <span class="t">in this line of work is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=894" target="_blank">00:14:54.580</a></span> | <span class="t">if you change the fundamental compute capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=898" target="_blank">00:14:58.260</a></span> | <span class="t">of your model and the way that it scales,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=900" target="_blank">00:15:00.260</a></span> | <span class="t">you can actually start to query it at test time differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=903" target="_blank">00:15:03.100</a></span> | <span class="t">And this actually, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=904" target="_blank">00:15:04.260</a></span> | <span class="t">goes back to those slides on test time compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=907" target="_blank">00:15:07.160</a></span> | <span class="t">So while everybody's looking at, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=909" target="_blank">00:15:09.020</a></span> | <span class="t">test time compute for big transformer models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=912" target="_blank">00:15:12.340</a></span> | <span class="t">I think potentially a really interesting research question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=914" target="_blank">00:15:14.660</a></span> | <span class="t">is how can you take those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=916" target="_blank">00:15:16.040</a></span> | <span class="t">and how does it change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=917" target="_blank">00:15:17.300</a></span> | <span class="t">with this new next generation of models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=920" target="_blank">00:15:20.560</a></span> | <span class="t">So I'll just briefly summarize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=923" target="_blank">00:15:23.620</a></span> | <span class="t">what some of those key ideas were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=925" target="_blank">00:15:25.780</a></span> | <span class="t">and then talk and then show you briefly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=927" target="_blank">00:15:27.720</a></span> | <span class="t">kind of what the state of the art is today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=930" target="_blank">00:15:30.440</a></span> | <span class="t">So the four key ideas are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=932" target="_blank">00:15:32.120</a></span> | <span class="t">instead of just doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=933" target="_blank">00:15:33.160</a></span> | <span class="t">a simple linear attention approximation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=935" target="_blank">00:15:35.800</a></span> | <span class="t">instead, take ideas that we know from other fields,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=939" target="_blank">00:15:39.120</a></span> | <span class="t">like signal processing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=940" target="_blank">00:15:40.480</a></span> | <span class="t">do a more principled approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=942" target="_blank">00:15:42.600</a></span> | <span class="t">to your modeling of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=944" target="_blank">00:15:44.760</a></span> | <span class="t">Another key idea throughout all these lines of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=947" target="_blank">00:15:47.240</a></span> | <span class="t">is you really want hardware and kernel support from day one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=951" target="_blank">00:15:51.160</a></span> | <span class="t">So even if your model is theoretically more efficient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=954" target="_blank">00:15:54.960</a></span> | <span class="t">if somebody goes and runs it and it's two times slower,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=958" target="_blank">00:15:58.120</a></span> | <span class="t">one of the things that we've learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=959" target="_blank">00:15:59.420</a></span> | <span class="t">is that if you're in that situation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=961" target="_blank">00:16:01.120</a></span> | <span class="t">it's just gonna be dead on arrival.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=963" target="_blank">00:16:03.520</a></span> | <span class="t">So you want to be designing your architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=966" target="_blank">00:16:06.200</a></span> | <span class="t">with the hardware in mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=967" target="_blank">00:16:07.840</a></span> | <span class="t">One of the key machine learning ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=971" target="_blank">00:16:11.980</a></span> | <span class="t">that has been important for the quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=973" target="_blank">00:16:13.840</a></span> | <span class="t">is just making sure that you encode different ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=976" target="_blank">00:16:16.440</a></span> | <span class="t">that you can select from your hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=978" target="_blank">00:16:18.720</a></span> | <span class="t">and really focus on that as a key decider of quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=982" target="_blank">00:16:22.200</a></span> | <span class="t">And finally, I think one of the emerging new things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=986" target="_blank">00:16:26.600</a></span> | <span class="t">for this line of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=987" target="_blank">00:16:27.960</a></span> | <span class="t">and something that's quite interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=989" target="_blank">00:16:29.560</a></span> | <span class="t">is what are the right test time paradigms for these models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=992" target="_blank">00:16:32.960</a></span> | <span class="t">How do they change relative to what you might do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=997" target="_blank">00:16:37.960</a></span> | <span class="t">for a standard transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=999" target="_blank">00:16:39.360</a></span> | <span class="t">I'll briefly end this section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1001" target="_blank">00:16:41.880</a></span> | <span class="t">So I've labeled this slide where we are yesterday</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1005" target="_blank">00:16:45.440</a></span> | <span class="t">because Eugene is gonna talk about some new models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1007" target="_blank">00:16:47.440</a></span> | <span class="t">that he released literally this morning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1009" target="_blank">00:16:49.840</a></span> | <span class="t">But as of yesterday, some of the really cool results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1012" target="_blank">00:16:52.080</a></span> | <span class="t">out of these efficient alternative models were,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1016" target="_blank">00:16:56.480</a></span> | <span class="t">so AI2 trained this hybrid MOE called Jamba</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1019" target="_blank">00:16:59.600</a></span> | <span class="t">that is currently the state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1023" target="_blank">00:17:03.120</a></span> | <span class="t">for these non-transformer architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1026" target="_blank">00:17:06.320</a></span> | <span class="t">There's this NVIDIA and MIT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1028" target="_blank">00:17:08.720</a></span> | <span class="t">put out this new diffusion model called SANA recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1032" target="_blank">00:17:12.640</a></span> | <span class="t">that one of their key observations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1035" target="_blank">00:17:15.760</a></span> | <span class="t">is that you can take a standard diffusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1038" target="_blank">00:17:18.240</a></span> | <span class="t">transformer diffusion model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1039" target="_blank">00:17:19.760</a></span> | <span class="t">replace the layers with linear attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1041" target="_blank">00:17:21.800</a></span> | <span class="t">and then that lets you scale to much larger images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1045" target="_blank">00:17:25.960</a></span> | <span class="t">much larger sequences more efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1050" target="_blank">00:17:30.720</a></span> | <span class="t">And one thing that I don't think anybody would have called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1054" target="_blank">00:17:34.360</a></span> | <span class="t">when a few years ago</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1056" target="_blank">00:17:36.320</a></span> | <span class="t">is that one of those gated SSM, gated states-based models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1061" target="_blank">00:17:41.840</a></span> | <span class="t">ended up on the cover of science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1063" target="_blank">00:17:43.480</a></span> | <span class="t">because a great group of folks went</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1065" target="_blank">00:17:45.880</a></span> | <span class="t">and trained some DNA models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1067" target="_blank">00:17:47.200</a></span> | <span class="t">So that's Michael Polley, Eric Yuen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1069" target="_blank">00:17:49.640</a></span> | <span class="t">from Stanford and the Ark Institute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1073" target="_blank">00:17:53.200</a></span> | <span class="t">So we're really at an exciting time in 2024</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1076" target="_blank">00:17:56.920</a></span> | <span class="t">where these non-transformer, post-transformer architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1080" target="_blank">00:18:00.240</a></span> | <span class="t">are showing promise across a wide range,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1083" target="_blank">00:18:03.280</a></span> | <span class="t">across a wide range of modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1087" target="_blank">00:18:07.360</a></span> | <span class="t">of applications, and of tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1090" target="_blank">00:18:10.760</a></span> | <span class="t">And with that, I'll pass it on to Eugene</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1092" target="_blank">00:18:12.280</a></span> | <span class="t">who can tell you a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1093" target="_blank">00:18:13.760</a></span> | <span class="t">about the latest and greatest with RWKV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1096" target="_blank">00:18:16.720</a></span> | <span class="t">- Yeah, so that's useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1099" target="_blank">00:18:19.120</a></span> | <span class="t">Yeah. - You're talking to here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1099" target="_blank">00:18:19.960</a></span> | <span class="t">- Oh, I'm talking to here, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1101" target="_blank">00:18:21.280</a></span> | <span class="t">So yeah, two streams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1103" target="_blank">00:18:23.240</a></span> | <span class="t">Yeah, so I think one common questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1105" target="_blank">00:18:25.040</a></span> | <span class="t">that we tend to get asked, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1106" target="_blank">00:18:26.920</a></span> | <span class="t">is what's the difference between RWKV and states-based?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1110" target="_blank">00:18:30.200</a></span> | <span class="t">So I think one of the key things to really understand,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1113" target="_blank">00:18:33.560</a></span> | <span class="t">right, the difference between the two groups, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1116" target="_blank">00:18:36.440</a></span> | <span class="t">is that we are actually more like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1118" target="_blank">00:18:38.680</a></span> | <span class="t">an open-source rental internet meets academia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1121" target="_blank">00:18:41.080</a></span> | <span class="t">kind of situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1122" target="_blank">00:18:42.200</a></span> | <span class="t">Like most of us never wrote any paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1125" target="_blank">00:18:45.040</a></span> | <span class="t">but we basically look at RNNs and linear intention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1128" target="_blank">00:18:48.960</a></span> | <span class="t">when intention is all you need came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1130" target="_blank">00:18:50.680</a></span> | <span class="t">And then we decided to like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1131" target="_blank">00:18:51.600</a></span> | <span class="t">"Hey, there is a quadratic scaling problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1134" target="_blank">00:18:54.480</a></span> | <span class="t">"Why don't we try fixing that instead?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1137" target="_blank">00:18:57.160</a></span> | <span class="t">So we end up developing our own branch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1140" target="_blank">00:19:00.120</a></span> | <span class="t">but we end up sharing ideas back and forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1142" target="_blank">00:19:02.600</a></span> | <span class="t">And we do all this actively in Discord, GitHub, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1147" target="_blank">00:19:07.760</a></span> | <span class="t">This was so bad for a few years, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1150" target="_blank">00:19:10.080</a></span> | <span class="t">that basically the average group's H-index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1152" target="_blank">00:19:12.520</a></span> | <span class="t">was so close to zero, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1153" target="_blank">00:19:13.760</a></span> | <span class="t">ILLUTR-AI actually came in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1155" target="_blank">00:19:15.480</a></span> | <span class="t">and helped us write our first paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1157" target="_blank">00:19:17.360</a></span> | <span class="t">Great, now our H-index is now three, apparently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1159" target="_blank">00:19:19.600</a></span> | <span class="t">So, but the thing is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1162" target="_blank">00:19:22.400</a></span> | <span class="t">a lot of these experiments led to results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1165" target="_blank">00:19:25.320</a></span> | <span class="t">And essentially, we took the same ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1170" target="_blank">00:19:30.280</a></span> | <span class="t">from linear intention and we built on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1173" target="_blank">00:19:33.320</a></span> | <span class="t">So to take a step back into like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1175" target="_blank">00:19:35.000</a></span> | <span class="t">how does RWKB handle its own attention mechanic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1178" target="_blank">00:19:38.520</a></span> | <span class="t">and achieve the same goals of like O(n) compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1181" target="_blank">00:19:41.600</a></span> | <span class="t">respectively, and in focus of our overall goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1185" target="_blank">00:19:45.720</a></span> | <span class="t">to make AI accessible to everyone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1187" target="_blank">00:19:47.120</a></span> | <span class="t">regardless of language, nation, or compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1188" target="_blank">00:19:48.880</a></span> | <span class="t">That's our open-source goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1190" target="_blank">00:19:50.560</a></span> | <span class="t">We actually train our models primarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1192" target="_blank">00:19:52.640</a></span> | <span class="t">on over a hundred language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1194" target="_blank">00:19:54.160</a></span> | <span class="t">which is another topic altogether.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1196" target="_blank">00:19:56.120</a></span> | <span class="t">And our goal is to train to even 200 languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1198" target="_blank">00:19:58.240</a></span> | <span class="t">to cover all languages in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1200" target="_blank">00:20:00.040</a></span> | <span class="t">But at the same time, we work on this architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1203" target="_blank">00:20:03.360</a></span> | <span class="t">to lower the compute cost so that people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1205" target="_blank">00:20:05.440</a></span> | <span class="t">can run in Raspberry Pis and on anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1208" target="_blank">00:20:08.600</a></span> | <span class="t">So how did RWKB break the dependency of LSTM token flow?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1213" target="_blank">00:20:13.600</a></span> | <span class="t">Because I think to understand architecture, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1216" target="_blank">00:20:16.120</a></span> | <span class="t">it's probably easier to understand it from the RNN lens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1219" target="_blank">00:20:19.760</a></span> | <span class="t">because that's where we built on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1221" target="_blank">00:20:21.680</a></span> | <span class="t">We all state space kind of like try to start anew</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1225" target="_blank">00:20:25.040</a></span> | <span class="t">and took lessons from that and say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1226" target="_blank">00:20:26.200</a></span> | <span class="t">so there's a little bit of divergence there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1228" target="_blank">00:20:28.200</a></span> | <span class="t">And AKA, this is our version of linear intention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1231" target="_blank">00:20:31.320</a></span> | <span class="t">So to take a step back, all foundation models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1235" target="_blank">00:20:35.000</a></span> | <span class="t">be it transformers or non-transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1237" target="_blank">00:20:37.440</a></span> | <span class="t">at a very high level, right, comes in a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1240" target="_blank">00:20:40.400</a></span> | <span class="t">I mean, takes things into embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1242" target="_blank">00:20:42.480</a></span> | <span class="t">and goes through a lot of layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1244" target="_blank">00:20:44.240</a></span> | <span class="t">generate a lot of internal states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1245" target="_blank">00:20:45.800</a></span> | <span class="t">whether QKB cache or RNN states or RWKB states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1250" target="_blank">00:20:50.360</a></span> | <span class="t">and outputs an embedding layer norm in something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1252" target="_blank">00:20:52.680</a></span> | <span class="t">and we just take more layers and more embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1254" target="_blank">00:20:54.360</a></span> | <span class="t">and somehow that magically works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1257" target="_blank">00:20:57.400</a></span> | <span class="t">So if you remember your ancient RNN lessons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1262" target="_blank">00:21:02.040</a></span> | <span class="t">which we call blessing these days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1267" target="_blank">00:21:07.000</a></span> | <span class="t">the general idea is that you have the embedding information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1269" target="_blank">00:21:09.360</a></span> | <span class="t">from all the way up, and you take that information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1273" target="_blank">00:21:13.080</a></span> | <span class="t">and you flow it back down,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1273" target="_blank">00:21:13.920</a></span> | <span class="t">and then you process it as part of your LSTM layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1276" target="_blank">00:21:16.480</a></span> | <span class="t">So this is how it generally works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1279" target="_blank">00:21:19.040</a></span> | <span class="t">Kapati is quoted saying that RNNs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1280" target="_blank">00:21:20.760</a></span> | <span class="t">are actually unreasonably effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1282" target="_blank">00:21:22.640</a></span> | <span class="t">The problem is this is not scalable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1285" target="_blank">00:21:25.160</a></span> | <span class="t">To start doing work on the second token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1287" target="_blank">00:21:27.160</a></span> | <span class="t">you need to wait for the first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1288" target="_blank">00:21:28.640</a></span> | <span class="t">and then you need to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1289" target="_blank">00:21:29.480</a></span> | <span class="t">and likewise for the third token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1290" target="_blank">00:21:30.320</a></span> | <span class="t">and fourth token, yada, yada.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1291" target="_blank">00:21:31.960</a></span> | <span class="t">That is CPU land, not GPU land.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1294" target="_blank">00:21:34.360</a></span> | <span class="t">So you can have a H100, and you can't even use 1% of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1298" target="_blank">00:21:38.280</a></span> | <span class="t">So that's kind of why RNNs didn't really take off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1301" target="_blank">00:21:41.400</a></span> | <span class="t">in the direction that when you wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1302" target="_blank">00:21:42.640</a></span> | <span class="t">like billions of parameters when it comes to training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1304" target="_blank">00:21:44.880</a></span> | <span class="t">So what did RWKB version zero do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1307" target="_blank">00:21:47.560</a></span> | <span class="t">We just did the dumbest, lamest thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1309" target="_blank">00:21:49.920</a></span> | <span class="t">Sorry, this is the bottleneck for RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1312" target="_blank">00:21:52.120</a></span> | <span class="t">We did the dumb thing of removing that line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1314" target="_blank">00:21:54.680</a></span> | <span class="t">and it kind of worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1316" target="_blank">00:21:56.360</a></span> | <span class="t">It trained, it sucked, but it kind of worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1319" target="_blank">00:21:59.920</a></span> | <span class="t">Then we were like, hey, then no one cared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1322" target="_blank">00:22:02.800</a></span> | <span class="t">because the loss was crap, but how do we improve that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1327" target="_blank">00:22:07.000</a></span> | <span class="t">And that's essentially where we move forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1329" target="_blank">00:22:09.640</a></span> | <span class="t">because if you see this kind of flow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1332" target="_blank">00:22:12.080</a></span> | <span class="t">you can actually get your GPU saturated quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1335" target="_blank">00:22:15.880</a></span> | <span class="t">where it essentially cascades respectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1337" target="_blank">00:22:17.920</a></span> | <span class="t">So I'm just waiting for this to loop again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1340" target="_blank">00:22:20.160</a></span> | <span class="t">So it's like once you get your first layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1341" target="_blank">00:22:21.840</a></span> | <span class="t">your token to be computed finish,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1344" target="_blank">00:22:24.200</a></span> | <span class="t">you start to cascade your compute all the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1346" target="_blank">00:22:26.360</a></span> | <span class="t">until you're, hey, I'm using 100% of GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1348" target="_blank">00:22:28.760</a></span> | <span class="t">So we worked on it and we started going along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1353" target="_blank">00:22:33.000</a></span> | <span class="t">the principle of that as long as we keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1354" target="_blank">00:22:34.960</a></span> | <span class="t">this general architecture where we can cascade</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1358" target="_blank">00:22:38.040</a></span> | <span class="t">and be highly efficient with our architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1360" target="_blank">00:22:40.960</a></span> | <span class="t">nothing is sacred in our architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1363" target="_blank">00:22:43.120</a></span> | <span class="t">And we have done some crazy ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1365" target="_blank">00:22:45.680</a></span> | <span class="t">In fact, if you ask me to explain some things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1368" target="_blank">00:22:48.920</a></span> | <span class="t">in the paper, right, officially in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1371" target="_blank">00:22:51.160</a></span> | <span class="t">I'll say we had this idea and we wrote it this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1373" target="_blank">00:22:53.640</a></span> | <span class="t">The reality is someone came with a code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1375" target="_blank">00:22:55.760</a></span> | <span class="t">we tested it, it worked, and then we rationalized it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1378" target="_blank">00:22:58.440</a></span> | <span class="t">So the general idea behind RWKVR is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1383" target="_blank">00:23:03.200</a></span> | <span class="t">we generally have two major blocks that we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1386" target="_blank">00:23:06.520</a></span> | <span class="t">We call it TimeMix and ChannelMix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1388" target="_blank">00:23:08.080</a></span> | <span class="t">And TimeMix generally handles long-term memory states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1392" target="_blank">00:23:12.520</a></span> | <span class="t">where essentially where we apply the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1397" target="_blank">00:23:17.520</a></span> | <span class="t">and SILU activation functions into processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1399" target="_blank">00:23:19.520</a></span> | <span class="t">an input embedding and an output embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1402" target="_blank">00:23:22.200</a></span> | <span class="t">I'm oversimplifying it because this calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1405" target="_blank">00:23:25.120</a></span> | <span class="t">changed every version and we have version seven right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1409" target="_blank">00:23:29.080</a></span> | <span class="t">ChannelMix is similar to Bayes in the sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1411" target="_blank">00:23:31.680</a></span> | <span class="t">that where it does shorter-term attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1413" target="_blank">00:23:33.840</a></span> | <span class="t">where it just look at the sister token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1416" target="_blank">00:23:36.680</a></span> | <span class="t">or the token before it, 'cause there's a shift</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1418" target="_blank">00:23:38.560</a></span> | <span class="t">in the token shift matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1421" target="_blank">00:23:41.480</a></span> | <span class="t">I don't really want to go too much into the papers itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1423" target="_blank">00:23:43.800</a></span> | <span class="t">because we do have three papers on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1426" target="_blank">00:23:46.240</a></span> | <span class="t">Basically, RWKV, RNN for the transformer era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1429" target="_blank">00:23:49.840</a></span> | <span class="t">Igor and Finch RWKV matrix value state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1432" target="_blank">00:23:52.040</a></span> | <span class="t">This is the updated version five, version six.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1434" target="_blank">00:23:54.680</a></span> | <span class="t">And GoFinch is our hybrid model, respectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1439" target="_blank">00:23:59.680</a></span> | <span class="t">We are writing the paper already for V7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1443" target="_blank">00:24:03.680</a></span> | <span class="t">and which is for RWKV7, codename Goose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1448" target="_blank">00:24:08.480</a></span> | <span class="t">all our architectures are codenamed by a bird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1451" target="_blank">00:24:11.000</a></span> | <span class="t">And I'm going to cover as well Q-RWKV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1453" target="_blank">00:24:13.680</a></span> | <span class="t">and MAMA-RWK and RWKVMU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1456" target="_blank">00:24:16.920</a></span> | <span class="t">So where did that lead to?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1458" target="_blank">00:24:18.560</a></span> | <span class="t">Wait, because we are all GPU poor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1461" target="_blank">00:24:21.760</a></span> | <span class="t">and to be clear, most of this research is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1464" target="_blank">00:24:24.200</a></span> | <span class="t">only on a handful of H100s,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1465" target="_blank">00:24:25.880</a></span> | <span class="t">which I had one Google researcher told me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1468" target="_blank">00:24:28.000</a></span> | <span class="t">that was his experiment budget for a single researcher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1471" target="_blank">00:24:31.520</a></span> | <span class="t">So our entire organization has less compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1474" target="_blank">00:24:34.360</a></span> | <span class="t">than a single researcher in Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1476" target="_blank">00:24:36.680</a></span> | <span class="t">One of the things that we explored into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1480" target="_blank">00:24:40.120</a></span> | <span class="t">was how do we convert transformer models instead?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1483" target="_blank">00:24:43.440</a></span> | <span class="t">Because someone already paid that million dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1486" target="_blank">00:24:46.200</a></span> | <span class="t">onto training, so why don't we take advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1487" target="_blank">00:24:47.840</a></span> | <span class="t">of those weights?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1489" target="_blank">00:24:49.560</a></span> | <span class="t">And I believe, together, AI worked on the locus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1492" target="_blank">00:24:52.840</a></span> | <span class="t">for the MAMA side of things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1495" target="_blank">00:24:55.480</a></span> | <span class="t">and we took some ideas from there as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1497" target="_blank">00:24:57.480</a></span> | <span class="t">and we essentially did that for RWKV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1499" target="_blank">00:24:59.920</a></span> | <span class="t">And that led to Q-RWKV6, which we just dropped today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1505" target="_blank">00:25:05.360</a></span> | <span class="t">a 32-bit instruct preview model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1507" target="_blank">00:25:07.400</a></span> | <span class="t">where we took the current 32-bit instruct model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1510" target="_blank">00:25:10.600</a></span> | <span class="t">freeze the feedforward layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1512" target="_blank">00:25:12.080</a></span> | <span class="t">remove the QKV attention layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1515" target="_blank">00:25:15.200</a></span> | <span class="t">and replace it with RWKV linear layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1517" target="_blank">00:25:17.840</a></span> | <span class="t">So to be clear, this means we do not have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1521" target="_blank">00:25:21.000</a></span> | <span class="t">the RWKV channel mixed layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1522" target="_blank">00:25:22.720</a></span> | <span class="t">we only have the time mixed layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1524" target="_blank">00:25:24.440</a></span> | <span class="t">But once we do that, we train the RWKV layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1528" target="_blank">00:25:28.600</a></span> | <span class="t">Important is that the feedforward layer needs to be frozen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1530" target="_blank">00:25:30.920</a></span> | <span class="t">so the new attention can be learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1533" target="_blank">00:25:33.240</a></span> | <span class="t">And then we unfreeze the feedforward layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1535" target="_blank">00:25:35.880</a></span> | <span class="t">and train all the layers together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1537" target="_blank">00:25:37.000</a></span> | <span class="t">with a custom learning rate schedule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1538" target="_blank">00:25:38.280</a></span> | <span class="t">so that they can learn how to work together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1541" target="_blank">00:25:41.040</a></span> | <span class="t">The end result, surprisingly, and to be honest,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1544" target="_blank">00:25:44.320</a></span> | <span class="t">to the frustration of the RWKV MOE team,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1546" target="_blank">00:25:46.760</a></span> | <span class="t">which ended up releasing the model on the same day,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1549" target="_blank">00:25:49.240</a></span> | <span class="t">was that with just a few hours of training on two nodes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1554" target="_blank">00:25:54.240</a></span> | <span class="t">we managed to get it to be on par</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1556" target="_blank">00:25:56.360</a></span> | <span class="t">kind of with the original QUANT32B model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1559" target="_blank">00:25:59.080</a></span> | <span class="t">So in fact, when the first run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1561" target="_blank">00:26:01.200</a></span> | <span class="t">that completely confused us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1562" target="_blank">00:26:02.800</a></span> | <span class="t">and I was telling Daniel Goldstein-Smithkey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1566" target="_blank">00:26:06.640</a></span> | <span class="t">who kind of leads most of our research coordination,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1569" target="_blank">00:26:09.480</a></span> | <span class="t">when you pitched me this idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1570" target="_blank">00:26:10.640</a></span> | <span class="t">you told me at best you would get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1572" target="_blank">00:26:12.120</a></span> | <span class="t">the same level of performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1573" target="_blank">00:26:13.040</a></span> | <span class="t">But you didn't tell me the challenge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1575" target="_blank">00:26:15.040</a></span> | <span class="t">and the score would shoot up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1579" target="_blank">00:26:19.160</a></span> | <span class="t">I don't know what's happening there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1581" target="_blank">00:26:21.240</a></span> | <span class="t">But it did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1582" target="_blank">00:26:22.080</a></span> | <span class="t">MMLU score dropping, that was expected,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1585" target="_blank">00:26:25.160</a></span> | <span class="t">because if you think about it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1586" target="_blank">00:26:26.560</a></span> | <span class="t">when we were training all the layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1588" target="_blank">00:26:28.680</a></span> | <span class="t">we were essentially like Frankensteining this thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1591" target="_blank">00:26:31.440</a></span> | <span class="t">and we did brain damage to the feedforward network layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1594" target="_blank">00:26:34.320</a></span> | <span class="t">with the new RWKV layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1596" target="_blank">00:26:36.040</a></span> | <span class="t">But 76%, hey, some of it is retained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1598" target="_blank">00:26:38.520</a></span> | <span class="t">and we can probably further train this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1600" target="_blank">00:26:40.760</a></span> | <span class="t">We didn't even spend three days training this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1602" target="_blank">00:26:42.600</a></span> | <span class="t">so there's a lot more that can be done,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1604" target="_blank">00:26:44.720</a></span> | <span class="t">hence the preview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1606" target="_blank">00:26:46.240</a></span> | <span class="t">But this brings up a big question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1609" target="_blank">00:26:49.800</a></span> | <span class="t">because we are already now in the process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1611" target="_blank">00:26:51.960</a></span> | <span class="t">of converting the SMPB.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1614" target="_blank">00:26:54.160</a></span> | <span class="t">This is actually extremely compute efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1616" target="_blank">00:26:56.480</a></span> | <span class="t">to test our attention mechanic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1619" target="_blank">00:26:59.080</a></span> | <span class="t">It's like, it becomes a shortcut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1621" target="_blank">00:27:01.000</a></span> | <span class="t">We are already planning to do our version seven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1622" target="_blank">00:27:02.920</a></span> | <span class="t">and our hybrid architecture for it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1624" target="_blank">00:27:04.920</a></span> | <span class="t">because we don't train from scratch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1626" target="_blank">00:27:06.400</a></span> | <span class="t">and we get a really good model out of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1628" target="_blank">00:27:08.720</a></span> | <span class="t">And the other thing that is uncomfortable to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1632" target="_blank">00:27:12.080</a></span> | <span class="t">is that, because we are doing right now the SMPB,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1634" target="_blank">00:27:14.920</a></span> | <span class="t">is that if this scales correctly to 128k context length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1639" target="_blank">00:27:19.480</a></span> | <span class="t">I'm not even talking about a million, 128k,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1642" target="_blank">00:27:22.840</a></span> | <span class="t">majority of enterprise workload today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1646" target="_blank">00:27:26.160</a></span> | <span class="t">is just on SMPB at under 32k context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1650" target="_blank">00:27:30.360</a></span> | <span class="t">That means if this works and the benchmark matches it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1654" target="_blank">00:27:34.040</a></span> | <span class="t">it means we can replace the vast majority</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1656" target="_blank">00:27:36.240</a></span> | <span class="t">of current AI workload,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1657" target="_blank">00:27:37.720</a></span> | <span class="t">unless you want super long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1659" target="_blank">00:27:39.240</a></span> | <span class="t">And then, sorry, can someone give us more GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1661" target="_blank">00:27:41.560</a></span> | <span class="t">because we don't need the VRAM for super long context, sadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1664" target="_blank">00:27:44.720</a></span> | <span class="t">So yeah, that's what we are working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1667" target="_blank">00:27:47.960</a></span> | <span class="t">And essentially, we are excited about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1670" target="_blank">00:27:50.280</a></span> | <span class="t">to just push it further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1671" target="_blank">00:27:51.480</a></span> | <span class="t">And this conversion process, to be clear,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1674" target="_blank">00:27:54.320</a></span> | <span class="t">I don't think it's going to be exclusive to RWKV,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1676" target="_blank">00:27:56.680</a></span> | <span class="t">but it probably will work for Mamba as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1679" target="_blank">00:27:59.760</a></span> | <span class="t">I don't see why not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1680" target="_blank">00:28:00.840</a></span> | <span class="t">And we will probably see more ideas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1683" target="_blank">00:28:03.000</a></span> | <span class="t">or more experiments, or more hybrids.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1685" target="_blank">00:28:05.080</a></span> | <span class="t">Like, yeah, one of the weirdest thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1687" target="_blank">00:28:07.400</a></span> | <span class="t">that I wanted to say outright,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1689" target="_blank">00:28:09.080</a></span> | <span class="t">and I confirm this with the Black Mamba team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1690" target="_blank">00:28:10.840</a></span> | <span class="t">and the Jamba team,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1692" target="_blank">00:28:12.520</a></span> | <span class="t">because we did the Goldfinch hybrid model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1694" target="_blank">00:28:14.600</a></span> | <span class="t">is that none of us understand why a hybrid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1698" target="_blank">00:28:18.520</a></span> | <span class="t">with a state-based model, be it RWKV state space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1700" target="_blank">00:28:20.960</a></span> | <span class="t">and transformer, performs better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1704" target="_blank">00:28:24.040</a></span> | <span class="t">than the baseline of both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1706" target="_blank">00:28:26.600</a></span> | <span class="t">It's like, when you train one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1709" target="_blank">00:28:29.040</a></span> | <span class="t">you expect, and then you replace,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1710" target="_blank">00:28:30.120</a></span> | <span class="t">you expect the same results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1711" target="_blank">00:28:31.040</a></span> | <span class="t">That's our pitch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1711" target="_blank">00:28:31.880</a></span> | <span class="t">That's our claim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1712" target="_blank">00:28:32.760</a></span> | <span class="t">But somehow, when we jam both together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1714" target="_blank">00:28:34.960</a></span> | <span class="t">it outperforms both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1716" target="_blank">00:28:36.240</a></span> | <span class="t">And that's one area of evolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1718" target="_blank">00:28:38.200</a></span> | <span class="t">that, like, we only have four experiments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1720" target="_blank">00:28:40.160</a></span> | <span class="t">plus four teams, that a lot more needs to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1722" target="_blank">00:28:42.760</a></span> | <span class="t">But these are things that excite me, essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1725" target="_blank">00:28:45.320</a></span> | <span class="t">because that is what, potentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1727" target="_blank">00:28:47.360</a></span> | <span class="t">we can move ahead for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1728" target="_blank">00:28:48.960</a></span> | <span class="t">which brings us to what comes next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1731" target="_blank">00:28:51.200</a></span> | <span class="t">- So this part is kind of just some,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1735" target="_blank">00:28:55.920</a></span> | <span class="t">where we'll talk a little bit about stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1737" target="_blank">00:28:57.480</a></span> | <span class="t">that we're excited about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1739" target="_blank">00:28:59.800</a></span> | <span class="t">maybe have some wild speculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1742" target="_blank">00:29:02.200</a></span> | <span class="t">on what's coming next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1745" target="_blank">00:29:05.760</a></span> | <span class="t">And, of course, this is also the part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1747" target="_blank">00:29:07.920</a></span> | <span class="t">that will be more open to questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1750" target="_blank">00:29:10.560</a></span> | <span class="t">So a couple of things that I'm excited about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1752" target="_blank">00:29:12.800</a></span> | <span class="t">is continued hardware model co-design for these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1757" target="_blank">00:29:17.800</a></span> | <span class="t">So one of the things that we've put out recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1762" target="_blank">00:29:22.040</a></span> | <span class="t">is this library called Thunder Kittens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1763" target="_blank">00:29:23.600</a></span> | <span class="t">It's a CUDA library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1765" target="_blank">00:29:25.320</a></span> | <span class="t">And one of the things that we found frustrating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1767" target="_blank">00:29:27.760</a></span> | <span class="t">is every time that we built one of these new architectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1770" target="_blank">00:29:30.280</a></span> | <span class="t">and I'm sure you had the exact same experience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1772" target="_blank">00:29:32.680</a></span> | <span class="t">we'd have to go and spend two months in CUDA land,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1775" target="_blank">00:29:35.160</a></span> | <span class="t">like, writing these new, efficient things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1777" target="_blank">00:29:37.640</a></span> | <span class="t">And if we decided to change one thing in PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1780" target="_blank">00:29:40.920</a></span> | <span class="t">like, one line of PyTorch code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1782" target="_blank">00:29:42.320</a></span> | <span class="t">is like a week of CUDA code, at least.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1785" target="_blank">00:29:45.000</a></span> | <span class="t">So one of our goals with a library like Thunder Kittens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1788" target="_blank">00:29:48.440</a></span> | <span class="t">so we just broke down what are the key principles,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1792" target="_blank">00:29:52.000</a></span> | <span class="t">what are the key hardware things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1794" target="_blank">00:29:54.640</a></span> | <span class="t">what are the key compute pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1796" target="_blank">00:29:56.600</a></span> | <span class="t">that you get from the hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1797" target="_blank">00:29:57.520</a></span> | <span class="t">So, for example, on H100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1799" target="_blank">00:29:59.640</a></span> | <span class="t">everything really revolves around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1802" target="_blank">00:30:02.320</a></span> | <span class="t">a warp group matrix multiply operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1805" target="_blank">00:30:05.840</a></span> | <span class="t">So you really want your operation to be able to split</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1808" target="_blank">00:30:08.760</a></span> | <span class="t">into a relatively small matrix-matrix multiply operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1813" target="_blank">00:30:13.560</a></span> | <span class="t">So, like, multiplying two 64 by 64 matrices, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1818" target="_blank">00:30:18.240</a></span> | <span class="t">And so if you know that ahead of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1819" target="_blank">00:30:19.920</a></span> | <span class="t">when you're designing your model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1821" target="_blank">00:30:21.240</a></span> | <span class="t">that probably gives you some information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1824" target="_blank">00:30:24.600</a></span> | <span class="t">about how you set the state sizes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1825" target="_blank">00:30:25.880</a></span> | <span class="t">how you set the update, how you set the update function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1828" target="_blank">00:30:28.960</a></span> | <span class="t">So with Thunder Kittens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1830" target="_blank">00:30:30.320</a></span> | <span class="t">we basically built a whole library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1831" target="_blank">00:30:31.880</a></span> | <span class="t">just around this basic idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1833" target="_blank">00:30:33.840</a></span> | <span class="t">that all your basic compute primitives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1836" target="_blank">00:30:36.280</a></span> | <span class="t">should not be a float, but it should be a matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1838" target="_blank">00:30:38.800</a></span> | <span class="t">and everything should just be matrix compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1841" target="_blank">00:30:41.280</a></span> | <span class="t">And we've been using that to try to both re-implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1844" target="_blank">00:30:44.160</a></span> | <span class="t">some existing architectures and also start to design</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1847" target="_blank">00:30:47.200</a></span> | <span class="t">some new ones that are really designed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1848" target="_blank">00:30:48.880</a></span> | <span class="t">with this core, with a tensor core primitive in mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1852" target="_blank">00:30:52.720</a></span> | <span class="t">Another thing that we're, at least I'm excited about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1857" target="_blank">00:30:57.720</a></span> | <span class="t">is we, over the last four or five years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1860" target="_blank">00:31:00.800</a></span> | <span class="t">we've really been looking at language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1862" target="_blank">00:31:02.600</a></span> | <span class="t">as the next thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1863" target="_blank">00:31:03.640</a></span> | <span class="t">But if you've been paying attention to Twitter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1866" target="_blank">00:31:06.000</a></span> | <span class="t">there's been a bunch of new next generation models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1868" target="_blank">00:31:08.600</a></span> | <span class="t">that are coming out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1869" target="_blank">00:31:09.640</a></span> | <span class="t">So there are video generation models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1873" target="_blank">00:31:13.880</a></span> | <span class="t">that can run real time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1876" target="_blank">00:31:16.080</a></span> | <span class="t">that are supported by your mouse and your keyboard,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1879" target="_blank">00:31:19.280</a></span> | <span class="t">that I'm told if you play with them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1881" target="_blank">00:31:21.600</a></span> | <span class="t">that they only have a few seconds of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1884" target="_blank">00:31:24.680</a></span> | <span class="t">Can we take that model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1885" target="_blank">00:31:25.600</a></span> | <span class="t">Can we give it a very long context length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1887" target="_blank">00:31:27.400</a></span> | <span class="t">so that you could actually maybe generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1889" target="_blank">00:31:29.360</a></span> | <span class="t">an entire game state at a time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1891" target="_blank">00:31:31.400</a></span> | <span class="t">What does that look like for the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1893" target="_blank">00:31:33.040</a></span> | <span class="t">You're certainly not gonna do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1894" target="_blank">00:31:34.440</a></span> | <span class="t">a giant quadratic attention computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1897" target="_blank">00:31:37.240</a></span> | <span class="t">to try to run that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1898" target="_blank">00:31:38.920</a></span> | <span class="t">Maybe use some of these new models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1901" target="_blank">00:31:41.320</a></span> | <span class="t">or some of these new video generation models that came out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1903" target="_blank">00:31:43.680</a></span> | <span class="t">So Sora came out, I don't know, two days ago now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1907" target="_blank">00:31:47.800</a></span> | <span class="t">but with super long queue times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1909" target="_blank">00:31:49.080</a></span> | <span class="t">and super long generation times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1911" target="_blank">00:31:51.040</a></span> | <span class="t">So that's probably a quadratic attention operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1913" target="_blank">00:31:53.440</a></span> | <span class="t">at the bottom of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1915" target="_blank">00:31:55.120</a></span> | <span class="t">What if we could remove that and get the same quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1917" target="_blank">00:31:57.160</a></span> | <span class="t">but a lot faster generation time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1920" target="_blank">00:32:00.320</a></span> | <span class="t">Or some of the demos that we saw from Paige earlier today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1924" target="_blank">00:32:04.040</a></span> | <span class="t">If I have a super long conversation with my Gemini bot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1929" target="_blank">00:32:09.040</a></span> | <span class="t">what if I wanted to remember everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1932" target="_blank">00:32:12.480</a></span> | <span class="t">that it's seen in the last week?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1934" target="_blank">00:32:14.120</a></span> | <span class="t">I mean, maybe you don't for personal reasons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1937" target="_blank">00:32:17.160</a></span> | <span class="t">but what if I did?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1938" target="_blank">00:32:18.440</a></span> | <span class="t">What does that mean for the architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1941" target="_blank">00:32:21.000</a></span> | <span class="t">And I think that's certainly something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1942" target="_blank">00:32:22.680</a></span> | <span class="t">I'm pretty excited about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1944" target="_blank">00:32:24.200</a></span> | <span class="t">I'm sure you're excited about it too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1946" target="_blank">00:32:26.040</a></span> | <span class="t">I think we were supposed to have some hot takes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1948" target="_blank">00:32:28.480</a></span> | <span class="t">but I honestly don't remember what our hot takes were.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1950" target="_blank">00:32:30.960</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1951" target="_blank">00:32:31.800</a></span> | <span class="t">- Hot take, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1954" target="_blank">00:32:34.360</a></span> | <span class="t">These are our hot takes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1955" target="_blank">00:32:35.480</a></span> | <span class="t">- I think the big one on Twitter that we saw,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1961" target="_blank">00:32:41.080</a></span> | <span class="t">that we shared was, the question is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1962" target="_blank">00:32:42.960</a></span> | <span class="t">is RAG relevant in the case of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1966" target="_blank">00:32:46.000</a></span> | <span class="t">the future of like state-based models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1968" target="_blank">00:32:48.200</a></span> | <span class="t">- Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1970" target="_blank">00:32:50.280</a></span> | <span class="t">I haven't played too much with RAG,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1974" target="_blank">00:32:54.640</a></span> | <span class="t">but when I have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1976" target="_blank">00:32:56.960</a></span> | <span class="t">I'll say I found it was a little bit challenging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1981" target="_blank">00:33:01.200</a></span> | <span class="t">to do research on it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1982" target="_blank">00:33:02.480</a></span> | <span class="t">because we had this experience over and over again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1986" target="_blank">00:33:06.240</a></span> | <span class="t">where you could have an embedding model of any quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1990" target="_blank">00:33:10.760</a></span> | <span class="t">So you could have a really, really bad embedding model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1992" target="_blank">00:33:12.680</a></span> | <span class="t">or you could have a really, really good one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1994" target="_blank">00:33:14.560</a></span> | <span class="t">by any measure of good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1996" target="_blank">00:33:16.800</a></span> | <span class="t">And for the final RAG application,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=1998" target="_blank">00:33:18.960</a></span> | <span class="t">it kind of didn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2000" target="_blank">00:33:20.440</a></span> | <span class="t">That's what I'll say about RAG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2003" target="_blank">00:33:23.800</a></span> | <span class="t">Well, being recorded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2005" target="_blank">00:33:25.360</a></span> | <span class="t">I know it doesn't actually answer the question, but.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2008" target="_blank">00:33:28.760</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2009" target="_blank">00:33:29.600</a></span> | <span class="t">So I think a lot of folks are like extremely excited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2013" target="_blank">00:33:33.240</a></span> | <span class="t">of the idea of RWKB or state-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2015" target="_blank">00:33:35.760</a></span> | <span class="t">potentially having infinite context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2017" target="_blank">00:33:37.680</a></span> | <span class="t">But I think the reality is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2020" target="_blank">00:33:40.680</a></span> | <span class="t">when we say infinite context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2021" target="_blank">00:33:41.760</a></span> | <span class="t">we just mean a different kind of infinite context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2025" target="_blank">00:33:45.240</a></span> | <span class="t">or as it's previously covered,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2026" target="_blank">00:33:46.520</a></span> | <span class="t">you need to test the model differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2028" target="_blank">00:33:48.480</a></span> | <span class="t">So think of it more along the lines of the human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2031" target="_blank">00:33:51.160</a></span> | <span class="t">Like, I don't remember what I eat for breakfast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2033" target="_blank">00:33:53.680</a></span> | <span class="t">yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2034" target="_blank">00:33:54.840</a></span> | <span class="t">Yeah, that's the statement that I'll say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2037" target="_blank">00:33:57.440</a></span> | <span class="t">And we humans are not quadratic transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2041" target="_blank">00:34:01.600</a></span> | <span class="t">If we did, if let's say we increase our brain size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2044" target="_blank">00:34:04.840</a></span> | <span class="t">for every second we live,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2046" target="_blank">00:34:06.360</a></span> | <span class="t">we would have exploded by the time we are five years old</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2048" target="_blank">00:34:08.320</a></span> | <span class="t">or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2049" target="_blank">00:34:09.440</a></span> | <span class="t">And I think basically fundamentally for us, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2053" target="_blank">00:34:13.160</a></span> | <span class="t">be it whether we, regardless of whether RWKB,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2055" target="_blank">00:34:15.720</a></span> | <span class="t">state-space, XLSTM, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2058" target="_blank">00:34:18.560</a></span> | <span class="t">our general idea is that instead of that expanding state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2061" target="_blank">00:34:21.600</a></span> | <span class="t">that increase in computational cost,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2063" target="_blank">00:34:23.520</a></span> | <span class="t">what if we have a fixed state size?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2066" target="_blank">00:34:26.240</a></span> | <span class="t">And information theory detects that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2069" target="_blank">00:34:29.000</a></span> | <span class="t">that fixed state size will have a limit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2071" target="_blank">00:34:31.320</a></span> | <span class="t">Just how big of a limit is a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2074" target="_blank">00:34:34.120</a></span> | <span class="t">Like, RWKB is running at 40 megabytes for a state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2079" target="_blank">00:34:39.120</a></span> | <span class="t">Its future version might run into 400 megabytes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2081" target="_blank">00:34:41.760</a></span> | <span class="t">That is like millions of tokens in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2085" target="_blank">00:34:45.680</a></span> | <span class="t">if you're talking about mathematically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2087" target="_blank">00:34:47.040</a></span> | <span class="t">the maximum possibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2089" target="_blank">00:34:49.280</a></span> | <span class="t">It's just that I guess we are all more inefficient about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2091" target="_blank">00:34:51.760</a></span> | <span class="t">So maybe you would hit 100,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2093" target="_blank">00:34:53.560</a></span> | <span class="t">and that's kind of like the work we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2095" target="_blank">00:34:55.080</a></span> | <span class="t">trying to like push it and maximize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2097" target="_blank">00:34:57.760</a></span> | <span class="t">And that's where the models will start deferring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2100" target="_blank">00:35:00.680</a></span> | <span class="t">because it will choose to forget things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2102" target="_blank">00:35:02.680</a></span> | <span class="t">it will choose to remember things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2104" target="_blank">00:35:04.240</a></span> | <span class="t">And that's why I think that there might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2106" target="_blank">00:35:06.280</a></span> | <span class="t">some element of right, but it may not be the same right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2108" target="_blank">00:35:08.480</a></span> | <span class="t">It may be the model learn things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2109" target="_blank">00:35:09.920</a></span> | <span class="t">And it's like, hmm, I can't remember that article.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2112" target="_blank">00:35:12.760</a></span> | <span class="t">Let me do a database search to search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2114" target="_blank">00:35:14.920</a></span> | <span class="t">Just like us humans,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2116" target="_blank">00:35:16.360</a></span> | <span class="t">when we can't remember the article in a company,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2118" target="_blank">00:35:18.360</a></span> | <span class="t">we do a search on Notion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2119" target="_blank">00:35:19.800</a></span> | <span class="t">- Yeah, I think something that would be really interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2122" target="_blank">00:35:22.480</a></span> | <span class="t">is if you could have facts that are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2125" target="_blank">00:35:25.680</a></span> | <span class="t">so right now the one intuition about language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2129" target="_blank">00:35:29.520</a></span> | <span class="t">is that all those parameters are around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2131" target="_blank">00:35:31.360</a></span> | <span class="t">just to store random facts about the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2133" target="_blank">00:35:33.640</a></span> | <span class="t">And this intuition comes from the observation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2135" target="_blank">00:35:35.840</a></span> | <span class="t">that if you take a really small language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2138" target="_blank">00:35:38.280</a></span> | <span class="t">it can do things like talk to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2139" target="_blank">00:35:39.800</a></span> | <span class="t">or it kind of has like the style of conversation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2144" target="_blank">00:35:44.000</a></span> | <span class="t">it can learn that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2144" target="_blank">00:35:44.960</a></span> | <span class="t">But where it will usually fall over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2146" target="_blank">00:35:46.600</a></span> | <span class="t">compared to a much larger one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2147" target="_blank">00:35:47.760</a></span> | <span class="t">is it'll just be a lot less factual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2149" target="_blank">00:35:49.640</a></span> | <span class="t">about things that it knows or that it can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2152" target="_blank">00:35:52.960</a></span> | <span class="t">But that points to all those weights that we're spending,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2157" target="_blank">00:35:57.360</a></span> | <span class="t">all that SGD that we're spending to train these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2159" target="_blank">00:35:59.800</a></span> | <span class="t">are just being used to store facts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2161" target="_blank">00:36:01.760</a></span> | <span class="t">And we have things like databases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2163" target="_blank">00:36:03.080</a></span> | <span class="t">that are pretty good at storing facts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2164" target="_blank">00:36:04.720</a></span> | <span class="t">So I think one thing that would be really interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2166" target="_blank">00:36:06.560</a></span> | <span class="t">is if we could actually have some sort of outside data store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2170" target="_blank">00:36:10.520</a></span> | <span class="t">that a language model can look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2173" target="_blank">00:36:13.600</a></span> | <span class="t">that maybe has some sort of gradient descent in it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2179" target="_blank">00:36:19.040</a></span> | <span class="t">but would be quite interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2181" target="_blank">00:36:21.600</a></span> | <span class="t">And then maybe you could edit it, delete facts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2183" target="_blank">00:36:23.680</a></span> | <span class="t">change who's president so that it doesn't get lost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2188" target="_blank">00:36:28.440</a></span> | <span class="t">- Can we open up Q&A and hot takes to the audience?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2191" target="_blank">00:36:31.640</a></span> | <span class="t">I have hot take Q&A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2195" target="_blank">00:36:35.440</a></span> | <span class="t">Do these scale?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2196" target="_blank">00:36:36.640</a></span> | <span class="t">When 405 being state space model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2200" target="_blank">00:36:40.680</a></span> | <span class="t">RAG exists, no one does long context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2203" target="_blank">00:36:43.320</a></span> | <span class="t">who's throwing in 2 million token questions, what takes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2208" target="_blank">00:36:48.120</a></span> | <span class="t">- The who's throwing in 2 million token question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2210" target="_blank">00:36:50.440</a></span> | <span class="t">I think is a really good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2212" target="_blank">00:36:52.400</a></span> | <span class="t">So I actually, I was gonna offer that as a hot take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2215" target="_blank">00:36:55.680</a></span> | <span class="t">I mean, my hot take was gonna be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2216" target="_blank">00:36:56.800</a></span> | <span class="t">that long context doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2218" target="_blank">00:36:58.560</a></span> | <span class="t">I know I just gave a whole talk about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2220" target="_blank">00:37:00.600</a></span> | <span class="t">You know, what's the point of doing research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2224" target="_blank">00:37:04.480</a></span> | <span class="t">if you can't play both sides?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2226" target="_blank">00:37:06.680</a></span> | <span class="t">But I think one of the, so I think for both of us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2231" target="_blank">00:37:11.320</a></span> | <span class="t">the reason that we first got into this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2232" target="_blank">00:37:12.960</a></span> | <span class="t">was just from the first principle of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2235" target="_blank">00:37:15.680</a></span> | <span class="t">of there's this quadratic thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2238" target="_blank">00:37:18.920</a></span> | <span class="t">Clearly intelligence doesn't need to be quadratic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2241" target="_blank">00:37:21.240</a></span> | <span class="t">What is going on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2242" target="_blank">00:37:22.080</a></span> | <span class="t">Can we understand it better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2243" target="_blank">00:37:23.440</a></span> | <span class="t">You know, since then it's kind of turned into a race,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2248" target="_blank">00:37:28.280</a></span> | <span class="t">which has been exciting to watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2249" target="_blank">00:37:29.440</a></span> | <span class="t">like how much context you can take in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2251" target="_blank">00:37:31.720</a></span> | <span class="t">But I think it's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2252" target="_blank">00:37:32.560</a></span> | <span class="t">Nobody is actually putting in a 2 million context prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2255" target="_blank">00:37:35.320</a></span> | <span class="t">into these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2257" target="_blank">00:37:37.120</a></span> | <span class="t">And, you know, if they are, maybe we can go, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2261" target="_blank">00:37:41.400</a></span> | <span class="t">design a better model to do that particular thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2265" target="_blank">00:37:45.280</a></span> | <span class="t">- Yeah, what do you think about that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2266" target="_blank">00:37:46.440</a></span> | <span class="t">So you've also been working on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2268" target="_blank">00:37:48.000</a></span> | <span class="t">Do you think long context matters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2269" target="_blank">00:37:49.880</a></span> | <span class="t">- So I'm gonna burn a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2271" target="_blank">00:37:51.840</a></span> | <span class="t">How many of you remember the news of Google Gemini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2274" target="_blank">00:37:54.760</a></span> | <span class="t">is supporting 3 million context, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2276" target="_blank">00:37:56.720</a></span> | <span class="t">Raise your hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2277" target="_blank">00:37:57.560</a></span> | <span class="t">Yeah. - 2 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2279" target="_blank">00:37:59.760</a></span> | <span class="t">- Oh, it's 2 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2280" target="_blank">00:38:00.800</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2286" target="_blank">00:38:06.640</a></span> | <span class="t">How many of you actually tried that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2289" target="_blank">00:38:09.360</a></span> | <span class="t">See? - I use it a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2291" target="_blank">00:38:11.240</a></span> | <span class="t">- You, you're off of Mind's TV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2293" target="_blank">00:38:13.200</a></span> | <span class="t">(laughs)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2294" target="_blank">00:38:14.040</a></span> | <span class="t">- I use it a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2295" target="_blank">00:38:15.560</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2296" target="_blank">00:38:16.400</a></span> | <span class="t">So for some people that is used,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2298" target="_blank">00:38:18.600</a></span> | <span class="t">and I think that's the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2300" target="_blank">00:38:20.800</a></span> | <span class="t">that's might be like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2303" target="_blank">00:38:23.040</a></span> | <span class="t">this is where my opinion starts to differ</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2304" target="_blank">00:38:24.560</a></span> | <span class="t">because I think the big labs may have a bigger role in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2307" target="_blank">00:38:27.360</a></span> | <span class="t">because like, even for RWKB,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2309" target="_blank">00:38:29.560</a></span> | <span class="t">even when we train long context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2310" target="_blank">00:38:30.640</a></span> | <span class="t">the reason why I say VRAM is a problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2312" target="_blank">00:38:32.400</a></span> | <span class="t">is that because when we did the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2313" target="_blank">00:38:33.840</a></span> | <span class="t">we need to back prop against the states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2315" target="_blank">00:38:35.960</a></span> | <span class="t">we actually need to maintain the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2317" target="_blank">00:38:37.840</a></span> | <span class="t">in between the tokens by the token length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2320" target="_blank">00:38:40.520</a></span> | <span class="t">So that means we need to actually roll out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2322" target="_blank">00:38:42.800</a></span> | <span class="t">the whole 1 million context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2324" target="_blank">00:38:44.600</a></span> | <span class="t">if we are actually training 1 million,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2326" target="_blank">00:38:46.360</a></span> | <span class="t">which is the same for transformers actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2328" target="_blank">00:38:48.520</a></span> | <span class="t">but it just means we don't magically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2330" target="_blank">00:38:50.880</a></span> | <span class="t">reuse the VRAM consumption in the training time space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2333" target="_blank">00:38:53.920</a></span> | <span class="t">So that is the one, the VRAM bottlenecks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2336" target="_blank">00:38:56.040</a></span> | <span class="t">and I'm neither OpenAI nor Google,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2338" target="_blank">00:38:58.440</a></span> | <span class="t">so donate GPUs if you have too much of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2341" target="_blank">00:39:01.000</a></span> | <span class="t">But then putting it back to another paradigm, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2345" target="_blank">00:39:05.640</a></span> | <span class="t">is that I think O1 style reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2348" target="_blank">00:39:08.760</a></span> | <span class="t">might be actually pushing that direction downwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2352" target="_blank">00:39:12.120</a></span> | <span class="t">In my opinion, this is my partial hot take,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2354" target="_blank">00:39:14.520</a></span> | <span class="t">is that if, let's say you have a super big 400B model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2358" target="_blank">00:39:18.960</a></span> | <span class="t">and let's say you have a 70B model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2360" target="_blank">00:39:20.960</a></span> | <span class="t">that may take double the tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2363" target="_blank">00:39:23.680</a></span> | <span class="t">but gets the same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2365" target="_blank">00:39:25.440</a></span> | <span class="t">Strictly speaking, a 70B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2368" target="_blank">00:39:28.200</a></span> | <span class="t">and this is even for transformer or non-transformer, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2371" target="_blank">00:39:31.080</a></span> | <span class="t">will take less resources than that 400B model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2375" target="_blank">00:39:35.920</a></span> | <span class="t">even if it did double the amount of thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2378" target="_blank">00:39:38.480</a></span> | <span class="t">And if that's the case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2379" target="_blank">00:39:39.320</a></span> | <span class="t">and we're still all trying to figure this out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2381" target="_blank">00:39:41.600</a></span> | <span class="t">maybe the direction for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2382" target="_blank">00:39:42.640</a></span> | <span class="t">is really getting the sub-200B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2384" target="_blank">00:39:44.560</a></span> | <span class="t">to be as fast, as efficient as possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2386" target="_blank">00:39:46.400</a></span> | <span class="t">with a very efficient architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2388" target="_blank">00:39:48.240</a></span> | <span class="t">that some folks happen to be working on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2390" target="_blank">00:39:50.520</a></span> | <span class="t">to just reason it out over larger and larger context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2395" target="_blank">00:39:55.360</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2396" target="_blank">00:39:56.200</a></span> | <span class="t">- One thing I'm super interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2397" target="_blank">00:39:57.200</a></span> | <span class="t">is models that can watch forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2400" target="_blank">00:40:00.560</a></span> | <span class="t">Obviously you cannot train something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2403" target="_blank">00:40:03.880</a></span> | <span class="t">on infinite context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2406" target="_blank">00:40:06.080</a></span> | <span class="t">How are y'all thinking about that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2408" target="_blank">00:40:08.560</a></span> | <span class="t">where you run on a much longer context length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2411" target="_blank">00:40:11.160</a></span> | <span class="t">than is possible to train on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2414" target="_blank">00:40:14.080</a></span> | <span class="t">- Yeah, it's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2417" target="_blank">00:40:17.080</a></span> | <span class="t">So I think when,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2420" target="_blank">00:40:20.120</a></span> | <span class="t">I think you guys probably had tweets along these lines too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2423" target="_blank">00:40:23.040</a></span> | <span class="t">When we first started doing these things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2425" target="_blank">00:40:25.720</a></span> | <span class="t">because these are all recurrent models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2428" target="_blank">00:40:28.200</a></span> | <span class="t">in theory, you could just run it forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2429" target="_blank">00:40:29.880</a></span> | <span class="t">You could just run it forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2431" target="_blank">00:40:31.560</a></span> | <span class="t">And at the very least it won't like air out on your crash.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2435" target="_blank">00:40:35.200</a></span> | <span class="t">There's another question of whether it can actually use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2438" target="_blank">00:40:38.440</a></span> | <span class="t">what it's seen in that infinite context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2440" target="_blank">00:40:40.840</a></span> | <span class="t">And I think there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2442" target="_blank">00:40:42.200</a></span> | <span class="t">so one place where probably the research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2444" target="_blank">00:40:44.600</a></span> | <span class="t">and architectures ran faster than other research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2447" target="_blank">00:40:47.880</a></span> | <span class="t">is actually the benchmarks for long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2449" target="_blank">00:40:49.840</a></span> | <span class="t">So you turn it on forever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2451" target="_blank">00:40:51.960</a></span> | <span class="t">you wanna do everything or watch everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2454" target="_blank">00:40:54.240</a></span> | <span class="t">What is it that you actually wanted to do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2456" target="_blank">00:40:56.080</a></span> | <span class="t">Can we actually build some benchmarks for that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2458" target="_blank">00:40:58.280</a></span> | <span class="t">then measure what's happening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2459" target="_blank">00:40:59.720</a></span> | <span class="t">and then ask the question, can the models do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2462" target="_blank">00:41:02.320</a></span> | <span class="t">Is there something else that they need?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2465" target="_blank">00:41:05.000</a></span> | <span class="t">Yeah, I think that if I were to turn back the clock to 2022,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2469" target="_blank">00:41:09.000</a></span> | <span class="t">that's probably one of the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2470" target="_blank">00:41:10.320</a></span> | <span class="t">I would have done differently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2471" target="_blank">00:41:11.200</a></span> | <span class="t">which would have been actually get some long context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2474" target="_blank">00:41:14.080</a></span> | <span class="t">benchmarks out at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2476" target="_blank">00:41:16.920</a></span> | <span class="t">as we started pushing context length on all these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2480" target="_blank">00:41:20.040</a></span> | <span class="t">- I will also say the use case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2481" target="_blank">00:41:21.520</a></span> | <span class="t">So like, I think we both agree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2482" target="_blank">00:41:22.920</a></span> | <span class="t">that there's no infinite memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2485" target="_blank">00:41:25.640</a></span> | <span class="t">and the model needs to be able to learn inside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2487" target="_blank">00:41:27.600</a></span> | <span class="t">I think what we have observed for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2488" target="_blank">00:41:28.880</a></span> | <span class="t">I think this also fits the state space model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2490" target="_blank">00:41:30.640</a></span> | <span class="t">is that one of the key advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2492" target="_blank">00:41:32.320</a></span> | <span class="t">of this alternate attention mechanic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2494" target="_blank">00:41:34.000</a></span> | <span class="t">that is not based on token position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2496" target="_blank">00:41:36.240</a></span> | <span class="t">is that the model don't suddenly become crazy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2498" target="_blank">00:41:38.280</a></span> | <span class="t">when you go past the 8K training context length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2500" target="_blank">00:41:40.960</a></span> | <span class="t">or a million context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2504" target="_blank">00:41:44.280</a></span> | <span class="t">It's actually still stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2505" target="_blank">00:41:45.520</a></span> | <span class="t">It's still able to run, it's still be able to rationalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2507" target="_blank">00:41:47.800</a></span> | <span class="t">It just starts forgetting things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2510" target="_blank">00:41:50.000</a></span> | <span class="t">But some of these things are still there in latent memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2513" target="_blank">00:41:53.120</a></span> | <span class="t">Some of these things are still somewhat there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2514" target="_blank">00:41:54.520</a></span> | <span class="t">That's the whole point of why reading twice works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2517" target="_blank">00:41:57.720</a></span> | <span class="t">things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2518" target="_blank">00:41:58.680</a></span> | <span class="t">And one of the biggest push in this direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2520" target="_blank">00:42:00.960</a></span> | <span class="t">is that I think both state space and RWKB</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2523" target="_blank">00:42:03.280</a></span> | <span class="t">have separate papers by other researchers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2525" target="_blank">00:42:05.920</a></span> | <span class="t">where they use this architecture for time series data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2528" target="_blank">00:42:08.480</a></span> | <span class="t">weather modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2529" target="_blank">00:42:09.640</a></span> | <span class="t">So you're not asking what was the weather five days ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2533" target="_blank">00:42:13.520</a></span> | <span class="t">You're asking what's the weather tomorrow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2535" target="_blank">00:42:15.160</a></span> | <span class="t">based on the infinite length that we,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2538" target="_blank">00:42:18.600</a></span> | <span class="t">as on this earth and the computer will keep running.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2541" target="_blank">00:42:21.320</a></span> | <span class="t">So, and they found that it is better than existing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2546" target="_blank">00:42:26.320</a></span> | <span class="t">like we transform our existing architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2549" target="_blank">00:42:29.120</a></span> | <span class="t">in modeling this weather data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2550" target="_blank">00:42:30.880</a></span> | <span class="t">Control for the param size and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2552" target="_blank">00:42:32.320</a></span> | <span class="t">I'm quite sure there are people with larger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2553" target="_blank">00:42:33.920</a></span> | <span class="t">So there are things that in this case, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2557" target="_blank">00:42:37.920</a></span> | <span class="t">there is future applications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2559" target="_blank">00:42:39.360</a></span> | <span class="t">if your question is just what's next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2561" target="_blank">00:42:41.120</a></span> | <span class="t">and not what's 10 years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LPe6iC73lrc&t=2562" target="_blank">00:42:42.880</a></span> | <span class="t">- Thanks so much for having us.</span></div></div></body></html>