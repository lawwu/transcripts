<html><head><title>Ep4. Tesla FSD 12, Imitation AI Models, Open vs Closed AI Models, Delaware vs Elon, & Market Update</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Ep4. Tesla FSD 12, Imitation AI Models, Open vs Closed AI Models, Delaware vs Elon, & Market Update</h2><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA" target="_blank"><img src="https://i.ytimg.com/vi/bD1rrBBcJSA/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA&t=0 target="_blank"">0:0</a> Intro + Phase Shifts<br><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA&t=222 target="_blank"">3:42</a> Tesla FSD 12 & Imitation Learning<br><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA&t=1712 target="_blank"">28:32</a> AI Model Improvements | Open vs Closed Models<br><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA&t=2950 target="_blank"">49:10</a> Elon Musk Delaware Court Case<br><a href="https://www.youtube.com/watch?v=bD1rrBBcJSA&t=3510 target="_blank"">58:30</a> Macro Market Outlook<br><h3>Transcript</h3><div class='max-width'><p>I would make the argument that every company in Delaware has to move to a different domicile because they could be sued in a future derivative law suit for the risk they've taken by staying in Delaware. Oh, my god. Oh, my god, you're so right. You are so right. Mic drop on that.</p><p>Hey, Bill, great to see you. Good to see you, man. People loved when you were here last week in person, so we've got to make that happen again. But now, where are you? It looks like you're in Texas somewhere. I'm back in Texas, yes. All right. All right. So what's on your mind?</p><p>It's been a lot of action the last couple of weeks. What's going on? One thing that I reflect on quite a bit is just kind of how lucky we are to be a part of the venture capital industry in the startup world simply because things change so fast. And if you're a curious person, if you're someone that likes constant learning, it's really amazing.</p><p>Like, the stuff we're talking about, the stuff I'm listening to podcasts on every day, you know, two years ago didn't exist. And now, it's 80% or 90%-- 80% or 90% of the dialogue. And that's just pretty wild. Yeah, I know. Our brains really aren't programmed to work in kind of these exponentials, right?</p><p>I mean, you and I both know every sell-side model on Wall Street has linear deceleration and growth rates. Like, we think really-- you know, we're really good at thinking in kind of these linear ways. You know, I had that thought this morning that the biggest investment opportunities really do occur around these phase shift moments.</p><p>I mean, Satya talks about all the value capture occurs in the two- to three-year period around phase shifts. But it's hard to forecast in those moments, right? I mean, that's when you see these massive deltas, you know, in these forecasts. And I just went back and looked at, for example, at the start of last year, the consensus estimate of the smartest people covering NVIDIA day-to-day was that the data center revenue was going to be $22 billion for the year, right?</p><p>Guess what it ended up being? $96 billion. Wow. OK, they were off almost by a factor of 3 or a 4, right? The EPS at the beginning of last year, the earnings per share, was expected to be $5.70. And now it looks like it's going to be $25, right?</p><p>Like, over the course of your career, have you ever seen sell-side estimates off by that much on a large-cap stock? I mean, just like, you know, very, very rare. Like, you know, once a decade, maybe, you know, that something like this happens. Yeah, it's amazing. So, you know, and I've had investors say to me when the stock was at 200, hell, you and I talked about this.</p><p>You know, should we sell it all at 200? Sell it all at 300? Sell it at 400? And now, you know, those investors are calling me every day saying, have you sold it yet? Our general view is that if the numbers are going up, so if our numbers are higher than the street's number for whatever variant perception that we have, right, then the stock is going to continue to go higher.</p><p>At some point, the street will get ahead of itself and its numbers will now be higher or at the same level as ours. And at that point, I think it becomes more of a market performer. But of course, some things will be wildly overestimated and some things will be wildly underestimated, but that sort of discontinuity really occurs around these moments of big phase shifts.</p><p>So speaking of a big phase shift, right, we teased on the pod, I think at the start last time, that I had taken a test ride in Tesla's new FSD-12. And I said, you know, it kind of felt like a little bit of a chat GPT moment, but I think we left the audience hanging.</p><p>We got a lot of feedback. Hey, you know, dig in more to that. So you and I spent some time on this, both together and with some folks on the Tesla team. So roughly the setup here, background, I want to get your reaction to it, is about 12 months ago, the team pretty dramatically forked their self-driving model, right?</p><p>Moving it from this really C++ deterministic model to what they refer to as an end-to-end model that's really driven by imitation learning, right? So we think of this new model, it's really video in and control out. It's faster, it's more accurate, you know, but after 11 different versions of FSD, I think there's a lot of skepticism in the world.</p><p>Like, is this going to be, you know, something different? You sent me a video and I have tons of these videos, you know, floating around at the moment, you know, that really kind of shows, you know, how this acts more like a human than prior models out there. So Bill, kind of just react, you know, you've watched this video, react to this video and give us your thoughts.</p><p>You know, I think you've been a longtime observer of self-driving. I might even describe you as a bit of a critic of, you know, or a skeptic when it comes to full self-driving. So is this a big moment? Did I overstate it? Kind of, what are your thoughts here?</p><p>Yeah, so, you know, one of the critiques and concerns people had about self-driving is they would say that, yeah, we're 98% of the way there are 99, but the last 1% is gonna take as long as the first 99. And one of the reasons for that is, it's nearly impossible to code for all of the corner cases.</p><p>And the corner cases are where you have problems, that's where you end up in wrecks, right? And so the approach Tesla had been taken up until this point in time was one where you would literally try and code every object, every circumstance, every case in like a piece of software.</p><p>This X happens, then Y, right? And that ends up being a patchwork kind of a, just a big nasty, you know, rat's nest of code. And it builds up and builds up and builds up and maybe even steps on itself. And it's not very elegant. What we learned this week is that they've completely tossed all of that out and gone with a neural network model where they're uploading videos from their best drivers.</p><p>And literally the videos are the input and the output is the steering wheel, the brake and the gas pedal. And, you know, there's this principle known as Occam's razor, which has been around forever in science. But the simplified version of it is a simpler approach is much more likely to be the optimal approach, right?</p><p>And when I fully understood what they had done here, it seems to me this approach has a much better chance of going all the way and of being successful. And certainly of being maintainable and reasonable. It's way more elegant. It requires them to upload a hell of a lot of video, which we can talk about.</p><p>But, and the other thing that's just so damn impressive is that this company, which is very large, hundreds of thousands of employees, made a decision so radical to kind of throw out the whole thing and start afresh. And it sounds like the genesis of that may have been, you know, three or four years ago, but they got to the point where they're like, this is gonna be way better and threw the whole thing out.</p><p>And I think about four months after they made the change, Elon did a drive where he uploaded and kind of streamed the drive. So we can put that in the notes and people can watch it. But it's way, way different. It's way, way different. And in my mind, you know, basically with this Occam razor's notion, it's got a much higher chance of being wildly successful.</p><p>Yeah, let's dig in a little bit into how it's different, right? So, and you referenced a little of this. So, you know, like for example, this model does not have a deterministic view of a stoplight, right? I mean, McCarthy has talked about this before, you know, before you have to label a stoplight, right?</p><p>So you would basically take the data from the car. That would be your perception data. You would draw a box around a stoplight. You would say, this is a, you know, this is a stoplight. So that your first job on the car would have to be to identify that you're at a stoplight.</p><p>Then the second thing is you would write all of this C++ that would deterministically say, when you are at a stoplight, here's what the controls should do, right? And so for all of that second half of the model, you know, the heuristics, the planning and the execution, that was all driven by this patchwork that you're talking about.</p><p>And that was like, you would just chase, you know, every one of these corner cases and you could never solve them all. Now in this new model, it's pixels in. So the model itself has no code. It doesn't know this is a stoplight per se. In fact, they just watched the driver's behavior.</p><p>So the driver's behavior is actually the label. It says, when we see pixels like this on the screen, here's how the model should behave, which I thought is just an extraordinary break. And I don't think there's a deep appreciation for the fact that, you know, again, because we've had 11 versions of what came before it, those were just slightly better patchwork models.</p><p>In fact, I think what, you know, we learned was that the rate of improvement of this is order of magnitude five to 10X better per month as a model versus the rate of improvement of those prior systems. - And once again, the audacity to throw out the whole old thing and put a new thing in is just crazy.</p><p>One thing for the listeners, well, actually two things I would mention. One, in terms of just how they got this going, you know, a lot of people I fear equate AI with LLMs because it was really the arrival of ChatGPT and the LLM that I think introduced what AI was capable of to most people, but those are language models.</p><p>That's what one of the L stands for. And these AI models that Tesla's used for FSD-12 are these generic open source AI models that you can find on Hugging Face, you know, and they obviously customized them. So there's some proprietary code there at Tesla, but, you know, AI has been evolving for a very long time.</p><p>And this notion of neural networks was around before the LLMs popped out, which is why, you know, they had started on this four years ago or whatever, right? But the foundational elements, you know, are there. And by the way, they use the hardware that we're talking about, right? They use the big NVIDIA clusters to do the training.</p><p>They need some type of GPU or TPU to do the inference at runtime. So it's the same hardware the LLMs use, but it's not the same type of code. I just thought that was worth mentioning. Yeah, no, it's a, to me, if we dig in a little bit to, you know, the model itself, you know, the transformers, the diffusion architecture, the convolution neural nets, those are all like these modular open source building blocks, right?</p><p>Like the thing that's extraordinary to me, and we're gonna get later in the pod to this open versus closed debate, but like, this is just this great example, you know, you talk about ideas having sex. I mean, these open source module, you know, kind of modular components, those have been worked on for the last decade.</p><p>And now they're bringing those components together, and now all of their energy, and I wanna dig into this a little bit, that is really going, they're taking all these engineers who were writing the C++, these deterministic, you know, patches effectively, and now they're focusing them on how do we make sure that our data infrastructure, that the data that we're pulling off of the edge comes in and makes these models better.</p><p>So all of a sudden it becomes about the data, because the model itself is just digesting this data, brute forcing it with a lot of this, you know, NVIDIA hardware and outputting better models. - You know, it's such a classic Silicon Valley startup thing where you need all the pieces to line up.</p><p>If you go back and watch, if you haven't watched, if anyone's watched the General Magic video, which is fantastic, it's on the internet, about why General Magic didn't work. And Tony Fidel, who ended up building the iPod and ran engineering for the iPhone, talks about how the pieces just weren't there.</p><p>So they were having to do all the pieces, right? The network and the chips, and it just wasn't there yet. And so these models have been around, maybe ahead of the hardware, and now NVIDIA's bringing the hardware, and these pieces start to come together. And then the data, and I think one of the most fascinating things about this story of Tesla and FSD-12 is when you understand where they get the data.</p><p>So they are tracking their best drivers with five cameras, and the drivers know it. They've opted into the program, and they upload the video overnight. And so, you know, talk about the pieces coming together. We've found Reddit forums and stuff we can put links to in the notes where Tesla drivers are saying they're uploading 10 gigabit a night.</p><p>And so, you know, you had to have the Wi-Fi infrastructure, like, how would it be possible to upload that much? Here's someone who's Tesla uploaded 115 gigabyte in a month, right? And so these are massive numbers, and the infrastructure, five years ago, your car couldn't have done this. And, you know, I think we'll talk about competition in a minute, but like, you know, who else has the capacity to do this, right?</p><p>It's unbelievable to, like, the footprint of cars they have. And then the notion that, oh yeah, we could just go upload this data, and it is a buttload of data that's coming. Right, and even with this architecture, so you just do the math. Five million cars, 30 miles a day, I think eight cameras on the car, five megapixels each, and then the data going back 10 years, right?</p><p>This amount of shadow data, you could combine the clusters of every hyperscaler in the world, and you couldn't possibly store all of this data, right? That's the size of the challenge. So what they've had to do is process this data on the edge. And in fact, I think 99% of the data that a car collects never makes it back to Tesla.</p><p>So, you know, they're using video compression, these remote send filters, they're running, you know, neural nets and software on the car itself. So basically they, you know, for example, if 80% of your driving is the highway and there's nothing interesting that happens on the highway, then you can just throw out all that data.</p><p>So what they're really looking for is, you know, what is the data that is a long way away from the mean data, right? So what are these outlier moments? And then can we find tens or hundreds or thousands of those moments to train the model? So they're literally pulling this compressed, filtered data every single night off of these cars.</p><p>They've built an autonomous system. So before they would have engineers look at that data and say, okay, what have we perceived here now? How do we write, you know, this patchwork code? Instead, this is simply going into the model itself. It's fine tuning the model. And they're constantly running this autonomous process of fine tuning these models.</p><p>And then they're re-uploading those models back to the car. Okay, this is why you get these exponential moments of improvement, right, that we're seeing now, which then brings us back to build this question. You know, Tesla has 5 million cars on the road. They have all this infrastructure. They are collecting this data.</p><p>We know they're a couple of years ahead. Think about Waymo, for example. They're still using the old architecture. It's geo-fenced. I don't know, they have 30 or 40 cars on a road, and they're only running the-- so do they have any chance? Does Waymo have any chance of competing or even adopting this architecture?</p><p>It'd be-- it's such an interesting question. And by the way, just one quick comment on the previous thing you said. It's genius, actually, that they are-- they've taught the car what moments it should record. Exactly. And so they mentioned to us an example of any time there's-- well, obviously, a disengagement.</p><p>So a disengagement becomes a moment where they want the video before and the video after. The other thing would be any abrupt movement. So if the gas goes fast, or if the brake has hit quickly, or if the steering wheel jerks, that becomes a recordable moment. And the part I didn't know, which they told us, which is just fascinating, people with LLMs have heard about reinforcement learning from human feedback.</p><p>RLHF. And they've talked about how that could make it-- even with Gemini, they said maybe that was what caused that. What we were told is that those moments, these moments where the car jerks or whatever, if it is super relevant, they can put that in the model with extra weight.</p><p>And so it tells the model, if this circumstance arises, this is something that's more important and you have to pay extra attention to. And so if you think about these corner case scenarios, which we all know are the biggest problems in self-driving, now they have a way to only capture the things that are most likely to be those things and to learn on them.</p><p>So the amount of data they needed to get started was this impossible amount of data with the millions of cars. And now the way that plays to their advantage is they're much more likely to capture these more severe, less frequent moments because of the bigger footprint. And so you ask the question, I don't know who could compete.</p><p>It certainly couldn't-- let's make an assertion. If this type of neural network approach is the right answer, and once again, Occam's razor seems that way to me, then who could compete? And several of the companies who would be least likely would be Cruise and Waymo and these things because they just don't have that many cars.</p><p>And their cars cost $150,000. So if they wanted to have-- the math just doesn't work. You can't build the footprint. And so who could? I don't know. I don't know. What would it cost to build a five-camera device to put on top of every Uber? I don't know. A lot, it would be weird.</p><p>They're not going to do it. And that, to me, is-- when you look at these alternative models, if this really is about data-- and remember, Bill just said an important point, which is it's not just about quantity of data. Something magic happens around a million cars. Yes, you've got to get all that quantity of data.</p><p>But to get the long-tail events-- these are events that occur tens or just hundreds of times. That's where you really need millions of cars. Otherwise, you don't have a statistically relevant pool of these long-tail instances. And what they're uploading from the Edge, Bill, he said, each instance is a few seconds long of video, plus some additional vehicle-driving metadata.</p><p>And it's those events. If you only have hundreds of cars or thousands of cars, you can get a lot of data quickly. It's not about quantum of data. 100 cars can produce a huge quantum of data driving 1,000 miles. It's about the quality of the data, those adverse events.</p><p>Yes, and I guess the other type of company that maybe could take a swing at it would be like Mobileye or something. The problem they have is they don't control the whole design of the car. And so this part where Tesla has the car in the garage at night and uploads gigabytes and puts it right into the model, are they going to be able to get that done working with other OEMs?</p><p>Are they going to be able to organize all that? Do they have the piece on the car that says when to record and when not to record? It's just a massive infrastructure question. I would probably, if I had to handicap anybody, it would probably be BYD or one of the Chinese manufacturers.</p><p>And if you think about it, they have a lot of miles driven. In China, much less so outside of China, I imagine you're going to have some of this nationalistic stuff that emerges on both ends of this. But one of the things I asked our analyst, Bill, is if we just step back, I think these guys have network advantage.</p><p>They have data advantage. They're clearly in the lead. They have bigger H100 clusters than the people they're competing against. I mean, they have all sorts of things that have come together here. But if you think about what's the so what to Tesla? And just in the first instance, and we'll pull up this slide that Frieda on our team made, if you look at the unit economics of a Tesla, with no FSD, they're making about $2,500 on a vehicle.</p><p>If you look at it today, they have about 7% penetration of FSD. That was, let's call it, through FSD-11. And those people paid $12,000 incrementally for that FSD. And as we know, you can go read about it on Twitter. People are like, yeah, it's good, but it's not as good as I thought it would be.</p><p>So now we have this big moment of what feels like kind of a step function, the model getting better at a much faster rate. So I asked the question, what if we reduce the price on this by half? What if Tesla said, this is such a good product, we think we want to drive penetration, so let's make it $500 a month, not $1,000 a month?</p><p>So if you assume that you have penetration go from 7% to 20%, give it to everybody for free, they drive around for a month, they're like, wow, this really does feel like a human driver. I'm happy to pay $500 a month. If you get to 20% penetration, then your contribution margin at Tesla is about the same, even though you're charging half as much.</p><p>Now, if you get to 50% penetration, all of a sudden you're creating billions of dollars in incremental EBITDA. Now, think about this from a Tesla perspective. Why do they want to drive even more adoption of FSD? Well, you get a lot more information and data about disengagements and all these other things.</p><p>So that data then continues to turn the flywheel. So my guess is that Tesla, seeing this meaningful improvement, is going to focus on penetration. My guess is that they want to get a lot more people trying the product, and they're going to play around with price. Why not? Maybe $100 a month is the right intersection between adoption or penetration and price.</p><p>But again, I think that all of these things are occurring at an accelerating rate at Tesla. And when I look around, I still hear people saying Waymo's worth $50 or $60 billion. But you could be in a situation on that business where it just gets passed really quickly, and they have a hard time structurally of catching up.</p><p>People have said that-- and if someone has data, once again, that they want to correct this, I'd be glad to state to recorrect the data. But we've been told they have a head count similar to Cruise. And the Cruise financials came out, and they were horrific. And so I don't have any reason to believe that the Waymo financials are any different than the Cruise ones.</p><p>And I've always thought this model, that we're going to build this incredible car, and our business model is going to be to run a service. Like the CapEx, if you just build a 10-year model, the CapEx you need, they would have to go raise $100 billion. And there's another element that's super interesting.</p><p>The team at Tesla feels very strongly that LIDAR does not need to be a component of this thing. And so the Waymo, Cruise, all those approaches, and Mobileye are LIDAR-dependent, which is a very costly piece of material in those designs. And so if this is all true, if this is how it plays out, it's a pretty radical new discovery.</p><p>So one of the things I also want to talk about, because one of the reasons I started going down this path is our team's been spending a lot of time with the robotics companies, new robotics companies. So we have Optimus at Tesla. Figure.ai just raised some money from OpenAI and Microsoft, and we met with those guys.</p><p>And they're all doing really interesting things. But again, they're shifting their models. The robotics companies also were using these deterministic models to teach the robot maybe how to pour a cup of coffee or something. And now they're moving to these imitation models. So I was searching around the other day, and I came across this video by a PhD student at Stanford, Ching Cheh.</p><p>And he showed how this robotic arm was basically just collecting data very quickly using a little camera on a handheld device. And then they literally take the SD card out of the camera. They plug it into the computer. It uploads this data to the computer. It refreshes the model.</p><p>And just based on two minutes of training data, now video in, control out, this robotic arm knows how to manipulate this coffee cup in all of these different situations. So I think we're going to see the application of these models, end-to-end learning models, imitation learning models, impact not just cars.</p><p>I mean, 5 million cars on the road, that's probably the best robot we could possibly imagine for data collection. The challenge, of course, in robotics is going to be data collection. But then I saw this video, and I said, well, maybe that's a manageable challenge, particularly for a discrete set of events.</p><p>Yeah, and the other great thing about that video, if people take the time to watch it, it actually explains pretty simply how the Tesla stuff's working, right? I mean, it's just a different scale, obviously, but that's the exact same thing, just at a very reduced state. Right, and you can imagine when that's just this autonomous flywheel without a lot of human intervention.</p><p>And that's the direction that Tesla still has some engineering intervention along the way. But I think the engineering team working on this at Tesla is about 1/10 the size of the teams at Cruze. Well, I mean, that gets back to this simplicity point, right? Like, this approach removes so much complexity that you should be able to do it with less people.</p><p>And the fact that you can have something better with less people is really powerful. So we talked a little bit about how models, these open source models, are driving a lot of the improvements at Tesla. We seem to get model improvements and model updates every day, Bill. Maybe I just go through a few of the recent ones.</p><p>And I want to explore this open versus closed. But last week, we heard about Gemini 1.5. It has a huge expanded context window. And Gemini 1.5, about a chat GPT-4 level. Then yesterday, we get Claude III announcements. Their best model, Opus, is just a little bit better than chat GPT-4.</p><p>But I think the significant thing there-- and we have a slide on this-- is just really about the cost breakthrough, that their SONET level model can do workloads at a fraction of the price of chat GPT-4, even though it's performing at or near that quality. And then we have-- those models were trained on a mixture, I think, of H100 and prior version of NVIDIA chips.</p><p>The first H100-only trained models, I think, will be LLAMA III and chat GPT-5. So we're hearing rumors that both of those models are going to come out in the May-July time frame. With respect to LLAMA III that was trained on Meta's H100 cluster, rumors are that it has Claude III-like performance, which is pretty extraordinary if you're thinking about a fully open-sourced model.</p><p>And then chat GPT-5, which we hear is done. And they're simply in kind of their post-training safety guardrails, their normal post-training work. We hear that's going to launch sometime in May versus June. And because that one was trained on H100s, we hear it is like a 2x improvement versus chat GPT-4.</p><p>But then we hear all the rest of the Frontier models are kind of in this holding pattern because they're waiting for the B100s to get launched to this Q3, Q4 out of NVIDIA, which probably means the next iteration of the Frontier models will come out in Q2 of next year, Q2 of '25.</p><p>That's after chat GPT-5. So Bill, if you go through this bedrock page on AWS, if you just scroll through, you see that Amazon is offering all these different models. I mean, you can run your workloads on LLAMA, on Mistral, on Claude, et cetera. Snowflake today just announced a deal with Mistral.</p><p>And they're going to have LLAMA as well. Imagine Databricks will. Microsoft, you can use LLAMA, or you can use Mistral, or OpenAI. So where do you think all of this goes in terms of the models that will actually get used by enterprises and consumers in practice? Yeah, so I have a lot of different thoughts.</p><p>My first one, when this new Anthropic thing came out and they list all the different math tests, and science tests, and PhD, and they're all listing the same thing, I wonder if they're racing up a hill. But they're all racing up the same hill. Yeah, there's the thing. Because they're all running these same comparative tests, and they're all releasing this data.</p><p>And what I don't know, if any of them are creating the type of differentiation that's going to lead to one of them becoming the wholesale winner versus the other. And is this type of micro-optimization in a way that's going to matter to people or to the users? And it's not clear to me.</p><p>I mean, I see some developers get way more excited about the pricing at the low end of those three choices than they do about the performance of the top end. So that's one thing. The second thing on my mind, I don't have a lot of logic to put around this.</p><p>It's more of an intuition. I wonder if these companies can simultaneously try and compete with Google to be this consumer app that you're going to rely on to get you information. So you could call that Wikipedia on steroids, Google search redefined, whatever market you want to call that. And simultaneously be great at enterprise models.</p><p>And I just don't know if they can do both. I really don't. And maybe that'll get to the third thing, which is more the essence of your question. What am I hearing about and seeing about when it comes to companies that are actually utilizing these things? The Tesla example was interesting because they start with these bedrock components that are open source.</p><p>And one thing that happened in the past 20 years-- it happened very slowly, but we definitely got there-- CIOs at large companies, they used to be an IBM shop or an Oracle shop or a Microsoft shop. That was their platform. They slowly got to the place where most of the best CIOs were open source first.</p><p>And so for any new project they start, they used to be skeptical of open source. And it slipped completely the other way. Like, oh, is there an open source choice we can use? And the reason is they don't-- one, there's more competition. And two, they don't want to get stuck on anything.</p><p>And so when I look at what I see going on in the startup world, they might start with one of these really well-known service models that's proprietary. But the minute they start thinking about production, they become very cost-focused and on the inference side. And they'll just play these things off of one another, and they'll run a whole bunch of different ones.</p><p>I saw one startup that moved between four different platforms. And I just think that that competition is very different than the competition to compete with Google on this consumer thing. And I'll give you another example. Like, I was talking to somebody. If you had a legal application you wanted to use, you'd be better off with a smaller model that had been trained on a bunch of legal data.</p><p>It wouldn't need some of the training of this overall LLM. And it might be way cheaper to have something that's very proprietary-- or not proprietary, but very focused from a vertical standpoint. And you could imagine that in a whole bunch of different verticals. So it just strikes me that on the B2B side, this stuff's getting cut up and into a bunch of different pieces where a bunch of different parties could be more competitive, and where those components are most likely to be open source first.</p><p>Yes, yes. I mean, you're causing me to think a couple of different things. One, I've said in the past, if I was Sam Altman running OpenAI, I think I might rename the company ChatGPT and just focused on the multi-trillion dollar opportunity to replace Google. Because I think winning at both-- beating Google at consumer and beating Microsoft at enterprise-- Andy wants to beat NVIDIA at building chips-- those are three big battlefronts.</p><p>And if I think about the road to AGI-- building memory, building all this thing that's going to differentiate you in the consumer competition-- that just seems best aligned with who they are, what they're doing. I mean, ChatGPT has become the verb in the age of AI. They replaced Google at the start.</p><p>Nobody's saying we're barding something. They're saying we're ChatGPTing something. So I think that they have a leg up there. When I look at the competition in enterprise-- I think Anthropic was up at the Morgan Stanley conference this morning, and they said they're hiring-- their sales force went from two people last year to 25 this year.</p><p>Think of the tens of thousands of salespeople at Microsoft, at Amazon, et cetera, that you got to go compete with. Now, of course, they're also partnering with Amazon. But when you think about that, these guys-- there's going to be all this margin stacking, Bill. So Amazon's got to get paid.</p><p>Anthropic's got to get paid. NVIDIA's got to get paid. Now, if you use an open source model, you can pull one of those pieces of the margin stacking out. So now, this is just Microsoft getting paid using LLAMA 3 or LLAMA 2. They don't have to pay for the use of that model.</p><p>And NVIDIA gets paid. So I think in the competitive dynamics of an open marketplace that that enterprise game is going to be tough for two different reasons for these model businesses. Number one, Zuckerberg is going to drive the price. He's going to give away Frontier-esque models on the cheap.</p><p>And that's going to be highly disruptive to your ability to stack margin. If I'm a CIO of JP Morgan or some other large institution, do I really want to pay a lot for that model? I'd rather have the benefit of open, because then I can move my data around a little bit more fluidly.</p><p>I get the safety benefits of an open source model. And I'm not sending my data to OpenAI. I'm not sending my data to some of these places. Huge point you just made that is in addition to everything we said, which is a lot of the big companies have concerns about their data being commingled or uploaded, even at all, into these proprietary models.</p><p>And so it's not just-- I think the challenge for them in enterprise is not just how do I build an enterprise fleet to go compete with the largest hyperscaler in the world who are great enterprise businesses, and you've got to compete with Databricks and Snowflake, et cetera. But I think the second thing is just there is this bias, this tendency that you say has evolved over a couple of decades of open versus closed, which then brings me a little bit to this-- But wait, there's one more element that I think that's important for everyone to understand.</p><p>One of the reasons open source is so powerful is because it can be replicated for free, you end up with just so much more experimentation. So it turns out right now there are multiple startups who believe they have an opportunity hosting open source models. So they're propping up Llama3 or Mistraw as a service provider competing with Amazon, but they're going to tune it a little different way.</p><p>They're going to play with it a little different way. So the number of places you can go buy one of these open source models delivered as a service is you have multiple choices. It's been proliferated. And that creates optionality. There's just so much more experimentation that's going to happen.</p><p>On top of the data privacy problem, the pricing stuff you talked about. So there's a lot of different elements that make me think that the open source component models are going to be way more successful in the enterprise. And it's a really tough thing to compete with. Now, go ahead.</p><p>Well, it kind of brings into stark relief a big debate that erupted this week, certainly on the Twitters, with Elon's lawsuit that he filed. And part of that was about this not-for-profit to for-profit conversion. That's, to me, a little bit less interesting. Don't want to talk a lot about that.</p><p>But it blew the doors wide open on this open versus closed debate, right? And the potential that exists here for regulatory capture. Nobody's more thoughtful about this topic than you. I think I saw somebody tweet this 2x2 matrix. It says dividing every conversation up between Mark and Vinod and Elon and Sam.</p><p>But we saw a lot of very sharp opinions expressed. So help us think about the risk of regulatory capture and why this moment is so important. Yeah, and I happened to mention this when I did my regulatory capture speech at the all-in conference. I mentioned very briefly when I showed a picture of Sam Altman that I was worried that they were attempting to use fear-mongering about dumerism and AI to build regulation that would be particularly beneficial to the proprietary models.</p><p>And then after that, there were rumors that people at some of the big model companies were going around saying we should kill open source, or we should make it illegal, or we should get the government to block it. And then Vinod started basically saying that, literally, like, yes, we should block open source.</p><p>And that became very concerning to me. I think it obviously became concerning to Mark Andreessen as well. And for me, the biggest reason that it's concerning is because I think it could become a precedent where all companies would try and eliminate open source. And there's a good reason why.</p><p>I mean, we just talked about it's a hell of a fucking competitor. Like, I wouldn't want to go up against it. But it's also really amazing for the world. It's great for startups. It's amazing for innovation. It's great for worldwide prosperity. Think about Tesla. We just talked about all this open source that they're using.</p><p>Yeah, yeah. So it's the last thing I would want to see happen. But we do live in this world where these pieces exist. And I would urge people to read-- we'll put a link in-- a Politico article that shows the amount of lobbying that has been done on behalf of the large proprietary models.</p><p>And I don't think you'll find-- literally, the only thing that comes close, perhaps, and people will think I'm being outlandish, but is SBF, who was also lobbying at this kind of level. But this Politico article shows they have three or four different super PACs. They're putting people-- they're literally inserting people onto the staffs of the different congressmen and senators to try and influence the outcome here.</p><p>I think we maybe escaped this. I think the open source models are so prolific right now that maybe we've gotten past it. And I also think their competitiveness has shown that there's a reason why they would want to stop them. I mean, I think at the time they started, maybe that wasn't clear.</p><p>But I think it's remarkably clear right now. I also don't believe in the Dumerism scenario. Someone who I admire quite a bit, Steve Pinker, posted a link to this article by Michael Totten where he goes through, I think in a very sophisticated way, the different arguments. And I would urge people maybe to read that on their own.</p><p>But yeah, I don't-- for me, if you want to spread the Dumerism, let's get people to tell that story that aren't running billion-dollar companies that are taking hundreds of millions out and giving it to their employees. I mean, there's a level of bias that's obvious here. And so I'd rather listen to a Dumerism argument from someone who's not standing to gain from regulation.</p><p>Yeah, I mean, I think you saw this tweet from Martin Casado that was in response to Vinod comparing open source-- would you use open source for the Manhattan Project, which really kind of opened up this box even more. What's your-- weigh in a little bit here. Just if you're in Washington and you're hearing these things like, we can't allow these types of models to be used on things like this.</p><p>We saw India is now requiring approval to release models. That also was, I think, a scary development for people in the open source community. But again, just reinforce, why should we not be worried about open source AI models? How do they send us to a better place? In the Totten article, Pinker uses an analogy that I just love, which he says, you could spread a Dumerism argument that a self-driving car would just go 200 miles an hour and run over everybody.</p><p>But he says, if you look at the evolution of self-driving cars, they're getting safer and safer and safer. We don't program the AI to give them this singular purpose that overrides all the other things they've been taught, and then they go crazy. That's not what's happening. That's not how the technology works.</p><p>That's not how we use the technology. And so I think the whole article is great, but I think-- and look, I also think Pinker is a really smart human. He's also one of the biggest outspoken proponents of nuclear, which is another topic that I think has been wildly misconstrued.</p><p>And so anyway, I'm more of an optimist about technology. These kind of Dumerism things go way back to the Luddites, hence the definition of the word, and ever since then. And someone else tweeted it'd be like telling the farmer, look out for the tractor. It's going to ruin you.</p><p>It's just not how our world evolves. Well, the reason I think this is so important is because the competition that's going to come from these models, all the evidence suggests that it moves us to a better place, but not worst place. However, during these moments where you do have a new thing, and it does sound scary, and then you have all these people coming to Washington saying, hey, we can't allow all this experimentation.</p><p>We can't allow these open source models. What I worry about is that that can actually win the day like it has in India. But I was in Washington last week talking to leadership in both the House and the Senate about a program near and dear to me called Invest America.</p><p>But the conversation about AI came up with many senators and many senior leadership folks in the House. And one of them said to me, when he was asking about AI, I said I was worried about excessive government oversight getting persuaded, particularly as it relates to open source models. And he said, don't worry.</p><p>He said, we had Sam Altman out here, and we know what he's up to. Oh, that's great. And I thought that was-- and he ended by saying, we need competition. Like, the way we stay ahead of China is we need competition. So that was highly encouraging to me from a senior member of the House.</p><p>By the way, it's interesting. That's so great to hear. And I think this China thing comes up all the time. The one thing that would cause us to get way behind China is if we played without open source and they had it. And the other thing I would just say is many academics I talk to are like, I have way more trust in open source where I can get in and see and analyze what's going on.</p><p>And the other side of this, because we talked about LLMs, or AI competing both in the B2B side and the B2C side, on the consumer side, the Gemini release from Google, I think, is proof of the type of-- the Google Gemini model was much more similar to something autocratic that you might equate with a communist society.</p><p>Like, it's intentionally limiting the information you can have and painting it in a very specific way. And so, yeah, I'm more afraid of the proprietary. Yeah, they're effectively imposing a worldview by massaging the kernel here in ways that we don't understand. It's a black box influencing our opinions. And I just find it ironic in this moment in time that the person putting the most dollars up against the open source is somebody we're critical of, Washington was pretty critical of a couple of years ago, which is Zuckerberg.</p><p>And the fact of the matter is you need to have a million H100s. He's going to have hundreds of thousands of B100s. You need somebody who has a business model that can fund this level of frontier magic on these open source models. And the good news, it appears we have it.</p><p>Yeah. That's awesome. I'm thrilled you heard that. You know, there was another interesting case over the course of the last couple of weeks that I know you and I-- By the way, actually, one last thing on this because I just recalled a conversation I was having with a senator.</p><p>Let's assume that Dumerism's right and you have to be worried about this. What are the odds that our government could put together a piece of effective legislation that would actually solve the problem? Right. Right. Well, I mean, I think the cost to society is certainly greater when you look at kind of the tail risk of it.</p><p>But again, how the node frames it, what I get worried about, I have no problem in him having an active defense and wanting to do everything in open AI's best interest. I just don't want to see us attack technological progress, which open source obviously contributes to in route to that.</p><p>Just compete against them heads up and win heads up, like that's fine. But let's not try to cap the other guys by taking their knees out before they even get started. So back to what I was saying, speaking of government's role in business, a couple of weeks ago, the state of Delaware, the chancery court, this judge, Kathleen McCormick, she pretty shockingly struck down Elon's 2018 pay package.</p><p>Remember, the company was on the verge of bankruptcy. They basically cut a pay package with him where he took nothing if the company didn't improve. But if the company hit certain targets, he would get paid out 1% tranches of options, I think over 12 tranches, which because the company had this extraordinary turnaround, he achieved his goal.</p><p>So now she's kind of Monday morning quarterbacking. She's looking back. And she says his pay package is unfathomable. And she said the board never asked the $55 billion question, Bill, was it even necessary to pay him this to retain him and to achieve the company's goals? So of course, this can be appealed to the Delaware Supreme Court, and it will be.</p><p>But in response to this, Elon and I think many others just said, hold on a second here. What the hell just happened? The state of Delaware has had this historical advantage in corporate law because of its predictability. And its predictability wasn't because of the code, but it was because the judiciary, right?</p><p>There was a lot of precedent in the state of Delaware. And this seemed to turn that totally on its head. He said he was going to move incorporation to the state of Texas. We're starting to see other companies follow suit and other people talking about this. So what was your reaction seeing something that was, I think most of us thought, was highly unlikely and pretty shocking?</p><p>Yeah, well, first of all, I think it's super important for everyone to pay attention to this. I don't actually think it's just an outlier event. I think it's so unprecedented in Delaware's history that it really marks a moment for everyone to pay attention. And there's a couple of things I would pay attention to.</p><p>One data point you left out, which came up recently, is the lawyers that pursued this case are asking for $5 or $6 billion in payment. And it turns out when you bring a derivative suit in Delaware, there have been cases where people ask for a percent and the judge gets to kind of decide that.</p><p>And if you step back and look, this is a victimless crime. And I think that's the thing that makes Delaware look like a kangaroo court here. Everyone knows the lawyer grabs someone that only had nine chairs. And those nine chairs went way up, but it's kind of silly because it's so small anyway.</p><p>So how could a client with nine chairs lead to a multi-billion dollar award to a lawyer? And that's only true if you've created a bounty hunter system, a bureaucratic bounty hunter system. There's something in California called PAGA that's evolved this way. And if that's the new norm in Delaware, that's really, really concerning.</p><p>The other thing that's different here is the stock went way, way up. So I think we've all become accustomed to when stock price is going down, these litigators grab a handful of shareholders and bring a shareholder lawsuit. And we're like, oh, yeah. Unfortunately, that's become a way of life.</p><p>But to attack companies that go way up, I would-- two things. One, I would offer this pay package-- I looked at it in detail-- to any CEO I work with, and I think they would all turn it down. Because there's no cash, no guarantee. And the first tranche was a 2x of the stock.</p><p>So that's fantastic. I think the biggest problem with compensation packages-- and you may tackle that some other day-- is a misalignment with shareholders, where people are getting paid when the stock doesn't move. That's how RSUs do. And here-- By the way, that's the standard in corporate America. We have this grift where people make a ton of money, and the stock doesn't do anything.</p><p>Look at the pay package for Mary Barra at GM. So the first tranche here was if the stock doubled. And I would offer that to anyone. I would also say, if any other CEO took a package like this, in a public company, I would be very encouraged to consider buying a lot of it.</p><p>Totally. And so it may be one of the most shareholder-aligned incentive packages ever, which is exactly what you would think Delaware courts would be looking after. And ISS as well, which is a whole other subject. So I think it's just really bad. And it does show a new side of Delaware, one that they haven't shown before.</p><p>And so I think everyone has to pay attention. Right, no, I mean, it's shocking. And if you-- I was a corporate lawyer in my first life. As you know, if you actually go and look at the actual corporate law code in the state of Delaware, it's almost word for word the same as Texas, the same as California, and so on.</p><p>The point here is it's not that Delaware has a legal code around corporations that's so much different than every other state. What has set it apart is it has way more legal precedent, way more trials that have occurred, and judges who have interpreted that in a way that is very shareholder-aligned, shareholder-friendly.</p><p>So the big-- And letter-- and they're known for letter of the law. So the construction of charges. Correct. And so here we have a moment. And the reason it's so shocking is because it's at odds with all of the precedent that people had come to expect. So I think there are going to be-- We left out they had 70% shareholder approval.</p><p>I mean, and there was a high-low probability event that happened to happen. And you can't look at that after the fact and say, oh, it was obvious this was going to happen. You know, you can't do that. Right, I think that if this stands-- so I imagine corporations right now are in holding patterns.</p><p>Elon is moving, reincorporating in Texas. I think a lot of other corporations will stay pending the Delaware Supreme Court appeals ruling. If they overturn this judge's ruling, then I think you may be back to the status quo in the state of Delaware. But if they uphold the ruling and deny-- I mean, I think Elon said, despite all the goodness that's occurred, saving the company from bankruptcy, this means he effectively gets paid zero for the last five years.</p><p>I mean, it's such an outlandish outcome. So if it gets upheld, I expect you're going to see significant flight from the state of Delaware by people reincorporating in these other states that, frankly, are pretty friendly as well. Brad, I just thought of something. So if it's upheld, and if these lawyers are paid anything as a percentage, anything other than maybe just their hourly fee, so if those two things happen, I would make the argument that every company in Delaware has to move to a different domicile because they could be sued in a future derivative law suit for the risk they've taken by staying in Delaware.</p><p>Oh my god, you're so right. You are so right. Oh, mic drop on that. So now on the boards that I sit on, I have to warn them that if they stay in the state of Delaware, then they're knowingly and negligently taking on this incremental risk. Absolutely. Oh, wow.</p><p>Let's just wrap with this, a quick market check. One of the things I like to do is be responsive to the feedback we get. A lot of people loved some of the charts we had put up on the market checked on the last show. So we get asked about this all the time.</p><p>We said on the prior pod, prices have run a lot this year, and the background noise around macro has not improved. Arguably, it's getting a little worse. Inflation is running a little hotter. Rates are not expected to come down as much. So just a quick check on the multiples of companies that we really care about-- Microsoft, Amazon, Apple, Meta, Google, and NVIDIA.</p><p>And I just want to walk through this really quick. So this is a chart that just shows the multiples between March of '21 and March of '24. And so if we look at-- let's start with Meta. You can look at that time. There are multiples gone from about 20 times earnings to about 23 times earnings.</p><p>So it's a little bit higher. Take a look at Google. Its multiples has gone from about 25 earnings to now down to just below 20 times earnings. Now, this is to be expected. We've been having this debate about whether or not Google's search share is going to go down and the impact that that will have.</p><p>And so this is just the market's voting machine at a moment time saying, hey, we hear that debate, and we're a little bit more worried about those future cash flows than we were in March of '21, which makes a lot of sense to me. If you look at Apple, it too is-- And by the way, on that one, I mean, the Gemini release, the world's looking at you with this lens, and then you release this thing, and then you trip.</p><p>I mean, they basically tripped, right? And we know they tripped because they've apologized for tripping. And so it's just not good. It's not confidence inspiring. Well, and now you're seeing the drumbeat starting. You and I are getting the text, the emails, the drumbeats around whether Sundar is going to make it past this moment in time.</p><p>I mean, listen, I think boards have one job-- hire, fire the CEO who leads the company forward. Can they execute against the plan? And I think that if I was on the board of Google, that's the question I'd be asking at this moment in time. Not is he a good human being, not is he a smart product guy, not is he a good technologist, not what's happened over the course of the last 10 years.</p><p>But at this moment in time, do we have any risk of innovator's dilemma? And is this the team? Is this the CEO who can lead us through what is likely to be a tricky moment? Just to finish it off, Apple's multiple is a little bit lower. That also makes sense to me.</p><p>You see what's happening in China. Some concerns about their-- they get $20 billion a year from Google. Like, what happens to that? In the case of Microsoft, their multiple is a little higher. But again, these multiples are all in the range. And then the final two, Amazon's multiple is actually quite a bit lower here.</p><p>And so that's interesting to me. I actually think the retail business is doing better. I actually think the cloud business is doing better. And now that stock looks cheaper to me. And then NVIDIA, of course, is the one that everybody's talking about. And this goes back to where we started the show.</p><p>I mean, if you look at NVIDIA's multiple to start the year bill, so hover there right above December 23, its multiple was at a 5, 10-year low, right? Why? Because earnings exploded last year from $5 to $25. Its multiple has obviously come up here a little bit at the start of the year.</p><p>But you can see it's well below some of its historical really frothy multiples. But I think the question in my mind-- we're big NVIDIA shareholders-- like in other people's minds is, is this earnings train durable for NVIDIA, right? Are these revenues durable? Have we pulled forward this training data?</p><p>We showed that chart a couple of weeks ago that we think the future build out of compute and supercompute of B100s of everything is longer and wider than people think. And then the interesting thing, when you see that note out of Klarna last week, Bill, and what they were able to achieve, this is really the question.</p><p>At the end of the day, are companies and consumers getting massive benefits out of the models and inference that's running on these chips? If the answer is no, then all of these stocks are going lower. If the answer is yes, then they probably have a lot of room to run.</p><p>But that's the quick-- maybe we'll do this at the end of each of them, do a quick market check. But why don't we leave it there? It's good seeing you. Next time, get back out here. Let's do this together again. All right. Take care. Take it easy. As a reminder to everybody, just our opinions, not investment advice.</p></div></div></body></html>