<html><head><title>LLMs will hit the data wall if they can’t generalize – OpenAI cofounder John Schulman</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>LLMs will hit the data wall if they can’t generalize – OpenAI cofounder John Schulman</h2><a href="https://www.youtube.com/watch?v=V6X_tLCxsZk" target="_blank"><img src="https://i.ytimg.com/vi/V6X_tLCxsZk/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>So because there doesn't seem to be a model really since GPT-4 that seems to be significantly better, there seems to be the hypothesis that potentially we're hitting some sort of plateau and that these models aren't actually generalizing that well, and we're going to hit some sort of data wall, beyond which point the abilities that are unlocked by memorizing a vast corpus of pre-training data won't actually help you get something much smarter than GPT-4.</p><p>I mean, I wouldn't draw too much from the time since GPT-4 was released because, I mean, it does, yeah, it takes a while to, like, train these models and to, like, get all the, do all the prep to train a new generation of models. So, yeah, I wouldn't draw too much from that fact.</p><p>I would say there are definitely some challenges from the limited amount of data, but I wouldn't expect us to immediately hit the data wall. But I would expect the nature of pre-training to somewhat change over time as we get closer to it. I think we've talked about some examples generically about generalization.</p><p>One example I was thinking of was the idea that there is transfer from reasoning and code. If you train a bunch of code, it gets better at reasoning and language. And if that's the, is that actually the case? Do you see things like that, which suggests that there's all this credit positive transfer between different modalities.</p><p>So once you charge training on a bunch of videos and images, it'll get smarter and it'll get smarter from synthetic data. Or does it seem like the abilities that are unlocked are extremely local to the exact kind of labels and data you put into the training corpus? In terms of generalization from different types of pre-training data, I would say it's pretty hard to do science on this type of question because you can't do that, create that many pre-trained models.</p><p>So maybe you can't train a GPT-4 size model. You can't do ablation studies at GPT-4 scale. Maybe you can do, train a ton of GPT-2 size models or maybe even a GPT-3 size model with different data blends and see what you get. So I'm not aware of any results or public results on ablations involving code data and reasoning performance and so forth.</p><p>So that would be, I'd be very interested to know about those results. With regards to the sort of plateau narrative, one of the things I've heard is that a lot of the abilities these models have to help you with specific things is related to the having very closely matched labels within the supervised fine-tuning data set.</p><p>Is that true? If it can teach me how to use FFmpeg correctly, there's somebody who's doing, figuring out, seeing the inputs and seeing what flags you need to add. And some human is figuring that out and smashing to that. And is it, yeah. Do you need to hire like all these laborers who have domain expertise in all these different domains?</p><p>Because if that's the case, it seems like it would be a much bigger slog to get these models to be smarter and smarter over time. Right. You don't exactly need that because, yeah, you can get quite a bit out of generalization. So if you like, like the base model has already been trained on tons of documentation, tons of code with shell scripts and so forth.</p><p>So it's already seen all the FFmpeg man pages and lots of bash scripts and everything. And it's so like the base, even just giving the base model a good few-shot prompt, you can get it to answer queries like this. And just training a preference model like for helpfulness will, even if you don't train it on, probably even if you don't train it on any STEM, it'll somewhat generalize to STEM.</p><p>So not only do you not need like examples of how to use FFmpeg, you might not even need anything with programming to get some reasonable behavior in the programming domain.</p></div></div></body></html>