<html><head><title>Getting Started With CUDA for Python Programmers</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Getting Started With CUDA for Python Programmers</h2><a href="https://www.youtube.com/watch?v=nOxKexn3iBo"><img src="https://i.ytimg.com/vi/nOxKexn3iBo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=0">0:0</a> Introduction to CUDA Programming<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=32">0:32</a> Setting Up the Environment<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=103">1:43</a> Recommended Learning Resources<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=159">2:39</a> Starting the Exercise<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=206">3:26</a> Image Processing Exercise<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=368">6:8</a> Converting RGB to Grayscale<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=470">7:50</a> Understanding Image Flattening<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=664">11:4</a> Executing the Grayscale Conversion<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=761">12:41</a> Performance Issues and Introduction to CUDA Cores<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=886">14:46</a> Understanding Cuda and Parallel Processing<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=983">16:23</a> Simulating Cuda with Python<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1144">19:4</a> The Structure of Cuda Kernels and Memory Management<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1302">21:42</a> Optimizing Cuda Performance with Blocks and Threads<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1456">24:16</a> Utilizing Cuda's Advanced Features for Speed<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1575">26:15</a> Setting Up Cuda for Development and Debugging<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1648">27:28</a> Compiling and Using Cuda Code with PyTorch<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1731">28:51</a> Including Necessary Components and Defining Macros<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1785">29:45</a> Ceiling Division Function<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1810">30:10</a> Writing the CUDA Kernel<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1939">32:19</a> Handling Data Types and Arrays in C<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2022">33:42</a> Defining the Kernel and Calling Conventions<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2149">35:49</a> Passing Arguments to the Kernel<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2209">36:49</a> Creating the Output Tensor<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2291">38:11</a> Error Checking and Returning the Tensor<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2341">39:1</a> Compiling and Linking the Code<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2406">40:6</a> Examining the Compiled Module and Running the Kernel<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2577">42:57</a> Cuda Synchronization and Debugging<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2607">43:27</a> Python to Cuda Development Approach<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2694">44:54</a> Introduction to Matrix Multiplication<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2817">46:57</a> Implementing Matrix Multiplication in Python<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3039">50:39</a> Parallelizing Matrix Multiplication with Cuda<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3110">51:50</a> Utilizing Blocks and Threads in Cuda<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3501">58:21</a> Kernel Execution and Output<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3508">58:28</a> Introduction to Matrix Multiplication with CUDA<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3601">60:1</a> Executing the 2D Block Kernel<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3651">60:51</a> Optimizing CPU Matrix Multiplication<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3755">62:35</a> Conversion to CUDA and Performance Comparison<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4070">67:50</a> Advantages of Shared Memory and Further Optimizations<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4122">68:42</a> Flexibility of Block and Thread Dimensions<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4248">70:48</a> Encouragement and Importance of Learning CUDA<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4350">72:30</a> Setting Up CUDA on Local Machines<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4379">72:59</a> Introduction to Conda and its Utility<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4440">74:0</a> Setting Up Conda<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4472">74:32</a> Configuring Cuda and PyTorch with Conda<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4535">75:35</a> Conda's Improvements and Compatibility<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4565">76:5</a> Benefits of Using Conda for Development<br><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4600">76:40</a> Conclusion and Next Steps<br><br><div style="text-align: left;"><a href="./nOxKexn3iBo.html">Whisper Transcript</a> | <a href="./transcript_nOxKexn3iBo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi there. I'm Jeremy Howard from answer.ai and this is Getting Started with CUDA. CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=10" target="_blank">00:00:10.160</a></span> | <span class="t">is of course what we use to program NVIDIA GPUs if we want them to go super fast and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=16" target="_blank">00:00:16.080</a></span> | <span class="t">we want maximum flexibility and it has a reputation of being very hard to get started with. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=23" target="_blank">00:00:23.720</a></span> | <span class="t">truth is it's actually not so bad. You just have to know some tricks and so in this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=29" target="_blank">00:00:29.720</a></span> | <span class="t">I'm going to show you some of those tricks. So let's switch to the screen and take a look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=37" target="_blank">00:00:37.020</a></span> | <span class="t">So I'm going to be doing all of the work today in notebooks. This might surprise you. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=42" target="_blank">00:00:42.180</a></span> | <span class="t">might be thinking that to do work with CUDA we have to do stuff with compilers and terminals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=46" target="_blank">00:00:46.760</a></span> | <span class="t">and things like that and the truth is actually it turns out we really don't thanks to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=52" target="_blank">00:00:52.320</a></span> | <span class="t">magic that is provided by PyTorch. You can follow along in all of these steps and I strongly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=59" target="_blank">00:00:59.440</a></span> | <span class="t">suggest you do so in your own computer. You can go to the CUDA mode organization in GitHub.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=68" target="_blank">00:01:08.560</a></span> | <span class="t">Find the lecture 2 repo there and you'll see there is a lecture 3 folder. This is lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=75" target="_blank">00:01:15.120</a></span> | <span class="t">3 of the CUDA mode series. You don't need to have seen any of the previous ones however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=79" target="_blank">00:01:19.800</a></span> | <span class="t">to follow along. In the read me there you'll see there's a lecture 3 section and at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=86" target="_blank">00:01:26.040</a></span> | <span class="t">bottom there is a click to go to the colab version. Yep you can run all of this in colab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=92" target="_blank">00:01:32.880</a></span> | <span class="t">for free. You don't even have to have a GPU available to run the whole thing. We're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=99" target="_blank">00:01:39.680</a></span> | <span class="t">to be following along with some of the examples from this book programming massively parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=106" target="_blank">00:01:46.480</a></span> | <span class="t">processes is a really great book to read and once you've completed today's lesson you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=119" target="_blank">00:01:59.960</a></span> | <span class="t">be able to make a great start on this book. It goes into a lot more details about some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=124" target="_blank">00:02:04.400</a></span> | <span class="t">of the things that we're going to cover on fairly quickly. It's okay if you don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=129" target="_blank">00:02:09.400</a></span> | <span class="t">the book but if you want to go deeper I strongly suggest you get it and in fact you'll see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=136" target="_blank">00:02:16.920</a></span> | <span class="t">in the repo that lecture 2 in this series actually was a deep dive into chapters 1-3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=142" target="_blank">00:02:22.640</a></span> | <span class="t">of that book and so actually you might want to do lecture 2 confusingly enough after this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=146" target="_blank">00:02:26.880</a></span> | <span class="t">one lecture 3 to get more details about some of what we're talking about. Okay so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=154" target="_blank">00:02:34.560</a></span> | <span class="t">dive into the notebook. So what we're going to be doing today is we're going to be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=160" target="_blank">00:02:40.560</a></span> | <span class="t">a whole lot of stuff with plain old PyTorch first to make sure that we get all the ideas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=166" target="_blank">00:02:46.320</a></span> | <span class="t">and then we will try to convert each of these things into CUDA. So in order to do this we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=174" target="_blank">00:02:54.600</a></span> | <span class="t">going to start by importing a bunch of stuff in fact let's do all of this in colab. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=180" target="_blank">00:03:00.440</a></span> | <span class="t">here we are in colab and you should make sure that you set in colab your runtime to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=187" target="_blank">00:03:07.200</a></span> | <span class="t">T4 GPU. That's one you can use plenty of for free and it's easily good enough to run everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=193" target="_blank">00:03:13.400</a></span> | <span class="t">we're doing today. And once you've got that running we can import the libraries we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=199" target="_blank">00:03:19.320</a></span> | <span class="t">going to need and we can start on our first exercise. So the first exercise actually comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=205" target="_blank">00:03:25.360</a></span> | <span class="t">from chapter 2 of the book and chapter 2 of the book teaches how to do this problem which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=213" target="_blank">00:03:33.120</a></span> | <span class="t">is converting an RGB color picture into a grayscale picture. And it turns out that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=218" target="_blank">00:03:38.640</a></span> | <span class="t">recommended formula for this is to take 0.21 of the red pixel, 0.72 of the green pixel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=224" target="_blank">00:03:44.400</a></span> | <span class="t">0.07 of the blue pixel and add them up together and that creates the luminance value which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">is what we're seeing here. That's a common way, kind of the standard way to go from RGB</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=235" target="_blank">00:03:55.960</a></span> | <span class="t">to grayscale. So we're going to do this, we're going to make a CUDA kernel to do this. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=242" target="_blank">00:04:02.600</a></span> | <span class="t">the first thing we're going to need is a picture and anytime you need a picture I recommend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=246" target="_blank">00:04:06.860</a></span> | <span class="t">going for a picture of a puppy. So we've got here a URL to a picture of a puppy so we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=253" target="_blank">00:04:13.520</a></span> | <span class="t">just go ahead and download it and then we can use torchvision.io to load that. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=262" target="_blank">00:04:22.560</a></span> | <span class="t">is already part of Colab. If you're interested in running stuff on your own machine or a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=268" target="_blank">00:04:28.280</a></span> | <span class="t">server in the cloud I'll show you how to set that up at the end of this lecture. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=274" target="_blank">00:04:34.000</a></span> | <span class="t">read in the image and if we have a look at the shape of it it says it's 3 by 1066 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=280" target="_blank">00:04:40.280</a></span> | <span class="t">1600. So I'm going to assume that you know the basics of PyTorch here. If you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=286" target="_blank">00:04:46.960</a></span> | <span class="t">know the basics of PyTorch I am a bit biased but I highly recommend my course which covers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=294" target="_blank">00:04:54.240</a></span> | <span class="t">exactly that. You can go to course.fast.ai and you get the benefit also of having seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=300" target="_blank">00:05:00.480</a></span> | <span class="t">some very cute bunnies and along with the very cute bunnies it basically takes you through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=306" target="_blank">00:05:06.680</a></span> | <span class="t">all of everything you need to be an effective practitioner of modern deep learning. So finish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=314" target="_blank">00:05:14.920</a></span> | <span class="t">part one if you want to go right into those details but even if you just do the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=319" target="_blank">00:05:19.840</a></span> | <span class="t">two or three lessons that will give you more than enough you need to know to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=325" target="_blank">00:05:25.560</a></span> | <span class="t">this kind of code and these kinds of outputs. So I'm assuming you've done all that. So you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=331" target="_blank">00:05:31.160</a></span> | <span class="t">see here we've got a rank 3 tensor. There are three channels so they're like the faces of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=338" target="_blank">00:05:38.360</a></span> | <span class="t">a cube if you like. There are 1066 rows on each face so that's the height and then there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=344" target="_blank">00:05:44.240</a></span> | <span class="t">are 16 columns in each row so that's the width. So if we then look at the first couple of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=350" target="_blank">00:05:50.720</a></span> | <span class="t">channels and the first three rows and the first four columns you can see here that these are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=358" target="_blank">00:05:58.720</a></span> | <span class="t">unsigned 8-bit integers so they're bytes and so here they are. So that's what an image looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=363" target="_blank">00:06:03.600</a></span> | <span class="t">like. Hopefully you know all that already. So let's take a look at our image. To do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=370" target="_blank">00:06:10.840</a></span> | <span class="t">I'm just going to create a simple little function show image that will create a mat lip plot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=376" target="_blank">00:06:16.720</a></span> | <span class="t">mat lip lip plot plot remove the axes if it's color which this one is it will change the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=385" target="_blank">00:06:25.200</a></span> | <span class="t">order of the axes from channel by height by width which is what PyTorch uses to height</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=391" target="_blank">00:06:31.440</a></span> | <span class="t">by width by channel which is what matplotlib I'm having trouble today expects. So we change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=400" target="_blank">00:06:40.360</a></span> | <span class="t">the order of the axes to be 1, 2, 0 and then we can show the image putting it on the CPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=407" target="_blank">00:06:47.360</a></span> | <span class="t">if necessary. Now we're going to be working with this image in Python which is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=412" target="_blank">00:06:52.600</a></span> | <span class="t">be just pure Python to start with before we switch to CUDA that's going to be really slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=416" target="_blank">00:06:56.720</a></span> | <span class="t">so we'll resize it to have the smallest length smallest dimension be 150 so that's the height</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=425" target="_blank">00:07:05.640</a></span> | <span class="t">in this case so we end up with a 150 by 225 shape which is a rectangle which is 3, 3, 750</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=434" target="_blank">00:07:14.920</a></span> | <span class="t">pixels each one with our GMB values and there is our puppy. So you see wasn't it a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=440" target="_blank">00:07:20.120</a></span> | <span class="t">idea to make this a puppy. Okay so how do we convert that to grayscale? Well the book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=448" target="_blank">00:07:28.800</a></span> | <span class="t">has told us the formula to use go through every pixel and do that to it. Alright so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=456" target="_blank">00:07:36.920</a></span> | <span class="t">here is the loop we're going to go through every pixel and do that to it and stick that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=464" target="_blank">00:07:44.360</a></span> | <span class="t">in the output so that's the basic idea so what are the details of this? Well here we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=469" target="_blank">00:07:49.040</a></span> | <span class="t">got channel by row by column so how do we loop through every pixel? Well the first thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=476" target="_blank">00:07:56.520</a></span> | <span class="t">we need to know is how many pixels are there so we can say channel by height by width is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=482" target="_blank">00:08:02.760</a></span> | <span class="t">the shape so now we have to find those three variables so the number of pixels is the height</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=488" target="_blank">00:08:08.440</a></span> | <span class="t">times the width and so to loop through all those pixels an easy way to do them is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=494" target="_blank">00:08:14.880</a></span> | <span class="t">flatten them all out into a vector. Now what happens when you flatten them all out into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=500" target="_blank">00:08:20.920</a></span> | <span class="t">a vector? Well as we saw they're currently stored in this format where we've got one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=509" target="_blank">00:08:29.440</a></span> | <span class="t">face and then another face and then there's a we haven't got it printed here but there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=513" target="_blank">00:08:33.040</a></span> | <span class="t">a third face within each face then there is one row we're just showing the first few and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=520" target="_blank">00:08:40.480</a></span> | <span class="t">then the next row and then the next row and then with each row you've got column column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=525" target="_blank">00:08:45.360</a></span> | <span class="t">column. So let's say we had a small image in which in fact we can do it like this we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=536" target="_blank">00:08:56.840</a></span> | <span class="t">could say here's our red so we've got the pixels 0, 1, 2, 3, 4, 5 so let's say this was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=548" target="_blank">00:09:08.520</a></span> | <span class="t">a height to width 3, 3 channel image so then there'll be 6, 7, 8, 9, 10, 11, GB, 12, 13,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=568" target="_blank">00:09:28.840</a></span> | <span class="t">14, 15, 16 so let's say these are the pixels so when these are flattened out it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=579" target="_blank">00:09:39.040</a></span> | <span class="t">to turn into a single vector just like so 6, 7, 8, 12, 13, 14. So actually when we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=598" target="_blank">00:09:58.000</a></span> | <span class="t">about an image we initially see it as a bunch of pixels we can think of it as having 3 channels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=614" target="_blank">00:10:14.320</a></span> | <span class="t">but in practice in our computer the memory is all laid out linearly everything has just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=623" target="_blank">00:10:23.520</a></span> | <span class="t">an address in memory it's just a whole bunch you can think of it as your computer's memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=628" target="_blank">00:10:28.080</a></span> | <span class="t">is one giant vector and so when we say when we say flatten then what that's actually doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=639" target="_blank">00:10:39.800</a></span> | <span class="t">is it's turning our channel by height by width into a big vector like this. Okay so now that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=652" target="_blank">00:10:52.240</a></span> | <span class="t">we've done that we can say all right our the place we're going to be putting this into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=659" target="_blank">00:10:59.520</a></span> | <span class="t">the result we're going to start out with just an empty vector of length n we'll go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=666" target="_blank">00:11:06.760</a></span> | <span class="t">all of the n values from 0 to n - 1 and we're going to put in the output value 0.29 ish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=676" target="_blank">00:11:16.400</a></span> | <span class="t">times the input value at xi so this will be here in the red bit and then 0.59 times xi</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=689" target="_blank">00:11:29.800</a></span> | <span class="t">plus n so n here is this distance it's the number of pixels one two three four five six</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=701" target="_blank">00:11:41.460</a></span> | <span class="t">see one two three four five six so that's why to get to green we have to jump up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=710" target="_blank">00:11:50.600</a></span> | <span class="t">i plus n and then to get to blue we have to jump to i plus 2n see and so that's how this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=724" target="_blank">00:12:04.520</a></span> | <span class="t">works we've flattened everything out and we're indexing into this flattened out thing directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=732" target="_blank">00:12:12.360</a></span> | <span class="t">and so at the end of that we're going to have our grayscale is all done so we can then just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=736" target="_blank">00:12:16.460</a></span> | <span class="t">reshape that into height by width and there it is there's our grayscale puppy and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=746" target="_blank">00:12:26.040</a></span> | <span class="t">can see here the flattened image is just a single vector with all those channel values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=754" target="_blank">00:12:34.760</a></span> | <span class="t">flattened out as we described okay now that is incredibly slow it's nearly two seconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=763" target="_blank">00:12:43.600</a></span> | <span class="t">to do something with only 34,000 pixels in so to speed it up we are going to want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=770" target="_blank">00:12:50.320</a></span> | <span class="t">use CUDA how come CUDA is able to speed things up well the reason CUDA is able to speed things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=779" target="_blank">00:12:59.400</a></span> | <span class="t">up is because it is set up in a very different way to how a normal CPU is set up and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=790" target="_blank">00:13:10.720</a></span> | <span class="t">actually see that if we look at some of this information about what is in an RTX 3090 card</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=801" target="_blank">00:13:21.200</a></span> | <span class="t">for example an RTX 3090 card is a fantastic GPU you can get them second hand pretty good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=807" target="_blank">00:13:27.600</a></span> | <span class="t">value so a really good choice particularly for hobbyists what is inside a 3090 it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=815" target="_blank">00:13:35.920</a></span> | <span class="t">82 SM's what's an SM and SM is a streaming multi processor so you can think of this as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=824" target="_blank">00:13:44.600</a></span> | <span class="t">almost like a separate CPU in your computer and so there's 82 of these so that's already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=830" target="_blank">00:13:50.960</a></span> | <span class="t">a lot more than you have CPUs in your computer but then each one of these has 128 CUDA cores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=841" target="_blank">00:14:01.640</a></span> | <span class="t">so these CUDA cores are all able to operate at the same time these multi processors are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=846" target="_blank">00:14:06.720</a></span> | <span class="t">all able to operate at the same time so that gives us 128 times 82 10,500 CUDA cores in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=856" target="_blank">00:14:16.200</a></span> | <span class="t">total that can all work at the same time so that's a lot more than any CPU we're familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=863" target="_blank">00:14:23.840</a></span> | <span class="t">with can do and the 3090 isn't even at the very top end it's really a very good GPU but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=871" target="_blank">00:14:31.600</a></span> | <span class="t">there are some with even more CUDA cores so how do we use them all well we need to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=880" target="_blank">00:14:40.440</a></span> | <span class="t">able to set up our code in such a way that we can say here is a piece of code that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=885" target="_blank">00:14:45.560</a></span> | <span class="t">can run on lots of different pieces of data lots of different pieces of memory at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=890" target="_blank">00:14:50.440</a></span> | <span class="t">same time so that you can do 10,000 things at the same time and so CUDA does this in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=896" target="_blank">00:14:56.520</a></span> | <span class="t">a really simple and pretty elegant way which is it basically says okay take out the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=902" target="_blank">00:15:02.800</a></span> | <span class="t">of the inner loop so here's our inner loop the stuff where you can run 10,000 of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=910" target="_blank">00:15:10.440</a></span> | <span class="t">at the same time they're not going to influence each other at all so you see these do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=914" target="_blank">00:15:14.080</a></span> | <span class="t">influence each other at all all they do is they stick something into some output memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=920" target="_blank">00:15:20.480</a></span> | <span class="t">so it doesn't even return something you can't return something from these CUDA kernels as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=925" target="_blank">00:15:25.120</a></span> | <span class="t">they're going to be called all you can do is you can modify memory in such a way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=930" target="_blank">00:15:30.440</a></span> | <span class="t">you don't know what order they're going to run in they could all run at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=933" target="_blank">00:15:33.440</a></span> | <span class="t">some could run a little bit before another one and so forth so the way that CUDA does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=939" target="_blank">00:15:39.080</a></span> | <span class="t">this is it says okay write a function right and in your function write a line of code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=946" target="_blank">00:15:46.920</a></span> | <span class="t">which I'm going to call as many dozens hundreds thousands millions of times as necessary to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=953" target="_blank">00:15:53.080</a></span> | <span class="t">do all the work that's needed and I'm going to do and I'm going to use do this in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=957" target="_blank">00:15:57.040</a></span> | <span class="t">for you as much as I can in the case of running on a 3090 up to 10,000 times up to 10,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=965" target="_blank">00:16:05.040</a></span> | <span class="t">things all at once and I will get this done as fast as possible so all you have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=971" target="_blank">00:16:11.160</a></span> | <span class="t">is basically write the line of code you want to be called lots of times and then the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=976" target="_blank">00:16:16.400</a></span> | <span class="t">thing you have to do is say how many times to call that code and so what will happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=980" target="_blank">00:16:20.720</a></span> | <span class="t">is that piece of code called the kernel will be called for you it'll be passed in whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=986" target="_blank">00:16:26.360</a></span> | <span class="t">arguments you ask to be passed in which in this case will be the input array tensor the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=991" target="_blank">00:16:31.440</a></span> | <span class="t">output tensor and the size of how many pixels are in each channel and it'll tell you okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1000" target="_blank">00:16:40.680</a></span> | <span class="t">this is the ith time I've called it now we can simulate that in Python very very simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1008" target="_blank">00:16:48.640</a></span> | <span class="t">a single for loop now this doesn't happen in parallel so it's not going to speed it up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1013" target="_blank">00:16:53.520</a></span> | <span class="t">but the kind of results the semantics are going to be identical to CUDA so here is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1019" target="_blank">00:16:59.760</a></span> | <span class="t">function we've called run kernel we're going to pass it in a function we're going to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1024" target="_blank">00:17:04.000</a></span> | <span class="t">how many times to run the function and what arguments to call the function with and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1028" target="_blank">00:17:08.840</a></span> | <span class="t">each time it will call the function passing in the index what time and the arguments that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1034" target="_blank">00:17:14.920</a></span> | <span class="t">we've requested okay so we can now create something to call that so let's get the just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1044" target="_blank">00:17:24.480</a></span> | <span class="t">like before get the channel number of channels height and width the number of pixels flatten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1049" target="_blank">00:17:29.760</a></span> | <span class="t">it out create the result tensor that we're going to put things in and this time rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1055" target="_blank">00:17:35.600</a></span> | <span class="t">than calling the loop directly we will call run kernel we will pass in the name of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1061" target="_blank">00:17:41.920</a></span> | <span class="t">function to be called as f we will pass in the number of times which is the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1068" target="_blank">00:17:48.920</a></span> | <span class="t">pixels for the loop and we'll pass in the arguments that are going to be required inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1079" target="_blank">00:17:59.200</a></span> | <span class="t">our kernel so we're going to need out we're going to need x and we're going to need n so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1085" target="_blank">00:18:05.600</a></span> | <span class="t">you can see here we're using no external libraries at all we have just plain python and a tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1097" target="_blank">00:18:17.200</a></span> | <span class="t">bit of pytorch just enough to create a tensor into index into tensors and that's all that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1102" target="_blank">00:18:22.720</a></span> | <span class="t">being used but conceptually it's doing the same thing as a CUDA kernel would do nearly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1112" target="_blank">00:18:32.040</a></span> | <span class="t">and we'll get to the nearly in just a moment but conceptually you could see that you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1117" target="_blank">00:18:37.080</a></span> | <span class="t">now potentially write something which if you knew that this was running a bunch of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1125" target="_blank">00:18:45.280</a></span> | <span class="t">totally independently of each other conceptually you could now truly easily paralyze that and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1130" target="_blank">00:18:50.760</a></span> | <span class="t">that's what CUDA does however it's not quite that simple it does not simply create a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1146" target="_blank">00:19:06.880</a></span> | <span class="t">list of numbers like range n does in python and pass each one in turn into your kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1154" target="_blank">00:19:14.360</a></span> | <span class="t">but instead it actually splits the range of numbers into what's called blocks so in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1163" target="_blank">00:19:23.200</a></span> | <span class="t">case you know maybe there's like a thousand pixels we wanted to get through it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1169" target="_blank">00:19:29.840</a></span> | <span class="t">to group them into blocks of 256 at a time and so in python it looks like this in practice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1181" target="_blank">00:19:41.240</a></span> | <span class="t">a CUDA kernel runner is not a single for loop that loops n times but instead it is a pair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1191" target="_blank">00:19:51.600</a></span> | <span class="t">of nested for loops so you don't just pass in a single number and say this is the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1198" target="_blank">00:19:58.200</a></span> | <span class="t">of pixels but you pass in two numbers number of blocks and the number of threads we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1204" target="_blank">00:20:04.440</a></span> | <span class="t">get into that in a moment but these are just numbers they're just you can put any numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1207" target="_blank">00:20:07.640</a></span> | <span class="t">you like here and if you choose two numbers that multiply to get the thing that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1215" target="_blank">00:20:15.880</a></span> | <span class="t">which is the n times we want to call it then this can do exactly the same thing because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1221" target="_blank">00:20:21.520</a></span> | <span class="t">we're now going to pass in which of the what's the index of the outer loop we're up to what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1227" target="_blank">00:20:27.480</a></span> | <span class="t">the index in the inner loop we're up to how many things do we go through in the inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1232" target="_blank">00:20:32.960</a></span> | <span class="t">loop and therefore inside the kernel we can find out what index we're up to by multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1240" target="_blank">00:20:40.560</a></span> | <span class="t">the block index times the block dimension so that is to say the i by the threads and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1247" target="_blank">00:20:47.120</a></span> | <span class="t">add the inner loop index the j so that's what we pass in with the i j threads but inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1255" target="_blank">00:20:55.240</a></span> | <span class="t">the kernel we call it block index thread index and block dimension so if you look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1260" target="_blank">00:21:00.120</a></span> | <span class="t">CUDA book you'll see here this is exactly what they do they say the index is equal to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1266" target="_blank">00:21:06.320</a></span> | <span class="t">the block index times the block dimension plus the thread index there's a dot x thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1272" target="_blank">00:21:12.560</a></span> | <span class="t">here that we can ignore for now we'll look at that in a moment but in practice this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1280" target="_blank">00:21:20.800</a></span> | <span class="t">actually how CUDA works so it has all these blocks and inside there are threads and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1290" target="_blank">00:21:30.440</a></span> | <span class="t">can just think of them as numbers so you can see these blocks they just have numbers o</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1293" target="_blank">00:21:33.760</a></span> | <span class="t">one dot dot dot dot and so forth now that does mean something a little bit tricky though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1300" target="_blank">00:21:40.440</a></span> | <span class="t">which is well the first thing i'll say is how do we pick these numbers the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1306" target="_blank">00:21:46.040</a></span> | <span class="t">blocks and the number of threads for now in practice we're just always going to say the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1309" target="_blank">00:21:49.960</a></span> | <span class="t">number of threads is 256 and that's a perfectly fine number to use as a default anyway you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1317" target="_blank">00:21:57.760</a></span> | <span class="t">can't go too far wrong just always picking 256 nearly always so don't worry about that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1323" target="_blank">00:22:03.720</a></span> | <span class="t">too much for now optimizing that number so if we say okay we want to have 256 threads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1332" target="_blank">00:22:12.000</a></span> | <span class="t">so remember that's the inner loop or if we look inside our kernel runner here that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1336" target="_blank">00:22:16.800</a></span> | <span class="t">our inner loop so we're going to call each of this is going to be called 256 times so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1342" target="_blank">00:22:22.160</a></span> | <span class="t">how many times you have to call this well you're going to have to call it n number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1349" target="_blank">00:22:29.000</a></span> | <span class="t">pixels divided by 256 times now that might not be an integer so you'll have to round</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1355" target="_blank">00:22:35.920</a></span> | <span class="t">that up so it's ceiling and so that's how we can calculate the number of blocks we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1362" target="_blank">00:22:42.040</a></span> | <span class="t">to make sure that our kernel is called enough times now we do have a problem though which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1369" target="_blank">00:22:49.800</a></span> | <span class="t">is that the number of times we would have liked to have called it which previously was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1376" target="_blank">00:22:56.160</a></span> | <span class="t">equal to the number of pixels might not be a multiple of 256 so we might end up going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1382" target="_blank">00:23:02.520</a></span> | <span class="t">too far and so that's why we also need in our kernel now this if statement and so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1388" target="_blank">00:23:08.640</a></span> | <span class="t">is making sure that the index that we're up to does not go past the number of pixels we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1395" target="_blank">00:23:15.360</a></span> | <span class="t">have and this appears and basically every CUDA kernel you'll see and it's called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1400" target="_blank">00:23:20.040</a></span> | <span class="t">guard or the guard block so this is our guard to make sure we don't go out of bounds so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1405" target="_blank">00:23:25.880</a></span> | <span class="t">this is the same line of code we had before and now we've also just added this thing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1411" target="_blank">00:23:31.560</a></span> | <span class="t">calculate the index and we've added the guard and this is like the pretty standard first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1417" target="_blank">00:23:37.920</a></span> | <span class="t">lines from any CUDA kernel so we can now run those and they'll do exactly the same thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1427" target="_blank">00:23:47.400</a></span> | <span class="t">as before and so the obvious question is well why do CUDA kernels work in this weird block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1435" target="_blank">00:23:55.680</a></span> | <span class="t">and thread way why don't we just tell them the number of times to run it why do we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1443" target="_blank">00:24:03.720</a></span> | <span class="t">to do it by blocks and threads and the reason why is because of some of this detail that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1449" target="_blank">00:24:09.400</a></span> | <span class="t">we've got here which is that CUDA sets things up for us so that everything in the same block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1457" target="_blank">00:24:17.920</a></span> | <span class="t">or to say it more completely thread block which is the same block they will all be given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1464" target="_blank">00:24:24.840</a></span> | <span class="t">some shared memory and they'll also all be given the opportunity to synchronize which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1469" target="_blank">00:24:29.560</a></span> | <span class="t">is to basically say okay everything in this block has to get to this point before you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1475" target="_blank">00:24:35.640</a></span> | <span class="t">can move on all of the threads in a block will be executed on the same streaming multiprocessor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1484" target="_blank">00:24:44.600</a></span> | <span class="t">and so we'll we'll see later in later lectures that won't be taught by me that by using blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1494" target="_blank">00:24:54.520</a></span> | <span class="t">smartly you can make your code run more quickly and the shared memory is particularly important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1501" target="_blank">00:25:01.320</a></span> | <span class="t">so shared memory is a little bit of memory in the GPU that all the threads in a block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1506" target="_blank">00:25:06.600</a></span> | <span class="t">share and it's fast it's super super super fast now when we say not very much it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1513" target="_blank">00:25:13.880</a></span> | <span class="t">on a 3090 it's 128k so very small so this is basically the same as a cache in a CPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1526" target="_blank">00:25:26.400</a></span> | <span class="t">the difference though is that on a CPU you're not going to be manually deciding what goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1531" target="_blank">00:25:31.520</a></span> | <span class="t">into your cache but on the GPU you do it's all up to you so at the moment this cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1536" target="_blank">00:25:36.680</a></span> | <span class="t">is not going to be used when we create our CUDA code because we're just getting started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1542" target="_blank">00:25:42.800</a></span> | <span class="t">and so we're not going to worry about that optimization but to go fast you want to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1546" target="_blank">00:25:46.480</a></span> | <span class="t">that cache and also you want to use the register file something a lot of people don't realize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1552" target="_blank">00:25:52.520</a></span> | <span class="t">is that there's actually quite a lot of register memory even more register memory than shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1556" target="_blank">00:25:56.600</a></span> | <span class="t">memory so anyway those are all things to worry about down the track not needed for getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1560" target="_blank">00:26:00.920</a></span> | <span class="t">started. So how do we go about using CUDA? There is a basically standard setup block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1576" target="_blank">00:26:16.960</a></span> | <span class="t">that I would add and we are going to add and what happens in this setup block is we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1582" target="_blank">00:26:22.880</a></span> | <span class="t">going to set an environment variable you wouldn't use this in kind of production or for going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1587" target="_blank">00:26:27.960</a></span> | <span class="t">fast but this says if you get an error stop right away basically so wait you know wait</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1595" target="_blank">00:26:35.000</a></span> | <span class="t">to see how things go and then that way you can tell us exactly when an error occurs and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1600" target="_blank">00:26:40.840</a></span> | <span class="t">where it happens so that slows things down but it's good for development. We're also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1608" target="_blank">00:26:48.480</a></span> | <span class="t">going to install two modules one is a build tool which is required by PyTorch to compile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1615" target="_blank">00:26:55.960</a></span> | <span class="t">your C++ CUDA code. The second is a very handy little thing called Wurlitzer and the only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1624" target="_blank">00:27:04.360</a></span> | <span class="t">place you're going to see that used is in this line here where we load this extension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1628" target="_blank">00:27:08.300</a></span> | <span class="t">called Wurlitzer. Without this anything you print from your CUDA code in fact from your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1634" target="_blank">00:27:14.880</a></span> | <span class="t">C++ code won't appear in a notebook so you always want to do this in a notebook where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1640" target="_blank">00:27:20.160</a></span> | <span class="t">you're doing stuff in CUDA so that you can use print statements to debug things. Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1647" target="_blank">00:27:27.120</a></span> | <span class="t">so if you've got some CUDA code how do you use it from Python? The answer is that PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1655" target="_blank">00:27:35.880</a></span> | <span class="t">comes with a very handy thing called load inline which is inside torch.utils.cpp extension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1671" target="_blank">00:27:51.640</a></span> | <span class="t">Load inline is a marvelous load inline is a marvelous function that you just pass in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1682" target="_blank">00:28:02.320</a></span> | <span class="t">a list of any of the CUDA code strings that you want to compile any of the plain C++ strings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1688" target="_blank">00:28:08.240</a></span> | <span class="t">you want to compile any functions in that C++ you want to make available to PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1695" target="_blank">00:28:15.360</a></span> | <span class="t">and it will go and compile it all turn it into a Python module and make it available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1700" target="_blank">00:28:20.640</a></span> | <span class="t">right away which is pretty amazing. I've just created a tiny little wrapper for that called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1707" target="_blank">00:28:27.160</a></span> | <span class="t">load CUDA just to streamline it a tiny bit but behind the scenes it's just going to call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1713" target="_blank">00:28:33.160</a></span> | <span class="t">load inline. The other thing I've done is I've created a string that contains some C++ code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1723" target="_blank">00:28:43.720</a></span> | <span class="t">I mean this is all C code I think but it's compiled as C++ code we'll call it C++ code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1731" target="_blank">00:28:51.160</a></span> | <span class="t">C++ code we want included in all of our CUDA files. We need to include this header file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1739" target="_blank">00:28:59.600</a></span> | <span class="t">to make sure that we can access PyTorch tensor stuff. We want to be able to use I/O and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1746" target="_blank">00:29:06.800</a></span> | <span class="t">want to be able to check for exceptions. And then I also define three macros. The first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1754" target="_blank">00:29:14.880</a></span> | <span class="t">macro just checks that a tensor is CUDA. The second one checks that it's contiguous in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1763" target="_blank">00:29:23.040</a></span> | <span class="t">memory because sometimes PyTorch can actually split things up over different memory pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1767" target="_blank">00:29:27.920</a></span> | <span class="t">and then if we try to access that in this flattened out form it won't work. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1773" target="_blank">00:29:33.320</a></span> | <span class="t">the way we're actually going to use it check input or just check both of those things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1777" target="_blank">00:29:37.480</a></span> | <span class="t">So if something's not on CUDA and it's not contiguous we aren't going to be able to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1781" target="_blank">00:29:41.360</a></span> | <span class="t">it so we always have this. And then the third thing we do here is we define ceiling division.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1788" target="_blank">00:29:48.840</a></span> | <span class="t">Ceiling division is just this although you can implement it a different way like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1799" target="_blank">00:29:59.400</a></span> | <span class="t">and so this will do ceiling division and so this is how we're going to this is what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1803" target="_blank">00:30:03.440</a></span> | <span class="t">going to call in order to figure out how many blocks we need. So this is just you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1808" target="_blank">00:30:08.040</a></span> | <span class="t">have to worry about the details of this too much it's just a standard setup we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1810" target="_blank">00:30:10.720</a></span> | <span class="t">to use. Okay so now we need to write our CUDA kernel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1816" target="_blank">00:30:16.880</a></span> | <span class="t">Now how do you write the CUDA kernel? Well all I did and I recommend you do is take your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1824" target="_blank">00:30:24.480</a></span> | <span class="t">Python kernel and paste it into chat GPT and say convert this to equivalent C code using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1833" target="_blank">00:30:33.240</a></span> | <span class="t">the same names formatting etc where possible paste it in and chat GPT will do it for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1840" target="_blank">00:30:40.840</a></span> | <span class="t">Unless you're very comfortable with C which case just write it yourself is fine but this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1844" target="_blank">00:30:44.840</a></span> | <span class="t">way since you've already got the Python why not just do this? It basically was pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1852" target="_blank">00:30:52.680</a></span> | <span class="t">much perfect I found although it did assume that these were floats they're actually not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1860" target="_blank">00:31:00.280</a></span> | <span class="t">floats we had to change a couple of data types but basically I was able to use it almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1864" target="_blank">00:31:04.640</a></span> | <span class="t">as is and so particularly you know for people who are much more Python programmers nowadays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1872" target="_blank">00:31:12.040</a></span> | <span class="t">like me this is a nice way to write 95 percent of the code you need. What else do we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1881" target="_blank">00:31:21.360</a></span> | <span class="t">to change? Well as we saw in our picture earlier it's not called block IDX it's called blockIDX.X</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1891" target="_blank">00:31:31.360</a></span> | <span class="t">blockDIM.X threadIDX.X so we have to add the dot X there. Other than that if we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1903" target="_blank">00:31:43.840</a></span> | <span class="t">so as you can see these two pieces of code look nearly identical we've had to add data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1908" target="_blank">00:31:48.000</a></span> | <span class="t">types to them we've had to add semicolons we had to get rid of the colon we had to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1915" target="_blank">00:31:55.160</a></span> | <span class="t">curly brackets that's about it. So it's not very different at all so if you haven't done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1922" target="_blank">00:32:02.080</a></span> | <span class="t">much C programming yeah don't worry about it too much because you know the truth is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1928" target="_blank">00:32:08.520</a></span> | <span class="t">actually it's not that different for this kind of calculation intensive work. One thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1937" target="_blank">00:32:17.300</a></span> | <span class="t">we should talk about is this. What's unsigned car star? This is just how you write Uint8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1946" target="_blank">00:32:26.760</a></span> | <span class="t">in C. You can just if you're not sure how to change change a data type between the PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1955" target="_blank">00:32:35.160</a></span> | <span class="t">spelling and the C spelling you could ask chat GPT or you can Google it but this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1960" target="_blank">00:32:40.200</a></span> | <span class="t">how you write byte. The star in practice it's basically how you say this is an array. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1967" target="_blank">00:32:47.280</a></span> | <span class="t">this says that X is an array of bytes. It actually means it's a pointer but pointers are treated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1980" target="_blank">00:33:00.000</a></span> | <span class="t">as you can see here as arrays by C. So you don't really have to worry about the fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1986" target="_blank">00:33:06.040</a></span> | <span class="t">that the pointer it just means for us that it's an array but in C the only kind of arrays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1992" target="_blank">00:33:12.160</a></span> | <span class="t">that it knows how to deal with these one-dimensional arrays and that's why we always have to flatten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=1997" target="_blank">00:33:17.160</a></span> | <span class="t">things out okay. We can't use multi-dimensional tensors really directly in these CUDA kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2002" target="_blank">00:33:22.560</a></span> | <span class="t">in this way. So we're going to end up with these one-dimensional C arrays. Yeah other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2008" target="_blank">00:33:28.280</a></span> | <span class="t">than that it's going to look exactly in fact I mean even because we did our Python like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2012" target="_blank">00:33:32.240</a></span> | <span class="t">that it's going to look identical. The void here just means it doesn't return anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2018" target="_blank">00:33:38.400</a></span> | <span class="t">and then the dunder global here is a special thing added by CUDA. There's three things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2026" target="_blank">00:33:46.360</a></span> | <span class="t">that can appear and this simply says what should I compile this to do and so you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2033" target="_blank">00:33:53.700</a></span> | <span class="t">put dunder device and that means compile it so that you can only call it on the GPU. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2040" target="_blank">00:34:00.080</a></span> | <span class="t">can say dunder global and that says okay you can call it from the CPU or GPU and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2046" target="_blank">00:34:06.720</a></span> | <span class="t">run on the GPU or you can write write dunder host which you don't have to and that just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2051" target="_blank">00:34:11.760</a></span> | <span class="t">means it's a normal C or C++ program that runs on the CPU side. So anytime we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2058" target="_blank">00:34:18.080</a></span> | <span class="t">call something from the CPU side to run something on the GPU which is basically almost always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2064" target="_blank">00:34:24.520</a></span> | <span class="t">when we're doing kernels you write dunder global. So here we've got dunder global we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2072" target="_blank">00:34:32.080</a></span> | <span class="t">got our kernel and that's it. So then we need the thing to call that kernel. So earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2080" target="_blank">00:34:40.400</a></span> | <span class="t">to call the kernel we called this block kernel function passed in the kernel and passed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2085" target="_blank">00:34:45.700</a></span> | <span class="t">the blocks and threads and the arguments. With CUDA we don't have to use a special function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2091" target="_blank">00:34:51.520</a></span> | <span class="t">there is a weird special syntax built into kernel to do it for us. To use the weird special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2097" target="_blank">00:34:57.480</a></span> | <span class="t">syntax you say okay what's the kernel the function that I want to call and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2103" target="_blank">00:35:03.600</a></span> | <span class="t">use these weird triple angle brackets. So the triple angle brackets is a special CUDA extension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2111" target="_blank">00:35:11.840</a></span> | <span class="t">to the C++ language and it means this is a kernel please call it on the GPU and between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2120" target="_blank">00:35:20.200</a></span> | <span class="t">the triple angle brackets there's a number of things you can pass but you have to pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2125" target="_blank">00:35:25.760</a></span> | <span class="t">at least the first two things which is how many blocks how many threads. So how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2133" target="_blank">00:35:33.160</a></span> | <span class="t">blocks ceiling division number of pixels divided by threads and how many threads as we said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2140" target="_blank">00:35:40.800</a></span> | <span class="t">before let's just pick 256 all the time and not worry about it. So that says call this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2145" target="_blank">00:35:45.600</a></span> | <span class="t">function as a GPU kernel and then passing in these arguments. We have to pass in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2152" target="_blank">00:35:52.680</a></span> | <span class="t">input tensor, our output tensor and how many pixels. And you'll see that for each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2158" target="_blank">00:35:58.480</a></span> | <span class="t">tensors we have to use a special method .data pointer and that's going to convert it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2164" target="_blank">00:36:04.320</a></span> | <span class="t">a C pointer to the tensor. So that's why by the time it arrives in our kernel it's a C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2171" target="_blank">00:36:11.440</a></span> | <span class="t">pointer. You also have to tell it what data type you want it to be treated as. This says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2177" target="_blank">00:36:17.680</a></span> | <span class="t">treat it as Uintates. So that's this is a C++ template parameter here and this is a method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2190" target="_blank">00:36:30.040</a></span> | <span class="t">The other thing you need to know is in C++ dot means call a method of an object or else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2198" target="_blank">00:36:38.120</a></span> | <span class="t">colon colon is basically like in C in Python calling a method of a class. So you don't say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2205" target="_blank">00:36:45.760</a></span> | <span class="t">torch dot empty you say torch colon colon empty to create our output or else back when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2211" target="_blank">00:36:51.920</a></span> | <span class="t">did it in Python we said torch dot empty. Also in Python oh okay so in Python that's right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2221" target="_blank">00:37:01.880</a></span> | <span class="t">we just created a length n vector and then did a dot view. It doesn't really matter how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2227" target="_blank">00:37:07.080</a></span> | <span class="t">we do it but in this case we actually created a two-dimensional tensor bypassing. We pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2232" target="_blank">00:37:12.160</a></span> | <span class="t">in this thing in curly brackets here this is called a C++ list initializer and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2237" target="_blank">00:37:17.000</a></span> | <span class="t">just basically a little list containing height comma width. So this tells it to create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2241" target="_blank">00:37:21.840</a></span> | <span class="t">two-dimensional matrix which is why we don't need dot view at the end. We could have done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2245" target="_blank">00:37:25.800</a></span> | <span class="t">it the dot view way as well. Probably be better to keep it consistent but this is what I wrote</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2250" target="_blank">00:37:30.600</a></span> | <span class="t">at the time. The other interesting thing when we create the output is if you pass in input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2257" target="_blank">00:37:37.720</a></span> | <span class="t">dot options so this is our input tensor that just says oh use the same data type and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2264" target="_blank">00:37:44.300</a></span> | <span class="t">same device CUDA device as our input has. This is a nice really convenient way which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2269" target="_blank">00:37:49.400</a></span> | <span class="t">I don't even think we have in Python to say make sure that this is the same data type</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2274" target="_blank">00:37:54.880</a></span> | <span class="t">in the same device. If you say auto here this is quite convenient you don't have to specify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2281" target="_blank">00:38:01.240</a></span> | <span class="t">what type this is. We could have written torch colon colon tensor but by writing auto it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2286" target="_blank">00:38:06.320</a></span> | <span class="t">just says figure it out yourself which is another convenient little C++ thing. After</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2293" target="_blank">00:38:13.040</a></span> | <span class="t">we call the kernel if there's an error in it we won't necessarily get told so to tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2298" target="_blank">00:38:18.080</a></span> | <span class="t">it to check for an error you have to write this. This is a macro that's again provided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2303" target="_blank">00:38:23.900</a></span> | <span class="t">by PyTorch. The details don't matter you should just always call it after you call a kernel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2309" target="_blank">00:38:29.440</a></span> | <span class="t">to make sure it works and then you can return the tensor that you allocated and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2316" target="_blank">00:38:36.320</a></span> | <span class="t">passed as a pointer and then that you filled in. Okay now as well as the CUDA source you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2328" target="_blank">00:38:48.360</a></span> | <span class="t">also need C++ source and the C++ source is just something that says here is a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2335" target="_blank">00:38:55.560</a></span> | <span class="t">all of the details of the functions that I want you to make available to the outside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2342" target="_blank">00:39:02.000</a></span> | <span class="t">world in this case Python and so this is basically your header effectively. So you can just copy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2348" target="_blank">00:39:08.960</a></span> | <span class="t">and paste the full line here from your function definition and stick a semicolon on the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2356" target="_blank">00:39:16.640</a></span> | <span class="t">So that's something you can always do and so then we call our load CUDA function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2361" target="_blank">00:39:21.400</a></span> | <span class="t">we looked at earlier passing in the CUDA source code the C++ source code and then a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2367" target="_blank">00:39:27.760</a></span> | <span class="t">the names of the functions that are defined there that you want to make available to Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2372" target="_blank">00:39:32.440</a></span> | <span class="t">So we just have one which is the RGB2 grayscale function and believe it or not that's all you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2378" target="_blank">00:39:38.840</a></span> | <span class="t">have to do this will automatically you can see it running in the background now compiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2385" target="_blank">00:39:45.440</a></span> | <span class="t">with a hugely long thing our files from so it's created a main.cpp for us and it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2396" target="_blank">00:39:56.960</a></span> | <span class="t">to put it into a main.o for us and compile everything up link it all together and create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2404" target="_blank">00:40:04.540</a></span> | <span class="t">a module and you can see here we then take that module it's been passed back and put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2409" target="_blank">00:40:09.880</a></span> | <span class="t">it into a variable called module and then when it's done it will load that module and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2417" target="_blank">00:40:17.880</a></span> | <span class="t">if we look inside the module that we just created you'll see now that apart from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2421" target="_blank">00:40:21.800</a></span> | <span class="t">normal auto generated stuff Python adds it's got a function in it RGB2 grayscale okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2428" target="_blank">00:40:28.840</a></span> | <span class="t">that's amazing we now have a CUDA function that's been made available from Python and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2435" target="_blank">00:40:35.040</a></span> | <span class="t">we can even see if we want to this is where it put it all so we can have a look and there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2445" target="_blank">00:40:45.480</a></span> | <span class="t">it is you can see it's created a main.cpp it's compiled it into a main.o it's created a library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2452" target="_blank">00:40:52.400</a></span> | <span class="t">that we can load up it's created a CUDA file it's created a build script and we could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2458" target="_blank">00:40:58.840</a></span> | <span class="t">a look at that build script if we wanted to and there it is so none of this matters too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2466" target="_blank">00:41:06.240</a></span> | <span class="t">much it's just nice to know that PyTorch is doing all this stuff for us and we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2470" target="_blank">00:41:10.680</a></span> | <span class="t">to worry about it so that's pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2476" target="_blank">00:41:16.080</a></span> | <span class="t">So in order to pass a tensor to this we're going to be checking that it's contiguous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2483" target="_blank">00:41:23.420</a></span> | <span class="t">and on CUDA so we'd better make sure it is so we're going to create an image C variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2489" target="_blank">00:41:29.360</a></span> | <span class="t">which is the image made contiguous and put on through the CUDA device and now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2498" target="_blank">00:41:38.800</a></span> | <span class="t">actually run this on the full sized image not on the tiny little minimized image we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2503" target="_blank">00:41:43.360</a></span> | <span class="t">created before this has got much more pixels it's got 1.7 million pixels where else before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2510" target="_blank">00:41:50.360</a></span> | <span class="t">we had I think it was 35,000 34,000 and it's gone down from one and a half seconds to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2520" target="_blank">00:42:00.000</a></span> | <span class="t">millisecond so that is amazing it's dramatically faster both because it's now running in compiled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2529" target="_blank">00:42:09.840</a></span> | <span class="t">code and because it's running on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2535" target="_blank">00:42:15.600</a></span> | <span class="t">The step of putting the data onto the GPU is not part of what we timed and that's probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2541" target="_blank">00:42:21.280</a></span> | <span class="t">fair enough because normally you do that once and then you run a whole lot of CUDA things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2545" target="_blank">00:42:25.400</a></span> | <span class="t">on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2548" target="_blank">00:42:28.020</a></span> | <span class="t">We have though included the step of moving it off the GPU and putting it onto the CPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2553" target="_blank">00:42:33.680</a></span> | <span class="t">as part of what we're timing and one key reason for that is that if we didn't do that it can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2559" target="_blank">00:42:39.680</a></span> | <span class="t">actually run our Python code at the same time that the CUDA code is still running and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2566" target="_blank">00:42:46.160</a></span> | <span class="t">the amount of time shown could be dramatically less because it hasn't finished synchronizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2571" target="_blank">00:42:51.880</a></span> | <span class="t">so by adding this it forces it to complete the CUDA run and to put the data back onto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2581" target="_blank">00:43:01.240</a></span> | <span class="t">the CPU that kind of synchronization you can also trigger this by printing a value from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2586" target="_blank">00:43:06.920</a></span> | <span class="t">it or you can synchronize it manually so after we've done that and we can have a look and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2593" target="_blank">00:43:13.600</a></span> | <span class="t">we should get exactly the same grayscale puppy okay so we have successfully created our first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2607" target="_blank">00:43:27.920</a></span> | <span class="t">real working code from Python CUDA kernel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2616" target="_blank">00:43:36.140</a></span> | <span class="t">This approach of writing it in Python and then converting it to CUDA is not particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2626" target="_blank">00:43:46.320</a></span> | <span class="t">common but I'm not just doing it as an educational exercise that's how I like to write my CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2633" target="_blank">00:43:53.400</a></span> | <span class="t">kernels at least as much of it as I can because it's much easier to debug in Python it's much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2644" target="_blank">00:44:04.600</a></span> | <span class="t">easier to see exactly what's going on and so and I don't have to worry about compiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2650" target="_blank">00:44:10.560</a></span> | <span class="t">it takes about 45 or 50 seconds to compile even our simple example here I can just run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2655" target="_blank">00:44:15.520</a></span> | <span class="t">it straight away and once it's working to convert that into C as I mentioned you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2660" target="_blank">00:44:20.720</a></span> | <span class="t">chatgpt can do most of it for us so I think this is actually a fantastically good way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2667" target="_blank">00:44:27.320</a></span> | <span class="t">of writing CUDA kernels even as you start to get somewhat familiar with them it's because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2674" target="_blank">00:44:34.280</a></span> | <span class="t">it lets you debug and develop much more quickly a lot of people avoid writing CUDA just because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2683" target="_blank">00:44:43.860</a></span> | <span class="t">that process is so painful and so here's a way that we can make that process less painful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2689" target="_blank">00:44:49.360</a></span> | <span class="t">so let's do it again and this time we're going to do it to implement something very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2696" target="_blank">00:44:56.000</a></span> | <span class="t">which is matrix multiplication so matrix multiplication as you probably know is fundamentally critical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2705" target="_blank">00:45:05.640</a></span> | <span class="t">for deep learning it's like the most basic linear algebra operation we have and the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2712" target="_blank">00:45:12.680</a></span> | <span class="t">it works is that you have a input matrix M and a second input matrix N and we go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2724" target="_blank">00:45:24.240</a></span> | <span class="t">every row of M so we go through every row of M till we get to here we are up to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2730" target="_blank">00:45:30.520</a></span> | <span class="t">one and every column of N and here we are up to this one and then we take the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2736" target="_blank">00:45:36.880</a></span> | <span class="t">at each point of that row with that column and this here is the dot product of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2745" target="_blank">00:45:45.200</a></span> | <span class="t">two things and that is what matrix multiplication is so it's a very simple operation conceptually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2758" target="_blank">00:45:58.720</a></span> | <span class="t">and it's one that we do many many many times in deep learning and basically every deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2764" target="_blank">00:46:04.560</a></span> | <span class="t">learning every neural network has this is its most fundamental operation of course we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2771" target="_blank">00:46:11.520</a></span> | <span class="t">actually need to implement matrix multiplication from scratch because it's done for us in libraries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2776" target="_blank">00:46:16.280</a></span> | <span class="t">but we will often do things where we have to kind of fuse in some kind of matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2782" target="_blank">00:46:22.400</a></span> | <span class="t">like paces and so you know and of course it's also just a good exercise so let's take a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2789" target="_blank">00:46:29.800</a></span> | <span class="t">look at how to do matrix multiplication first of all in pure Python so in the actually in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2798" target="_blank">00:46:38.680</a></span> | <span class="t">the first AI course that I mentioned there's a very complete in-depth dive into matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2805" target="_blank">00:46:45.000</a></span> | <span class="t">multiplication in part two less than 11 where we spend like an hour or two talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2812" target="_blank">00:46:52.120</a></span> | <span class="t">nothing but matrix multiplication we're not going to go into that much detail here but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2817" target="_blank">00:46:57.040</a></span> | <span class="t">what we do do in that is we use the MNIST data set to to do this and so we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2824" target="_blank">00:47:04.560</a></span> | <span class="t">to do the same thing here we're going to grab the MNIST data set of handwritten digits and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2831" target="_blank">00:47:11.280</a></span> | <span class="t">they are 28 by 28 digits they look like this 28 by 28 is 784 so to do a you know to basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2841" target="_blank">00:47:21.320</a></span> | <span class="t">do a single layer of a neural net or without the activation function we would do a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2849" target="_blank">00:47:29.400</a></span> | <span class="t">multiplication of the image flattened out by a weight matrix with 784 rows and however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2858" target="_blank">00:47:38.040</a></span> | <span class="t">many columns we like and I'm going to need if we're going to go straight to the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2861" target="_blank">00:47:41.320</a></span> | <span class="t">so this would be a linear function a linear model we'd have 10 layers one for each digit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2866" target="_blank">00:47:46.080</a></span> | <span class="t">so here's this is our weights we're not actually going to do any learning here this is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2870" target="_blank">00:47:50.440</a></span> | <span class="t">not any deep learning or logistic regression learning is just for an example okay so we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2876" target="_blank">00:47:56.680</a></span> | <span class="t">got our weights and we've got our input our input data x train and x valid and so we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2886" target="_blank">00:48:06.360</a></span> | <span class="t">going to start off by implementing this in Python now again Python's really slow so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2891" target="_blank">00:48:11.960</a></span> | <span class="t">make this smaller so matrix one will just be five rows matrix two will be all the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2898" target="_blank">00:48:18.920</a></span> | <span class="t">so that's going to be a five by seven eighty four matrix multiplied by a seven eighty four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2905" target="_blank">00:48:25.480</a></span> | <span class="t">by ten matrix now these two have to match of course they have to match because otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2912" target="_blank">00:48:32.800</a></span> | <span class="t">this product won't work those two are going to have to match the row by the column okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2920" target="_blank">00:48:40.560</a></span> | <span class="t">so let's pull that out into a rows a columns b rows b columns and obviously a columns and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2927" target="_blank">00:48:47.640</a></span> | <span class="t">b rows are the things that have to match and then the output will be a rows by b columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2933" target="_blank">00:48:53.580</a></span> | <span class="t">so five by ten so let's create an output fill of zeros with rows by columns in it and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2943" target="_blank">00:49:03.600</a></span> | <span class="t">now we can go ahead and go through every row of a every column of b and do the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2951" target="_blank">00:49:11.360</a></span> | <span class="t">which involves going through every item in the innermost dimension or 784 of them multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2957" target="_blank">00:49:17.880</a></span> | <span class="t">together the equivalent things from m1 and m2 and summing them up into the output tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2967" target="_blank">00:49:27.920</a></span> | <span class="t">that we created so that's going to give us as we said a five by ten five by ten output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2981" target="_blank">00:49:41.760</a></span> | <span class="t">and here it is okay so this is how I always create things in python I basically almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2989" target="_blank">00:49:49.400</a></span> | <span class="t">never have to debug I almost never have like errors unexpected errors in my code because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=2995" target="_blank">00:49:55.520</a></span> | <span class="t">I've written every single line one step at a time in python I've checked them all as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3000" target="_blank">00:50:00.000</a></span> | <span class="t">they go and then I copy all the cells and merge them together stick a function header</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3004" target="_blank">00:50:04.200</a></span> | <span class="t">on like so and so here is matmul so this is exactly the code we've already seen and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3010" target="_blank">00:50:10.600</a></span> | <span class="t">can call it and we'll see that for 39,200 innermost operations we took us about a second so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3026" target="_blank">00:50:26.600</a></span> | <span class="t">pretty slow okay so now that we've done that you might not be surprised to hear that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3032" target="_blank">00:50:32.840</a></span> | <span class="t">now need to do the innermost loop as a kernel call in such a way that it is can be run in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3041" target="_blank">00:50:41.720</a></span> | <span class="t">parallel now in this case the innermost loop is not this line of code it's actually this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3050" target="_blank">00:50:50.400</a></span> | <span class="t">line of code I mean we can choose to be whatever we want it to be but in this case this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3054" target="_blank">00:50:54.400</a></span> | <span class="t">how we're going to do it we're going to say for every pixel we're not every pixel for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3058" target="_blank">00:50:58.560</a></span> | <span class="t">every cell in the output tensor like this one here is going to be one CUDA thread so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3067" target="_blank">00:51:07.040</a></span> | <span class="t">one CUDA thread is going to do the dot product so this is the bit that does the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3074" target="_blank">00:51:14.200</a></span> | <span class="t">so that'll be our kernel so we can write that matmul block kernel is going to contain that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3085" target="_blank">00:51:25.680</a></span> | <span class="t">okay so that's exactly the same thing that we just copied from above and so now we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3091" target="_blank">00:51:31.520</a></span> | <span class="t">going to need a something to run this kernel and you might not be surprised to hear that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3101" target="_blank">00:51:41.400</a></span> | <span class="t">in CUDA we are going to call this using blocks and threads but something that's rather handy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3110" target="_blank">00:51:50.240</a></span> | <span class="t">in CUDA is that the blocks and threads don't have to be just a 1d vector they can be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3118" target="_blank">00:51:58.440</a></span> | <span class="t">2d or even 3d tensor so in this case you can see we've got one two a little hard to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3129" target="_blank">00:52:09.080</a></span> | <span class="t">exactly where they stop two three four blocks and so then for each block that's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3143" target="_blank">00:52:23.400</a></span> | <span class="t">in one dimension and then there's also one two three four five blocks in the other dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3155" target="_blank">00:52:35.600</a></span> | <span class="t">and so each of these blocks has an index so this one here is going to be zero zero a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3165" target="_blank">00:52:45.760</a></span> | <span class="t">bit hard to see this one here is going to be one three and so forth and this one over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3172" target="_blank">00:52:52.720</a></span> | <span class="t">here is going to be three four so rather than just having a integer block index we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3186" target="_blank">00:53:06.500</a></span> | <span class="t">to have a tuple block index and then within a block there's going to be to pick let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3199" target="_blank">00:53:19.160</a></span> | <span class="t">say this exact spot here didn't do that very well there's going to be a thread index and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3210" target="_blank">00:53:30.840</a></span> | <span class="t">again the thread index won't be a single index into a vector it'll be a two elements so in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3217" target="_blank">00:53:37.480</a></span> | <span class="t">this case it would be 0 1 2 3 4 5 6 rows down and 0 1 2 3 4 5 6 7 8 9 10 is that 11 12 I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3232" target="_blank">00:53:52.440</a></span> | <span class="t">can't count 12 maybe across so the this here is actually going to be defined by two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3239" target="_blank">00:53:59.120</a></span> | <span class="t">one is by the block and so the block is 3 comma 4 and the thread is 6 comma 12 so that's how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3262" target="_blank">00:54:22.080</a></span> | <span class="t">CUDA lets us index into two-dimensional grids using blocks and threads we don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3271" target="_blank">00:54:31.880</a></span> | <span class="t">it's just a convenience if we want to and in fact it can we can use up to three dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3281" target="_blank">00:54:41.640</a></span> | <span class="t">so to create our kernel runner now rather than just having so rather than just having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3290" target="_blank">00:54:50.640</a></span> | <span class="t">two nested loops for blocks and threads we're going to have to have two lots of two nested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3298" target="_blank">00:54:58.360</a></span> | <span class="t">loops for our both of our X and Y blocks and threads or our rows and columns blocks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3306" target="_blank">00:55:06.920</a></span> | <span class="t">threads so it ends up looking a bit messy because we now have four nested for loops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3316" target="_blank">00:55:16.720</a></span> | <span class="t">so we'll go through our blocks on the Y axis and then through our blocks on the X axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3322" target="_blank">00:55:22.320</a></span> | <span class="t">and then through our threads on the Y axis and then through our threads on the X axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3327" target="_blank">00:55:27.040</a></span> | <span class="t">and so what that means is that for you can think of this Cartesian product as being for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3332" target="_blank">00:55:32.040</a></span> | <span class="t">each block for each thread now to get the dot Y and the dot X will use this handy little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3340" target="_blank">00:55:40.320</a></span> | <span class="t">Python standard library thing called simple namespace I'd use that so much I just give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3344" target="_blank">00:55:44.400</a></span> | <span class="t">it an NS name because I use namespaces all the time and my quick and dirty code so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3350" target="_blank">00:55:50.240</a></span> | <span class="t">go through all those four we then call our kernel and we pass in an object containing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3358" target="_blank">00:55:58.320</a></span> | <span class="t">the Y and X coordinates and that's going to be our block and we also pass in our thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3369" target="_blank">00:56:09.600</a></span> | <span class="t">which is an object with the Y and X coordinates of our thread and it's going to eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3377" target="_blank">00:56:17.280</a></span> | <span class="t">do all possible blocks and all possible threads numbers for each of those blocks and we also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3384" target="_blank">00:56:24.240</a></span> | <span class="t">need to tell it how big is each block how how high and how wide and so that's what this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3390" target="_blank">00:56:30.240</a></span> | <span class="t">is this is going to be a simple namespace and object with an X and Y as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3396" target="_blank">00:56:36.400</a></span> | <span class="t">so I need to know how big they are just like earlier on we had to know the block dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3403" target="_blank">00:56:43.920</a></span> | <span class="t">that's why we passed in threads so remember this is all pure PyTorch we're not actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3410" target="_blank">00:56:50.480</a></span> | <span class="t">calling any out to any CUDA we're not calling out to any libraries other than just a tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3414" target="_blank">00:56:54.800</a></span> | <span class="t">bit of PyTorch for the indexing and tensor creation so you can run all of this by hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3420" target="_blank">00:57:00.720</a></span> | <span class="t">make sure you understand you can put it in the debugger you can step through it and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3426" target="_blank">00:57:06.720</a></span> | <span class="t">it's going to call our function so here's our matrix modification function as we said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3430" target="_blank">00:57:10.760</a></span> | <span class="t">it's a kernel that contains the dot product that we wrote earlier so now the guard is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3436" target="_blank">00:57:16.840</a></span> | <span class="t">going to have to check that the row number we're up to is not taller than we have and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3443" target="_blank">00:57:23.520</a></span> | <span class="t">the column number we're up to is not wider than we have and we also need to know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3447" target="_blank">00:57:27.400</a></span> | <span class="t">row number we're up to and this is exactly the same actually I should say the column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3452" target="_blank">00:57:32.760</a></span> | <span class="t">is exactly the same as we've seen before and in fact you might remember in the CUDA we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3456" target="_blank">00:57:36.880</a></span> | <span class="t">had block idx dot x and this is why right because in CUDA it's always gives you these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3464" target="_blank">00:57:44.320</a></span> | <span class="t">three-dimensional dim three structures so you have to put this dot x so we can find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3472" target="_blank">00:57:52.560</a></span> | <span class="t">out the column this way and then we can find out the row by seeing how many blocks have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3478" target="_blank">00:57:58.760</a></span> | <span class="t">we gone through how big is each block in the y-axis and how many threads have we gone through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3483" target="_blank">00:58:03.800</a></span> | <span class="t">in the y-axis so which row number are we up to what column number are we up to is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3489" target="_blank">00:58:09.840</a></span> | <span class="t">inside the bounds of our tensor if not then just stop and then otherwise do our dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3501" target="_blank">00:58:21.360</a></span> | <span class="t">and put it into our output tensor so that's all pure Python and so now we can call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3509" target="_blank">00:58:29.960</a></span> | <span class="t">by getting the height and width of our first input the height and width of our second input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3516" target="_blank">00:58:36.640</a></span> | <span class="t">and so then K and K2 the inner dimensions ought to match we can then create our output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3523" target="_blank">00:58:43.860</a></span> | <span class="t">and so now threads per block is not just the number 256 but it's a pair of numbers it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3530" target="_blank">00:58:50.360</a></span> | <span class="t">an x and a y and we've selected two numbers that multiply together to create 256 so again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3535" target="_blank">00:58:55.960</a></span> | <span class="t">this is a reasonable choice if you've got two dimensional inputs to spread it out nicely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3544" target="_blank">00:59:04.940</a></span> | <span class="t">one thing to be aware of here is that your threads per block can't be bigger than 1024</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3556" target="_blank">00:59:16.320</a></span> | <span class="t">so we're using 256 which is safe right and notice that you have to multiply these together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3560" target="_blank">00:59:20.920</a></span> | <span class="t">16 times 16 is going to be the number of threads per block so this is a these are safe numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3566" target="_blank">00:59:26.600</a></span> | <span class="t">to use you're not going to run out of blocks though 2 to the 31 is the number of maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3572" target="_blank">00:59:32.520</a></span> | <span class="t">blocks for dimension 0 and then 2 to the 16 for dimensions 1 and 2 I think it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3577" target="_blank">00:59:37.920</a></span> | <span class="t">minus 1 but don't worry about that so don't have too many 10 threads but you can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3583" target="_blank">00:59:43.520</a></span> | <span class="t">lots of blocks but of course each symmetric model processor is going to run all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3589" target="_blank">00:59:49.160</a></span> | <span class="t">on the same device and they're also going to have access to shared memory so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3593" target="_blank">00:59:53.880</a></span> | <span class="t">why you use a few threads per block so our blocks the x we're going to use the ceiling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3601" target="_blank">01:00:01.520</a></span> | <span class="t">division the y we're going to use the same ceiling division so if any of this is unfamiliar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3606" target="_blank">01:00:06.880</a></span> | <span class="t">go back to our earlier example because the code's all copied from there and now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3610" target="_blank">01:00:10.840</a></span> | <span class="t">call our 2D block kernel runner passing in the kernel the number of blocks the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3616" target="_blank">01:00:16.600</a></span> | <span class="t">of threads per block our input matrices flattened out our output matrix flattened out and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3623" target="_blank">01:00:23.360</a></span> | <span class="t">dimensions that it needs because they get all used here and return the result and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3633" target="_blank">01:00:33.080</a></span> | <span class="t">if we call that matmul with a 2D block and we can check that they are close to what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3639" target="_blank">01:00:39.480</a></span> | <span class="t">got in our original manual loops and of course they are because it's running the same code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3646" target="_blank">01:00:46.680</a></span> | <span class="t">so now that we've done that we can do the CUDA version now the CUDA version is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3654" target="_blank">01:00:54.800</a></span> | <span class="t">to be so much faster we do not need to use this slimmed down matrix anymore we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3665" target="_blank">01:01:05.560</a></span> | <span class="t">the whole thing so to check that it's correct I want a fast CPU-based approach that I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3672" target="_blank">01:01:12.080</a></span> | <span class="t">compare to so previously I took about a second to do 39,000 elements so I'm not going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3681" target="_blank">01:01:21.680</a></span> | <span class="t">explain how this works but I'm going to use a broadcasting approach to get a fast CPU-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3686" target="_blank">01:01:26.040</a></span> | <span class="t">approach if you check the fast AI course we teach you how to do this broadcasting approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3691" target="_blank">01:01:31.400</a></span> | <span class="t">but it's a pure Python approach which manages to do it all in a single loop rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3696" target="_blank">01:01:36.080</a></span> | <span class="t">three nested loops it gives the same answer for the cut down tensors but much faster only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3710" target="_blank">01:01:50.920</a></span> | <span class="t">four milliseconds so it's fast enough that we can now run it on the whole input matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3720" target="_blank">01:02:00.920</a></span> | <span class="t">and it takes about 1.3 seconds and so this broadcast optimized version as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3726" target="_blank">01:02:06.880</a></span> | <span class="t">it's much faster and now we've got 392 million additions going on in the middle of our three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3734" target="_blank">01:02:14.720</a></span> | <span class="t">loops effectively three loops but we're broadcasting them so this is much faster but the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3739" target="_blank">01:02:19.960</a></span> | <span class="t">I'm really doing this is so that we can store this result to compare to so that makes sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3747" target="_blank">01:02:27.520</a></span> | <span class="t">that our CUDA version is correct okay so how do we convert this to CUDA you might not be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3755" target="_blank">01:02:35.120</a></span> | <span class="t">surprised to hear that what I did was I grabbed this function and I passed it over to chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3759" target="_blank">01:02:39.920</a></span> | <span class="t">GPT and said please rewrite this in C and it gave me something basically that I could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3765" target="_blank">01:02:45.480</a></span> | <span class="t">use first time and here it is this time I don't have unsigned cast I have float star</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3774" target="_blank">01:02:54.000</a></span> | <span class="t">other than that this looks almost exactly like the Python we had with exactly the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3781" target="_blank">01:03:01.980</a></span> | <span class="t">changes we saw before we've now got the dot Y and dot X versions once again we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3789" target="_blank">01:03:09.480</a></span> | <span class="t">done to global which says please run this on the GPU when we call it from the CPU so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3794" target="_blank">01:03:14.720</a></span> | <span class="t">the CUDA the kernel I don't think there's anything to talk about there and then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3798" target="_blank">01:03:18.680</a></span> | <span class="t">thing that calls the kernel is going to be passed in two tenses we're going to check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3804" target="_blank">01:03:24.240</a></span> | <span class="t">that they're both contiguous and check that they are on the CUDA device we'll grab the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3809" target="_blank">01:03:29.880</a></span> | <span class="t">height and width of the first and second tenses we're going to grab the inner dimension we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3817" target="_blank">01:03:37.120</a></span> | <span class="t">make sure that the inner dimensions of the two matrices match just like before and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3823" target="_blank">01:03:43.020</a></span> | <span class="t">is how you do an assertion in PyTorch CUDA code you call torch check pass anything to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3830" target="_blank">01:03:50.080</a></span> | <span class="t">check pass in the message to pop up if there's a problem so these are a really good thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3833" target="_blank">01:03:53.840</a></span> | <span class="t">to spread around all through your CUDA code to make sure that everything is as you thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3841" target="_blank">01:04:01.460</a></span> | <span class="t">it was going to be just like before we create an output so now when we create a number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3847" target="_blank">01:04:07.800</a></span> | <span class="t">threads we don't say threads is 256 we instead say this is a special thing provided by CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3855" target="_blank">01:04:15.320</a></span> | <span class="t">for us dim three so this is a basically a tuple with three elements so we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3861" target="_blank">01:04:21.040</a></span> | <span class="t">create a dim three called TPB it's going to be 16 by 16 now I said it has three elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3868" target="_blank">01:04:28.640</a></span> | <span class="t">where's the third one that's okay it just treats the third one as being one so it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3873" target="_blank">01:04:33.640</a></span> | <span class="t">we can ignore it so that's the number of threads per block and then how many blocks will there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3880" target="_blank">01:04:40.840</a></span> | <span class="t">be well in the X dimension it'll be W divided by X ceiling division in the Y dimension it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3890" target="_blank">01:04:50.120</a></span> | <span class="t">will be H divided by Y C division and ceiling division and so that's the number of blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3896" target="_blank">01:04:56.080</a></span> | <span class="t">we have so just like before we call our kernel just by calling it like a normal function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3903" target="_blank">01:05:03.120</a></span> | <span class="t">but then we add this weird triple angle bracket thing telling it how many blocks and how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3908" target="_blank">01:05:08.600</a></span> | <span class="t">threads so these aren't ints anymore these are now dim three structures and that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3916" target="_blank">01:05:16.660</a></span> | <span class="t">we use these dim three structures and in fact even before what actually happened behind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3922" target="_blank">01:05:22.800</a></span> | <span class="t">the scenes when we did the grayscale thing is even though we passed in 256 instance we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3932" target="_blank">01:05:32.960</a></span> | <span class="t">actually ended up with a dim three structure and just in which case the second the index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3940" target="_blank">01:05:40.920</a></span> | <span class="t">one and two or the dot X and dot Z values were just set to one automatically so we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3947" target="_blank">01:05:47.440</a></span> | <span class="t">actually already used a dim three structure without quite realizing it and then just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3955" target="_blank">01:05:55.080</a></span> | <span class="t">before pass in all of the tensors we want cast casting them to pointers maybe they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3961" target="_blank">01:06:01.640</a></span> | <span class="t">not just casting converting them to pointers through some particular data type and passing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3967" target="_blank">01:06:07.240</a></span> | <span class="t">in any other information that our function will need that kernel will need okay so then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3974" target="_blank">01:06:14.720</a></span> | <span class="t">we call load CUDA again that'll compile this into a module make sure that they're both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3982" target="_blank">01:06:22.520</a></span> | <span class="t">contiguous and on the CUDA device and then after we call module.mapmol passing those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3989" target="_blank">01:06:29.320</a></span> | <span class="t">in putting on the CPU and checking that they're all close and it says yes they are so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=3996" target="_blank">01:06:36.320</a></span> | <span class="t">this is now running not on just the first five rows but on the entire MNIST data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4001" target="_blank">01:06:41.440</a></span> | <span class="t">and on the entire MNIST data set using a optimized CPU approach it took 1.3 seconds using CUDA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4011" target="_blank">01:06:51.840</a></span> | <span class="t">it takes six milliseconds so that is quite a big improvement cool the other thing I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4024" target="_blank">01:07:04.920</a></span> | <span class="t">mention of course is PyTorch can do a matrix modification for us just by using at how long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4031" target="_blank">01:07:11.200</a></span> | <span class="t">does and obviously gives the same answer how long does that take to run that takes two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4037" target="_blank">01:07:17.720</a></span> | <span class="t">milliseconds so three times faster and in many situations it'll be much more than three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4045" target="_blank">01:07:25.940</a></span> | <span class="t">times faster so why are we still pretty slow compared to PyTorch I mean this isn't bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4052" target="_blank">01:07:32.080</a></span> | <span class="t">to do 392 million of these calculations in six milliseconds but if PyTorch can do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4058" target="_blank">01:07:38.480</a></span> | <span class="t">so much faster what are they doing well the trick is that they are taking advantage in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4066" target="_blank">01:07:46.700</a></span> | <span class="t">particular of this shared memory so shared memory is a small memory space that is shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4073" target="_blank">01:07:53.960</a></span> | <span class="t">amongst the threads in a block and it is much faster than global memory in our matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4082" target="_blank">01:08:02.460</a></span> | <span class="t">when we have one of these blocks and so it's going to do one block at a time all in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4087" target="_blank">01:08:07.160</a></span> | <span class="t">same SM it's going to be reusing the same 16 by 16 block it's going to be using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4094" target="_blank">01:08:14.760</a></span> | <span class="t">same 16 rows and columns again and again and again each time with access to the same shared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4100" target="_blank">01:08:20.060</a></span> | <span class="t">memory so you can see how you could really potentially cache the information a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4105" target="_blank">01:08:25.400</a></span> | <span class="t">the information you need and reuse it rather than going back to the slower memory again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4110" target="_blank">01:08:30.680</a></span> | <span class="t">and again so this is an example of the kinds of things that you could optimize potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4117" target="_blank">01:08:37.040</a></span> | <span class="t">once you get to that point. The only other thing that I wanted to mention here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4126" target="_blank">01:08:46.720</a></span> | <span class="t">this 2D block idea is totally optional you can do everything with 1D blocks or with 2D</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4135" target="_blank">01:08:55.000</a></span> | <span class="t">blocks or with 3D blocks and threads and just to show that I've actually got an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4140" target="_blank">01:09:00.320</a></span> | <span class="t">at the end here which converts RGB to grayscale using the 2D blocks because remember earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4151" target="_blank">01:09:11.920</a></span> | <span class="t">when we did this it was with 1D blocks. It gives exactly the same result and if we compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4161" target="_blank">01:09:21.160</a></span> | <span class="t">the code, so if we compare the code the version actually that was done with 1D threads and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4172" target="_blank">01:09:32.660</a></span> | <span class="t">blocks is quite a bit shorter than the version that uses 2D threads and blocks and so in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4178" target="_blank">01:09:38.320</a></span> | <span class="t">this case even as though we're manipulating pixels where you might think that using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4183" target="_blank">01:09:43.440</a></span> | <span class="t">2D approach would be neater and more convenient in this particular case it wasn't really I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4190" target="_blank">01:09:50.160</a></span> | <span class="t">mean it's still pretty simple code that we have to deal with the columns and rows.x.y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4196" target="_blank">01:09:56.560</a></span> | <span class="t">separately, the guards a little bit more complex, we have to find out what index we're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4202" target="_blank">01:10:02.840</a></span> | <span class="t">up to here or else this kernel we just there was just much more direct just two lines of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4209" target="_blank">01:10:09.400</a></span> | <span class="t">code and then calling the kernel you know again it's a little bit more complex with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4213" target="_blank">01:10:13.480</a></span> | <span class="t">the threads per blocks stuff rather than this but the key thing I wanted to point out is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4218" target="_blank">01:10:18.080</a></span> | <span class="t">that these two pieces of code do exactly the same thing so don't feel like if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4225" target="_blank">01:10:25.520</a></span> | <span class="t">want to use a 2D or 3D block thread structure you don't have to. You can just use a 1D one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4232" target="_blank">01:10:32.720</a></span> | <span class="t">the 2D stuff is only there if it's convenient for you to use and you want to use it. Don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4238" target="_blank">01:10:38.720</a></span> | <span class="t">feel like you have to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4244" target="_blank">01:10:44.040</a></span> | <span class="t">So yeah I think that's basically like all the key things that I wanted to show you all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4249" target="_blank">01:10:49.360</a></span> | <span class="t">today. The main thing I hope you take from this is that even for Python programmers for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4256" target="_blank">01:10:56.100</a></span> | <span class="t">data scientists it's not way outside our comfort zone you know we can write these things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4264" target="_blank">01:11:04.600</a></span> | <span class="t">Python we can convert them pretty much automatically we end up with code that doesn't look you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4271" target="_blank">01:11:11.160</a></span> | <span class="t">know it looks reasonably familiar even though it's now in a different language we can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4275" target="_blank">01:11:15.880</a></span> | <span class="t">everything inside notebooks we can test everything as we go we can print things from our kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4283" target="_blank">01:11:23.760</a></span> | <span class="t">and so you know it's hopefully feeling a little bit less beyond our capabilities than we might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4294" target="_blank">01:11:34.500</a></span> | <span class="t">have previously imagined. So I'd say yeah you know go for it I think it's also like I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4301" target="_blank">01:11:41.840</a></span> | <span class="t">it's increasingly important to be able to write CUDA code nowadays because for things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4307" target="_blank">01:11:47.640</a></span> | <span class="t">like flash attention or for things like quantization GPTQ AWQ bits and bytes these are all things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4317" target="_blank">01:11:57.320</a></span> | <span class="t">you can't write in PyTorch. You know our models are getting more sophisticated the kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4323" target="_blank">01:12:03.560</a></span> | <span class="t">assumptions that libraries like PyTorch make about what we want to do you know increasingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4330" target="_blank">01:12:10.880</a></span> | <span class="t">less and less accurate so we're having to do more and more of this stuff ourselves nowadays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4335" target="_blank">01:12:15.320</a></span> | <span class="t">in CUDA and so I think it's a really valuable capability to have. Now the other thing I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4343" target="_blank">01:12:23.640</a></span> | <span class="t">mentioned is we did it all in CoLab today but we can also do things on our own machines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4352" target="_blank">01:12:32.200</a></span> | <span class="t">if you have a GPU or on a cloud machine and getting set up for this again it's much less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4359" target="_blank">01:12:39.440</a></span> | <span class="t">complicated than you might expect and in fact I can show you it's basically like four lines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4365" target="_blank">01:12:45.800</a></span> | <span class="t">of code or four lines or three or four lines of bash script to get it all set up it'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4370" target="_blank">01:12:50.360</a></span> | <span class="t">run on Windows or under WSL it'll also run on Linux of course Mac stuff doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4375" target="_blank">01:12:55.720</a></span> | <span class="t">work on CUDA stuff doesn't really work on Mac so not on Mac. Actually I'll put a link</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4384" target="_blank">01:13:04.480</a></span> | <span class="t">to this into the video notes but for now I'm just going to jump to a Twitter thread where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4391" target="_blank">01:13:11.640</a></span> | <span class="t">I wrote this all down to show you all the steps. So the way to do it is to use something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4399" target="_blank">01:13:19.640</a></span> | <span class="t">called Conda. Conda is something that very very very few people understand a lot of people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4406" target="_blank">01:13:26.080</a></span> | <span class="t">think it's like a replacement for like pip or poetry or something it's not it's better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4411" target="_blank">01:13:31.440</a></span> | <span class="t">to think of it as a replacement for docker. You can literally have multiple different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4415" target="_blank">01:13:35.560</a></span> | <span class="t">versions of Python multiple different versions of CUDA multiple different C++ compilation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4421" target="_blank">01:13:41.600</a></span> | <span class="t">systems all in parallel at the same time on your machine and switch between them you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4428" target="_blank">01:13:48.680</a></span> | <span class="t">only do this with Conda and everything just works right so you don't have to worry about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4435" target="_blank">01:13:55.440</a></span> | <span class="t">all the confusing stuff around .run files or Ubuntu packages or anything like that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4440" target="_blank">01:14:00.920</a></span> | <span class="t">can do everything with just Conda. You need to install Conda I've actually got a script</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4449" target="_blank">01:14:09.000</a></span> | <span class="t">which you just run the script it's a tiny script as you see if you just run the script</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4452" target="_blank">01:14:12.600</a></span> | <span class="t">it'll automatically figure out which many Conda you need it'll automatically figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4457" target="_blank">01:14:17.120</a></span> | <span class="t">out what shell you're on and it'll just go ahead and download it and install it for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4461" target="_blank">01:14:21.200</a></span> | <span class="t">Okay so run that script restart your terminal now you've got Conda. Step two is find out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4470" target="_blank">01:14:30.720</a></span> | <span class="t">what version of CUDA PyTorch wants you to have so if I click Linux Conda CUDA 12.1 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4478" target="_blank">01:14:38.920</a></span> | <span class="t">the latest so then step three is run this shell command replacing 12.1 with whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4490" target="_blank">01:14:50.000</a></span> | <span class="t">the current version of PyTorch is it actually still 12.1 for me at this point and that'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4496" target="_blank">01:14:56.280</a></span> | <span class="t">install everything all the stuff you need to profile debug build etc all the nvidia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4504" target="_blank">01:15:04.600</a></span> | <span class="t">tools you need the full suite will all be installed and it's coming directly from nvidia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4508" target="_blank">01:15:08.720</a></span> | <span class="t">so you'll have like the proper versions as I said you can have multiple versions it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4513" target="_blank">01:15:13.480</a></span> | <span class="t">stored at once in different environments no problem at all and then finally install PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4521" target="_blank">01:15:21.300</a></span> | <span class="t">and this command here will install PyTorch for some reason I wrote nightly here you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4525" target="_blank">01:15:25.600</a></span> | <span class="t">need the nightly so just remove just nightly so this will install the latest version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4529" target="_blank">01:15:29.520</a></span> | <span class="t">PyTorch using the nvidia CUDA stuff that you just installed if you've used Conda before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4536" target="_blank">01:15:36.240</a></span> | <span class="t">and it was really slow that's because it used to use a different solver which was thousands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4542" target="_blank">01:15:42.160</a></span> | <span class="t">or tens of thousands of times slower than the modern one just has been added and made</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4546" target="_blank">01:15:46.880</a></span> | <span class="t">default in the last couple of months so nowadays this should all run very fast and as I said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4553" target="_blank">01:15:53.960</a></span> | <span class="t">it'll run under WSL on Windows it'll run on Ubuntu it'll run on Fedora it'll run on Debian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4560" target="_blank">01:16:00.720</a></span> | <span class="t">it'll all just work so that's how I strongly recommend getting yourself set up for local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4571" target="_blank">01:16:11.760</a></span> | <span class="t">development you don't need to worry about using Docker as I said you can switch between different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4577" target="_blank">01:16:17.800</a></span> | <span class="t">CUDA versions different Python versions different compilers and so forth without having to worry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4582" target="_blank">01:16:22.320</a></span> | <span class="t">about any of the Docker stuff and it's also efficient enough that if you've got the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4587" target="_blank">01:16:27.400</a></span> | <span class="t">libraries and so forth installed in multiple environments it'll hard link them so it won't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4592" target="_blank">01:16:32.760</a></span> | <span class="t">even use additional hard drive space so it's also very efficient great so that's how you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4600" target="_blank">01:16:40.880</a></span> | <span class="t">can get started on your own machine or on the cloud or whatever so hopefully you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4605" target="_blank">01:16:45.800</a></span> | <span class="t">find that helpful as well alright thanks very much for watching I hope you found this useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4614" target="_blank">01:16:54.040</a></span> | <span class="t">and I look forward to hearing about what you create with CUDA in terms of going to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4621" target="_blank">01:17:01.960</a></span> | <span class="t">next steps check out the other CUDA mode lectures I will link to them and I would also recommend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4629" target="_blank">01:17:09.760</a></span> | <span class="t">trying out some projects of your own so for example you could try to implement something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4637" target="_blank">01:17:17.760</a></span> | <span class="t">like 4-bit quantization or flash attention or anything like that now those are kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4646" target="_blank">01:17:26.000</a></span> | <span class="t">pretty big projects but you can try to break them up into smaller things you build up one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4650" target="_blank">01:17:30.200</a></span> | <span class="t">step at a time and of course look at other people's code so look at the implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4657" target="_blank">01:17:37.440</a></span> | <span class="t">of flash attention look at the implementation of bits and bytes look at the implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4661" target="_blank">01:17:41.880</a></span> | <span class="t">of GPTQ and so forth the more time you spend reading other people's code the better alright</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4670" target="_blank">01:17:50.800</a></span> | <span class="t">I hope you found this useful and thank you very much for watching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=nOxKexn3iBo&t=4674" target="_blank">01:17:54.440</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>