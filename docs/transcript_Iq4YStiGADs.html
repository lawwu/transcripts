<html><head><title>How Did Dario & Ilya Know LLMs Could Lead to AGI?</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>How Did Dario & Ilya Know LLMs Could Lead to AGI?</h2><a href="https://www.youtube.com/watch?v=Iq4YStiGADs" target="_blank"><img src="https://i.ytimg.com/vi_webp/Iq4YStiGADs/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>just before OpenAI started, I met Ilya, who you, who you interviewed. One of the first things he said to me was, look, the models, they just want to learn, you have to understand this, the models, they just want to learn. And it was a bit like a Zen Cohen, like I kind of like, I listened to this, and I became enlightened.</p><p>The models just want to learn, you get the obstacles out of their way, right? You give them, you give them good data, you, you give them enough space to operate in, you don't do something stupid, like condition them badly numerically, and they want to learn, they'll do it, they'll do it.</p><p>There are many people who were aware back at that time, probably weren't working on it directly. But we're aware that these things are really good at speech recognition, or at playing these constrained games. Very few extrapolated from there, like you and Ilya did to something that is generally intelligent.</p><p>What was different about the way you were thinking about it versus how others think that you went from like, is getting better at speech in this consistent way, it will get better at everything in this consistent way? Yeah, so I genuinely don't know. I mean, at first, when I saw it for speech, I assumed this was just true for speech or for this narrow class of models.</p><p>I think it was just over the period between 2014 and 2017. I tried it for a lot of things and saw the same thing over and over again. I watched the same being true with Dota. I watched the same being true with robotics, which many people thought of as a counter example.</p><p>But I just thought, well, it's hard to get data for robotics. But if we operate within if we look within the data that we have, we see the same patterns. And so I don't I don't know, I think people were very focused on solving the problem in front of them.</p><p>Why one person thinks one way and other person is very, it's very hard to explain. I think people just see it through a different lens, you know, are looking like vertically instead of horizontally, they're not thinking about the scaling, they're thinking about how do I solve my problem? And well, for robotics, there's not enough data.</p><p>And so, you know, and so you know, that can easily abstractable scaling doesn't work, because we don't have the data. And so I don't, I don't know, I just, for some reason, and it may just, it may just have been random chance was obsessed with that particular direction, this big blob of compute document, which I still have not made public, I probably should for like, historical reasons, I don't think it would tell anyone anything they don't know now.</p><p>But when I wrote it, I actually said, Look, there are seven factors that and you know, I wasn't, I wasn't like, these are the factors, but I was just like, let me give some sense of the kinds of things that matter and what don't. And so number of parameters, scale of the model, like, you know, the compute and compute matters, quantity of data matters, quality of data matters, loss function matters.</p><p>So like, you know, are you doing RL or doing next word prediction, if your loss function isn't rich, or doesn't incentivize the right thing, you will, you won't get anything. So those were the key four ones. Which I think are the core of the hypothesis. But then I said three more things.</p><p>One was symmetries, which is basically like, if your architecture doesn't take into account the right kinds of symmetries, it doesn't work. Or it's it's very inefficient. So for example, convolutional neural networks take into account translational symmetry, LSTM is taking into account time symmetry. And but a weakness of LSTM is that they can't attend over the whole context.</p><p>So there's kind of this structural weakness, like if a model isn't structurally capable of like, absorbing and managing things that happened in a far enough distant past, and it's just like, it's kind of like, you know, like, the compute doesn't flow, like the spice doesn't flow. It's like, you can't like, like, the blob has to be unencumbered, right?</p><p>It kind of, it's not, it's not going to work if, if you artificially close things off. And I think RNNs and LSTMs artificially close things off, because they close you off to the distant past. And so again, things need to flow freely. If they don't, it doesn't work. If you set things up in kind of a way that's, that's set up to fail, or that doesn't allow the compute to work in an uninhibited way, then it won't work.</p><p>And so transformers were kind of within that, even though I can't remember if the transformer paper had been published, it was around the same time as I wrote that document, it might have been just before it might have been just after. It sounds like from that view, the way to think about these algorithmic progresses is not as increasing the power of the blob of compute, but simply getting rid of the artificial hindrances that older architectures have.</p><p>Is that, is that a fair way to- That's a little, that, yeah, that's, that's a little how I think about it. You know, again, if you go back to like, Ilyas, like the models want to learn, like, like the compute wants to be free. And like, you know, it's being blocked in various ways where you like, don't understand that it's being blocked.</p><p>And so you need to like, free it up. Right, right. I love the, the gradients, change that to spice. Okay. When did it become obvious to you that language is the means to just feed a bunch of data into these things that, or was it just, you ran out of other things, like robotics, there's not enough data, this other thing, there's not enough data.</p><p>Yeah. I mean, I think this whole idea of like the next word prediction that you could do self-supervised learning, you know, that together with the idea that it's like, wow, for predicting the next word, there's so much richness and structure there. Right. You know, it might say two plus two equals, and you have to know the answer is four.</p><p>And you know, it might be telling the story about a character. And then basically, it's posing to the model, you know, the equivalent of these developmental tests that get posed to children, you know, Mary walks into the room and, you know, puts an item in there. And then, you know, Chuck walks into the room and removes the item and Mary doesn't see it.</p><p>What does Mary think, you know, so like, so the models are going to have to get this right in the service of predicting the next word, they're going to have to solve, you know, solve all these theory of mind problems, solve all these math problems. And so I did, you know, I, my thinking was just, well, you know, scale it up as much as you can, you, you know, there's, there's kind of no limit to it.</p><p>And I think I kind of had abstractly that view. But the thing, of course, that like really solidified and convinced me was the work that Alec Radford did on GPT-1, which was not only could you get this, this language model that could predict things very well, but also you could fine tune it, you needed to fine tune it in those days to do all these other tasks.</p><p>And so I was like, wow, you know, it, this isn't just some narrow thing where you get the language model, right. It's sort of halfway to everywhere, right? It's like, you know, you get the language model, right. And then with a little move in this direction, it can, you know, it can solve this, this, you know, logical dereference test or whatever.</p><p>And, you know, with this, this other thing, you know, it can, it can solve translation or something. And then you're like, wow, I think there's, there's really something to do it. And of and of course we can really scale it.</p></div></div></body></html>