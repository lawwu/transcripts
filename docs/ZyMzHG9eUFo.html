<html><head><title>How Far Can We Scale AI? Gen 3, Claude 3.5 Sonnet and AI Hype</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>How Far Can We Scale AI? Gen 3, Claude 3.5 Sonnet and AI Hype</h2><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo"><img src="https://i.ytimg.com/vi/ZyMzHG9eUFo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ZyMzHG9eUFo.html">Whisper Transcript</a> | <a href="./transcript_ZyMzHG9eUFo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Artificial worlds generated by AI video models have never been more tangible and accessible and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=8" target="_blank">00:00:08.480</a></span> | <span class="t">look set to transform how millions and then billions of people consume content. And artificial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=14" target="_blank">00:00:14.000</a></span> | <span class="t">intelligence in the form of the new free Claude 3.5 SONNET is more capable than it has ever been.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=20" target="_blank">00:00:20.960</a></span> | <span class="t">But I will draw on interviews in the last few days to show that there are more questions than ever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=26" target="_blank">00:00:26.400</a></span> | <span class="t">not just about the merits of continued scaling of language models but about whether we can rely on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=32" target="_blank">00:00:32.800</a></span> | <span class="t">the words of those who lead these giant AI orgs. But first AI video generation which is truly on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=40" target="_blank">00:00:40.720</a></span> | <span class="t">fire at the moment. These outputs are from Runway Gen 3 available to many now and to everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=47" target="_blank">00:00:47.600</a></span> | <span class="t">apparently in the coming days. The audio by the way is also AI generated this time from UDIO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=55" target="_blank">00:00:55.360</a></span> | <span class="t">[Music]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=80" target="_blank">00:01:20.240</a></span> | <span class="t">And as you watch these videos remember that the AI models that are generating them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=84" target="_blank">00:01:24.720</a></span> | <span class="t">are likely trained on far less than 1% of the video data that's available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=90" target="_blank">00:01:30.400</a></span> | <span class="t">Unlike high quality text data video data isn't even close to being used up. Expect generations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=96" target="_blank">00:01:36.560</a></span> | <span class="t">to get far more realistic and not in too long either. And by the way if you're bored while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=101" target="_blank">00:01:41.920</a></span> | <span class="t">waiting on the Gen 3 wait list do play about with the Luma Dream Machine. I've got to admit it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=108" target="_blank">00:01:48.640</a></span> | <span class="t">pretty fun to generate two images or submit two real ones and have the model interpolate between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=115" target="_blank">00:01:55.040</a></span> | <span class="t">them. Now those of you in China have actually already been able to play with a model of similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=120" target="_blank">00:02:00.800</a></span> | <span class="t">capabilities called Kling. But we are all waiting on the release of Sora the most promising video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=128" target="_blank">00:02:08.400</a></span> | <span class="t">generation model of them all from OpenAI. Here are a couple of comparisons between Runway Gen 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=135" target="_blank">00:02:15.120</a></span> | <span class="t">and Sora. The prompts used in both cases are identical and there's one example that particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=141" target="_blank">00:02:21.440</a></span> | <span class="t">caught my eye. As many of us may have realized by now simply training models on more data doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=147" target="_blank">00:02:27.120</a></span> | <span class="t">necessarily mean they pick up accurate world models. Now I strongly suspect that Sora was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=153" target="_blank">00:02:33.120</a></span> | <span class="t">trained on way more data with way more compute. With its generation at the bottom you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=158" target="_blank">00:02:38.960</a></span> | <span class="t">that the dust emerges from behind the car. This neatly demonstrates the benefits of scale but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=164" target="_blank">00:02:44.960</a></span> | <span class="t">still leaves open the question about whether scale will solve all. Now yes it would be simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=171" target="_blank">00:02:51.280</a></span> | <span class="t">to extrapolate a straight line upwards and say that with enough scale we get a perfect world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=177" target="_blank">00:02:57.040</a></span> | <span class="t">simulation but I just don't think it will be like that. And there are already more than tentative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=182" target="_blank">00:03:02.000</a></span> | <span class="t">hints that scale won't solve everything. More on that in just a moment but there is one more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=187" target="_blank">00:03:07.440</a></span> | <span class="t">modality I am sure we were all looking forward to which is going to be delayed. That's the real-time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=193" target="_blank">00:03:13.840</a></span> | <span class="t">advanced voice mode from OpenAI. It was the star of the demo of GPC 4.0 and was promised in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=201" target="_blank">00:03:21.040</a></span> | <span class="t">coming weeks. Alas though it has now been delayed to the fall or the autumn and they say that's in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=207" target="_blank">00:03:27.840</a></span> | <span class="t">part because they want to improve the model's ability to detect and refuse certain content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=212" target="_blank">00:03:32.960</a></span> | <span class="t">I also suspect though like dodgy physics with the video generation and hallucinations with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=218" target="_blank">00:03:38.240</a></span> | <span class="t">language generation they also realized it occasionally goes off the rails. Now I personally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=224" target="_blank">00:03:44.480</a></span> | <span class="t">find this funny but you let me know whether this would be acceptable to release. "Refreshing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=229" target="_blank">00:03:49.680</a></span> | <span class="t">coolness in the air that just makes you want to smile and take a deep breath of that crisp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=235" target="_blank">00:03:55.040</a></span> | <span class="t">invigorating breeze. The sun's shining but it's got this lovely gentle warmth that's just perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=242" target="_blank">00:04:02.880</a></span> | <span class="t">for a light jacket." So either way we're definitely gonna have epic entertainment but the question is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=248" target="_blank">00:04:08.320</a></span> | <span class="t">what's next? Particularly when it comes to the underlying intelligence of models is it a case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=253" target="_blank">00:04:13.360</a></span> | <span class="t">of shooting past human level or diminishing returns? Well here's some anecdotal evidence with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=259" target="_blank">00:04:19.520</a></span> | <span class="t">the recent release of Claude 3.5 Sonnet from Anthropic. It's free and fast and in certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=266" target="_blank">00:04:26.240</a></span> | <span class="t">domains more capable than comparable language models. This table I would say shows you a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=271" target="_blank">00:04:31.200</a></span> | <span class="t">comparison on things like basic mathematical ability and general knowledge compared to models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=276" target="_blank">00:04:36.320</a></span> | <span class="t">like GPT-40 and Gemini 1.5 Pro from Google. I would caution that many of these benchmarks have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=282" target="_blank">00:04:42.640</a></span> | <span class="t">significant flaws so decimal point differences I wouldn't pay too much attention to. The most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=288" target="_blank">00:04:48.000</a></span> | <span class="t">interesting comparison I would argue is between Claude 3.5 Sonnet and Claude 3 Sonnet. There is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=293" target="_blank">00:04:53.840</a></span> | <span class="t">some evidence that Claude 3.5 Sonnet was trained on about four times as much compute as Claude 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=299" target="_blank">00:04:59.520</a></span> | <span class="t">Sonnet and you can see the difference that makes. Definitely a boost across the board but it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=304" target="_blank">00:05:04.640</a></span> | <span class="t">be hard to argue that it's four times better and in the visual domain it is noticeably better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=311" target="_blank">00:05:11.360</a></span> | <span class="t">its predecessor and than many other models and I got early access so I tested it a fair bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=316" target="_blank">00:05:16.720</a></span> | <span class="t">These kind of benchmarks test reading charts and diagrams and answering basic questions about them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=322" target="_blank">00:05:22.240</a></span> | <span class="t">but the real question is how much extra compute and therefore money can these companies continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=327" target="_blank">00:05:27.680</a></span> | <span class="t">to scale up and invest if the returns are still incremental? In other words how much more will you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=334" target="_blank">00:05:34.720</a></span> | <span class="t">and more importantly businesses continue to pay for these incremental benefits? After all in no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=340" target="_blank">00:05:40.720</a></span> | <span class="t">domains are these models reaching a hundred percent and let me try to illustrate that with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=345" target="_blank">00:05:45.760</a></span> | <span class="t">an example and as we follow this example ask yourself whether you would pay four times as much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=350" target="_blank">00:05:50.240</a></span> | <span class="t">for a five percent hallucination rate versus an eight percent hallucination rate if in both cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=356" target="_blank">00:05:56.240</a></span> | <span class="t">you have to check the answer anyway. Let me demonstrate with the brilliant new feature you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=360" target="_blank">00:06:00.240</a></span> | <span class="t">can use with Claude 3.5 Sonnet from Anthropic. It's called Artifacts. Think of it like an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=365" target="_blank">00:06:05.600</a></span> | <span class="t">interactive project that you can work on alongside the language model. I dumped a multi-hundred page</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=371" target="_blank">00:06:11.520</a></span> | <span class="t">document on the model and asked the following question. Find three questions on functions from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=376" target="_blank">00:06:16.560</a></span> | <span class="t">this document and turn them into clickable flashcards in an artifact with full answers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=381" target="_blank">00:06:21.440</a></span> | <span class="t">and explanations revealed interactively. It did it and that is amazing but there's one slight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=387" target="_blank">00:06:27.600</a></span> | <span class="t">problem. Question one is perfect. It's a real question from the document displayed perfectly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=393" target="_blank">00:06:33.120</a></span> | <span class="t">and interactive with the correct answer and explanation. Same thing for question two but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=398" target="_blank">00:06:38.400</a></span> | <span class="t">then we get to question three where it copied the question incorrectly. Worse than that it rejigged</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=404" target="_blank">00:06:44.160</a></span> | <span class="t">and changed the answer options. Also is there a real difference between q squared and negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=410" target="_blank">00:06:50.160</a></span> | <span class="t">q squared when it claimed that negative q squared is the answer? Now you might find this example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=415" target="_blank">00:06:55.760</a></span> | <span class="t">trivial but I think it's revealing. Don't get me wrong this feature is immensely useful and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=420" target="_blank">00:07:00.640</a></span> | <span class="t">wouldn't take me long to simply tweak that third question and by the way finding those three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=425" target="_blank">00:07:05.520</a></span> | <span class="t">examples strewn across a multi-hundred page document is impressive. Even though it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=430" target="_blank">00:07:10.640</a></span> | <span class="t">save me some time I would still have to diligently check every character of Claude's answer and at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=436" target="_blank">00:07:16.800</a></span> | <span class="t">the moment as I discussed in more detail in my previous video there is no indication that scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=442" target="_blank">00:07:22.720</a></span> | <span class="t">will solve this issue. Now if you think I'm just quibbling and benchmarks show the real progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=448" target="_blank">00:07:28.240</a></span> | <span class="t">well here is the reasoning lead at Google DeepMind working on their Gemini series of models. Someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=454" target="_blank">00:07:34.640</a></span> | <span class="t">pointed out a classic reasoning error made by Claude 3.5 Sonnet and Denny Zhou said this "Love</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=460" target="_blank">00:07:40.880</a></span> | <span class="t">seeing tweets like this rather than those on LLMs with PhD/superhuman intelligence or fancy results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=468" target="_blank">00:07:48.400</a></span> | <span class="t">on leaked benchmarks." I'm definitely not the only one skeptical of benchmark results and an even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=474" target="_blank">00:07:54.320</a></span> | <span class="t">more revealing response to Claude 3.5's basic errors came from OpenAI's Noam Brown. I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=480" target="_blank">00:08:00.320</a></span> | <span class="t">it's more revealing because it shows that those AI labs Anthropic and OpenAI had their hopes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=485" target="_blank">00:08:05.840</a></span> | <span class="t">slightly dashed based on the results they expected in reasoning from multimodal training. Noam Brown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=492" target="_blank">00:08:12.080</a></span> | <span class="t">said frontier models like GPT-40 and now Claude 3.5 Sonnet may be at the level of a "smart high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=499" target="_blank">00:08:19.280</a></span> | <span class="t">schooler" mimicking the words of Mira Murati CTO of OpenAI in some respects but they still struggle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=505" target="_blank">00:08:25.520</a></span> | <span class="t">on basic tasks like tic-tac-toe. And here's the key quote "There was hope that native multimodal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=512" target="_blank">00:08:32.480</a></span> | <span class="t">training would help with this kind of reasoning but that hasn't been the case." That last sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=518" target="_blank">00:08:38.560</a></span> | <span class="t">is somewhat devastating to the naive scaling hypothesis. "There was hope that native</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=524" target="_blank">00:08:44.240</a></span> | <span class="t">multimodal training on things like video from YouTube would teach models a world model. It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=529" target="_blank">00:08:49.600</a></span> | <span class="t">would help but that hasn't been the case." Now of course these companies are working on far more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=534" target="_blank">00:08:54.480</a></span> | <span class="t">than just naive scaling as we'll hear in a moment from Bill Gates but it's not like you can look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=538" target="_blank">00:08:58.800</a></span> | <span class="t">the benchmark results on a chart and just extrapolate forwards. Here's Bill Gates promising</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=544" target="_blank">00:09:04.000</a></span> | <span class="t">two more turns of scaling, I think he means two more orders of magnitude,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=547" target="_blank">00:09:07.840</a></span> | <span class="t">but notice how he looks sceptical about how that will be enough. "The big frontier is not so much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=553" target="_blank">00:09:13.840</a></span> | <span class="t">scaling. We have probably two more turns of the crank on scaling whereby accessing video data and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=563" target="_blank">00:09:23.440</a></span> | <span class="t">getting very good at synthetic data that we can scale up probably you know two more times. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=572" target="_blank">00:09:32.160</a></span> | <span class="t">not the most interesting dimension. The most interesting dimension is what I call metacognition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=577" target="_blank">00:09:37.920</a></span> | <span class="t">where understanding how to think about a problem in a broad sense and step back and say okay how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=585" target="_blank">00:09:45.760</a></span> | <span class="t">important is this answer, how could I check my answer, you know what external tools would help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=590" target="_blank">00:09:50.800</a></span> | <span class="t">me with this? So we're going to get the scaling benefits but at the same time the various actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=600" target="_blank">00:10:00.400</a></span> | <span class="t">to change the underlying reasoning algorithm from the trivial that we have today to more human-like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=610" target="_blank">00:10:10.320</a></span> | <span class="t">metacognition, that's the big frontier. It's a little hard to predict how quickly that'll happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=618" target="_blank">00:10:18.320</a></span> | <span class="t">You know I've seen that we will make progress on that next year but we won't completely solve it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=624" target="_blank">00:10:24.160</a></span> | <span class="t">for some time after that." And there were others who used to be incredibly bullish on scaling that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=630" target="_blank">00:10:30.800</a></span> | <span class="t">now sound a little different. Here's Microsoft AI CEO Mustafa Suleiman perhaps drawing on lessons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">from the mostly defunct inflection AI that he used to run saying it won't be until GPT-6 that AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=643" target="_blank">00:10:43.040</a></span> | <span class="t">models will be able to follow instructions and take consistent action. "There's a lot of cherry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=647" target="_blank">00:10:47.120</a></span> | <span class="t">picked examples that are impressive you know on Twitter and stuff like that but to really get it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=652" target="_blank">00:10:52.640</a></span> | <span class="t">to consistently do it in novel environments is pretty hard and I think that it's going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=658" target="_blank">00:10:58.640</a></span> | <span class="t">not one but two orders of magnitude more computation of training the models so not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=664" target="_blank">00:11:04.160</a></span> | <span class="t">GPT-5 but more like GPT-6 scale models. So I think we're talking about two years before we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=670" target="_blank">00:11:10.720</a></span> | <span class="t">systems that can really take action." Now based on the evidence that I put forward in my previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=675" target="_blank">00:11:15.920</a></span> | <span class="t">video let me know if you agree with me that I still think that's kind of naive. Reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=680" target="_blank">00:11:20.880</a></span> | <span class="t">breakthroughs will rely on new research breakthroughs not just more scale. And even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=685" target="_blank">00:11:25.920</a></span> | <span class="t">Sam Altman said as much about a year ago saying the era of ever more scaling of parameter count</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=692" target="_blank">00:11:32.240</a></span> | <span class="t">is over. Now as we'll hear he has since contradicted that saying current models are small relative to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=698" target="_blank">00:11:38.160</a></span> | <span class="t">where they'll be. But at this point you might be wondering about emergent behaviors. Don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=702" target="_blank">00:11:42.480</a></span> | <span class="t">certain capabilities just spring out when you reach a certain scale? Well I simply can't resist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=707" target="_blank">00:11:47.520</a></span> | <span class="t">a quick plug for my new Coursera series that is out this week. The second module covers emergent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=713" target="_blank">00:11:53.360</a></span> | <span class="t">behaviors and if you already have a Coursera account do please check it out it'd be free for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=718" target="_blank">00:11:58.160</a></span> | <span class="t">you and if you were thinking of getting one there'll be a link in the description. Anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=722" target="_blank">00:12:02.560</a></span> | <span class="t">here's that quote from Sam Altman somewhat contradicting the comments he made a year ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=727" target="_blank">00:12:07.040</a></span> | <span class="t">Models he says get predictably better with scale. "We're still just like so early in developing such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=733" target="_blank">00:12:13.440</a></span> | <span class="t">a complex system. There's data issues, there's algorithmic issues, the models are still quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=740" target="_blank">00:12:20.880</a></span> | <span class="t">small relative to what they will be someday and we know they get predictably better."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=744" target="_blank">00:12:24.080</a></span> | <span class="t">But this was the point I was trying to make at the start of the video. As I argued in my previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=749" target="_blank">00:12:29.920</a></span> | <span class="t">video I think we're now at a time in AI where we really have to work hard to separate the hype</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=756" target="_blank">00:12:36.240</a></span> | <span class="t">from the reality. Simply trusting the words of the leaders of these AI labs is less advisable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=762" target="_blank">00:12:42.800</a></span> | <span class="t">than ever and of course it's not just Sam Altman. Here's the commitment from Anthropic led by Dario</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=768" target="_blank">00:12:48.640</a></span> | <span class="t">Amadei back last year. They described why they don't publish their research and they said it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=772" target="_blank">00:12:52.960</a></span> | <span class="t">because "we do not wish to advance the rate of AI capabilities progress" but their CEO just three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=779" target="_blank">00:12:59.440</a></span> | <span class="t">days ago said AI is progressing fast due in part to their own efforts. "To try and keep pace with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=786" target="_blank">00:13:06.800</a></span> | <span class="t">the rate at which the complexity of the models is increasing. I think this is one of the biggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=790" target="_blank">00:13:10.800</a></span> | <span class="t">challenges in the field. The field is moving so fast, including by our own efforts, that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=796" target="_blank">00:13:16.160</a></span> | <span class="t">to make sure that our understanding keeps pace with our abilities, our capabilities to produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=801" target="_blank">00:13:21.840</a></span> | <span class="t">powerful models." He then went on to say that today's models are like undergraduates, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=807" target="_blank">00:13:27.040</a></span> | <span class="t">if you've interacted with these models seems pretty harsh on undergraduates. "If we go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=812" target="_blank">00:13:32.800</a></span> | <span class="t">to the analogy of like today's models are like undergraduates, you know, let's say those models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=817" target="_blank">00:13:37.680</a></span> | <span class="t">get to the point where, you know, they're kind of, you know, graduate level or strong professional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=822" target="_blank">00:13:42.720</a></span> | <span class="t">level. Think of biology and drug discovery. Think of a model that is as strong as, you know, a Nobel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=831" target="_blank">00:13:51.440</a></span> | <span class="t">Prize winning scientist or, you know, the head of the, you know, the head of drug discovery at a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=836" target="_blank">00:13:56.080</a></span> | <span class="t">major pharmaceutical company." Now, I don't know if he's basing that on a naive trust in benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=842" target="_blank">00:14:02.240</a></span> | <span class="t">or whether he is deliberately hyping. And then later in the conversation with the guy who's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=847" target="_blank">00:14:07.440</a></span> | <span class="t">in charge of the world's largest sovereign wealth fund, he described how the kind of AI that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=852" target="_blank">00:14:12.400</a></span> | <span class="t">Anthropic works on could be instrumental in curing cancer. "I look at all the things that have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=857" target="_blank">00:14:17.920</a></span> | <span class="t">invented. You know, if I look back at biology, you know, CRISPR, the ability to like edit genes. If</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=863" target="_blank">00:14:23.280</a></span> | <span class="t">I look at, you know, CAR-T therapies, which have cured certain kinds of cancers, there's probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=870" target="_blank">00:14:30.800</a></span> | <span class="t">dozens of discoveries like that lying around. And if we had a million copies of an AI system that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=878" target="_blank">00:14:38.400</a></span> | <span class="t">are as knowledgeable and as creative about the field as all those scientists that invented those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=883" target="_blank">00:14:43.680</a></span> | <span class="t">things, then I think the rate of those discoveries could really proliferate. And, you know, some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=889" target="_blank">00:14:49.280</a></span> | <span class="t">our really, really longstanding diseases, you know, could be addressed or even cured." Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=895" target="_blank">00:14:55.760</a></span> | <span class="t">he added some caveats, of course, but that was a claim echoed on the same day, actually, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=901" target="_blank">00:15:01.120</a></span> | <span class="t">by OpenAI's Sam Altman. "One of our partners, Color Health, is now using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=905" target="_blank">00:15:05.360</a></span> | <span class="t">GPT-4 for cancer screening and treatment plans. And that's great. And then maybe a future version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=911" target="_blank">00:15:11.280</a></span> | <span class="t">will help discover cures for cancer." Other AI lab leaders like Mark Zuckerberg think those claims</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=918" target="_blank">00:15:18.000</a></span> | <span class="t">are getting out of hand. "But, you know, part of that is the open source thing too. So that way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=922" target="_blank">00:15:22.000</a></span> | <span class="t">other companies out there can create different things and people can just hack on it themselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=925" target="_blank">00:15:25.440</a></span> | <span class="t">and mess around with it. So I guess that's a pretty deep worldview that I have. And I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=931" target="_blank">00:15:31.120</a></span> | <span class="t">know, I find it a pretty big turnoff when people in the tech industry kind of talk about building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=937" target="_blank">00:15:37.200</a></span> | <span class="t">this one true AI. It's like, it's almost as if they kind of think they're creating God or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=942" target="_blank">00:15:42.640</a></span> | <span class="t">And it's like, it's just, that's not what we're doing. I don't think that's how this plays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=947" target="_blank">00:15:47.920</a></span> | <span class="t">out." Implicitly, he's saying that companies like OpenAI and Anthropic are getting carried away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=953" target="_blank">00:15:53.920</a></span> | <span class="t">And later though, in that interview, the CEO of Anthropic admitted that he was somewhat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=958" target="_blank">00:15:58.800</a></span> | <span class="t">pulling things out of his hat when it came to biology and actually with scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=964" target="_blank">00:16:04.080</a></span> | <span class="t">"You know, let's say, you know, you extend people's productive ability to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=968" target="_blank">00:16:08.080</a></span> | <span class="t">by 10 years, right? That could be, you know, one sixth of the whole economy."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=971" target="_blank">00:16:11.760</a></span> | <span class="t">"Do you think that's a realistic target?" "I mean, again, like I know some biology,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=977" target="_blank">00:16:17.680</a></span> | <span class="t">I know something about how the AMLs are going to happen. I wouldn't be able to tell you exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=982" target="_blank">00:16:22.240</a></span> | <span class="t">what would happen, but like, I can tell a story where it's possible."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=986" target="_blank">00:16:26.000</a></span> | <span class="t">"So 15%, and when will we, so when could we have added the equivalent of 10 years to our life? I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=992" target="_blank">00:16:32.640</a></span> | <span class="t">mean, how long, what's the timeframe?" "Again, like, you know, this involves so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=996" target="_blank">00:16:36.560</a></span> | <span class="t">many unknowns, right? If I try and give an exact number, it's just going to sound like hype. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1001" target="_blank">00:16:41.200</a></span> | <span class="t">like, a thing I could, a thing I could imagine is like, I don't know, like two to three years from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1006" target="_blank">00:16:46.720</a></span> | <span class="t">now, we have AI systems that are like capable of making that kind of discovery. Five years from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1012" target="_blank">00:16:52.560</a></span> | <span class="t">now, those, those discoveries are actually being made. And five years after that, it's all gone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1017" target="_blank">00:16:57.360</a></span> | <span class="t">through the regulatory apparatus and, and really has. So, you know, we're talking about more,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1021" target="_blank">00:17:01.760</a></span> | <span class="t">we're talking about, you know, a little over a decade, but really I'm just pulling things out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1025" target="_blank">00:17:05.840</a></span> | <span class="t">of my hat here. Like, I don't know that much about drug discovery. I don't know that much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1029" target="_blank">00:17:09.440</a></span> | <span class="t">about biology. And frankly, although I invented AI scaling, I don't know that much about that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1035" target="_blank">00:17:15.040</a></span> | <span class="t">either. I can't predict it." The truth, of course, is that we simply don't know what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1040" target="_blank">00:17:20.160</a></span> | <span class="t">ramifications will be of further scaling and of course, of new research. Regardless, these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1045" target="_blank">00:17:25.600</a></span> | <span class="t">companies are pressing ahead. "Right now, a hundred million. There are models in training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1050" target="_blank">00:17:30.800</a></span> | <span class="t">today that are more like a billion. I think if we go to 10 or a hundred billion, and I think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1056" target="_blank">00:17:36.000</a></span> | <span class="t">will happen in 2025, 2026, maybe 2027, and the algorithmic improvements continue apace and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1064" target="_blank">00:17:44.000</a></span> | <span class="t">chip improvements continue apace, then I think there, there is in my mind a good chance that by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1069" target="_blank">00:17:49.680</a></span> | <span class="t">that time we'll be able to get models that are better than most humans at most things." But I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1075" target="_blank">00:17:55.440</a></span> | <span class="t">want to know what you think. Are we at the dawn of a new era in entertainment and intelligence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1081" target="_blank">00:18:01.680</a></span> | <span class="t">or has the hype gone too far? If you want to hear more of my reflections, do check out my podcasts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1087" target="_blank">00:18:07.280</a></span> | <span class="t">on Patreon on AI Insiders. You could also check out the dozens of bonus videos I've got on there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1093" target="_blank">00:18:13.360</a></span> | <span class="t">and the live meetups arranged via Discord. But regardless, I just want to thank you for getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZyMzHG9eUFo&t=1099" target="_blank">00:18:19.040</a></span> | <span class="t">all the way to the end and joining me in these wild times. Have a wonderful day.</span></div></div></body></html>