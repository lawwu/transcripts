<html><head><title>Stanford XCS224U: NLU I Fantastic Language Models and How to Build Them, Part 1 I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Fantastic Language Models and How to Build Them, Part 1 I Spring 2023</h2><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M"><img src="https://i.ytimg.com/vi/4-kuJpVrr7M/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=57">0:57</a> Addressing the known limitations with BERT<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=120">2:0</a> Core model structure (Clark et al. 2019)<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=349">5:49</a> Generator/Discriminator relationships<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=530">8:50</a> ELECTRA efficiency analyses<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=742">12:22</a> ELECTRA model releases<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=894">14:54</a> From the RNN era<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=945">15:45</a> Transformer-based options<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1269">21:9</a> Trends in model size<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1321">22:1</a> Distillation objectives<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1464">24:24</a> Distillation performance<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1696">28:16</a> Pretraining data<br><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1715">28:35</a> Current trends<br><br><div style="text-align: left;"><a href="./4-kuJpVrr7M.html">Whisper Transcript</a> | <a href="./transcript_4-kuJpVrr7M.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=6" target="_blank">00:00:06.080</a></span> | <span class="t">Welcome everyone. Welcome back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=9" target="_blank">00:00:09.480</a></span> | <span class="t">Let's get started. We have another action-packed day for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=13" target="_blank">00:00:13.520</a></span> | <span class="t">Time's a wasting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=15" target="_blank">00:00:15.280</a></span> | <span class="t">To start here, I'm gonna finish up our big,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=19" target="_blank">00:00:19.120</a></span> | <span class="t">uh, slide deck on contextual word representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=22" target="_blank">00:00:22.020</a></span> | <span class="t">There are just a few more small things to cover.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=25" target="_blank">00:00:25.320</a></span> | <span class="t">And then Sid is gonna get- get us- help us get hands</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=28" target="_blank">00:00:28.680</a></span> | <span class="t">on with training really big models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=32" target="_blank">00:00:32.360</a></span> | <span class="t">So there's the link as usual, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=36" target="_blank">00:00:36.080</a></span> | <span class="t">from the website if you wanna follow along and we're gonna</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=38" target="_blank">00:00:38.400</a></span> | <span class="t">skip right to this section called Electra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=41" target="_blank">00:00:41.360</a></span> | <span class="t">Electra is a model that came from Stanford from Kevin Clark and collaborators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=45" target="_blank">00:00:45.920</a></span> | <span class="t">Uh, and I think it's really exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=48" target="_blank">00:00:48.520</a></span> | <span class="t">It shows you the kind of design space we're in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=51" target="_blank">00:00:51.840</a></span> | <span class="t">a really creative example of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=54" target="_blank">00:00:54.160</a></span> | <span class="t">doing something that was different from what had come before in the space of transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=58" target="_blank">00:00:58.640</a></span> | <span class="t">Last time we talked about some known limitations of the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=63" target="_blank">00:01:03.120</a></span> | <span class="t">Most of them identified in the BERT paper itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=65" target="_blank">00:01:05.880</a></span> | <span class="t">I covered that first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=67" target="_blank">00:01:07.520</a></span> | <span class="t">You know, we- we just wanted more ablation studies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=69" target="_blank">00:01:09.880</a></span> | <span class="t">more exploration of the BERT architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=72" target="_blank">00:01:12.600</a></span> | <span class="t">The Roberta team kicked that off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=74" target="_blank">00:01:14.480</a></span> | <span class="t">I think they did a great job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=76" target="_blank">00:01:16.080</a></span> | <span class="t">With Electra, we're gonna address known limitations two and three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=80" target="_blank">00:01:20.640</a></span> | <span class="t">The first is that we have a mismatch between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=83" target="_blank">00:01:23.880</a></span> | <span class="t">the trained vocabulary and the fine-tuned vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=87" target="_blank">00:01:27.240</a></span> | <span class="t">because of the role of the mask token in training BERT models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=91" target="_blank">00:01:31.600</a></span> | <span class="t">And the second one which might feel more pressing to you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=95" target="_blank">00:01:35.040</a></span> | <span class="t">is that BERT is pretty inefficient when it comes to learning from data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=99" target="_blank">00:01:39.000</a></span> | <span class="t">because we mask out or replace about 15% of the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=103" target="_blank">00:01:43.720</a></span> | <span class="t">And as you recall from the BERT learning objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=106" target="_blank">00:01:46.840</a></span> | <span class="t">those are the only tokens that contribute to the learning objective itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=110" target="_blank">00:01:50.960</a></span> | <span class="t">All of the other work is kind of redundant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=113" target="_blank">00:01:53.320</a></span> | <span class="t">And so we might hope that we could make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=115" target="_blank">00:01:55.240</a></span> | <span class="t">more efficient use of all these sequences that we're processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=118" target="_blank">00:01:58.640</a></span> | <span class="t">Electra is gonna make some progress on that too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=121" target="_blank">00:02:01.640</a></span> | <span class="t">So let's focus on the core model structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=124" target="_blank">00:02:04.560</a></span> | <span class="t">and then we'll look at all the other things they did in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=127" target="_blank">00:02:07.440</a></span> | <span class="t">We'll start with our input sequence X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=129" target="_blank">00:02:09.960</a></span> | <span class="t">This is the chef cooked the meal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=132" target="_blank">00:02:12.200</a></span> | <span class="t">And the first thing we do is mask out some of those tokens and that could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=136" target="_blank">00:02:16.080</a></span> | <span class="t">a random sample of 15% of the tokens just like in most work with BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=141" target="_blank">00:02:21.320</a></span> | <span class="t">Then we have what could be literally a BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=144" target="_blank">00:02:24.240</a></span> | <span class="t">We're gonna call it the generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=145" target="_blank">00:02:25.880</a></span> | <span class="t">Typically a small one that has a mass language modeling objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=149" target="_blank">00:02:29.760</a></span> | <span class="t">And that can produce output sequences as usual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=153" target="_blank">00:02:33.200</a></span> | <span class="t">However, the twist here is that we're gonna replace some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=157" target="_blank">00:02:37.880</a></span> | <span class="t">the tokens that came from the input with kind of randomly sampled ones from the MLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=163" target="_blank">00:02:43.480</a></span> | <span class="t">You can see here that we've copied over the and copied over chef,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=167" target="_blank">00:02:47.640</a></span> | <span class="t">but now eight has been replaced by cook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=169" target="_blank">00:02:49.980</a></span> | <span class="t">That might not have been the most probable output for the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=172" target="_blank">00:02:52.880</a></span> | <span class="t">but we're gonna do that replacement step there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=175" target="_blank">00:02:55.520</a></span> | <span class="t">So what we've created- created here is a sequence that we can call X corrupt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=180" target="_blank">00:03:00.280</a></span> | <span class="t">a corrupted version of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=182" target="_blank">00:03:02.720</a></span> | <span class="t">And that is the primary job of this generator model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=186" target="_blank">00:03:06.360</a></span> | <span class="t">At this point, the heart of Electra takes over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=189" target="_blank">00:03:09.360</a></span> | <span class="t">This is called the discriminator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=191" target="_blank">00:03:11.080</a></span> | <span class="t">but we can also talk about it as the Electra model itself in essence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=194" target="_blank">00:03:14.880</a></span> | <span class="t">The job of the discriminator is to figure out which of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=198" target="_blank">00:03:18.780</a></span> | <span class="t">those tokens were originals and which ones were replacements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=202" target="_blank">00:03:22.800</a></span> | <span class="t">So that's a kind of contrastive learning objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=205" target="_blank">00:03:25.360</a></span> | <span class="t">You can see here that the actual label it's gonna learn from from eight is that it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=209" target="_blank">00:03:29.560</a></span> | <span class="t">replaced and for the- that it wasn't original even though it was a sampled token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=214" target="_blank">00:03:34.760</a></span> | <span class="t">And the actual loss for the model is the generator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=217" target="_blank">00:03:37.800</a></span> | <span class="t">that is the- the typical BERT MLM loss together with this Electra loss with a weighting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=224" target="_blank">00:03:44.160</a></span> | <span class="t">That's how the model is trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=225" target="_blank">00:03:45.860</a></span> | <span class="t">but there is as I said an asymmetry here in the sense that once we've done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=229" target="_blank">00:03:49.640</a></span> | <span class="t">the pre-training phase we can let the generator fall away entirely and focus just on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=235" target="_blank">00:03:55.360</a></span> | <span class="t">the discriminator as the model that we're gonna use for downstream fine-tuning tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=240" target="_blank">00:04:00.680</a></span> | <span class="t">And so you can see already that we've in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=244" target="_blank">00:04:04.040</a></span> | <span class="t">solved the problem of having this weird mask token that comes from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=247" target="_blank">00:04:07.440</a></span> | <span class="t">the pre-training phase because the discriminator never sees mask tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=252" target="_blank">00:04:12.000</a></span> | <span class="t">All it sees are these corrupted inputs and it learns to figure out which ones are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=256" target="_blank">00:04:16.960</a></span> | <span class="t">the corrupted versions and which ones are the originals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=260" target="_blank">00:04:20.740</a></span> | <span class="t">Which is a different capability intuitively than the one we were imbuing the core BERT model with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=267" target="_blank">00:04:27.960</a></span> | <span class="t">Right? So for BERT it's kind of like the objective is to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=270" target="_blank">00:04:30.720</a></span> | <span class="t">out what was missing from the surrounding context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=273" target="_blank">00:04:33.720</a></span> | <span class="t">And here it's like trying to figure out which of the words in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=277" target="_blank">00:04:37.120</a></span> | <span class="t">the sequence doesn't belong and which of them do belong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=280" target="_blank">00:04:40.520</a></span> | <span class="t">A kind of more discriminating objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=283" target="_blank">00:04:43.920</a></span> | <span class="t">So that is Electra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=286" target="_blank">00:04:46.760</a></span> | <span class="t">Before we dive into the experiments and stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=290" target="_blank">00:04:50.000</a></span> | <span class="t">questions about how that model works. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=293" target="_blank">00:04:53.920</a></span> | <span class="t">Yes, I'm wondering what the uses of this model are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=296" target="_blank">00:04:56.880</a></span> | <span class="t">So it tries to predict which ones have been replaced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=300" target="_blank">00:05:00.040</a></span> | <span class="t">Like do you- like what applications do you use Electra for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=303" target="_blank">00:05:03.600</a></span> | <span class="t">For pre-training. Yeah, that's what you got to get your head around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=306" target="_blank">00:05:06.560</a></span> | <span class="t">This is a great subtlety to bring out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=308" target="_blank">00:05:08.360</a></span> | <span class="t">So the discriminator is now gonna be our pre-trained artifact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=312" target="_blank">00:05:12.200</a></span> | <span class="t">So just the way you download BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=314" target="_blank">00:05:14.000</a></span> | <span class="t">and when you do that you're downloading some MLN trained object- uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=317" target="_blank">00:05:17.320</a></span> | <span class="t">thing, now you download Electra which is the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=321" target="_blank">00:05:21.360</a></span> | <span class="t">And it's been trained to do this distinguishing thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=325" target="_blank">00:05:25.520</a></span> | <span class="t">as opposed to the filling in the blank or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=327" target="_blank">00:05:27.600</a></span> | <span class="t">continuing thing from the models we've seen so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=330" target="_blank">00:05:30.240</a></span> | <span class="t">The eye-opening thing is that that contrastive objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=333" target="_blank">00:05:33.500</a></span> | <span class="t">leads to a really good pre-trained state for fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=337" target="_blank">00:05:37.680</a></span> | <span class="t">And we might hope that it's doing it much more efficiently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=342" target="_blank">00:05:42.080</a></span> | <span class="t">but that's what we can dive into now here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=343" target="_blank">00:05:43.840</a></span> | <span class="t">So first, generator-discriminator relationships.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=347" target="_blank">00:05:47.760</a></span> | <span class="t">They observe in the paper that when the generator and the discriminator are the same size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=352" target="_blank">00:05:52.720</a></span> | <span class="t">they can share all their transformer parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=355" target="_blank">00:05:55.560</a></span> | <span class="t">and more sharing is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=357" target="_blank">00:05:57.280</a></span> | <span class="t">So already we have an efficiency gain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=358" target="_blank">00:05:58.900</a></span> | <span class="t">and that's kind of intriguing that one in the same set of weights would be playing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=362" target="_blank">00:06:02.560</a></span> | <span class="t">the role of the MLN and the discriminator- generator-discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=366" target="_blank">00:06:06.760</a></span> | <span class="t">But they observe that they guess the- get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=369" target="_blank">00:06:09.640</a></span> | <span class="t">the best results from having a generator that is small compared to the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=375" target="_blank">00:06:15.400</a></span> | <span class="t">And this plot kind of teases that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=377" target="_blank">00:06:17.360</a></span> | <span class="t">So we've got our glue score along the y-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=379" target="_blank">00:06:19.880</a></span> | <span class="t">This will just be a measure of system quality for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=383" target="_blank">00:06:23.520</a></span> | <span class="t">And along the x-axis here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=385" target="_blank">00:06:25.160</a></span> | <span class="t">we have generator size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=387" target="_blank">00:06:27.080</a></span> | <span class="t">And what they mean by that is the dimensionality of the model in the BERT sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=392" target="_blank">00:06:32.040</a></span> | <span class="t">So essentially the size of each one of the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=395" target="_blank">00:06:35.080</a></span> | <span class="t">And if we zoom in, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=396" target="_blank">00:06:36.960</a></span> | <span class="t">on this blue line, the best performing model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=398" target="_blank">00:06:38.840</a></span> | <span class="t">this is where we have 768 as our dimensionality for the discriminator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=403" target="_blank">00:06:43.960</a></span> | <span class="t">and 768 for the generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=407" target="_blank">00:06:47.520</a></span> | <span class="t">As we make the generator smaller,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=410" target="_blank">00:06:50.600</a></span> | <span class="t">all the way down to 256,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=412" target="_blank">00:06:52.800</a></span> | <span class="t">performance improves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=414" target="_blank">00:06:54.640</a></span> | <span class="t">And that's what we mean when we say better to have a small generator and a large discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=420" target="_blank">00:07:00.000</a></span> | <span class="t">And that kind of U-shaped pattern is repeated across all the different discriminator sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=425" target="_blank">00:07:05.800</a></span> | <span class="t">And that's probably an insight about how this model is working,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=429" target="_blank">00:07:09.080</a></span> | <span class="t">which is the sense that you kind of want the generator to be a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=432" target="_blank">00:07:12.560</a></span> | <span class="t">a noisy process so that the discriminator has some interesting work to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=437" target="_blank">00:07:17.760</a></span> | <span class="t">And by making the discriminator more powerful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=440" target="_blank">00:07:20.480</a></span> | <span class="t">I guess you're creating that kind of opportunity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=444" target="_blank">00:07:24.000</a></span> | <span class="t">They also do a lot of work looking at efficiency because one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=449" target="_blank">00:07:29.400</a></span> | <span class="t">the side goals of the elector paper was to end up with models that were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=453" target="_blank">00:07:33.560</a></span> | <span class="t">overall more efficient in terms of the pre-training compute and in terms of the model size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=458" target="_blank">00:07:38.520</a></span> | <span class="t">Here's another way to quantify that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=460" target="_blank">00:07:40.200</a></span> | <span class="t">Again, along the y-axis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=461" target="_blank">00:07:41.600</a></span> | <span class="t">we have the glue score,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=462" target="_blank">00:07:42.920</a></span> | <span class="t">and along the x-axis now we have pre-trained flops,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=465" target="_blank">00:07:45.800</a></span> | <span class="t">which you could just think of as a very low level measure of how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=469" target="_blank">00:07:49.040</a></span> | <span class="t">much compute resources we need to do the pre-training part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=473" target="_blank">00:07:53.520</a></span> | <span class="t">The blue line at the top is Electra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=475" target="_blank">00:07:55.960</a></span> | <span class="t">and it's the best no matter what your computational budget is along the x-axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=481" target="_blank">00:08:01.040</a></span> | <span class="t">They also explore adversarial Electra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=484" target="_blank">00:08:04.240</a></span> | <span class="t">This is very intuitive to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=485" target="_blank">00:08:05.960</a></span> | <span class="t">That is a slightly different objective where the generator is trying to fool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=490" target="_blank">00:08:10.560</a></span> | <span class="t">the discriminator by creating corrupted sequences that are hard for the generator to distinguish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=496" target="_blank">00:08:16.840</a></span> | <span class="t">That's a really good model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=498" target="_blank">00:08:18.400</a></span> | <span class="t">but it's less than the kind of more cooperative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=501" target="_blank">00:08:21.120</a></span> | <span class="t">um, joint objective that I showed you before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=504" target="_blank">00:08:24.040</a></span> | <span class="t">And then the green line is cool too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=505" target="_blank">00:08:25.840</a></span> | <span class="t">So the green line is where I start training with BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=509" target="_blank">00:08:29.760</a></span> | <span class="t">and that at a certain point I switch to having also the discriminator loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=514" target="_blank">00:08:34.640</a></span> | <span class="t">And at that point, the BERT model is less good for any compute budget,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=518" target="_blank">00:08:38.640</a></span> | <span class="t">whereas the Electra variant starts to do its Electra thing and get better and better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=523" target="_blank">00:08:43.640</a></span> | <span class="t">So a bunch of perspectives on Electra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=526" target="_blank">00:08:46.200</a></span> | <span class="t">all pointing to it being a good and efficient model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=530" target="_blank">00:08:50.040</a></span> | <span class="t">And then finally, they do a bunch more efficiency analyses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=533" target="_blank">00:08:53.760</a></span> | <span class="t">So this is that picture that I showed you of the full Electra model before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=537" target="_blank">00:08:57.160</a></span> | <span class="t">where I have the generator creating corrupted sequences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=540" target="_blank">00:09:00.480</a></span> | <span class="t">and then the discriminator doing its discriminating part there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=544" target="_blank">00:09:04.640</a></span> | <span class="t">You could also explore Electra 15 percent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=547" target="_blank">00:09:07.400</a></span> | <span class="t">and this is different from full Electra in the sense that on the right for default Electra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=552" target="_blank">00:09:12.760</a></span> | <span class="t">we make predictions about all of these tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=555" target="_blank">00:09:15.240</a></span> | <span class="t">whether they were original or replaced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=557" target="_blank">00:09:17.600</a></span> | <span class="t">For the 15 percent version,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=559" target="_blank">00:09:19.360</a></span> | <span class="t">we kind of do a BERT-like thing where we're going to assume that the ones that weren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=562" target="_blank">00:09:22.720</a></span> | <span class="t">part of those corrupted chains there, the sampled part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=566" target="_blank">00:09:26.920</a></span> | <span class="t">are just not part of the objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=568" target="_blank">00:09:28.880</a></span> | <span class="t">There'll be fewer tokens there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=571" target="_blank">00:09:31.040</a></span> | <span class="t">Replace MLM. This is an ablation where actually we drop away the Electra part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=576" target="_blank">00:09:36.200</a></span> | <span class="t">and we're just looking at the MLM here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=578" target="_blank">00:09:38.800</a></span> | <span class="t">and we're going to not have the mask token at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=582" target="_blank">00:09:42.160</a></span> | <span class="t">Because remember for BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=583" target="_blank">00:09:43.760</a></span> | <span class="t">there are a few ways that they do this learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=585" target="_blank">00:09:45.680</a></span> | <span class="t">They do the mask token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=586" target="_blank">00:09:46.960</a></span> | <span class="t">and they also do the one where they just replace it with some random tokens here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=590" target="_blank">00:09:50.620</a></span> | <span class="t">like cook to run, and then the model has to reproduce a new token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=595" target="_blank">00:09:55.200</a></span> | <span class="t">Oh, that should say cooked I guess,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=597" target="_blank">00:09:57.440</a></span> | <span class="t">because it's pure BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=600" target="_blank">00:10:00.080</a></span> | <span class="t">This is a kind of look at what happens if we don't introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=604" target="_blank">00:10:04.720</a></span> | <span class="t">that mask token addressing that question about whether that was disrupting learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=608" target="_blank">00:10:08.960</a></span> | <span class="t">Then finally, all tokens MLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=610" target="_blank">00:10:10.840</a></span> | <span class="t">This is again just a BERT-based objective over here where instead of turning off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=614" target="_blank">00:10:14.920</a></span> | <span class="t">the objective for these ones here that weren't part of the corrupted sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=618" target="_blank">00:10:18.760</a></span> | <span class="t">we do learning from all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=620" target="_blank">00:10:20.720</a></span> | <span class="t">That's a way of saying for BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=623" target="_blank">00:10:23.040</a></span> | <span class="t">if we were making more efficient use of the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=625" target="_blank">00:10:25.120</a></span> | <span class="t">could we learn more quickly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=627" target="_blank">00:10:27.200</a></span> | <span class="t">Here are the results. So Electra is at the top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=630" target="_blank">00:10:30.720</a></span> | <span class="t">but just below it is all tokens MLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=634" target="_blank">00:10:34.120</a></span> | <span class="t">So that's just BERT learning from all of the tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=636" target="_blank">00:10:36.400</a></span> | <span class="t">and I think that does show that BERT could have been a little better if they had not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=640" target="_blank">00:10:40.640</a></span> | <span class="t">turned off the objective for every single token that wasn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=643" target="_blank">00:10:43.600</a></span> | <span class="t">part of the masking or corruption for that learning process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=647" target="_blank">00:10:47.400</a></span> | <span class="t">Replace MLM is just below that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=649" target="_blank">00:10:49.520</a></span> | <span class="t">and that's where we don't have any mask token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=651" target="_blank">00:10:51.340</a></span> | <span class="t">So there's no fine-tuning pre-trained mismatch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=653" target="_blank">00:10:53.960</a></span> | <span class="t">Electra 15 below that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=655" target="_blank">00:10:55.720</a></span> | <span class="t">and then BERT at the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=656" target="_blank">00:10:56.920</a></span> | <span class="t">So overall, you're seeing these ablations are showing us that every piece of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=661" target="_blank">00:11:01.240</a></span> | <span class="t">Electra is contributing something to the overall performance of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=665" target="_blank">00:11:05.760</a></span> | <span class="t">and that's quite nice as well. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=668" target="_blank">00:11:08.400</a></span> | <span class="t">How is the efficiency of all tokens MLM?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=671" target="_blank">00:11:11.960</a></span> | <span class="t">The efficiency?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=673" target="_blank">00:11:13.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=674" target="_blank">00:11:14.640</a></span> | <span class="t">Well, we're making more efficient use of the data because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=677" target="_blank">00:11:17.480</a></span> | <span class="t">we're getting a learning signal from every token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=680" target="_blank">00:11:20.000</a></span> | <span class="t">and I guess that would be the important dimension because a funny thing about BERT where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=684" target="_blank">00:11:24.220</a></span> | <span class="t">we turn off the learning for the ones that weren't masked or corrupted,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=688" target="_blank">00:11:28.980</a></span> | <span class="t">is that we still have to do the work of computing them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=691" target="_blank">00:11:31.420</a></span> | <span class="t">It's just that then they don't become part of the objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=694" target="_blank">00:11:34.360</a></span> | <span class="t">and here we're just kind of bringing that in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=696" target="_blank">00:11:36.640</a></span> | <span class="t">So for free or close to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=699" target="_blank">00:11:39.420</a></span> | <span class="t">My question was, how is the glue score calculated?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=704" target="_blank">00:11:44.540</a></span> | <span class="t">What does it represent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=705" target="_blank">00:11:45.580</a></span> | <span class="t">Some accuracy in language generation afterwards,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=708" target="_blank">00:11:48.940</a></span> | <span class="t">or is it the classifier that's being scored?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=711" target="_blank">00:11:51.580</a></span> | <span class="t">Oh, yeah. So glue is a big multitask classification benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=716" target="_blank">00:11:56.220</a></span> | <span class="t">It's a pretty diverse set of tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=718" target="_blank">00:11:58.540</a></span> | <span class="t">maybe biased toward natural language inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=721" target="_blank">00:12:01.620</a></span> | <span class="t">The reason they're using it in the paper is just that it has been,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=726" target="_blank">00:12:06.100</a></span> | <span class="t">it has been adopted as a kind of general purpose measure of performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=731" target="_blank">00:12:11.020</a></span> | <span class="t">and it's driven a lot of reasoning about what's good and what's bad in the field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=737" target="_blank">00:12:17.060</a></span> | <span class="t">Then here are some model releases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=739" target="_blank">00:12:19.860</a></span> | <span class="t">Base and large, kind of align with BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=742" target="_blank">00:12:22.580</a></span> | <span class="t">and then we have this small model here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=744" target="_blank">00:12:24.460</a></span> | <span class="t">and that was designed to be quickly trained on a single GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=747" target="_blank">00:12:27.340</a></span> | <span class="t">again, as a nod toward efficiency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=748" target="_blank">00:12:28.940</a></span> | <span class="t">and all three are really good models. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=754" target="_blank">00:12:34.100</a></span> | <span class="t">The things that we've observed at Electra,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=757" target="_blank">00:12:37.420</a></span> | <span class="t">so like putting our text into some kind of representation space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=762" target="_blank">00:12:42.140</a></span> | <span class="t">that's better than BERT and for BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=764" target="_blank">00:12:44.700</a></span> | <span class="t">Or is it just like generally glue-wise, it's like that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=770" target="_blank">00:12:50.060</a></span> | <span class="t">Oh, I like that question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=771" target="_blank">00:12:51.860</a></span> | <span class="t">That could kind of queue up some analysis work that you could do for a final project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=775" target="_blank">00:12:55.980</a></span> | <span class="t">Because I think the insight behind your question is that a lot of the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=779" target="_blank">00:12:59.540</a></span> | <span class="t">we reason about these models just based on their performance on something like glue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=783" target="_blank">00:13:03.740</a></span> | <span class="t">You could ask a deeper question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=785" target="_blank">00:13:05.520</a></span> | <span class="t">what are their internal representations like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=787" target="_blank">00:13:07.940</a></span> | <span class="t">and are there places where they're transformatively better or worse?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=791" target="_blank">00:13:11.500</a></span> | <span class="t">That you could tie that back to the fact that the learning objective is different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=795" target="_blank">00:13:15.700</a></span> | <span class="t">We're doing this discrimination thing as opposed to filling in the blanks in some sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=800" target="_blank">00:13:20.140</a></span> | <span class="t">Maybe there are some underlying differences. I love that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=803" target="_blank">00:13:23.940</a></span> | <span class="t">All right. Couple more topics here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=812" target="_blank">00:13:32.500</a></span> | <span class="t">just quickly because we're going to do more work with seek-to-seek models later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=816" target="_blank">00:13:36.780</a></span> | <span class="t">We're going to train some of our own from scratch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=818" target="_blank">00:13:38.740</a></span> | <span class="t">and you all might use some fine-tuned ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=820" target="_blank">00:13:40.420</a></span> | <span class="t">So I thought it would be good to just get them on the table as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=824" target="_blank">00:13:44.060</a></span> | <span class="t">Seek-to-seek, here's some natural tasks that fall into the seek-to-seek structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=829" target="_blank">00:13:49.060</a></span> | <span class="t">Machine translation, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=830" target="_blank">00:13:50.460</a></span> | <span class="t">Source language to target language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=832" target="_blank">00:13:52.620</a></span> | <span class="t">Summarization, big text to hopefully smaller text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=836" target="_blank">00:13:56.720</a></span> | <span class="t">Freeform question answering, where you go from a question and then maybe you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=839" target="_blank">00:13:59.980</a></span> | <span class="t">generating as opposed to just extracting an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=843" target="_blank">00:14:03.640</a></span> | <span class="t">Dialogue, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=845" target="_blank">00:14:05.420</a></span> | <span class="t">Semantic parsing, this is the one we're going to tackle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=847" target="_blank">00:14:07.980</a></span> | <span class="t">where you go from a sentence to some kind of logical form representing its meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=852" target="_blank">00:14:12.740</a></span> | <span class="t">Code generation, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=854" target="_blank">00:14:14.540</a></span> | <span class="t">that's similar, and on and on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=856" target="_blank">00:14:16.380</a></span> | <span class="t">I think there are lots of problems that are pretty naturally cast as seek-to-seek problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=860" target="_blank">00:14:20.980</a></span> | <span class="t">especially when you've got different stuff on the input and the output side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=867" target="_blank">00:14:27.100</a></span> | <span class="t">Yeah, and the more general class of things we could be talking about would be encoder-decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=872" target="_blank">00:14:32.380</a></span> | <span class="t">where that's just more general in the sense that at that point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=875" target="_blank">00:14:35.380</a></span> | <span class="t">the input could be a picture you're encoding and the output a text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=878" target="_blank">00:14:38.980</a></span> | <span class="t">Picture-to-picture, video-to-picture, in principle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=882" target="_blank">00:14:42.500</a></span> | <span class="t">anything could be happening on the two sides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=884" target="_blank">00:14:44.260</a></span> | <span class="t">Seek-to-seek would, for me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=885" target="_blank">00:14:45.740</a></span> | <span class="t">just be the special case where we're looking at sequential data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=888" target="_blank">00:14:48.620</a></span> | <span class="t">typically language data or computer code or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=893" target="_blank">00:14:53.060</a></span> | <span class="t">From the RNN era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=896" target="_blank">00:14:56.980</a></span> | <span class="t">this is just nice if you hearken back to that era, if you live through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=901" target="_blank">00:15:01.940</a></span> | <span class="t">This is a paper from Tang Luong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=903" target="_blank">00:15:03.820</a></span> | <span class="t">Doing seek-to-seek on the left in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=906" target="_blank">00:15:06.140</a></span> | <span class="t">the traditional way with a recurrent neural network, an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=909" target="_blank">00:15:09.020</a></span> | <span class="t">Pretty simple, right? We've got A, B, C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=910" target="_blank">00:15:10.900</a></span> | <span class="t">D coming in, and then it transitions into maybe other parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=914" target="_blank">00:15:14.340</a></span> | <span class="t">and it's trying to produce this new sequence left to right coming out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=917" target="_blank">00:15:17.820</a></span> | <span class="t">Just to remind you, this is part of the journey the field went on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=921" target="_blank">00:15:21.260</a></span> | <span class="t">What Tang did, very influentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=923" target="_blank">00:15:23.540</a></span> | <span class="t">is think a lot about how you would add attention layers in to that RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=929" target="_blank">00:15:29.660</a></span> | <span class="t">That's what you see depicted here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=931" target="_blank">00:15:31.100</a></span> | <span class="t">This is a schematic diagram hinting at the fact that we were moving into an era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=936" target="_blank">00:15:36.340</a></span> | <span class="t">the Vaswani et al era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=938" target="_blank">00:15:38.020</a></span> | <span class="t">attention is all you need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=939" target="_blank">00:15:39.340</a></span> | <span class="t">where basically that attention layer would do all the work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=943" target="_blank">00:15:43.380</a></span> | <span class="t">That's where we're at now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=946" target="_blank">00:15:46.300</a></span> | <span class="t">For seek-to-seek problems in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=949" target="_blank">00:15:49.580</a></span> | <span class="t">this is a nice framework from the T5 paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=952" target="_blank">00:15:52.320</a></span> | <span class="t">There are a few different ways you could think about them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=954" target="_blank">00:15:54.800</a></span> | <span class="t">On the left is the one I'm nudging you toward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=957" target="_blank">00:15:57.340</a></span> | <span class="t">where we have an encoder and a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=959" target="_blank">00:15:59.620</a></span> | <span class="t">If we're talking about transformer models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=962" target="_blank">00:16:02.100</a></span> | <span class="t">what essentially that means is that when we do encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=965" target="_blank">00:16:05.180</a></span> | <span class="t">we can connect everything to everything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=967" target="_blank">00:16:07.140</a></span> | <span class="t">You can think of that as a process of simultaneously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=970" target="_blank">00:16:10.220</a></span> | <span class="t">encoding the entire input with all its connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=974" target="_blank">00:16:14.060</a></span> | <span class="t">But as we do decoding for many of these problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=976" target="_blank">00:16:16.780</a></span> | <span class="t">we need to do some sequential generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=979" target="_blank">00:16:19.700</a></span> | <span class="t">The result of that is we can look back to the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=983" target="_blank">00:16:23.420</a></span> | <span class="t">sorry, the encoder all we want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=985" target="_blank">00:16:25.180</a></span> | <span class="t">but for the decoder, we have to do that masking that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=987" target="_blank">00:16:27.860</a></span> | <span class="t">described with the autoregressive loss last time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=990" target="_blank">00:16:30.900</a></span> | <span class="t">so that we don't look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=993" target="_blank">00:16:33.220</a></span> | <span class="t">But with that constraint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=994" target="_blank">00:16:34.900</a></span> | <span class="t">we can do this encoder-decoder thing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=997" target="_blank">00:16:37.340</a></span> | <span class="t">a decoding step be truly sequential decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1000" target="_blank">00:16:40.740</a></span> | <span class="t">But that's not the only way to think about these problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1003" target="_blank">00:16:43.540</a></span> | <span class="t">In fact, I don't want to presuppose that for a sequence-to-sequence thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1007" target="_blank">00:16:47.220</a></span> | <span class="t">you'll use a sequence-to-sequence model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1009" target="_blank">00:16:49.180</a></span> | <span class="t">You could, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1010" target="_blank">00:16:50.340</a></span> | <span class="t">use a language model and the way you might do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1012" target="_blank">00:16:52.260</a></span> | <span class="t">that is to say I'm just going to encode everything left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1015" target="_blank">00:16:55.660</a></span> | <span class="t">That's the version in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1017" target="_blank">00:16:57.620</a></span> | <span class="t">Then a kind of compromise position would be that you would take your language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1021" target="_blank">00:17:01.540</a></span> | <span class="t">which might be autoregressive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1023" target="_blank">00:17:03.060</a></span> | <span class="t">but simultaneously encode the entire input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1026" target="_blank">00:17:06.620</a></span> | <span class="t">and then begin your process of decoding without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1029" target="_blank">00:17:09.820</a></span> | <span class="t">explicitly having an encoder part and a decoder part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1033" target="_blank">00:17:13.980</a></span> | <span class="t">I think all of them are on the table and people are solving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1036" target="_blank">00:17:16.740</a></span> | <span class="t">seek-to-seek tasks right now using all of these variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1041" target="_blank">00:17:21.300</a></span> | <span class="t">T5, I'm going to show you two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1045" target="_blank">00:17:25.180</a></span> | <span class="t">There are lots out there but these are very prominent ones that you might download.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1048" target="_blank">00:17:28.820</a></span> | <span class="t">So T5, this is a wonderful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1050" target="_blank">00:17:30.700</a></span> | <span class="t">very rich paper that does a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1052" target="_blank">00:17:32.420</a></span> | <span class="t">exploration of which of these architectures are effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1055" target="_blank">00:17:35.420</a></span> | <span class="t">T5 ended up on an encoder-decoder variant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1058" target="_blank">00:17:38.460</a></span> | <span class="t">and what they did is an impressive amount of multitask training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1062" target="_blank">00:17:42.860</a></span> | <span class="t">unsupervised and supervised objectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1065" target="_blank">00:17:45.700</a></span> | <span class="t">An innovative thing that they did is have these task prefixes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1070" target="_blank">00:17:50.620</a></span> | <span class="t">like translate English to German and then an English sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1073" target="_blank">00:17:53.940</a></span> | <span class="t">or this is a COLA sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1075" target="_blank">00:17:55.900</a></span> | <span class="t">that's just a data set people use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1077" target="_blank">00:17:57.660</a></span> | <span class="t">or an STSB sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1080" target="_blank">00:18:00.020</a></span> | <span class="t">and that's the model's cue to take that input and condition it very informally on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1085" target="_blank">00:18:05.500</a></span> | <span class="t">that task so that the output behavior is the expected behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1090" target="_blank">00:18:10.180</a></span> | <span class="t">There are lots of T5 models that you can download.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1093" target="_blank">00:18:13.860</a></span> | <span class="t">This is nice for development because some of them are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1095" target="_blank">00:18:15.860</a></span> | <span class="t">very small and some of them are very, very large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1098" target="_blank">00:18:18.540</a></span> | <span class="t">More recently, there are these FLAN models which took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1101" target="_blank">00:18:21.460</a></span> | <span class="t">a T5 architecture and did a lot of reinforcement learning with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1104" target="_blank">00:18:24.460</a></span> | <span class="t">human feedback to even further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1107" target="_blank">00:18:27.060</a></span> | <span class="t">specialize them to different tasks in interesting ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1110" target="_blank">00:18:30.660</a></span> | <span class="t">So that's T5, and then the other one that you often hear about that's very effective is BART.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1115" target="_blank">00:18:35.900</a></span> | <span class="t">BART is interestingly different yet again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1118" target="_blank">00:18:38.740</a></span> | <span class="t">So BART is an encoder-decoder framework,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1121" target="_blank">00:18:41.700</a></span> | <span class="t">and it's really got a BERT style thing on the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1125" target="_blank">00:18:45.420</a></span> | <span class="t">and then a GPT style thing on the right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1127" target="_blank">00:18:47.740</a></span> | <span class="t">that is, joint encoding of everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1130" target="_blank">00:18:50.100</a></span> | <span class="t">and then that autoregressive part if you want to do sequential generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1133" target="_blank">00:18:53.940</a></span> | <span class="t">The innovative thing about BART is that the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1137" target="_blank">00:18:57.500</a></span> | <span class="t">involves a lot of corrupting of that input sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1141" target="_blank">00:19:01.220</a></span> | <span class="t">They tried to like do text infilling of pieces,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1144" target="_blank">00:19:04.220</a></span> | <span class="t">they shuffled sentences around,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1146" target="_blank">00:19:06.020</a></span> | <span class="t">they did some masking,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1147" target="_blank">00:19:07.340</a></span> | <span class="t">and they did some token deletion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1148" target="_blank">00:19:08.980</a></span> | <span class="t">rotating of documents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1150" target="_blank">00:19:10.360</a></span> | <span class="t">all of this corrupting of the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1152" target="_blank">00:19:12.520</a></span> | <span class="t">and then the model's objective is to learn how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1155" target="_blank">00:19:15.100</a></span> | <span class="t">essentially uncorrupt what it got as the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1158" target="_blank">00:19:18.100</a></span> | <span class="t">They found that the joint process of this text infilling thing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1162" target="_blank">00:19:22.260</a></span> | <span class="t">sentence shuffling was the most effective for training BART.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1166" target="_blank">00:19:26.220</a></span> | <span class="t">So that was for the pre-training phase,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1168" target="_blank">00:19:28.220</a></span> | <span class="t">and when then you- then when you fine-tune with BART,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1170" target="_blank">00:19:30.820</a></span> | <span class="t">for classification tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1172" target="_blank">00:19:32.460</a></span> | <span class="t">you just put in two uncorrupted copies of your sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1175" target="_blank">00:19:35.600</a></span> | <span class="t">and then you could fit your task specific labels on like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1179" target="_blank">00:19:39.140</a></span> | <span class="t">the class token or the final token of the GPT output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1182" target="_blank">00:19:42.420</a></span> | <span class="t">and for seek-to-seek, you just use it as a standard encoder-decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1186" target="_blank">00:19:46.380</a></span> | <span class="t">And again, the intuition is that the pre-training phase which did all this corruption,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1190" target="_blank">00:19:50.580</a></span> | <span class="t">has helped the model learn what sequences are like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1193" target="_blank">00:19:53.980</a></span> | <span class="t">And that blends together for me a lot of the intuitions we've seen from MLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1198" target="_blank">00:19:58.700</a></span> | <span class="t">and from what we just talked about with Electra. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1203" target="_blank">00:20:03.460</a></span> | <span class="t">Um, kind of a question that I asked last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1205" target="_blank">00:20:05.300</a></span> | <span class="t">Have any of these models worked with like spelling mistakes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1209" target="_blank">00:20:09.180</a></span> | <span class="t">Yeah. So BART is a really good option if you want to do spelling correction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1213" target="_blank">00:20:13.660</a></span> | <span class="t">And I actually think that that might be because spelling correction is kind of as a task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1219" target="_blank">00:20:19.180</a></span> | <span class="t">a corrupting of the input where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1221" target="_blank">00:20:21.100</a></span> | <span class="t">trying to learn the uncorrupted version of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1223" target="_blank">00:20:23.580</a></span> | <span class="t">So I think if you want to do grammar correction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1225" target="_blank">00:20:25.540</a></span> | <span class="t">spelling correction, things like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1227" target="_blank">00:20:27.280</a></span> | <span class="t">it's outstanding to use BART,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1228" target="_blank">00:20:28.900</a></span> | <span class="t">and you might just think of training from scratch on a model that you know is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1232" target="_blank">00:20:32.900</a></span> | <span class="t">going to be aware of characters for these character level things. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1237" target="_blank">00:20:37.580</a></span> | <span class="t">Sorry, what is text in building?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1239" target="_blank">00:20:39.820</a></span> | <span class="t">That was where they like removed parts of the text essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1244" target="_blank">00:20:44.580</a></span> | <span class="t">and added other pieces to corrupt it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1246" target="_blank">00:20:46.820</a></span> | <span class="t">Different from masking where you just hide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1249" target="_blank">00:20:49.140</a></span> | <span class="t">Yeah, where you just, that's more like the BERT style thing where you hide some.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1252" target="_blank">00:20:52.340</a></span> | <span class="t">Yeah. And okay, final quick topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1259" target="_blank">00:20:59.260</a></span> | <span class="t">I just want you all to know about distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1262" target="_blank">00:21:02.220</a></span> | <span class="t">Again, because a theme of this course could be how can we do more with less?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1265" target="_blank">00:21:05.980</a></span> | <span class="t">And distillation is a vision for how we could do more with less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1269" target="_blank">00:21:09.340</a></span> | <span class="t">Right. We saw this trend in model sizes here where they're getting bigger and bigger,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1273" target="_blank">00:21:13.460</a></span> | <span class="t">and there is some hope that they might now be getting smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1276" target="_blank">00:21:16.980</a></span> | <span class="t">But we should all be pushing to make them ever smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1280" target="_blank">00:21:20.300</a></span> | <span class="t">And one way to think about doing that is distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1282" target="_blank">00:21:22.700</a></span> | <span class="t">And the metaphor here is that we're going to have two models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1285" target="_blank">00:21:25.460</a></span> | <span class="t">Maybe a really big teacher model that was trained in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1288" target="_blank">00:21:28.540</a></span> | <span class="t">a very expensive way and might run only on a supercomputer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1291" target="_blank">00:21:31.840</a></span> | <span class="t">and then a much smaller student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1294" target="_blank">00:21:34.140</a></span> | <span class="t">And we're going to train that student to mimic the behavior of the teacher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1298" target="_blank">00:21:38.220</a></span> | <span class="t">And we could do that by just observing the output behavior of the teacher,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1302" target="_blank">00:21:42.380</a></span> | <span class="t">and then trying to get the student to align at the level of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1306" target="_blank">00:21:46.260</a></span> | <span class="t">And that would basically just be treating this teacher as a kind of input output device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1310" target="_blank">00:21:50.980</a></span> | <span class="t">We could also though think about aligning the internal representations of these two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1316" target="_blank">00:21:56.020</a></span> | <span class="t">to get a deeper alignment between teacher and student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1320" target="_blank">00:22:00.100</a></span> | <span class="t">Here's some objectives in fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1322" target="_blank">00:22:02.220</a></span> | <span class="t">and this is from least to most heavy duty,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1324" target="_blank">00:22:04.620</a></span> | <span class="t">and you could combine them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1326" target="_blank">00:22:06.060</a></span> | <span class="t">So we could just use our gold data for the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1328" target="_blank">00:22:08.940</a></span> | <span class="t">I put that as step zero because you might want it in the mix here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1332" target="_blank">00:22:12.080</a></span> | <span class="t">even as you use your teacher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1333" target="_blank">00:22:13.940</a></span> | <span class="t">We could also learn just from the teacher's output labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1337" target="_blank">00:22:17.440</a></span> | <span class="t">That's a bit of a funny idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1339" target="_blank">00:22:19.180</a></span> | <span class="t">but I think the intuition is that the teacher might be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1342" target="_blank">00:22:22.260</a></span> | <span class="t">some very complicated regularization that helps the student learn more efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1347" target="_blank">00:22:27.420</a></span> | <span class="t">So even if there are mistakes in the teacher's behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1350" target="_blank">00:22:30.260</a></span> | <span class="t">the student actually benefits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1352" target="_blank">00:22:32.740</a></span> | <span class="t">You could also think about going one level deeper and using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1355" target="_blank">00:22:35.620</a></span> | <span class="t">the full output scores like the logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1357" target="_blank">00:22:37.620</a></span> | <span class="t">so not just the discrete outputs but the whole distribution that the model predicts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1362" target="_blank">00:22:42.140</a></span> | <span class="t">And that's what they did in one of the original distillation papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1365" target="_blank">00:22:45.900</a></span> | <span class="t">You could also tie together the final output states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1369" target="_blank">00:22:49.180</a></span> | <span class="t">If the two models have the same layer-wise dimensionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1372" target="_blank">00:22:52.260</a></span> | <span class="t">then for example in this distilbert paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1374" target="_blank">00:22:54.740</a></span> | <span class="t">they enforce as part of the objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1376" target="_blank">00:22:56.720</a></span> | <span class="t">a cosine similarity between the output states of teacher and student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1380" target="_blank">00:23:00.820</a></span> | <span class="t">And now you need to have access to the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1383" target="_blank">00:23:03.540</a></span> | <span class="t">And this will be much more expensive because you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1385" target="_blank">00:23:05.540</a></span> | <span class="t">run the teacher as part of distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1388" target="_blank">00:23:08.700</a></span> | <span class="t">You could also think about doing this with lots of other hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1392" target="_blank">00:23:12.300</a></span> | <span class="t">People have explored lots of other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1394" target="_blank">00:23:14.500</a></span> | <span class="t">And you could even, this is a paper that we did,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1396" target="_blank">00:23:16.860</a></span> | <span class="t">try to mimic them under different counterfactuals where you kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1400" target="_blank">00:23:20.260</a></span> | <span class="t">change around the input representations of the teacher,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1403" target="_blank">00:23:23.180</a></span> | <span class="t">observe the output, and then try to get the student to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1405" target="_blank">00:23:25.560</a></span> | <span class="t">that to mimic very strange behavior from the teacher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1410" target="_blank">00:23:30.980</a></span> | <span class="t">And then there are a bunch of other things you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1413" target="_blank">00:23:33.620</a></span> | <span class="t">So standard distillation is where you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1415" target="_blank">00:23:35.940</a></span> | <span class="t">your big model frozen and the teacher is being updated by the process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1419" target="_blank">00:23:39.980</a></span> | <span class="t">If you have multi-teacher,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1421" target="_blank">00:23:41.300</a></span> | <span class="t">that's where there are lots of big models maybe doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1423" target="_blank">00:23:43.180</a></span> | <span class="t">multiple tasks and you try to distill them all at once down into a student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1427" target="_blank">00:23:47.100</a></span> | <span class="t">That's a very exciting new frontier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1429" target="_blank">00:23:49.860</a></span> | <span class="t">Co-distillation is where they're trained jointly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1433" target="_blank">00:23:53.220</a></span> | <span class="t">sometimes also called online distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1435" target="_blank">00:23:55.500</a></span> | <span class="t">That's where both the teacher and the student are learning together simultaneously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1439" target="_blank">00:23:59.300</a></span> | <span class="t">Might be unnerving in the classroom but effective for a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1442" target="_blank">00:24:02.580</a></span> | <span class="t">And then self-distillation is actually where you try to get like usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1446" target="_blank">00:24:06.580</a></span> | <span class="t">lower parts of the model to be like other parts of the model by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1450" target="_blank">00:24:10.300</a></span> | <span class="t">having them mimic themselves as part of the core model training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1454" target="_blank">00:24:14.060</a></span> | <span class="t">So that's a special case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1455" target="_blank">00:24:15.560</a></span> | <span class="t">I guess, of co-distillation where there's only one model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1458" target="_blank">00:24:18.600</a></span> | <span class="t">you're trying to distill parts of it into other parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1461" target="_blank">00:24:21.420</a></span> | <span class="t">That's kind of wild to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1463" target="_blank">00:24:23.860</a></span> | <span class="t">And this has been applied in many domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1466" target="_blank">00:24:26.860</a></span> | <span class="t">And the reason I can be encouraging about this is that as we get better and better at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1470" target="_blank">00:24:30.980</a></span> | <span class="t">distillation we're finding that distilled models are as good or better than the teacher models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1476" target="_blank">00:24:36.580</a></span> | <span class="t">Maybe for a fraction of the cost and this is especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1480" target="_blank">00:24:40.020</a></span> | <span class="t">relevant if the model is being used in production on a small device or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1484" target="_blank">00:24:44.580</a></span> | <span class="t">So here are just some glue performance numbers that show across a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1488" target="_blank">00:24:48.140</a></span> | <span class="t">these different papers that with distillation you can still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1491" target="_blank">00:24:51.300</a></span> | <span class="t">get glue performance like the teacher with a tiny model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1496" target="_blank">00:24:56.420</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1499" target="_blank">00:24:59.100</a></span> | <span class="t">Something really puzzling to me is how can a smaller simple model be able to mimic a teacher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1504" target="_blank">00:25:04.620</a></span> | <span class="t">when the training set is fixed?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1506" target="_blank">00:25:06.500</a></span> | <span class="t">Couldn't you have just trained the simple model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1509" target="_blank">00:25:09.100</a></span> | <span class="t">Or is it just that the teacher has access to some point of learning that is easier to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1514" target="_blank">00:25:14.180</a></span> | <span class="t">navigate to for a student but not for a student to get to a node?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1519" target="_blank">00:25:19.860</a></span> | <span class="t">I think something like what you just said has to be right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1524" target="_blank">00:25:24.260</a></span> | <span class="t">I actually don't- so you're asking about the special case where the teacher just does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1528" target="_blank">00:25:28.900</a></span> | <span class="t">its input output thing and produces a dataset that we train the student on, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1533" target="_blank">00:25:33.460</a></span> | <span class="t">And you're asking why is that better than just training the student on your original data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1537" target="_blank">00:25:37.660</a></span> | <span class="t">It's very mysterious to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1538" target="_blank">00:25:38.940</a></span> | <span class="t">[LAUGHTER]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1540" target="_blank">00:25:40.660</a></span> | <span class="t">I- the best metaphor I can give you is that it is a kind of regularizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1544" target="_blank">00:25:44.740</a></span> | <span class="t">So the teacher is doing something very complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1547" target="_blank">00:25:47.500</a></span> | <span class="t">and even its mistakes are useful for the student.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1550" target="_blank">00:25:50.740</a></span> | <span class="t">I guess this may be a simple way that I'm understanding it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1554" target="_blank">00:25:54.460</a></span> | <span class="t">It's okay to make certain mistakes and the teacher has figured out which mistakes you can-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1558" target="_blank">00:25:58.420</a></span> | <span class="t">[NOISE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1559" target="_blank">00:25:59.660</a></span> | <span class="t">Not worry about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1560" target="_blank">00:26:00.420</a></span> | <span class="t">I like that. I like- that's a beautiful opening line of a paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1563" target="_blank">00:26:03.900</a></span> | <span class="t">We need to make it substantive by actually explaining what that means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1567" target="_blank">00:26:07.180</a></span> | <span class="t">But it's a- I like it as a vision for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1570" target="_blank">00:26:10.380</a></span> | <span class="t">I want to be a little careful of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1573" target="_blank">00:26:13.220</a></span> | <span class="t">One more question and then I'll just wrap up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1575" target="_blank">00:26:15.060</a></span> | <span class="t">Do we have some comparisons where the student is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1577" target="_blank">00:26:17.780</a></span> | <span class="t">I mean, less- less general versus- versus the teacher?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1581" target="_blank">00:26:21.420</a></span> | <span class="t">I mean, does it- does it overfit the data in a sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1585" target="_blank">00:26:25.100</a></span> | <span class="t">more than the teacher?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1587" target="_blank">00:26:27.700</a></span> | <span class="t">The student?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1589" target="_blank">00:26:29.300</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1590" target="_blank">00:26:30.260</a></span> | <span class="t">I don't know. I mean, you would guess less if it has a tiny capacity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1593" target="_blank">00:26:33.820</a></span> | <span class="t">It won't have as much of a capacity to overfit than the teacher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1597" target="_blank">00:26:37.780</a></span> | <span class="t">And maybe that's why in some situations the students outperform the teachers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1603" target="_blank">00:26:43.180</a></span> | <span class="t">I hope that's inspiring to you all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1605" target="_blank">00:26:45.740</a></span> | <span class="t">Let me wrap up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1607" target="_blank">00:26:47.340</a></span> | <span class="t">So you can go on to outperform me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1609" target="_blank">00:26:49.540</a></span> | <span class="t">Architectures I didn't mention, Transformer, Excel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1612" target="_blank">00:26:52.340</a></span> | <span class="t">wonderful creative attempt to model long sequences by essentially creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1616" target="_blank">00:26:56.940</a></span> | <span class="t">a recurrent process across cached versions of earlier parts of the long document you're processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1624" target="_blank">00:27:04.140</a></span> | <span class="t">ExcelNet, this is a beautiful and creative attempt to use mask language modeling, sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1630" target="_blank">00:27:10.700</a></span> | <span class="t">an autoregressive language modeling objective but still have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1634" target="_blank">00:27:14.620</a></span> | <span class="t">bidirectional context and they do this by creating all these permutation orders of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1639" target="_blank">00:27:19.580</a></span> | <span class="t">the original sequence so that you can effectively condition on the left and the right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1644" target="_blank">00:27:24.020</a></span> | <span class="t">even though you can't look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1647" target="_blank">00:27:27.220</a></span> | <span class="t">And then DeBerta, this is really cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1649" target="_blank">00:27:29.460</a></span> | <span class="t">I regret not fitting this in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1650" target="_blank">00:27:30.980</a></span> | <span class="t">DeBerta is an attempt to separate out the word and positional encodings for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1655" target="_blank">00:27:35.900</a></span> | <span class="t">these models and kind of make the word embeddings more like first-class citizens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1661" target="_blank">00:27:41.100</a></span> | <span class="t">And that's very intuitive for me because that's like showing that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1664" target="_blank">00:27:44.460</a></span> | <span class="t">the model to learn some semantics for these things that's separate from their position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1669" target="_blank">00:27:49.340</a></span> | <span class="t">And they did that by reorganizing the attention mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1673" target="_blank">00:27:53.300</a></span> | <span class="t">The known limitations, we did a good job on these except for this final one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1677" target="_blank">00:27:57.940</a></span> | <span class="t">BERT assumes that the predicted tokens are all independent of each other given the unmasked tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1682" target="_blank">00:28:02.820</a></span> | <span class="t">I gave you that example of masking new in York and it thinking that both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1687" target="_blank">00:28:07.060</a></span> | <span class="t">those are independent of the other given the surrounding context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1690" target="_blank">00:28:10.460</a></span> | <span class="t">ExcelNet again addresses that and that might be something that you want to meditate on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1696" target="_blank">00:28:16.140</a></span> | <span class="t">Pre-training data, here's a whole mess of resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1699" target="_blank">00:28:19.700</a></span> | <span class="t">If you did want to pre-train your own model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1701" target="_blank">00:28:21.420</a></span> | <span class="t">maybe Sid will talk more about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1703" target="_blank">00:28:23.260</a></span> | <span class="t">I'm offering these primarily because I think you might want to audit them as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1706" target="_blank">00:28:26.900</a></span> | <span class="t">you observe strange behavior from your large models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1709" target="_blank">00:28:29.860</a></span> | <span class="t">The data might be the key to figuring out where that behavior came from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1714" target="_blank">00:28:34.500</a></span> | <span class="t">And then finally, current trends, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1716" target="_blank">00:28:36.700</a></span> | <span class="t">Autoregressive architecture seem to have taken over,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1719" target="_blank">00:28:39.340</a></span> | <span class="t">but that could be just because everyone is so focused on generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1722" target="_blank">00:28:42.940</a></span> | <span class="t">I have an intuition that models like BERT are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1726" target="_blank">00:28:46.540</a></span> | <span class="t">better if you just want to represent examples as opposed to doing generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1730" target="_blank">00:28:50.900</a></span> | <span class="t">Seek-to-seek is still a dominant choice for tasks with that structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1734" target="_blank">00:28:54.780</a></span> | <span class="t">Although again, 0.1 might be pushing everyone to just use GPT-3 or 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1740" target="_blank">00:29:00.500</a></span> | <span class="t">even for models with seek-to-seek structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1742" target="_blank">00:29:02.840</a></span> | <span class="t">We'll see how that plays out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1744" target="_blank">00:29:04.380</a></span> | <span class="t">And then people are still obsessed with scaling up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1746" target="_blank">00:29:06.860</a></span> | <span class="t">But we might be seeing a counter movement towards smaller models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1750" target="_blank">00:29:10.380</a></span> | <span class="t">especially with reinforcement learning with human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1753" target="_blank">00:29:13.340</a></span> | <span class="t">And that's something that we're going to talk about next week and the week after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1757" target="_blank">00:29:17.660</a></span> | <span class="t">So I kind of restructured a little bit of my talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1762" target="_blank">00:29:22.460</a></span> | <span class="t">So like we're only going to get through like part one today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1764" target="_blank">00:29:24.580</a></span> | <span class="t">which is actually back to basics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1766" target="_blank">00:29:26.700</a></span> | <span class="t">how transformers work, and then we're going to talk about the other stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1770" target="_blank">00:29:30.180</a></span> | <span class="t">I should maybe introduce myself. I'm Sid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1773" target="_blank">00:29:33.140</a></span> | <span class="t">I am a fourth year PhD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1774" target="_blank">00:29:34.900</a></span> | <span class="t">I actually work primarily on language for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1777" target="_blank">00:29:37.700</a></span> | <span class="t">robotics and kind of channeling one of the core concepts of the class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1780" target="_blank">00:29:40.820</a></span> | <span class="t">It's all about doing a whole lot with very, very little.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1784" target="_blank">00:29:44.420</a></span> | <span class="t">Like I'm really just working on how we get robots to follow instructions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1789" target="_blank">00:29:49.740</a></span> | <span class="t">given just like one example of a human,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1791" target="_blank">00:29:51.740</a></span> | <span class="t">you know, opening a fridge or pouring coffee, things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1796" target="_blank">00:29:56.620</a></span> | <span class="t">But in kind of doing that research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1799" target="_blank">00:29:59.060</a></span> | <span class="t">it became really, really clear that we needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1800" target="_blank">00:30:00.860</a></span> | <span class="t">better raw materials, better starting points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1803" target="_blank">00:30:03.140</a></span> | <span class="t">So I started working on pre-training, first in language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1806" target="_blank">00:30:06.220</a></span> | <span class="t">and then more recently in vision, video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1808" target="_blank">00:30:08.980</a></span> | <span class="t">and robotics with language, kind of as the central theme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1812" target="_blank">00:30:12.380</a></span> | <span class="t">So I want to talk today about fantastic language models and how to build them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1817" target="_blank">00:30:17.100</a></span> | <span class="t">So Richard Feynman is probably not only one of the greatest physicists of all time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1821" target="_blank">00:30:21.740</a></span> | <span class="t">but he's one of the greatest educators of all time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1824" target="_blank">00:30:24.500</a></span> | <span class="t">one of the greatest science educators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1826" target="_blank">00:30:26.420</a></span> | <span class="t">And he has this quote, "What I cannot create, I do not understand."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1831" target="_blank">00:30:31.620</a></span> | <span class="t">And for him, it was really just kind of about building blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1834" target="_blank">00:30:34.340</a></span> | <span class="t">How do I understand what is going on at the lowest level,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1837" target="_blank">00:30:37.060</a></span> | <span class="t">so I can compose them together and figure out what to do next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1840" target="_blank">00:30:40.300</a></span> | <span class="t">Where is the next innovation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1841" target="_blank">00:30:41.500</a></span> | <span class="t">Where is the next discovery come from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1843" target="_blank">00:30:43.340</a></span> | <span class="t">And so kind of with that in mind,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1845" target="_blank">00:30:45.620</a></span> | <span class="t">I actually just want to spend the next 12 or so minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1848" target="_blank">00:30:48.740</a></span> | <span class="t">talking about building language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1851" target="_blank">00:30:51.140</a></span> | <span class="t">building transformers, and how that all happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1853" target="_blank">00:30:53.420</a></span> | <span class="t">So it's a practical take on these large-scale language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1857" target="_blank">00:30:57.380</a></span> | <span class="t">We're really not going to get to the large-scale bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1859" target="_blank">00:30:59.500</a></span> | <span class="t">And we're going to get to the full pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1861" target="_blank">00:31:01.300</a></span> | <span class="t">Again, we're only going to focus on the model architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1862" target="_blank">00:31:02.860</a></span> | <span class="t">but today, the evolution of the transformer, how we got there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1866" target="_blank">00:31:06.660</a></span> | <span class="t">Training at scale, we'll probably cover some other time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1870" target="_blank">00:31:10.340</a></span> | <span class="t">And then we'll talk about very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1872" target="_blank">00:31:12.540</a></span> | <span class="t">very briefly efficient fine-tuning and inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1874" target="_blank">00:31:14.340</a></span> | <span class="t">And we have some other great CAs who might actually be talking about this more in depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1878" target="_blank">00:31:18.300</a></span> | <span class="t">But the punchline is the last few years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1881" target="_blank">00:31:21.860</a></span> | <span class="t">like I started my PhD in 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1884" target="_blank">00:31:24.380</a></span> | <span class="t">I trained my first deep learning MNIST model in 2018.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1889" target="_blank">00:31:29.580</a></span> | <span class="t">Feels changed a lot since then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1891" target="_blank">00:31:31.380</a></span> | <span class="t">And with every new model, with every new GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1894" target="_blank">00:31:34.780</a></span> | <span class="t">one, two, three, four, the five that's training right now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1899" target="_blank">00:31:39.260</a></span> | <span class="t">there's been more and more folk knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1901" target="_blank">00:31:41.260</a></span> | <span class="t">things that are hidden from plain sight that we never get to see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1905" target="_blank">00:31:45.380</a></span> | <span class="t">And it's been the job of people like us, students,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1909" target="_blank">00:31:49.020</a></span> | <span class="t">people in academia to kind of rediscover,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1911" target="_blank">00:31:51.580</a></span> | <span class="t">find the insights, find the intuition behind these ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1914" target="_blank">00:31:54.460</a></span> | <span class="t">And in kind of rediscovering those pipelines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1916" target="_blank">00:31:56.860</a></span> | <span class="t">it's actually our comparative advantage and figuring out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1920" target="_blank">00:32:00.780</a></span> | <span class="t">okay, so these are how these pieces came to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1923" target="_blank">00:32:03.540</a></span> | <span class="t">What do I do next? And so I don't really care about time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1928" target="_blank">00:32:08.060</a></span> | <span class="t">If we get through five slides,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1929" target="_blank">00:32:09.740</a></span> | <span class="t">that is a success for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1931" target="_blank">00:32:11.300</a></span> | <span class="t">But be selfish, like this is your class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1933" target="_blank">00:32:13.860</a></span> | <span class="t">So if you have any questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1935" target="_blank">00:32:15.340</a></span> | <span class="t">if I say anything you don't understand, that's the contract.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1937" target="_blank">00:32:17.420</a></span> | <span class="t">Call me out, ask a question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1939" target="_blank">00:32:19.260</a></span> | <span class="t">and we're just going to kind of go step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1941" target="_blank">00:32:21.300</a></span> | <span class="t">So how did we get to the transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1945" target="_blank">00:32:25.580</a></span> | <span class="t">How did this become the bedrock of language modeling?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1948" target="_blank">00:32:28.940</a></span> | <span class="t">And now, vision, also video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1951" target="_blank">00:32:31.900</a></span> | <span class="t">also robotics for some reason as of late.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1954" target="_blank">00:32:34.780</a></span> | <span class="t">How did we get here? So what is the recipe for a good language model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1959" target="_blank">00:32:39.460</a></span> | <span class="t">We've talked a bit about contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1961" target="_blank">00:32:41.740</a></span> | <span class="t">Chris was talking through kind of the various,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1963" target="_blank">00:32:43.420</a></span> | <span class="t">you know, different phase changes in language modeling history.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1965" target="_blank">00:32:45.900</a></span> | <span class="t">Lisa was talking about diffusion language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1967" target="_blank">00:32:47.300</a></span> | <span class="t">which is a completely different perspective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1970" target="_blank">00:32:50.020</a></span> | <span class="t">I'm going to kind of simplify things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1972" target="_blank">00:32:52.140</a></span> | <span class="t">oversimplify things, like two steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1974" target="_blank">00:32:54.260</a></span> | <span class="t">I need massive amounts of cheap, easy to acquire data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1977" target="_blank">00:32:57.300</a></span> | <span class="t">We're a language model and we're building these contextual representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1980" target="_blank">00:33:00.260</a></span> | <span class="t">because we want to learn patterns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1981" target="_blank">00:33:01.500</a></span> | <span class="t">we want to learn truths about the world from data at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1985" target="_blank">00:33:05.540</a></span> | <span class="t">And to do that at scale, we need data at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1988" target="_blank">00:33:08.580</a></span> | <span class="t">So that's one component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1990" target="_blank">00:33:10.580</a></span> | <span class="t">And the other is we need a simple and high throughput way to consume it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1994" target="_blank">00:33:14.940</a></span> | <span class="t">So what does that mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=1998" target="_blank">00:33:18.060</a></span> | <span class="t">We need to be able to chew through all of this data as fast as we possibly can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2002" target="_blank">00:33:22.660</a></span> | <span class="t">in the least opinionated way to figure out, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2006" target="_blank">00:33:26.540</a></span> | <span class="t">all of the possible patterns, all of the possible things that could be useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2009" target="_blank">00:33:29.900</a></span> | <span class="t">for people fine-tuning, generating, using these models for arbitrary things downstream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2015" target="_blank">00:33:35.380</a></span> | <span class="t">This isn't just applicable to language, it's applicable to pretty much everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2018" target="_blank">00:33:38.580</a></span> | <span class="t">So vision does this, video does this, video and language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2021" target="_blank">00:33:41.820</a></span> | <span class="t">vision and language, language and robotics, all of them follow a similar strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2028" target="_blank">00:33:48.100</a></span> | <span class="t">So, right, so simple in that it's natural to scale the approach with data as we get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2033" target="_blank">00:33:53.300</a></span> | <span class="t">you know, go from 300 billion to 600 billion tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2036" target="_blank">00:33:56.340</a></span> | <span class="t">maybe make the model bigger to handle that in a pretty simple way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2039" target="_blank">00:33:59.340</a></span> | <span class="t">The model should be composable in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2042" target="_blank">00:34:02.460</a></span> | <span class="t">The training, the way that we actually ingest this data should be fast and parallelizable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2047" target="_blank">00:34:07.940</a></span> | <span class="t">and we should be, you know, making the most of our hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2050" target="_blank">00:34:10.060</a></span> | <span class="t">If we're going to run a data center with, I don't know, 512 GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2054" target="_blank">00:34:14.340</a></span> | <span class="t">with each 8 GPU box costing $120,000, I'd better be getting my money's worth at the end of the day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2060" target="_blank">00:34:20.740</a></span> | <span class="t">And the consumption part, right, like this minimal assumptions on relationships,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2065" target="_blank">00:34:25.940</a></span> | <span class="t">the less opinionated I am about how different parts of my data is connected,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2070" target="_blank">00:34:30.540</a></span> | <span class="t">the more I can learn given the first thing, massive amounts of data at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2076" target="_blank">00:34:36.980</a></span> | <span class="t">So, kind of like figure out how we got to the transformer, I want to kind of wind time back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2081" target="_blank">00:34:41.700</a></span> | <span class="t">to kind of what Chris was alluding to earlier with RNNs, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2086" target="_blank">00:34:46.020</a></span> | <span class="t">So, this is an RNN model, kind of complicated, but it's from 224M.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2091" target="_blank">00:34:51.420</a></span> | <span class="t">I took it literally from their slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2093" target="_blank">00:34:53.540</a></span> | <span class="t">I hope John doesn't get mad at me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2096" target="_blank">00:34:56.500</a></span> | <span class="t">And it's this very powerful class of model in theory, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2100" target="_blank">00:35:00.820</a></span> | <span class="t">I am ingesting arbitrary length sequences left to right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2104" target="_blank">00:35:04.460</a></span> | <span class="t">and I'm learning arbitrary patterns around them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2107" target="_blank">00:35:07.820</a></span> | <span class="t">People decided later on to, you know, add these attention mechanisms on top of the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2113" target="_blank">00:35:13.340</a></span> | <span class="t">to sequence RNN models to figure out like how to kind of sharpen their focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2117" target="_blank">00:35:17.420</a></span> | <span class="t">as they were decoding token by token, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2119" target="_blank">00:35:19.940</a></span> | <span class="t">So, the strengths are like I get to handle arbitrary long context and like we see kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2123" target="_blank">00:35:23.700</a></span> | <span class="t">of the first semblance of attention appear kind of very motivated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2127" target="_blank">00:35:27.980</a></span> | <span class="t">by like the way we do language translation, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2130" target="_blank">00:35:30.020</a></span> | <span class="t">Like when I'm translating word by word, there are certain words in the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2132" target="_blank">00:35:32.580</a></span> | <span class="t">that are going to matter, I'm going to sharpen my focus too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2135" target="_blank">00:35:35.900</a></span> | <span class="t">But there are issues with RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2137" target="_blank">00:35:37.540</a></span> | <span class="t">They're not the most scalable, producing the next token requires me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2141" target="_blank">00:35:41.380</a></span> | <span class="t">to produce every single token beforehand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2144" target="_blank">00:35:44.260</a></span> | <span class="t">I can't really make them deeper without training stability going to pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2149" target="_blank">00:35:49.180</a></span> | <span class="t">So, that's rough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2151" target="_blank">00:35:51.140</a></span> | <span class="t">And so, chewing through a large amount of data with an RNN is hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2154" target="_blank">00:35:54.940</a></span> | <span class="t">Some people refuse to believe that and they've actually done immense work in trying to scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2159" target="_blank">00:35:59.740</a></span> | <span class="t">up RNNs, make them more parallelizable using lots and lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2161" target="_blank">00:36:01.900</a></span> | <span class="t">of really cool linear algebra tricks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2163" target="_blank">00:36:03.540</a></span> | <span class="t">I'll post some links.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2164" target="_blank">00:36:04.940</a></span> | <span class="t">And then separately, kind of from the vision community that kind of bled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2169" target="_blank">00:36:09.820</a></span> | <span class="t">into the language community, we have convolutional neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2173" target="_blank">00:36:13.100</a></span> | <span class="t">And this is from a other course from Lena Vojta about using CNNs for language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2181" target="_blank">00:36:21.500</a></span> | <span class="t">And the idea here is we have this ability to do immense, deep, parallelizable training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2189" target="_blank">00:36:29.060</a></span> | <span class="t">by kind of taking these like little windows, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2192" target="_blank">00:36:32.180</a></span> | <span class="t">I'm going to look at, you know, each layer is only going to look at like the, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2194" target="_blank">00:36:34.540</a></span> | <span class="t">three word contexts at a time and going to give me representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2197" target="_blank">00:36:37.460</a></span> | <span class="t">But if I stack this enough times and I have these residual connections that combine, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2202" target="_blank">00:36:42.220</a></span> | <span class="t">earlier inputs with later inputs, by the time I'm like 10 layers deep,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2205" target="_blank">00:36:45.700</a></span> | <span class="t">I've seen everything in the window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2207" target="_blank">00:36:47.740</a></span> | <span class="t">But I need that depth and that's kind of a drawback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2212" target="_blank">00:36:52.260</a></span> | <span class="t">But there are like really cool, powerful ideas here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2214" target="_blank">00:36:54.180</a></span> | <span class="t">And I'd actually say that the transformers have way more to do with CNNs and the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2217" target="_blank">00:36:57.540</a></span> | <span class="t">that they behave than the way RNNs behave, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2220" target="_blank">00:37:00.940</a></span> | <span class="t">So we have this idea of a CNN layer kind of having multiple filters, multiple kernels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2226" target="_blank">00:37:06.380</a></span> | <span class="t">different ways of looking at and extracting features from an image or features from text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2232" target="_blank">00:37:12.340</a></span> | <span class="t">You have, you know, this ability to kind of scale depth using these residual connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2237" target="_blank">00:37:17.380</a></span> | <span class="t">The deepest networks that we had, you know, from 2012, 2015,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2241" target="_blank">00:37:21.460</a></span> | <span class="t">even now are still vision models, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2243" target="_blank">00:37:23.940</a></span> | <span class="t">ResNet 151 isn't called 151 because it's, you know, the 151st edition of the ResNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2249" target="_blank">00:37:29.540</a></span> | <span class="t">It's 151 because it's 151 layers deep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2252" target="_blank">00:37:32.100</a></span> | <span class="t">It's actually 151 blocks deep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2254" target="_blank">00:37:34.020</a></span> | <span class="t">Layers, it's actually like probably 4x that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2257" target="_blank">00:37:37.980</a></span> | <span class="t">And it's parallelizable, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2259" target="_blank">00:37:39.700</a></span> | <span class="t">Every little window that I see at every layer can be computed completely independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2264" target="_blank">00:37:44.380</a></span> | <span class="t">of every other layer, which is really, really great for modern hardware, modern GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2271" target="_blank">00:37:51.460</a></span> | <span class="t">So looking at this, seems like CNNs are cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2275" target="_blank">00:37:55.140</a></span> | <span class="t">Seems like RNNs are cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2277" target="_blank">00:37:57.060</a></span> | <span class="t">There's a natural question, which is like how do you do better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2280" target="_blank">00:38:00.700</a></span> | <span class="t">This is the picture from Chris's slides that Lisa also used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2283" target="_blank">00:38:03.700</a></span> | <span class="t">This is a very scary looking picture, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2287" target="_blank">00:38:07.060</a></span> | <span class="t">Like what does self-attention mean in a transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2290" target="_blank">00:38:10.340</a></span> | <span class="t">Where do those ideas come from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2292" target="_blank">00:38:12.500</a></span> | <span class="t">So one idea, like one key component, like one missing component for how you get from a CNN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2299" target="_blank">00:38:19.180</a></span> | <span class="t">to an RNN and an RNN to a transformer is the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2303" target="_blank">00:38:23.140</a></span> | <span class="t">that each individual token is its own query key and value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2307" target="_blank">00:38:27.220</a></span> | <span class="t">It's its own entity that can be used to shape the representations of all of the other tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2313" target="_blank">00:38:33.500</a></span> | <span class="t">Right. So I'm going to turn this word "the" into its own query key and value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2316" target="_blank">00:38:36.500</a></span> | <span class="t">I'm going to use the attention from the RNNs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2319" target="_blank">00:38:39.540</a></span> | <span class="t">and I'm going to use the depth parallelizability scaling from the CNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2324" target="_blank">00:38:44.940</a></span> | <span class="t">And then the multi-headed part of self-attention is exactly like what the, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2329" target="_blank">00:38:49.380</a></span> | <span class="t">different convolutional filters, the different kernels are doing in a CNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2333" target="_blank">00:38:53.340</a></span> | <span class="t">It's giving you different perspectives on that same token, different ways to come up with queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2337" target="_blank">00:38:57.780</a></span> | <span class="t">different insights into like how we can use, you know, a single token and come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2341" target="_blank">00:39:01.900</a></span> | <span class="t">up with multiple different representations and fuse them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2345" target="_blank">00:39:05.940</a></span> | <span class="t">As a code, code is semi-unimportant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2349" target="_blank">00:39:09.580</a></span> | <span class="t">It's a kind of very terse description of like what multi-headed self-attention looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2353" target="_blank">00:39:13.900</a></span> | <span class="t">Key parts here are this, you know, little bit where we kind of project an input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2359" target="_blank">00:39:19.020</a></span> | <span class="t">of tokens to the queries, keys, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2361" target="_blank">00:39:21.020</a></span> | <span class="t">And then we're just going to rearrange them kind of in some way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2364" target="_blank">00:39:24.940</a></span> | <span class="t">And the important part here is that like we are rearranging them in a way that splits them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2369" target="_blank">00:39:29.380</a></span> | <span class="t">into these different views, these different heads, where each head has some fixed dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2374" target="_blank">00:39:34.060</a></span> | <span class="t">which is like the key dimension of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2376" target="_blank">00:39:36.620</a></span> | <span class="t">And then we have these query keys and values that we're then going to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2380" target="_blank">00:39:40.580</a></span> | <span class="t">for this attention operation which comes directly from the RNN literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2384" target="_blank">00:39:44.740</a></span> | <span class="t">Right? It is really just this dot product between queries and keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2388" target="_blank">00:39:48.820</a></span> | <span class="t">That is a very complicated way of saying that's a matrix multiply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2393" target="_blank">00:39:53.300</a></span> | <span class="t">And then we're going to just project them and combine them back into our tensor of, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2399" target="_blank">00:39:59.700</a></span> | <span class="t">batch size by sequence length by embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2403" target="_blank">00:40:03.340</a></span> | <span class="t">>> There's a nice subtlety here I think that caught me off guard at one point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2408" target="_blank">00:40:08.620</a></span> | <span class="t">When you look at these models, like you download BERT and it says multi-headed attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2412" target="_blank">00:40:12.860</a></span> | <span class="t">or whatever, there's only one set of weights even though it's multi-headed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2417" target="_blank">00:40:17.780</a></span> | <span class="t">Do you want to unpack that for us a little bit?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2419" target="_blank">00:40:19.980</a></span> | <span class="t">>> Yeah. So the convolutional kernel has kind of this really nice way of expressing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2426" target="_blank">00:40:26.940</a></span> | <span class="t">like I have multiple resolutions of an image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2429" target="_blank">00:40:29.300</a></span> | <span class="t">Right? It's like that depth channel of a convolutional filter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2432" target="_blank">00:40:32.300</a></span> | <span class="t">And if you can unpack like the conv 2D layer in PyTorch, you kind of see that come out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2438" target="_blank">00:40:38.140</a></span> | <span class="t">You don't really see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2438" target="_blank">00:40:38.860</a></span> | <span class="t">You see just like this one big weight matrix that is literally like this, you know, dimensionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2443" target="_blank">00:40:43.900</a></span> | <span class="t">you know, embed dimension by three times embed dimension is usually what it is in BERT or GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2449" target="_blank">00:40:49.340</a></span> | <span class="t">That three is the way you split it into queries, keys, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2455" target="_blank">00:40:55.060</a></span> | <span class="t">But we're actually going to kind of like take that vector that is like, you know, embed dim size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2460" target="_blank">00:41:00.300</a></span> | <span class="t">and just chunk it up into each of these different filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2463" target="_blank">00:41:03.300</a></span> | <span class="t">Right? So rather than make those filters explicit as by providing them as a parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2467" target="_blank">00:41:07.620</a></span> | <span class="t">that defines some weight layer, for efficiency purposes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2470" target="_blank">00:41:10.540</a></span> | <span class="t">we're actually just going to treat it all as one matrix and then just chunk it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2473" target="_blank">00:41:13.580</a></span> | <span class="t">up as we're doing the linear algebra operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2477" target="_blank">00:41:17.140</a></span> | <span class="t">Does that make sense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2478" target="_blank">00:41:18.620</a></span> | <span class="t">>> So you're chunking it twice, the times three is queries, keys, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2482" target="_blank">00:41:22.420</a></span> | <span class="t">And then number of heads is further chunking each one of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2485" target="_blank">00:41:25.260</a></span> | <span class="t">>> Yeah. So one of the kind of like the key rules that like no one ever tells you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2488" target="_blank">00:41:28.820</a></span> | <span class="t">about transformers is that your number of heads has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2491" target="_blank">00:41:31.940</a></span> | <span class="t">to evenly divide your transformer hidden dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2495" target="_blank">00:41:35.860</a></span> | <span class="t">That's usually a check that is explicitly done in the code for training like BERT or GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2500" target="_blank">00:41:40.580</a></span> | <span class="t">And usually the code doesn't work if that doesn't happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2503" target="_blank">00:41:43.220</a></span> | <span class="t">And that's kind of how you get away with a lot of these evisions tricks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2506" target="_blank">00:41:46.140</a></span> | <span class="t">It's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2506" target="_blank">00:41:46.780</a></span> | <span class="t">>> So we should do a whole course on broadcasting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2509" target="_blank">00:41:49.380</a></span> | <span class="t">>> Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2509" target="_blank">00:41:49.700</a></span> | <span class="t">>> Before you even start this so that you can do this mess of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2512" target="_blank">00:41:52.740</a></span> | <span class="t">>> Yeah. And there's a great professor at Cornell Tech, Sasha Rush, who kind of has like a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2517" target="_blank">00:41:57.380</a></span> | <span class="t">of like tutorials on just like basic like broadcasting tensor operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2520" target="_blank">00:42:00.340</a></span> | <span class="t">It's fantastic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2521" target="_blank">00:42:01.100</a></span> | <span class="t">They should check out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2522" target="_blank">00:42:02.180</a></span> | <span class="t">There's a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2523" target="_blank">00:42:03.020</a></span> | <span class="t">>> Yeah. Can you just clarify what you mean by chunking?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2525" target="_blank">00:42:05.220</a></span> | <span class="t">>> Yeah. So if I have a vector of let's say length 1024, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2530" target="_blank">00:42:10.500</a></span> | <span class="t">That is my embedding dimensions, the hidden dimension for my transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2533" target="_blank">00:42:13.700</a></span> | <span class="t">And if I have keys of say dimension let's say two, make it easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2539" target="_blank">00:42:19.980</a></span> | <span class="t">Chunking just means that I'm going to split that vector of 1024</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2543" target="_blank">00:42:23.020</a></span> | <span class="t">into two heads each of dimension 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2546" target="_blank">00:42:26.020</a></span> | <span class="t">Right? So I'm literally just going to like reshape that vector and chunk them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2549" target="_blank">00:42:29.700</a></span> | <span class="t">up into like two views of the same input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2554" target="_blank">00:42:34.580</a></span> | <span class="t">Cool. Is this actually better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2558" target="_blank">00:42:38.060</a></span> | <span class="t">Is this alone enough to define a transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2562" target="_blank">00:42:42.580</a></span> | <span class="t">Maybe. Maybe not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2564" target="_blank">00:42:44.340</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2564" target="_blank">00:42:44.660</a></span> | <span class="t">The answer is no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2565" target="_blank">00:42:45.940</a></span> | <span class="t">So it's good because like we get all of the parallelization advantages and all of kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2570" target="_blank">00:42:50.700</a></span> | <span class="t">of the attention advantages that I talked about on the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2573" target="_blank">00:42:53.060</a></span> | <span class="t">This is a slide from like a Justin Johnson and Dante Zhu who are both ex-Stanford alums now teaching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2578" target="_blank">00:42:58.700</a></span> | <span class="t">courses about transformers and deep learning at various colleges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2582" target="_blank">00:43:02.820</a></span> | <span class="t">But you're missing kind of like one key component, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2588" target="_blank">00:43:08.460</a></span> | <span class="t">So if you just look at this and squint at this for like a little bit of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2594" target="_blank">00:43:14.180</a></span> | <span class="t">what you're missing is like, okay, so I am just taking different weighted averages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2598" target="_blank">00:43:18.260</a></span> | <span class="t">of the same underlying values over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2604" target="_blank">00:43:24.940</a></span> | <span class="t">Relative to the things that are coming out of my transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2607" target="_blank">00:43:27.300</a></span> | <span class="t">there actually is no non-linearity, but just self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2611" target="_blank">00:43:31.500</a></span> | <span class="t">This is basically a glorified linear network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2616" target="_blank">00:43:36.180</a></span> | <span class="t">So we need some way to fix that because that's where the expressivity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2620" target="_blank">00:43:40.740</a></span> | <span class="t">the kind of the magic of deep learning happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2622" target="_blank">00:43:42.420</a></span> | <span class="t">It's kind of when we stack these non-linearities, go deeper and learn new patterns at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2628" target="_blank">00:43:48.780</a></span> | <span class="t">So this is how we do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2632" target="_blank">00:43:52.100</a></span> | <span class="t">We had an MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2633" target="_blank">00:43:53.220</a></span> | <span class="t">We had an MLP to the very end of the transformer block and it's very simple, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2637" target="_blank">00:43:57.180</a></span> | <span class="t">It all it does is it kind of takes the embedding dimension that comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2639" target="_blank">00:43:59.820</a></span> | <span class="t">out of the self-attention block that we just defined,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2642" target="_blank">00:44:02.340</a></span> | <span class="t">projects it to a higher dimensional space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2646" target="_blank">00:44:06.060</a></span> | <span class="t">adds a value on linearity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2648" target="_blank">00:44:08.180</a></span> | <span class="t">and then down projects it back to the embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2651" target="_blank">00:44:11.020</a></span> | <span class="t">Usually what you're going to see is a factor of four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2652" target="_blank">00:44:12.660</a></span> | <span class="t">Why is it a factor of four?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2654" target="_blank">00:44:14.300</a></span> | <span class="t">No one knows is the honest answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2657" target="_blank">00:44:17.100</a></span> | <span class="t">Two didn't seem to work well enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2658" target="_blank">00:44:18.900</a></span> | <span class="t">Eight seemed to be too big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2660" target="_blank">00:44:20.340</a></span> | <span class="t">But here's some like soft intuition for kind of why this might work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2664" target="_blank">00:44:24.660</a></span> | <span class="t">This kind of is a throwback to like 229, you know, OGML days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2670" target="_blank">00:44:30.060</a></span> | <span class="t">So you want your network as a whole to be able to kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2674" target="_blank">00:44:34.660</a></span> | <span class="t">both forget the things that are unimportant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2677" target="_blank">00:44:37.900</a></span> | <span class="t">but also remember the things that are important, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2679" target="_blank">00:44:39.820</a></span> | <span class="t">That's kind of the role. So the sharpening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2681" target="_blank">00:44:41.500</a></span> | <span class="t">the remembering that are important are these residual connections,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2683" target="_blank">00:44:43.980</a></span> | <span class="t">like the fact that I'm adding X to some transform of X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2687" target="_blank">00:44:47.260</a></span> | <span class="t">The forgetting is what this MLP is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2689" target="_blank">00:44:49.860</a></span> | <span class="t">It's basically saying like what stuff can I throw away and should I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2693" target="_blank">00:44:53.140</a></span> | <span class="t">basically forget because it's not really relevant to what I care about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2696" target="_blank">00:44:56.460</a></span> | <span class="t">at the end of the day, which are good contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2699" target="_blank">00:44:59.100</a></span> | <span class="t">And so the role of the MLP is very similar to the role of kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2703" target="_blank">00:45:03.620</a></span> | <span class="t">kernel and you know the good old support vector machine literature, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2707" target="_blank">00:45:07.900</a></span> | <span class="t">So if I have two classes that are kind of like this in a plane,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2711" target="_blank">00:45:11.740</a></span> | <span class="t">and I want to draw a line that partitions them, how do I do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2715" target="_blank">00:45:15.540</a></span> | <span class="t">Well, it's hard if I'm only working in 2D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2718" target="_blank">00:45:18.460</a></span> | <span class="t">But with just a very simple learn transform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2721" target="_blank">00:45:21.300</a></span> | <span class="t">if I just implicitly lift these things up to like 3D,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2725" target="_blank">00:45:25.220</a></span> | <span class="t">I can turn this into a surface in 3D that can just cut in half,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2728" target="_blank">00:45:28.900</a></span> | <span class="t">separates my stuff. So projecting up with this MLP is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2732" target="_blank">00:45:32.700</a></span> | <span class="t">this way of kind of like aligning or crystallizing the structure of our features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2736" target="_blank">00:45:36.420</a></span> | <span class="t">learning a good decision boundary in space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2738" target="_blank">00:45:38.780</a></span> | <span class="t">and compressing from there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2740" target="_blank">00:45:40.780</a></span> | <span class="t">I think we're out of time, so we're going to go through like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2742" target="_blank">00:45:42.700</a></span> | <span class="t">the rest of the transformer evolution in a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2745" target="_blank">00:45:45.020</a></span> | <span class="t">But all the slides are up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2746" target="_blank">00:45:46.540</a></span> | <span class="t">I have office hours tomorrow and I will be back. Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2751" target="_blank">00:45:51.260</a></span> | <span class="t">>> Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2752" target="_blank">00:45:52.260</a></span> | <span class="t">[ Applause ]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=4-kuJpVrr7M&t=2752" target="_blank">00:45:52.260</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>