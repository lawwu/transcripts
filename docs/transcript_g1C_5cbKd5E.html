<html><head><title>Ep6. AI Demand / Supply - Models, Agents, the $2T Compute Build Out, Need for More Nuclear & More</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Ep6. AI Demand / Supply - Models, Agents, the $2T Compute Build Out, Need for More Nuclear & More</h2><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E" target="_blank"><img src="https://i.ytimg.com/vi/g1C_5cbKd5E/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E&t=0 target="_blank"">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E&t=169 target="_blank"">2:49</a> Demand for Compute Is Virtually Unlimited<br><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E&t=1673 target="_blank"">27:53</a> Everywhere we Look we are building more and bigger Supercomputers<br><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E&t=2149 target="_blank"">35:49</a> The AI Induced Power Gap & The Need for Nuclear<br><a href="https://www.youtube.com/watch?v=g1C_5cbKd5E&t=3021 target="_blank"">50:21</a> Valuations / Market Check<br><h3>Transcript</h3><div class='max-width'><p>If World War II never happened, the atomic bomb never happened, and someone just showed up in 2024 and said, "I figured out this thing, it's nuclear fission," people would be like, "Oh my god!" Like, probably be more excited than you are about AI, right? Like, because it would solve all of our problems, and...</p><p>Hey man, what's going on? Nice hat. How you doing? Hook 'em horns. Definitely. I mean, is this a little promotion for the state of Texas we got going on? Yeah, yeah, I'm proud of the state of Texas, and a little bit, you know, I'm sad that the... especially watching the women's tournament quite a bit, and Texas just came this close to the Final Four, but the games last night with Iowa and LSU were just amazing.</p><p>So anyway, it's been a fun time. So yeah, reminiscing perhaps a little bit. March is a good month. Well, speaking of reminiscing, I've been reminiscing, you know, I'm on spring break with my son, and we see these markets around AI getting... there's a lot of commentary about how frothy they're getting.</p><p>And it reminded me of that question, you know, it's different this time. You know, people say the four most dangerous words in the investing universe, but yet every big breakthrough we've gone through in tech, it actually has been different, right? And so as analysts, as anthropologists, as forecasters, right, we have to try to sort this out, both in the short run and the long run, right?</p><p>And it's hard to do. I mean, you do this bottoms up, you do this tops down, you study it. Lots of people in 1998 knew the internet was going to be massive, right? I mean, Henry Blodgett calling Amazon 400, right? People thought it was blasphemous, but it didn't stop us from having a boom and a bust along the way, right?</p><p>And today Amazon's at 3000 bucks, almost 10X what Henry, you know, got shouted down for saying in 1998. And I think at the time he said it was a 10-year call. But the fact of the matter is trying to marry up the short and the long-term, I think, is really, really tough.</p><p>Well, and I think it gets even tougher if enthusiasm builds, because that impacts the entry price on the marginal investment that one might make. And I think this particular moment in time is very, very difficult for investors that are looking at the marginal investment, because the prices infer some amount of optimism already.</p><p>For sure. And so that's really what I think we're going to dig into a little bit today. Tap on this and go a little bit deeper. Maybe starting with this idea is there is increasing evidence that demand for training and inferences is maybe deeper and wider than we thought.</p><p>You know, we're reading a lot of headlines about the world building ever bigger supercomputers, and then a lot of conversation about what those bottlenecks become. But why don't we just start with this question about why do we need bigger, right? And I guess first principles, you know, generative AI produces these tokens.</p><p>These tokens are a proxy for human intelligence. And there's a lot of conversation about there's no limit to how much incremental human intelligence we want to buy, right? You and I've talked a fair bit about co-pilots for engineering. We talked about that with Dara, co-pilots for call centers. And in a pretty short period of time, these have become really ubiquitous development projects for almost every enterprise in just a few short years wanting these co-pilots.</p><p>And now we're seeing a lot of conversation about autonomous agents. I mean, I think Benchmark is investors in Lang chain. Harrison Chase gave a great talk last week about what's next for AI agents that I encourage folks to watch around planning, UI, and memory. And, you know, we hosted an AI dinner last week, and there was an interesting conversation that came up just about a search use case within the enterprise.</p><p>And in this particular company, the CTO took their million, well, 100,000 lines of code, their entire code base, about a million tokens, dropped it all into the prompt, and then just asked if it spotted any common bugs. And he said it found six, like non-trivial bugs in the code base.</p><p>So the point is, we've gone from co-pilot to some of these maybe a little bit more autonomous systems, magic and cognition, all of this in less than two years. And let me ask you the question, if we compare this to 1998, like looking at just demand, you know, do you think this demand is as real as it appears?</p><p>And what do you think are the most interesting use cases you're seeing from a demand perspective today? I think it's super difficult to unpack, not opting out of the question, but, you know, you and I went deep on the topic of full self-driving at Tesla and Model 12. And based on what we were told and what we learned, that's an application use case of AI that's pretty amazing, you know, coded differently than it had been for the past decade, and has results that people are claiming they can just feel is demonstrably better.</p><p>And we've talked about the use case like coding. We've also talked about the fact that when you move away from coding, the efficacy drops a bit in terms of the productivity gain you might get. And I've watched, you know, certain companies say, oh, we're going to use AI, or I think they mean LLMs, but we're going to use LLMs to improve workflows in every aspect of our business.</p><p>And when I've circled back with those people and say, did it change every aspect of your business? They come off of it a little bit. So, and one thing that I think is really different between the full self-driving data point, and it's not built on LLMs, it's a traditional use case of AI, which allows it to, I think, have much more finite input/output.</p><p>And that example use case is fairly finite input/output, rather than this kind of open-ended conversation thing. So, I'm insanely curious, as I have always been. And so, in the two weeks since we've talked, you know, if someone has a use case that they've uncovered, and they're willing to share it with me, and I'll throw this open to the audience, like anyone, feel free to reach out.</p><p>I'm infinitely curious. But one thing that I've seen that is causing me to pause a bit is a lot of the incremental work that's being done on AI projects are stitching together external databases with the LLM. And the LLM gets relegated to being an interpreter of the human, or to being an interpreter of the data back to the human, but is not involved necessarily in the data store, and is partially involved but not wholly involved in the decision-making.</p><p>And on one hand, you could say, "Well, that's just how this is going to evolve." And maybe that is how this is going to evolve. On the other hand, for me, it questions whether we're still on the same evolutionary track we were with the LLMs. And look, there's all kind of open questions, like, "Will the LLMs that are being worked on at Anthropica OpenAce start to have a native data store inside of them?" Well, on the other side, we're seeing companies that have a ton of corporate data bolting their own LLM on top of theirs, and saying, "You don't need the traditional LLM vendors because we'll just allow you to query the data store you already have." So I think there's a lot of questions that caused me not to have the overwhelming confidence that it sounds like you're building, that it can go even higher from where we are today.</p><p>And as you and I talk about all the time, like, I think about the fact that these entry prices are quite accelerated and bake in quite a bit of optimism. So one thing we've seen since you and I talked is a couple of, we'll just call them early falls from grace with inflection and stability AI.</p><p>And when you raise money for a non-revenue company in the billions, it's pretty easy to lose confidence when you actually start having revenue, because all of a sudden, you can see the delta from what it's going to take to get to your next round. And in some way, I used to always tell people that valuation is discounted future expectations.</p><p>They think about it as they won an award. What they should really think is, "Holy shit, we just set ourselves up." And that gets back to that point we were making before, which is Amazon 400 in 1998 was ahead of where the company was. The fact of the matter is it ended up being a bargain relative to the ultimate price that Amazon achieved.</p><p>But we all know there were a lot of companies along the way that never achieved it. Yeah. And I mean, I've made this point over and over again, but I I'll state it again for the record, which is public companies structures are much more flexible for downward performance than private company structures.</p><p>Right. And they just like, if you raise money, let's say your generic AI company, we call it company G, and company G raises money at $1.2 billion, and they have very little revenue. And 18 months from now, they need to be in the market again. And because they raise that much, they're burning, I don't know, what, $50 million a year?</p><p>And the revenue's $2 million, $5 million, $10 million, $15 million. Like, all of a sudden, that looks very difficult to raise an incremental round. Well, I would I would say it's, you know, and again, I think inflection did some important work. I don't think it was a terrible outcome for the investors, for Mustafa, for Microsoft.</p><p>But it does, I think, shine a light on one of the things you said. If you are trying to build your own LLM today, as inflection was, and you want to get your hands on 40,000 H100s, which I think they did, that's like, you know, digging a gold mine, right?</p><p>The capital intensity of that undertaking is very different than starting a software company, right? And so the risk reward to both the founders and the risk reward to the investors changes a lot. I want to touch on something you said, you know, I was getting perhaps a little bit more bullish and excited.</p><p>You know, I think I have a good company. I think I have a good company here. Remember, it was just not even a year ago that, you know, perhaps those of us who owned NVIDIA went from 125 to 250 or 300. And everybody yelled and said, you got to sell your NVIDIA.</p><p>And I said, why would I do that? My numbers have gone up by more than the stock is appreciated. And the argument was that we pulled forward all the training demand models are going to get smaller. And that, you know, perhaps some of the production use cases are overhyped.</p><p>And, you know, what we've seen from our bottoms up, both in terms of the training demand, okay, so this is training small models, medium sized models, frontier models, that is a lot bigger than we thought. And then the production use cases around inference, the co-pilots that we talked about, like, I can't think of another technology that became almost ubiquitous within enterprises, right?</p><p>The need for it. Like if you're an enterprise with a call center, and you're not leveraging AI today, like what are you doing? If you're not leveraging co-pilot today, what are you doing? So in a matter of just a few years, this enterprise technology has become ubiquitous. And I want to explore two quotes that, you know, I came across recently, both from interviews that I think get to this point.</p><p>And I want to try to break those down a little bit for where we go from here. But the first was from Jensen Huang. And he said, "You have to redefine the way you think about the traditional sources of demand for data centers and compute. The production at scale of intelligence matters to every single country and to every single industry." Now, the immediate reaction to this and to the quote from Sam Altman, I'm going to read here in a second, is of course, Jensen's going to say that he's talking his book.</p><p>Right? But given that Jensen has more demand today than he can satisfy, right? There's no need to talk his book. And Jensen's been at this for 30 years. And I think he's been a pretty straight shooter for a long period of time. And so when you have somebody like him who's saying, "Listen, this is going to impact every sovereign in every industry." And then my own two eyes look at every single company, and every one of them wants a co-pilot for engineering.</p><p>Every one of them wants a co-pilot for call centers. And the capabilities of these co-pilots is still on a very steep curve. It tells me that the use cases are unlike anything else we've seen since perhaps the database or the internet itself, where those became ubiquitous in every single enterprise.</p><p>What do you think about Jensen's quote? Will this touch every industry? I think... Well, I already made a statement earlier that it is such an interesting question, too, because I really think you have to separate LLMs from AI. Now, Jensen's exposed to both, so it doesn't... One's a subset of the other.</p><p>Explain that a little bit. Explain what you mean by that a little bit. Look, I'm always open to being wrong, and I've been wrong many times before. But I'll tell you, my current best thinking was just that LLMs were developed, starting with language translation, and then when the attention window part was added in, where context mattered, became very good at structured language.</p><p>And it turns out coding is a subset of... And an even better subset of structured language, because it's more structured than language itself. And so, LLMs are very, very, very good at coding. Although I did see one tweet this week where someone said that the... They were saying the percentage of time spent on debugging has gone up massively because you don't even understand the code that was written, so to debug it takes longer, which would make sense.</p><p>But anyway, as you... So, LLMs work really well against things that are language intensive. So, programming is a very particular use case of language that's even better for... And then customer support, you know, because the data content stores that have all the answers are very textual, and the user representing their problem is in language.</p><p>And so, that one's a really good one too. And then, you know, people have talked about legal. There are others, but LLMs not going to do much for you on a manufacturing floor. The LLM part, AI might, but the LLM won't. And so... And clearly, NVIDIA is taking... There are going to be all these different models, vision models, omniverse, models of the real world, et cetera, that tackle the other problems that you're talking about.</p><p>But that's also going to lead to massive TAM expansion. We met with... I spent time with Fei-Fei Li this past week, the incredible researcher, author, thinker at Stanford who's working on her own business, and that's going to require model training. So many people are still... And I would argue in the...</p><p>Not only do I think it's going higher, Bill, I think we're in the very, very early phases, and we're going to look back at this period and say those were pretty incapable models. I want to talk about this Lex Friedman podcast, which I thought was pretty terrific with Sam Altman.</p><p>But Sam, I think, referenced that, you know, the step forward in CHAT GPT 5 versus 4 is at least as big as 4 versus 3. I've heard this from others who've actually had some exposure to that. I also hear similar things about Anthropic where you have these agent capabilities that are, you know, built right in and become intrinsic, inherent in the model itself.</p><p>And so I think it's not just language. I think we're going to have, you know, these real world models. Obviously, we've seen the video models, etc. And these things are just massively consumptive of data and compute. And so in that regard, I think, if anything, we're still probably underestimating where we go from here.</p><p>But I will stipulate to you as venture investors, you're going to have a lot of roadkill along the way, right? You know, there were 20 search engines that went to zero, there are going to be 20 LLMs that go to zero, or many more. And I'm not even sure, right?</p><p>If you said to me, where does value capture happen at the end of the day? Where is durable value capture in the land of AI? I'm not sure that LLMs, like if that's all you have going for you, I'm not sure that's going to be the winner. At the end of the day, you get paid by solving problems for consumers or solving problems for enterprises in a way that's durable and differentiated such that other people can't compete.</p><p>And you can achieve, you know, some some dominant market share type pricing, some monopoly like pricing. And when I look at the commoditization of models, it seems to me that apart from the people who are on the very frontier, if you're on the frontier, and you have something totally different, it seems to me that that's a place where that is defensible.</p><p>But if you're not on the frontier, man, it seems that these are going to be really fast depreciating assets that are going to be commoditized by the llamas of the world, other open source models. So I think you can have these two simultaneous truths. One can be that we're at the early stages in terms of the demand cycle for AI.</p><p>But number two, that if you're building an index of LLMs, it may not be a very successful investment approach. Yeah, I mean, you raised so many different questions. I'll try and unpack a couple and then and then and then I want to go back to the same thing for a minute.</p><p>But the the I really look forward to seeing five and seeing if I feel that way about it. I think part of what people are doing is extrapolating the feeling they had the first time they used chat GPT or the first time they had to write a letter, you know, and it just had this magical appeal to it.</p><p>And then the next step was multimodal. And I would argue that didn't really land as well as people thought. And the first version of I think what you're calling agents were these third party attachments you could click and sign up for like that didn't really land. And so I what what I don't have the ability to understand and where this is where I could get it way wrong is.</p><p>It doesn't appear to me that if agents are connectors to the external world, you know, to me, that's a lot like an enterprise when people would build connectors to other apps. And I just don't see it on this kind of exponential scale that we got, you know, as the LM was passing the LSAT, you know, you had each of these hurdles that it was driving through.</p><p>And, you know, I don't I don't know that it'll feel that way. But if it does, then you I mean, this will happen. So when does five ship? Everybody thinks summer end of the year. OK, perfect. So today's April 2nd. So within six months, if it's so magical, then I want you to call me out and say, you see, but but but but we're playing a lot with hyperbole.</p><p>And that's where I wanted to ask, which is like like just saying, oh, it's way better or oh, it's you know, I I have I it causes my skeptical meter to go up. I want to there's two things Sam did this in this recent interview that I think put him in the promotion hall of fame.</p><p>And one of them was he read he juxtapositioned AI versus the smartphone market and then on the smartphone market as being small. He said, well, that's limited to five billion purchases every two years. And we're going to go way beyond that, which was I just thought genius. Right. Like like bring up something really big and then say degrade that and say it's going to be way bigger.</p><p>It's genius. But the second thing he did was he said he's afraid they're going to run out of energy. Now, this is genius also, because the minute you leave that conversation and say, holy, how much energy is used by data centers, you've succumbed to the game, like because you've now accepted that this thing's going to be energy limited and you're going off and trying to figure out whether that's true or not.</p><p>I think both of these things were and I it may sound like I'm kidding. I'm not. I think they're jobs and level promotional techniques like so much. So considering that I am on the other side of that from you, let's actually look at the quote and then let's take it apart in its parts.</p><p>Right. He starts off by saying, I think compute is going to be the currency of the future. I think it'll be the most precious commodity in the world. OK, I'll give you that. That's that's nice prose and a little hyperbolic, perhaps. He says, I think we should be investing heavily to make a lot more compute.</p><p>And then he he addressed this question of why are people skeptical of demand? I would say you're skeptical of demand. We have some other friends who are skeptical of demand. And he says people think they compare it to the market. So they think about the market for chips, for mobile phones.</p><p>And let's say there's something I never did, by the way. I never thought about it that way. That's my point. But you know that seven billion people, they upgrade every two years. So the market's three billion systems on a chip. And he says, here's the important point, Bill, because it gets down to the elasticity of pricing.</p><p>He says, if you make 30 billion chips on a system, you're not going to sell 10 times as many phones because most people only have one phone. Now, then he goes on to say, but what if you produce more compute? This is why it's so important. But compute is different.</p><p>Intelligence is going to be more like energy, where the only thing that makes sense to talk about at price X is how much of this compute the world will consume and at price Y, how much it will consume. Because if you think about energy, right, and I was trying to come up with an analogy on this, Bill, if the application, let's talk about an application that we use of energy every day that we like, like a hot water heater.</p><p>I like to take a hot shower. But if the cost of that was $100,000, right, not many people would take hot showers. But he's saying that if you drive down the price of the cost of compute, then the reflexivity is people will consume a lot more of it. Now, this is also known as the Jevons paradox, right?</p><p>As price goes down, we demand more of it. The aggregate amount of consumed, of the compute consumed actually goes up, right? And that's really, you know, just a fancy way of talking about the elasticity of demand. And so to me, I think that is the way when people, when the traditional semiconductor chip analysts look at this market or data center analysts look at this market, they do, Bill, compare it to things like the smartphone market, et cetera.</p><p>And they say, okay, well, how does this compare? Is it bigger or is it smaller? And I think the key point he was making, whether we agree or disagree, he was saying, if you produce it in abundance, right, there is going to be dramatic workloads demanded of that abundance since compute.</p><p>And he used the examples, you know, whether it's, you know, tracking my email, doing very pedestrian things, running co-pilots for engineering and customer care, running these autonomous agents, or whether it's solving in the future, much more difficult problems like finding cures to cancer using some of these search and planning techniques, inference planning techniques that we're looking to in the future.</p><p>So I thought that was actually interesting. But, so let me stop there, get your reaction because the second half of it I also think is important. Don't you agree that if we drive down the cost of compute, if we build a lot more of this, today we're clearly constrained that we're going to consume a lot more of it in the future.</p><p>I mean, it's not limited by the number of chips on a smartphone. It's just not that interesting, Brad. Like if I drove down the cost of housing, I would produce more of it and people would use more of it. If I drove down the cost of gasoline, people would use more of it.</p><p>If I drove down the cost of a robot, there'd be more automation. Like, of course, that's true. If an airplane cost one-tenth the price that it does today, we would fly more. That's just not that interesting. It's not provocative. It's not provocative. Okay. So let me tell you why I think it is provocative.</p><p>We did drop the cost of flying on an airplane by one-tenth since 1971. And guess what? We got 10X as much travel. Okay. So all he's saying is that there, you know, today we're limited in the amount of these things that we can do. We can't build a multi-trillion parameter model today if you don't have the compute to do it.</p><p>And so I do think that you're going to get to intelligence faster. You're going to get to more use cases faster, just like you got to more plane tickets and more hot water heaters, but we got to actually build out the infrastructure. The truth is, whether we believe it or not, it's very clear to me, and maybe we can move to this topic, it's very clear to me that enough people in the world believe, because I'm reading headline after headline about massive supercomputers that are being built, right, in almost all of the GCC countries.</p><p>So Omnivan, Kuwait, MGX is backing a lot of efforts in the Emirates. The Saudis are doing the same. The French, a lot of talk about what's going on with the Singaporeans. The United States is out talking about their supercomputers. And just this week, we saw some headlines, I don't know whether or not they're true, about this project that Microsoft and OpenAI execs are drawing up on something, I think they're calling, what is it, Stargate?</p><p>And again, I don't know the specifics whether or not that's true in '26 or '27, that they're going to spend $100 billion on a single supercomputer, but it does seem to me that there are enough breadcrumbs in the world today that people are making forward bets, that the need for more training and the need for more inference, there's sufficient evidence to cause them to put real money up against this.</p><p>And so I think we're, you know, remember, we talked about a few weeks ago, that chart where Jensen said the five-year replacement cycle for data centers is not going to be $1 trillion, it's going to be $2 trillion. And then we'll reshow it here in the pod, and we broke down what that's likely to look like on an annualized buildup.</p><p>In many ways, this is just a lot more commentary about the same, Bill, which is there are people placing orders, hyperscalers placing orders, enterprises placing orders, and now sovereigns placing orders that get you to a much bigger number than the traditional data center market. Well, then Sam's problem is solved.</p><p>Well, I think it will be solved. There will be more capabilities brought online to train ever bigger models. But I think that he's suggesting, and this is, you know, I think as forecasters, the hard thing is, are these things going to be sufficiently demanding of compute and power that on current trajectories, we're going to fall short.</p><p>And I think there are a lot of people sounding alarm bells on this, including, I saw a headline from the Biden administration, says the Biden administration wants to accelerate its conversations with big tech companies on how to generate more electricity, including with nuclear power, to meet their massive demand for AI computing.</p><p>So on this one, I think we have a forecast. You know, if you look at the general forecast for power demand, it's been about GDP growth, right? Couple percent a year. And the fact of the matter is with renewables, we got sufficiently more efficient every year that you really didn't have to bring on a lot of new power generation to meet the demand that was going on in the world.</p><p>But now if you look at some of these forecasts, and here's a forecast, I think coming out of semiconductor analysis, it's similar to a lot of the other ones. And it has data centers as a percentage of US power generation going from something like four percent today to something like, you know, 18, 19 percent in 2030.</p><p>Right. So these are parabolic moves in terms of consumption that aren't satisfied with the current improvements that we have in terms of both power generation and our grid. You've been out front, I should say, on calling for the need to have regulatory reform as it comes to comes down to nuclear fission.</p><p>A lot of people are talking about nuclear fission with respect to solving this problem. Are you a let's say that you believe the demand is going to be great, you know, and we just talked that there's some debate back and forth. But talk a little bit about what would need to happen from a nuclear fission perspective in order to meet some of this power demand.</p><p>Yeah, I think there are better people like to talk about this than me. But when I look across the number of people whose intellect I respect and this came to my thinking, you know, maybe five years ago, and that's everyone from Steve Pinker to John Collison to Ilan to Toby at Shopify, like they all believe that nuclear is the very best way to get us past the climate change problem.</p><p>And one of them, I can't remember which one, tweeted like if if if you're vocal about climate change and anti-nuclear, I don't have time for you. And and I I'm in I've been in that mindset. What what has happened and another person that's been remarkably outspoken on this is Josh Wolfe at Lux, who I spent time with at Santa Fe.</p><p>And he even went in front of Congress and was talking about how much China's investing in fission. I think we have some charts we can show here, but their plan is to build 100 or 200 new fission plants while we're, you know, and if you go back three or four years ago, we were only decommissioning plants.</p><p>We weren't. And of course, people talk about the horrific thing that happened in Germany where they were a leader in the market and turned them off and now are aggressively using coal and buying energy externally. And, you know, I think the best quote I heard, and maybe it was from Josh, it may have been from Pinker.</p><p>He'll love that. I confuse those two was that if World War Two never happened, the atomic bomb never happened. And someone just showed up in twenty twenty four and said, I figured out this thing. It's nuclear fission. People be like, oh, my God, like probably be more excited than you are about.</p><p>I write like because it would solve all of our problems and we let there become this negative association. Now, in the past three or four years, that's been flipping and there's been a lot. I think it's because of thought leaders like the ones I mentioned being outspoken that people have come to realize that we are we are it's it is the most energy efficient, the most energy dense, the cleanest thing we have available.</p><p>It's way more durable because it can be put in so many different places that don't have geographic limitations, don't have sunshine limitations, don't have wind limitations, don't have transport limitations that some of the renewables have. And so it's been great. I would just say exceptional even to see this flipping of expectation about what's possible with nuclear and, you know, the the plant that didn't get shut down in California and then this Biden thing this week, which is all new, right?</p><p>It's all and it's new in a different direction. So winds have been going this way and they're going this way. So it's fantastic. Now, despite it being fantastic, China is way ahead of us. And we'll we'll put a link in the notes of Josh Wolfe telling this to Congress, but they're way ahead of us.</p><p>They're executing faster. And the biggest problem, as I understand it, once again, from talking to experts, not from my my own direct knowledge, but our cost of deploying new nuclear fission infrastructure is limited by our own regulatory framework, not by the technology. Or if you look at the cost differential between us and say China, that is the problem.</p><p>And I don't know that we know how to do regulatory reform. And if I if I could snap my fingers and say, what would I want? I'd love for there to be a a zero based like like you do zero costing in an organization, a zero based regulatory rewrite of nuclear fission and one done without oil executives in the room.</p><p>Now, I don't know how to make that happen. Well, you know, it's you bring up so many good points. One of the points is, you know, we've got this chart from our world and data that nuclear is both the safest and the cleanest source of energy. Right. And so if you just start there, you have to ask the question, like, how did we end up here?</p><p>Like, we're all of the age that we remember Chernobyl and all the fear that got propagated in the world. One of the things that worries me about A.I. itself. Right. Here's a technology much like supersonic aviation, where scare tactics literally shut down all of the innovation in massive industries that would have inured to our national strategic advantage, the environmental advantage, right, geopolitical advantage.</p><p>And we literally shut it down. And worse yet, there wasn't even a robust debate about it. It's just like, you know, the anti nuclear lobby declared victory and it was game over. And it's only what's interesting to me if you say, well, why, Bill, has have the winds begun to change on nuclear?</p><p>I think the reason the Biden administration came out this week. Right. If you think that A.I. is on the front lines of the new Cold War about national economic security, national defense and offensive security, and if you think the limiting factor to pushing forward in A.I. is power, then all of a sudden.</p><p>Right. You're sitting here worrying about China from the perspective of the video chips. You think you're ahead of China, but all of a sudden you say if the bottleneck is power, at least on nuclear, we're way behind, way behind on two dimensions. They have over 300 plants currently either being built or in development.</p><p>We have zero, zero plants being built. I think 13 plants that are proposed according to this data that we have here and the cycle time to put up a plant. Right. If you if you do it as fast as humanly possible, something like seven to 10 years. Right. And that's that's assuming that somebody says go tomorrow.</p><p>On top of that, if you look at some of, you know, just enriched uranium, I think a lot of the plants in Canada have been mothballed. We've got to spin those plants back up in Saskatchewan, whereas I think, you know, we have another chart on this. Eighty percent of the world's uranium is basically coming from Kazakhstan and is being shipped to the border between Russia and China.</p><p>So like the U.S. has got to get serious about this. But if I'm right, that the lead time is 10 years, then all the stuff that we just talked about, the demand that we've got to satisfy over the next 10 years is not going to come from nuclear. It's got to be an all of the above strategy.</p><p>But, you know, here's an FT article that just just came out. It said data centers, voracious power needs are set to rocket as cloud storage facilities, crypto mining and AI all strain the grids. Microsoft alone's opening a new data center globally every three days. These power hungry operations will together consume more than 480 terawatts hours of electricity or almost a tenth of U.S.</p><p>demand. So when you start to think about that, wind and solar are important. Costs are plummeting. They're clean, but they have baseload issues and they certainly can't scale at the rate I think that we need this to scale. So it seems to me that the power source that we're going to need to scale up is not gas.</p><p>And part of the reason I think you're seeing a lot of this go down in the Middle East is because there are abundant sources of natural gas. The U.S. has abundant sources of natural gas. You can spin up a natural gas facility much faster, right, two to three years, very well-known technology, less regulatory headwind.</p><p>But it seems to me that there's a lot of energy in Washington around national AI strategy. We need to have an equal commitment. Part of the same conversation needs to be a national energy policy that supports the buildout we're going to have to see. Well, and look, you could leave the AI part out.</p><p>I think there are enough people who have such a strong view of climate risk that you could do it just for that reason. And let's say both, you know, OK, that's kind of a slam dunk. When I talked to our friend, Phil, who would be thrilled to hear your comments on natural gas, you know, he told me that the base load in the U.S.</p><p>has been flat for many decades now. And so obviously the demand for energy has gone up over those decades, but it's been offset by efficiency gains. And some of that will play out here. Like the more you make an issue out of this and the more people like us talk about it is part of the market adjusting.</p><p>And so you'll see some of that get corrected. One thing that set off a little bit of fear in the markets was Amazon went and did this deal where they bought some land right next to a nuclear facility. And once again, according to Phil, about 30, 25, 30 percent of the production in the U.S.</p><p>is from independent energy producers who basically sell to whoever they decide they want to sell to. So it's a plant owned by a corporation that sells. And if you start to see data centers pick off the independents, that could put pressure on the grid in a way that might bring the regulators in to voice their opinion.</p><p>So I think that'd be something to watch out for. Back to the nuclear thing, like I just, once again, if I could do anything, I would just encourage those in Washington, if they do get on this bandwagon, to commission, to build, to create a commission of people who have the best interests of the country in mind to think about reevaluating nuclear regulatory structure from the ground up and starting over from scratch.</p><p>Because I suspect that the 10-year thing that you're talking about, in addition to the cost thing, are heavily impacted by regulatory. In fact, if you'll allow me, sorry to spin off of this, when I was thinking about this, I had a flash in my brain about this bridge that went down on I-95 in Pennsylvania.</p><p>And I'll put a link in here. And for those of you that don't remember, I mean, now everyone's focused on the Delaware Bridge, but this other bridge collapsed. And within 12 days, it was replaced. And there's articles that say the governor's now a presidential candidate, like he's definitely going to get reelected because of how wonderful this is.</p><p>And when you peel underneath that and read the articles about what happened is he went with no bid contracts. He was able to get people to work 24/7. It basically broke the rules of the regulations that were put in place that slow everything down. And if we've gotten to a place where we celebrate how quickly things can happen, and you've probably seen these things on Twitter about how quick the Golden Gate Bridge was built and how much it cost and all these infrastructure projects and how today they're like exponents of that.</p><p>And I also thought about when they cleaned up San Francisco for Xi in like a week. The problem isn't that we don't know how to solve these problems. The problem is that we're our own worst enemy. We put in place the things that limit what we're capable of. Another article that I want to link to shows that renewable projects are way more successful in Texas than California, which is the ultimate irony, right?</p><p>- But what does that tell you? What does that tell you, Bill? - It tells you that there's less regulatory mud in Texas than in California. And despite the intent of both the people of California and the people that I guess are representing them, the intent doesn't matter if the outcome's not possible.</p><p>In fact, in some ways, you should hold representatives only accountable for output. Intent is kind of silly if it never achieves the output. And so anyway, there's a long way of saying that. - I think it's a super important topic, Bill, because when we look at great advances in innovation that have been stymied by regulatory excess, right?</p><p>I talked about supersonic aircraft, right? Think about the productivity gains that would occur in the world if we were able to get from point A to point B faster. But literally, it was the environmental lobby that stymied supersonic aircraft. You think about calcium CT scans, which I've been promoting from the top of the mountain.</p><p>Calcium CT scans cost $100 and save lives, can bend the healthcare curve around the 3 million sudden cardiac events we have on an annual basis in this country. But the reason it doesn't happen is they lost the lobbying game in Washington. You think about nuclear fission. I mean, think about the amount of dollars in energy the government is using to subsidize nuclear fusion, right?</p><p>Nuclear fusion, which we all would agree would be a fantastic thing if and when it happens, but it's long dated, right? We have nuclear fission that has all these benefits that exist today. But again, for whatever reason, mass hysteria and otherwise, government regulation got in the way of that industry.</p><p>And this is what we kicked off this pod talking about. And consumer perception, which you talked about. The hysteria around Chernobyl. Actually, I think you have data in here, but the number of deaths that happen per year from traditional fossil fuels is exponentially higher than the number of deaths we've ever had from nuclear energy.</p><p>Yet, there's this weightiness to it. It's like fear of flying. The irony is they're not even fearful anymore. I mean, the data suggests that well over a majority of people understand that nuclear fission is safe. But what happens is it takes a long time for the government regulatory process to revert back to that pre-paranoia.</p><p>So it takes some external force to cause us to change. Well, I think the external force that's causing it today is when we start forecasting ahead power demand needs to power all these things that we're talking about over the course of the next five to 10 years, we're going to have to take in all of the above strategy.</p><p>And I'm glad Phil's going to be excited. I'm not sure the price of nat gas is going to go up that much, but I do think that natural gas is super plentiful. The mechanics of it in terms of building production facilities are super easy, well done. And I think it can come online in big chunks, like two, three gigawatt chunks, which we're going to need to power some of these big data centers.</p><p>Yeah. And some of the graphs that you've included here show that the cost of producing new nuclear facilities in France has been going down and in the U.S. it's only been going up. And that should be remarkably upsetting to everyone. Just that core reality. And I'll pose a question to you.</p><p>Do you think they'll go up or down in China? They're going to go down. Okay. You know that innately, right? Yes. Okay. And even worse, Bill, I mean, I love to visit France as much as the next person, but if it's not the height of embarrassment, if it doesn't prove the point better than any other point we could possibly make, the French are driving more innovation and more efficiency out of nuclear fission than the United States.</p><p>And we pride ourselves on innovation, right? That should be a shock and a national embarrassment. It should. And at risk of being redundant, in case anyone who is listening has any authority whatsoever, I would just highly, highly recommend that there be a consideration of a zero-based re-regulation of the market, because it's clearly the problem.</p><p>I don't think there's anyone in the energy market that doesn't think the bureaucracy and regulation is the problem in the American nuclear fission market. Well, I think that before we move to the market check, the one thing that seems to me is that we have increasing entanglement. You know, you gave this talk that one of America's greatest strengths was, you know, the fact that Silicon Valley is 3000 miles away from Washington DC, right?</p><p>That's been a source of our innovation. And you and I both know 25 years ago, Washington wasn't all that interested in what was happening in Silicon Valley. Well, today, these are all matters of sovereign importance, right? AI is our national security. It is what's driving productivity gains, you know, in our economy.</p><p>Technology is what's driving productivity gains in the economy. And now I look at power being integral to all of this, you know, whether, you know, the front lines of our battle with China, the hot war and the cold war, if you will, are now being fought on whether or not we're going to give them chips, whether or not we're going to let certain engineers work on certain projects, etc.</p><p>And so if Washington wants to get serious, it needs to be an integrated, right national policy, you can't fix AI without also taking on our future energy needs. How about we talk a little bit just about a little tech check. Speaking of energy needs, I think I know somebody on this podcast, and it's not me, who was faster to invest in some of these companies that would benefit from energy policy.</p><p>But we have a chart here for CEG. They're one of the leading nuclear companies in the United States. And that looks like a pretty good chart over the course of the past couple years, in particular this year. But yeah, I bought this stock. And by the way, I bought a lot of stocks that went down.</p><p>So I think we got to be careful about cherry picking. But I bought this stock solely based on the bet that perceptions would shift on nuclear. This is a company that was carved out of a larger producer. They took the nuclear assets and made it a singular company. What I didn't know was that the AI thing would happen.</p><p>And there are clearly people talking about this company. I saw a Morgan Stanley chart that listed everything about AI. And I saw their ticker. I was like, holy crap, like I didn't know it was an AI play. But once again, I'm happy that the nuclear thing has shifted. But certainly Wall Street is focused on this and this energy issue that you've brought up.</p><p>As a reminder to everybody, just our opinions, not investment advice. But why don't we talk a little bit just about the Fed met since we last were on, Bill. And they update, the Fed every quarter updates their own estimates for things like GDP. December of last year, they expected our GDP to grow 1.4% this year.</p><p>And now they're expecting it to grow 2.1%. So expecting our economy to be more robust. In December, they expected core PC. So this is their favorite inflation metric to watch was going to be 2.4%. Now they expect it's going to be 2.6%. So coming in a little bit hotter.</p><p>And now the market is wrestling with how many rate cuts, if any, we're going to get. I'm still in the camp that we're going to have rate cuts ahead of the election. But the market is now pricing in, I think, only three rate cuts between now and the end of the year.</p><p>And we see the 10-year just today in the markets is up to 4-4. I think we had bottomed at 3.5. So that's adding more restrictiveness to the economy. The markets, particularly high growth stocks, are starting to sell off again. Stocks that are levered are starting to sell off again.</p><p>And so when we started the year, I think one of the big debates, we covered the AI debate, whether or not we had pulled forward all the demand for training, whether or not these stocks had gotten ahead of themselves. But there was a second question. Are we going to be able to avoid a recession?</p><p>How many rate cuts we'd get? When would we get those rate cuts at the start of the year? People thought we'd get rate cuts in March or April. And so I think, as I read the tea leaves, I see pockets of weakness in the economy, particularly if you look at housing, autos.</p><p>Tesla really missed their number today in terms of new cars shipped in the quarter. Some of that has to do with China, but it also has to do with demand here. So I think that that's worthy of watching. And then I've got three charts that we spend time looking at.</p><p>One is a Morgan Stanley chart. So the first chart is the hedge fund net exposure. So this is how many of their dollars they have invested on the long side of the market in a particular sector. So this is in technology. So you can see that in October of '21 or the end of '21, the MAG 7 or TMT X, the MAG 7 had become 31% of its book.</p><p>So what is that? That's technology if you exclude the largest names. And now that's down to something like 19%. And so hedge funds are starting to pull back a little bit on that part of the risk curve to your point as to whether things are getting a little too bubbly, Bill.</p><p>The next one is people have seen this a lot. This is our software index. So where is software trading relative to interest rates? You can see that in the blue line is it's trading at about 6.1 times. The 10-year average is 6.9 times. Rates have obviously moved up a lot off the bottom.</p><p>But this is a long-winded way of saying that software is not really participating in this run-up. So software came down hard coming out of COVID, and it kind of remains there. It's bouncing around a little bit. Everybody's trying to figure out whether or not AI is going to catalyze a re-acceleration in software.</p><p>Within software, kind of the data stack companies, so the Databricks and the Snowflakes and maybe even the ServiceNow and Salesforce, they've caught a little bit of a bid. But a lot of software is trading well below its pre-COVID multiple. And then the final chart, because I knew you would ask me, I just asked the team to pull together, get rid of Tesla out of the index, show me the MAG-6 index.</p><p>So I just want to see the multiples for that relative to interest rates. And there again, you can see the 10-year average has been about 22 times. And so that is trading up. All of these companies have re-rated a bit. And the interesting thing here is if you look, you had like a reverse correlation between these two things, which is what you would expect up until about March of '22, where rates kept going up, but the multiple expanded on this group of companies.</p><p>And so some people would say, hey, that's a warning sign, right? Rates have gone up, but these MAG-6 has kind of a 10-year high multiple. Shouldn't that be a warning sign? Shouldn't these companies be trading down a little bit? But I would argue to the reason that they're trading at those multiples is that people's confidence or their forecast for future growth is a lot higher than it's been at over the course of the last several years.</p><p>Bill, you pointed out a tweet to me that was, I think, where the generative AI company multiples were trading. And it says OpenAI at 18 times, Anthropic at 20 times revenue, Glean at 56 times revenue, et cetera. Perplexity has a bunch of names on here. It has Huggingface at 150 times.</p><p>It's just a question whether or not this is the prelude to a bubble. So I think this kind of brings us a little bit full circle. We started the pod asking the question, you know, you can believe that the long run in AI is going to be massive, but you can kind of have booms and busts, winners and losers along the way.</p><p>When you look at these multiples for these companies, what do you sniff out? Well, I mean, the thing that made me forward this to you was that it lists them all and then it says, is OpenAI a bargain? So rather than the takeaway being, oh my God, these things are really highly priced, there's this relative valuation game, which is how bubbles are built.</p><p>And because you adjust up, so you re-rate to a new norm, right? You know, I would just say, like, literally in, you know, since in the past, maybe not since we talked, but let's say in the past four weeks, a couple of things have happened. So we've had what I called these fast failures.</p><p>You might, maybe the inflection team will get their money back, but that's not what venture is about, right? Like, so the 10X is off the table. That seems to be true at stability AI as well. You had a really interesting data point in the past week where I think people are abandoning the notion of the $20 premium personal AI tool.</p><p>Perplexity came out and said that they were, you know, considering advertising and they, there were statements in their previous releases that were very negative on advertising. And then OpenAI said there was going to be a non-login version of ChatGPT, which would just inherently be cannibalistic to their $20, even though I think most of us believe that's over a billion run rate already, right?</p><p>But if they're abandoning that, that's an interesting data point. Like the leading players are saying they don't think that's durable. Or it might be that, you know, the land grab is on, I want to sign up as many consumers to use my product as anybody else. And I'm willing to forego some of those early revenues, but I would agree with you.</p><p>There's marginal cost here. I think the run rate here that's been rumored is that OpenAI consumer revenues are about a billion and a half. So if you look at that, you've got just under maybe a million customers using, you know, paying for ChatGPT. I do think I've said this many, many times, listen, I don't think a $20 a month fee for my consumer AI is going to be defensible.</p><p>And the reason I don't think that's defensible is because they're going to be way too many people in the pool. Apple's going to have one, Google's going to have one, Meta's going to have one, ChatGPT is going to have one, go through the list, perplexity, et cetera. It only takes one person to give away a frontier quality experience, you know, at a much lower price.</p><p>I'm going to say two things that are going to sound like they're at odds with one another. I agree with the current state of LLMs. Exactly what you're saying is true. I think if one of them can seamlessly integrate memory in a way where I can become reliant on it, I would gladly pay 20 and maybe more.</p><p>A lot more. And so, you know, I think it's TBD. I think we got to see whether or not someone can, you know, everyone seems aware of the memory issue that we started talking about a while back, but that doesn't mean it's solved. And the architecture doesn't have an elegant solution.</p><p>So it's the number one thing I'm interested in and looking out for. The last data point I'd bring up on this front, I did a tweet where I was curious why, you know, there's a 60X reduction from top model at a LLM company to their next model. 60 is just a lot.</p><p>Like, you don't see this. Just so we understand what you're talking about. Like, the difference between, I don't know, maybe a CHAT-TPT4 and CHAT-TPT3 is a huge differential in price. 60X. And I would use this, you know, relative to your, like, that's not true in energy. That's not true in cars.</p><p>I said to you, if new car production were limited, what would happen to used car prices? They would go up. So there's something weird here. Now, I got some good responses. One person said that there are throughput limitations on the high-end model. And so they're priced artificially high on purpose, which I could see that making sense.</p><p>But the second thing people said was that the runtime models are just highly competitive. And many people think that some of the models are already priced under cost, which goes back to the credit investment theme that you and I have talked about. But if things are already that kind of hyper-price competitive, it's just a data point.</p><p>Like, it's a data point worth paying attention to. So when I look at all those things and the entry prices that a marginal late-stage investor would be asked to pay, which are these multiples here, I'd be, like, nervous. I'd be nervous. I'll leave it at that. No, I mean, listen, it brings me back to the conversation about investing in Alta Vista, Lycos, AOL, et cetera, versus Google.</p><p>I mean, the reality is we were in the fog of war, and we didn't know it. Every venture firm was paying up to get a search logo in '98 and '99. There were early revenue curves that looked promising for all of those companies. But the fact of the matter is we hadn't even gotten to the starting line.</p><p>Like, we didn't figure-- like, we hadn't even determined who the market leader was going to be. And ultimately, Google, by 2004, 2005, it was very clear that they were going to capture a dominant share of that market, that the market was going to grow longer, be bigger. And so '98, '99% of all the profits ever created in internet search went to Google and went to them five or six years after, right?</p><p>Search really emerged as a category. And so you at least have to leave open the possibility that that's the moment that we're in, that you look at these names on the list. And obviously, OpenAI is an early leader, is in pole position to be the non-Google Google, right? But the fact is we're super early, and it's not even clear that they're going to have sustainable economics in order to do that.</p><p>But if you said to me, I look-- we passed on a lot of these companies simply because we couldn't get comfortable with the entry multiples, given how opaque it still is at this moment. But here's what I also believe. I believe the winners, the ultimate winners in AI have the potential to be way bigger than the winners of prior generations.</p><p>And so you can afford to miss those early rounds, perhaps, in some of those companies. Not if you're benchmarked, perhaps, and your stock and trade is Series A. But if you're Altimeter, and we can-- like, guess what? Investing-- we had Bob Milad on last week. Investing in price line at a billion dollars, that was 120x in the public markets.</p><p>Google also produced venture-like returns in the public markets. Amazon produced venture-like returns in the public markets. So I think the most important thing is-- But anyway, you were able to buy all of those at very reasonable relative prices, all three of the examples you used. Yeah, but I would also tell you-- I mean, at the beginning of the 100x in the public market.</p><p>Yeah, but my mentor, Paul Reeder, used to walk into my office, and Google would-- query volume would go down a little bit, and he would be like, why are you so confident? This is 2005, 2006. And so it never appears clear at that moment in time, right? You had to believe in the size, the length, and the width of the market.</p><p>And so all I'm saying here is that you've got to tread with caution, right? By the way, NVIDIA's already-- 2 trillion. Won. 2.2 trillion. Yeah. Like, that already happened. That's liquid today. It clearly has been the AI winner. 18 months ago, 18 months ago, NVIDIA was at $125 a share, and Meta was at $90 a share, okay?</p><p>Meta's at $500, NVIDIA's at a huge multiple, $900 of where it was then. Markets shift very quickly, right? And then they forecast ahead levels of durability that may or may not exist. And so when you're at these moments, the beginning of a phase shift, we talked about-- you and I on this pod-- ERR, experimental run rate revenues, versus something that you believe is monopolistic, annually recurring.</p><p>Those are two radically different things. One, you have no ability to forecast the future, and therefore it should have a one or a two X multiple because you don't know if it's going to occur again. The other one, if it's monopoly-like recurring, then you can give it a 10 or a 20 X because you think it's going to last for a very long period of time.</p><p>Anybody who tells me that these revenues deserve monopoly recurring like multiples today, that's a real struggle once you get to the late stage. I think it's a good lead-in for maybe a future podcast, which is I have this big question about whether the leading LLMs are going to incorporate their own data store, which if developers become reliant on, would greatly increase switching costs.</p><p>Because today, the switching costs based on every developer I've talked to are-- we mentioned this before-- but about as low as you could possibly imagine. And if you're just using big context windows and not doing fine tuning, then you're just not-- you can bounce from one model to the next.</p><p>And this gets back to that point I was making about are you just using the LLM for language translation versus are you relying on it for something greater? And so anyway, it's something I'm watching out for. Could someone build an LLM with an inherent data store that developers start to become reliant on because then they're in a different place than the ones are today?</p><p>And answering the question in that tweet, is OpenAI a bargain? I will tell you, the only way I see you getting to switching costs is you have to develop something that is fundamentally differentiated, right, that gives you monopoly-like capability, whether because you've got memory and nobody else could do it, whether it's because it-- Maybe it'll be the agent you're talking about.</p><p>I'm skeptical, but people that have seen them are excited, including you. But if I had to make a bet, you bet on the smartest group of people that's come together, that's delivering at the fastest cycle time. I mean, an OpenAI would certainly be in that camp. Anthropic would be in that camp.</p><p>They're going to compete against these big hyperscalers. I do think the winner here is going to be massive, but you're making an excellent point that that could be true, and 90% of these other things could go to zero. So for venture investors who are paying up for things with that level of variability, that's a very different game than betting on a software startup with low capex, high predictability, $10 or $20 million in early indicative revenue in 2016 or 2017.</p><p>Yeah. Hey, Bill, it's been fun as always. Take care. See you soon. Bye.</p></div></div></body></html>