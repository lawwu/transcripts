<html><head><title>Make your LLM app a Domain Expert: How to Build an Expert System — Christopher Lovejoy, Anterior</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Make your LLM app a Domain Expert: How to Build an Expert System — Christopher Lovejoy, Anterior</h2><a href="https://www.youtube.com/watch?v=MRM7oA3JsFs" target="_blank"><img src="https://i.ytimg.com/vi_webp/MRM7oA3JsFs/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Hi everybody, so I'm Christopher Lovejoy. I'm a medical doctor turned AI engineer and I'm going to share a playbook for building a domain native LLM application. So I spent about eight years training and working as a medical doctor and then I spent the last seven years building AI systems that incorporate medical domain expertise and I did that at a few different startups.</p><p>So I worked at a health tech startup called Sarah Care doing tech-enabled home care. The startup recently hit 500 million ARR, worked at various other startups and I currently work at Anterior. And Anterior is a New York-based clinician-led company. We provide clinical reasoning tools to automate and accelerate health insurance and healthcare administration.</p><p>We serve about 50 million, we serve health insurance providers that cover about 50 million lives in the US and we spend a lot of time thinking about what does it mean to build a domain native LLM application, whether it's in healthcare or otherwise. And that's what I'm going to talk about today.</p><p>And in particular our bet really is that when it comes to vertical AI applications, the system that you build for incorporating your domain insights is far more important than the sophistication of your models and your pipelines. So the limitation these days is not like how powerful is your model and whether it can reason to the level you need it to, it's more it's more can your model understand the context in that industry for that particular customer and perform the reason that it needs to.</p><p>And the way that you enable that and the way that you kind of iterate quickly with your customers is by building the system around it. And there's various components to that and that's what I'm going to talk about. So this is the kind of a high level schematic. And we're going to go through each of these parts throughout the talk.</p><p>As you'll see right in the middle, there's the PM. And this is, you know, in our experience, it makes sense for this to be a domain expert product manager. So in our context, it's clinical. And I'm going to go through this in more detail shortly. But first, I think it's worth taking a quick step back and asking, you know, why is it so hard to successfully apply large language models to specialised industries?</p><p>We think it's because of the last mile problem. And what I mean by the last mile problem is this problem that I kind of touched on just now around giving the model and your kind of AI system more generally context and understanding of the specific workflow for that customer, for that industry.</p><p>And I'm going to illustrate that with an example from a clinical case that we've processed, our AI anteriors called Florence and a 78 year old female patient presented with right knee pain. The doctor recommended a knee arthroscopy. And as part of deciding whether this treatment was appropriate, whether the doctor made an appropriate decision, Florence needs to answer various questions.</p><p>One of those questions is, is there documentation of unsuccessful conservative therapy for at least six weeks? And, you know, on the surface of it, that might seem relatively simple. I mean, I appreciate maybe not a lot of doctors in the room, so you might not know necessarily what conservative therapy is.</p><p>But actually, there's a lot of kind of like hidden complexity in answering a question like this. So for example, you know, conservative therapy, typically what we mean by conservative therapy is when there's some kind of option for, you know, a more aggressive treatment, maybe a surgical operation. That's like the, you know, the surgical treatment.</p><p>And then if you're deciding not to operate and you want to try something conservative first, that's like the conservative therapy. So it might be, you know, do physiotherapy, lose weight, do kind of, you know, non-invasive things that might help resolve the problem. But actually, there's still some ambiguity there because, you know, in some cases, giving medication might be a conservative therapy.</p><p>In some cases, that's actually the more aggressive treatment and there's something else that's more conservative. So there's one layer of ambiguity there. Then when we talk about unsuccessful, well, what does unsuccessful mean? Let's say that somebody has some knee pain, they do some treatment, and their symptoms improve significantly, but they don't like fully resolve.</p><p>So is that successful? Do we need like a full resolution of symptoms? Or is it just like a partial resolution is enough? If it's partial, like at what point is that enough to be quantified as successful? So again, there's kind of complexity and nuance with how that's interpreted. And then finally, documentation for at least six weeks.</p><p>Again, you know, documentation, are we saying that the medical record said they started physical therapy eight weeks ago, then it's never mentioned again? We can therefore assume that they've been doing it for eight weeks? Or do we need like explicit documentation that they started treatment, they did it for eight weeks, and, you know, it's completed?</p><p>Where do we draw the line there in terms of what we can infer? And yeah, just kind of coming back to echo our point. So this is really our bet that the system is more important. We believe that in every vertical industry, the, you know, the team, the company that wins, is the one that builds the best system for taking those domain insights and quickly translating them into the pipeline, And then giving it that context and iterating to create this improvements.</p><p>And we also, you know, I guess to talk to this counterpoint, the models, I mean, models obviously are important. And the progress in models makes it easier to have a good starting point. But that's only getting up to a certain baseline. And we found we kind of hit a saturation around like 95% level.</p><p>So we invested a lot of time and effort in improving our pipelines. Obviously, 95% is still pretty reasonable. And this is that performing the, like, primary task that our AI system does, which is approving these care requests in a health insurance context. So we're at 95%. And we then iterated based on this system that I'm going to walk through.</p><p>And we really got to, you know, kind of almost silly accuracy of like 99%. We got this class point of light award a few weeks ago for this. And really what we found here and what we observed is that the models reason very well. They get to a great baseline.</p><p>But if you're in an industry where you really need to eke out that, like, final mile of performance, you need to be able to then kind of give the model, give the pipeline that context. So how do we do that? Well, we call this our adaptive domain intelligence engine.</p><p>And what this is performing is it's taking customer-specific domain insights and it's converting them into performance improvements and kind of building a system around that. And there's broadly two main parts to this. The first part is the measurement side of things. So, you know, how is our current pipeline doing?</p><p>And then the rest of this is the improvement side. So I'm going to talk first a bit more about measurement in more detail and then a bit about improvements. So measuring domain-specific performance. The first thing, and I think, you know, a lot of this is really just kind of best practice more generally.</p><p>But the first step is to define what is it that your users really care about as metrics. So in a health context, obviously, I've been talking about medical necessity reviews. This is our bread and butter. And there, the customers really care about false approvals. They want to minimize false approvals because a false approval where you've approved care means that, you know, a patient who didn't need the care might get given some care they don't need.</p><p>And obviously, from an insurance provider point of view, they're then paying for treatment that they don't necessarily want to pay for. And often, defining these metrics is like a collaboration between the domain experts in your company and the customers to kind of like really translate what are the metrics that you care about.</p><p>There might be like one or two, or like usually there's a few metrics that matter most. So in a few other industries, like legal, when you're analyzing contracts, it might be that you really want to minimize the number of missed critical terms when you're identifying these clauses in the contract.</p><p>For fraud detection, your top-line metric might be something like preventing dollar loss from fraud. You know, education, it might be you want to optimize for test score improvements. I think it's definitely a helpful exercise to push yourself to think of like really, if I'm optimizing for like one or two metrics, what is like the metric that is most important?</p><p>And then what you can also do hand in hand with that, which is very helpful, just going off the bottom there a little bit, but is designing a failure mode ontology. And what I mean by this is taking the task that you're performing and identifying what are all the different ways in which my AI fails.</p><p>And it might be at the level of like higher order categories. So for example, here we've got medical record extraction, clinical reasoning and rules interpretation. We found that for medical necessity review, these are the three broad categories, the three broad ways in which the AI can fail. And then within those, there's various like different subtypes.</p><p>And this is an iterative process. There's like various techniques for doing this. I think it's important here to bring in your domain experts. I think one failure mode is that you have somebody kind of looking at your AI traces in isolation and coming up with these who don't necessarily have the context on how things are working.</p><p>I think this is a step that's critical to have domain experts leading this process. But really, I think the big value add is when you do both of these at the same time together. Because what this gives you, and this is a dashboard that we've built internally. I appreciate the text might be a little bit small.</p><p>But essentially, on the right hand side, you have a patient's medical record. You also have the guidelines that the record is being appraised against. On the left hand side, you have the AI outputs. So this is the decision that it's made, the reasoning behind its decision. And what we enable our domain experts to do here, enable our clinicians, is they can come in, they can mark whether it's correct or incorrect.</p><p>And if it's incorrect, then this box here is for defining the failure mode. So from that ontology we just saw on the slide before, they can say, this failed in this way. And doing those at the same point and having your domain experts sit at that point doing both of these is super valuable.</p><p>Because it then enables you to understand things like this. So on the x-axis here, we have number of false approvals. That's the metric that we really care about in our context. And then we have the different failure modes on the y-axis. And obviously, that tells us that if we want to minimize our false approvals and we want to optimize for this, this top North Star metric that we care about, these are what we want to address first.</p><p>Kind of in this order. Which, as a PM, is then a useful piece of information to help you prioritize the work that you want to do. So that's the measure side of things. I'm now going to go on to talk about the improvements. And particularly with this domain-specific context.</p><p>So what that also gives you, this kind of failure mode labeling we talked about before, is you get these ready-made data sets that you can iterate against. And these data sets are super valuable because they're coming directly from production data, which means you know that they're representative of the kind of input data distribution that you're going to see, more so than synthetic data would be.</p><p>And you can now, you know, when you had those priorities on the previous slide, we saw which sort of failure modes were causing the most false approvals. We can then pick that data set of, you know, 100 cases that came through prod in the last week that had this particular failure mode.</p><p>You can give that to an engineer, an engineer can iterate against it, and you can keep on testing, okay, how is my performance against that particular failure mode right now. And that lets you do something like this, where on the x-axis here, we have the pipeline version. On the y-axis, we have the performance score.</p><p>And by definition, on these floors, we're starting very low for each of these, like, failure mode data sets. But every time you increment your pipeline version, maybe you spent some time focusing on this particular failure mode, and you were able to get a big jump in performance. And then you can see the other ones also jumping up as well on kind of subsequent releases.</p><p>And you can also use this to then track that you're not regressing on any particular failure mode as well. So it's a useful visualization to be able to make. And you can then go one step further and actually bring your domain experts into the kind of improvements in the iteration itself.</p><p>And what that looks like is creating this tooling that enables a domain expert who's not necessarily technical to come in. They can then suggest changes to the application pipeline. They can also suggest new domain knowledge that's made available to the pipeline. And obviously, they're the best position to be making these kind of, you know, opinions of what sort of domain knowledge might be relevant.</p><p>And then you have your pipeline in the middle that's ready to use those if it wants to. And on the right-hand side, you have those domain evals, which might be these failure set evals. You might have more generic eval sets as well. And they can then tell you in a data-driven way, okay, given this domain knowledge suggestion from a domain expert, should that go live in the platform and now it's in production.</p><p>And then, you know, it should be improving the performance for live customers. And this whole loop can happen very quickly. So, for example, and I think actually on the next slide, yeah, I'll just show. So this is a dashboard we saw before. But this is with this extra button, which is like a domain knowledge addition button.</p><p>And so, again, we're keeping the same context. We have, you know, a domain expert clinician coming in here. They're reviewing the case. They're saying, is it correct? Is it incorrect? They're saying, what's the failure mode? And now they can say, I think this domain knowledge would be helpful for the application's performance.</p><p>And, you know, it might be, I think in this case, I appreciate it might not be that easy to read. But the model is kind of making some mistake related to understanding suspicion of a condition. Because the patient like has the condition and it says, oh, there's no suspicion of the condition.</p><p>But actually they have it. And, like, you could give some information to the model for the medical context of how we interpret suspicious or suspicion as a word. That would then influence the answer. Or it could be that maybe the reasoning uses some kind of scoring system and you realize actually the model doesn't have access to that scoring system.</p><p>You could, again, you could add that as domain knowledge to continually build out what the model can handle. And what that helps with, yeah, in terms of kind of the iteration speed from that, you can do that. Maybe you want to let your evals automatically let that go in.</p><p>Or maybe you want to have some kind of human in the loop. But it just means that you can have this very quick process. This prod case comes through. You analyze it through a clinical lens. And then the same day, you've essentially fixed it because you added the domain knowledge that should solve it.</p><p>You can prove that with the evals. And then it's live. And what this means is that these domain expert reviews that are really kind of powering a lot of the insights you're getting here are giving you three main things. They're giving you performance metrics. They're giving you these failure modes.</p><p>And they're giving you these suggested improvements all in one. Yep. Can you define domain expert? Like, what level are we talking about? Yeah, good question. So the question is, how do you define a domain expert? Like, what level of expertise do you need here? I think it really depends on the specific workflow that you're doing and what you're kind of optimizing for.</p><p>So in our context, if you're optimizing for clinical reasoning and the quality of the clinical reasoning, you therefore want somebody with as much clinical experience, ideally a doctor. Ideally, they have relevant expertise in the speciality that you're dealing with. But it kind of really depends on your use case.</p><p>It might be that there's actually simpler things we also can do, in which case that level of expertise is not necessary. And you could have a more junior clinical person. But the idea being that it's either like a nurse or a doctor or somebody that has experience of doing this workflow in the real world.</p><p>Does that make sense? Yeah, another question? Can you elaborate a little bit more on the tooling for the domain expert? Like, is this bespoke tooling or is this something off the shelf? Yeah, this is bespoke tooling. And I think in general, my philosophy on this is that if you're really placing a lot of weight on what you're kind of generating and this feeds into your system in various other different ways in the kind of ways I'm describing, it probably makes most sense to do this with bespoke tooling that you build yourself because you want to integrate it into the rest of your platform.</p><p>And it's just generally going to be easier to do that if you're kind of like doing everything yourself. Yeah. Are these, like are your domain experts users or are you paying them to come in and eval? Yeah, great question. I think it can be both. We, like in our experience, typically we start with we will hire some people in-house who kind of come and do this for us to give us this initial data so that we can do that iteration.</p><p>I think there's definitely a world in which the customer themselves might also want to do validation of your AI and they might actually do this kind of process themselves, in which case this then becomes a customer facing product for them to use as well. Yeah. Okay, so. I love the questions, but we're going to reserve time for Chris to keep going.</p><p>Yeah, sounds good. And I'm just, there's the last couple of slides now as well. So putting everything together, this is the overall flow. And essentially what this can look like is you have your production application. It's generating these decisions, these AI outputs. You're having your domain experts review that, giving these performance insights.</p><p>That's things like the metrics, the failure modes. You then have your PM, your domain expert PM who sits in the middle. They then have this rich information on, okay, what should I prioritize based on the failure modes, based on the metrics. They can then turn to an engineer and say, I want you to fix this failure mode because I really care about it.</p><p>And I want you to fix it up to this performance threshold. So they can say right now, you know, in production, we're getting 0% or 10% on this particular data set. I want you to go away and work on this until you get to 50%. And then the engineer can go and, you know, run different experiments, have different ideas of how they might improve this, changing prompting, changing models, doing fine tuning, all this kind of thing.</p><p>They then have a very tight iteration loop because they have these ready-made failure mode data sets. They can run the eval. They can see the impact of those evals. And then once they've kind of done that loop and they're hitting the percentage that they need, they can then go and give it back to the PM and say, hey, here are the changes I made.</p><p>This is the impact. The PM can then take that information and make some decision about going live. They can take those eval metrics. They can look at the kind of wider context of what this change might impact elsewhere in the product and then decide whether to go live with that in production.</p><p>So final takeaways just to wrap up. You know, to build a domain-native application, you need to solve the last mile problem. This isn't solved by just using more powerful models or more sophisticated pipelines. You need what we call an adaptive domain intelligence engine. Domain experts can power their system by reviewing their AI outputs to generate metrics, to generate failure modes, and to generate suggested improvements.</p><p>And this is really powerful because it takes production data live from kind of inside your customer's context. And it uses that to give your LLM product the nuanced understanding of the customer workflows and continually iterate towards that and eke out the kind of final performance level. And the end result is you have this self-improving data-driven process that can be managed by a domain expert PM sitting in the middle.</p><p>So thank you for your attention. If you're interested in kind of vertical AI applications or like evals and AI product management or generally, I've written about that at my website, chrislovejoy.me. Always interested to talk about this. So feel free to drop an email at chris@antirio.com. And we're also hiring as well at the moment.</p><p>So check out antirio.com/company for open roles. Thank you. Thank you. Thank you. We'll see you next time.</p></div></div></body></html>