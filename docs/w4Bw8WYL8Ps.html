<html><head><title>Stanford CS25: V1 I Decision Transformer: Reinforcement Learning via Sequence Modeling</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Decision Transformer: Reinforcement Learning via Sequence Modeling</h2><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps"><img src="https://i.ytimg.com/vi/w4Bw8WYL8Ps/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=203">3:23</a> Reinforcement Learning<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=209">3:29</a> What Is Reinforcement Learning<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=274">4:34</a> Offline Reinforcement Learning<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=482">8:2</a> Cause for Why Rl Typically Has Several Orders of Magnitude Fewer Parameters<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=599">9:59</a> Reason Why You Chose Offline Rl versus Online Rl<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=704">11:44</a> Causal Causal Transformer<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=878">14:38</a> Output<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=928">15:28</a> Differences with How a Decision Transformer Operates as Opposed to a Normal Transformer<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1218">20:18</a> Partial Observability<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1967">32:47</a> Offline Rl<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2676">44:36</a> How Much Time Does It Take To Train Distant Transformer<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2724">45:24</a> Person Behavioral Cloning<br><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3538">58:58</a> The Keycard Environment<br><br><div style="text-align: left;"><a href="./w4Bw8WYL8Ps.html">Whisper Transcript</a> | <a href="./transcript_w4Bw8WYL8Ps.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So I'm excited to talk today about our recent work on using transformers for reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=12" target="_blank">00:00:12.420</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=13" target="_blank">00:00:13.420</a></span> | <span class="t">And this is joint work with a bunch of really exciting collaborators, most of them at UC</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=20" target="_blank">00:00:20.640</a></span> | <span class="t">Berkeley, and some of them at Facebook and Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=24" target="_blank">00:00:24.480</a></span> | <span class="t">I should mention this work was led by two talented undergrads, Li Chen and Kevin Liu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=31" target="_blank">00:00:31.920</a></span> | <span class="t">And I'm excited to present the results we had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=35" target="_blank">00:00:35.280</a></span> | <span class="t">So let's try to motivate why we even care about this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=40" target="_blank">00:00:40.360</a></span> | <span class="t">So we have seen in the last three or four years that transformers, since the introduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=48" target="_blank">00:00:48.080</a></span> | <span class="t">in 2017, have taken over lots and lots of different fields of artificial intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=55" target="_blank">00:00:55.600</a></span> | <span class="t">So we saw them having a big impact for language processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=59" target="_blank">00:00:59.040</a></span> | <span class="t">We saw them being used for vision, using the vision transformer very recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=65" target="_blank">00:01:05.540</a></span> | <span class="t">They were in nature trying to solve protein folding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=69" target="_blank">00:01:09.560</a></span> | <span class="t">And very soon they might just replace us as computer scientists by having an automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=74" target="_blank">00:01:14.080</a></span> | <span class="t">generate code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=76" target="_blank">00:01:16.280</a></span> | <span class="t">So with all of these advances, it seems like we are getting closer to having a unified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=81" target="_blank">00:01:21.160</a></span> | <span class="t">model for decision-making for artificial intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=85" target="_blank">00:01:25.880</a></span> | <span class="t">But artificial intelligence is much more about not just having perception, but also using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=92" target="_blank">00:01:32.760</a></span> | <span class="t">the perception knowledge to make decisions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=95" target="_blank">00:01:35.120</a></span> | <span class="t">And this is what this talk is going to be about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=98" target="_blank">00:01:38.000</a></span> | <span class="t">But before I go into actually thinking about how we will use these models for decision-making,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=104" target="_blank">00:01:44.640</a></span> | <span class="t">here is a motivation for why I think it is important to ask this question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=110" target="_blank">00:01:50.020</a></span> | <span class="t">So unlike models for RL, when we look at transformers for perception modalities, like I showed in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=118" target="_blank">00:01:58.280</a></span> | <span class="t">the previous slide, we find that these models are very scalable and have very stable training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=125" target="_blank">00:02:05.200</a></span> | <span class="t">dynamics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=126" target="_blank">00:02:06.200</a></span> | <span class="t">So you can keep-- as long as you have enough computation and you have more and more data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=131" target="_blank">00:02:11.400</a></span> | <span class="t">that can be sourced, you can train bigger and bigger models and you'll see very smooth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=136" target="_blank">00:02:16.640</a></span> | <span class="t">reductions in the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=140" target="_blank">00:02:20.240</a></span> | <span class="t">And the overall training dynamics are very stable and this makes it very easy for practitioners</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=146" target="_blank">00:02:26.880</a></span> | <span class="t">and researchers to build these models and learn richer and richer distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=154" target="_blank">00:02:34.160</a></span> | <span class="t">So like I said, all of these advances have so far occurred in perception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=159" target="_blank">00:02:39.360</a></span> | <span class="t">What we'll be interested in this talk is to think about how we can go from perception,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=164" target="_blank">00:02:44.800</a></span> | <span class="t">looking at images, looking at text, and all these kinds of sensory signals, to then going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=169" target="_blank">00:02:49.840</a></span> | <span class="t">into the field of actually taking actions and making our agents do interesting things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=176" target="_blank">00:02:56.280</a></span> | <span class="t">in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=179" target="_blank">00:02:59.800</a></span> | <span class="t">And here, throughout the talk, we should be thinking about why this perspective is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=185" target="_blank">00:03:05.280</a></span> | <span class="t">to enable us to do scalable learning, like I showed in the previous slide, as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=190" target="_blank">00:03:10.480</a></span> | <span class="t">bring stability into the whole procedure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=193" target="_blank">00:03:13.760</a></span> | <span class="t">So sequential decision making is a very broad area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=197" target="_blank">00:03:17.560</a></span> | <span class="t">And what I'm specifically going to be focusing on today is the one route to sequential decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=203" target="_blank">00:03:23.560</a></span> | <span class="t">making, that's reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=206" target="_blank">00:03:26.460</a></span> | <span class="t">So just as a brief background, what is reinforcement learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=211" target="_blank">00:03:31.920</a></span> | <span class="t">So we are given an agent who is in a current state, and the agent is going to interact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=218" target="_blank">00:03:38.840</a></span> | <span class="t">with the environment by taking actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=222" target="_blank">00:03:42.960</a></span> | <span class="t">And by taking these actions, the environment is going to return to it a reward for how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=229" target="_blank">00:03:49.160</a></span> | <span class="t">good that action was, as well as the next state into which the agent will transition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=234" target="_blank">00:03:54.680</a></span> | <span class="t">and this whole feedback loop will continue on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=238" target="_blank">00:03:58.720</a></span> | <span class="t">The goal here for an intelligent agent is to then, using trial and error-- so try out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=245" target="_blank">00:04:05.520</a></span> | <span class="t">different actions, see what rewards will lead to-- learn a policy which maps your states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=251" target="_blank">00:04:11.240</a></span> | <span class="t">to actions, such that the policy maximizes the agent's cumulative rewards over time horizon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=257" target="_blank">00:04:17.760</a></span> | <span class="t">So you take a sequence of actions, and then based on the reward you accumulate for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=262" target="_blank">00:04:22.560</a></span> | <span class="t">sequence of actions, we'll judge how good your policy is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=267" target="_blank">00:04:27.960</a></span> | <span class="t">This talk is also going to be specifically focused on a form of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=273" target="_blank">00:04:33.800</a></span> | <span class="t">that goes by the name of offline reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=277" target="_blank">00:04:37.520</a></span> | <span class="t">So the idea here is that what changes from the previous picture where I was talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=283" target="_blank">00:04:43.040</a></span> | <span class="t">online reinforcement learning is that here, now instead of doing actively interacting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=290" target="_blank">00:04:50.080</a></span> | <span class="t">with the environment, you have a collection of log data of interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=295" target="_blank">00:04:55.740</a></span> | <span class="t">So think about some robot that's going out in the fields, and it collects a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=300" target="_blank">00:05:00.240</a></span> | <span class="t">sensory data, and you've all logged it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=302" target="_blank">00:05:02.960</a></span> | <span class="t">And using that log data, you now want to train another agent-- it could be another robot--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=309" target="_blank">00:05:09.200</a></span> | <span class="t">to then learn something interesting about that environment just by looking at the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=314" target="_blank">00:05:14.400</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=315" target="_blank">00:05:15.400</a></span> | <span class="t">So there's no trial and error component, which is currently one of the extensions of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=324" target="_blank">00:05:24.380</a></span> | <span class="t">framework, which will be very exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=326" target="_blank">00:05:26.520</a></span> | <span class="t">So I'll talk about this towards the end of the talk, why it's exciting to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=330" target="_blank">00:05:30.400</a></span> | <span class="t">how we can extend this framework to include an exploration component and have trial and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=337" target="_blank">00:05:37.000</a></span> | <span class="t">OK, so now to go more concretely into what the motivating challenge of this talk was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=344" target="_blank">00:05:44.240</a></span> | <span class="t">now that we have introduced RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=347" target="_blank">00:05:47.140</a></span> | <span class="t">So let's look at some statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=350" target="_blank">00:05:50.200</a></span> | <span class="t">So large language models have billions of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=355" target="_blank">00:05:55.380</a></span> | <span class="t">And today, they have roughly about 100 layers and transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=361" target="_blank">00:06:01.600</a></span> | <span class="t">They're very stable to train using supervised learning style losses, which are the building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=367" target="_blank">00:06:07.940</a></span> | <span class="t">blocks of autoregressive generation, for instance, or for mass language modeling, as in BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=375" target="_blank">00:06:15.920</a></span> | <span class="t">And this is like a field that's growing every day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=380" target="_blank">00:06:20.280</a></span> | <span class="t">And there's a course at Stanford that we're all taking just because it has had such a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=384" target="_blank">00:06:24.640</a></span> | <span class="t">monumental impact on AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=387" target="_blank">00:06:27.240</a></span> | <span class="t">RL policies, on the other hand-- and I'm talking about deep RL-- the maximum they would extend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=395" target="_blank">00:06:35.920</a></span> | <span class="t">to is maybe millions of parameters or 20 layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=402" target="_blank">00:06:42.080</a></span> | <span class="t">And what's really unnerving is that they're very unstable to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=406" target="_blank">00:06:46.200</a></span> | <span class="t">So the current algorithms for reinforcement learning, they're built on a mostly dynamic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=411" target="_blank">00:06:51.960</a></span> | <span class="t">programming, which involves solving an inner loop optimization problem that's very unstable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=417" target="_blank">00:06:57.280</a></span> | <span class="t">And it's very common to see practitioners in RL looking at reward codes that look like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=422" target="_blank">00:07:02.920</a></span> | <span class="t">this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=423" target="_blank">00:07:03.960</a></span> | <span class="t">So what I really want you to see here is the variance in the returns that we tend to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=429" target="_blank">00:07:09.720</a></span> | <span class="t">in RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=431" target="_blank">00:07:11.120</a></span> | <span class="t">It's really huge, even after doing multiple rounds of experimentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=435" target="_blank">00:07:15.660</a></span> | <span class="t">And that is really at the core got to done with the fact that our algorithms, our learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=442" target="_blank">00:07:22.960</a></span> | <span class="t">objectives, need better improvements so that the performance can be stably achieved by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=448" target="_blank">00:07:28.840</a></span> | <span class="t">agents in complex environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=453" target="_blank">00:07:33.440</a></span> | <span class="t">So what this work is hoping to do is it's going to introduce transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=461" target="_blank">00:07:41.440</a></span> | <span class="t">And I'll first show in one slide what exactly that model looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=465" target="_blank">00:07:45.640</a></span> | <span class="t">And then we're going to go into deeper details of each of the components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=469" target="_blank">00:07:49.920</a></span> | <span class="t">I have a--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=470" target="_blank">00:07:50.920</a></span> | <span class="t">Quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=471" target="_blank">00:07:51.920</a></span> | <span class="t">Yeah, can I ask a question real quick?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=472" target="_blank">00:07:52.920</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=473" target="_blank">00:07:53.920</a></span> | <span class="t">And--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=474" target="_blank">00:07:54.920</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=475" target="_blank">00:07:55.920</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=476" target="_blank">00:07:56.920</a></span> | <span class="t">What I'm curious to know is, what is the cause for why RL typically has several orders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=485" target="_blank">00:08:05.720</a></span> | <span class="t">of magnitude fewer parameters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=487" target="_blank">00:08:07.440</a></span> | <span class="t">That's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=491" target="_blank">00:08:11.000</a></span> | <span class="t">So typically, when you think about reinforcement learning algorithms, in deep RL in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=498" target="_blank">00:08:18.560</a></span> | <span class="t">so the most common algorithms, for example, have different networks playing different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=505" target="_blank">00:08:25.000</a></span> | <span class="t">roles in the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=509" target="_blank">00:08:29.200</a></span> | <span class="t">So you have a network, for instance, playing the role of an actor, so it's trying to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=514" target="_blank">00:08:34.400</a></span> | <span class="t">out a policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=515" target="_blank">00:08:35.400</a></span> | <span class="t">And then there'll be a different network that's playing the role of a critic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=519" target="_blank">00:08:39.600</a></span> | <span class="t">And these networks are trained on data that's adaptively gathered.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=525" target="_blank">00:08:45.340</a></span> | <span class="t">So unlike perception, where you will have a huge data set of interactions on which you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=531" target="_blank">00:08:51.760</a></span> | <span class="t">can train your models, in this case, the architectures and even the environments, to some extent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=540" target="_blank">00:09:00.160</a></span> | <span class="t">are very simplistic because of the fact that we are trying to train very small components,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=547" target="_blank">00:09:07.240</a></span> | <span class="t">the functions that we are training, and then bringing them all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=551" target="_blank">00:09:11.200</a></span> | <span class="t">And these functions are often trained in not super complex environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=557" target="_blank">00:09:17.760</a></span> | <span class="t">So it's a mix of different issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=560" target="_blank">00:09:20.020</a></span> | <span class="t">I wouldn't say it's purely just about the fact that the learning objectives are at fault,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=566" target="_blank">00:09:26.720</a></span> | <span class="t">but it's a combination of the environments we use, the combination of the targets that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=571" target="_blank">00:09:31.200</a></span> | <span class="t">each of the neural networks are predicting, which leads to networks which are much bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=577" target="_blank">00:09:37.560</a></span> | <span class="t">than what we currently see, tending to overfit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=581" target="_blank">00:09:41.920</a></span> | <span class="t">And that's why it's very common to see neural networks with much fewer layers being used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=587" target="_blank">00:09:47.640</a></span> | <span class="t">in RL as opposed to perception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=590" target="_blank">00:09:50.640</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=592" target="_blank">00:09:52.480</a></span> | <span class="t">Do you want to ask a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=598" target="_blank">00:09:58.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=599" target="_blank">00:09:59.200</a></span> | <span class="t">Yeah, I was going to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=600" target="_blank">00:10:00.200</a></span> | <span class="t">Is there a reason why you chose offline RL versus online RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=606" target="_blank">00:10:06.440</a></span> | <span class="t">That's another great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=607" target="_blank">00:10:07.680</a></span> | <span class="t">So the question is, why offline RL as opposed to online RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=612" target="_blank">00:10:12.080</a></span> | <span class="t">And the plain reason is because this is the first work trying to look at reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=617" target="_blank">00:10:17.120</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=618" target="_blank">00:10:18.120</a></span> | <span class="t">So offline RL avoids this problem of exploration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=623" target="_blank">00:10:23.640</a></span> | <span class="t">You are given a log data set of interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=625" target="_blank">00:10:25.800</a></span> | <span class="t">You're not allowed to further interact with the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=629" target="_blank">00:10:29.400</a></span> | <span class="t">So just from this data set, you're trying to unearth a policy of what the optimal agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=635" target="_blank">00:10:35.080</a></span> | <span class="t">would look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=636" target="_blank">00:10:36.620</a></span> | <span class="t">So it would.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=637" target="_blank">00:10:37.620</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=638" target="_blank">00:10:38.620</a></span> | <span class="t">If you do online RL, wouldn't that just give you this opportunity of exploration, basically?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=644" target="_blank">00:10:44.840</a></span> | <span class="t">It would.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=645" target="_blank">00:10:45.840</a></span> | <span class="t">It would.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=646" target="_blank">00:10:46.840</a></span> | <span class="t">And what it would also do, which is technically challenging here, is that the exploration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=653" target="_blank">00:10:53.000</a></span> | <span class="t">would be harder to encode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=654" target="_blank">00:10:54.800</a></span> | <span class="t">So offline RL is the first step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=656" target="_blank">00:10:56.480</a></span> | <span class="t">There's no reason why we should not study why online RL cannot be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=660" target="_blank">00:11:00.960</a></span> | <span class="t">It's just that it provides a more contained setup where ideas from transformers will directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=666" target="_blank">00:11:06.920</a></span> | <span class="t">extend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=667" target="_blank">00:11:07.920</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=668" target="_blank">00:11:08.920</a></span> | <span class="t">Sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=669" target="_blank">00:11:09.920</a></span> | <span class="t">So let's look at the model and it's really simple on purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=679" target="_blank">00:11:19.480</a></span> | <span class="t">So what we're going to do is we're going to look at our offline data, which is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=684" target="_blank">00:11:24.860</a></span> | <span class="t">in the form of trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=687" target="_blank">00:11:27.140</a></span> | <span class="t">So offline data would look like a sequence of states, actions, returns over multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=694" target="_blank">00:11:34.200</a></span> | <span class="t">time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=695" target="_blank">00:11:35.200</a></span> | <span class="t">It's a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=697" target="_blank">00:11:37.420</a></span> | <span class="t">So it's natural to think of us as directly feeding as input to a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=704" target="_blank">00:11:44.100</a></span> | <span class="t">In this case, we use a causal transformer as it's common in GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=710" target="_blank">00:11:50.160</a></span> | <span class="t">So we go from left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=711" target="_blank">00:11:51.820</a></span> | <span class="t">And because this dataset comes with the notion of time step, causality here is much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=717" target="_blank">00:11:57.400</a></span> | <span class="t">well-intended than the general meaning that's used for perception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=721" target="_blank">00:12:01.640</a></span> | <span class="t">This is really causality, how it should be in perspective of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=727" target="_blank">00:12:07.200</a></span> | <span class="t">What we predict out of this transformer are the actions conditioned on everything that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=734" target="_blank">00:12:14.240</a></span> | <span class="t">comes before that token in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=737" target="_blank">00:12:17.840</a></span> | <span class="t">So if you want to predict the action at this T minus one step, we'll use everything that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=742" target="_blank">00:12:22.480</a></span> | <span class="t">came at time step T minus two, as well as the returns and states at time step T minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=751" target="_blank">00:12:31.160</a></span> | <span class="t">one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=754" target="_blank">00:12:34.400</a></span> | <span class="t">So we will go into the details of how exactly each of these are encoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=761" target="_blank">00:12:41.320</a></span> | <span class="t">But essentially, this is in a one liner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">It's taking the trajectory data from the offline data, treating it as a sequence of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=770" target="_blank">00:12:50.400</a></span> | <span class="t">passing it through a causal transformer, and getting a sequence of actions as the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=776" target="_blank">00:12:56.280</a></span> | <span class="t">OK, so how exactly do we do the forward pass through the network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=783" target="_blank">00:13:03.720</a></span> | <span class="t">So one important aspect of this work, which is we use states, actions, and this quantity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=792" target="_blank">00:13:12.440</a></span> | <span class="t">called returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=794" target="_blank">00:13:14.880</a></span> | <span class="t">So these are not direct rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=796" target="_blank">00:13:16.400</a></span> | <span class="t">These are returns to go, and let's see what they really mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=802" target="_blank">00:13:22.880</a></span> | <span class="t">So this is our trajectory that goes as input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=808" target="_blank">00:13:28.160</a></span> | <span class="t">And the returns to go are the sum of rewards starting from the current time step until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=815" target="_blank">00:13:35.840</a></span> | <span class="t">the end of the episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=818" target="_blank">00:13:38.080</a></span> | <span class="t">So really what we want the transformer is to get better at using a target return-- this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=825" target="_blank">00:13:45.760</a></span> | <span class="t">is how you should think of returns to go-- as the input in deciding what action to take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=832" target="_blank">00:13:52.880</a></span> | <span class="t">This perspective is going to have multiple advantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=835" target="_blank">00:13:55.040</a></span> | <span class="t">It will allow us to actually do much more than offline RL and generalizing to different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=840" target="_blank">00:14:00.000</a></span> | <span class="t">tasks by just changing the returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=844" target="_blank">00:14:04.520</a></span> | <span class="t">And here it's very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=846" target="_blank">00:14:06.360</a></span> | <span class="t">So at time step one, we will just have the overall sum of rewards for the entire trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=852" target="_blank">00:14:12.840</a></span> | <span class="t">At time step two, we subtract the reward we get by taking the first action, and then have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=858" target="_blank">00:14:18.600</a></span> | <span class="t">the sum of rewards for the remainder of the trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=862" target="_blank">00:14:22.720</a></span> | <span class="t">OK, so that's how we call it returns to go, like how many more rewards in accumulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=870" target="_blank">00:14:30.360</a></span> | <span class="t">you need to acquire to fulfill your return goal that you set in the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=879" target="_blank">00:14:39.240</a></span> | <span class="t">What is the output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=880" target="_blank">00:14:40.600</a></span> | <span class="t">The output is the sequence of predicted actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=884" target="_blank">00:14:44.440</a></span> | <span class="t">So as I showed in the previous slide, we use a causal transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=889" target="_blank">00:14:49.040</a></span> | <span class="t">So we'll predict in sequence the desired actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=895" target="_blank">00:14:55.320</a></span> | <span class="t">The attention, which is going to be computed inside the transformer, will take in an important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=903" target="_blank">00:15:03.680</a></span> | <span class="t">hyperparameter k, which is the context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=906" target="_blank">00:15:06.680</a></span> | <span class="t">We see that in perception as well here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=909" target="_blank">00:15:09.480</a></span> | <span class="t">And for the rest of the talk, I'm going to use the notation k to denote how many tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=914" target="_blank">00:15:14.000</a></span> | <span class="t">in the past would we be attending over to predict the action and the current time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=921" target="_blank">00:15:21.680</a></span> | <span class="t">OK, so again, digging a little bit deeper into code, there are some subtle differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=929" target="_blank">00:15:29.680</a></span> | <span class="t">with how a decision transformer operates as opposed to a normal transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=937" target="_blank">00:15:37.920</a></span> | <span class="t">The first is that here, the time step notion is going to have a much bigger semantics that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=946" target="_blank">00:15:46.880</a></span> | <span class="t">extends across three tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=950" target="_blank">00:15:50.400</a></span> | <span class="t">So in perception, you just think about the time step per word, for instance, like an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=957" target="_blank">00:15:57.040</a></span> | <span class="t">NLP or per patch for vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=959" target="_blank">00:15:59.880</a></span> | <span class="t">And in this case, we will have a time step encapsulating three tokens, one for the states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=965" target="_blank">00:16:05.680</a></span> | <span class="t">one for the actions, and one for the rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=969" target="_blank">00:16:09.120</a></span> | <span class="t">And then we'll embed each of these tokens and then add the position embedding as is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=975" target="_blank">00:16:15.200</a></span> | <span class="t">common in a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=978" target="_blank">00:16:18.040</a></span> | <span class="t">And we feed those inputs to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=983" target="_blank">00:16:23.000</a></span> | <span class="t">At the output, we only care about one of these three tokens in this default setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=987" target="_blank">00:16:27.920</a></span> | <span class="t">I will show experiments where even the other tokens might be of interest as target predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=993" target="_blank">00:16:33.360</a></span> | <span class="t">But for now, let's keep it simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=994" target="_blank">00:16:34.800</a></span> | <span class="t">We want to learn a policy, a policy that's trying to predict actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=998" target="_blank">00:16:38.840</a></span> | <span class="t">So when we try to decode, we'll only be looking at the actions from the hidden representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1006" target="_blank">00:16:46.160</a></span> | <span class="t">in the pre-final layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1007" target="_blank">00:16:47.640</a></span> | <span class="t">OK, so this is the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1009" target="_blank">00:16:49.920</a></span> | <span class="t">Now, what do we do with this network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1011" target="_blank">00:16:51.720</a></span> | <span class="t">We train it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1012" target="_blank">00:16:52.720</a></span> | <span class="t">How do we train it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1013" target="_blank">00:16:53.720</a></span> | <span class="t">Sorry, just a quick question on semantics there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1017" target="_blank">00:16:57.760</a></span> | <span class="t">If you go back one slide, the plus in this case, the syntax means that you are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1023" target="_blank">00:17:03.040</a></span> | <span class="t">adding the values element-wise and not concatenating them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1025" target="_blank">00:17:05.440</a></span> | <span class="t">Is that right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1026" target="_blank">00:17:06.440</a></span> | <span class="t">That is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1027" target="_blank">00:17:07.440</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1028" target="_blank">00:17:08.440</a></span> | <span class="t">So let me check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1029" target="_blank">00:17:09.440</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1030" target="_blank">00:17:10.440</a></span> | <span class="t">OK, so what's the last function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1031" target="_blank">00:17:11.440</a></span> | <span class="t">Follow up on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1032" target="_blank">00:17:12.440</a></span> | <span class="t">I thought it was concatenated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1033" target="_blank">00:17:13.440</a></span> | <span class="t">Why are we just adding it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1034" target="_blank">00:17:14.440</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1035" target="_blank">00:17:15.440</a></span> | <span class="t">Can you go back?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1036" target="_blank">00:17:16.440</a></span> | <span class="t">Yeah, I think it's a design choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1037" target="_blank">00:17:17.440</a></span> | <span class="t">You can concatenate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1038" target="_blank">00:17:18.440</a></span> | <span class="t">You can add it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1039" target="_blank">00:17:19.440</a></span> | <span class="t">It leads to different functions being encoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1040" target="_blank">00:17:20.440</a></span> | <span class="t">In our case, it was addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1054" target="_blank">00:17:34.760</a></span> | <span class="t">OK, why did you-- did you try the other one and it just didn't work, or why is that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1062" target="_blank">00:17:42.920</a></span> | <span class="t">Because I think intuitively, concatenating would make more sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1068" target="_blank">00:17:48.880</a></span> | <span class="t">So I think both of them have different use cases for the functional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1075" target="_blank">00:17:55.520</a></span> | <span class="t">One is really mixing in the embeddings for the state and basically shifting it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1081" target="_blank">00:18:01.760</a></span> | <span class="t">So when you add something, if you think of the embedding of the states as a vector, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1089" target="_blank">00:18:09.400</a></span> | <span class="t">you add something, you are actually shifting it, whereas in the concatenation case, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1095" target="_blank">00:18:15.080</a></span> | <span class="t">are actually increasing the dimensionality of the space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1100" target="_blank">00:18:20.640</a></span> | <span class="t">So those are different choices, which are doing very different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1105" target="_blank">00:18:25.680</a></span> | <span class="t">We found this one to work better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1107" target="_blank">00:18:27.960</a></span> | <span class="t">I'm not sure I remember if the results were very significantly different if you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1112" target="_blank">00:18:32.880</a></span> | <span class="t">concatenate them, but this is the one which we operate with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1117" target="_blank">00:18:37.560</a></span> | <span class="t">But wouldn't there-- because if you're shifting it, if you have an embedding for a state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1121" target="_blank">00:18:41.440</a></span> | <span class="t">let's say you perform certain actions and you end up at the same state again, you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1127" target="_blank">00:18:47.120</a></span> | <span class="t">want these embeddings to be the same, however, now you're at a different time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1131" target="_blank">00:18:51.480</a></span> | <span class="t">So you shifted it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1132" target="_blank">00:18:52.960</a></span> | <span class="t">So wouldn't that be harder to learn?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1135" target="_blank">00:18:55.960</a></span> | <span class="t">So there's a bigger and interesting question in that what you said is basically, are we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1141" target="_blank">00:19:01.640</a></span> | <span class="t">losing the Markov property?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1144" target="_blank">00:19:04.800</a></span> | <span class="t">Because as you said, if you come back to the same state at a different time step, shouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1151" target="_blank">00:19:11.400</a></span> | <span class="t">we be doing similar operations?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1155" target="_blank">00:19:15.680</a></span> | <span class="t">And the answer here is yes, we are actually being non-Markov.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1160" target="_blank">00:19:20.080</a></span> | <span class="t">And this might seem very non-intuitive at first, that why is non-Markovness important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1168" target="_blank">00:19:28.080</a></span> | <span class="t">here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1170" target="_blank">00:19:30.080</a></span> | <span class="t">And I want to refer to another paper which came very much in conjunction with this, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1175" target="_blank">00:19:35.640</a></span> | <span class="t">triarchy transformer, that actually shows in more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1178" target="_blank">00:19:38.680</a></span> | <span class="t">And it basically says that if you were trying to predict the transition dynamics, then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1183" target="_blank">00:19:43.480</a></span> | <span class="t">could have actually had a Markovian system built in here, which would do just as good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1190" target="_blank">00:19:50.520</a></span> | <span class="t">However, for the perspective of trying to actually predict actions, it does have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1198" target="_blank">00:19:58.440</a></span> | <span class="t">look at the previous time steps, even more so when you have missing observations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1203" target="_blank">00:20:03.200</a></span> | <span class="t">So for instance, if you have the observations being a substrate of the true state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1209" target="_blank">00:20:09.440</a></span> | <span class="t">So looking at the previous states and actions helps you better fill in the missing pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1216" target="_blank">00:20:16.640</a></span> | <span class="t">in some sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1217" target="_blank">00:20:17.640</a></span> | <span class="t">So this is commonly known as partial observability, where by looking at the previous tokens, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1222" target="_blank">00:20:22.840</a></span> | <span class="t">can do a better job at predicting the actions that you should take at the current time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1231" target="_blank">00:20:31.320</a></span> | <span class="t">So non-Markovness is on purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1235" target="_blank">00:20:35.280</a></span> | <span class="t">And it's not intuitive, but I think it's one of the things that separates this framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1241" target="_blank">00:20:41.760</a></span> | <span class="t">from existing ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1244" target="_blank">00:20:44.680</a></span> | <span class="t">So it will basically help you-- because RL usually works better on infinite horizon problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1251" target="_blank">00:20:51.400</a></span> | <span class="t">right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1252" target="_blank">00:20:52.400</a></span> | <span class="t">So technically, the way you formulate it, it would work better on finite horizon problems,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1256" target="_blank">00:20:56.040</a></span> | <span class="t">I'm assuming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1257" target="_blank">00:20:57.040</a></span> | <span class="t">Because you want to take different actions based on the history, based on given a fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1260" target="_blank">00:21:00.800</a></span> | <span class="t">that now you're at a different time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1264" target="_blank">00:21:04.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1265" target="_blank">00:21:05.040</a></span> | <span class="t">So if you wanted to work on infinite horizon, maybe something like discounting would work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1270" target="_blank">00:21:10.280</a></span> | <span class="t">just as well to get that effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1272" target="_blank">00:21:12.680</a></span> | <span class="t">In this case, we were using a discount factor of 1, or basically no discounting at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1279" target="_blank">00:21:19.960</a></span> | <span class="t">But you're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1280" target="_blank">00:21:20.960</a></span> | <span class="t">If I think we really want to extend it to infinite horizon, we would need to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1284" target="_blank">00:21:24.400</a></span> | <span class="t">the discount factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1285" target="_blank">00:21:25.400</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1286" target="_blank">00:21:26.400</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1287" target="_blank">00:21:27.400</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1291" target="_blank">00:21:31.680</a></span> | <span class="t">So--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1292" target="_blank">00:21:32.680</a></span> | <span class="t">Quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1293" target="_blank">00:21:33.680</a></span> | <span class="t">Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1294" target="_blank">00:21:34.680</a></span> | <span class="t">I think it was just answered in chat, but I'll ask it anyways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1298" target="_blank">00:21:38.120</a></span> | <span class="t">I think I might have missed this, or maybe you're about to talk about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1300" target="_blank">00:21:40.760</a></span> | <span class="t">The offline data that was collected, what policy was used to collect it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1304" target="_blank">00:21:44.600</a></span> | <span class="t">So this is a very important question, and it will be something I mentioned in the experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1312" target="_blank">00:21:52.640</a></span> | <span class="t">So we were using the benchmarks that exist for offline RL, where essentially the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1318" target="_blank">00:21:58.560</a></span> | <span class="t">these benchmarks are constructed is you train an agent using online RL, and then you look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1323" target="_blank">00:22:03.840</a></span> | <span class="t">at its replay buffer at some time step while it's training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1328" target="_blank">00:22:08.960</a></span> | <span class="t">So while it's like a medium sort of expert, you collect the transitions it's experienced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1335" target="_blank">00:22:15.420</a></span> | <span class="t">so far and make that as the offline data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1338" target="_blank">00:22:18.240</a></span> | <span class="t">It's something which is-- like our framework is very agnostic to what offline data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1343" target="_blank">00:22:23.560</a></span> | <span class="t">you use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1344" target="_blank">00:22:24.560</a></span> | <span class="t">So I've not discussed it so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1347" target="_blank">00:22:27.680</a></span> | <span class="t">But it's something that in our experiments is based on traditional benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1352" target="_blank">00:22:32.000</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1353" target="_blank">00:22:33.000</a></span> | <span class="t">So the reason I ask isn't-- I'm sure that your framework can accommodate any offline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1357" target="_blank">00:22:37.200</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1358" target="_blank">00:22:38.200</a></span> | <span class="t">But it seems to me like the results that you're about to present are going to be heavily contingent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1361" target="_blank">00:22:41.840</a></span> | <span class="t">on what that data collection policy is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1365" target="_blank">00:22:45.240</a></span> | <span class="t">Indeed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1366" target="_blank">00:22:46.240</a></span> | <span class="t">Indeed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1367" target="_blank">00:22:47.240</a></span> | <span class="t">And also-- so we will-- I think I have a slide where we show an experiment where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1373" target="_blank">00:22:53.720</a></span> | <span class="t">amount of data can make a difference in how we compare with baselines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1378" target="_blank">00:22:58.760</a></span> | <span class="t">And essentially, we will see how this [INAUDIBLE] especially shines when there is small amounts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1386" target="_blank">00:23:06.060</a></span> | <span class="t">of offline data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1387" target="_blank">00:23:07.060</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1388" target="_blank">00:23:08.060</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1389" target="_blank">00:23:09.060</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1390" target="_blank">00:23:10.060</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1391" target="_blank">00:23:11.060</a></span> | <span class="t">Great questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1392" target="_blank">00:23:12.060</a></span> | <span class="t">So let's go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1394" target="_blank">00:23:14.320</a></span> | <span class="t">So we have defined our model, which is going to look at these trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1401" target="_blank">00:23:21.200</a></span> | <span class="t">And now, let's see how we train it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1403" target="_blank">00:23:23.720</a></span> | <span class="t">So very simple, we are trying to predict actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1407" target="_blank">00:23:27.520</a></span> | <span class="t">We'll try to match them to the ones we have in our data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1410" target="_blank">00:23:30.840</a></span> | <span class="t">If they are continuous, using the mean squared error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1413" target="_blank">00:23:33.160</a></span> | <span class="t">If they are discrete, then we can use the cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1418" target="_blank">00:23:38.320</a></span> | <span class="t">But there is something very deep in here for our research, which is that these objectives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1425" target="_blank">00:23:45.760</a></span> | <span class="t">are very stable to train and easy to regularize because they've been developed for supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1430" target="_blank">00:23:50.520</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1432" target="_blank">00:23:52.360</a></span> | <span class="t">In contrast, what RL is more used to is dynamic programming style objectives, which are based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1437" target="_blank">00:23:57.640</a></span> | <span class="t">on the Bellman equation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1440" target="_blank">00:24:00.040</a></span> | <span class="t">And those end up being much harder to optimize and scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1445" target="_blank">00:24:05.160</a></span> | <span class="t">And that's why you see a lot of the variance in the results as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1449" target="_blank">00:24:09.920</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1451" target="_blank">00:24:11.540</a></span> | <span class="t">So this is how we train the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1453" target="_blank">00:24:13.280</a></span> | <span class="t">Now, how do we use the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1454" target="_blank">00:24:14.920</a></span> | <span class="t">And that's the point about trying to do rollout for the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1459" target="_blank">00:24:19.800</a></span> | <span class="t">So here, again, this is going to be similar to doing an autoregressive generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1466" target="_blank">00:24:26.640</a></span> | <span class="t">There is an important token here, which was the returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1470" target="_blank">00:24:30.840</a></span> | <span class="t">And what we need to set during evaluation, presumably, we want export level performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1477" target="_blank">00:24:37.640</a></span> | <span class="t">because that will have the highest returns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1481" target="_blank">00:24:41.080</a></span> | <span class="t">So we set the initial returns to go, not based on our trajectory, because now we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1487" target="_blank">00:24:47.280</a></span> | <span class="t">a trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1488" target="_blank">00:24:48.280</a></span> | <span class="t">We're going to generate a trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1489" target="_blank">00:24:49.280</a></span> | <span class="t">So this is at entrance time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1491" target="_blank">00:24:51.080</a></span> | <span class="t">So we'll set it to the export return, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1494" target="_blank">00:24:54.800</a></span> | <span class="t">So in code, what this whole procedure would look like is basically you set this returns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1499" target="_blank">00:24:59.560</a></span> | <span class="t">to go token to have some target return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1503" target="_blank">00:25:03.800</a></span> | <span class="t">And you set your initial state to run from the environment distribution of initial states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1512" target="_blank">00:25:12.240</a></span> | <span class="t">And then you just roll out your decision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1516" target="_blank">00:25:16.100</a></span> | <span class="t">So you get a new action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1518" target="_blank">00:25:18.840</a></span> | <span class="t">This action will also give you a state and reward from the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1523" target="_blank">00:25:23.400</a></span> | <span class="t">You append them to your sequence, and you get a new returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1529" target="_blank">00:25:29.760</a></span> | <span class="t">And you take just the context and key, because that's what's used by the transformer to making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1533" target="_blank">00:25:33.920</a></span> | <span class="t">predictions, and then feed it back to the decision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1538" target="_blank">00:25:38.480</a></span> | <span class="t">So it's regular autoregressive generation, but the only key point to notice is how you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1544" target="_blank">00:25:44.600</a></span> | <span class="t">initialize the transformer for RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1549" target="_blank">00:25:49.000</a></span> | <span class="t">Sorry, I had one question here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1552" target="_blank">00:25:52.920</a></span> | <span class="t">How much does the choice of the export target return matter?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1555" target="_blank">00:25:55.840</a></span> | <span class="t">Does it have to be the mean export reward, or can it be the maximum reward possible in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1559" target="_blank">00:25:59.280</a></span> | <span class="t">the environment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1560" target="_blank">00:26:00.280</a></span> | <span class="t">Does the choice of the number really matter?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1564" target="_blank">00:26:04.480</a></span> | <span class="t">That's a very good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1565" target="_blank">00:26:05.640</a></span> | <span class="t">So we generally would set it to be slightly higher than the max return in the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1576" target="_blank">00:26:16.040</a></span> | <span class="t">So I think the factor we use is 1.1 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1580" target="_blank">00:26:20.280</a></span> | <span class="t">But I think we have done a lot of experimentation in the range, and it's fairly robust to what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1589" target="_blank">00:26:29.880</a></span> | <span class="t">choice you use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1591" target="_blank">00:26:31.080</a></span> | <span class="t">So for example, for Hopper, export returns about 3,600, and we have found very stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1596" target="_blank">00:26:36.760</a></span> | <span class="t">performance all the way from 3,500, 3,400 to even going to very high numbers, like 5,000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1606" target="_blank">00:26:46.340</a></span> | <span class="t">it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1608" target="_blank">00:26:48.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1611" target="_blank">00:26:51.000</a></span> | <span class="t">So however, I would want to point out that this is something which is not typically needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1618" target="_blank">00:26:58.240</a></span> | <span class="t">in regular RL, like knowing the export return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1622" target="_blank">00:27:02.560</a></span> | <span class="t">Here we are actually going beyond regular RL in that we can choose a return we want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1627" target="_blank">00:27:07.000</a></span> | <span class="t">so we also actually need this information about what the export return is at test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1633" target="_blank">00:27:13.720</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1634" target="_blank">00:27:14.720</a></span> | <span class="t">There's another--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1635" target="_blank">00:27:15.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1636" target="_blank">00:27:16.720</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1637" target="_blank">00:27:17.720</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1638" target="_blank">00:27:18.720</a></span> | <span class="t">So it's just that you cannot be on the regular RL, but I'm curious about do you also restrict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1648" target="_blank">00:27:28.240</a></span> | <span class="t">this framework to only offline RL, because if you want to run this kind of framework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1654" target="_blank">00:27:34.520</a></span> | <span class="t">in online RL, you'll have to determine the returns to go a priori.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1660" target="_blank">00:27:40.220</a></span> | <span class="t">So this kind of framework, I think it's kind of restricted to only offline RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1664" target="_blank">00:27:44.600</a></span> | <span class="t">Do you think so?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1666" target="_blank">00:27:46.320</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1667" target="_blank">00:27:47.320</a></span> | <span class="t">And I think asking this question as well earlier, that yes, I think for now, this is the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1675" target="_blank">00:27:55.320</a></span> | <span class="t">book, so we were focusing on offline RL where this information can be gathered from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1680" target="_blank">00:28:00.920</a></span> | <span class="t">offline data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1684" target="_blank">00:28:04.700</a></span> | <span class="t">It is possible to think about strategies on how you can even get this online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1690" target="_blank">00:28:10.720</a></span> | <span class="t">What you'll need is a curriculum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1692" target="_blank">00:28:12.080</a></span> | <span class="t">So early on during training as we're gathering data, you will set-- when you're doing rollouts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1698" target="_blank">00:28:18.760</a></span> | <span class="t">you will set your expert return to whatever you see in the data set, and then increment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1705" target="_blank">00:28:25.400</a></span> | <span class="t">it as and when you start seeing that the transformer can actually exceed that performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1711" target="_blank">00:28:31.160</a></span> | <span class="t">So you can think of specifying a curriculum from slow to high for what that expert return</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1717" target="_blank">00:28:37.440</a></span> | <span class="t">could be for which you roll out the decision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1722" target="_blank">00:28:42.920</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1723" target="_blank">00:28:43.920</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1724" target="_blank">00:28:44.920</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1725" target="_blank">00:28:45.920</a></span> | <span class="t">So yeah, this was about the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1729" target="_blank">00:28:49.940</a></span> | <span class="t">So we discussed how this model is-- what the input to this model are, what the outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1737" target="_blank">00:28:57.500</a></span> | <span class="t">are, what the loss function is used for training this model, and how do we use this model at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1742" target="_blank">00:29:02.920</a></span> | <span class="t">test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1745" target="_blank">00:29:05.160</a></span> | <span class="t">There is a connection to this framework as being one way to instantiate what is often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1752" target="_blank">00:29:12.680</a></span> | <span class="t">known as RLS probabilistic inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1755" target="_blank">00:29:15.760</a></span> | <span class="t">So we can formulate RL as a graphical model problem where you have the states and actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1763" target="_blank">00:29:23.920</a></span> | <span class="t">being used to determine what the next state is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1767" target="_blank">00:29:27.520</a></span> | <span class="t">And to encode a notion of optimality, typically you would also have these additional auxiliary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1772" target="_blank">00:29:32.320</a></span> | <span class="t">variables, O1, O2, and so on and forth, which are implicitly saying that encoding some notion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1779" target="_blank">00:29:39.400</a></span> | <span class="t">of reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1780" target="_blank">00:29:40.720</a></span> | <span class="t">And conditioned on this optimality being true, RL is the task of learning a policy, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1788" target="_blank">00:29:48.120</a></span> | <span class="t">is the mapping from states to actions such that we get optimal behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1796" target="_blank">00:29:56.920</a></span> | <span class="t">And if you really squint your eyes, you can see that these optimality variables and decision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1803" target="_blank">00:30:03.440</a></span> | <span class="t">transformers are actually being encoded by the returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1807" target="_blank">00:30:07.600</a></span> | <span class="t">So if when we give a value that's high enough at test time during rollouts, like the expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1813" target="_blank">00:30:13.680</a></span> | <span class="t">return, we are essentially saying that conditioned on this being the mathematical quantification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1823" target="_blank">00:30:23.480</a></span> | <span class="t">of optimality, roll out your decision transformer to hopefully satisfy this condition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1834" target="_blank">00:30:34.200</a></span> | <span class="t">So yeah, so this was all I want to talk about the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1838" target="_blank">00:30:38.480</a></span> | <span class="t">Can you explain that, please?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1841" target="_blank">00:30:41.200</a></span> | <span class="t">What do you mean by optimality variables in the decision transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1844" target="_blank">00:30:44.680</a></span> | <span class="t">And how do you mean like return to go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1846" target="_blank">00:30:46.960</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1847" target="_blank">00:30:47.960</a></span> | <span class="t">So optimality variables, we can think in the most simplest context as, let's just say they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1855" target="_blank">00:30:55.760</a></span> | <span class="t">were binary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1856" target="_blank">00:30:56.880</a></span> | <span class="t">So 1 is if you solve the goal, and 0 is if you did not solve the goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1863" target="_blank">00:31:03.600</a></span> | <span class="t">And what basically in that case, you could also think of your decision transformer as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1872" target="_blank">00:31:12.200</a></span> | <span class="t">at test time and we encode the returns to go, we could set it to 1, which would basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1879" target="_blank">00:31:19.040</a></span> | <span class="t">mean that conditioned on optimality-- so optimality here means solving the goal as 1-- generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1888" target="_blank">00:31:28.760</a></span> | <span class="t">me the sequence of actions such that this would be true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1894" target="_blank">00:31:34.160</a></span> | <span class="t">Of course, our learning is not perfect, so it's not guaranteed we'll get that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1899" target="_blank">00:31:39.720</a></span> | <span class="t">But we have trained the transformer in a way to interpret the returns to go as some notion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1905" target="_blank">00:31:45.520</a></span> | <span class="t">of optimality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1909" target="_blank">00:31:49.300</a></span> | <span class="t">So if I'm interpreting this correctly, it's roughly like saying, show me what an optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1915" target="_blank">00:31:55.920</a></span> | <span class="t">sequence of transitions look like, because the model has learned both successful and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1920" target="_blank">00:32:00.920</a></span> | <span class="t">unsuccessful transitions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1922" target="_blank">00:32:02.440</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1923" target="_blank">00:32:03.440</a></span> | <span class="t">Exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1924" target="_blank">00:32:04.440</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1925" target="_blank">00:32:05.440</a></span> | <span class="t">And as we've seen some experiments, for the binary case, it's either optimal or non-optimal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1932" target="_blank">00:32:12.720</a></span> | <span class="t">But really, this can be a continuous variable, which it is in our experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1936" target="_blank">00:32:16.720</a></span> | <span class="t">So we can also see what happens in between experimentally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1940" target="_blank">00:32:20.840</a></span> | <span class="t">OK, so let's jump into the experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1947" target="_blank">00:32:27.520</a></span> | <span class="t">So there are a bunch of experiments, and I've picked out a few which I think are interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1953" target="_blank">00:32:33.400</a></span> | <span class="t">and give the key results in the paper, but feel free to refer to the paper for an even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1959" target="_blank">00:32:39.240</a></span> | <span class="t">more detailed analysis on some of the components of our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1966" target="_blank">00:32:46.520</a></span> | <span class="t">So first, we can look at how well does it do an offline RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1969" target="_blank">00:32:49.880</a></span> | <span class="t">So there are benchmarks for the Atari suite of environments and the OpenAI Gym.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1976" target="_blank">00:32:56.800</a></span> | <span class="t">And we have another environment, Key2Door, which is especially hard because it contains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1981" target="_blank">00:33:01.680</a></span> | <span class="t">sparse rewards and requires you to do credit assignment that I'll talk about later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1986" target="_blank">00:33:06.340</a></span> | <span class="t">But across the board, we see that decision transformer is competitive with the state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1993" target="_blank">00:33:13.600</a></span> | <span class="t">model-free offline RL methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=1995" target="_blank">00:33:15.480</a></span> | <span class="t">In this case, this was a version of Q-learning designed for offline RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2001" target="_blank">00:33:21.960</a></span> | <span class="t">And it can do excellent, especially when there is long-term credit assignment where traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2008" target="_blank">00:33:28.280</a></span> | <span class="t">methods based on TD learning would fail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2011" target="_blank">00:33:31.480</a></span> | <span class="t">Yeah, so the takeaway here should not be that we should be at the stage where we can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2017" target="_blank">00:33:37.560</a></span> | <span class="t">simply substitute the existing algorithms for the decision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2022" target="_blank">00:33:42.600</a></span> | <span class="t">But this is a very strong evidence in favor that this paradigm which is building on transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2029" target="_blank">00:33:49.640</a></span> | <span class="t">will permit us to better iterate and improve the models to hopefully surpass the existing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2037" target="_blank">00:33:57.640</a></span> | <span class="t">algorithms uniformly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2039" target="_blank">00:33:59.600</a></span> | <span class="t">And there's some early evidence of that in harder environments, which do require long-term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2044" target="_blank">00:34:04.520</a></span> | <span class="t">credit assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2045" target="_blank">00:34:05.520</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2046" target="_blank">00:34:06.520</a></span> | <span class="t">Can I ask a question here about the baseline, specifically TD learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2052" target="_blank">00:34:12.240</a></span> | <span class="t">I'm curious to know, because I know that a lot of TD learning agents are feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2055" target="_blank">00:34:15.640</a></span> | <span class="t">networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2056" target="_blank">00:34:16.640</a></span> | <span class="t">Are these baselines, do they have recurrence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2059" target="_blank">00:34:19.560</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2062" target="_blank">00:34:22.280</a></span> | <span class="t">So I think the conservative Q-learning baselines here did have recurrence, but I'm not very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2069" target="_blank">00:34:29.040</a></span> | <span class="t">sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2070" target="_blank">00:34:30.040</a></span> | <span class="t">So I can check back on this offline and get back to you on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2073" target="_blank">00:34:33.640</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2074" target="_blank">00:34:34.640</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2075" target="_blank">00:34:35.640</a></span> | <span class="t">Also, another quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2076" target="_blank">00:34:36.640</a></span> | <span class="t">So just how exactly do you evaluate the decision transformer here in the experiment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2086" target="_blank">00:34:46.440</a></span> | <span class="t">So because you need to supply the returns to go, so do you use the optimal policy to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2094" target="_blank">00:34:54.080</a></span> | <span class="t">get what's the optimal rewards and speed that in?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2097" target="_blank">00:34:57.360</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2098" target="_blank">00:34:58.360</a></span> | <span class="t">So here we basically look at the offline data set that was used for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2102" target="_blank">00:35:02.160</a></span> | <span class="t">And we said, whatever was the maximum return in the offline data, we set the desired target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2110" target="_blank">00:35:10.400</a></span> | <span class="t">return to go as slightly higher than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2114" target="_blank">00:35:14.080</a></span> | <span class="t">So 1.1 was the coefficient we used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2117" target="_blank">00:35:17.680</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2118" target="_blank">00:35:18.680</a></span> | <span class="t">So the performance-- sorry, I'm not really well-versed in RLs, but how is the performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2124" target="_blank">00:35:24.360</a></span> | <span class="t">defined here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2125" target="_blank">00:35:25.360</a></span> | <span class="t">It's just like, is it how much reward you get actually from the--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2128" target="_blank">00:35:28.800</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2129" target="_blank">00:35:29.800</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2130" target="_blank">00:35:30.800</a></span> | <span class="t">So you can specify a target return to go, but there's no guarantee that the actual actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2136" target="_blank">00:35:36.560</a></span> | <span class="t">that you take will achieve that return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2140" target="_blank">00:35:40.120</a></span> | <span class="t">So you measure the true environment return based on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2145" target="_blank">00:35:45.880</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2146" target="_blank">00:35:46.880</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2147" target="_blank">00:35:47.880</a></span> | <span class="t">But then just curious, so are these performance the percentage you get for how much reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2155" target="_blank">00:35:55.320</a></span> | <span class="t">you recover from the actual environment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2158" target="_blank">00:35:58.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2159" target="_blank">00:35:59.200</a></span> | <span class="t">So these are not percentages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2160" target="_blank">00:36:00.200</a></span> | <span class="t">These are some way of normalizing the return so that everything falls between 0 to 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2165" target="_blank">00:36:05.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2166" target="_blank">00:36:06.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2167" target="_blank">00:36:07.520</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2168" target="_blank">00:36:08.520</a></span> | <span class="t">Then I just wonder if you have a rough idea about how much reward actually is recovered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2173" target="_blank">00:36:13.720</a></span> | <span class="t">by decision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2175" target="_blank">00:36:15.600</a></span> | <span class="t">Does it say, if you specify, I want to get 50 rewards, does it get 49?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2180" target="_blank">00:36:20.040</a></span> | <span class="t">Or is this even better sometimes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2184" target="_blank">00:36:24.320</a></span> | <span class="t">That's an excellent question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2185" target="_blank">00:36:25.600</a></span> | <span class="t">And my next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2186" target="_blank">00:36:26.600</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2187" target="_blank">00:36:27.600</a></span> | <span class="t">I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2188" target="_blank">00:36:28.600</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2189" target="_blank">00:36:29.600</a></span> | <span class="t">So here we're going to answer precisely this question that we're asked is like, if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2194" target="_blank">00:36:34.360</a></span> | <span class="t">feed in the target return, it could be expert or it could also not be expert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2199" target="_blank">00:36:39.200</a></span> | <span class="t">How well does the model actually do in attaining it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2202" target="_blank">00:36:42.960</a></span> | <span class="t">So the x-axis is what we specify as the target return we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2209" target="_blank">00:36:49.840</a></span> | <span class="t">And the y-axis is basically how much, how well do we actually get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2214" target="_blank">00:36:54.960</a></span> | <span class="t">For reference, we have this green line, which is the oracle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2218" target="_blank">00:36:58.640</a></span> | <span class="t">Which means whatever you desire, the decision transformer gives it to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2223" target="_blank">00:37:03.320</a></span> | <span class="t">So this would have been the ideal case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2225" target="_blank">00:37:05.320</a></span> | <span class="t">So it's a diagonal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2228" target="_blank">00:37:08.040</a></span> | <span class="t">We also have, because this is offline RL, we have in orange what was the best trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2235" target="_blank">00:37:15.600</a></span> | <span class="t">data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2236" target="_blank">00:37:16.600</a></span> | <span class="t">So the offline data is not perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2240" target="_blank">00:37:20.320</a></span> | <span class="t">So we just plot what is the upper bound on the offline data performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2246" target="_blank">00:37:26.560</a></span> | <span class="t">And here we find that for the majority of the environments, there is a good fit between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2254" target="_blank">00:37:34.600</a></span> | <span class="t">the target return we feed in and the actual performance of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2259" target="_blank">00:37:39.800</a></span> | <span class="t">And there are some other observations which I wanted to take from the slide is that because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2268" target="_blank">00:37:48.200</a></span> | <span class="t">we can vary this notion of reward, we can, in some sense, do multitask RL by return conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2277" target="_blank">00:37:57.600</a></span> | <span class="t">This is not the only way to do multitask RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2279" target="_blank">00:37:59.800</a></span> | <span class="t">You can specify a task via natural language, you can via goal state, and so on and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2284" target="_blank">00:38:04.960</a></span> | <span class="t">But this is one notion where the notion of a task could be how much reward you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2293" target="_blank">00:38:13.400</a></span> | <span class="t">And another thing to notice is occasionally these models extrapolate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2297" target="_blank">00:38:17.080</a></span> | <span class="t">This is not a trend we have been seeing consistently, but we do see some signs of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2301" target="_blank">00:38:21.480</a></span> | <span class="t">So if you look at, for example, Sequest, here the highest return trajectory in a data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2309" target="_blank">00:38:29.040</a></span> | <span class="t">was pretty low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2311" target="_blank">00:38:31.120</a></span> | <span class="t">And if we specify a return higher than that for our decision transformer, we do find that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2318" target="_blank">00:38:38.280</a></span> | <span class="t">the model is able to achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2321" target="_blank">00:38:41.200</a></span> | <span class="t">So it is able to generate trajectories with returns higher than it ever saw in the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2330" target="_blank">00:38:50.120</a></span> | <span class="t">I do believe that future work in this space trying to improve this model should think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2335" target="_blank">00:38:55.720</a></span> | <span class="t">about how can this trend be more consistent across environments, because this would really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2341" target="_blank">00:39:01.560</a></span> | <span class="t">achieve the goal of offline RL, which is given suboptimal behavior, how do you get optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2347" target="_blank">00:39:07.960</a></span> | <span class="t">behavior out of it, but remains to be seen how well this trend can be made consistent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2353" target="_blank">00:39:13.720</a></span> | <span class="t">across environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2354" target="_blank">00:39:14.720</a></span> | <span class="t">Can I jump in with a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2358" target="_blank">00:39:18.120</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2359" target="_blank">00:39:19.160</a></span> | <span class="t">So I think that last point is really interesting, and it's cool that you guys occasionally see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2363" target="_blank">00:39:23.160</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2364" target="_blank">00:39:24.160</a></span> | <span class="t">I'm curious to know what happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2366" target="_blank">00:39:26.520</a></span> | <span class="t">So this is all conditioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2367" target="_blank">00:39:27.920</a></span> | <span class="t">You give as an input what return you would like, and it tries to select a sequence of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2372" target="_blank">00:39:32.080</a></span> | <span class="t">actions that gives it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2373" target="_blank">00:39:33.080</a></span> | <span class="t">I'm curious to know what happens if you just give it ridiculous inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2376" target="_blank">00:39:36.600</a></span> | <span class="t">Like, for example, here the order of magnitude for the return is like 50 to 100.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2381" target="_blank">00:39:41.600</a></span> | <span class="t">What happens if you put in 10,000?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2382" target="_blank">00:39:42.600</a></span> | <span class="t">Good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2383" target="_blank">00:39:43.600</a></span> | <span class="t">And this is something we tried early on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2389" target="_blank">00:39:49.440</a></span> | <span class="t">I don't want to say we went up to 10,000, but we try really high returns that not even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2393" target="_blank">00:39:53.680</a></span> | <span class="t">an expert would get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2395" target="_blank">00:39:55.400</a></span> | <span class="t">And generally, we see this leveling performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2397" target="_blank">00:39:57.520</a></span> | <span class="t">So you can see hints of it in Half Cheetah and Pong as well, or Walker to some extent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2405" target="_blank">00:40:05.680</a></span> | <span class="t">And if you look at the very end, things start saturating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2409" target="_blank">00:40:09.600</a></span> | <span class="t">So if you exceed what is like certain threshold, which often corresponds with the best trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2417" target="_blank">00:40:17.080</a></span> | <span class="t">threshold but not always, beyond that, everything is similar returns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2424" target="_blank">00:40:24.200</a></span> | <span class="t">So at least one good thing is it does not degrade in performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2427" target="_blank">00:40:27.600</a></span> | <span class="t">So it would have been a little bit worrying if you specified a return of 10,000 and gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2431" target="_blank">00:40:31.800</a></span> | <span class="t">you a return which is 20 or something really low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2437" target="_blank">00:40:37.520</a></span> | <span class="t">So it's good that it stabilizes, but it's not that it keeps increasing on and on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2442" target="_blank">00:40:42.600</a></span> | <span class="t">So there would be a point where the performance would get saturated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2446" target="_blank">00:40:46.400</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2447" target="_blank">00:40:47.400</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2448" target="_blank">00:40:48.400</a></span> | <span class="t">I was also curious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2449" target="_blank">00:40:49.400</a></span> | <span class="t">So usually, for transform models, you need a lot of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2452" target="_blank">00:40:52.640</a></span> | <span class="t">So do you know how much data do you need?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2455" target="_blank">00:40:55.400</a></span> | <span class="t">Where does it scale with data, the performance of decision transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2459" target="_blank">00:40:59.240</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2460" target="_blank">00:41:00.240</a></span> | <span class="t">So we actually use the standard data, like the D4RL benchmarks for MuJoCo, which I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2469" target="_blank">00:41:09.720</a></span> | <span class="t">have a million transitions in the order of millions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2474" target="_blank">00:41:14.040</a></span> | <span class="t">For Atari, we used 1% of the replay buffer, which is smaller than the one we used for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2486" target="_blank">00:41:26.200</a></span> | <span class="t">the MuJoCo benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2488" target="_blank">00:41:28.400</a></span> | <span class="t">And I actually have a result in the very next slide, which shows decision transformer especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2496" target="_blank">00:41:36.200</a></span> | <span class="t">being useful when you have little data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2501" target="_blank">00:41:41.440</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2503" target="_blank">00:41:43.680</a></span> | <span class="t">So I guess one question to ask--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2505" target="_blank">00:41:45.680</a></span> | <span class="t">Before you move on in the last slide, what do you mean, again, by return conditioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2511" target="_blank">00:41:51.800</a></span> | <span class="t">for the multitask part?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2513" target="_blank">00:41:53.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2515" target="_blank">00:41:55.040</a></span> | <span class="t">So if you think about the returns to go at test time, the one you have to feed in as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2520" target="_blank">00:42:00.440</a></span> | <span class="t">the starting token, as one way of specifying what policy you want, why--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2533" target="_blank">00:42:13.560</a></span> | <span class="t">How is that multitask?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2535" target="_blank">00:42:15.760</a></span> | <span class="t">So it's multitask in the sense that because you can get different policies by changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2540" target="_blank">00:42:20.960</a></span> | <span class="t">your target return to go, you're essentially getting different behaviors encoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2546" target="_blank">00:42:26.600</a></span> | <span class="t">So think about, for instance, a hopper, and you specify a return to go that's really low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2552" target="_blank">00:42:32.120</a></span> | <span class="t">So you're basically saying, get me an agent which will just stick around its initial state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2558" target="_blank">00:42:38.720</a></span> | <span class="t">and not go into unchartered territory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2566" target="_blank">00:42:46.040</a></span> | <span class="t">And if you give it really, really high, then you're asking it to do the traditional task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2571" target="_blank">00:42:51.400</a></span> | <span class="t">which is to hop and go as far as possible without falling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2576" target="_blank">00:42:56.080</a></span> | <span class="t">Can you qualify those multitask because that basically just means that your return conditioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2581" target="_blank">00:43:01.400</a></span> | <span class="t">is a cue for it to memorize, which is usually like one of the pitfalls of multitask?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2591" target="_blank">00:43:11.760</a></span> | <span class="t">So I'm not sure--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2592" target="_blank">00:43:12.760</a></span> | <span class="t">It's a task identifier, that's what I'm trying to say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2598" target="_blank">00:43:18.240</a></span> | <span class="t">So I'm not sure if it's memorization because I think the purpose of this, I mean, having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2609" target="_blank">00:43:29.240</a></span> | <span class="t">an offline data set that's fixed is basically saying that it's very, very specific to if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2615" target="_blank">00:43:35.360</a></span> | <span class="t">you had the same start state, and you took the same actions, and you had the same target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2620" target="_blank">00:43:40.680</a></span> | <span class="t">if it turns, that would qualify as memorization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2624" target="_blank">00:43:44.720</a></span> | <span class="t">But here at this time, we allow all of these things to change, and in fact, they do change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2629" target="_blank">00:43:49.840</a></span> | <span class="t">So your initial state would be different, your target return, which could be a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2637" target="_blank">00:43:57.120</a></span> | <span class="t">scaler than one you ever saw during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2643" target="_blank">00:44:03.120</a></span> | <span class="t">And so essentially, the model has to learn to generate that behavior starting from a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2649" target="_blank">00:44:09.240</a></span> | <span class="t">different initial state, and maybe a different value of the target return than it saw during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2655" target="_blank">00:44:15.040</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2656" target="_blank">00:44:16.040</a></span> | <span class="t">If the dynamics are stochastic, that also makes it that even if you memorize the actions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2662" target="_blank">00:44:22.720</a></span> | <span class="t">you're not guaranteed to get the same next state, so you would actually have a bad correlation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2668" target="_blank">00:44:28.760</a></span> | <span class="t">with the performance if the dynamics are also stochastic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2673" target="_blank">00:44:33.560</a></span> | <span class="t">I also was very curious, how much time does it take to train this new transformer in general?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2681" target="_blank">00:44:41.160</a></span> | <span class="t">So it takes about a few hours, so I want to say like about four to five hours, depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2692" target="_blank">00:44:52.480</a></span> | <span class="t">on what quality GPU you use, but yeah, that's a reasonable estimate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2698" target="_blank">00:44:58.040</a></span> | <span class="t">Yep, got it, thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2700" target="_blank">00:45:00.840</a></span> | <span class="t">Okay, so actually, while doing this experiment, this project, we thought of a baseline, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2710" target="_blank">00:45:10.720</a></span> | <span class="t">we were surprised is not there in previous literature on offline RL, but makes very much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2716" target="_blank">00:45:16.160</a></span> | <span class="t">sense, and we thought we should also think about whether decision transformer is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2720" target="_blank">00:45:20.600</a></span> | <span class="t">doing something very similar to that baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2723" target="_blank">00:45:23.240</a></span> | <span class="t">And the baseline is what we call as person-behavioral cloning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2727" target="_blank">00:45:27.160</a></span> | <span class="t">So behavioral cloning, what it does is basically it ignores the returns and simply imitates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2733" target="_blank">00:45:33.320</a></span> | <span class="t">the agent by just trying to map the actions given the current states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2743" target="_blank">00:45:43.200</a></span> | <span class="t">This is not a good idea with an offline data set, which will have project trees of both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2747" target="_blank">00:45:47.720</a></span> | <span class="t">low returns and high returns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2750" target="_blank">00:45:50.940</a></span> | <span class="t">So traditional behavioral cloning, it's common to see that as a baseline in offline RL methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2757" target="_blank">00:45:57.040</a></span> | <span class="t">and it is, unless you have a very high quality data set, it is not a good baseline for offline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2764" target="_blank">00:46:04.440</a></span> | <span class="t">RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2765" target="_blank">00:46:05.440</a></span> | <span class="t">However, there is a version that we call as person-BC, which actually makes quite a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2770" target="_blank">00:46:10.560</a></span> | <span class="t">of sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2771" target="_blank">00:46:11.560</a></span> | <span class="t">And in this version, we filter out the top trajectories from our offline data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2777" target="_blank">00:46:17.880</a></span> | <span class="t">What's top?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2778" target="_blank">00:46:18.880</a></span> | <span class="t">The ones that have the highest rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2780" target="_blank">00:46:20.640</a></span> | <span class="t">You know the rewards for each transition, you calculate the returns of the trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2785" target="_blank">00:46:25.080</a></span> | <span class="t">and you take the trajectories with the highest returns and keep a certain percentage of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2791" target="_blank">00:46:31.480</a></span> | <span class="t">which is going to be hyperparameter here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2795" target="_blank">00:46:35.480</a></span> | <span class="t">And once you keep those top fraction of your trajectories, you then just ask your model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2801" target="_blank">00:46:41.880</a></span> | <span class="t">to imitate them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2806" target="_blank">00:46:46.420</a></span> | <span class="t">So imitation learning also uses, especially when it's used in the form of behavioral cloning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2811" target="_blank">00:46:51.440</a></span> | <span class="t">it uses supervised learning essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2814" target="_blank">00:46:54.160</a></span> | <span class="t">It's a supervised learning problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2815" target="_blank">00:46:55.160</a></span> | <span class="t">So you could actually also get supervised learning objective functions if you did this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2820" target="_blank">00:47:00.480</a></span> | <span class="t">filtering step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2825" target="_blank">00:47:05.360</a></span> | <span class="t">And what we find actually that for the moderate and high data regimes, the descent transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2830" target="_blank">00:47:10.880</a></span> | <span class="t">is actually very comparable to person-BC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2833" target="_blank">00:47:13.240</a></span> | <span class="t">So it's a very strong baseline, which I think all of future work in offline RL should include.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2837" target="_blank">00:47:17.920</a></span> | <span class="t">There's actually an ICARE submission from last week, which has a much more detailed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2844" target="_blank">00:47:24.360</a></span> | <span class="t">analysis on just this baseline that we introduced in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2848" target="_blank">00:47:28.760</a></span> | <span class="t">And what we do find is that for low data regimes, the descent transformer does much better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2855" target="_blank">00:47:35.080</a></span> | <span class="t">person-behavioral cloning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2856" target="_blank">00:47:36.360</a></span> | <span class="t">So this is for the Atari benchmarks where, like I previously mentioned, we have a much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2861" target="_blank">00:47:41.560</a></span> | <span class="t">smaller data set as compared to the Mojoco environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2866" target="_blank">00:47:46.960</a></span> | <span class="t">And here we find that even after varying the different fraction of the percentage hyperparameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2873" target="_blank">00:47:53.400</a></span> | <span class="t">here, we are generally not able to get the strong performance that a descent transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2878" target="_blank">00:47:58.120</a></span> | <span class="t">gets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2880" target="_blank">00:48:00.440</a></span> | <span class="t">So 10% BC basically means that we filter out and keep the top 10% of the trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2886" target="_blank">00:48:06.560</a></span> | <span class="t">If you go even lower, then this data set becomes very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2890" target="_blank">00:48:10.200</a></span> | <span class="t">So the baseline would become meaningless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2893" target="_blank">00:48:13.280</a></span> | <span class="t">But for even the reasonable ranges, we never find the performance matching that of descent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2898" target="_blank">00:48:18.040</a></span> | <span class="t">transformers for the Atari benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2901" target="_blank">00:48:21.560</a></span> | <span class="t">Diti, if I may.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2906" target="_blank">00:48:26.800</a></span> | <span class="t">So I noticed in table 3, for example, which is not this table, but the one just before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2910" target="_blank">00:48:30.160</a></span> | <span class="t">in the paper, there's a report on the CQL performance, which to me also feels intuitively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2916" target="_blank">00:48:36.800</a></span> | <span class="t">pretty similar to the percent BC in the sense of you pick trajectories you know are performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2922" target="_blank">00:48:42.000</a></span> | <span class="t">well, and you try and stay roughly within sort of the same kind of policy distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2927" target="_blank">00:48:47.120</a></span> | <span class="t">and state space distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2931" target="_blank">00:48:51.800</a></span> | <span class="t">I was curious, on this one, do you have a sense of what the CQL performance was relative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2935" target="_blank">00:48:55.640</a></span> | <span class="t">to, say, the percent BC performance here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2939" target="_blank">00:48:59.360</a></span> | <span class="t">So that's a great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2941" target="_blank">00:49:01.840</a></span> | <span class="t">The question is that even for CQL, you rely on this notion of pessimism, where you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2950" target="_blank">00:49:10.280</a></span> | <span class="t">to pick trajectories where you're more confident in and make sure policy remains in that region.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2957" target="_blank">00:49:17.400</a></span> | <span class="t">So I don't have the numbers of CQL on this table, but if you look at the detailed results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2963" target="_blank">00:49:23.240</a></span> | <span class="t">for Atari, then I think they should have the CQL for sure, because that's the numbers we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2972" target="_blank">00:49:32.880</a></span> | <span class="t">are reporting here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2976" target="_blank">00:49:36.760</a></span> | <span class="t">So I can tell you what the CQL performance is actually pretty good, and it's very competitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2982" target="_blank">00:49:42.520</a></span> | <span class="t">with the decision transformer for Atari.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2986" target="_blank">00:49:46.760</a></span> | <span class="t">So this TD learning baseline here is CQL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2991" target="_blank">00:49:51.520</a></span> | <span class="t">So naturally by extension, I would imagine it doing better than percent BC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2996" target="_blank">00:49:56.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=2997" target="_blank">00:49:57.320</a></span> | <span class="t">And I apologize if this was mentioned, I just missed it, but do you have the sense that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3002" target="_blank">00:50:02.240</a></span> | <span class="t">this is basically like a failure of CQL to be able to extrapolate well, or sort of stitch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3007" target="_blank">00:50:07.720</a></span> | <span class="t">together different parts of trajectories, whereas the decision transformer can sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3012" target="_blank">00:50:12.320</a></span> | <span class="t">of make that extrapolation between-- you have like the first half of one trajectory is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3015" target="_blank">00:50:15.920</a></span> | <span class="t">good, the second half of one trajectory is really good, and so you can actually piece</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3018" target="_blank">00:50:18.480</a></span> | <span class="t">those together with decision transformer, where you can't necessarily do that with CQL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3021" target="_blank">00:50:21.680</a></span> | <span class="t">because the path connecting those may not necessarily be well covered by the behavior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3026" target="_blank">00:50:26.040</a></span> | <span class="t">policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3027" target="_blank">00:50:27.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3028" target="_blank">00:50:28.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3029" target="_blank">00:50:29.040</a></span> | <span class="t">So this actually goes to one of the intuitions, which I did not emphasize too much, but we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3037" target="_blank">00:50:37.000</a></span> | <span class="t">have a discussion on the paper where essentially, why do we expect a transformer, or any model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3043" target="_blank">00:50:43.240</a></span> | <span class="t">for that matter, to look at offline data that's suboptimal, and get a policy that generates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3049" target="_blank">00:50:49.840</a></span> | <span class="t">optimal rollouts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3050" target="_blank">00:50:50.840</a></span> | <span class="t">The intuition is that, as Scott was mentioning, you could perhaps stitch together good behaviors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3061" target="_blank">00:51:01.200</a></span> | <span class="t">from suboptimal trajectories, and that stitching could perhaps lead to a behavior that is better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3066" target="_blank">00:51:06.840</a></span> | <span class="t">than anything you saw in individual trajectories in your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3071" target="_blank">00:51:11.760</a></span> | <span class="t">It's something we find early evidence of in a small scale experiment for graphs, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3080" target="_blank">00:51:20.800</a></span> | <span class="t">is really our hope also, that something that the transformer is really good at, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3086" target="_blank">00:51:26.800</a></span> | <span class="t">it can attend to very long sequences, so it could identify those segments of behavior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3093" target="_blank">00:51:33.160</a></span> | <span class="t">which when stitched together would give you optimal behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3104" target="_blank">00:51:44.440</a></span> | <span class="t">And it's very much possible that is something unique to decision transformers, and something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3109" target="_blank">00:51:49.240</a></span> | <span class="t">like CQL would not be able to do, PersonBC, because it's filtering out the data, is automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3116" target="_blank">00:51:56.720</a></span> | <span class="t">being limited and not being able to do that, because the segments of good behavior could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3121" target="_blank">00:52:01.480</a></span> | <span class="t">be in trajectories which overall do not have a high return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3125" target="_blank">00:52:05.080</a></span> | <span class="t">So if you filter them out, you are losing all of that information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3131" target="_blank">00:52:11.520</a></span> | <span class="t">OK, so I said there is a hyperparameter, the context in K, and like with most of perception,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3141" target="_blank">00:52:21.480</a></span> | <span class="t">one of the big advantages of transformers, as opposed to other sequence models like LSTMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3147" target="_blank">00:52:27.080</a></span> | <span class="t">is that they can process very large sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3151" target="_blank">00:52:31.840</a></span> | <span class="t">And here, at a first glance, it might seem that being Markovian would have been helpful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3157" target="_blank">00:52:37.600</a></span> | <span class="t">for RL, which also was a question that was raised earlier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3162" target="_blank">00:52:42.440</a></span> | <span class="t">So we did this experiment where we did compare performance with context and K equals 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3168" target="_blank">00:52:48.360</a></span> | <span class="t">And here, we had context between 30 for the environments and 50 for Pong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3176" target="_blank">00:52:56.360</a></span> | <span class="t">And we find that increasing the context length is very, very important to get good performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3186" target="_blank">00:53:06.280</a></span> | <span class="t">OK, now, so far I've showed you how decision transformer, which is very simple, there was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3201" target="_blank">00:53:21.720</a></span> | <span class="t">no slide I had which was going into the details of dynamic programming, which is the crux</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3207" target="_blank">00:53:27.240</a></span> | <span class="t">of most RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3208" target="_blank">00:53:28.240</a></span> | <span class="t">This was just pure supervised learning in an autoregressive framework that was getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3213" target="_blank">00:53:33.880</a></span> | <span class="t">us this good performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3218" target="_blank">00:53:38.280</a></span> | <span class="t">What about cases where this approach actually starts outperforming some of the traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3225" target="_blank">00:53:45.720</a></span> | <span class="t">methods for RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3227" target="_blank">00:53:47.280</a></span> | <span class="t">So to probe a little bit further, we started looking at sparse reward environments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3231" target="_blank">00:53:51.600</a></span> | <span class="t">And basically, we just took our existing MuJoCo environments, and then instead of giving it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3238" target="_blank">00:53:58.800</a></span> | <span class="t">the information for reward for every transition, we fed in the cumulative reward at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3244" target="_blank">00:54:04.200</a></span> | <span class="t">of the trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3245" target="_blank">00:54:05.760</a></span> | <span class="t">So every transition will have a zero reward, except the very end where you get the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3249" target="_blank">00:54:09.840</a></span> | <span class="t">reward at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3250" target="_blank">00:54:10.840</a></span> | <span class="t">So it's a very sparse reward perform scenario for that reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3256" target="_blank">00:54:16.160</a></span> | <span class="t">And here, we find that compared to the original dense results, the delayed results for DT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3265" target="_blank">00:54:25.640</a></span> | <span class="t">they will deteriorate a little bit, which is expected, because now you are withholding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3271" target="_blank">00:54:31.320</a></span> | <span class="t">some of the more fine-grained information at every time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3274" target="_blank">00:54:34.120</a></span> | <span class="t">But the drop is not too significant compared to the original DT performance here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3280" target="_blank">00:54:40.760</a></span> | <span class="t">Whereas for something like CQL, there is a drastic drop in performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3285" target="_blank">00:54:45.920</a></span> | <span class="t">So CQL suffers quite a lot in sparse reward scenarios, but the decision transformer does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3292" target="_blank">00:54:52.880</a></span> | <span class="t">not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3294" target="_blank">00:54:54.480</a></span> | <span class="t">And just for completeness, you also have performance of behavioral cloning and person-behavioral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3298" target="_blank">00:54:58.240</a></span> | <span class="t">cloning, which, because they don't look at reward information, except maybe a person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3303" target="_blank">00:55:03.480</a></span> | <span class="t">basically looks at only for preprocessing the data set, these are agnostic to whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3308" target="_blank">00:55:08.320</a></span> | <span class="t">the environments have sparse rewards or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3314" target="_blank">00:55:14.480</a></span> | <span class="t">>> Would you expect this to be different if you were doing online RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3327" target="_blank">00:55:27.280</a></span> | <span class="t">>> What's the intuition for it being different?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3328" target="_blank">00:55:28.880</a></span> | <span class="t">I would say no, but maybe I'm missing out on a key piece of intuition behind that question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3337" target="_blank">00:55:37.760</a></span> | <span class="t">I think that because you're training offline, the next input will always be the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3345" target="_blank">00:55:45.760</a></span> | <span class="t">action in that sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3347" target="_blank">00:55:47.080</a></span> | <span class="t">So you don't just deviate and go off the rails technically because you just don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3352" target="_blank">00:55:52.760</a></span> | <span class="t">So I could see how online would have a really hard cold start, basically, because it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3359" target="_blank">00:55:59.040</a></span> | <span class="t">doesn't know and it's just tapping in the dark until it maybe eventually hits the jackpot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3364" target="_blank">00:56:04.800</a></span> | <span class="t">>> Right, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3366" target="_blank">00:56:06.960</a></span> | <span class="t">I think I agree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3367" target="_blank">00:56:07.960</a></span> | <span class="t">That's a good piece of intuition out there, but yeah, I think here, because offline RL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3376" target="_blank">00:56:16.280</a></span> | <span class="t">is really getting rid of the trial and error aspect of it, and for sparse reward environments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3383" target="_blank">00:56:23.920</a></span> | <span class="t">that would be harder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3386" target="_blank">00:56:26.240</a></span> | <span class="t">So the drop in DT performance should be more prominent there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3394" target="_blank">00:56:34.740</a></span> | <span class="t">I'm not sure how it would compare with the drop in performance for other algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3402" target="_blank">00:56:42.240</a></span> | <span class="t">but it does seem like an interesting setup to test DTN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3409" target="_blank">00:56:49.040</a></span> | <span class="t">>> Well, maybe I'm wrong here, but my understanding with the decision transformer as well is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3415" target="_blank">00:56:55.680</a></span> | <span class="t">critical piece that in the training, you use the rewards to go, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3419" target="_blank">00:56:59.000</a></span> | <span class="t">So is it not the sense that essentially like for each trajectory from the initial state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3426" target="_blank">00:57:06.120</a></span> | <span class="t">based on the training regime, the model has access to whether or not the final result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3430" target="_blank">00:57:10.520</a></span> | <span class="t">was a success or failure, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3435" target="_blank">00:57:15.720</a></span> | <span class="t">But that's sort of a unique aspect of the training regime for decision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3439" target="_blank">00:57:19.920</a></span> | <span class="t">In CQL, my understanding is that it's based on sort of a per transition training regime,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3448" target="_blank">00:57:28.480</a></span> | <span class="t">and so each transition is decoupled somewhat to what the final reward was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3452" target="_blank">00:57:32.600</a></span> | <span class="t">Is that correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3453" target="_blank">00:57:33.600</a></span> | <span class="t">>> Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3454" target="_blank">00:57:34.600</a></span> | <span class="t">Although like one difficulty which at a first glance you kind of imagined the decision transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3461" target="_blank">00:57:41.400</a></span> | <span class="t">having is that that initial token will not change throughout the trajectory because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3469" target="_blank">00:57:49.320</a></span> | <span class="t">a sparse reward scenario.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3470" target="_blank">00:57:50.800</a></span> | <span class="t">So except the very last token where it will drop down to zero all of a sudden, this token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3475" target="_blank">00:57:55.400</a></span> | <span class="t">remains the same throughout, but maybe that, but I think you're right that maybe just even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3481" target="_blank">00:58:01.880</a></span> | <span class="t">at the start feeding it in a manner which looks at the future rewards that you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3488" target="_blank">00:58:08.320</a></span> | <span class="t">to get to is perhaps one part of the reason why the drop in performance is not noticeable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3496" target="_blank">00:58:16.080</a></span> | <span class="t">>> Yeah, I mean, I guess one sort of obligation experiment here would be if you change the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3502" target="_blank">00:58:22.360</a></span> | <span class="t">training regime so that only the last trajectory had the reward, but I'm trying to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3508" target="_blank">00:58:28.400</a></span> | <span class="t">whether or not that would just be compensated for by sort of the attention mechanism anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3515" target="_blank">00:58:35.120</a></span> | <span class="t">And vice versa, right, if you embedded that reward information into the CQL training procedure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3519" target="_blank">00:58:39.600</a></span> | <span class="t">as well, I'd be curious to see what would happen there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3523" target="_blank">00:58:43.080</a></span> | <span class="t">>> Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3524" target="_blank">00:58:44.080</a></span> | <span class="t">>> And how it would go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3525" target="_blank">00:58:45.080</a></span> | <span class="t">>> Those are good experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3526" target="_blank">00:58:46.080</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3527" target="_blank">00:58:47.080</a></span> | <span class="t">So related to this, there's another environment we tested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3535" target="_blank">00:58:55.280</a></span> | <span class="t">I gave you a brief preview of the results in one of the earlier slides, so this is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3539" target="_blank">00:58:59.400</a></span> | <span class="t">the key-to-door environment, and it has three phases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3542" target="_blank">00:59:02.960</a></span> | <span class="t">So in the first phase, the agent is placed in a room with the key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3548" target="_blank">00:59:08.880</a></span> | <span class="t">A good agent will pick up the key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3551" target="_blank">00:59:11.160</a></span> | <span class="t">And then in phase two, it will be placed in an empty room, and in phase three, it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3556" target="_blank">00:59:16.280</a></span> | <span class="t">be placed in a room with a door where it will actually use the key that it collected in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3561" target="_blank">00:59:21.600</a></span> | <span class="t">phase one, if it did, to open the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3566" target="_blank">00:59:26.320</a></span> | <span class="t">So essentially, the agent is going to receive a binding reward corresponding to whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3571" target="_blank">00:59:31.520</a></span> | <span class="t">it reached and opened the door in phase three, conditioned on the fact that it did pick up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3580" target="_blank">00:59:40.720</a></span> | <span class="t">the key in phase one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3583" target="_blank">00:59:43.160</a></span> | <span class="t">So there is this national notion on that you want to assign credit to something that happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3588" target="_blank">00:59:48.560</a></span> | <span class="t">to an event that happened really in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3591" target="_blank">00:59:51.280</a></span> | <span class="t">So it's a very challenging and sensible scenario if you want to test your models for how well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3597" target="_blank">00:59:57.600</a></span> | <span class="t">they are at long-term credit assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3601" target="_blank">01:00:01.080</a></span> | <span class="t">And here we find that, so we tested it for different amounts of trajectories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3606" target="_blank">01:00:06.200</a></span> | <span class="t">So here, the number of trajectories basically says how often would you actually see this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3610" target="_blank">01:00:10.920</a></span> | <span class="t">kind of behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3614" target="_blank">01:00:14.640</a></span> | <span class="t">And the Lissen transformer and person-behavioral cloning, both of these actually baselines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3622" target="_blank">01:00:22.920</a></span> | <span class="t">do much better than other models which struggle at this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3631" target="_blank">01:00:31.640</a></span> | <span class="t">There's a related experiment there, which is also of interest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3636" target="_blank">01:00:36.160</a></span> | <span class="t">So generally, a lot of algorithms have this notion of an actor and a critic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3641" target="_blank">01:00:41.840</a></span> | <span class="t">Actor is basically someone that takes actions, condition on the states, and think of a policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3647" target="_blank">01:00:47.560</a></span> | <span class="t">A critic is basically evaluating how good these actions are in terms of achieving a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3654" target="_blank">01:00:54.160</a></span> | <span class="t">long-term, in terms of the cumulative sum of rewards in the long-term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3661" target="_blank">01:01:01.360</a></span> | <span class="t">This is a good environment because we can see how well the Lissen transformer would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3667" target="_blank">01:01:07.640</a></span> | <span class="t">do if it was trained as a critic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3670" target="_blank">01:01:10.320</a></span> | <span class="t">So here, what we did is instead of having the actions as the output target, what if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3676" target="_blank">01:01:16.760</a></span> | <span class="t">we substituted that with the rewards?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3681" target="_blank">01:01:21.200</a></span> | <span class="t">So that's very much possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3684" target="_blank">01:01:24.000</a></span> | <span class="t">We can again use the same causal transformer machinery to only look at transitions in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3690" target="_blank">01:01:30.160</a></span> | <span class="t">previous time step and try to pick the reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3693" target="_blank">01:01:33.320</a></span> | <span class="t">And here, we see this interesting pattern where in the three phases that we had in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3698" target="_blank">01:01:38.600</a></span> | <span class="t">key-to-door environment, we do see the reward probability changing very much in how we expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3706" target="_blank">01:01:46.800</a></span> | <span class="t">So basically, there are three scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3709" target="_blank">01:01:49.080</a></span> | <span class="t">So the first scenario, let's look at Bloom, in which the agent does not pick up the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3715" target="_blank">01:01:55.200</a></span> | <span class="t">in phase one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3717" target="_blank">01:01:57.080</a></span> | <span class="t">So the reward probability, they all start around the same, but as it becomes apparent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3721" target="_blank">01:02:01.640</a></span> | <span class="t">that the agent is not going to pick up the key, the reward starts going down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3725" target="_blank">01:02:05.720</a></span> | <span class="t">And then it stays very much close to zero throughout the episode because there is no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3731" target="_blank">01:02:11.720</a></span> | <span class="t">way you will have the key to open the door in the future phases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3738" target="_blank">01:02:18.320</a></span> | <span class="t">If you pick up the key, there are two possibilities, which are essentially the same in phase two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3749" target="_blank">01:02:29.120</a></span> | <span class="t">where you had an empty room, which is just a distractor to make the episode really long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3755" target="_blank">01:02:35.840</a></span> | <span class="t">But at the very end, the two possibilities are one, that you take the key and you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3760" target="_blank">01:02:40.280</a></span> | <span class="t">reach the door, which is the one we see in orange and brown here, where you see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3767" target="_blank">01:02:47.000</a></span> | <span class="t">the reward probability goes up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3769" target="_blank">01:02:49.320</a></span> | <span class="t">And there's this other possibility that you actually pick up the key, but do not reach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3772" target="_blank">01:02:52.720</a></span> | <span class="t">the door.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3773" target="_blank">01:02:53.720</a></span> | <span class="t">In which case, again, you start seeing that the reward probability that's predicted starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3778" target="_blank">01:02:58.600</a></span> | <span class="t">going down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3780" target="_blank">01:03:00.640</a></span> | <span class="t">So the takeaway from this experiment is that machine transformers are not just great actors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3788" target="_blank">01:03:08.600</a></span> | <span class="t">which is what we've been seeing so far in the results from the optimized policy, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3794" target="_blank">01:03:14.880</a></span> | <span class="t">they're also very impressive critics in doing this long-term pattern assignment where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3800" target="_blank">01:03:20.080</a></span> | <span class="t">reward is also very sparse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3802" target="_blank">01:03:22.240</a></span> | <span class="t">So Aditya, just to be correct, are you predicting the rewards to go at each time step, or is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3806" target="_blank">01:03:26.960</a></span> | <span class="t">this the reward at each time step that you're predicting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3812" target="_blank">01:03:32.880</a></span> | <span class="t">So this was the rewards to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3816" target="_blank">01:03:36.440</a></span> | <span class="t">And I can also check-- my impression was in this part of the experiment, it didn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3821" target="_blank">01:03:41.600</a></span> | <span class="t">make a difference whether we were predicting rewards to go or the actual rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3826" target="_blank">01:03:46.880</a></span> | <span class="t">But I think whether it turns to go for this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3829" target="_blank">01:03:49.080</a></span> | <span class="t">Also, I was curious, so how do you get the probability distribution of the rewards?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3832" target="_blank">01:03:52.840</a></span> | <span class="t">Is it just like you just evaluate a lot of different episodes and just plot the rewards?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3836" target="_blank">01:03:56.960</a></span> | <span class="t">Or are you explicitly predicting some sort of distribution?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3840" target="_blank">01:04:00.920</a></span> | <span class="t">So this is a binary reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3844" target="_blank">01:04:04.000</a></span> | <span class="t">So you can have a probabilistic outcome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3846" target="_blank">01:04:06.960</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3849" target="_blank">01:04:09.360</a></span> | <span class="t">Son, you have a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3853" target="_blank">01:04:13.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3854" target="_blank">01:04:14.080</a></span> | <span class="t">So generally, we will call something predicts state value or state action value as a critic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3860" target="_blank">01:04:20.720</a></span> | <span class="t">But in this case, you ask decision transformer to only predict the reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3866" target="_blank">01:04:26.320</a></span> | <span class="t">So why should you still call it a critic?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3871" target="_blank">01:04:31.160</a></span> | <span class="t">So I think the analogy here gets a bit clearer with returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3875" target="_blank">01:04:35.040</a></span> | <span class="t">Like, if you think about returns to go, it's really capturing that essence that you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3879" target="_blank">01:04:39.080</a></span> | <span class="t">to see the future rewards that--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3881" target="_blank">01:04:41.560</a></span> | <span class="t">Oh, I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3882" target="_blank">01:04:42.560</a></span> | <span class="t">So you mean it's just going to predict the return to go instead of single-step reward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3887" target="_blank">01:04:47.440</a></span> | <span class="t">right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3888" target="_blank">01:04:48.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3889" target="_blank">01:04:49.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3890" target="_blank">01:04:50.440</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3891" target="_blank">01:04:51.440</a></span> | <span class="t">So if we're going to predict the returns to go, it's kind of counterintuitive to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3895" target="_blank">01:04:55.760</a></span> | <span class="t">Because in phase one, when the agent is still in the K room, I think it should have a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3902" target="_blank">01:05:02.280</a></span> | <span class="t">returns to go if it picked up the K. But in the plot, in the K room, the agents pick up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3913" target="_blank">01:05:13.360</a></span> | <span class="t">K, and the agent that didn't pick up K has the same kind of level of returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3920" target="_blank">01:05:20.320</a></span> | <span class="t">So that's quite counterintuitive to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3924" target="_blank">01:05:24.560</a></span> | <span class="t">I think this is reflecting on a good property, which is that your distribution-- like, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3933" target="_blank">01:05:33.080</a></span> | <span class="t">you interpret returns to go in the right way, in phase one, you don't know which of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3938" target="_blank">01:05:38.120</a></span> | <span class="t">three outcomes are really possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3940" target="_blank">01:05:40.640</a></span> | <span class="t">And phase one also, I'm talking about the very beginning, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3943" target="_blank">01:05:43.240</a></span> | <span class="t">Slowly, you will learn about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3944" target="_blank">01:05:44.920</a></span> | <span class="t">But essentially, in phase one, if you see the returns to go as 1 or 0, all three possibilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3956" target="_blank">01:05:56.680</a></span> | <span class="t">are equally likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3959" target="_blank">01:05:59.080</a></span> | <span class="t">And all three possibilities-- so if we try to evaluate the predicted reward for these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3965" target="_blank">01:06:05.440</a></span> | <span class="t">possibilities, it shouldn't be the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3968" target="_blank">01:06:08.860</a></span> | <span class="t">Because we really haven't done-- we don't know what's going to happen in phase three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3975" target="_blank">01:06:15.000</a></span> | <span class="t">Sorry, it's my mistake, because previously, I thought the green line is the agent which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3982" target="_blank">01:06:22.120</a></span> | <span class="t">doesn't pick up the K. But it turns out the blue line is the agent which doesn't pick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3987" target="_blank">01:06:27.000</a></span> | <span class="t">up K. So yeah, it's my mistake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3989" target="_blank">01:06:29.520</a></span> | <span class="t">It makes sense to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3990" target="_blank">01:06:30.760</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3991" target="_blank">01:06:31.760</a></span> | <span class="t">Also, it was not fully clear from the paper, but did you do experiments where you're predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3997" target="_blank">01:06:37.160</a></span> | <span class="t">both the actions and both the rewards to go?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=3999" target="_blank">01:06:39.280</a></span> | <span class="t">And does it-- can it improve performance if you're doing both together?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4003" target="_blank">01:06:43.920</a></span> | <span class="t">So actually, we did some preliminary experiments on that, and it didn't help us much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4009" target="_blank">01:06:49.040</a></span> | <span class="t">However, I do want to, again, put in a plug for a paper that came concurrently, Trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4014" target="_blank">01:06:54.000</a></span> | <span class="t">Transformer, which tried to predict states, actions, and rewards, actually, all three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4021" target="_blank">01:07:01.320</a></span> | <span class="t">of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4022" target="_blank">01:07:02.480</a></span> | <span class="t">They were in a model-based setup, where it made sense, also, to try to learn each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4028" target="_blank">01:07:08.920</a></span> | <span class="t">the components, like the transition dynamics, the policy, and maybe even the critic in their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4033" target="_blank">01:07:13.800</a></span> | <span class="t">setup together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4035" target="_blank">01:07:15.120</a></span> | <span class="t">We did not find any significant improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4039" target="_blank">01:07:19.040</a></span> | <span class="t">So in favor of simplicity and keeping it model-free, we did not try to predict them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4046" target="_blank">01:07:26.240</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4048" target="_blank">01:07:28.880</a></span> | <span class="t">OK, so the summary, we showed this in Transformers, which is a first work in trying to approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4060" target="_blank">01:07:40.480</a></span> | <span class="t">RL based on sequence modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4063" target="_blank">01:07:43.720</a></span> | <span class="t">The main advantages of previous approaches is it's simple by design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4068" target="_blank">01:07:48.640</a></span> | <span class="t">The hope is that for the extensions, we will find it to scale much better than existing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4074" target="_blank">01:07:54.600</a></span> | <span class="t">RL algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4075" target="_blank">01:07:55.600</a></span> | <span class="t">It is stable to train, because the loss functions we are using have been tested and iterated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4083" target="_blank">01:08:03.360</a></span> | <span class="t">upon a lot by research and perception.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4088" target="_blank">01:08:08.680</a></span> | <span class="t">And in the future, we will also hope that because of these similarities in the architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4095" target="_blank">01:08:15.600</a></span> | <span class="t">and the training with how perception-based tasks are conducted, it would also be easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4100" target="_blank">01:08:20.980</a></span> | <span class="t">to integrate them within this loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4104" target="_blank">01:08:24.160</a></span> | <span class="t">So the states, the actions, or even the task of interest, they could be specified based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4111" target="_blank">01:08:31.040</a></span> | <span class="t">on perceptual-based senses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4115" target="_blank">01:08:35.480</a></span> | <span class="t">So you could have a target task being specified by a natural language instruction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4122" target="_blank">01:08:42.520</a></span> | <span class="t">And because these models can very well play with these kinds of inputs, the hope is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4128" target="_blank">01:08:48.720</a></span> | <span class="t">they would be easy to integrate within the decision-making process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4136" target="_blank">01:08:56.280</a></span> | <span class="t">And empirically, we saw strong performance in the range of offline RL settings, and especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4142" target="_blank">01:09:02.040</a></span> | <span class="t">good performance in scenarios which required us to do long-term credit assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4149" target="_blank">01:09:09.660</a></span> | <span class="t">So there's a lot of future work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4151" target="_blank">01:09:11.640</a></span> | <span class="t">This is definitely not the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4156" target="_blank">01:09:16.160</a></span> | <span class="t">This is a first work in rethinking how do we build RL agents that can scale and generalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4165" target="_blank">01:09:25.700</a></span> | <span class="t">A few things that I picked out, which I feel would be very exciting to extend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4172" target="_blank">01:09:32.240</a></span> | <span class="t">The first is multi-modality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4175" target="_blank">01:09:35.080</a></span> | <span class="t">So really, one of our big motivations with going after these kinds of models is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4181" target="_blank">01:09:41.080</a></span> | <span class="t">we can combine different kinds of inputs, both online and offline, to really build decision-making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4188" target="_blank">01:09:48.940</a></span> | <span class="t">agents which work like humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4191" target="_blank">01:09:51.440</a></span> | <span class="t">We process so many inputs around us in different modalities, and we act on them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4198" target="_blank">01:09:58.060</a></span> | <span class="t">So we do take decisions, and we want the same to happen in artificial agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4202" target="_blank">01:10:02.800</a></span> | <span class="t">And maybe decision transformers is one important step in that route.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4209" target="_blank">01:10:09.140</a></span> | <span class="t">Multi-task, so I described a very limited form of multi-tasking here, which was based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4218" target="_blank">01:10:18.960</a></span> | <span class="t">on the desired returns to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4223" target="_blank">01:10:23.360</a></span> | <span class="t">But it could be more richer in terms of specifying a command to be a robot or a desired goal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4231" target="_blank">01:10:31.280</a></span> | <span class="t">state, which could be, for example, even visual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4235" target="_blank">01:10:35.560</a></span> | <span class="t">So trying to better explore the different multi-task capabilities of this model would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4241" target="_blank">01:10:41.400</a></span> | <span class="t">also be an interesting extension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4245" target="_blank">01:10:45.520</a></span> | <span class="t">Finally, multi-agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4249" target="_blank">01:10:49.880</a></span> | <span class="t">As human beings, we never act in isolation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4252" target="_blank">01:10:52.800</a></span> | <span class="t">We are always acting within an environment that involves many, many more agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4260" target="_blank">01:11:00.080</a></span> | <span class="t">Agents become partially observable in those scenarios, which plays to the strengths of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4265" target="_blank">01:11:05.520</a></span> | <span class="t">decision transformers being non-Markovian by design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4269" target="_blank">01:11:09.160</a></span> | <span class="t">So I think there is great possibilities of exploring even multi-agent scenarios where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4275" target="_blank">01:11:15.260</a></span> | <span class="t">the fact that transformers can process very large sequences compared to existing algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4281" target="_blank">01:11:21.200</a></span> | <span class="t">could again help build better models of other agents in your environment and act.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4290" target="_blank">01:11:30.160</a></span> | <span class="t">So that, yeah, there's some just useful links in case you're interested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4294" target="_blank">01:11:34.040</a></span> | <span class="t">The project website, the paper, and the code are all public, and I'm happy to take any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4300" target="_blank">01:11:40.620</a></span> | <span class="t">more questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4301" target="_blank">01:11:41.620</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4302" target="_blank">01:11:42.620</a></span> | <span class="t">So as I said, thanks for the good talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4305" target="_blank">01:11:45.040</a></span> | <span class="t">Really appreciate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4306" target="_blank">01:11:46.040</a></span> | <span class="t">Everyone had a good time here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4307" target="_blank">01:11:47.040</a></span> | <span class="t">So I think we are near the class limit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4313" target="_blank">01:11:53.280</a></span> | <span class="t">So usually I have a round of rapid fire questions for the speaker that the students usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4319" target="_blank">01:11:59.200</a></span> | <span class="t">know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4320" target="_blank">01:12:00.200</a></span> | <span class="t">But if someone is in a hurry, you can just ask general questions first before we stop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4326" target="_blank">01:12:06.760</a></span> | <span class="t">the recording.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4327" target="_blank">01:12:07.760</a></span> | <span class="t">So if anyone wants to leave earlier at this time, just feel free to ask your questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4333" target="_blank">01:12:13.040</a></span> | <span class="t">Otherwise, I will just continue on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4338" target="_blank">01:12:18.400</a></span> | <span class="t">So what do you think is the future of transformers in RL?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4340" target="_blank">01:12:20.920</a></span> | <span class="t">Do you think they will take over-- so they've already taken over language and vision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4344" target="_blank">01:12:24.640</a></span> | <span class="t">So do you think for model-based and model-free learning, do you think you'll see a lot more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4348" target="_blank">01:12:28.680</a></span> | <span class="t">transformers pop up in RL literature?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4352" target="_blank">01:12:32.360</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4353" target="_blank">01:12:33.360</a></span> | <span class="t">I think we'll see a flurry of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4356" target="_blank">01:12:36.600</a></span> | <span class="t">If not already, we have good-- there's so many works using transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4363" target="_blank">01:12:43.000</a></span> | <span class="t">And this year's ICLR conference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4366" target="_blank">01:12:46.560</a></span> | <span class="t">Having said that, I feel that an important piece of the puzzle that needs to be solved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4370" target="_blank">01:12:50.720</a></span> | <span class="t">is expiration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4372" target="_blank">01:12:52.800</a></span> | <span class="t">It's non-trivial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4376" target="_blank">01:12:56.520</a></span> | <span class="t">And it will have to-- my guess is that you will have to forego some of the advantages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4383" target="_blank">01:13:03.800</a></span> | <span class="t">that I talked about for transformers in terms of loss functions to actually enable expiration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4390" target="_blank">01:13:10.960</a></span> | <span class="t">So it remains to be seen whether those modified loss functions for expiration actually hurt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4400" target="_blank">01:13:20.000</a></span> | <span class="t">performance significantly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4402" target="_blank">01:13:22.600</a></span> | <span class="t">But as long as we cannot cross that bottleneck, I think it is-- I'm not-- I do not want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4410" target="_blank">01:13:30.800</a></span> | <span class="t">commit that this is indeed the future of RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4414" target="_blank">01:13:34.720</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4415" target="_blank">01:13:35.720</a></span> | <span class="t">Also, you think that something--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4417" target="_blank">01:13:37.640</a></span> | <span class="t">Wait.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4418" target="_blank">01:13:38.640</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4419" target="_blank">01:13:39.640</a></span> | <span class="t">I have a follow-up question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4420" target="_blank">01:13:40.640</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4421" target="_blank">01:13:41.640</a></span> | <span class="t">I'm not sure I understood that point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4423" target="_blank">01:13:43.480</a></span> | <span class="t">So you're saying that in order to apply transformers in RL to do expiration, there have to be particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4428" target="_blank">01:13:48.880</a></span> | <span class="t">loss functions, and they're tricky for some reason?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4431" target="_blank">01:13:51.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4432" target="_blank">01:13:52.960</a></span> | <span class="t">Could you explain more?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4434" target="_blank">01:13:54.320</a></span> | <span class="t">Like, what are the modified loss functions, and why do they seem tricky?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4438" target="_blank">01:13:58.760</a></span> | <span class="t">So essentially in expiration, you have to do the opposite of exploitation, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4445" target="_blank">01:14:05.000</a></span> | <span class="t">non-aggregating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4447" target="_blank">01:14:07.800</a></span> | <span class="t">And there is right now no-- nothing in built-in the transformer right now which encourages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4453" target="_blank">01:14:13.360</a></span> | <span class="t">that sort of random behavior where you seek out unfamiliar parts of the state space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4464" target="_blank">01:14:24.720</a></span> | <span class="t">That is something which is in built-in to traditional RL algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4469" target="_blank">01:14:29.800</a></span> | <span class="t">So usually you have some sort of entropy bonus to encourage expiration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4474" target="_blank">01:14:34.960</a></span> | <span class="t">And those are the sort of modifications which one would also need to think about if one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4481" target="_blank">01:14:41.120</a></span> | <span class="t">were to use decision transformers for online RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4485" target="_blank">01:14:45.680</a></span> | <span class="t">So what happens if somebody-- I mean, just naively, suppose I have this exact same setup,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4489" target="_blank">01:14:49.840</a></span> | <span class="t">and the way that I sample the action is I sample epsilon greedily, or I create a Boltzmann</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4495" target="_blank">01:14:55.580</a></span> | <span class="t">distribution and I sample from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4497" target="_blank">01:14:57.440</a></span> | <span class="t">I mean, just what happens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4499" target="_blank">01:14:59.440</a></span> | <span class="t">It seems that's what RL does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4501" target="_blank">01:15:01.640</a></span> | <span class="t">So what happens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4502" target="_blank">01:15:02.640</a></span> | <span class="t">So RL does a little bit more than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4506" target="_blank">01:15:06.920</a></span> | <span class="t">It indeed does those kinds of things where it would change the distribution, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4512" target="_blank">01:15:12.400</a></span> | <span class="t">to be a Boltzmann distribution and sample from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4514" target="_blank">01:15:14.840</a></span> | <span class="t">But it's also-- there are these-- as I said, the devil lies in the detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4519" target="_blank">01:15:19.960</a></span> | <span class="t">It's also about how it controls that expiration component with the exploitation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4525" target="_blank">01:15:25.600</a></span> | <span class="t">And it remains to be seen whether that is compatible with decision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4532" target="_blank">01:15:32.240</a></span> | <span class="t">I don't want to jump the gun, but I would say it's-- I mean, preliminary evidence suggests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4538" target="_blank">01:15:38.560</a></span> | <span class="t">that it's not directly transferable, the exact same setup to the online case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4544" target="_blank">01:15:44.800</a></span> | <span class="t">That's what we have found.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4547" target="_blank">01:15:47.000</a></span> | <span class="t">There has to be some adjustments to be made, to make-- which we are still figuring it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4553" target="_blank">01:15:53.640</a></span> | <span class="t">So the reason why I ask is, as you said, the devil's in the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4556" target="_blank">01:15:56.740</a></span> | <span class="t">And so someone naively like me might just come along and try doing what's in RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4560" target="_blank">01:16:00.000</a></span> | <span class="t">I want to hear more about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4561" target="_blank">01:16:01.440</a></span> | <span class="t">So you're saying that what works in RL may not work for decision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4566" target="_blank">01:16:06.040</a></span> | <span class="t">Can you tell us why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4568" target="_blank">01:16:08.480</a></span> | <span class="t">What pathologies emerge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4571" target="_blank">01:16:11.760</a></span> | <span class="t">What are those devils hiding in the details?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4574" target="_blank">01:16:14.280</a></span> | <span class="t">I also remind you-- sorry, we are also over time, so if you're in a hurry, feel free to--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4581" target="_blank">01:16:21.760</a></span> | <span class="t">I'll send an email and follow up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4583" target="_blank">01:16:23.040</a></span> | <span class="t">But to me, that's really exciting, and I'm sure that it's tricky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4585" target="_blank">01:16:25.920</a></span> | <span class="t">Yeah, I will just ask two more questions, and you can finish after this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4592" target="_blank">01:16:32.400</a></span> | <span class="t">So one is, did you think something like decision transformer is a way to solve the credit assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4597" target="_blank">01:16:37.440</a></span> | <span class="t">problem in RL, instead of using some sort of discount factor?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4602" target="_blank">01:16:42.080</a></span> | <span class="t">Sorry, can you repeat the question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4606" target="_blank">01:16:46.080</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4607" target="_blank">01:16:47.080</a></span> | <span class="t">So I think usually in RL, we have to rely on some sort of discount factor to encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4610" target="_blank">01:16:50.400</a></span> | <span class="t">the rewards to go something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4612" target="_blank">01:16:52.240</a></span> | <span class="t">But decision transformer is able to do this credit assignment without that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4616" target="_blank">01:16:56.720</a></span> | <span class="t">So do you think something like this book is the way we should do it, like we should, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4622" target="_blank">01:17:02.560</a></span> | <span class="t">of having some discount, try to directly predict the rewards?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4627" target="_blank">01:17:07.120</a></span> | <span class="t">So I would go on to say that I feel that discount factor is an important consideration in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4633" target="_blank">01:17:13.800</a></span> | <span class="t">and it's not incompatible with decision transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4637" target="_blank">01:17:17.600</a></span> | <span class="t">So basically, what would change, and I think the code actually gives that functionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4642" target="_blank">01:17:22.640</a></span> | <span class="t">where the returns to go would be computed as the discounted sum of rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4650" target="_blank">01:17:30.280</a></span> | <span class="t">And so it is very much compatible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4652" target="_blank">01:17:32.080</a></span> | <span class="t">So there are scenarios where our context length is not enough to actually capture the long-term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4659" target="_blank">01:17:39.760</a></span> | <span class="t">behavior we really need for credit assignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4663" target="_blank">01:17:43.840</a></span> | <span class="t">Any traditional tricks that are used could be brought in back to solve those kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4669" target="_blank">01:17:49.680</a></span> | <span class="t">problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4670" target="_blank">01:17:50.680</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4671" target="_blank">01:17:51.680</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4672" target="_blank">01:17:52.680</a></span> | <span class="t">I also thought, when I was reading the decision transformer work, that the interesting thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4676" target="_blank">01:17:56.720</a></span> | <span class="t">is that you don't have a fixed gamma, like a gamma is usually a hyperparameter, but you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4681" target="_blank">01:18:01.820</a></span> | <span class="t">don't have a fixed gamma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4682" target="_blank">01:18:02.820</a></span> | <span class="t">So do you think, can we also learn this thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4684" target="_blank">01:18:04.960</a></span> | <span class="t">And could this also be, can you have a different gamma for each time step or something, possibly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4691" target="_blank">01:18:11.040</a></span> | <span class="t">That would be interesting, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4692" target="_blank">01:18:12.520</a></span> | <span class="t">I had not thought of that, but maybe learning to predict the discount factor could be another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4699" target="_blank">01:18:19.600</a></span> | <span class="t">extension of this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4702" target="_blank">01:18:22.600</a></span> | <span class="t">Also do you think this decision transformer work, is it compatible with Q-learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4707" target="_blank">01:18:27.000</a></span> | <span class="t">So if you have something like CQL, stuff like that, can you also implement those sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4711" target="_blank">01:18:31.240</a></span> | <span class="t">loss functions on top of decision transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4716" target="_blank">01:18:36.240</a></span> | <span class="t">So I think maybe I could imagine ways in which you could encode pessimism in here as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4725" target="_blank">01:18:45.960</a></span> | <span class="t">which is key to how CQL works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4729" target="_blank">01:18:49.920</a></span> | <span class="t">And actually most of the neural algorithms work, including the model-based ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4738" target="_blank">01:18:58.000</a></span> | <span class="t">Our focus here deliberately was to go after simplicity, because we feel that part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4744" target="_blank">01:19:04.160</a></span> | <span class="t">reason why our literature has been so scattered as well, if you think about different subproblems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4749" target="_blank">01:19:09.440</a></span> | <span class="t">that everyone tries to solve, has been because everyone's tried to pick up on ideas which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4758" target="_blank">01:19:18.080</a></span> | <span class="t">are very well suited for that narrow problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4763" target="_blank">01:19:23.400</a></span> | <span class="t">Like for example, you have, whether you're doing offline or you're doing online or you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4767" target="_blank">01:19:27.800</a></span> | <span class="t">doing imitation, you're doing multitask and all these different variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4771" target="_blank">01:19:31.600</a></span> | <span class="t">And so by design, we did not want to incorporate exactly the components that exist in the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4782" target="_blank">01:19:42.960</a></span> | <span class="t">algorithms, because then it just starts looking more like an architecture change as opposed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4790" target="_blank">01:19:50.080</a></span> | <span class="t">to a more conceptual change into thinking about RLS sequence modeling very generally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4795" target="_blank">01:19:55.640</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4796" target="_blank">01:19:56.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4797" target="_blank">01:19:57.640</a></span> | <span class="t">That sounds interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4798" target="_blank">01:19:58.680</a></span> | <span class="t">So do you think we can use some sort of TD learning objectives instead of supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4803" target="_blank">01:20:03.280</a></span> | <span class="t">learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4806" target="_blank">01:20:06.920</a></span> | <span class="t">It's possible, and maybe like I'm saying that for certain, like for online RL, it might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4813" target="_blank">01:20:13.560</a></span> | <span class="t">be necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4814" target="_blank">01:20:14.560</a></span> | <span class="t">Often RL, we were happy to see it was not necessary, but it remains to be seen more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4822" target="_blank">01:20:22.720</a></span> | <span class="t">generally for transformer model or any other model for that matter, encompassing RL more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4828" target="_blank">01:20:28.160</a></span> | <span class="t">broadly, whether that becomes a necessity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4832" target="_blank">01:20:32.080</a></span> | <span class="t">Got it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4833" target="_blank">01:20:33.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4834" target="_blank">01:20:34.080</a></span> | <span class="t">Well, thanks for your time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4835" target="_blank">01:20:35.080</a></span> | <span class="t">This was great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4836" target="_blank">01:20:36.080</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4837" target="_blank">01:20:37.080</a></span> | <span class="t">Bye-bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4838" target="_blank">01:20:38.080</a></span> | <span class="t">Bye-bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=w4Bw8WYL8Ps&t=4838" target="_blank">01:20:38.080</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>