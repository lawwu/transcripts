<html><head><title>Mastering LLM Inference Optimization From Theory to Cost Effective Deployment: Mark Moyou</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Mastering LLM Inference Optimization From Theory to Cost Effective Deployment: Mark Moyou</h2><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o"><img src="https://i.ytimg.com/vi_webp/9tvJ_GYJA-o/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./9tvJ_GYJA-o.html">Whisper Transcript</a> | <a href="./transcript_9tvJ_GYJA-o.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=2" target="_blank">00:00:02.000</a></span> | <span class="t">- It's very difficult to teach extremely technical material</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=15" target="_blank">00:00:15.680</a></span> | <span class="t">in about 20 minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=16" target="_blank">00:00:16.680</a></span> | <span class="t">Initially, I had planned for at least a 45-minute session,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=19" target="_blank">00:00:19.600</a></span> | <span class="t">so I left some reading material for you at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=22" target="_blank">00:00:22.440</a></span> | <span class="t">And all of the resources, you could download slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=25" target="_blank">00:00:25.160</a></span> | <span class="t">and everything, so feel free to take screenshots or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=27" target="_blank">00:00:27.920</a></span> | <span class="t">And so I work at NVIDIA, I'm a solutions architect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=30" target="_blank">00:00:30.720</a></span> | <span class="t">so I work primarily with retail clients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=32" target="_blank">00:00:32.440</a></span> | <span class="t">and it's my job to essentially work with those clients,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=35" target="_blank">00:00:35.160</a></span> | <span class="t">understand sort of what their main challenges are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=38" target="_blank">00:00:38.080</a></span> | <span class="t">This is data processing, computer vision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=40" target="_blank">00:00:40.560</a></span> | <span class="t">across all of the different use cases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=42" target="_blank">00:00:42.720</a></span> | <span class="t">and then now I'm focused on LLM inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=45" target="_blank">00:00:45.360</a></span> | <span class="t">So my hope today is that you get a better intuition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=49" target="_blank">00:00:49.480</a></span> | <span class="t">of exactly what's happening with this particular workload</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=52" target="_blank">00:00:52.160</a></span> | <span class="t">and how you go about, to some degree, sizing things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=55" target="_blank">00:00:55.200</a></span> | <span class="t">choosing different GPUs, et cetera, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=57" target="_blank">00:00:57.680</a></span> | <span class="t">and more importantly, controlling the cost of a deployment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=60" target="_blank">00:01:00.000</a></span> | <span class="t">'cause that's oftentimes the thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=61" target="_blank">00:01:01.600</a></span> | <span class="t">that's going to really prevent you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=63" target="_blank">00:01:03.600</a></span> | <span class="t">from taking this to any meaningful scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=67" target="_blank">00:01:07.360</a></span> | <span class="t">is that overall cost of a deployment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=69" target="_blank">00:01:09.680</a></span> | <span class="t">Most folks that I've seen are doing some kind of hybrid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=72" target="_blank">00:01:12.280</a></span> | <span class="t">so you choose a big box API,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=74" target="_blank">00:01:14.080</a></span> | <span class="t">you have some set of queries that go there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=76" target="_blank">00:01:16.040</a></span> | <span class="t">In addition, you have some set of queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=78" target="_blank">00:01:18.080</a></span> | <span class="t">that go to some open-source hosted model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=81" target="_blank">00:01:21.080</a></span> | <span class="t">or some fine-tuned model that you have internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=85" target="_blank">00:01:25.160</a></span> | <span class="t">So just reference, if you go to build.nvidia.com or ai.nvidia.com,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=90" target="_blank">00:01:30.080</a></span> | <span class="t">everyone can get 1,000 inference requests for free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=93" target="_blank">00:01:33.040</a></span> | <span class="t">So I typically recommend this to folks who are benchmarking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=96" target="_blank">00:01:36.960</a></span> | <span class="t">different types of open-source models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=98" target="_blank">00:01:38.680</a></span> | <span class="t">We have all of those models hosted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=100" target="_blank">00:01:40.480</a></span> | <span class="t">It's optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=101" target="_blank">00:01:41.400</a></span> | <span class="t">If you're teaching a course and you are trying to evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=104" target="_blank">00:01:44.680</a></span> | <span class="t">all of the different LLMs that are out there for your business,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=107" target="_blank">00:01:47.640</a></span> | <span class="t">there are also multi-modal LLMs, speech LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=111" target="_blank">00:01:51.040</a></span> | <span class="t">Every model that NVIDIA accelerates will be available there for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=115" target="_blank">00:01:55.520</a></span> | <span class="t">And that's sort of a path to you to either go optimize them yourselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=119" target="_blank">00:01:59.280</a></span> | <span class="t">or to work with us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=120" target="_blank">00:02:00.560</a></span> | <span class="t">You'll see things about NVIDIA inference microservice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=123" target="_blank">00:02:03.600</a></span> | <span class="t">and all of those things that you can take to enterprise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=126" target="_blank">00:02:06.280</a></span> | <span class="t">So we have sometimes the, I'll call it the rocky road,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=130" target="_blank">00:02:10.360</a></span> | <span class="t">and then there's smooth roads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=131" target="_blank">00:02:11.200</a></span> | <span class="t">Whatever path you want to take, we're here to support you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=135" target="_blank">00:02:15.200</a></span> | <span class="t">In terms of agenda, very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=137" target="_blank">00:02:17.040</a></span> | <span class="t">I want you to understand the LLM inference workload,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=139" target="_blank">00:02:19.960</a></span> | <span class="t">and then we'll move to how you go about measuring a production deployment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=144" target="_blank">00:02:24.200</a></span> | <span class="t">and some of the things you need to be watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=146" target="_blank">00:02:26.200</a></span> | <span class="t">It's a little more than, let's say, you know, the total time to generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=150" target="_blank">00:02:30.080</a></span> | <span class="t">and really understanding what's happening on the GPUs as you sort of scale out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=154" target="_blank">00:02:34.160</a></span> | <span class="t">Even if you have a single GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=155" target="_blank">00:02:35.720</a></span> | <span class="t">I think it's very important for you to just have that intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=158" target="_blank">00:02:38.480</a></span> | <span class="t">And then lastly, I'll show you some software that you can use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=161" target="_blank">00:02:41.840</a></span> | <span class="t">some open source packages that you can use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=163" target="_blank">00:02:43.480</a></span> | <span class="t">and then point to some paid offerings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=166" target="_blank">00:02:46.400</a></span> | <span class="t">Okay, we're going to get into the LLM inference workload itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=170" target="_blank">00:02:50.360</a></span> | <span class="t">So the first part is really understanding what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=173" target="_blank">00:02:53.320</a></span> | <span class="t">when you send a prompt onto the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=176" target="_blank">00:02:56.200</a></span> | <span class="t">So I have this example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=177" target="_blank">00:02:57.200</a></span> | <span class="t">I'm saying, okay, write me a presentation so I sound smart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=179" target="_blank">00:02:59.840</a></span> | <span class="t">I come to the AI engineer conference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=181" target="_blank">00:03:01.840</a></span> | <span class="t">and you guys are maybe going to like to talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=184" target="_blank">00:03:04.160</a></span> | <span class="t">And essentially what I'm going to do is I'm going to put that on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=186" target="_blank">00:03:06.640</a></span> | <span class="t">So the moment that I send that prompt on the GPU, it stays on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=190" target="_blank">00:03:10.520</a></span> | <span class="t">So think about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=191" target="_blank">00:03:11.800</a></span> | <span class="t">And then from there, I'm going to generate one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=194" target="_blank">00:03:14.680</a></span> | <span class="t">So I'm generating the tokens, LLM inference is hard,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=198" target="_blank">00:03:18.160</a></span> | <span class="t">and I put the timestamps T1 through T4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=201" target="_blank">00:03:21.240</a></span> | <span class="t">So in every single deployment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=202" target="_blank">00:03:22.720</a></span> | <span class="t">no matter how fast anyone claims they're doing things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=206" target="_blank">00:03:26.320</a></span> | <span class="t">it's typically one token that's generated at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=209" target="_blank">00:03:29.360</a></span> | <span class="t">that's very important to understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=211" target="_blank">00:03:31.320</a></span> | <span class="t">The next thing is that in order for an LLM to give you a coherent answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=215" target="_blank">00:03:35.800</a></span> | <span class="t">just like how you speak,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=217" target="_blank">00:03:37.400</a></span> | <span class="t">you have to remember every single thing that you said before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=220" target="_blank">00:03:40.240</a></span> | <span class="t">and that you'll understand the mechanism of how LLMs are able to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=225" target="_blank">00:03:45.240</a></span> | <span class="t">So that's why I'm putting LLM inference is in red and putting that back onto the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=230" target="_blank">00:03:50.240</a></span> | <span class="t">So every token that I generate gets locked onto the GPU, and then you'll actually see what that looks like in terms of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=236" target="_blank">00:03:56.680</a></span> | <span class="t">How many of you have heard of KvCache before?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=241" target="_blank">00:04:01.120</a></span> | <span class="t">Okay, some of you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=243" target="_blank">00:04:03.120</a></span> | <span class="t">Typically, I don't see maybe many leaders hear about this thing called KvCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=247" target="_blank">00:04:07.120</a></span> | <span class="t">KvCache is this thing that really drives to some degree the cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=251" target="_blank">00:04:11.640</a></span> | <span class="t">So whether or not you use some big box API or you're using a single GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=257" target="_blank">00:04:17.080</a></span> | <span class="t">it's all the same sort of mechanisms, the same algorithm that everyone is trying to solve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=261" target="_blank">00:04:21.080</a></span> | <span class="t">So in terms of steps, here I like to, as I said, sharpen your intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=267" target="_blank">00:04:27.880</a></span> | <span class="t">So the first thing, if we move from the left, my first job is to convert these texts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=271" target="_blank">00:04:31.760</a></span> | <span class="t">whatever texts that you send, we're going to focus on LLM inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=275" target="_blank">00:04:35.320</a></span> | <span class="t">into some words that the model understands.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=277" target="_blank">00:04:37.800</a></span> | <span class="t">So the model will have its own vocabulary, and it's my job to translate that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=281" target="_blank">00:04:41.400</a></span> | <span class="t">I'll give you the technical terms coming up after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=284" target="_blank">00:04:44.600</a></span> | <span class="t">And the first thing that happens is I do some initial prompt processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=288" target="_blank">00:04:48.200</a></span> | <span class="t">So I have to compute the attention mechanism on the entire prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=292" target="_blank">00:04:52.600</a></span> | <span class="t">I repeat that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=293" target="_blank">00:04:53.640</a></span> | <span class="t">I have to compute the attention mechanism on the entire prompt per user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=297" target="_blank">00:04:57.800</a></span> | <span class="t">So if I have a million people hitting my service and a million people send 10,000 tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=302" target="_blank">00:05:02.840</a></span> | <span class="t">that's a million times 10,000 attention mechanisms that I need to compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=307" target="_blank">00:05:07.560</a></span> | <span class="t">also while generating tokens for other people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=310" target="_blank">00:05:10.360</a></span> | <span class="t">So it's good for you to appreciate sort of that complexity that's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=314" target="_blank">00:05:14.600</a></span> | <span class="t">And once I finish processing that prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=316" target="_blank">00:05:16.760</a></span> | <span class="t">then I'm going to start generating one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=319" target="_blank">00:05:19.400</a></span> | <span class="t">And that typically happens very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=321" target="_blank">00:05:21.000</a></span> | <span class="t">And then from there, every token that gets generated, that's in the LLM's vocabulary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=326" target="_blank">00:05:26.360</a></span> | <span class="t">I need to now de-tokenize that back into your language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=330" target="_blank">00:05:30.200</a></span> | <span class="t">So here's the technical terms that you'll see when you read the literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=334" target="_blank">00:05:34.040</a></span> | <span class="t">or you read super technical documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=336" target="_blank">00:05:36.680</a></span> | <span class="t">First is tokenization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=338" target="_blank">00:05:38.280</a></span> | <span class="t">Each model will have its own tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=340" target="_blank">00:05:40.360</a></span> | <span class="t">And the thing to think about, when you think of tokenizers, when they did pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=346" target="_blank">00:05:46.760</a></span> | <span class="t">they downloaded the internet and some, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=349" target="_blank">00:05:49.960</a></span> | <span class="t">And they cleaned it up, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=351" target="_blank">00:05:51.800</a></span> | <span class="t">So tokenizer, and as you start thinking of the complexity across languages, coding languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=356" target="_blank">00:05:56.840</a></span> | <span class="t">regions, et cetera, et cetera, they tried to get what is the minimal set of character groups that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=364" target="_blank">00:06:04.600</a></span> | <span class="t">represent this entire training data set efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=367" target="_blank">00:06:07.240</a></span> | <span class="t">Because it's really all about efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=369" target="_blank">00:06:09.400</a></span> | <span class="t">So for instance, the LLAMA tokenizer has 128,000 tokens, all right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=373" target="_blank">00:06:13.880</a></span> | <span class="t">And I'll talk a bit more about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=375" target="_blank">00:06:15.160</a></span> | <span class="t">So here's what it actually kind of looks like on a GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=378" target="_blank">00:06:18.200</a></span> | <span class="t">So I tokenize, the LLAMA understands it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=380" target="_blank">00:06:20.680</a></span> | <span class="t">I go into this thing called pre-fill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=382" target="_blank">00:06:22.360</a></span> | <span class="t">Pre-fill is the stage where you compute the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=385" target="_blank">00:06:25.400</a></span> | <span class="t">And many people are doing advancements with attention mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=389" target="_blank">00:06:29.000</a></span> | <span class="t">I'll talk a bit more about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=390" target="_blank">00:06:30.520</a></span> | <span class="t">So there are tons of different schemes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=392" target="_blank">00:06:32.360</a></span> | <span class="t">People leverage all of the different types of memory hierarchies in GPUs to really accelerate this type of workload.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=399" target="_blank">00:06:39.800</a></span> | <span class="t">And then I start generating tokens one at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=402" target="_blank">00:06:42.520</a></span> | <span class="t">The red and the green just signify, hey, I'm storing those tokens on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=406" target="_blank">00:06:46.760</a></span> | <span class="t">The green is the latest one that I sent out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=409" target="_blank">00:06:49.160</a></span> | <span class="t">So hopefully that makes sort of intuitive sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=411" target="_blank">00:06:51.400</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=412" target="_blank">00:06:52.680</a></span> | <span class="t">So the other thing I want you to visualize, I think it's nice to visualize what is the actual data that sits on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=421" target="_blank">00:07:01.720</a></span> | <span class="t">So first, a token is approximately four characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=424" target="_blank">00:07:04.840</a></span> | <span class="t">That's a nice way for you to think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=427" target="_blank">00:07:07.160</a></span> | <span class="t">So from here, I have two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=430" target="_blank">00:07:10.680</a></span> | <span class="t">So the first vector is just showing token one through token V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=434" target="_blank">00:07:14.200</a></span> | <span class="t">V is the total number of tokens that I have in my tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=436" target="_blank">00:07:16.920</a></span> | <span class="t">And the second vector below is just I have some numeric index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=441" target="_blank">00:07:21.640</a></span> | <span class="t">I don't want to keep using the token to reference itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=444" target="_blank">00:07:24.520</a></span> | <span class="t">I just use the number as a lookup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=446" target="_blank">00:07:26.760</a></span> | <span class="t">So my job when a prompt comes in is to convert that text into those token, what I'm going to call token IDs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=455" target="_blank">00:07:35.400</a></span> | <span class="t">So I have make me sound smart and make me sound smarter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=458" target="_blank">00:07:38.520</a></span> | <span class="t">You see two vector sets of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=460" target="_blank">00:07:40.680</a></span> | <span class="t">And the key thing I want you to walk away from that distinction is that an LLM token is not a human word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=468" target="_blank">00:07:48.040</a></span> | <span class="t">Sometimes it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=468" target="_blank">00:07:48.920</a></span> | <span class="t">Sometimes it's not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=469" target="_blank">00:07:49.720</a></span> | <span class="t">It's typically some sub-party words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=471" target="_blank">00:07:51.720</a></span> | <span class="t">You'll see weird symbols when you look at tokenizers from different models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=475" target="_blank">00:07:55.960</a></span> | <span class="t">But you want that first framing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=477" target="_blank">00:07:57.320</a></span> | <span class="t">So now we have text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=478" target="_blank">00:07:58.440</a></span> | <span class="t">We hit to a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=479" target="_blank">00:07:59.400</a></span> | <span class="t">So from there, each one of those LLM tokens had a corresponding embedding vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=486" target="_blank">00:08:06.920</a></span> | <span class="t">So embedding vector is everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=488" target="_blank">00:08:08.280</a></span> | <span class="t">We embed videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=489" target="_blank">00:08:09.960</a></span> | <span class="t">We embed images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=491" target="_blank">00:08:11.000</a></span> | <span class="t">We embed text tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=493" target="_blank">00:08:13.000</a></span> | <span class="t">Think of it as a representation that an LLM can use to compare things and do math on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=499" target="_blank">00:08:19.160</a></span> | <span class="t">So that's why we always want to convert into some vector representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=503" target="_blank">00:08:23.960</a></span> | <span class="t">Because some vector representation is just some high dimensional coordinate space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=508" target="_blank">00:08:28.120</a></span> | <span class="t">And we're just rearranging objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=510" target="_blank">00:08:30.280</a></span> | <span class="t">That's to some degree what you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=511" target="_blank">00:08:31.720</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=513" target="_blank">00:08:33.160</a></span> | <span class="t">So from those token IDs, I went to the actual embedding vectors themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=518" target="_blank">00:08:38.920</a></span> | <span class="t">So if you look, make me sound smart, now becomes a matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=521" target="_blank">00:08:41.720</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=522" target="_blank">00:08:42.760</a></span> | <span class="t">Make me sound smarter becomes a matrix with an extra column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=526" target="_blank">00:08:46.120</a></span> | <span class="t">So in reality, what you're doing every time you submit a prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=529" target="_blank">00:08:49.800</a></span> | <span class="t">I don't care what LLM you submit it to, who you submit it to, this is what you're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=534" target="_blank">00:08:54.440</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=535" target="_blank">00:08:55.240</a></span> | <span class="t">You are converting your text, now images as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=538" target="_blank">00:08:58.440</a></span> | <span class="t">They get converted to some IDA tokens or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=541" target="_blank">00:09:01.640</a></span> | <span class="t">That'll be another interesting talk to do diffusion models, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=545" target="_blank">00:09:05.640</a></span> | <span class="t">But you're really putting this large matrix on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=549" target="_blank">00:09:09.880</a></span> | <span class="t">So the next question you should ask is, okay, why are GPUs good for this workload?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=554" target="_blank">00:09:14.520</a></span> | <span class="t">Because they process matrices really, really fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=557" target="_blank">00:09:17.320</a></span> | <span class="t">So that's sort of the advantage and the thing, hopefully that makes a lot more sense to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=562" target="_blank">00:09:22.360</a></span> | <span class="t">Now, the next thing I want to talk about is how the LLM is going to process these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=567" target="_blank">00:09:27.320</a></span> | <span class="t">And I'll keep in mind if any, well, I'm not even going to ask you to raise your hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=571" target="_blank">00:09:31.320</a></span> | <span class="t">I'm 100% sure each of you has used an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=573" target="_blank">00:09:33.640</a></span> | <span class="t">If you have not, I'm not sure what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=578" target="_blank">00:09:38.200</a></span> | <span class="t">The other one is the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=579" target="_blank">00:09:39.800</a></span> | <span class="t">I truly think it's one of the things that you should understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=585" target="_blank">00:09:45.480</a></span> | <span class="t">If we ever drift away from it, that's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=587" target="_blank">00:09:47.800</a></span> | <span class="t">But the fundamentals of that mechanism and seeing sort of the innovations around that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=592" target="_blank">00:09:52.840</a></span> | <span class="t">I think, can help anyone, any business leader, et cetera, et cetera,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=596" target="_blank">00:09:56.040</a></span> | <span class="t">just because you are able to speak a different kind of language in this generative future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=601" target="_blank">00:10:01.320</a></span> | <span class="t">So as you think of the attention mechanism, the intuition that you should have is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=605" target="_blank">00:10:05.240</a></span> | <span class="t">that mechanism of relating tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=607" target="_blank">00:10:07.480</a></span> | <span class="t">How do I distinguish in a sentence what is important?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=610" target="_blank">00:10:10.360</a></span> | <span class="t">And then for the next token that's going to be generated, hey, what tokens that I said before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=616" target="_blank">00:10:16.120</a></span> | <span class="t">were really important for me to make a next good decision for that next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=620" target="_blank">00:10:20.120</a></span> | <span class="t">So that's the intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=621" target="_blank">00:10:21.960</a></span> | <span class="t">And now we're going to -- we won't necessarily touch too much of the math,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=624" target="_blank">00:10:24.520</a></span> | <span class="t">but I want you to see sort of what's happening on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=627" target="_blank">00:10:27.720</a></span> | <span class="t">So once again, the prompt comes in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=629" target="_blank">00:10:29.240</a></span> | <span class="t">I'm just going to do a short one, make me sound smart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=631" target="_blank">00:10:31.800</a></span> | <span class="t">I'm going to generate this token called LLM, all right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=634" target="_blank">00:10:34.840</a></span> | <span class="t">We saw these same matrices that I said before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=637" target="_blank">00:10:37.960</a></span> | <span class="t">So remember, my text now turns into a matrix hitting onto the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=641" target="_blank">00:10:41.960</a></span> | <span class="t">And the main thing I want you to understand or visualize here is actually how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=648" target="_blank">00:10:48.520</a></span> | <span class="t">an LLM memory works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=649" target="_blank">00:10:49.720</a></span> | <span class="t">So now when you're speaking, you've recorded everything that I've said for the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=654" target="_blank">00:10:54.680</a></span> | <span class="t">10 minutes in your brain, somewhere it's stored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=657" target="_blank">00:10:57.960</a></span> | <span class="t">So now you're going to see how the LLM is storing what it is that you just said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=661" target="_blank">00:11:01.320</a></span> | <span class="t">So from there, a lot of folks will hear about these query key and value matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=666" target="_blank">00:11:06.840</a></span> | <span class="t">This is what the actual model weights look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=669" target="_blank">00:11:09.080</a></span> | <span class="t">So when you look at a model weights file, if you go on Hugging Face, there's typically a JSON file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=673" target="_blank">00:11:13.720</a></span> | <span class="t">that will show you all of the different pieces of model files.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=676" target="_blank">00:11:16.920</a></span> | <span class="t">And you'll see this thing called Q, K, and V. So I have these model weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=680" target="_blank">00:11:20.920</a></span> | <span class="t">So now I've went from text to a matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=683" target="_blank">00:11:23.720</a></span> | <span class="t">I'm going to matrix multiply against the weights of the models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=686" target="_blank">00:11:26.440</a></span> | <span class="t">So now I get these three output models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=689" target="_blank">00:11:29.320</a></span> | <span class="t">So think of these weight matrices that I showed here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=692" target="_blank">00:11:32.520</a></span> | <span class="t">Think as -- when you're doing a projection, what you're doing is you're taking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=697" target="_blank">00:11:37.000</a></span> | <span class="t">some coordinates and you're putting it into a different space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=700" target="_blank">00:11:40.360</a></span> | <span class="t">That's really what you're doing when you do vector matrix math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=703" target="_blank">00:11:43.160</a></span> | <span class="t">So now when I do this matrix multiplication, this query key and value matrix -- so if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=708" target="_blank">00:11:48.840</a></span> | <span class="t">different tutorials on attention, you'll see these things pop up a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=712" target="_blank">00:11:52.280</a></span> | <span class="t">So hopefully that will help you to read it a lot more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=714" target="_blank">00:11:54.440</a></span> | <span class="t">This is now the LLM's interpretation of each of those tokens that you sent in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=719" target="_blank">00:11:59.960</a></span> | <span class="t">Right? And now the job is how do I now take these query key and value matrices and sort of interpret it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=727" target="_blank">00:12:07.240</a></span> | <span class="t">to try to generate the next best token. And this is just happening constantly over and over every single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=732" target="_blank">00:12:12.920</a></span> | <span class="t">token that's happening. But the key thing I want you to walk away on the slide is where I drew the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=737" target="_blank">00:12:17.640</a></span> | <span class="t">and the value. Right? When people talk about KV cache optimization, every LLM performance engineer is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=744" target="_blank">00:12:24.600</a></span> | <span class="t">literally trying to make that thing as fast and small as possible. And that will make a little more sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=750" target="_blank">00:12:30.440</a></span> | <span class="t">as to what that does to your cost. But ultimately, these key and value matrices, this is like your LLM's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=755" target="_blank">00:12:35.720</a></span> | <span class="t">memory. So it will make a little more sense coming up. I know I didn't show a ton of the math. I show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=759" target="_blank">00:12:39.960</a></span> | <span class="t">some tutorials afterwards so you can go read more about that. My intention here is for you to visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">key and value. So every time you see a prompt, I just want you to thinking, crap, key and value is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=769" target="_blank">00:12:49.720</a></span> | <span class="t">my GPU. Okay? The next. So here's the real value of the KV cache. So remember we said that whenever I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=780" target="_blank">00:13:00.120</a></span> | <span class="t">generate a token, I'm going to push it back into the GPU. Right? So every token I generate, it goes back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=785" target="_blank">00:13:05.560</a></span> | <span class="t">into the GPU. And then I have to compute an attention mechanism. So this is what's happening. This new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=790" target="_blank">00:13:10.200</a></span> | <span class="t">token I generated, LLM, I get its vector representation, as you see in blue. But now I do that vector matrix math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=798" target="_blank">00:13:18.920</a></span> | <span class="t">now. So before I did matrix matrix math, that's my first prompt first comes in. I generated my first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=806" target="_blank">00:13:26.120</a></span> | <span class="t">Now I'm doing vector matrix math. You know, people will batch this across all requests, but I'm just showing you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=812" target="_blank">00:13:32.360</a></span> | <span class="t">a single request so you can see it. Now, the value of the KV cache is, if I were to, if I didn't have the KV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=820" target="_blank">00:13:40.840</a></span> | <span class="t">cache, I would have to reprocess all of that work I did on the prompt that I did before. So this is the benefit of your KV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=828" target="_blank">00:13:48.120</a></span> | <span class="t">KV cache. Now I'm just going to compute attention on this newest token. How does this new token relate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=833" target="_blank">00:13:53.400</a></span> | <span class="t">to everything that I said before? That's the thing that's really happening intuitively. So if I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=839" target="_blank">00:13:59.320</a></span> | <span class="t">this KV cache, my generation is going to be fast. And it's really up to what's called the batch manager</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=846" target="_blank">00:14:06.520</a></span> | <span class="t">on the GPU to make sure that I'm just pushing out as many tokens as possible. Okay. So if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=852" target="_blank">00:14:12.920</a></span> | <span class="t">an LLM, these groups of three matrices are called an attention head. There are more matrices than that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=858" target="_blank">00:14:18.520</a></span> | <span class="t">but these are the main ones. LLAMA has 32 attention heads. So I just kind of want you to appreciate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=863" target="_blank">00:14:23.800</a></span> | <span class="t">what an LLM really looks like. All right. So I have 32 sets of these matrices. I have 32 of those KV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=870" target="_blank">00:14:30.200</a></span> | <span class="t">caches happening at the same time. And now I have to combine all of that to then generate the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=874" target="_blank">00:14:34.840</a></span> | <span class="t">token. So there's an incredible amount of work that happens in a very short space of time to give you a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=880" target="_blank">00:14:40.840</a></span> | <span class="t">coherent token. Okay. A good mental model for you to keep in your head -- I'm going to speed up a little bit -- is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=887" target="_blank">00:14:47.000</a></span> | <span class="t">to -- if you see the number of parameters, multiply that by two, and that is your FP16 gigabyte memory on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=894" target="_blank">00:14:54.120</a></span> | <span class="t">the GPU. So if you have, let's say, an L4, I think is 20 gigs, and I have a Lama 8B, that's automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=901" target="_blank">00:15:01.800</a></span> | <span class="t">16 gigs FP16. So I only have four gigs left for my KV cache. So on the GPU, it's either the model weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=909" target="_blank">00:15:09.480</a></span> | <span class="t">or tokens. That's it. There's nothing else on the GPU. And I have a thing to read on that. This is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=914" target="_blank">00:15:14.760</a></span> | <span class="t">really good blog. It shows you all of the different optimizations that you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=919" target="_blank">00:15:19.720</a></span> | <span class="t">Okay. Now let's talk about measuring. So if you ever see this thing called ISL or -- do I have it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=927" target="_blank">00:15:27.000</a></span> | <span class="t">there? Oh, sorry. ISL or OSL, that's input sequence link, output sequence link. So now I want you to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=933" target="_blank">00:15:33.560</a></span> | <span class="t">see what some advanced monitoring might look like. If any of you are DevOps folks, these are things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=937" target="_blank">00:15:37.880</a></span> | <span class="t">you want to record. The first thing that we measure is time to first token. So how long does it take me to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=942" target="_blank">00:15:42.920</a></span> | <span class="t">generate? Process the prompt and then generate my first token. And that's typically a measurement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=949" target="_blank">00:15:49.000</a></span> | <span class="t">of how good your attention mechanism processing is. That's really what you're trying to suss out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=954" target="_blank">00:15:54.840</a></span> | <span class="t">So that's time to first token. Into token latencies. So after I've generated my first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=959" target="_blank">00:15:59.880</a></span> | <span class="t">every single token after that, I'm looking at those individual spaces. So everything that's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=964" target="_blank">00:16:04.680</a></span> | <span class="t">happen there, think about when the system is on the load. I have, you know, a thousand requests coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=969" target="_blank">00:16:09.800</a></span> | <span class="t">into my system, I'm generating a thousand sets of different tokens. And the more memory I occupy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=974" target="_blank">00:16:14.760</a></span> | <span class="t">typically that slows down processing. So if you start to see drift in this metric, then -- so I'll show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=979" target="_blank">00:16:19.800</a></span> | <span class="t">you some plots that you can look at. And then time to total generation. How long did it take me to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=984" target="_blank">00:16:24.280</a></span> | <span class="t">initially get the prompt, fully finish the answer. All right? Super intuitive. Like I said, ISL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=989" target="_blank">00:16:29.880</a></span> | <span class="t">OSL, that's all that means when you see them on the plots coming up. Okay. This is a very important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=996" target="_blank">00:16:36.520</a></span> | <span class="t">paradigm for you to understand in your mind. So I worked with a lot of folks on, you know, maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1001" target="_blank">00:16:41.880</a></span> | <span class="t">Rex's deployments or deployments of other types of models. So on the GPU, if you're only deploying one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1007" target="_blank">00:16:47.640</a></span> | <span class="t">model on a GPU, outside of LLM inference, in my opinion, I think you're wasting the GPU. You can put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1012" target="_blank">00:16:52.920</a></span> | <span class="t">multiple models on the GPU to actually increase your throughput. That's why it was really created.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1017" target="_blank">00:16:57.240</a></span> | <span class="t">So this is a slide -- excuse me. This figure is just showing I can have multiple models. I have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1023" target="_blank">00:17:03.800</a></span> | <span class="t">space for data and that's how I increase my throughput per unit hardware. However, on the LLM inference side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1029" target="_blank">00:17:09.720</a></span> | <span class="t">it's very different. I have one model. You know, folks can fit multiple models on a GPU. That's cool,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1034" target="_blank">00:17:14.680</a></span> | <span class="t">but that's not a real production use case. You'll typically have a single model. The remaining space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1039" target="_blank">00:17:19.400</a></span> | <span class="t">that you have is all for KVCache and generating all those tokens. So I just put four different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1044" target="_blank">00:17:24.280</a></span> | <span class="t">requests and I just kind of want you to see the boxes that are happening. Okay. I would say this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1049" target="_blank">00:17:29.400</a></span> | <span class="t">the most important slide in the entire presentation because this is the thing that will determine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1053" target="_blank">00:17:33.800</a></span> | <span class="t">both your cost and performance. So there are four different querying patterns that happen. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1059" target="_blank">00:17:39.720</a></span> | <span class="t">is something that you must measure in your deployment because oftentimes you might read benchmarks and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1065" target="_blank">00:17:45.000</a></span> | <span class="t">just say, all right, they'll cherry pick one or two of these. But in reality, in your production</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1069" target="_blank">00:17:49.880</a></span> | <span class="t">system, you might have several of these different patterns that are occurring. So let's take a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1074" target="_blank">00:17:54.520</a></span> | <span class="t">at the first one. Long input, short output. So long input means it's going to take me technically longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1081" target="_blank">00:18:01.160</a></span> | <span class="t">to compute the attention mechanism. So my pre-filled stage will be longer. It occupies more memory from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1086" target="_blank">00:18:06.760</a></span> | <span class="t">my prompt. Does that make sense intuitively? Hopefully it's grabbing you. But then on the generation side,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1092" target="_blank">00:18:12.360</a></span> | <span class="t">I don't generate much tokens. So there's not much -- those tokens are not picking up a lot of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1097" target="_blank">00:18:17.400</a></span> | <span class="t">And they will tend to finish fast. So the second one, or maybe the most costly use case is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1103" target="_blank">00:18:23.560</a></span> | <span class="t">so I have clients that will message me and say, hey, my data scientists are putting two bigger prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1108" target="_blank">00:18:28.200</a></span> | <span class="t">on my GPUs. So now they're killing my deployment. Because if everyone went and put the maximum context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1113" target="_blank">00:18:33.800</a></span> | <span class="t">length, I can only fit so many requests on the GPU. So that's something for you to think about. You'll have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1118" target="_blank">00:18:38.920</a></span> | <span class="t">manage that internally with your deployments. So that's why I'm putting, you know, okay, the GPU is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1123" target="_blank">00:18:43.960</a></span> | <span class="t">really full. Because long text -- excuse me, long input, long output. The next one, short, long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1129" target="_blank">00:18:49.080</a></span> | <span class="t">you know, your time to first token will be really fast. I don't have much to compute the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1133" target="_blank">00:18:53.880</a></span> | <span class="t">on. But hey, I'm generating a ton of tokens. That's really, really fast. So hopefully, as you start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1139" target="_blank">00:18:59.080</a></span> | <span class="t">measuring these types of different query patterns, you'll see different results. I just put, you know, what a random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1145" target="_blank">00:19:05.800</a></span> | <span class="t">sampling set might actually look like on the GPU. Because not everyone will send the same length of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1150" target="_blank">00:19:10.600</a></span> | <span class="t">input and output. So that will -- it'll be good for you to just sort of visualize and track these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1155" target="_blank">00:19:15.640</a></span> | <span class="t">statistics. More importantly, why we're doing that internally -- I'm going to steal the time here, Peter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1161" target="_blank">00:19:21.400</a></span> | <span class="t">More importantly, why we're doing that or why we're tracking these things is that the whole goal is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1167" target="_blank">00:19:27.160</a></span> | <span class="t">build -- I have a big model. My goal is to shrink it as much as I can, but to keep it as accurate as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1173" target="_blank">00:19:33.960</a></span> | <span class="t">So the more that I shrink, the faster it runs, the more GPU memory I have for what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1178" target="_blank">00:19:38.840</a></span> | <span class="t">Tokens. All right? So that's how you really try to improve your cost. This is why I'm sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1184" target="_blank">00:19:44.680</a></span> | <span class="t">proposing to you to build inference engines. So all I'm showing here is a 2D histogram of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1189" target="_blank">00:19:49.880</a></span> | <span class="t">input sequence length versus output sequence length. Because the question that you'll have to answer is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1194" target="_blank">00:19:54.600</a></span> | <span class="t">hey, how long are my actual prompts? Someone might say, okay, here's the max prompt length that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1200" target="_blank">00:20:00.760</a></span> | <span class="t">ingest, and the max prompt you can out -- excuse me, get on the output. And all of the big box model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1207" target="_blank">00:20:07.000</a></span> | <span class="t">providers have to estimate this when they go into costing or providing a service to you, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1212" target="_blank">00:20:12.200</a></span> | <span class="t">Because they have to host all of that machinery under the hood now that you understand what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1217" target="_blank">00:20:17.640</a></span> | <span class="t">So we use this to statistically determine what is the max input sequence length and the max output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1224" target="_blank">00:20:24.280</a></span> | <span class="t">sequence length across all of my users. And this will give you a really good indication of how you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1229" target="_blank">00:20:29.880</a></span> | <span class="t">size your engines. We use that to actually build more optimized engines. In addition, it will just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1235" target="_blank">00:20:35.960</a></span> | <span class="t">give you good view as to maybe what you call it, scaling out and things like that. The next one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1241" target="_blank">00:20:41.800</a></span> | <span class="t">time-to-first token analysis. Remember, time-to-first token is measuring my performance of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1247" target="_blank">00:20:47.080</a></span> | <span class="t">mechanism on the load. So someone might show attention mechanism at one query. Show me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1253" target="_blank">00:20:53.240</a></span> | <span class="t">attention mechanism on the load. When this thing is fully maxed out 24/7, that's when you really need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1258" target="_blank">00:20:58.840</a></span> | <span class="t">to start measuring these types of things. So this is something you can look at. These are sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1262" target="_blank">00:21:02.520</a></span> | <span class="t">experimental plots. There's a package called GenAI Perf that will be released open source. It's out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1268" target="_blank">00:21:08.520</a></span> | <span class="t">already. I have a link to it there. This is where it will generate these plots for you. But I'm just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1273" target="_blank">00:21:13.560</a></span> | <span class="t">showing you what the engineers are looking at internally to measure the performance of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1278" target="_blank">00:21:18.040</a></span> | <span class="t">compute platform. Next, time to completion analysis. How long did it take me to go from start to finish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1283" target="_blank">00:21:23.320</a></span> | <span class="t">across every single request. Naturally, the wider that box plot, you have to intuitively ask what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1290" target="_blank">00:21:30.360</a></span> | <span class="t">happening. Why did this person's prompt take longer than another? So you can investigate either batching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1296" target="_blank">00:21:36.040</a></span> | <span class="t">issues, scheduling issues, different things like that. I'll take questions in the end. Oh, I have to move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1300" target="_blank">00:21:40.920</a></span> | <span class="t">really fast. Sorry there, Peter. Okay. I'm going to speed up here. Token to token latency. Peter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1305" target="_blank">00:21:45.880</a></span> | <span class="t">how much time I got? Oh, you're fine. We'll definitely have time for the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1309" target="_blank">00:21:49.400</a></span> | <span class="t">Okay, cool. I'm going to steal. I'm definitely over. Sorry. I realize I may have gone a little too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1314" target="_blank">00:21:54.360</a></span> | <span class="t">fast. So forgive me for that. No, you have five minutes. Cool. All right. Token to token latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1319" target="_blank">00:21:59.880</a></span> | <span class="t">So that is I'm generating tokens. I'm looking at that spacing versus token position. So the longer a sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1326" target="_blank">00:22:06.920</a></span> | <span class="t">sequence gets, remember, my memory grows. So typically that means that system is under more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1331" target="_blank">00:22:11.880</a></span> | <span class="t">load. It has more throttling that might happen under high load of requests. So if I see a large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1338" target="_blank">00:22:18.280</a></span> | <span class="t">variation in token to token latency as the sequence gets longer when I'm generating, that means I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1345" target="_blank">00:22:25.160</a></span> | <span class="t">very performant. All right. So we look at that to see I try to make sure that that's constant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1349" target="_blank">00:22:29.720</a></span> | <span class="t">no matter how much tokens I'm generating. That means I'm really proficient. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1354" target="_blank">00:22:34.600</a></span> | <span class="t">Last one would be time to first token versus number of input tokens. So time to first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1361" target="_blank">00:22:41.480</a></span> | <span class="t">remember, is computing the attention mechanism, okay, versus number of input tokens. So if I have a bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1367" target="_blank">00:22:47.480</a></span> | <span class="t">prompt, my attentions will take longer. But if that plot goes up, like, from your perspective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1373" target="_blank">00:22:53.400</a></span> | <span class="t">it goes up like this in terms of sequence length, that's not really good performance. We really look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1378" target="_blank">00:22:58.120</a></span> | <span class="t">that slope and we try to get that slope almost, you know, as low as possible. So if you send me this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1384" target="_blank">00:23:04.040</a></span> | <span class="t">long sequence, I can get that thing done really fast. Okay. Okay. In terms of software, you will see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1390" target="_blank">00:23:10.520</a></span> | <span class="t">this thing called TRT LLM. Triton is an open source inference server. So you can deploy models on CPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1397" target="_blank">00:23:17.000</a></span> | <span class="t">on GPU, computer vision, Reksis, Python, PyTorch, TensorFlow. It will host all of the different types of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1403" target="_blank">00:23:23.960</a></span> | <span class="t">So there's one way that your deployment team deploys. All their data scientists are happy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1408" target="_blank">00:23:28.040</a></span> | <span class="t">because they don't have to do conversion. You're happy as a deployment person because you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1411" target="_blank">00:23:31.720</a></span> | <span class="t">have to manage a TorchServe versus TFServe and Flask and all of it is done through one. It's written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1417" target="_blank">00:23:37.560</a></span> | <span class="t">in C++, blazingly fast. And then the other thing you'll see in video, you'll see a lot more coming out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1422" target="_blank">00:23:42.600</a></span> | <span class="t">of NVIDIA's NVIDIA inference microservice because building these engines, getting them deployed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1427" target="_blank">00:23:47.240</a></span> | <span class="t">optimize the scale, it's not easy. So we've sort of made that easy for you as an enterprise offering,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1431" target="_blank">00:23:51.560</a></span> | <span class="t">but you guys can try it out for free. Okay. So TRT LLM, let me just give you lots of stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1437" target="_blank">00:23:57.800</a></span> | <span class="t">on this slide. But the main thing I want you to walk away with is this is the model compilation package</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1443" target="_blank">00:24:03.560</a></span> | <span class="t">for LLMs on NVIDIA GPUs. If you want to get best performance from NVIDIA GPUs, please make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1449" target="_blank">00:24:09.240</a></span> | <span class="t">you use TRT LLM. Unnaturally, once we're investing more in NIM, you'll see some more things come out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1455" target="_blank">00:24:15.560</a></span> | <span class="t">So you'll see performances on A100 and H100. Really focus on FP8 GPUs. So FP8 will be Hopper and Ada Lovelace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1464" target="_blank">00:24:24.520</a></span> | <span class="t">Okay. So FP8, I'll talk a bit more about that, what the advantage there is. But mainly is if I go from FP16,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1471" target="_blank">00:24:31.720</a></span> | <span class="t">FP8 is this. Half my memory. Almost the same accuracy. And so we measure the accuracy and we publish the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1478" target="_blank">00:24:38.200</a></span> | <span class="t">accuracy. So now I have this much more space for tokens. But more importantly, this model is that much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1483" target="_blank">00:24:43.480</a></span> | <span class="t">faster. Okay. So I want you to understand where the sort of industry is going. This is why Hopper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1488" target="_blank">00:24:48.600</a></span> | <span class="t">the world ate Hopper for breakfast and lunch and dinner because of FP8. It gave folks that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1494" target="_blank">00:24:54.120</a></span> | <span class="t">cost benefit to do this thing a lot faster. Okay. In-flight batching, it just means I don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1500" target="_blank">00:25:00.840</a></span> | <span class="t">to wait for all the requests to finish to start a new request. The moment your request finishes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1505" target="_blank">00:25:05.640</a></span> | <span class="t">I can inject a new request while others are going. Okay. Tons of features here. I put the features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1511" target="_blank">00:25:11.720</a></span> | <span class="t">So some ones to focus on are quantized KV cache. So I can actually represent my KV cache in different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1519" target="_blank">00:25:19.240</a></span> | <span class="t">precision. So that means I'm actively shrinking that memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1523" target="_blank">00:25:23.720</a></span> | <span class="t">making it more performant. You have page KV cache. That's just you managing your GPUs a lot better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1529" target="_blank">00:25:29.240</a></span> | <span class="t">in terms of all of that memory. So there are tons of things you can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1532" target="_blank">00:25:32.600</a></span> | <span class="t">Tenser parallelism. The thing to remember about tensor parallelism, if you want to increase latency,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1537" target="_blank">00:25:37.720</a></span> | <span class="t">use tensor parallelism. Split the model up across multiple GPUs. That's typically done within a node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1544" target="_blank">00:25:44.440</a></span> | <span class="t">I repeat that. That's typically done within a node. You don't like to do tensor parallelism across a node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1549" target="_blank">00:25:49.960</a></span> | <span class="t">You'll see pipeline parallelism go across a node. Pipeline parallelism is more sequential,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1554" target="_blank">00:25:54.760</a></span> | <span class="t">so I process this chunk. So in a multi-node model, like huge models, this box will finish and pass off to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1561" target="_blank">00:26:01.960</a></span> | <span class="t">the next box. But most folks will typically just work -- most models will work within a single node.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1567" target="_blank">00:26:07.320</a></span> | <span class="t">So those are some of the things. In terms of models that you have access to, we optimize those models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1582" target="_blank">00:26:22.280</a></span> | <span class="t">and we give you a lot of scripts where you can go do that on your own, or you can sort of take our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1586" target="_blank">00:26:26.280</a></span> | <span class="t">software and take an easy path. Either way, we support you. So here are some of the models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1591" target="_blank">00:26:31.480</a></span> | <span class="t">are there. All of the LAMAs, Mixtrel, Mixtrels, we work with all those teams behind the scenes. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1596" target="_blank">00:26:36.440</a></span> | <span class="t">typically before any foundation model comes out, we work with those teams to get them deployed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1601" target="_blank">00:26:41.480</a></span> | <span class="t">Okay, what does it mean for TensorRT? So you might have seen TensorRT before, which was a deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1608" target="_blank">00:26:48.200</a></span> | <span class="t">compilation package for NVIDIA GPUs. Lots of folks in computer vision, et cetera, et cetera, have used that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1613" target="_blank">00:26:53.720</a></span> | <span class="t">We took the best practices from there and added all of the extra things that need to happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1618" target="_blank">00:26:58.440</a></span> | <span class="t">in the LLM inference loop. So that's what TRT LLM is really about. So mainly focus on LLM inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1625" target="_blank">00:27:05.720</a></span> | <span class="t">Here's a good visual. An engine that's built to a specific GPU cannot be moved to another GPU. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1632" target="_blank">00:27:12.840</a></span> | <span class="t">always have to compile to that GPU. That's why it's that performant, because we really leverage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1638" target="_blank">00:27:18.440</a></span> | <span class="t">all of the actual hardware on that system to rewrite the algorithm, rewrite that model to that specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1644" target="_blank">00:27:24.680</a></span> | <span class="t">piece of hardware. Okay. TRT LLM and Triton. So TRT LLM will give me an inference engine. I need something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1652" target="_blank">00:27:32.200</a></span> | <span class="t">to host that inference engine and accept requests, batching, et cetera, et cetera. So we have Triton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1657" target="_blank">00:27:37.400</a></span> | <span class="t">Triton works very simply. It's literally a folder where you specify it works on tensor in and tensor out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1663" target="_blank">00:27:43.880</a></span> | <span class="t">So it will tell you what are my inputs coming in and out. And then it will basically understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1668" target="_blank">00:27:48.600</a></span> | <span class="t">how to interpret that file. Or you can host any other different models. That's a thing I do a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1674" target="_blank">00:27:54.120</a></span> | <span class="t">with folks. Just two more slides. This is where the future of inference is going. So a lot of folks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1678" target="_blank">00:27:58.920</a></span> | <span class="t">do FP16 inference today. A lot of folks are moving towards FP8 just because, hey, I now half the model size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1688" target="_blank">00:28:08.040</a></span> | <span class="t">almost twice the speed, more space for tokens. It just makes more sense from a cost perspective. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1693" target="_blank">00:28:13.560</a></span> | <span class="t">why folks like that. And then you saw Blackwell was announced. That's the major innovation. I get FP4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1699" target="_blank">00:28:19.480</a></span> | <span class="t">So that's where things are really going to get interesting. I'll end with NVIDIA inference microservice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1705" target="_blank">00:28:25.080</a></span> | <span class="t">So we've made this thing really easy. We've gone and actually found the best configurations for all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1709" target="_blank">00:28:29.720</a></span> | <span class="t">these models on each piece of GPU. And we're slowly rolling out all of the models because it, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1715" target="_blank">00:28:35.080</a></span> | <span class="t">will just take some time to optimize the world, essentially. And yeah, you can use this to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1720" target="_blank">00:28:40.360</a></span> | <span class="t">download all the slides. I put papers. Tons of other things for you to read. So, yeah. Hopefully,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1726" target="_blank">00:28:46.200</a></span> | <span class="t">your intuition has sharpened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1727" target="_blank">00:28:47.800</a></span> | <span class="t">Shall we just conclude with the, because if someone had a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1736" target="_blank">00:28:56.440</a></span> | <span class="t">Sure. Yeah, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1737" target="_blank">00:28:57.160</a></span> | <span class="t">Where was the question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1738" target="_blank">00:28:58.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1738" target="_blank">00:28:58.840</a></span> | <span class="t">Oh, hang on. I'm going to come over and point my mic at you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1741" target="_blank">00:29:01.640</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1746" target="_blank">00:29:06.440</a></span> | <span class="t">So, hi. Sorry. My question is actually on the heat map that you shared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1751" target="_blank">00:29:11.880</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1751" target="_blank">00:29:11.880</a></span> | <span class="t">Do you mind walking through the heat map and how to interpret it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1755" target="_blank">00:29:15.400</a></span> | <span class="t">Because it was a little small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1756" target="_blank">00:29:16.680</a></span> | <span class="t">Yeah, sorry about that. Yeah. So the heat map,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1758" target="_blank">00:29:18.520</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1758" target="_blank">00:29:18.920</a></span> | <span class="t">All I'm looking at is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1760" target="_blank">00:29:20.040</a></span> | <span class="t">um, so when you go to build an engine, you build an engine to the max input sequence length and the max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1766" target="_blank">00:29:26.200</a></span> | <span class="t">output sequence length. So we actually change how that matrix math is happening under the hood based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1772" target="_blank">00:29:32.120</a></span> | <span class="t">those settings. So you might say, all right, my users are only going to send 4,000 tokens. But in reality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1778" target="_blank">00:29:38.680</a></span> | <span class="t">they might have been sending 1,300 over the past week that you measured. So now you can stay with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1783" target="_blank">00:29:43.720</a></span> | <span class="t">statistical certainty that, hey, the majority of people that were serving, um, during this time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1791" target="_blank">00:29:51.000</a></span> | <span class="t">these were the querying patterns. So I can rebuild an engine for that period of time. What gets super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1795" target="_blank">00:29:55.960</a></span> | <span class="t">interesting, this is a topic I'm very interested in, is seasonal engines. So during the day, you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1800" target="_blank">00:30:00.920</a></span> | <span class="t">different querying patterns. So you'll scale down, you'll scale up. And so you might have different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1805" target="_blank">00:30:05.880</a></span> | <span class="t">engines built for different types of querying patterns based on traffic and stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1810" target="_blank">00:30:10.520</a></span> | <span class="t">So hopefully that may have answered the question, yeah. But it's just saying, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1815" target="_blank">00:30:15.560</a></span> | <span class="t">looking at the bounds of what's the minimum number of tokens that came in, the max, uh, min, min out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1821" target="_blank">00:30:21.240</a></span> | <span class="t">and max out, and just looking at that over the entire distribution. Yes, sir?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1825" target="_blank">00:30:25.800</a></span> | <span class="t">Oh, well, yeah, right there. When it comes to those, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1831" target="_blank">00:30:31.240</a></span> | <span class="t">inference strategies you talked about, like Lilo and Liso, um, how do you, what kind of strategies do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1835" target="_blank">00:30:35.880</a></span> | <span class="t">you have to manage? Like which ones are used at, like, because obviously each session is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1840" target="_blank">00:30:40.920</a></span> | <span class="t">pretty generic. You don't know which one to use at first. Correct. Um, do you split those between GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1845" target="_blank">00:30:45.320</a></span> | <span class="t">or do you stick with one and does it switch between...? So typically we'll, we'll go to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1849" target="_blank">00:30:49.240</a></span> | <span class="t">you try to find what's one configuration that will manage the, the plethora of types of requests that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1855" target="_blank">00:30:55.480</a></span> | <span class="t">you have coming in. So we, we're typically at a, a one engine per all the different querying types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1861" target="_blank">00:31:01.400</a></span> | <span class="t">And I think you'll start seeing, I'm giving you a little bit of future ways to think about it on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1865" target="_blank">00:31:05.720</a></span> | <span class="t">DevOps side because that's something you'll have to test, right? If I look at this querying pattern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1871" target="_blank">00:31:11.320</a></span> | <span class="t">that came into my system with this engine, if I switch the engine, does it still satisfy the querying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1876" target="_blank">00:31:16.440</a></span> | <span class="t">pattern? And how much cost does it save? How much faster is it? So that's more of a, an engineering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1881" target="_blank">00:31:21.720</a></span> | <span class="t">exercise that you'll have to deploy. Sorry, I, I didn't have a...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1884" target="_blank">00:31:24.120</a></span> | <span class="t">Yeah, yeah, so I, I just, I'm very interested in the seasonal side just because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1888" target="_blank">00:31:28.920</a></span> | <span class="t">okay, querying patterns will change. Um, especially when agents come, it'll just be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1893" target="_blank">00:31:33.880</a></span> | <span class="t">that's going to get super interesting when agents are just throwing stuff. Yes, sir?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1897" target="_blank">00:31:37.720</a></span> | <span class="t">Um, so question about how you measure quality of attention. Um, is it, is it correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1903" target="_blank">00:31:43.720</a></span> | <span class="t">intuition to think that attention is a fundamentally scarce resource in the sense of, it's about paying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1908" target="_blank">00:31:48.440</a></span> | <span class="t">attention to one thing at the expense of other contexts? So then how can you, like, scale attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1914" target="_blank">00:31:54.840</a></span> | <span class="t">mechanisms infinitely the way we can conduct? Yeah, so, so what people do in order to scale the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1921" target="_blank">00:32:01.560</a></span> | <span class="t">mechanism is, here's another interesting fact that, um, why folks don't train huge context models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1927" target="_blank">00:32:07.720</a></span> | <span class="t">because it's actually, now you've seen, the bigger my, uh, prompt, the more memory I need. So imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1933" target="_blank">00:32:13.400</a></span> | <span class="t">what that does to a huge, I don't know, 10,000, 100,000 GPU deployment. It might make it a million GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1940" target="_blank">00:32:20.360</a></span> | <span class="t">just to do that context length. So people will train the small context length and then interpolate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1946" target="_blank">00:32:26.040</a></span> | <span class="t">in that value to, to give you that length of context length and then you're sort of bound to what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1950" target="_blank">00:32:30.760</a></span> | <span class="t">attention mechanism you were using. Designed there, there's things like flash attention that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1955" target="_blank">00:32:35.320</a></span> | <span class="t">just do everything in the L1 cache really, really fast. So it depends on the speed of some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1961" target="_blank">00:32:41.000</a></span> | <span class="t">different, it also depends on the GPU as well. So that's why, um, if you look at Blackwell that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1966" target="_blank">00:32:46.120</a></span> | <span class="t">announced by Jensen, they literally have connected, I think, 72 different GPUs on one NVLink. So NVLink</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1974" target="_blank">00:32:54.040</a></span> | <span class="t">connects GPUs together, that's how we can move data insanely fast, and now we've connected like 72 GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1979" target="_blank">00:32:59.560</a></span> | <span class="t">on one. That's, that's just to show you, um, like mixture of experts trying to compute attention across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1985" target="_blank">00:33:05.320</a></span> | <span class="t">all of these different things. But that's actually a really good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1990" target="_blank">00:33:10.840</a></span> | <span class="t">No, I don't necessarily think so. Like the entire industry is, you know, going after that problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=1996" target="_blank">00:33:16.360</a></span> | <span class="t">That's why everybody wants to maybe see something other than attention and, ah, you know, there's so much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9tvJ_GYJA-o&t=2001" target="_blank">00:33:21.480</a></span> | <span class="t">excitement there. Yeah, unfortunately you don't have to call Tom on us now, but that's been fantastic. Thank you, Mo.</span></div></div></body></html>