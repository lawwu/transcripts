<html><head><title>What Is a Humanoid Foundation Model? An Introduction to GR00T N1 - Annika & Aastha</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>What Is a Humanoid Foundation Model? An Introduction to GR00T N1 - Annika & Aastha</h2><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50"><img src="https://i.ytimg.com/vi_webp/mWKYvT9Lc50/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./mWKYvT9Lc50.html">Whisper Transcript</a> | <a href="./transcript_mWKYvT9Lc50.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone, I'm Annika. This is Asta. We both work at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=19" target="_blank">00:00:19.420</a></span> | <span class="t">Advidia and we were part of the team that developed the GRU 10.1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=22" target="_blank">00:00:22.680</a></span> | <span class="t">robotics foundation model. So today we're going to give you a sense of what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=27" target="_blank">00:00:27.360</a></span> | <span class="t">is and how you go about building a robotics foundation model. But before we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=31" target="_blank">00:00:31.920</a></span> | <span class="t">get into it, I feel like a lot of people start talks here with a hot take. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=37" target="_blank">00:00:37.340</a></span> | <span class="t">hot take that I'm bringing to an AI conference is that we're not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=40" target="_blank">00:00:40.860</a></span> | <span class="t">running out of jobs. So this was a report done by McKinsey, part of the Global</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=46" target="_blank">00:00:46.260</a></span> | <span class="t">Institute, showing that in the world's 30 most advanced economies, there's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=52" target="_blank">00:00:52.240</a></span> | <span class="t">too many jobs for the number of people that could fill them. And really the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=57" target="_blank">00:00:57.100</a></span> | <span class="t">things you should look at in this whole graph is the 4.2x that's been the rate at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=61" target="_blank">00:01:01.720</a></span> | <span class="t">which we're getting more jobs and people could fill it over the last decade. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=65" target="_blank">00:01:05.020</a></span> | <span class="t">this line that I'm highlighting in red, that's where we're in trouble, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=68" target="_blank">00:01:08.980</a></span> | <span class="t">there's just more jobs than able-bodied people to fill those jobs. And obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=73" target="_blank">00:01:13.840</a></span> | <span class="t">there's a real conversation around AI and jobs, so it helps to look at what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=78" target="_blank">00:01:18.540</a></span> | <span class="t">industries are largely affected. I'm gonna highlight a couple in red. So leisure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=84" target="_blank">00:01:24.760</a></span> | <span class="t">hospitality, health care, construction, transportation, manufacturing. I guess you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=91" target="_blank">00:01:31.840</a></span> | <span class="t">can figure out what they have in common. None of them can be solved by ChatGPT alone. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=97" target="_blank">00:01:37.840</a></span> | <span class="t">require operating instruments and devices in the physical world, and they require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=103" target="_blank">00:01:43.960</a></span> | <span class="t">physical AI. So that's really the big challenge that I see over the coming years is how do we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=109" target="_blank">00:01:49.200</a></span> | <span class="t">this huge amount of intelligence that we're seeing in language models and make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=114" target="_blank">00:01:54.560</a></span> | <span class="t">operable in the in the physical world. The other question around humanoids is why do we build them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=120" target="_blank">00:02:00.540</a></span> | <span class="t">look like humans? It's not just because we want them to look like us. The world was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=125" target="_blank">00:02:05.520</a></span> | <span class="t">made for humans. It's very hard to have generalist robots operate in our world and be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=131" target="_blank">00:02:11.220</a></span> | <span class="t">generally useful without copying our physical form. There's a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=136" target="_blank">00:02:16.740</a></span> | <span class="t">specialist robots that do incredible things. I don't know if you got to try the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=139" target="_blank">00:02:19.980</a></span> | <span class="t">espresso from the barista robot downstairs makes a good espresso, but that robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=145" target="_blank">00:02:25.020</a></span> | <span class="t">couldn't even cook rice. So if we want a robot that can do multiple tasks for you, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=151" target="_blank">00:02:31.020</a></span> | <span class="t">just a lot easier to try and imagine that that robot can operate in in our human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=156" target="_blank">00:02:36.080</a></span> | <span class="t">world. So how do we do this? There's three big buckets, three big stages. First one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=162" target="_blank">00:02:42.120</a></span> | <span class="t">data collecting, or generating, or multiplying data, which we'll talk quite a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=166" target="_blank">00:02:46.260</a></span> | <span class="t">lot about. And now that you have this synthetic and real data, but largely synthetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=171" target="_blank">00:02:51.900</a></span> | <span class="t">data, you train a model. We'll also talk about what that architecture and training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=177" target="_blank">00:02:57.540</a></span> | <span class="t">paradigm looks like. And then finally we deploy on the robot, or at the edge. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=184" target="_blank">00:03:04.260</a></span> | <span class="t">is what we call the physical AI lifecycle. So generate the data, consume the data, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=190" target="_blank">00:03:10.300</a></span> | <span class="t">then finally deploy and have this robot operable in the physical world. NVIDIA also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=195" target="_blank">00:03:15.920</a></span> | <span class="t">likes to call this the three computer problem, because they have very different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=200" target="_blank">00:03:20.100</a></span> | <span class="t">compute characteristics. So at the simulation stage, you're looking for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=204" target="_blank">00:03:24.320</a></span> | <span class="t">computer that's powerful at simulating, something like an OVX omniverse machine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=208" target="_blank">00:03:28.400</a></span> | <span class="t">There's a lot of really interesting work happening on the simulation side, but it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=212" target="_blank">00:03:32.780</a></span> | <span class="t">has a very different type of workload than when we're training and we're using a DGX to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=218" target="_blank">00:03:38.060</a></span> | <span class="t">just consume this enormous amounts of data and learn from that. And then finally when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=223" target="_blank">00:03:43.340</a></span> | <span class="t">we're deploying at the edge, it needs to be a model that's small enough and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=226" target="_blank">00:03:46.820</a></span> | <span class="t">efficient enough to run on an edge edge device like an AGX. And really this is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=233" target="_blank">00:03:53.960</a></span> | <span class="t">is Project Root. So Project Root is NVIDIA's strategy for bringing humanoid and other forms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=240" target="_blank">00:04:00.360</a></span> | <span class="t">of robotics into the world. And it's everything from the compute infrastructure to the software,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=245" target="_blank">00:04:05.900</a></span> | <span class="t">to the research that's needed. It's not simply just one foundation model. But that is what we'll be focusing on in this talk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=253" target="_blank">00:04:13.540</a></span> | <span class="t">because that's what we worked on. So the Groot N1 Foundation model was announced at GTC in March.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=259" target="_blank">00:04:19.620</a></span> | <span class="t">It is open source. It is highly customizable. And a very big part of it that is cross embodiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=266" target="_blank">00:04:26.700</a></span> | <span class="t">So basically you can take this base model, there's specific embodiments that we have fine-tuned for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=271" target="_blank">00:04:31.620</a></span> | <span class="t">but the whole premise is that you can take this base model, it's a two billion parameter model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=276" target="_blank">00:04:36.340</a></span> | <span class="t">which in the world of LLMs is tiny, but still pretty sizable for a robot, and then go and modify it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=282" target="_blank">00:04:42.820</a></span> | <span class="t">for your embodiments, your use cases. So let's start with the first huge daunting task in the world of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=290" target="_blank">00:04:50.180</a></span> | <span class="t">robotics data. When the Groot team actually started thinking about data, they put together this idea of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=297" target="_blank">00:04:57.620</a></span> | <span class="t">the data pyramid, which is very elegant, but it was born out of desperation and necessity. The data you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=304" target="_blank">00:05:04.180</a></span> | <span class="t">does not exist in quantities. There is no internet scale data set to scrape or download or put together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=311" target="_blank">00:05:11.540</a></span> | <span class="t">because robots haven't made it YouTube yet. So really at the top of the pyramid, we have the real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=317" target="_blank">00:05:17.540</a></span> | <span class="t">world data, which is robots doing things, real robots doing real tasks and solving them. And how it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=323" target="_blank">00:05:23.700</a></span> | <span class="t">collected is humans teleoperate a robot most of the time. So wearing like an Apple Vision Pro and wearing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=331" target="_blank">00:05:31.140</a></span> | <span class="t">gloves, there's all kinds of ways to teleoperate the robot. But you have a real robot successfully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=335" target="_blank">00:05:35.620</a></span> | <span class="t">completing a task, and then you have that ground truth data. So you can imagine this is very small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=341" target="_blank">00:05:41.620</a></span> | <span class="t">in quantity, very expensive. And we put 24 hours per robot per day, because that's how many hours a human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=348" target="_blank">00:05:48.660</a></span> | <span class="t">has. But the reality is that humans and robots get tired. So it's not even 24 hours. So really, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=355" target="_blank">00:05:55.620</a></span> | <span class="t">a very, very limited data set. And then at the bottom of the pyramid, we have the internet. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=360" target="_blank">00:06:00.900</a></span> | <span class="t">have huge amounts of video data, and it's typically human solving tasks. So you can imagine someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=367" target="_blank">00:06:07.620</a></span> | <span class="t">collecting a cooking video tutorial and putting that out there. With this unstructured data, it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=373" target="_blank">00:06:13.620</a></span> | <span class="t">necessarily relevant to robots, but there is some value in that. So we didn't want to completely discard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=380" target="_blank">00:06:20.420</a></span> | <span class="t">it. It forms part of this cohesive data strategy. And then in the middle, synthetic data. And this is a topic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=388" target="_blank">00:06:28.900</a></span> | <span class="t">that could fill this whole entire talk. And I've cut down so many slides on just this section, because in theory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=395" target="_blank">00:06:35.940</a></span> | <span class="t">this is infinite, right? You could just let the GPU keep generating more data. But in practice, creating high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=402" target="_blank">00:06:42.980</a></span> | <span class="t">quality simulation environments is very labor intensive, and it requires serious skill. And then on top of that, the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=411" target="_blank">00:06:51.140</a></span> | <span class="t">technique, which I will share a little bit about is taking the human trajectories that we do collect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=420" target="_blank">00:07:00.100</a></span> | <span class="t">so human teleoperation data, and trying to multiply it through essentially video generation models. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=427" target="_blank">00:07:07.140</a></span> | <span class="t">through World Foundation models that we fine tune to do this task. But even in that case, there's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=432" target="_blank">00:07:12.180</a></span> | <span class="t">of active research in how we take the little bits of high quality data that we have and multiply it as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=438" target="_blank">00:07:18.660</a></span> | <span class="t">as how we effectively combine a simulation data with this real world data. So this is DreamGen. This was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=445" target="_blank">00:07:25.780</a></span> | <span class="t">something that was announced at Computex very recently. All in all, this data piece is a huge part of what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=453" target="_blank">00:07:33.220</a></span> | <span class="t">project root is about. So there's many, many solutions here in terms of the tele-op and the data strategy. But for now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=461" target="_blank">00:07:41.220</a></span> | <span class="t">the next piece is how do we bring all this data into an architecture? So I'm going to hand over to Asda to explain that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=467" target="_blank">00:07:47.300</a></span> | <span class="t">about. Thank you, Anika. Do you guys hear me? All right, awesome. So before we dive into the architecture, I'm going to show to you what an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=479" target="_blank">00:07:59.220</a></span> | <span class="t">example input looks like and what an example output looks like. So what you see here is the image observation, the robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=486" target="_blank">00:08:06.180</a></span> | <span class="t">state and the language prompt. That's the input. And then what's the output? The output is a robot action trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=493" target="_blank">00:08:13.380</a></span> | <span class="t">So the prompt was to pick up the industrial object and place it in the yellow bin. And that's what the robot does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=499" target="_blank">00:08:19.940</a></span> | <span class="t">It picks up and places it in the yellow bin very neatly. But this is what it appears to us as humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=506" target="_blank">00:08:26.740</a></span> | <span class="t">But is the robot or the humanoid seeing the same? Not really. The humanoid sees this. It sees a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=515" target="_blank">00:08:35.860</a></span> | <span class="t">vectors, floating point vectors, which control the different joints. So you're seeing the output as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=521" target="_blank">00:08:41.300</a></span> | <span class="t">trajectory, which is like motion of the robot hand, but that's not what the robot is seeing. It uses these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=528" target="_blank">00:08:48.020</a></span> | <span class="t">vectors to actually generate a continuous action. And to set context on what a robot state and action is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=535" target="_blank">00:08:55.380</a></span> | <span class="t">So you can imagine the state is the robot snapshot at an instant instance of time. So including the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=542" target="_blank">00:09:02.660</a></span> | <span class="t">physique of the robot and the environment, that's the state. And then the action is what the robot decides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=548" target="_blank">00:09:08.100</a></span> | <span class="t">to do next based on the state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=552" target="_blank">00:09:12.260</a></span> | <span class="t">So moving on and diving a bit deeper into the architecture. The Groot N1 system introduced a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=561" target="_blank">00:09:21.060</a></span> | <span class="t">very interesting concept. And this concept is inspired from Daniel Kahneman's book, Thinking Fast and Slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=566" target="_blank">00:09:26.980</a></span> | <span class="t">Show of hands, how many of you have read the book? Amazing. That helps to explain. So it's inspired by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=574" target="_blank">00:09:34.100</a></span> | <span class="t">the same concept, but it's applied to a robotics context. So we have two systems, system one, system two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=582" target="_blank">00:09:42.180</a></span> | <span class="t">System two, you can imagine, is the brain of the robot or the brain of the model. So that's the part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=588" target="_blank">00:09:48.820</a></span> | <span class="t">which is actually trying to break down the complex tasks. So make it simpler such that the system one can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=596" target="_blank">00:09:56.420</a></span> | <span class="t">execute on it. So you can think of system two as the planner, which executes slowly to break down the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=602" target="_blank">00:10:02.260</a></span> | <span class="t">complex task. And then system one is the first one. It operates almost at 120 hertz and it basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=609" target="_blank">00:10:09.060</a></span> | <span class="t">executes on the task that system two puts out for it. And then now we're going to delve another level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=616" target="_blank">00:10:16.180</a></span> | <span class="t">deeper into the architecture. And it's okay if all of this is complicated to you because it's not very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=621" target="_blank">00:10:21.620</a></span> | <span class="t">straightforward. So we have the input as the robot state and the noised action. You must be wondering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=628" target="_blank">00:10:28.580</a></span> | <span class="t">why we've called it noised action. Noised action is a natural state because these sensors don't capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=634" target="_blank">00:10:34.260</a></span> | <span class="t">the action perfectly. So we have noised action. And then they're passed to a state encoder and an action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=640" target="_blank">00:10:40.980</a></span> | <span class="t">encoder which generates some tokens. And you may be familiar with tokens. We've talked about LLMs, agents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=649" target="_blank">00:10:49.060</a></span> | <span class="t">a lot. So the same concept but just different kinds of tokens. So state tokens, action tokens. And then it's passed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=656" target="_blank">00:10:56.020</a></span> | <span class="t">through a diffusion transformer block. And the diffusion transformer block is essentially multiple layers of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=663" target="_blank">00:11:03.220</a></span> | <span class="t">cross-attention and self-attention. And bringing in the other piece, which is the vision input and the text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=670" target="_blank">00:11:10.740</a></span> | <span class="t">input. So you have the vision encoder which takes the image input, generates some tokens, passes it to the VLM to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=677" target="_blank">00:11:17.780</a></span> | <span class="t">bring it to like a standardized encoding format. And then the text tokenizer, which takes the text input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=683" target="_blank">00:11:23.780</a></span> | <span class="t">again, does the same passes through the VLM. And then all of this, all of the output tokens from the VLM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=689" target="_blank">00:11:29.860</a></span> | <span class="t">in this case, in case of root N1, it was the eagle to VLM, is passed into the cross-attention layer of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=698" target="_blank">00:11:38.740</a></span> | <span class="t">diffusion transformer block. And then you get some output tokens. These are the output tokens. But these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=706" target="_blank">00:11:46.660</a></span> | <span class="t">output tokens are still not ready to be consumed by the physical robot. So you need to make it consumable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=713" target="_blank">00:11:53.620</a></span> | <span class="t">by the physical robot. And that's where you have this key piece called the action decoder. So it may seem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=720" target="_blank">00:12:00.660</a></span> | <span class="t">like there's lots of encoders, lots of decoders, but you can say that the action decoder is the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=726" target="_blank">00:12:06.180</a></span> | <span class="t">which gives the model capability to be a generalist. So you're giving it an action decoder which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=731" target="_blank">00:12:11.860</a></span> | <span class="t">specific to the embodiment that you're going to use. Whether it's a humanoid hand or a robot arm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=737" target="_blank">00:12:17.460</a></span> | <span class="t">an industrial robot arm, that's where that action decoder comes into place. It's specific to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=742" target="_blank">00:12:22.740</a></span> | <span class="t">embodiment you're trying to use. And then it's going to translate it specific to your embodiment and output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=748" target="_blank">00:12:28.820</a></span> | <span class="t">an action vector which can be translated into continuous robot motion or embodiment motion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=754" target="_blank">00:12:34.580</a></span> | <span class="t">Just going to give you a second to digest all of this. So you can see that the action decoder is very,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=761" target="_blank">00:12:41.460</a></span> | <span class="t">very important because otherwise you would only be able to train a model for one specific embodiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=767" target="_blank">00:12:47.540</a></span> | <span class="t">But this model can leverage foundation knowledge from all different embodiments and then bring it to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=773" target="_blank">00:12:53.780</a></span> | <span class="t">one particular embodiment. The concept is similar to the concept of a foundation model essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=781" target="_blank">00:13:01.060</a></span> | <span class="t">Moving on to the next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=784" target="_blank">00:13:04.340</a></span> | <span class="t">There are two main ways of robot learning. And the reason I chose to keep this slide is because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=792" target="_blank">00:13:12.580</a></span> | <span class="t">came up a lot in the conversations I was having the past couple of days. There are two ways of training robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=799" target="_blank">00:13:19.460</a></span> | <span class="t">One is imitation learning and the other is through reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=804" target="_blank">00:13:24.500</a></span> | <span class="t">Imitation learning in simple English terms, imitation means to copy someone like learn by copying. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=813" target="_blank">00:13:33.300</a></span> | <span class="t">exactly what's happening here. So you have a human expert and the robot is trying to copy the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=819" target="_blank">00:13:39.300</a></span> | <span class="t">expert. And you're trying to minimize the loss between the expert and the human. So you have a gold standard. You're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=825" target="_blank">00:13:45.940</a></span> | <span class="t">trying to match up to the gold standard. And then in case of reinforcement learning it's more of a trial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=830" target="_blank">00:13:50.740</a></span> | <span class="t">and error format. So what you're doing is you just maximize the reward. So you don't have a golden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=837" target="_blank">00:13:57.860</a></span> | <span class="t">state. You're trying to just reach wherever you can the best you can. You can think of it similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=844" target="_blank">00:14:04.020</a></span> | <span class="t">having siblings. When siblings, parents try to compare between the two and they're like you need to be like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=848" target="_blank">00:14:08.980</a></span> | <span class="t">your elder sibling. But then there's no sibling and in that case you can just be as good as you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=853" target="_blank">00:14:13.940</a></span> | <span class="t">So that's reinforcement learning for you. And like all things good and bad in this world both of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=860" target="_blank">00:14:20.740</a></span> | <span class="t">come with pros and cons. With the imitation learning you're severely bottlenecked by the expert data which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=867" target="_blank">00:14:27.060</a></span> | <span class="t">quite expensive. But in case of reinforcement learning you don't have that bottleneck. But it's the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=873" target="_blank">00:14:33.540</a></span> | <span class="t">challenge is the sim to real. So there's a huge gap between going from sim to real and it's an active</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=879" target="_blank">00:14:39.220</a></span> | <span class="t">area of research. A lot of research labs, universities are going behind it. So that was the two ways of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=886" target="_blank">00:14:46.820</a></span> | <span class="t">training robots. And GrootN1 used both of these in some ways. Here's an example of the train model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=895" target="_blank">00:14:55.540</a></span> | <span class="t">What can it do? So on the left you see the model being able to do a few pick and place tasks in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=901" target="_blank">00:15:01.060</a></span> | <span class="t">kitchen. On the right top you can see with enough training the model can be taught how to be romantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=907" target="_blank">00:15:07.140</a></span> | <span class="t">as well. You don't see all the fallen champagne glasses and fallen flowers which went behind capturing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=914" target="_blank">00:15:14.180</a></span> | <span class="t">this perfect snap. And then the bottom right is two robot friends trying to get to an industrial task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=921" target="_blank">00:15:21.940</a></span> | <span class="t">like a pick and place task again. But these are not the only tasks that these humanoids or robots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=928" target="_blank">00:15:28.420</a></span> | <span class="t">can be doing. They can be extended to any task, any environment. And that's why we have a foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=935" target="_blank">00:15:35.780</a></span> | <span class="t">model. A generalist foundation model which can be expanded to any downstream task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=942" target="_blank">00:15:42.500</a></span> | <span class="t">So this is going to be my conclusion. There are three core principles that we spoke about today. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=950" target="_blank">00:15:50.500</a></span> | <span class="t">each of these is very hefty by itself. But primarily the data pyramid. Annika spoke about this. In case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=958" target="_blank">00:15:58.980</a></span> | <span class="t">LLMs or text data or text models, you have the whole internet which you can be scraping to generate data. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=966" target="_blank">00:16:06.900</a></span> | <span class="t">there's no such internet scale data for actions. So that is one of the key challenges that you need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=972" target="_blank">00:16:12.340</a></span> | <span class="t">address either via simulation or by imitation learning, generating expert data, tele-operation, all sorts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=978" target="_blank">00:16:18.980</a></span> | <span class="t">things. The next thing is the dual system architecture. Previously what used to happen was each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=986" target="_blank">00:16:26.260</a></span> | <span class="t">components was trained independently. And that resulted in some kind of disagreement between the two systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=993" target="_blank">00:16:33.300</a></span> | <span class="t">The Groot N1 introduces this coherent architecture where both the system one and system two are being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1001" target="_blank">00:16:41.060</a></span> | <span class="t">co-trained. And that kind of helps to optimize the whole stack instead of individually trying to train the pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1010" target="_blank">00:16:50.020</a></span> | <span class="t">And then the third piece and the final piece is the generalist model. So in case of the generalist model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1017" target="_blank">00:16:57.140</a></span> | <span class="t">you are able to leverage foundation knowledge from the model and extend it to different embodiments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1024" target="_blank">00:17:04.500</a></span> | <span class="t">different tasks. You can think of it like how you have, in case of large language models, you have a base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1030" target="_blank">00:17:10.660</a></span> | <span class="t">foundation Lama 2.7TB model or there's Lama 4 now or Lama 3. I don't know which is the latest, but you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1038" target="_blank">00:17:18.340</a></span> | <span class="t">extend it to any, you can fine tune it to any task or like domain adapted. Similarly, you have the Groot N1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1045" target="_blank">00:17:25.540</a></span> | <span class="t">model which can be adapted to any embodiment and any downstream task. Thank you so much for attending our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1051" target="_blank">00:17:31.620</a></span> | <span class="t">talk today. We're really happy you were here. Please let us know if you have questions. We'll be outside hanging out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=mWKYvT9Lc50&t=1058" target="_blank">00:17:38.420</a></span> | <span class="t">Thank you so much. We appreciate it.</span></div></div></body></html>