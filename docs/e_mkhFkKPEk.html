<html><head><title>[Paper Club] Upcycling Large Language Models into Mixture of Experts</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>[Paper Club] Upcycling Large Language Models into Mixture of Experts</h2><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk"><img src="https://i.ytimg.com/vi/e_mkhFkKPEk/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./e_mkhFkKPEk.html">Whisper Transcript</a> | <a href="./transcript_e_mkhFkKPEk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">>> My inter works too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=4" target="_blank">00:00:04.440</a></span> | <span class="t">>> Okay, cool. Today, I'm going to present a mixture of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=10" target="_blank">00:00:10.960</a></span> | <span class="t">I'm Ethan from NVIDIA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=13" target="_blank">00:00:13.160</a></span> | <span class="t">I work on scaling, LLMs, transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=18" target="_blank">00:00:18.340</a></span> | <span class="t">I might sound a bit muffled because I'm having a cold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=22" target="_blank">00:00:22.560</a></span> | <span class="t">Apologies for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=25" target="_blank">00:00:25.000</a></span> | <span class="t">To this topic, I'm going to give a brief introduction on mix of experts or MOE first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=33" target="_blank">00:00:33.740</a></span> | <span class="t">I'm going to talk about Megatron Core MOE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=37" target="_blank">00:00:37.280</a></span> | <span class="t">like how we accelerate these MOEs and train and do inference efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=44" target="_blank">00:00:44.400</a></span> | <span class="t">Finally, going to talk about the upcycling LLM into MOE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=49" target="_blank">00:00:49.640</a></span> | <span class="t">which is our recent paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=54" target="_blank">00:00:54.440</a></span> | <span class="t">So the AI models are growing larger and larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=59" target="_blank">00:00:59.600</a></span> | <span class="t">This is a rather old picture from 2021.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=63" target="_blank">00:01:03.720</a></span> | <span class="t">Switch transformer is the first model that surpassed the one trillion model size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=69" target="_blank">00:01:09.900</a></span> | <span class="t">Before that, it's only hundreds of billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=74" target="_blank">00:01:14.520</a></span> | <span class="t">I think at that time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=76" target="_blank">00:01:16.840</a></span> | <span class="t">it's growing 10 times each year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=79" target="_blank">00:01:19.840</a></span> | <span class="t">It looks like it's slowing down recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=83" target="_blank">00:01:23.760</a></span> | <span class="t">The question is, we only have so much compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=90" target="_blank">00:01:30.480</a></span> | <span class="t">how can we make the model better without increasing the compute?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=95" target="_blank">00:01:35.380</a></span> | <span class="t">From Noam, he said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=98" target="_blank">00:01:38.400</a></span> | <span class="t">"My unsubstantiated theory is that parameters are good for knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=104" target="_blank">00:01:44.640</a></span> | <span class="t">and compute or the flop is good for intelligence."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=109" target="_blank">00:01:49.440</a></span> | <span class="t">Whatever those term means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=111" target="_blank">00:01:51.960</a></span> | <span class="t">So MOE is a good way of growing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=115" target="_blank">00:01:55.840</a></span> | <span class="t">the parameters or growing knowledge without increasing the compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">So what is MOE?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=125" target="_blank">00:02:05.640</a></span> | <span class="t">Here is a very simple diagram from switch transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=131" target="_blank">00:02:11.520</a></span> | <span class="t">For the traditional LLMs transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=136" target="_blank">00:02:16.040</a></span> | <span class="t">you have the self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=138" target="_blank">00:02:18.280</a></span> | <span class="t">the residual layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=141" target="_blank">00:02:21.800</a></span> | <span class="t">and then you have a FFN layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=145" target="_blank">00:02:25.280</a></span> | <span class="t">which is just a two-layer layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=147" target="_blank">00:02:27.200</a></span> | <span class="t">and then the residual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=149" target="_blank">00:02:29.800</a></span> | <span class="t">MOEs transform the FFN layer into multiple copy of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=156" target="_blank">00:02:36.680</a></span> | <span class="t">You see here, there are four FFN layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=161" target="_blank">00:02:41.400</a></span> | <span class="t">and then each token selectively activate a few experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=166" target="_blank">00:02:46.040</a></span> | <span class="t">which is selected by router.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=168" target="_blank">00:02:48.680</a></span> | <span class="t">The router is simply a matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=172" target="_blank">00:02:52.160</a></span> | <span class="t">a learnable matrix to select one of the expert based on the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=179" target="_blank">00:02:59.360</a></span> | <span class="t">The model size increase enhancing its capability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=183" target="_blank">00:03:03.400</a></span> | <span class="t">while the compute roughly remains the same as the original model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=188" target="_blank">00:03:08.880</a></span> | <span class="t">If we look into the MOE layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=194" target="_blank">00:03:14.320</a></span> | <span class="t">it actually consists of several steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=197" target="_blank">00:03:17.920</a></span> | <span class="t">It's more complicated than the original FFN layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=203" target="_blank">00:03:23.560</a></span> | <span class="t">You can think of the original FFN layer as the third step computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=210" target="_blank">00:03:30.080</a></span> | <span class="t">whereas the expert layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=213" target="_blank">00:03:33.840</a></span> | <span class="t">which was the original FFN layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=216" target="_blank">00:03:36.320</a></span> | <span class="t">is applied to the input token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=220" target="_blank">00:03:40.320</a></span> | <span class="t">The first step here is routing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=224" target="_blank">00:03:44.920</a></span> | <span class="t">Given the input token, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=227" target="_blank">00:03:47.680</a></span> | <span class="t">here you have like six tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=230" target="_blank">00:03:50.840</a></span> | <span class="t">the quick brown fox jumped over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=235" target="_blank">00:03:55.240</a></span> | <span class="t">It went over the router.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=238" target="_blank">00:03:58.120</a></span> | <span class="t">Router is simply a matrix that is applied on these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=242" target="_blank">00:04:02.720</a></span> | <span class="t">It generates the probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=245" target="_blank">00:04:05.280</a></span> | <span class="t">and we will take the highest probability as the router selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=253" target="_blank">00:04:13.680</a></span> | <span class="t">Here is the expert indices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=256" target="_blank">00:04:16.400</a></span> | <span class="t">are the experts which have the highest probability on these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=263" target="_blank">00:04:23.240</a></span> | <span class="t">The second step is permutation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=266" target="_blank">00:04:26.560</a></span> | <span class="t">Given the token selected experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=271" target="_blank">00:04:31.560</a></span> | <span class="t">you need to align these input features with the experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=277" target="_blank">00:04:37.200</a></span> | <span class="t">All of the tokens that select expert zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=282" target="_blank">00:04:42.120</a></span> | <span class="t">you need to arrange them into a single matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=287" target="_blank">00:04:47.600</a></span> | <span class="t">that are only for experts zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=291" target="_blank">00:04:51.800</a></span> | <span class="t">expert one, expert two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=294" target="_blank">00:04:54.840</a></span> | <span class="t">Depending on the capacity factor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=297" target="_blank">00:04:57.760</a></span> | <span class="t">some of the token might be dropped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">The third step is the same as the original FFN layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=304" target="_blank">00:05:04.720</a></span> | <span class="t">where you do the computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=307" target="_blank">00:05:07.640</a></span> | <span class="t">After the computation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=309" target="_blank">00:05:09.440</a></span> | <span class="t">there will be an on-premier step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=314" target="_blank">00:05:14.080</a></span> | <span class="t">where you need to arrange these tokens back to the original shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=319" target="_blank">00:05:19.040</a></span> | <span class="t">Then the router probability is applied as a scaling factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=325" target="_blank">00:05:25.000</a></span> | <span class="t">on all of these expert features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=332" target="_blank">00:05:32.520</a></span> | <span class="t">Scaling this MOE during training is very challenging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=342" target="_blank">00:05:42.000</a></span> | <span class="t">First, the models are massive scales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=346" target="_blank">00:05:46.080</a></span> | <span class="t">Usually, for example, mixed row 8x7b,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=349" target="_blank">00:05:49.400</a></span> | <span class="t">you increase the model parameters roughly by 6x or 7x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=355" target="_blank">00:05:55.880</a></span> | <span class="t">This puts substantial pressure on the memory usage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=361" target="_blank">00:06:01.040</a></span> | <span class="t">And the router dispatching also has overhead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=364" target="_blank">00:06:04.920</a></span> | <span class="t">In the previous slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=367" target="_blank">00:06:07.560</a></span> | <span class="t">you will notice there's a permute on on-permute operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=371" target="_blank">00:06:11.680</a></span> | <span class="t">That essentially increases the activation memory by two times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=378" target="_blank">00:06:18.800</a></span> | <span class="t">because all of those need to be stored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=381" target="_blank">00:06:21.200</a></span> | <span class="t">If you have top-k routing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=383" target="_blank">00:06:23.840</a></span> | <span class="t">it would also increase the memory of the activation by k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=388" target="_blank">00:06:28.200</a></span> | <span class="t">because the hidden states need to go to each expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=393" target="_blank">00:06:33.400</a></span> | <span class="t">and you need to duplicate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=397" target="_blank">00:06:37.360</a></span> | <span class="t">And also reduce the jam efficiency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=399" target="_blank">00:06:39.800</a></span> | <span class="t">because you need to do a loop over all the experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=402" target="_blank">00:06:42.760</a></span> | <span class="t">to do the jam separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=406" target="_blank">00:06:46.480</a></span> | <span class="t">There's also an imbalance issue if all of the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=410" target="_blank">00:06:50.440</a></span> | <span class="t">go to one expert,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=411" target="_blank">00:06:51.840</a></span> | <span class="t">other experts on other GPU would be idle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=418" target="_blank">00:06:58.160</a></span> | <span class="t">Now, let me introduce the Megatron Core MOE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=424" target="_blank">00:07:04.760</a></span> | <span class="t">which is how we accelerate these MOE models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=428" target="_blank">00:07:08.480</a></span> | <span class="t">given these challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=433" target="_blank">00:07:13.240</a></span> | <span class="t">So Megatron LLAM and Megatron Core</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=436" target="_blank">00:07:16.520</a></span> | <span class="t">is an open-source library on GitHub available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=441" target="_blank">00:07:21.360</a></span> | <span class="t">We accelerate not only MOE and also all of the LLAMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=448" target="_blank">00:07:28.240</a></span> | <span class="t">including like GPT, BERT, T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=451" target="_blank">00:07:31.880</a></span> | <span class="t">I'm not sure if anyone is still using those now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=455" target="_blank">00:07:35.080</a></span> | <span class="t">but primarily GPT models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=457" target="_blank">00:07:37.360</a></span> | <span class="t">And inside the transformer layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=461" target="_blank">00:07:41.440</a></span> | <span class="t">the attention is accelerated with all kinds of parallelism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=466" target="_blank">00:07:46.680</a></span> | <span class="t">including pipeline parallel, tensor parallel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=470" target="_blank">00:07:50.760</a></span> | <span class="t">also the parallelism and the MOEs are also accelerated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=475" target="_blank">00:07:55.920</a></span> | <span class="t">This is what we are primarily talking about today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=478" target="_blank">00:07:58.560</a></span> | <span class="t">Megatron Core MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=481" target="_blank">00:08:01.800</a></span> | <span class="t">On top, you have two customizable training loops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=486" target="_blank">00:08:06.640</a></span> | <span class="t">Megatron LLAM provides simple bare-bone training loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=490" target="_blank">00:08:10.520</a></span> | <span class="t">And you can easily hack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=493" target="_blank">00:08:13.200</a></span> | <span class="t">And NEMO provide a high-level interface</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=497" target="_blank">00:08:17.080</a></span> | <span class="t">where you can just provide Pythonic configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=500" target="_blank">00:08:20.520</a></span> | <span class="t">to train these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=501" target="_blank">00:08:21.560</a></span> | <span class="t">In the Megatron Core MOE, we provide different approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=509" target="_blank">00:08:29.960</a></span> | <span class="t">to accelerate these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=512" target="_blank">00:08:32.240</a></span> | <span class="t">For the router, there are Oxloss and Sinkhorn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=516" target="_blank">00:08:36.760</a></span> | <span class="t">Basically, Oxloss is a token trace MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=522" target="_blank">00:08:42.160</a></span> | <span class="t">And Sinkhorn, without token dropping,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=525" target="_blank">00:08:45.600</a></span> | <span class="t">it can usually use in expert trace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=530" target="_blank">00:08:50.120</a></span> | <span class="t">And the tokens dispatcher, they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=533" target="_blank">00:08:53.760</a></span> | <span class="t">in permute, unpermute for efficient memory saving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=537" target="_blank">00:08:57.760</a></span> | <span class="t">And for the expert, we have grouped MLP to accelerate this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=545" target="_blank">00:09:05.720</a></span> | <span class="t">So first, it's expert model parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=550" target="_blank">00:09:10.360</a></span> | <span class="t">This is available in Megatron Core MOE now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=558" target="_blank">00:09:18.240</a></span> | <span class="t">You can-- usually, you would put all of the experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=562" target="_blank">00:09:22.720</a></span> | <span class="t">on one single GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=564" target="_blank">00:09:24.480</a></span> | <span class="t">And then you do a for loop over all of the experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=567" target="_blank">00:09:27.840</a></span> | <span class="t">to compute the result. But instead, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=570" target="_blank">00:09:30.920</a></span> | <span class="t">can put one expert on each GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=575" target="_blank">00:09:35.360</a></span> | <span class="t">This will release a lot of memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=577" target="_blank">00:09:37.840</a></span> | <span class="t">and also accelerate the training on the inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=582" target="_blank">00:09:42.080</a></span> | <span class="t">So token dropping, the default we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=589" target="_blank">00:09:49.880</a></span> | <span class="t">is dropless, meaning all of the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=594" target="_blank">00:09:54.160</a></span> | <span class="t">can go to one expert and no tokens are dropped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=600" target="_blank">00:10:00.280</a></span> | <span class="t">We also support token dropping with padding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=605" target="_blank">00:10:05.960</a></span> | <span class="t">meaning give a set capacity factor, for example, four here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=611" target="_blank">00:10:11.560</a></span> | <span class="t">Each expert can, at max, accept four tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=615" target="_blank">00:10:15.960</a></span> | <span class="t">Tokens beyond that are going to be dropped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=620" target="_blank">00:10:20.040</a></span> | <span class="t">So accuracy-wise and efficiency-wise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=624" target="_blank">00:10:24.680</a></span> | <span class="t">there are a lot of discussion around here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=629" target="_blank">00:10:29.400</a></span> | <span class="t">A lot of the pre-training experiments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=632" target="_blank">00:10:32.880</a></span> | <span class="t">shows that token dropping is very efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=635" target="_blank">00:10:35.760</a></span> | <span class="t">and it doesn't impact performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=638" target="_blank">00:10:38.480</a></span> | <span class="t">But in some of the downstream fine-tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=642" target="_blank">00:10:42.000</a></span> | <span class="t">people realize dropless is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=644" target="_blank">00:10:44.000</a></span> | <span class="t">Maybe it's because of the domain shifts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=648" target="_blank">00:10:48.920</a></span> | <span class="t">The tokens are no longer balanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=651" target="_blank">00:10:51.520</a></span> | <span class="t">So we provide both of the options.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=657" target="_blank">00:10:57.160</a></span> | <span class="t">So recently, there are a lot of new MOEs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=662" target="_blank">00:11:02.120</a></span> | <span class="t">that have increasing number of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=665" target="_blank">00:11:05.120</a></span> | <span class="t">This will cause a very huge overhead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=669" target="_blank">00:11:09.280</a></span> | <span class="t">For example, the DeepSeq v2 MOE, it has 160 experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=674" target="_blank">00:11:14.520</a></span> | <span class="t">and eight of them are active.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=677" target="_blank">00:11:17.720</a></span> | <span class="t">If you think about the memory overhead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=682" target="_blank">00:11:22.760</a></span> | <span class="t">let's say first you would have the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=687" target="_blank">00:11:27.160</a></span> | <span class="t">And these tokens need to go to different experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=690" target="_blank">00:11:30.560</a></span> | <span class="t">And at this step, you would have a scatter and a copy operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=695" target="_blank">00:11:35.520</a></span> | <span class="t">Depending on the number of the top k,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=698" target="_blank">00:11:38.200</a></span> | <span class="t">say the top k is eight here, each of the hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=702" target="_blank">00:11:42.800</a></span> | <span class="t">would be copied eight times, which is a very huge overhead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=707" target="_blank">00:11:47.920</a></span> | <span class="t">And after the expert operation is done,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=713" target="_blank">00:11:53.400</a></span> | <span class="t">there's another eight copy of it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=716" target="_blank">00:11:56.280</a></span> | <span class="t">So we have a fused permutation operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=723" target="_blank">00:12:03.880</a></span> | <span class="t">available in Maxwell for MOE, where all of these operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=729" target="_blank">00:12:09.000</a></span> | <span class="t">are fused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=730" target="_blank">00:12:10.320</a></span> | <span class="t">Because the copy operation here, it has zero compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=737" target="_blank">00:12:17.640</a></span> | <span class="t">but it could cause duplicated memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=741" target="_blank">00:12:21.240</a></span> | <span class="t">If you have these fused operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=744" target="_blank">00:12:24.880</a></span> | <span class="t">you can easily compute these features during backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=750" target="_blank">00:12:30.520</a></span> | <span class="t">while saving a lot of memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=752" target="_blank">00:12:32.040</a></span> | <span class="t">Let's also look at the implementation of Maxwell 8x7</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=762" target="_blank">00:12:42.600</a></span> | <span class="t">on Hagen-Fitts transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=764" target="_blank">00:12:44.880</a></span> | <span class="t">You also notice in the expert operation there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=770" target="_blank">00:12:50.560</a></span> | <span class="t">you would iterate over all of the experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=773" target="_blank">00:12:53.240</a></span> | <span class="t">and compute each of the gem operation one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=779" target="_blank">00:12:59.400</a></span> | <span class="t">We found that this is very inefficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=783" target="_blank">00:13:03.240</a></span> | <span class="t">Instead, we provide an interface to Catalyst group gem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=790" target="_blank">00:13:10.880</a></span> | <span class="t">where the Catalyst group gem groups all of the looping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=796" target="_blank">00:13:16.480</a></span> | <span class="t">over experts and calculate gem into a single operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=803" target="_blank">00:13:23.520</a></span> | <span class="t">Given any number of experts and any number of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=809" target="_blank">00:13:29.920</a></span> | <span class="t">you can efficiently compute the output in one operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=814" target="_blank">00:13:34.920</a></span> | <span class="t">And this is very efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=816" target="_blank">00:13:36.960</a></span> | <span class="t">So that's pretty much all of the optimizations for MOE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=831" target="_blank">00:13:51.000</a></span> | <span class="t">in Matron-Core MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=832" target="_blank">00:13:52.640</a></span> | <span class="t">If any one of you have questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=835" target="_blank">00:13:55.560</a></span> | <span class="t">I can first answer those questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=838" target="_blank">00:13:58.240</a></span> | <span class="t">and then go to MOE upcycling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=840" target="_blank">00:14:00.680</a></span> | <span class="t">I'm just curious, are these available for everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=853" target="_blank">00:14:13.480</a></span> | <span class="t">to use in a way that attracts your attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=856" target="_blank">00:14:16.640</a></span> | <span class="t">Or are these specific to the Megatron implementation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=862" target="_blank">00:14:22.880</a></span> | <span class="t">So you can import them as a standalone module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=873" target="_blank">00:14:33.200</a></span> | <span class="t">and apply them to any of your--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=877" target="_blank">00:14:37.680</a></span> | <span class="t">Core library, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=880" target="_blank">00:14:40.200</a></span> | <span class="t">Yeah, you can import Megatron Core as a library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=884" target="_blank">00:14:44.040</a></span> | <span class="t">and just use it in your network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=886" target="_blank">00:14:46.160</a></span> | <span class="t">But I think there are some caveats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=889" target="_blank">00:14:49.160</a></span> | <span class="t">So for example, if you want to use expert parallelism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=893" target="_blank">00:14:53.720</a></span> | <span class="t">you will need to also use Megatron's parallelism strategy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=901" target="_blank">00:15:01.760</a></span> | <span class="t">to initialize a strategy first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=904" target="_blank">00:15:04.800</a></span> | <span class="t">But if you're using, for example, the group gem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=908" target="_blank">00:15:08.360</a></span> | <span class="t">I think you can just get away with any kind of network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=912" target="_blank">00:15:12.720</a></span> | <span class="t">You can combine it with Huggins phase transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=915" target="_blank">00:15:15.800</a></span> | <span class="t">with any problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=918" target="_blank">00:15:18.080</a></span> | <span class="t">This is just a PyTorch layer with a fused operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=923" target="_blank">00:15:23.880</a></span> | <span class="t">You can just import it as a library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=926" target="_blank">00:15:26.320</a></span> | <span class="t">It's very standalone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=927" target="_blank">00:15:27.720</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=931" target="_blank">00:15:31.720</a></span> | <span class="t">Do you have any intuition as far as the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=946" target="_blank">00:15:46.480</a></span> | <span class="t">contained in each expert?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=949" target="_blank">00:15:49.720</a></span> | <span class="t">I guess previous to seeing this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=952" target="_blank">00:15:52.000</a></span> | <span class="t">I had assumed that maybe one expert was good at economics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=956" target="_blank">00:15:56.680</a></span> | <span class="t">another one was good at physics, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=959" target="_blank">00:15:59.120</a></span> | <span class="t">But this makes it seem like it's more token by token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=965" target="_blank">00:16:05.920</a></span> | <span class="t">So do you have any intuition around that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=969" target="_blank">00:16:09.240</a></span> | <span class="t">Yeah, so we haven't started in our research,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=972" target="_blank">00:16:12.680</a></span> | <span class="t">but I saw a lot of research on interpretability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=976" target="_blank">00:16:16.120</a></span> | <span class="t">of the MOE models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=979" target="_blank">00:16:19.280</a></span> | <span class="t">So unfortunately, people didn't find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=984" target="_blank">00:16:24.320</a></span> | <span class="t">a significant interpretability inside these experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=991" target="_blank">00:16:31.120</a></span> | <span class="t">One expert focused on math, the other focused on literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=996" target="_blank">00:16:36.320</a></span> | <span class="t">I think the problem is that neural network hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1001" target="_blank">00:16:41.560</a></span> | <span class="t">are already very entangled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1004" target="_blank">00:16:44.560</a></span> | <span class="t">So one hidden states are in this kind of superposition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1009" target="_blank">00:16:49.440</a></span> | <span class="t">where it can represent multiple different features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1014" target="_blank">00:16:54.960</a></span> | <span class="t">So it's very hard to tell which expert focus on which area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1020" target="_blank">00:17:00.080</a></span> | <span class="t">There are some evidence of specializations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1024" target="_blank">00:17:04.720</a></span> | <span class="t">on early layers of the experts, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1028" target="_blank">00:17:08.440</a></span> | <span class="t">first and second layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1030" target="_blank">00:17:10.160</a></span> | <span class="t">You'll find some expert focus on multiple tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1034" target="_blank">00:17:14.520</a></span> | <span class="t">some expert focus on single tokens, things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1040" target="_blank">00:17:20.760</a></span> | <span class="t">And there's also one pretty interesting research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1043" target="_blank">00:17:23.520</a></span> | <span class="t">from Facebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1046" target="_blank">00:17:26.440</a></span> | <span class="t">They do a specialized training of dense model first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1052" target="_blank">00:17:32.040</a></span> | <span class="t">then combine those dense experts specialized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1056" target="_blank">00:17:36.760</a></span> | <span class="t">into different domain into a MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1060" target="_blank">00:17:40.520</a></span> | <span class="t">In that case, it still preserves some of the specializations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1066" target="_blank">00:17:46.600</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1067" target="_blank">00:17:47.120</a></span> | <span class="t">Just a quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1072" target="_blank">00:17:52.800</a></span> | <span class="t">We talked about top T sampling to select the expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1076" target="_blank">00:17:56.400</a></span> | <span class="t">throughout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1077" target="_blank">00:17:57.400</a></span> | <span class="t">Is there any benefit to using maybe a top T sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1080" target="_blank">00:18:00.200</a></span> | <span class="t">approach similar to how you would use top T sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1083" target="_blank">00:18:03.960</a></span> | <span class="t">for selecting an expert as compared to top K?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1089" target="_blank">00:18:09.000</a></span> | <span class="t">Top P?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1090" target="_blank">00:18:10.360</a></span> | <span class="t">What do you mean top P?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1092" target="_blank">00:18:12.520</a></span> | <span class="t">Considering a list of experts until they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1096" target="_blank">00:18:16.080</a></span> | <span class="t">exceed a given probability cumulatively?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1100" target="_blank">00:18:20.720</a></span> | <span class="t">Does that make sense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1102" target="_blank">00:18:22.120</a></span> | <span class="t">I see, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1102" target="_blank">00:18:22.920</a></span> | <span class="t">So since the top--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1107" target="_blank">00:18:27.040</a></span> | <span class="t">it will be dynamic, let's say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1110" target="_blank">00:18:30.160</a></span> | <span class="t">Sometimes it can select some more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1113" target="_blank">00:18:33.360</a></span> | <span class="t">Sometimes it selects less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1115" target="_blank">00:18:35.400</a></span> | <span class="t">I think that promotes a little more diversity, I've heard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1119" target="_blank">00:18:39.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1121" target="_blank">00:18:41.400</a></span> | <span class="t">Yeah, I think that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1124" target="_blank">00:18:44.080</a></span> | <span class="t">That will create some difficulty in optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1129" target="_blank">00:18:49.320</a></span> | <span class="t">But I think another thing pretty exciting is the expert choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1134" target="_blank">00:18:54.520</a></span> | <span class="t">You see, in expert choice model, the selection is reversed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1140" target="_blank">00:19:00.480</a></span> | <span class="t">So here, we talk about all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1143" target="_blank">00:19:03.000</a></span> | <span class="t">Usually, this is token choice, which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1148" target="_blank">00:19:08.560</a></span> | <span class="t">the token selects K experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1151" target="_blank">00:19:11.680</a></span> | <span class="t">So each token always have, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1154" target="_blank">00:19:14.400</a></span> | <span class="t">two experts applied on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1157" target="_blank">00:19:17.320</a></span> | <span class="t">But expert choice is another way where the experts select tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1163" target="_blank">00:19:23.800</a></span> | <span class="t">Each expert only selects K tokens constantly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1168" target="_blank">00:19:28.720</a></span> | <span class="t">So even though this is fixed, but from the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1172" target="_blank">00:19:32.960</a></span> | <span class="t">perspective, each token can have zero expert applied to it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1179" target="_blank">00:19:39.000</a></span> | <span class="t">or more than zero, or all of the experts applied to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1182" target="_blank">00:19:42.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1183" target="_blank">00:19:43.440</a></span> | <span class="t">In that case, you're either overloading the expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1185" target="_blank">00:19:45.520</a></span> | <span class="t">or not considering all the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1187" target="_blank">00:19:47.280</a></span> | <span class="t">It's like a trade-off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1189" target="_blank">00:19:49.840</a></span> | <span class="t">Yeah, in fact, expert choice applies pretty well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1193" target="_blank">00:19:53.600</a></span> | <span class="t">to vision models, because vision models do not have causal mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1198" target="_blank">00:19:58.400</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1201" target="_blank">00:20:01.080</a></span> | <span class="t">Thank you so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1202" target="_blank">00:20:02.160</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1209" target="_blank">00:20:09.040</a></span> | <span class="t">I guess I can go to the next section, upcycling MOEs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1215" target="_blank">00:20:15.400</a></span> | <span class="t">So if you are going to remember one thing, remember this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1226" target="_blank">00:20:26.240</a></span> | <span class="t">So you can upcycle your dense models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1229" target="_blank">00:20:29.160</a></span> | <span class="t">into a mix of experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1231" target="_blank">00:20:31.920</a></span> | <span class="t">By training these upcycled models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1234" target="_blank">00:20:34.200</a></span> | <span class="t">you can achieve better accuracy than simply training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1237" target="_blank">00:20:37.560</a></span> | <span class="t">the dense model further for the same number of flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1242" target="_blank">00:20:42.240</a></span> | <span class="t">The context is we have so many big dense models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1246" target="_blank">00:20:46.280</a></span> | <span class="t">For example, there's a Lama 405B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1249" target="_blank">00:20:49.840</a></span> | <span class="t">And from NVIDIA, we have Nemotron 340B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1253" target="_blank">00:20:53.680</a></span> | <span class="t">These models are huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1256" target="_blank">00:20:56.280</a></span> | <span class="t">It's very expensive to retrain MOE variant of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1260" target="_blank">00:21:00.840</a></span> | <span class="t">If we want to further improve these models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1265" target="_blank">00:21:05.320</a></span> | <span class="t">you can upcycle these models into MOE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1269" target="_blank">00:21:09.680</a></span> | <span class="t">to achieve better accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1272" target="_blank">00:21:12.240</a></span> | <span class="t">On other scaling experiments, we tried on 15B models upcycling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1280" target="_blank">00:21:20.040</a></span> | <span class="t">and applied on 1 trillion tokens and achieved roughly about 5%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1285" target="_blank">00:21:25.800</a></span> | <span class="t">improvement in terms of the validation loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1289" target="_blank">00:21:29.080</a></span> | <span class="t">and 4% improvement on MMLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1293" target="_blank">00:21:33.320</a></span> | <span class="t">It's exciting because the original sparse upcycling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1298" target="_blank">00:21:38.280</a></span> | <span class="t">paper found it difficult to scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1302" target="_blank">00:21:42.000</a></span> | <span class="t">beyond 1 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1305" target="_blank">00:21:45.480</a></span> | <span class="t">We found there are several key factors to go beyond 1 billion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1315" target="_blank">00:21:55.400</a></span> | <span class="t">So this is a concept of MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1319" target="_blank">00:21:59.840</a></span> | <span class="t">Let's say you have the MLP in the original plane boat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1325" target="_blank">00:22:05.320</a></span> | <span class="t">transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1327" target="_blank">00:22:07.040</a></span> | <span class="t">And here, this is a mixture of two experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1331" target="_blank">00:22:11.360</a></span> | <span class="t">One of the experts is activated, so the flop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1335" target="_blank">00:22:15.160</a></span> | <span class="t">is the same as the original model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1337" target="_blank">00:22:17.400</a></span> | <span class="t">Now the parameters is increased.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1341" target="_blank">00:22:21.400</a></span> | <span class="t">To upcycle such a model, you do two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1346" target="_blank">00:22:26.640</a></span> | <span class="t">First is to copy the MLP layer into a number of expert copies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1355" target="_blank">00:22:35.920</a></span> | <span class="t">Here is just two copies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1358" target="_blank">00:22:38.200</a></span> | <span class="t">And then you randomly initialize the router rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1361" target="_blank">00:22:41.800</a></span> | <span class="t">Then you just train this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1363" target="_blank">00:22:43.640</a></span> | <span class="t">Pretty straightforward, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1367" target="_blank">00:22:47.320</a></span> | <span class="t">There is one caveat here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1370" target="_blank">00:22:50.960</a></span> | <span class="t">This model needs to perform the same as the original model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1376" target="_blank">00:22:56.000</a></span> | <span class="t">in the first forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1378" target="_blank">00:22:58.440</a></span> | <span class="t">Otherwise, it will lead to catastrophe forgetting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1381" target="_blank">00:23:01.800</a></span> | <span class="t">So the trick to maintain this feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1392" target="_blank">00:23:12.240</a></span> | <span class="t">is through the swapping of the top-k softmax operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1397" target="_blank">00:23:17.680</a></span> | <span class="t">This is introduced in Mixture 8x7b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1401" target="_blank">00:23:21.840</a></span> | <span class="t">Let's say you have the MLP copied,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1404" target="_blank">00:23:24.960</a></span> | <span class="t">and then you do the top-k first to select two experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1415" target="_blank">00:23:35.000</a></span> | <span class="t">And then you do the softmax on top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1418" target="_blank">00:23:38.200</a></span> | <span class="t">of the logits from the top-k router.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1423" target="_blank">00:23:43.640</a></span> | <span class="t">In this way, because the softmax is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1426" target="_blank">00:23:46.160</a></span> | <span class="t">applied to the top-k output, it always sum up to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1431" target="_blank">00:23:51.960</a></span> | <span class="t">Here I give example 0.7, 0.3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1435" target="_blank">00:23:55.560</a></span> | <span class="t">And then you add these two outputs together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1441" target="_blank">00:24:01.240</a></span> | <span class="t">In this way, because the MLP layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1443" target="_blank">00:24:03.840</a></span> | <span class="t">is exactly the same as the dense model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1446" target="_blank">00:24:06.520</a></span> | <span class="t">these two copies are just the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1449" target="_blank">00:24:09.160</a></span> | <span class="t">So the model output is the same as the original dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1453" target="_blank">00:24:13.800</a></span> | <span class="t">This is a very important feature in upcycling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1458" target="_blank">00:24:18.280</a></span> | <span class="t">because the upcycled MLE model actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1462" target="_blank">00:24:22.760</a></span> | <span class="t">behaves exactly the same as dense without any training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1467" target="_blank">00:24:27.400</a></span> | <span class="t">And this will help stabilize the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1470" target="_blank">00:24:30.160</a></span> | <span class="t">and avoid catastrophe forgetting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1474" target="_blank">00:24:34.120</a></span> | <span class="t">But the problem here is we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1478" target="_blank">00:24:38.120</a></span> | <span class="t">found the mixed-source approach didn't work as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1481" target="_blank">00:24:41.120</a></span> | <span class="t">expected, because the original switch transformer from Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1488" target="_blank">00:24:48.440</a></span> | <span class="t">uses softmax and top-k for a reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1492" target="_blank">00:24:52.040</a></span> | <span class="t">And because of upcycling, if you switch to top-k, then softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1497" target="_blank">00:24:57.080</a></span> | <span class="t">it actually hurts some performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1501" target="_blank">00:25:01.480</a></span> | <span class="t">The difference is simply the swap of top-k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1506" target="_blank">00:25:06.560</a></span> | <span class="t">and the softmax order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1509" target="_blank">00:25:09.960</a></span> | <span class="t">I've already explained how mixed-source did top-k then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1514" target="_blank">00:25:14.800</a></span> | <span class="t">softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1516" target="_blank">00:25:16.160</a></span> | <span class="t">So in the original switch transformer paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1519" target="_blank">00:25:19.240</a></span> | <span class="t">you apply softmax first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1521" target="_blank">00:25:21.680</a></span> | <span class="t">So the probability of all of the experts would sum up to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1528" target="_blank">00:25:28.360</a></span> | <span class="t">So if you apply top-k, the output no longer sum up to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1534" target="_blank">00:25:34.120</a></span> | <span class="t">Here, it's the example 0.4, 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1538" target="_blank">00:25:38.080</a></span> | <span class="t">So this is smaller than the original output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1540" target="_blank">00:25:40.920</a></span> | <span class="t">If you just train this model naively on a large scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1546" target="_blank">00:25:46.720</a></span> | <span class="t">the model would catastrophically forget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1549" target="_blank">00:25:49.800</a></span> | <span class="t">But on a smaller scale model like 1B or under,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1553" target="_blank">00:25:53.680</a></span> | <span class="t">you can kind of get away with this problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1556" target="_blank">00:25:56.040</a></span> | <span class="t">because small model adapts very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1558" target="_blank">00:25:58.640</a></span> | <span class="t">This is probably one of the reasons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1560" target="_blank">00:26:00.560</a></span> | <span class="t">the original sparse upcycling paper didn't go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1563" target="_blank">00:26:03.960</a></span> | <span class="t">beyond 1 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1566" target="_blank">00:26:06.720</a></span> | <span class="t">So we found a very simple approach to solve this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1576" target="_blank">00:26:16.360</a></span> | <span class="t">Because the output scale is smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1578" target="_blank">00:26:18.520</a></span> | <span class="t">than the original model, we can simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1582" target="_blank">00:26:22.640</a></span> | <span class="t">scale up the MLP output by the number of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1588" target="_blank">00:26:28.880</a></span> | <span class="t">divided by top-k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1590" target="_blank">00:26:30.680</a></span> | <span class="t">For example, if it's mixed-row 8 by 7,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1594" target="_blank">00:26:34.400</a></span> | <span class="t">we simply scale the MLP layer output by 4x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1599" target="_blank">00:26:39.360</a></span> | <span class="t">This will solve the problem of the scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1602" target="_blank">00:26:42.480</a></span> | <span class="t">And the model still behaves the same as the original model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1605" target="_blank">00:26:45.720</a></span> | <span class="t">You can train the upcycle model normally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1611" target="_blank">00:26:51.120</a></span> | <span class="t">And then we found with this approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1614" target="_blank">00:26:54.360</a></span> | <span class="t">it consistently outperforms the mixed-row approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1620" target="_blank">00:27:00.280</a></span> | <span class="t">So we can get the benefit of the original three-transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1627" target="_blank">00:27:07.000</a></span> | <span class="t">A bit of intuition behind why softmax and top-k is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1633" target="_blank">00:27:13.680</a></span> | <span class="t">Because if you apply softmax to all of the experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1639" target="_blank">00:27:19.200</a></span> | <span class="t">the probability distribution is always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1643" target="_blank">00:27:23.600</a></span> | <span class="t">measured on all of the experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1646" target="_blank">00:27:26.080</a></span> | <span class="t">However, in the swap case, the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1651" target="_blank">00:27:31.880</a></span> | <span class="t">is only on two experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1653" target="_blank">00:27:33.840</a></span> | <span class="t">And it's dynamic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1655" target="_blank">00:27:35.120</a></span> | <span class="t">It's harder for the model to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1657" target="_blank">00:27:37.880</a></span> | <span class="t">Additionally, if you only use top-1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1661" target="_blank">00:27:41.800</a></span> | <span class="t">this method will not work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1663" target="_blank">00:27:43.640</a></span> | <span class="t">Because if the softmax on one expert is always 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1669" target="_blank">00:27:49.240</a></span> | <span class="t">there wouldn't be any gradient to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1672" target="_blank">00:27:52.520</a></span> | <span class="t">So next, let's go to fine-grained MLE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1685" target="_blank">00:28:05.720</a></span> | <span class="t">This is very popular in the most recent MLEs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1689" target="_blank">00:28:09.720</a></span> | <span class="t">For example, the Quain V2 uses 64 experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1695" target="_blank">00:28:15.400</a></span> | <span class="t">And DeepSeq V2 uses 120, 128, or 160-something experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1703" target="_blank">00:28:23.360</a></span> | <span class="t">Granularity uses more experts, but smaller ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1707" target="_blank">00:28:27.000</a></span> | <span class="t">So this gives the flexibility of more combinations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1712" target="_blank">00:28:32.000</a></span> | <span class="t">of different experts, so more representation power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1717" target="_blank">00:28:37.640</a></span> | <span class="t">For example, here, originally you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1720" target="_blank">00:28:40.840</a></span> | <span class="t">have the BigSurf2 expert, and 1 is selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1724" target="_blank">00:28:44.800</a></span> | <span class="t">Instead, you can expand the number of experts into 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1732" target="_blank">00:28:52.120</a></span> | <span class="t">Each expert is smaller than before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1736" target="_blank">00:28:56.200</a></span> | <span class="t">So the compute is the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1738" target="_blank">00:28:58.880</a></span> | <span class="t">And the parameters is also the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1742" target="_blank">00:29:02.440</a></span> | <span class="t">Here, some notation here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1745" target="_blank">00:29:05.480</a></span> | <span class="t">E2 means expansion factor is 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1749" target="_blank">00:29:09.680</a></span> | <span class="t">This is how many times the expert is copied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1755" target="_blank">00:29:15.320</a></span> | <span class="t">And G2 is granularity, or how many times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1761" target="_blank">00:29:21.840</a></span> | <span class="t">the expert is segmented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1764" target="_blank">00:29:24.720</a></span> | <span class="t">So you can think of this like the expert is first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1769" target="_blank">00:29:29.880</a></span> | <span class="t">copied into two copies here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1772" target="_blank">00:29:32.720</a></span> | <span class="t">And then each copy is segmented two times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1777" target="_blank">00:29:37.400</a></span> | <span class="t">T2 means how many experts we route to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1785" target="_blank">00:29:45.320</a></span> | <span class="t">Here, you route to two experts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1788" target="_blank">00:29:48.120</a></span> | <span class="t">So the flop is the same as the original one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1795" target="_blank">00:29:55.680</a></span> | <span class="t">So you would soon notice a problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1799" target="_blank">00:29:59.680</a></span> | <span class="t">if you upcycle such model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1802" target="_blank">00:30:02.600</a></span> | <span class="t">Let's say these two segments are not the same anymore,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1810" target="_blank">00:30:10.000</a></span> | <span class="t">because you segment one expert into two shards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1818" target="_blank">00:30:18.440</a></span> | <span class="t">For the top two cases, if you select one shard two times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1822" target="_blank">00:30:22.960</a></span> | <span class="t">and the other shard zero time, the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1826" target="_blank">00:30:26.160</a></span> | <span class="t">is no longer the same as the original MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1830" target="_blank">00:30:30.000</a></span> | <span class="t">And we mentioned that it's very important for upcycling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1834" target="_blank">00:30:34.800</a></span> | <span class="t">to maintain the same forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1838" target="_blank">00:30:38.800</a></span> | <span class="t">as the original dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1842" target="_blank">00:30:42.280</a></span> | <span class="t">The solution here is also rather straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1846" target="_blank">00:30:46.760</a></span> | <span class="t">So instead of randomly initialize a router,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1850" target="_blank">00:30:50.520</a></span> | <span class="t">we would initialize half as a router, and then duplicate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1856" target="_blank">00:30:56.200</a></span> | <span class="t">So this would ensure that the probability distribution would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1861" target="_blank">00:31:01.080</a></span> | <span class="t">be the same in each virtual group, or in each shard group.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1867" target="_blank">00:31:07.760</a></span> | <span class="t">Because if these are the same, the top case selection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1871" target="_blank">00:31:11.040</a></span> | <span class="t">will be exactly the same, the highest score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1877" target="_blank">00:31:17.240</a></span> | <span class="t">would be the same for these two groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1880" target="_blank">00:31:20.640</a></span> | <span class="t">I say here the example is that I shard the MLP into two parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1889" target="_blank">00:31:29.280</a></span> | <span class="t">The first, I need to select the orange exacting ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1894" target="_blank">00:31:34.040</a></span> | <span class="t">and the blue exacting ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1896" target="_blank">00:31:36.800</a></span> | <span class="t">By duplicating the router base, this achieves the purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1904" target="_blank">00:31:44.880</a></span> | <span class="t">And the formula for scaling the weights are also the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1911" target="_blank">00:31:51.480</a></span> | <span class="t">except there's an additional granularity factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1915" target="_blank">00:31:55.480</a></span> | <span class="t">This is a bit complicated, so I'll skip this part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1920" target="_blank">00:32:00.480</a></span> | <span class="t">So for our experiment, we do our experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1930" target="_blank">00:32:10.280</a></span> | <span class="t">on the 8 trillion tokens Nemotron trained on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1934" target="_blank">00:32:14.120</a></span> | <span class="t">The ablation study is on a smaller model, Nemotron2B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1938" target="_blank">00:32:18.840</a></span> | <span class="t">And the bigger model, we do it on 8 by 15B on 1 trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1943" target="_blank">00:32:23.360</a></span> | <span class="t">tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1943" target="_blank">00:32:23.860</a></span> | <span class="t">And we found that the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1949" target="_blank">00:32:29.440</a></span> | <span class="t">is the most important hyperparameter, as always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1952" target="_blank">00:32:32.480</a></span> | <span class="t">in machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1954" target="_blank">00:32:34.360</a></span> | <span class="t">And this is true also for upcycling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1957" target="_blank">00:32:37.120</a></span> | <span class="t">Learning rate is the most important parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1961" target="_blank">00:32:41.560</a></span> | <span class="t">So in the original sparse upcycling paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1966" target="_blank">00:32:46.440</a></span> | <span class="t">the learning rate is taken from the minimum--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1971" target="_blank">00:32:51.440</a></span> | <span class="t">the ending learning rate from pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1974" target="_blank">00:32:54.080</a></span> | <span class="t">So sometimes it can be rather small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1977" target="_blank">00:32:57.760</a></span> | <span class="t">So we found that if you have a high learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1984" target="_blank">00:33:04.120</a></span> | <span class="t">it could help the upcycling a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1987" target="_blank">00:33:07.720</a></span> | <span class="t">Here is the orange line is the lowest learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1991" target="_blank">00:33:11.600</a></span> | <span class="t">Basically, you continue to fine-tune the model into MOE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=1998" target="_blank">00:33:18.440</a></span> | <span class="t">This learning rate is typical, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2001" target="_blank">00:33:21.520</a></span> | <span class="t">for other tasks like alignment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2004" target="_blank">00:33:24.360</a></span> | <span class="t">But for upcycling, the model needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2008" target="_blank">00:33:28.240</a></span> | <span class="t">to adapt to a new local minima.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2011" target="_blank">00:33:31.520</a></span> | <span class="t">We need a larger learning rate for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2014" target="_blank">00:33:34.400</a></span> | <span class="t">We found that the best is to use the original highest peak</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2019" target="_blank">00:33:39.800</a></span> | <span class="t">learning rate from pre-training, which works the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2023" target="_blank">00:33:43.120</a></span> | <span class="t">And if you dive into the base of this upcycled model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2033" target="_blank">00:33:53.600</a></span> | <span class="t">you will find something really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2037" target="_blank">00:33:57.680</a></span> | <span class="t">So if you apply a constant small learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2042" target="_blank">00:34:02.560</a></span> | <span class="t">like a fine-tuning or alignment, the constant similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2048" target="_blank">00:34:08.640</a></span> | <span class="t">between the base model and the upcycled model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2052" target="_blank">00:34:12.440</a></span> | <span class="t">would be almost 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2054" target="_blank">00:34:14.600</a></span> | <span class="t">This is true for most of the aligned model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2058" target="_blank">00:34:18.160</a></span> | <span class="t">for example, LamaChat versus LamaBase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2063" target="_blank">00:34:23.120</a></span> | <span class="t">If you use a high peak learning rate for upcycling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2067" target="_blank">00:34:27.520</a></span> | <span class="t">the constant similarity would be much lower, around 0.7.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2074" target="_blank">00:34:34.560</a></span> | <span class="t">Also, we also analyzed the mixed-row 8x7 base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2082" target="_blank">00:34:42.320</a></span> | <span class="t">versus mixed-row 7B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2084" target="_blank">00:34:44.800</a></span> | <span class="t">We found that the similarity is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2087" target="_blank">00:34:47.560</a></span> | <span class="t">around there, which means you need higher learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2092" target="_blank">00:34:52.720</a></span> | <span class="t">rates for upcycling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2097" target="_blank">00:34:57.160</a></span> | <span class="t">An additional experiment on number of experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2102" target="_blank">00:35:02.760</a></span> | <span class="t">we found 64 experts is kind of like the sweet spot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2108" target="_blank">00:35:08.160</a></span> | <span class="t">If you increase the number of experts beyond 64,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2112" target="_blank">00:35:12.320</a></span> | <span class="t">it provides diminishing return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2114" target="_blank">00:35:14.760</a></span> | <span class="t">Finally, this is the large-scale upcycling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2124" target="_blank">00:35:24.760</a></span> | <span class="t">We upcycled the 8x15B model on 1 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2130" target="_blank">00:35:30.000</a></span> | <span class="t">So there are three models here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2138" target="_blank">00:35:38.520</a></span> | <span class="t">Let me explain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2139" target="_blank">00:35:39.480</a></span> | <span class="t">The base model, 15B, is trained on 8 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2144" target="_blank">00:35:44.520</a></span> | <span class="t">This is pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2146" target="_blank">00:35:46.160</a></span> | <span class="t">So validation loss, 1.6, and the MMLU, 59.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2152" target="_blank">00:35:52.320</a></span> | <span class="t">So the continual training model, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2156" target="_blank">00:35:56.360</a></span> | <span class="t">target more on the academic benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2161" target="_blank">00:36:01.960</a></span> | <span class="t">to obtain higher performance on the evaluations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2167" target="_blank">00:36:07.320</a></span> | <span class="t">This is roughly 1 trillion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2169" target="_blank">00:36:09.680</a></span> | <span class="t">The upcycling is performed on the same data for comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2174" target="_blank">00:36:14.480</a></span> | <span class="t">Continuous training is just a dense model of continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2177" target="_blank">00:36:17.480</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2178" target="_blank">00:36:18.720</a></span> | <span class="t">You will notice that actually, the data plays the biggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2183" target="_blank">00:36:23.520</a></span> | <span class="t">factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2185" target="_blank">00:36:25.200</a></span> | <span class="t">Even the base model continuous training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2190" target="_blank">00:36:30.600</a></span> | <span class="t">it can have a huge boost on MMLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2194" target="_blank">00:36:34.960</a></span> | <span class="t">And the upcycled model is another 4% to 5% improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2200" target="_blank">00:36:40.160</a></span> | <span class="t">on top of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2201" target="_blank">00:36:41.760</a></span> | <span class="t">Continuous training is like 20% improvement because of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2207" target="_blank">00:36:47.200</a></span> | <span class="t">This reminds us, again, data is the most important in ML.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2211" target="_blank">00:36:51.880</a></span> | <span class="t">If you put the 5% improvement into the scaling law</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2223" target="_blank">00:37:03.080</a></span> | <span class="t">perspective, we can roughly gauge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2226" target="_blank">00:37:06.360</a></span> | <span class="t">how much the 5% improvement means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2230" target="_blank">00:37:10.920</a></span> | <span class="t">Actually, the fine-grained MOE, the upcycled,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2236" target="_blank">00:37:16.280</a></span> | <span class="t">this has the same plot as the dense model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2240" target="_blank">00:37:20.600</a></span> | <span class="t">And this has about 4% improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2244" target="_blank">00:37:24.200</a></span> | <span class="t">in terms of the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2246" target="_blank">00:37:26.000</a></span> | <span class="t">4% improvement if you plug in the scaling law from point AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2250" target="_blank">00:37:30.600</a></span> | <span class="t">it's roughly represent 1.7x bigger model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2256" target="_blank">00:37:36.480</a></span> | <span class="t">And the non-fine-grained model, well, top two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2263" target="_blank">00:37:43.680</a></span> | <span class="t">this is the same config as the 8x7 mixed row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2268" target="_blank">00:37:48.800</a></span> | <span class="t">It has increased flops because of top two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2273" target="_blank">00:37:53.200</a></span> | <span class="t">That's roughly 1.7x more flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2276" target="_blank">00:37:56.560</a></span> | <span class="t">And it's roughly two times as powerful as the original model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2283" target="_blank">00:38:03.280</a></span> | <span class="t">Given that we only spent like 1/8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2285" target="_blank">00:38:05.880</a></span> | <span class="t">of the original pre-training compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2290" target="_blank">00:38:10.080</a></span> | <span class="t">this is indeed some saving compared to training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2294" target="_blank">00:38:14.760</a></span> | <span class="t">these MOEs from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2296" target="_blank">00:38:16.280</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2300" target="_blank">00:38:20.040</a></span> | <span class="t">Thank you for listening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2301" target="_blank">00:38:21.440</a></span> | <span class="t">I put the paper links, Microsoft Core MOE GitHub there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2306" target="_blank">00:38:26.920</a></span> | <span class="t">and Nevo GitHub where we provide high-level training interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2312" target="_blank">00:38:32.720</a></span> | <span class="t">You can also follow me on LinkedIn and Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2317" target="_blank">00:38:37.560</a></span> | <span class="t">I can take questions now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2320" target="_blank">00:38:40.720</a></span> | <span class="t">Hey, nice to see you again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2323" target="_blank">00:38:43.720</a></span> | <span class="t">There are a whole bunch of questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2325" target="_blank">00:38:45.200</a></span> | <span class="t">I think we'll stop the recording so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2326" target="_blank">00:38:46.760</a></span> | <span class="t">can open up for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2328" target="_blank">00:38:48.280</a></span> | <span class="t">But I think everyone is very excited by the presentations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2332" target="_blank">00:38:52.280</a></span> | <span class="t">There's a lot of questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2333" target="_blank">00:38:53.400</a></span> | <span class="t">And also people want your slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2335" target="_blank">00:38:55.120</a></span> | <span class="t">So let people know where to get their slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2337" target="_blank">00:38:57.840</a></span> | <span class="t">Yeah, sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2338" target="_blank">00:38:58.440</a></span> | <span class="t">I can share the slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=e_mkhFkKPEk&t=2340" target="_blank">00:39:00.680</a></span> | <span class="t">I can share this slide.</span></div></div></body></html>