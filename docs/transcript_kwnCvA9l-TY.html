<html><head><title>AI Frontiers in Trust and Safety  Combatting Multifaceted Harm on Tinder at Scale: Vibhor Kumar</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>AI Frontiers in Trust and Safety  Combatting Multifaceted Harm on Tinder at Scale: Vibhor Kumar</h2><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY" target="_blank"><img src="https://i.ytimg.com/vi_webp/kwnCvA9l-TY/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>. Hello, AI engineer World's Fair. My name is Vibor, VB for short, and I'm a senior AI engineer at Tinder, where I've been working on trust and safety for the last five years. I also work on and maintain some open-source AI projects, and I'm an advisor to a few AI startups.</p><p>But we only have 15 minutes, so I'll jump right into it, maybe a little bit less than 15 now. Today I'll be talking about AI frontiers in trust and safety, combating multifaceted harm on Tinder at scale. We'll first go over what trust and safety actually is for everyone in the audience, and more specifically what it means at Tinder.</p><p>Then we'll go over the complex interaction between generative AI and trust and safety, some of the problems which most people think about, but also some of the tremendous opportunities which will fundamentally change the space. Next we'll dive specifically into how to actually use LLMs for detecting trust and safety violations in text, covering the end-to-end stack from training to fine-tuning to productionization, and an overview of how we're doing this at Tinder.</p><p>Finally we'll cover what the future looks like for this effort, and what we should be most excited about. So what is trust and safety actually? It's not really something that's well defined, and it's more of an art than a science. Oftentimes we have to make ethical judgment calls, but it's helpful to look at a breakdown of the goals of TNS by Del Harvey, who led trust and safety at Twitter for 13 years.</p><p>Ultimately TNS is about preventing risk, reducing risk, detecting harm, and mitigating harm. The ultimate goal is to protect users and also the companies creating the products that they use. In this presentation we'll focus on the detecting harm part of it, where we devote a lot of our time to at Tinder.</p><p>Speaking of which, as the largest dating app in the world, we encounter many, many types of violative behavior on Tinder. Here are some of the different categories, and a representative, synthetic textual example of each. First, we have social media in your profile, a relatively minor, but rather prevalent violation of our private information policy.</p><p>And it's often done by low intent users. On the other side of the spectrum, we have things that are low prevalence, but high harm, things like hate speech, harassment, and pig butchering scams. So now that we have a sense of what TNS is, let's move on to the problems that generative AI is causing for the industry.</p><p>One of the biggest problems is that Gen AI enables rapid generation of content, which makes it particularly easy to spread misinformation, propaganda, and low-quality spam by drowning out genuine content. That's what's known as the content pollution phenomenon. Additionally, there's some risk that platforms where the content is posted will essentially inherit the known copyright issues plaguing consumer Gen AI tools today, like OpenAI.</p><p>Another problem is the accessibility of deepfake technology, which lowers the bar of entry to impersonation. and catfishing. This also enables malicious interpersonal harm, like in the case of revenge porn. Lastly, Gen AI can be used to scale up organized spam and scam operations. Bad actors can rapidly create profiles by generating text and images, which means that existing signals, the ones we are using today, rely on similarity matching or hashes, will be increasingly less likely to work.</p><p>As a side note, this is why TNS teams dealing with automated fraud will need to increasingly take advantage of metadata-type signals associated with physical bottlenecks in meatspace, things like IP address, ISP information, and phone numbers. Now that we've covered some of the major problems that Gen AI is causing for the trust and safety industry, but there's actually some big reasons to be hopeful as well.</p><p>The first is that AI labs at both startups and big companies have already done the hard work of pre-training and open sourcing LLMs for everyone's benefit. Out of the box, these are really powerful in that they have latent semantic capability and often global language coverage as well. Just try doing a few-shot example with prompt engineering LLM3 or Mist rule to detect any kind of violation.</p><p>It usually works pretty well. By fine-tuning these models, we can actually achieve state-of-the-art performance, in some cases better than few-shot GPT-4 performance on downstream textual detection tasks. And the act of fine-tuning has been made a lot easier because the open source community has produced libraries and tools that are relatively mature and maintained now.</p><p>It's easier than ever to do fine-tuning, with the low-level details being abstracted away, and there's libraries built for every stage of model development and productionization. The next two opportunities I wanted to bring up are things that every trust and safety organization should be paying attention to. First is that we're fine-tuning rather than starting from scratch, and because of that, and the strong open source library support, we can actually accelerate the model development process from months to weeks.</p><p>And additionally, one of the major reasons we see such a good performance from the fine-tuned open source LLMs is that, in general, model performance degrades quickly in trust and safety due to the adversarial nature of automated fraud. For example, whenever we release a rule to block one spam wave, bad actors are incentivized to, and ultimately do change their behavior to get around it.</p><p>But, the generalization performance of LLMs slows the model degradation curve significantly, and we basically get this for free when we use LLMs. Okay, so let's move on to some of the specifics of actually using LLMs for TNS violation detection. The first major step is creating our training dataset. This is often the hardest part.</p><p>That's due, in part, to how easy some of the later steps are, as we'll see, but it's also because smaller datasets are required for fine-tuning versus training from scratch. In some cases, hundreds to thousands of examples. And this necessitates creating a pretty high-quality dataset. What does this dataset look like?</p><p>Well, GPT-type LLMs, like the closed-source GPT-4 and Cloud Opus, and also like the open-source LAM and MISTRO models, can be thought of as a text-in to text-out. This is an approximate mental model, but it helps for understanding what the dataset should look like. In our case, the text-in is the potentially violating text we want the model to be able to make a prediction on, wrapped by some prompt.</p><p>And the text-out is a classification label or some extracted characters representing the violation. It's not a very complicated format. And there's a synthetic example for how we would detect users if they're underage in their written bio. As for actually assembling this dataset, it's possible to do it entirely by hand, because, again, we only need hundreds to thousands of examples.</p><p>One thing we've seen work quite well is actually to incorporate the largest LLMs in the data generation process. We could generate purely synthetic training examples with few-shot prompting, but this introduces some risk that the data won't resemble the true data distribution, and it's platform-agnostic. What we found works better is to actually do a hybrid process where we can use GPT-4 with some clever prompting to ensure we don't run into the alignment built-in to actually make predictions on internal analytics data and to mine examples for our training set that way.</p><p>The cost of doing this is inversely proportional to the true prevalence of the harm, but that cost is still pretty negligible, and it provides a metric that alone is actually really helpful to track for TNS operations teams, anyways. It's possible to restrict the LLM calls to more likely candidates, and finally, when we get the mined examples from using GPT-4 effectively as an auto-moderation, we can then do a manual verification, fixing any mislabeled data, and making judgment calls where the label is more ambiguous.</p><p>Okay, so I've got the training set, now let's talk about fine-tuning. One question you might have is, why don't we just directly use the API LLMs, like GPT-4, to do this detection and production? One fundamental reason is scale. One fundamental reason is scale. Tinder has a huge, real-time volume of profile interactions, and hitting OpenAI APIs that often doesn't scale in terms of cost, latency, and throughput.</p><p>The other reason is maintainability. By fine-tuning our own models, we have full control over the model weights and can re-fine-tune when production performance inevitably degrades, without needing to worry about changes in the underlying base model. One additional benefit for us for classification tasks is that we have access to the output probability distribution, which means we can create, essentially, a confidence of the prediction, like in a traditional machine learning model.</p><p>As for actually doing the fine-tuning, well, the relatively mature open-source ecosystem makes this really easy. Hugging face libraries make this as simple as writing a few hundred lines of code, without really needing to understand anything about deep learning or transformers. We've also had particular success building out training pipelines in notebooks.</p><p>There are also libraries which abstract fine-tuning to just config files, like Axolotl, Ludwig, Lama Factory. And finally, there's managed solutions emerging that provide additional UI and dataset management support for rapid experimentation, like H2LM Studio and Predibase, the latter with whom we've had a lot of success with. So many of you are probably familiar with parameter efficient fine-tuning.</p><p>This is critical for us. Low-rank adaptation, or LoRa, freezes the weights of the base model and can create a fine-tuned model while only really needing to learn megabytes of additional weights. Accordingly, the fine-tuning can be done quickly and only on one or a few GPUs, which enables rapid experimentation and also unlocks using larger base models.</p><p>PEFs of larger base models are more likely to be better than full fine-tunes of smaller base models, especially for classification tasks. We've had a lot of success also with QLaura, which unlocks fine-tuning on a single GPU, even the largest models. Lastly, one of the biggest reasons to use LoRa is that we can take advantage of the massive inference optimizations, as we'll see now.</p><p>So, production. In production, we use LoRaX, which is an open-source framework that allows users to efficiently serve thousands of fine-tuned models on a single GPU. It exploits the fact that a fine-tuned LoRa adapter, which is just a single fine-tuned model, is only a few megabytes in size. Many adapters can be efficiently served jointly by simply shuffling and batching adapters and requests for efficient serving.</p><p>In practice, this means that the marginal cost of serving a new adapter on the same base model is virtually zero. I just want to let the implication of that sink in. It means that we can train adapters for the many, many different types of trust and safety violations possible.</p><p>Hate speech, promotion, catfishing, pig butchering scams, and so on. And we can serve all those adapters on one or even a small set of GPUs and not need to worry about horizontal scaling. Incorporating a new adapter in production is as simple as storing the megabytes of weights on some file system and modifying a request to the LoRaX client.</p><p>Special thanks to the Predibase team who developed and maintains LoRaX and have provided us a lot of support in it. The optimizations in LoRaX basically enable us to do real-time inference. And at Tinder, that's critical. We can support on 7 billion parameter models, tens of QPS on 100-ish milliseconds of latency on A10 GPUs.</p><p>This is good enough for some use cases. And for those other use cases, the high-frequency domains, we can further reduce throughput by gating requests with heuristics. For example, for detecting social media in profiles, we can make predictions only on bios that contain some word that's not in a dictionary.</p><p>And then we're also exploring doing cascade classification through some distillation process where we train adapters on smaller base models optimizing for recall, and only call the larger base model adapters when the smaller one gives a high enough score. Another advantage for us in this TNS space is, in general, LLM outputs are computationally expensive, because the generation is done autoregressively, one token at a time.</p><p>But classification or extraction tasks require only exactly one token or a few tokens to output, which means our time to prediction is low. And compared to NLP models of the past, we're seeing that we can get massive improvements in precision and recall, just due to the much higher latent semantic capability of today's LLMs.</p><p>We can achieve near 100% recall in simpler tasks like social handle detection, and significant improvements over the baseline in more semantically complex tasks. The other huge benefit that we get is way better generalization performance, which I've talked about before. In particular, this is important for TNS because it's an adversarial game.</p><p>Bad actors and other violative users always try to avoid detection. For example, with intentional typos, mixing letters and numbers and innuendos. But LLMs are much better at making sense of these, meaning that these new adapter-based models get stale less quickly than other traditional machine learning models and are a better defense against harm in the long run.</p><p>So where do we go from here? We're interested in the growing work on non-textual modalities and how we can leverage that for detection purposes. For example, we can use pre-trained visual language models like LAVA to do explicit image detection, and that's an active area of exploration for us. Overall, we're excited about rapidly training adapters for detecting harm along the long tail of TNS violations.</p><p>We can create high-quality data sets with trust and safety operations and policy experts with that AI in the loop. We can automate training and retraining pipelines for fine-tuning adapters. And we can take advantage of Lorax to slot in new adapters for inference with low marginal cost. Ultimately, we can build a next-generation defensive moat against harm that takes advantage of the Gen.AI landscape today, ultimately leading to a safer, healthier platform.</p><p>Thanks for listening. . . . . . . . . . . . . .</p></div></div></body></html>