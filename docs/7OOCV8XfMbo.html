<html><head><title>Stanford XCS224U: NLU I In-context Learning, Part 2: Core Concepts I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I In-context Learning, Part 2: Core Concepts I Spring 2023</h2><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo"><img src="https://i.ytimg.com/vi/7OOCV8XfMbo/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgZShlMA8=&rs=AOn4CLBacANagIqG5MXiEaqTOUD5LassUg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./7OOCV8XfMbo.html">Whisper Transcript</a> | <a href="./transcript_7OOCV8XfMbo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=6" target="_blank">00:00:06.000</a></span> | <span class="t">This is part two in our series on in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=8" target="_blank">00:00:08.900</a></span> | <span class="t">I thought I'd cover some core concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=10" target="_blank">00:00:10.740</a></span> | <span class="t">For the most part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=11" target="_blank">00:00:11.760</a></span> | <span class="t">I think these concepts are a review for you all,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=14" target="_blank">00:00:14.560</a></span> | <span class="t">but I thought it would be good to get them into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=16" target="_blank">00:00:16.780</a></span> | <span class="t">our common ground to help us think about them as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=20" target="_blank">00:00:20.000</a></span> | <span class="t">we think about what's happening with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=21" target="_blank">00:00:21.440</a></span> | <span class="t">in-context learning techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=23" target="_blank">00:00:23.600</a></span> | <span class="t">To start, let's just establish some terminology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=26" target="_blank">00:00:26.760</a></span> | <span class="t">I think there's a lot of variation in how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=28" target="_blank">00:00:28.280</a></span> | <span class="t">these terms are used in the literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=30" target="_blank">00:00:30.080</a></span> | <span class="t">and I thought I would just try to be clear about what I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=32" target="_blank">00:00:32.440</a></span> | <span class="t">mean with these various crucial phrases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=35" target="_blank">00:00:35.220</a></span> | <span class="t">Let's start with in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=37" target="_blank">00:00:37.120</a></span> | <span class="t">When I say in-context learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=38" target="_blank">00:00:38.600</a></span> | <span class="t">I mean a frozen language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=41" target="_blank">00:00:41.160</a></span> | <span class="t">performs a task only by conditioning on the prompt text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=45" target="_blank">00:00:45.640</a></span> | <span class="t">It's frozen, that is there are no gradient updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=48" target="_blank">00:00:48.100</a></span> | <span class="t">The only mechanism we have for learning is that we input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=51" target="_blank">00:00:51.320</a></span> | <span class="t">some text and that puts the model in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=53" target="_blank">00:00:53.680</a></span> | <span class="t">a temporary state that we hope is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=55" target="_blank">00:00:55.640</a></span> | <span class="t">useful for having it generate things that we regard as useful for our task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=61" target="_blank">00:01:01.240</a></span> | <span class="t">Few shot in-context learning is a special case of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=65" target="_blank">00:01:05.000</a></span> | <span class="t">This is where the prompt includes examples of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=67" target="_blank">00:01:07.880</a></span> | <span class="t">the intended behavior and no examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=70" target="_blank">00:01:10.880</a></span> | <span class="t">of the intended behavior were seen in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=74" target="_blank">00:01:14.040</a></span> | <span class="t">Of course, we are unlikely to be able to verify two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=78" target="_blank">00:01:18.400</a></span> | <span class="t">In this modern era where models are trained on massive amounts of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=82" target="_blank">00:01:22.060</a></span> | <span class="t">we'll have no idea typically what was in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=84" target="_blank">00:01:24.420</a></span> | <span class="t">those training datasets often we have no ability to audit them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=88" target="_blank">00:01:28.160</a></span> | <span class="t">We might not be sure whether we're actually doing few shot in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=92" target="_blank">00:01:32.720</a></span> | <span class="t">But this is the ideal and the spirit of this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=96" target="_blank">00:01:36.200</a></span> | <span class="t">that if models have seen examples of this type in training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=99" target="_blank">00:01:39.560</a></span> | <span class="t">it's hardly few shot anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=101" target="_blank">00:01:41.000</a></span> | <span class="t">The whole point is to see whether with just a few instances,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=104" target="_blank">00:01:44.620</a></span> | <span class="t">models can do what we want them to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=107" target="_blank">00:01:47.560</a></span> | <span class="t">I'll also acknowledge that the term few shot is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=110" target="_blank">00:01:50.940</a></span> | <span class="t">used in more traditional supervised learning settings in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=115" target="_blank">00:01:55.140</a></span> | <span class="t">the sense of training on a few examples with gradient updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=119" target="_blank">00:01:59.060</a></span> | <span class="t">I'm just emphasizing that when I say few shot in this lecture series,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=123" target="_blank">00:02:03.300</a></span> | <span class="t">I'm always going to mean few shot in-context learning with no gradient updates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=128" target="_blank">00:02:08.860</a></span> | <span class="t">Zero shot in-context learning is another special case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=132" target="_blank">00:02:12.500</a></span> | <span class="t">This is where the prompt includes no examples of the intended behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=136" target="_blank">00:02:16.320</a></span> | <span class="t">but I'll allow that it could contain some instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=140" target="_blank">00:02:20.180</a></span> | <span class="t">As before, item 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=142" target="_blank">00:02:22.300</a></span> | <span class="t">no examples of the intended behavior were seen in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=145" target="_blank">00:02:25.280</a></span> | <span class="t">Again, we're unlikely to be able to verify two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=148" target="_blank">00:02:28.460</a></span> | <span class="t">so we won't know whether this is truly zero shot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=151" target="_blank">00:02:31.620</a></span> | <span class="t">but the concept is clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=154" target="_blank">00:02:34.020</a></span> | <span class="t">For item 1, this is more interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=156" target="_blank">00:02:36.460</a></span> | <span class="t">I'll say that formatting and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=158" target="_blank">00:02:38.220</a></span> | <span class="t">other instructions that you include in the prompt are a gray area,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=161" target="_blank">00:02:41.960</a></span> | <span class="t">but let's allow them in the zero shot category.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=164" target="_blank">00:02:44.440</a></span> | <span class="t">What I mean by that is that as you give more elaborate instructions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=167" target="_blank">00:02:47.380</a></span> | <span class="t">you might in effect be demonstrating the intended behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=171" target="_blank">00:02:51.120</a></span> | <span class="t">But the other side of this is that instructions are conceptually very different kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=175" target="_blank">00:02:55.860</a></span> | <span class="t">of things for machine learning in general than actual demonstrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=181" target="_blank">00:03:01.100</a></span> | <span class="t">It's interesting to separate out the case where you demonstrate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=184" target="_blank">00:03:04.460</a></span> | <span class="t">directly from the case where you just describe the intended behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=188" target="_blank">00:03:08.780</a></span> | <span class="t">We'll allow mere descriptions to still be zero shot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=193" target="_blank">00:03:13.620</a></span> | <span class="t">Another reminder is just how GPT and other models work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=199" target="_blank">00:03:19.900</a></span> | <span class="t">We covered this in the unit on contextual representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=204" target="_blank">00:03:24.440</a></span> | <span class="t">and I thought I'd just remind us so that this is front of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=207" target="_blank">00:03:27.000</a></span> | <span class="t">mind as we think about the in-context learning techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=210" target="_blank">00:03:30.420</a></span> | <span class="t">Here's a slide repeating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=212" target="_blank">00:03:32.060</a></span> | <span class="t">the autoregressive loss function that these models use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=214" target="_blank">00:03:34.780</a></span> | <span class="t">Again, the essence of this is that scoring happens on the basis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=218" target="_blank">00:03:38.820</a></span> | <span class="t">of the embedding representation for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=221" target="_blank">00:03:41.020</a></span> | <span class="t">the token that we want to predict at time step t,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=224" target="_blank">00:03:44.100</a></span> | <span class="t">and the hidden state that the model has created up until the time step preceding t.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=230" target="_blank">00:03:50.140</a></span> | <span class="t">Those are the two crucial ingredients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=233" target="_blank">00:03:53.460</a></span> | <span class="t">Here's how that plays out for GPT style models in the context of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=239" target="_blank">00:03:59.900</a></span> | <span class="t">Then I'll show you first training with teacher forcing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=242" target="_blank">00:04:02.780</a></span> | <span class="t">This slide is a repeat from one we had in the contextual representations unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=246" target="_blank">00:04:06.940</a></span> | <span class="t">but again, I want to issue a reminder here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=249" target="_blank">00:04:09.900</a></span> | <span class="t">At the bottom, we have one-hot vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=253" target="_blank">00:04:13.700</a></span> | <span class="t">representing the sequence of tokens in the sequence that we are using for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=258" target="_blank">00:04:18.640</a></span> | <span class="t">Normally, we represent these as actual sequences of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=261" target="_blank">00:04:21.860</a></span> | <span class="t">but I'm trying to remind us at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=263" target="_blank">00:04:23.620</a></span> | <span class="t">a mechanical level of how these things actually operate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=266" target="_blank">00:04:26.580</a></span> | <span class="t">These are one-hot vectors and those are used to look up vectors in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=271" target="_blank">00:04:31.340</a></span> | <span class="t">our embedding layer that's given in gray here and the result of that lookup is a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=276" target="_blank">00:04:36.700</a></span> | <span class="t">At this stage, I have given the names of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=278" target="_blank">00:04:38.940</a></span> | <span class="t">the vectors according to our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=281" target="_blank">00:04:41.400</a></span> | <span class="t">But again, what we really have here is a sequence of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=286" target="_blank">00:04:46.060</a></span> | <span class="t">Those vectors are the input to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=288" target="_blank">00:04:48.620</a></span> | <span class="t">the big transformer model that we're using for language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=292" target="_blank">00:04:52.260</a></span> | <span class="t">I've shown a schematic of this and the one thing I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=295" target="_blank">00:04:55.300</a></span> | <span class="t">highlighted is the pattern of attention mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=298" target="_blank">00:04:58.380</a></span> | <span class="t">Recall that when we're doing autoregressive modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=301" target="_blank">00:05:01.140</a></span> | <span class="t">we can't look into the future with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=303" target="_blank">00:05:03.340</a></span> | <span class="t">those dot product attention mechanisms only into the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=306" target="_blank">00:05:06.900</a></span> | <span class="t">You see that characteristic pattern for the attention connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=311" target="_blank">00:05:11.260</a></span> | <span class="t">We do all our processing with all of these transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=314" target="_blank">00:05:14.460</a></span> | <span class="t">Then at the very top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=315" target="_blank">00:05:15.700</a></span> | <span class="t">we're going to use our embedding layer again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=318" target="_blank">00:05:18.580</a></span> | <span class="t">The labels, so to speak,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=321" target="_blank">00:05:21.320</a></span> | <span class="t">are again our sequence offset by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=323" target="_blank">00:05:23.740</a></span> | <span class="t">one from the sequence that we have at the bottom here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=326" target="_blank">00:05:26.820</a></span> | <span class="t">Like this was the start token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=328" target="_blank">00:05:28.980</a></span> | <span class="t">we use that to predict the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=331" target="_blank">00:05:31.300</a></span> | <span class="t">then at the next time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=332" target="_blank">00:05:32.780</a></span> | <span class="t">the comes in down here and that is the basis for predicting rock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=336" target="_blank">00:05:36.320</a></span> | <span class="t">Rock comes in down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=338" target="_blank">00:05:38.220</a></span> | <span class="t">predicts rules, rules down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=340" target="_blank">00:05:40.740</a></span> | <span class="t">and then we finally predict the end of token sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=343" target="_blank">00:05:43.220</a></span> | <span class="t">We're offset by one using the previous context to predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=349" target="_blank">00:05:49.500</a></span> | <span class="t">Again, I've given these as one-hot vectors because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=352" target="_blank">00:05:52.500</a></span> | <span class="t">those one-hot vectors are the actual learning signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=356" target="_blank">00:05:56.340</a></span> | <span class="t">Those are compared for learning with the vector of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=359" target="_blank">00:05:59.540</a></span> | <span class="t">scores that the model produces at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=362" target="_blank">00:06:02.520</a></span> | <span class="t">scores over the entire vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=365" target="_blank">00:06:05.500</a></span> | <span class="t">It's the difference between the one-hot vector and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=368" target="_blank">00:06:08.620</a></span> | <span class="t">the score vector that we use to get gradient updates to improve the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=373" target="_blank">00:06:13.420</a></span> | <span class="t">Again, I'm emphasizing this because we tend to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=376" target="_blank">00:06:16.020</a></span> | <span class="t">think that the model has predicted tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=378" target="_blank">00:06:18.540</a></span> | <span class="t">But in fact, predicting tokens is something that we make them do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=381" target="_blank">00:06:21.740</a></span> | <span class="t">What they actually do is predict score vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=385" target="_blank">00:06:25.500</a></span> | <span class="t">What's depicted on the slide here is teacher forcing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=389" target="_blank">00:06:29.300</a></span> | <span class="t">There's an interesting thing that happened at this time step where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=392" target="_blank">00:06:32.500</a></span> | <span class="t">the score vector actually put the highest score on the final element here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=396" target="_blank">00:06:36.780</a></span> | <span class="t">which is different from the one-hot vector that we wanted to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=401" target="_blank">00:06:41.500</a></span> | <span class="t">In teacher forcing, I still use this one-hot vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=404" target="_blank">00:06:44.820</a></span> | <span class="t">down at the next time step to continue my predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=408" target="_blank">00:06:48.300</a></span> | <span class="t">There are versions of training where I would instead use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=411" target="_blank">00:06:51.820</a></span> | <span class="t">the one-hot vector that had a one here at the next time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=415" target="_blank">00:06:55.380</a></span> | <span class="t">and that can be useful for introducing some diversity into the mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=419" target="_blank">00:06:59.440</a></span> | <span class="t">That is also a reminder that these models don't predict tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=423" target="_blank">00:07:03.420</a></span> | <span class="t">they predict score vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=424" target="_blank">00:07:04.760</a></span> | <span class="t">In principle, even in training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=427" target="_blank">00:07:07.340</a></span> | <span class="t">we could use their predicted score vectors in lots of different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=431" target="_blank">00:07:11.160</a></span> | <span class="t">We could do some beam search and use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=433" target="_blank">00:07:13.560</a></span> | <span class="t">the entire prediction that they make over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=435" target="_blank">00:07:15.800</a></span> | <span class="t">beam search to do training for future time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=438" target="_blank">00:07:18.740</a></span> | <span class="t">We could pick the lowest scoring item if we wanted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=441" target="_blank">00:07:21.440</a></span> | <span class="t">This is all up to us because fundamentally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=444" target="_blank">00:07:24.340</a></span> | <span class="t">what these models do is predict score vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=447" target="_blank">00:07:27.960</a></span> | <span class="t">That was for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=450" target="_blank">00:07:30.120</a></span> | <span class="t">Our actual focus is on frozen language models for this unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=453" target="_blank">00:07:33.520</a></span> | <span class="t">and so we're really going to be thinking about generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=456" target="_blank">00:07:36.600</a></span> | <span class="t">Let's think about how that happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=458" target="_blank">00:07:38.880</a></span> | <span class="t">Let's imagine that the model has been prompted with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=461" target="_blank">00:07:41.800</a></span> | <span class="t">the beginning of sequence token and the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=464" target="_blank">00:07:44.480</a></span> | <span class="t">and it has produced the token rock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=466" target="_blank">00:07:46.960</a></span> | <span class="t">We use rock, the one-hot vector there as the input to the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=471" target="_blank">00:07:51.720</a></span> | <span class="t">We process that and make another prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=475" target="_blank">00:07:55.140</a></span> | <span class="t">In this case, we could think of the prediction as rolls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=478" target="_blank">00:07:58.640</a></span> | <span class="t">Rolls comes in as a one-hot vector at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=480" target="_blank">00:08:00.760</a></span> | <span class="t">the next time step and we continue our predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=484" target="_blank">00:08:04.280</a></span> | <span class="t">That's the generation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=486" target="_blank">00:08:06.280</a></span> | <span class="t">Again, I want to emphasize that at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=489" target="_blank">00:08:09.660</a></span> | <span class="t">the model is predicting score vectors over the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=494" target="_blank">00:08:14.380</a></span> | <span class="t">We are using our own rule to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=496" target="_blank">00:08:16.880</a></span> | <span class="t">decide what token that actually corresponds to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=499" target="_blank">00:08:19.900</a></span> | <span class="t">What I've depicted here is something that you might call greedy decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=503" target="_blank">00:08:23.520</a></span> | <span class="t">where the highest scoring token at each time step is used at the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=509" target="_blank">00:08:29.600</a></span> | <span class="t">But again, that just reveals that there are lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=512" target="_blank">00:08:32.720</a></span> | <span class="t">decision rules that I could use at this point to guide generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=517" target="_blank">00:08:37.220</a></span> | <span class="t">I mentioned beam search before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=519" target="_blank">00:08:39.000</a></span> | <span class="t">that would be where we do a rollout and look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=521" target="_blank">00:08:41.520</a></span> | <span class="t">all the score distributions that we got for a few time steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=524" target="_blank">00:08:44.640</a></span> | <span class="t">and pick one that seems to be the best scoring across that whole sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=528" target="_blank">00:08:48.460</a></span> | <span class="t">which could yield very different behaviors from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=531" target="_blank">00:08:51.280</a></span> | <span class="t">the behavior that we get from greedy decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=534" target="_blank">00:08:54.160</a></span> | <span class="t">If you look at the APIs for our really large language models now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=538" target="_blank">00:08:58.480</a></span> | <span class="t">you'll see that they have a lot of different parameters that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=541" target="_blank">00:09:01.760</a></span> | <span class="t">essentially shaping how generation actually happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">That is again a reminder that generation is not really intrinsic to these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=550" target="_blank">00:09:10.480</a></span> | <span class="t">What's intrinsic to them is predicting score vectors over the vocabulary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=554" target="_blank">00:09:14.800</a></span> | <span class="t">and the generation part is something that we make them do via a rule that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=559" target="_blank">00:09:19.740</a></span> | <span class="t">decide separately from their internal structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=563" target="_blank">00:09:23.960</a></span> | <span class="t">That queues up a nice question that you could debate with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=567" target="_blank">00:09:27.600</a></span> | <span class="t">your fellow researchers and friends and loved ones and people out in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=572" target="_blank">00:09:32.200</a></span> | <span class="t">Do autoregressive language models simply predict the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=576" target="_blank">00:09:36.720</a></span> | <span class="t">Well, your first answer might be yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=578" target="_blank">00:09:38.840</a></span> | <span class="t">that's all they do, and that is a reasonable answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=581" target="_blank">00:09:41.740</a></span> | <span class="t">However, we just saw that it's more precise to say that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=586" target="_blank">00:09:46.600</a></span> | <span class="t">predict scores over the entire vocabulary at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=590" target="_blank">00:09:50.480</a></span> | <span class="t">and then we use those scores to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=592" target="_blank">00:09:52.720</a></span> | <span class="t">compel them to predict some token or other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=595" target="_blank">00:09:55.380</a></span> | <span class="t">We compel them to speak in a particular way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=599" target="_blank">00:09:59.240</a></span> | <span class="t">That feels more correct at a technical level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=602" target="_blank">00:10:02.360</a></span> | <span class="t">You might reflect also that they actually represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=605" target="_blank">00:10:05.860</a></span> | <span class="t">data in their internal and output representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=608" target="_blank">00:10:08.900</a></span> | <span class="t">and very often in NLP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=610" target="_blank">00:10:10.680</a></span> | <span class="t">it's those representations that we care about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=612" target="_blank">00:10:12.840</a></span> | <span class="t">not any particular generation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=615" target="_blank">00:10:15.700</a></span> | <span class="t">That just points to the fact that autoregressive LMs do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=618" target="_blank">00:10:18.360</a></span> | <span class="t">a lot more than just speak, so to speak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=622" target="_blank">00:10:22.080</a></span> | <span class="t">But on balance, I would say that it's saying they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=626" target="_blank">00:10:26.580</a></span> | <span class="t">simply predict the next token might be the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=629" target="_blank">00:10:29.000</a></span> | <span class="t">in terms of science communication with the public.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=631" target="_blank">00:10:31.460</a></span> | <span class="t">You can talk in nuanced ways with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=633" target="_blank">00:10:33.400</a></span> | <span class="t">your fellow researchers about what they're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=635" target="_blank">00:10:35.380</a></span> | <span class="t">doing and how they represent examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=637" target="_blank">00:10:37.420</a></span> | <span class="t">But out in the world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=638" target="_blank">00:10:38.700</a></span> | <span class="t">it might give people the best mental model if you simply say that they predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=642" target="_blank">00:10:42.900</a></span> | <span class="t">the next token based on the tokens that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=645" target="_blank">00:10:45.280</a></span> | <span class="t">have already generated and the ones that you put in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=647" target="_blank">00:10:47.820</a></span> | <span class="t">It's an appropriately mechanistic explanation that I think might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=651" target="_blank">00:10:51.160</a></span> | <span class="t">help people out in the world calibrate to what's actually happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=655" target="_blank">00:10:55.280</a></span> | <span class="t">Because we should even remind ourselves as we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=658" target="_blank">00:10:58.780</a></span> | <span class="t">more impressive behaviors from these models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=661" target="_blank">00:11:01.400</a></span> | <span class="t">underlyingly the mechanism is uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=664" target="_blank">00:11:04.580</a></span> | <span class="t">If you prompt the model with better late than, and it says never,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=668" target="_blank">00:11:08.680</a></span> | <span class="t">transparently we can see that that's just a high probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=672" target="_blank">00:11:12.260</a></span> | <span class="t">continuation of the prompt sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=674" target="_blank">00:11:14.940</a></span> | <span class="t">When you have every day I eat breakfast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=677" target="_blank">00:11:17.300</a></span> | <span class="t">lunch, and it will probably say dinner and you might immediately think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=681" target="_blank">00:11:21.540</a></span> | <span class="t">that reflects some world knowledge that the model has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=684" target="_blank">00:11:24.620</a></span> | <span class="t">But as far as we know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=686" target="_blank">00:11:26.900</a></span> | <span class="t">all that really is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=688" target="_blank">00:11:28.180</a></span> | <span class="t">is a continuation of the sequence with a high probability token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=692" target="_blank">00:11:32.540</a></span> | <span class="t">It's high probability because of regularities in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=695" target="_blank">00:11:35.700</a></span> | <span class="t">But for the language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=697" target="_blank">00:11:37.140</a></span> | <span class="t">this is simply a high probability continuation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=700" target="_blank">00:11:40.580</a></span> | <span class="t">Again, when you prompt it with the president of the US is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=704" target="_blank">00:11:44.060</a></span> | <span class="t">and it gives you the name of a person as an answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=706" target="_blank">00:11:46.900</a></span> | <span class="t">that might look like it has stored some knowledge about the world,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=710" target="_blank">00:11:50.100</a></span> | <span class="t">and maybe there is a sense in which it has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=712" target="_blank">00:11:52.480</a></span> | <span class="t">But as far as we know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=713" target="_blank">00:11:53.880</a></span> | <span class="t">and mechanistically, that is simply offering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=716" target="_blank">00:11:56.660</a></span> | <span class="t">a high probability continuation of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=719" target="_blank">00:11:59.780</a></span> | <span class="t">When you get to something like the key to happiness is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=723" target="_blank">00:12:03.220</a></span> | <span class="t">and it offers you an answer that seems insightful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=726" target="_blank">00:12:06.260</a></span> | <span class="t">you should again remind yourself that that is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=728" target="_blank">00:12:08.700</a></span> | <span class="t">a high probability continuation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=731" target="_blank">00:12:11.320</a></span> | <span class="t">the input sequence based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=733" target="_blank">00:12:13.260</a></span> | <span class="t">all the training experience that the model has had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=736" target="_blank">00:12:16.740</a></span> | <span class="t">We really have no ability to audit what those training sequences were like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=740" target="_blank">00:12:20.800</a></span> | <span class="t">The mechanism is uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=742" target="_blank">00:12:22.420</a></span> | <span class="t">There might be something interesting happening in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=744" target="_blank">00:12:24.740</a></span> | <span class="t">representation under the hood here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=747" target="_blank">00:12:27.660</a></span> | <span class="t">But we should remind ourselves that really it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=750" target="_blank">00:12:30.200</a></span> | <span class="t">high probability continuations for all of these cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=754" target="_blank">00:12:34.160</a></span> | <span class="t">The final core concept that I want to mention here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=758" target="_blank">00:12:38.140</a></span> | <span class="t">is that one that we're going to return to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=759" target="_blank">00:12:39.980</a></span> | <span class="t">at various points throughout the series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=762" target="_blank">00:12:42.260</a></span> | <span class="t">This is this notion of instruction fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=765" target="_blank">00:12:45.820</a></span> | <span class="t">This is from the blog post that announced chat GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=770" target="_blank">00:12:50.980</a></span> | <span class="t">It's a description of how they do instruct fine-tuning for that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=774" target="_blank">00:12:54.800</a></span> | <span class="t">There are three steps here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=776" target="_blank">00:12:56.580</a></span> | <span class="t">I think the thing to highlight is that in step 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=779" target="_blank">00:12:59.700</a></span> | <span class="t">we have what looks like fairly standard supervised learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=784" target="_blank">00:13:04.300</a></span> | <span class="t">where at some level we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=785" target="_blank">00:13:05.980</a></span> | <span class="t">human curated examples of prompts with good outputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=789" target="_blank">00:13:09.500</a></span> | <span class="t">and the model is trained on those instances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=793" target="_blank">00:13:13.180</a></span> | <span class="t">Then at step 2, we again have humans coming in now to look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=798" target="_blank">00:13:18.440</a></span> | <span class="t">model outputs that have been generated and rank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=801" target="_blank">00:13:21.460</a></span> | <span class="t">them according to quality conditional on the prompt input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=805" target="_blank">00:13:25.220</a></span> | <span class="t">That's two stages at which people are playing a crucial role.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=810" target="_blank">00:13:30.160</a></span> | <span class="t">We have left behind the very pure version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=813" target="_blank">00:13:33.560</a></span> | <span class="t">the distributional hypothesis that says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=816" target="_blank">00:13:36.020</a></span> | <span class="t">just doing language model training of the sort I described before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=820" target="_blank">00:13:40.540</a></span> | <span class="t">on entirely unstructured sequence symbols,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=823" target="_blank">00:13:43.280</a></span> | <span class="t">gives us models that are powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=825" target="_blank">00:13:45.280</a></span> | <span class="t">We have now entered back into a mode where a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=828" target="_blank">00:13:48.520</a></span> | <span class="t">the most interesting behaviors are certainly happening because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=831" target="_blank">00:13:51.980</a></span> | <span class="t">people are coming in to offer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=834" target="_blank">00:13:54.180</a></span> | <span class="t">direct supervision about what's a good output given an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=838" target="_blank">00:13:58.300</a></span> | <span class="t">It's not magic when these models seem to do very sophisticated things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=842" target="_blank">00:14:02.120</a></span> | <span class="t">It is largely because they have been instructed to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=845" target="_blank">00:14:05.220</a></span> | <span class="t">very sophisticated things by very sophisticated humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=848" target="_blank">00:14:08.960</a></span> | <span class="t">That is important in terms of understanding why these models work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=852" target="_blank">00:14:12.420</a></span> | <span class="t">and I think it's also important for understanding how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=855" target="_blank">00:14:15.500</a></span> | <span class="t">various in-context learning techniques behave because increasingly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=860" target="_blank">00:14:20.040</a></span> | <span class="t">we're seeing a feedback loop where the kinds of things that we want to do with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=864" target="_blank">00:14:24.200</a></span> | <span class="t">our prompts are informing the kinds of things that happen in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=867" target="_blank">00:14:27.420</a></span> | <span class="t">the supervised learning phase making them more powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=871" target="_blank">00:14:31.000</a></span> | <span class="t">Again, it's not a mysterious discovery about how large language models work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=875" target="_blank">00:14:35.640</a></span> | <span class="t">but rather just a reflection of the kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=878" target="_blank">00:14:38.460</a></span> | <span class="t">instruct fine-tuning that are very commonly happening now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=7OOCV8XfMbo&t=882" target="_blank">00:14:42.740</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>