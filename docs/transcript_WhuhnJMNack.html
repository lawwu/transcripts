<html><head><title>Live coding 16</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Live coding 16</h2><a href="https://www.youtube.com/watch?v=WhuhnJMNack" target="_blank"><img src="https://i.ytimg.com/vi/WhuhnJMNack/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=0 target="_blank"">0:0</a> Start<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=64 target="_blank"">1:4</a> About Weighting (WeightedDL)<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=110 target="_blank"">1:50</a> Curriculum Learning / Top Losses<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=188 target="_blank"">3:8</a> Distribution of the test set vs training set<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=215 target="_blank"">3:35</a> Is Curriculum Learning related to Boosting?<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=265 target="_blank"">4:25</a> Focusing on examples that the model is getting wrong<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=278 target="_blank"">4:38</a> Are the labels ever wrong? By accident, or intentionally?<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=400 target="_blank"">6:40</a> Image annotation issues: Paddy Kaggle discussion 4<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=503 target="_blank"">8:23</a> UNIFESP X-ray Body Part Classifier Competition 4<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=620 target="_blank"">10:20</a> Medical images / DICOM Images<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=657 target="_blank"">10:57</a> fastai for medical imaging<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=700 target="_blank"">11:40</a> JPEG 2000 Compression<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=760 target="_blank"">12:40</a> ConvNet Paper<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=830 target="_blank"">13:50</a> On Research Field<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=930 target="_blank"">15:30</a> When a paper is worth reading?<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1034 target="_blank"">17:14</a> Quoc V. Le<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1070 target="_blank"">17:50</a> When to stop iterating on a model? - Using the right data.<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1210 target="_blank"">20:10</a> Taking advantage of Semi-Supervised Learning, Transfer Learning<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1293 target="_blank"">21:33</a> Not enough data on certain category. Binary Sigmoid instead of SoftMax<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1430 target="_blank"">23:50</a> Question about submitting to Kaggle<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1533 target="_blank"">25:33</a> Public and private leaderboard on Kaggle<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1770 target="_blank"">29:30</a> Where did we get to in the last lesson?<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=1880 target="_blank"">31:20</a> GradientAccumulation on Jeremy’s Road to the Top, Part 3<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2240 target="_blank"">37:20</a> “Save & Run” a Kaggle notebook<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2335 target="_blank"">38:55</a> Next: How outputs and inputs to a model looks like<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2455 target="_blank"">40:55</a> Next: How the “middle” (convnet) of a model looks like<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2492 target="_blank"">41:32</a> Part 2: Outputs of a hidden layer<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2573 target="_blank"">42:53</a> The Ethical Side<br><a href="https://www.youtube.com/watch?v=WhuhnJMNack&t=2670 target="_blank"">44:30</a> fastai1/courses/dl1/excel<br><h3>Transcript</h3><div class='max-width'><p>I am recording now so, but please keep talking. Don't be shy, Serada. It's okay. Oh, getting a bit noisy out here with street cleaning something. I think your headset is very good. Oh, good. I didn't hear anything so. I mean, I say street cleaning. It's more like footpath cleaning. We have a walking path along the front of our house. Oh, come on. I start the press in the recording, but that everybody stops talking. Well, you know. People don't want to just hear my voice all the time on these recordings, guys. There's things I wanted to cover in today's session, but then the responsible part of me says I probably ought to create a lesson before Tuesday's class, so maybe we'll do that. I've got a question, Jeremy. I had to leave before you finished that code change yesterday. Did you, was that actually, do you want to recap on where we got to with waiting with data levels? Probably not because you can just watch the video and so, like, otherwise, I guess we're just doing it. So, is it working now? Yeah, yeah, it's all good. You know, I mean, it's, the concept is working correctly in terms of the code. We didn't, like, get a better score, but I didn't particularly expect to either. You know, maybe after next Tuesday's lesson, we will revisit it because I actually think the main thing it might be useful for is what's called curriculum learning, which is basically focusing on the hard bits. Looks like Nick's internet still isn't working, but Nick was saying the other day that he looked at which ones we're having the errors on, which is like what we've, what we look at in the book. Like, looking at the classification interpretation and looking at, like, plot top losses and stuff and he said, like, yeah, all the ones that we're making, that we're getting wrong are basically from the same one or two classes. So, I haven't done much with curriculum learning in practice, like I, like all it means in theory is that we use our weighted data loader to weight the ones that we're getting wrong higher. And whether that will actually give us a better result or not, I'm not sure, but that I think that's more likely to be a useful path than simply reweighting things to be more balanced because we don't want things to be more balanced because the ones that we observe the most often in the test set are actually the ones we want to be the best at, you know. I will say I didn't check whether the distribution of the test set is the same as the training set. If it's randomly selected, then it will be, and if it's not, then that would be a reason to use a weighted data loader as well. Yeah. Okay, so what's the difference, I mean, I guess like what's the, is it is a curriculum learning kind of related to boosting and conceptually? Not really. I mean, maybe. So boosting is where you calculate the difference between the actuals and your predictions to get residuals, and then you create a model that tries to predict the residuals. And then you can add those two predictions together, which is, if not done carefully, is a recipe for overfitting. But if done carefully can be very effective. Yeah, we're talking about something which is conceptually very different, which is saying like, oh, we're like really bad at recognizing this category, so let's show that category more often during training. That's a good question. Of kind of focusing on examples, you're getting it wrong, like more conceptually doing something similar. I was just gonna ask, are the labels ever wrong, like by accident, or intentionally in Kaggle? Of course, absolutely. So, or both intentionally as well? No, not intentionally. I mean, not normally, like sometimes there might be a competition where they say like, oh, this is a synthetically generated data set, and some of the data is wrong. Because we're trying to do something like what happens in practice, but we can't share the real data. So is there any advantage in trying something like some uncertainty values from something like MC Dropout, try to find like a threshold of things that are too difficult, and then potentially they're wrongly labeled? I'm not sure you would need that, like the thing we use in the book and the course is simply to find the things that we are confident of, but we know we're wrong, but turn out to be wrong, and then just look at the pictures. Is that max values enough, you think, to basically know whether or not it fits? I do, yeah. I mean, that seems to work pretty well. I mean, the only thing is you would need to be able to recognize these things in photos. But I'm sure if you spend an hour reading on the internet about what these different diseases are and how they look, you would be able to pick it up faster soon enough. And then, you know, just like we did in chapter two for recognizing the things that aren't black and brown and teddy bears. So plausibly, even just knocking out some of the extremely difficult examples might get you higher on the leaderboard purely by virtue of them misleading them. Not by knocking out the hard ones, but by knocking out the wrong ones, yes. Unless the test set is mislabeled consistently with the training set, in which case you would not want to knock them out because you would want to be able to correctly predict the things which people are incorrectly recognizing as the wrong disease. Something to try, though. Yeah. Yeah. So I would do exactly what we did in chapter two. You know, you can use exactly the same widget. But as I say, you'd have to probably spend an hour learning about rice disease, which probably be a reasonably interesting thing to do anyway. I just a link. There's a discussion in the Patty. Some people identify there's some mislabeling at least over 20 already. Yeah. Yeah. So it definitely happens. It says we have manually annotated every image with the help of agricultural experts, but there could be errors. Well, this person knows more about rice than I do. I think the images in the tongue grow have a chance of issues. The symptoms can easily be confused with potassium deficiency. Fair enough. Is that an example of what you're talking about where if layman or sorry, if a semi expert gets confused, then the labeling in the test sets probably the same. Yeah, exactly. So you're probably fixing these would probably screw up your model because assuming that the test set was labeled used by the same people in the same way. I mean, sometimes test sets. The test set is more of a gold standard. They'll make more effort to talk to like a larger number of high quality experts and have them vote or something. Honestly, this competition seems like it doesn't even have any prize money attached. So like, I think it's really low, low investment, probably. And so I doubt they did that. But, but that can happen. Yeah, that the test set could have. I mean, it makes sense to invest in getting really good labels for the test set. I was looking at one of the other competitions on UNIFESP, the x-rays. And I think there there was one, somebody had identified that a wrist was wrongly labeled as something like that. Yeah, it's not, there's no money again but it's been running for a while. What's it called? UNIFESP, U-N-I-F-E-S-P. It's another community competition. Gosh, it's not very popular. Why is there only 70-14s? Yeah, sorry, go on. Yeah, I don't know. I was just looking around and it looked interesting. So I'm number 15 at the moment. But it is a slightly weird one because, well, it's interesting because some of the x-rays have multiple labels, but the labels are just concatenated. It's an interesting discussion on how you'd analyze that. Would you treat a combination as a distinct classification, whether it was like a neck and a chest or something? Or do you look at each of them individually and then try and label a multiple one from the different things? Okay, so I'm just having a look at this competition. So when does it close? This is a month to go, but I don't know. Exactly when that is. Normally, there's July 31st. Okay, where do you see that? When you go to the bottom of on the overview and it says there's a whole timeline. So then you just hover over that. Oh my God, I see. It says close in a month, but you actually have to get a toolkit by hovering. Okay, thanks, Tanishk. That's strange. It works. Okay, so we've actually got more than a month, so maybe next week we could have a look at this one because it would be a good opportunity to play around with medical image stuff because they're using DICOM, I think. Yeah, somebody has also, which I used, supplied a library of PNGs, which made it easier to use, but I don't know what you'd lose in using that rather than the DICOM images. It rather depends. So DICOM is a very generic file format that can contain lots of different things in it, but one of the things DICOM contains is higher bit depth images than a PNG allows. So if they've, yes, they might have gotten rid of that. FastAI has a nice medical imaging, pretty small but like has some useful stuff, Medical Imaging Library, which I think is fastai.vision.medical, which can handle DICOM directly. And I see there's a FastAI entry as well. That'd be fun. We should try this next week. I see there's the PNGs. I think the DICOM has come to about 27 gigabytes. Oh my God. So the PNG was quite attractive from that point of view. So one thing that you can do with DICOM is to compress them, particularly using JPEG 2000, which is a really good compression. But yeah, people often don't for some reason. So probably the first thing I'd look at in that competition is to look at DICOM and see is it storing 16 bit data or not. And if it is, I would try to find a way to resave that without losing the extra information. Which I think we've got examples of in our medical imaging tutorial. I'll take a look at that. All right. I'm going to share my screen, even though I don't know what I'm doing. I'm going to have to drop in a few minutes, but I'll catch the rest on the recording. Thanks for this. Nice to see you. By the way, I was looking at this ConfNEXT paper. And gosh, everybody congratulates transformers on everything. Vision transformers bring new ideas like the Adam W Optimizer. I guess who actually wrote the first thing saying we should always use the Adam W Optimizer. Be silver. In fast AI, I think that was years before vision transformers. Adam W. There we go. Mid 2018. I read that paper last night and I was just thinking like they kind of talk about how all of these things were already there, right? They just rediscovered them like slightly larger kernel size and things like that, which begs the question, why is it no one just done like experiments to just tweak these things together? I mean, we do, but nobody takes any notice because they're not written in PDFs, you know? Is it, I mean, these benchmarks, they're like... The thing is that like a lot of researchers aren't good practitioners. So they just, they're not very good at training accurate neural networks and they don't know these tricks, you know, and they don't hang out on Kaggle and learn about what actually works. So, but then the thing is like, it's not always easy to publish, like even if you did stick it into a PDF and submit it to NeurIPS, there's no particular high likelihood that they're going to accept it because the field research wise is very focused on theory results and, you know, things with lots of Greek letters in them. Does that mean that the part of the problem is that the data sets, the benchmarks are just too inaccessible to the average person? No, I wouldn't say that. No, I wouldn't say that. The issue is I think the culture of research is not particularly interested in experimental results, you know? With my limited experience, I will say it's very hard to find reviewers as well, especially you have a very strong domain, not just running all the sample data set you can find in open source. When you call the domain and then a lot of peer reviewers, they're just not picking up to review it. Even if we pay for the reviewers we're using so people can get it for free and we take a few months just to find reviewers. Jeremy, so on the topic of papers, when do you know when a paper is worth reading, given the situation? You don't? I mean, I'm very fond of papers that describe things which did very well in an actual competition, you know, that then we know this is something that actually predicts things accurately. You know, you can get similar results if they've got a good, you know, just table of results. So generally speaking, I like things that actually have good results, particularly if they show like how long it took to check. Like how long it took to train and how much data they trained on. And yeah, so are they getting good results using less data and less time than you might expect from the same thing? And yeah, I certainly wouldn't focus only on those that get good results on really big data sets. That's not necessarily more interesting. I'm very interested in things that show good results using transfer learning. It's not practically useful. I don't train that much from random. So I'm very interested in things that do well in transfer learning. Also, like, look for people who you've liked their work before, you know, and in particular, that doesn't mean like always reading the latest papers. You know, if you come across a paper from somebody that you find useful, go back and look at their Google Scholar and read their older papers. See who they collaborate with and read their papers. So, for example, I really like Kwok Lee in Google Brain. He and his team do a lot of good work. It tends to be, you know, very practical and high quality results. And so we know when his team releases a paper, I also know like he seems to have similar interests in mind, like he tends to do stuff involving transfer learning and getting good results in less epochs and stuff like that. So if I say he's got a new paper out, I'm pretty likely to read it. I have a question. And I mean, for for the category competitions and like like in a lab type of environment is, I mean, when the question that I have is when to stop iterating on a model on a model that that you have is is I have the someone asked me when is enough enough to do the training on the data that you have. When is enough. So that question, I mean. There's some reason you're doing this work, right? So like you hopefully know when it does what you want it to do. I mean, the thing that happens all the that happens to me all the time is that it trained the model and it works perfectly fine on the lab when we're doing it. And then as soon as we throw a couple of images that they are not part of the set. I mean, that goes nuts. OK. So that's like light or more light or the temperature or stuff like that. Well, that's a different problem, right? So that that means your problem is that you're you're not using the, you know, the right data to train on. So like you need to you. You need to be thinking about how you're going to deploy this thing. When you train it and if you train it with data that's different to how you're going to deploy it, it's not going to work. Yeah, so that's that's what that means. And. It might be difficult to get data, enough data of the kind you're going to deploy it on. But like at some point, you're going to be deploying this thing, which means by definition, you've got some way of getting that data you're going to deploy it with. So like do the exact thing you're going to use to deploy it, but don't deploy it. Just capture that data until you've got some actual data from the actual environment you want to deploy the model in. You can also take advantage of semi supervised learning techniques to then, you know, and transfer learning to maximize the amount of juice you get from that data that you've collected. And finally, I'd say, like, let's say for medical imaging, like, okay, you want to deploy a model to like a new hospital, they've got a different brand of MRI machine you haven't seen before. I would take advantage of fine tuning, you know, each time I deployed it to some different environment where things a bit different, I would expect to have to go through a fine tuning process to train it to recognize that particular MRI machines images. But you know, each time you do that fine tuning, it shouldn't take very much data or very much time because it's your models already learnt the key features, and you're just asking it to learn to recognize slightly different ways of seeing those features. Yeah, I don't think you'll solve this by training for longer, you know, you'll solve it by figuring out your, your data pipeline your data labeling and your rollout strategy. Usually the issues that we're having is that we don't have enough data of a certain category. But, I mean, the thing that you did yesterday, it results a little bit of that problem I think we're going to start using. Yeah, well also like, if you don't have enough data of some category, don't use the model for that category, you know, so like you know, rather than using softbacks use binary sigmoid, you know, as your last layer and so then you've kind of got like a probability that x appears in this image and so then you can you can recognize when none of the things that you can predict well appear in the image. And so, then have a, you know, you always want a human in the loop anyway. So when you didn't find any of the categories of things you've got enough data to be able to find then triage those to human review. One thing that we did is, I mean we have like 50 something categories, just one moment, hang on. Sorry about that. So, we had like 50 categories and some of them are like, they have a lot like 10 of them have a lot of items. So we end up doing like in a three step kind of process, like the ones with a lot, the ones with medium number. With a smaller number, and it looks like it resolved the problem a little bit. Cool, but this was to classify metadata coming from, from other systems and classified for legal purposes for legal retention. I see. Got it. I had a question actually, you tried the weighted data loaders right so I think you submitted that to Kaggle notebook. So, did you do any validation locally first before submitting to Kaggle, something like that. No, I mean you saw what I did right. And when I did it, so I just. I just like I was intentionally using a very mechanistic approach. Because it was part of like just showing like here's the basic steps of pretty much any computer vision model which is entirely mechanical and doesn't require any domain expertise. So yeah, my question more was like, shouldn't be always treat the public leaderboard like a good or like should we take a hold out local data set first to validate. Yeah, so I mean, I always have a validation set. Yeah, which we saw in this, and this I just used a random splitter, because as far as I know the test set in the Kaggle competition is a randomly split validation set. Yeah, so like, whether it be for Kaggle or anything. I think creating a validation set that as closely as possible represents the, the data you expect to get in deployment or in your test set is really important. And, yeah, I actually didn't spend the time doing that on this patty competition. Normally on Kaggle if somebody does and notices there's a difference between the private leaderboard and the public leaderboard like the test set and the training set normally it'll appear, you know, in discussions or on a Kaggle kernel or something, which is partly why I didn't look into it but yeah I mean you should probably check, doesn't have the same distribution of disease types, you know, from the predictions that you create. Do the images look similar, do they have the same sizes. And for me if I said as I see any difference between the test set and the trading set that puts my alarm bells on, right, because now I know that's not randomly selected. And if you know it's not randomly selected then you immediately have to think okay they're trying to trick us. So, I would then look everything I could for differences. Because it takes effort to not randomly select a trade a test set so they must be doing it very intentionally for some reason. I think so, like I don't think a Kaggle competition should ever silently give you a systematically different test set. I think there's great reasons to create a systematically different test set, but there's never a reason not to tell people. So if it's like medical imaging is a different hospital you should say this is a different hospital or if it's fishing you should say these are different boats, or, you know, because like you want people to do well on your data, so if you tell them, then they can use that information to give you better models. So, Korean, like, going back to what you asked about, there's this validation in training, then there's this, whether your local validation maps to what's happening on the leaderboard, the score on the hidden test set. But there's one other scenario that I encountered recently, and maybe it would be interesting to someone. When you're working on a competition, sometimes you might miss something in your code or the prediction. You know your model is doing something useful but you're failing to output a correctly formatted submission file, and not in a sense that the submission fails on Kaggle, but some predictions are not aligned where there should be or, you know, therefore a different customer ID or stuff like that. So, once you have one good submission file, relatively good, you can just store it locally and then see, you know, run a check the correlation between your new submission and the one that you know that this, okay, and you know the correlation should be upwards of 0.9, and then you know yeah okay so I didn't mess up anything with the technical aspect of outputting the prediction. I mean, it's not a great trick but, you know, I was like putting my hair out, why is this not working with a better model. So this was like a sanity checks that maybe at some point. Thanks. Cool. Thanks. All right. So, let me share my screen. Let's find zoom. Here's my screen. Oh, that's not the right button. Control up shift H. Okay. Where did we get to. In the last lesson. We finished random forests right. And Oh that's right and I haven't posted that video yet. That's okay we can check live. Last year live. Okay, so we could small models. Until we get to the end of this. Okay, so that basically. So we basically finished the second one of our Kaggle things. So next week. See what's in part three. Right, gradient accumulation. I think that's worth covering. So one thing that somebody pointed out on Kaggle is I've actually, I'm using gradient accumulation wrong. I was passing in two here to mean to make create two batch like do two batches before you accumulate. But actually what I meant to be putting in here is the kind of target batch size I want. So that would be actually I should be putting 64 here. So I feel a bit stupid. So what I've been doing is I've been actually not using gradient growth accumulation at all I guess it's been doing a batch and saying that's over. I'm saying my maximum batch size should be two. Okay, so this has actually been not working at all. That's interesting. Oops. So it's been using a batch size of 32 and not accumulating. Okay. So that's one thing to note. So when I get Kaggle GPU time again, we'll need to rerun this. Actually, it only took 4000 seconds. So I guess we should, we could just get it running right now, couldn't we? So that should be 64. How many paths defines how large the effective batch size you want is. Over batches. We can just remove this sentence entirely. We divide the batch size by some number based on how small we need it to be. It to be for our GPUs. Okay. So. And on Kaggle I think these were all smaller I don't know why but the Kaggle GPUs use less memory than my GPU for some reason. Okay. So right now. Let's try running it. Jeremy, you would increase the outcome number until no longer get good out of memory. Yeah, and you could be at a pretty much guess it by looking at like, I mean you can just. Once you found a batch size that fits, you know, so the default batch size I believe is 32. Once you find a batch size that fits, sorry 64 is the default batch size it fits you just like, it's like okay well if it fits in 32 then I just need to set it to two because 64 divided by two is enough. And the key thing I do here is, you know, so I've got this report GPU function. So what I did at home was I just, you know, changed this until it got less than 16 gig. And as you can see I'm just doing like a single a park on small images so this ran in. I don't know 15 seconds or something. Yeah, batch size 64 by default. Yeah, so then I just went through checking the memory use of conflicts large or different image sizes, again just keeping on using just one epoch. And that's how I figured out what I could do to set a QM to trip to work. Alright, so that should be right to save and run. And then turn off this one. So when you're running something like he clicks a version, and you click run, you'll then see it down here. And that runs in the background you don't have to leave this open. And so you can go back to it later. So if I just copy that can close it. And if I go to my notebook in Kaggle this shows be version three or four because version four hasn't finished running yet. So if I click here, I can go to version four and it says all it's still running and I can see here it's been running for about a minute. And it shows me anything that you print out will appear, including warnings. So that's, yeah, that's what happens in Kaggle. So if we also do the multi objective loss function thing. That would be cool. So I thought like next time in our next lesson, broadly speaking. This is taking a long time. I kind of want to cover like what the inputs to a model look like and what the outputs to a model look like. So like in terms of inputs, really the key thing is embeddings. That's the key thing we haven't seen yet in terms of what model inputs look like. The model outputs, I think we need to look at softmax. Softmax cross entropy loss, entropy loss. And then, you know, our multi target loss, which we could do first kind of a segue. So maybe in terms of the ordering, the segue would be like doing multi target loss first. And we could talk about softmax and cross entropy, which would then lead us potentially to like looking at the bear classifier. What if there's no bears? So we can just use the binary sigmoid. So then for embeddings, I guess that's where we'd cover the collaborative filtering, collaborative filtering, because that's like a really nice version of embeddings. So I guess the question is, for those who have done the course before, are there any other topics? I guess like time permitting, it would be nice to look at like the conf net, what a conf net is. Just kind of say that. So then we've got like the outputs, the inputs, and then the middle. What about more NLP stuff? I know people like what? Well, I've heard that. I can face is going to integrate it with past day I may be looking at that how it works. Well, it's not done yet. So you can't do that yet, but definitely in part two. I got a question. I don't know if it's helpful, but there's a lot of emphasis on outputs and inputs. But like in the middle, just understanding like the outputs of a hidden layer, whether they're going awry or not, how do you kind of debug that? How do you understand when to kind of look at that? Yeah, very helpful. Last time we did a part two, we did a very deep dive into that. And I think we should do that again in a part two, because like most people won't have to debug that because if you're using an off-the-shelf model, you know, like it's, you know, with off-the-shelf initializations, that shouldn't happen. So it's probably more of an advanced debugging technique, I would say. But yeah, if you're interested in looking at it now, definitely check out our previous part two, because we did a very deep dive into that and developed the so-forth colorful dimension plot, which is absolutely great for that. Yeah. So that would exactly, so collaborative filtering would lead us exactly into that. Thank you. Yeah, sorry, Sarada. Do you like to spend finally talking about the importance of the ethical side? At least you point to the resources Rachel prepared before, so I think people, because it's so easy to build a model, but how to apply is getting more scary now. Yes. Yes, I mentioned in lesson one, the data ethics course, but you're right, it would be nice to kind of like touch on something there, wouldn't it? A lecture by Rachel from part one before. That was a great lecture. Yeah, I mean, okay, that actually would be a great thing just to talk about, you know, that that lecture is not at all out of date. Yes. So maybe touch on it in this one. So talk linked to, you know, for varying levels of interest, the two hour version would be Rachel's talk in the 2020 lecture, and then deeper interest you all would be the, yes, the full ethics course. That's a great point. Thank you. So then, for, for actually pretty much all of these things, we have Excel spreadsheets, which is fun. So there's, let's have a look collaborative filtering. Oh, looks like I've already downloaded that. Thank you, Jeremy, I will encourage you to continue teaching in Excel. Yesterday, I, on the panel in a data science conference, and when I mentioned I start with Excel actually inspire a lot of people, they want to have a go with data science and learning it. So please do feedback. Because there's certainly some people who don't find it useful at all. And they tend to be quite loud about about it so certainly nice to hear that, that feedback. What bother you, Jeremy. Sorry. So I thought you didn't let those people get to you. So, I only pretend that anybody doesn't get to me. I was gonna back up so I don't say that's that was really great to see. I've only seen it done once before. And that was in a physicist in Belgium who explained, ready to transfer modeling using Excel, and it was just so nice to see the clarity. Great. Okay, thank you. I will. Let's see, so we've got. So I think these are actually from the 2019 course faster one courses deal one. So I'm just going to grab them all. So one thing I don't think we're going to cover this year. This part one that we will cover in part two is like different optimizers like momentum and Adam and stuff, but I think that's okay because I feel like nowadays. Just use the default Adam W and it works. So I don't. I think it's fine. Not to know too much more than that. It's, it's a little bit of a technicality nowadays. Yeah. It used to be something we did in one of the first lessons you know but that was when you kind of had to know it right because you always fiddled around with momentum and blah blah blah. To me, always like the biggest thing when starting on something is to how to, you know, once I figure out how to read in the data, then, so I'm really grateful that there's such an emphasis in this edition of the course on the reading, you know, data, and, you know, data, that is something that we also stay on the lookout for just understanding better within the data. Great. I don't think we did this one anymore, because we kind of have better versions in. In Jupiter with iPod widgets so we've got this fun convolutions example. Which I think is still valuable. Okay, we've got soft max and cross entropy examples. And we've got collaborative filtering interesting wonder what that is. And then, also we've got word embeddings. Embeddings are such a cool and important subject, and it's something that we haven't discussed that much in this course. No, we haven't touched them at all. Great. All right. It feels like a lot to cover. That we will. We will do our best. Okay, I think we're up to our hour so thanks everybody. Nice chat today, and I will get to work on putting this together. Have a nice weekend. Thank you so much. Thanks. Everyone. With and bias video today. I think six o'clock basement time so with anyone interest. The guy mentioned he Thomas mentioned you're going to have another US session as well, you can join. Yes, I think there's details on the forum. Yeah. Thanks. See ya.</p></div></div></body></html>