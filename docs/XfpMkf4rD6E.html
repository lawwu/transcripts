<html><head><title>Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy</h2><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E"><img src="https://i.ytimg.com/vi/XfpMkf4rD6E/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=47">0:47</a> Introducing the Course<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=199">3:19</a> Basics of Transformers<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=215">3:35</a> The Attention Timeline<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=301">5:1</a> Prehistoric Era<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=370">6:10</a> Where we were in 2021<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=450">7:30</a> The Future<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=615">10:15</a> Transformers - Andrej Karpathy<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=639">10:39</a> Historical context<br><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3630">60:30</a> Thank you - Go forth and transform<br><br><div style="text-align: left;"><a href="./XfpMkf4rD6E.html">Whisper Transcript</a> | <a href="./transcript_XfpMkf4rD6E.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone. Welcome to CS25 Transformers United V2. This was a course that was held</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=11" target="_blank">00:00:11.000</a></span> | <span class="t">at Stanford in the winter of 2023. This course is not about robots that can transform into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=15" target="_blank">00:00:15.720</a></span> | <span class="t">cars, as this picture might suggest. Rather, it's about deep learning models that have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=19" target="_blank">00:00:19.980</a></span> | <span class="t">taken the world by the storm and have revolutionized the field of AI and others. Starting from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=24" target="_blank">00:00:24.460</a></span> | <span class="t">natural language processing, transformers have been applied all over, from computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=27" target="_blank">00:00:27.880</a></span> | <span class="t">vision, reinforcement learning, biology, robotics, etc. We have an exciting set of videos lined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=33" target="_blank">00:00:33.560</a></span> | <span class="t">up for you, with some truly fascinating speakers giving talks, presenting how they're applying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=38" target="_blank">00:00:38.720</a></span> | <span class="t">transformers to the research in different fields and areas. We hope you'll enjoy and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=46" target="_blank">00:00:46.280</a></span> | <span class="t">learn from these videos. So without any further ado, let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=52" target="_blank">00:00:52.200</a></span> | <span class="t">This is a purely introductory lecture, and we'll go into the building blocks of transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=59" target="_blank">00:00:59.000</a></span> | <span class="t">So first, let's start with introducing the instructors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=63" target="_blank">00:01:03.600</a></span> | <span class="t">So for me, I'm currently on a temporary deferral from the PhD program, and I'm leading AI at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=68" target="_blank">00:01:08.000</a></span> | <span class="t">a robotic startup, Collaborative Robotics, working on some general-purpose robots, somewhat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=73" target="_blank">00:01:13.000</a></span> | <span class="t">like a robot. And yeah, I'm very passionate about robotics and building efficient learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=78" target="_blank">00:01:18.920</a></span> | <span class="t">systems. My research interests are in reinforcement learning, computer vision, linear modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=83" target="_blank">00:01:23.800</a></span> | <span class="t">and I have a bunch of publications in robot technology and other areas. My undergrad was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=89" target="_blank">00:01:29.080</a></span> | <span class="t">at Cornell, it's a municipal Cornell, so nice to meet all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=93" target="_blank">00:01:33.960</a></span> | <span class="t">So I'm Stephen, I'm a first-year CSP speaker. Previously did my master's at CMU, and undergrad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=99" target="_blank">00:01:39.800</a></span> | <span class="t">at Waterloo. I'm mainly into NLP research, anything involving language and text. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=104" target="_blank">00:01:44.760</a></span> | <span class="t">more recently, I've been getting more into computer vision as well as multilingual. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=109" target="_blank">00:01:49.080</a></span> | <span class="t">just some stuff I do for fun, a lot of music stuff, mainly piano. Some self-promo, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=114" target="_blank">00:01:54.080</a></span> | <span class="t">I post a lot on my Insta, YouTube, and TikTok, so if you guys want to check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=118" target="_blank">00:01:58.840</a></span> | <span class="t">My friends and I are also starting a Stanford Piano Club, so if anybody's interested, feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">free to email me for details. Other than that, martial arts, bodybuilding, and huge fan of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=131" target="_blank">00:02:11.600</a></span> | <span class="t">K-dramas, anime, and occasional gamer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=134" target="_blank">00:02:14.880</a></span> | <span class="t">Okay, cool. Yeah, so my name's Ryland. Instead of talking about myself, I just want to very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=142" target="_blank">00:02:22.160</a></span> | <span class="t">briefly say that I'm super excited to take this class. I took it the last time it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=147" target="_blank">00:02:27.920</a></span> | <span class="t">offered, I had a bunch of fun. I thought we brought in a really great group of speakers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=152" target="_blank">00:02:32.640</a></span> | <span class="t">last time, I'm super excited for this offering. And yeah, I'm thankful that you're all here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=156" target="_blank">00:02:36.960</a></span> | <span class="t">and I'm looking forward to a really fun quarter together. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=159" target="_blank">00:02:39.360</a></span> | <span class="t">Yeah, so fun fact, Ryland was the most outspoken student last year, and so if someone wants</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=164" target="_blank">00:02:44.720</a></span> | <span class="t">to become an instructor next year, you know what to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=172" target="_blank">00:02:52.240</a></span> | <span class="t">Okay, cool. So what we hope you will learn in this class is, first of all, how do task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=180" target="_blank">00:03:00.320</a></span> | <span class="t">forms work? How they're being applied? And nowadays, like, we are pretty much everywhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=186" target="_blank">00:03:06.960</a></span> | <span class="t">in AI machine learning. And what are some new interesting directions of research in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=193" target="_blank">00:03:13.680</a></span> | <span class="t">this topics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=195" target="_blank">00:03:15.680</a></span> | <span class="t">Cool. So this class is just an introductory, so we'll be just talking about the basics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=201" target="_blank">00:03:21.280</a></span> | <span class="t">of transformers, introducing them, talking about the self-attention mechanism on which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=205" target="_blank">00:03:25.520</a></span> | <span class="t">they're founded, and we'll do a deep dive more on, like, models like BERT, GPT, stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=211" target="_blank">00:03:31.520</a></span> | <span class="t">like that. So, great, happy to get started. Okay, so let me start with presenting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=218" target="_blank">00:03:38.160</a></span> | <span class="t">attention timeline. Attention all started with this one paper, Attention is All You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=223" target="_blank">00:03:43.760</a></span> | <span class="t">Need, by Baswani et al. in 2017. That was the beginning of transformers. Before that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=229" target="_blank">00:03:49.760</a></span> | <span class="t">we had the prehistoric era, where we had models like RNNs, LSTMs, and their simple attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=236" target="_blank">00:03:56.800</a></span> | <span class="t">mechanisms that didn't evolve or scale at all. Starting 2017, we saw this explosion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=242" target="_blank">00:04:02.400</a></span> | <span class="t">of transformers into NLP, where people started using it for everything. I even heard this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=247" target="_blank">00:04:07.840</a></span> | <span class="t">quote from Google, it's like, "Our performance increased every time we fired our linguists."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=251" target="_blank">00:04:11.520</a></span> | <span class="t">For the first 90, 80, after 2018 to 2020, we saw this explosion of transformers into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=259" target="_blank">00:04:19.040</a></span> | <span class="t">other fields, like vision, a bunch of other stuff, and biology, alpha foli. And last year,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=267" target="_blank">00:04:27.120</a></span> | <span class="t">2021 was the start of the generative era, where we got a lot of generative modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=270" target="_blank">00:04:30.720</a></span> | <span class="t">Started with models like CODEX, GPT, DALI, stable diffusion, so a lot of things happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=277" target="_blank">00:04:37.520</a></span> | <span class="t">in generative modeling. And we started scaling up in AI. And now it's the present. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=286" target="_blank">00:04:46.160</a></span> | <span class="t">is 2022 and the start of 2023. And now we have models like Chai-3PP, Whisper, a bunch of others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=294" target="_blank">00:04:54.560</a></span> | <span class="t">And we are scaling onwards without slowing down. So that's great. So that's the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=299" target="_blank">00:04:59.440</a></span> | <span class="t">So going more into this, so once there were RNNs, so we had sequence-to-sequence models, LSTMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=309" target="_blank">00:05:09.200</a></span> | <span class="t">GLUs. What worked here was that they were good at encoding history. But what did not work was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=315" target="_blank">00:05:15.440</a></span> | <span class="t">they didn't encode long sequences. And they were very bad at encoding context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=319" target="_blank">00:05:19.440</a></span> | <span class="t">So consider this example. Consider trying to predict the last word in the text, "I grew up in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=327" target="_blank">00:05:27.920</a></span> | <span class="t">France, dot, dot, dot. I speak fluent, dash." Here, you need to understand the context for it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=333" target="_blank">00:05:33.600</a></span> | <span class="t">to predict French. And attention mechanism is very good at that. Whereas if you're just using LSTMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=339" target="_blank">00:05:39.600</a></span> | <span class="t">it doesn't work that well. Another thing transformers are good at is more based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=347" target="_blank">00:05:47.040</a></span> | <span class="t">content is-- sorry. Also, context prediction is like finding attention maps. If I have something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=354" target="_blank">00:05:54.480</a></span> | <span class="t">like a word like "it," what noun does it correlate to? And we can give a probability attention on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=362" target="_blank">00:06:02.400</a></span> | <span class="t">what are the possible activations. And this works better than existing mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=369" target="_blank">00:06:09.120</a></span> | <span class="t">OK. So where we were in 2021, we were on the verge of takeoff. We were starting to realize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=376" target="_blank">00:06:16.880</a></span> | <span class="t">the potential of transformers in different fields. We solved a lot of long-sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=381" target="_blank">00:06:21.680</a></span> | <span class="t">problems like protein folding, alpha fold, offline RL. We started to see few shots,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=389" target="_blank">00:06:29.280</a></span> | <span class="t">zero-shot generalization. We saw multimodal tasks and applications like generating images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=394" target="_blank">00:06:34.160</a></span> | <span class="t">from language. So that was DALI. Yeah. And it feels like Asian, but it was only like two years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=400" target="_blank">00:06:40.160</a></span> | <span class="t">ago. And this is also a talk on transformers that you can watch on YouTube. Cool. And this is where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=411" target="_blank">00:06:51.040</a></span> | <span class="t">we were going from 2021 to 2022, which is we have gone from the verge of taking off to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=417" target="_blank">00:06:57.200</a></span> | <span class="t">taking off. And now we are seeing unique applications in audio generation, art, music,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=421" target="_blank">00:07:01.920</a></span> | <span class="t">storytelling. We are starting to see reasoning capabilities like common sense, logical reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=428" target="_blank">00:07:08.160</a></span> | <span class="t">mathematical reasoning. We are also able to now get human enlightenment and interaction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=434" target="_blank">00:07:14.000</a></span> | <span class="t">They're able to use reinforcement learning and human feedback. That's how trajectories train to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=438" target="_blank">00:07:18.160</a></span> | <span class="t">perform really good. We have a lot of mechanisms for controlling toxicity, bias, and ethics now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=444" target="_blank">00:07:24.400</a></span> | <span class="t">And also a lot of developments in other areas like digital models. Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=450" target="_blank">00:07:30.640</a></span> | <span class="t">So the future is a spaceship, and we are all excited about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=456" target="_blank">00:07:36.400</a></span> | <span class="t">And there's a lot more applications that we can enable. And it'd be great if you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=465" target="_blank">00:07:45.600</a></span> | <span class="t">transformers also work there. One big example is video understanding and generation. That is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=470" target="_blank">00:07:50.080</a></span> | <span class="t">something that everyone is interested in. And I'm hoping we'll see a lot of models in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=474" target="_blank">00:07:54.000</a></span> | <span class="t">area this year. Also finance, business. I'll be very excited to see GBT author novel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=481" target="_blank">00:08:01.920</a></span> | <span class="t">but we need to solve very long sequence modeling. And most transformers models are still limited to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=487" target="_blank">00:08:07.920</a></span> | <span class="t">like 4,000 tokens or something like that. So we need to make them generalize much more better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=494" target="_blank">00:08:14.240</a></span> | <span class="t">on long sequences. We also want to have generalized agents that can do a lot of multitask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=502" target="_blank">00:08:22.320</a></span> | <span class="t">analytic input predictions like Gato. And so I think we will see more of that too. And finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=510" target="_blank">00:08:30.640</a></span> | <span class="t">we also want domain-specific models. So you might want like a GBT model that's good at like maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=518" target="_blank">00:08:38.480</a></span> | <span class="t">like your health. So that could be like a doctor GBT model. You might have like a lawyer GBT model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=523" target="_blank">00:08:43.200</a></span> | <span class="t">that's like gain on only on law data. So currently we have like GBT models that have gain on everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=527" target="_blank">00:08:47.440</a></span> | <span class="t">but we might start to see more niche models that are like good at one task. And we could have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=532" target="_blank">00:08:52.480</a></span> | <span class="t">a mixture of experts. It's like, you can think like, this is like how you normally consult an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=536" target="_blank">00:08:56.640</a></span> | <span class="t">expert. You'll have like expert AI models and you can go to a different AI model for your different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=540" target="_blank">00:09:00.080</a></span> | <span class="t">needs. There are still a lot of missing ingredients to make this all successful. The first of all is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=549" target="_blank">00:09:09.840</a></span> | <span class="t">external memory. We are already starting to see this with the models like GenGBT, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=555" target="_blank">00:09:15.840</a></span> | <span class="t">the interactions are short-lived. There's no long-term memory, and they don't have ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=560" target="_blank">00:09:20.320</a></span> | <span class="t">to remember or store conversations for long term. And this is something we want to fix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=566" target="_blank">00:09:26.800</a></span> | <span class="t">Second is reducing the computation complexity. So attention mechanism is quadratic over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=574" target="_blank">00:09:34.320</a></span> | <span class="t">the sequence length, which is slow. And we want to reduce it or make it faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=579" target="_blank">00:09:39.520</a></span> | <span class="t">Another thing we want to do is we want to enhance the controllability of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=585" target="_blank">00:09:45.760</a></span> | <span class="t">It's like a lot of these models can be stochastic, and we want to be able to control what sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=590" target="_blank">00:09:50.560</a></span> | <span class="t">outputs we get from them. And you might have experienced with GenGBT, if you just refresh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=595" target="_blank">00:09:55.200</a></span> | <span class="t">you get like different output each time, but you might want to have a mechanism that controls</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=599" target="_blank">00:09:59.040</a></span> | <span class="t">what sort of things you get. And finally, we want to align our state of art language models with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=604" target="_blank">00:10:04.800</a></span> | <span class="t">how the human brain works. And we are seeing the search, but we still need more research on seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=610" target="_blank">00:10:10.320</a></span> | <span class="t">how it can be manipulated. Thank you. Great. Hi. Yes, I'm excited to be here. I live very nearby,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=619" target="_blank">00:10:19.360</a></span> | <span class="t">so I got the invites to come to class. And I was like, OK, I'll just walk over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=622" target="_blank">00:10:22.560</a></span> | <span class="t">But then I spent like 10 hours on the slides, so it wasn't as simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=626" target="_blank">00:10:26.880</a></span> | <span class="t">So yeah, I want to talk about transformers. I'm going to skip the first two over there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=632" target="_blank">00:10:32.560</a></span> | <span class="t">We're not going to talk about those. We'll talk about that one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=635" target="_blank">00:10:35.040</a></span> | <span class="t">just to simplify the lecture since we don't have time. OK. So I wanted to provide a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=641" target="_blank">00:10:41.440</a></span> | <span class="t">context of why does this transformers class even exist. So a little bit of historical context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=645" target="_blank">00:10:45.840</a></span> | <span class="t">I feel like Bilbo over there. I joined, like telling you guys about this. I don't know if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=650" target="_blank">00:10:50.880</a></span> | <span class="t">you guys saw the drinks. And basically, I joined AI in roughly 2012 in full force,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=656" target="_blank">00:10:56.720</a></span> | <span class="t">so maybe a decade ago. And back then, you wouldn't even say that you joined AI, by the way. That was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=660" target="_blank">00:11:00.800</a></span> | <span class="t">like a dirty word. Now it's OK to talk about. But back then, it was not even deep learning. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=665" target="_blank">00:11:05.760</a></span> | <span class="t">machine learning. That was a term you would use if you were serious. But now, AI is OK to use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=670" target="_blank">00:11:10.880</a></span> | <span class="t">I think. So basically, do you even realize how lucky you are potentially entering this area</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=675" target="_blank">00:11:15.280</a></span> | <span class="t">in roughly 2003? So back then, in 2011 or so, when I was working specifically on computer vision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=681" target="_blank">00:11:21.200</a></span> | <span class="t">your pipelines looked like this. So you wanted to classify some images. You would go to a paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=689" target="_blank">00:11:29.120</a></span> | <span class="t">and I think this is representative. You would have three pages in the paper describing all kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=693" target="_blank">00:11:33.680</a></span> | <span class="t">zoo of kitchen sink of different kinds of features and descriptors. And you would go to a poster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=698" target="_blank">00:11:38.560</a></span> | <span class="t">session and in computer vision conference, and everyone would have their favorite feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=701" target="_blank">00:11:41.840</a></span> | <span class="t">descriptors that they're proposing. It was totally ridiculous. And you would take notes on which one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=705" target="_blank">00:11:45.200</a></span> | <span class="t">you should incorporate into your pipeline, because you would extract all of them, and then you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=708" target="_blank">00:11:48.640</a></span> | <span class="t">put an SVM on top. So that's what you would do. So there's two pages. Make sure you get your sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=713" target="_blank">00:11:53.040</a></span> | <span class="t">SIP histograms, your SSIMs, your color histograms, textiles, tiny images. And don't forget the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=717" target="_blank">00:11:57.920</a></span> | <span class="t">geometry-specific histograms. All of them had basically complicated code by themselves. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=722" target="_blank">00:12:02.640</a></span> | <span class="t">you're collecting code from everywhere and running it, and it was a total nightmare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=725" target="_blank">00:12:05.680</a></span> | <span class="t">So on top of that, it also didn't work. So this would be, I think, represents the prediction from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=734" target="_blank">00:12:14.480</a></span> | <span class="t">that time. You would just get predictions like this once in a while, and you'd be like, you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=738" target="_blank">00:12:18.480</a></span> | <span class="t">shrug your shoulders like that just happens once in a while. Today, you would be looking for a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=743" target="_blank">00:12:23.680</a></span> | <span class="t">And worse than that, every single chunk of AI had their own completely separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=752" target="_blank">00:12:32.480</a></span> | <span class="t">vocabulary that they work with. So if you go to NLP papers, those papers would be completely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=757" target="_blank">00:12:37.520</a></span> | <span class="t">different. So you're reading the NLP paper, and you're like, what is this part of speech tagging,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=762" target="_blank">00:12:42.320</a></span> | <span class="t">morphological analysis, syntactic parsing, coreference resolution? What is NP, BT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=767" target="_blank">00:12:47.520</a></span> | <span class="t">KJ, and your compute? So the vocabulary and everything was completely different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=771" target="_blank">00:12:51.280</a></span> | <span class="t">and you couldn't read papers, I would say, across different areas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=773" target="_blank">00:12:53.920</a></span> | <span class="t">So now that changed a little bit starting in 2012 when Oskar Krzyzewski and the colleagues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=780" target="_blank">00:13:00.960</a></span> | <span class="t">basically demonstrated that if you scale a large neural network on a large data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=786" target="_blank">00:13:06.640</a></span> | <span class="t">you can get very strong performance. And so up till then, there was a lot of focus on algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=790" target="_blank">00:13:10.960</a></span> | <span class="t">but this showed that actually neural nets scale very well. So you need to now worry about compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=794" target="_blank">00:13:14.880</a></span> | <span class="t">and data, and if you scale it up, it works pretty well. And then that recipe actually did copy-paste</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=799" target="_blank">00:13:19.360</a></span> | <span class="t">across many areas of AI. So we started to see neural networks pop up everywhere since 2012.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=805" target="_blank">00:13:25.040</a></span> | <span class="t">So we saw them in computer vision, and NLP in speech, and translation in RL, and so on. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=810" target="_blank">00:13:30.480</a></span> | <span class="t">everyone started to use the same kind of modeling tool kit, modeling framework. And now when you go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=814" target="_blank">00:13:34.720</a></span> | <span class="t">to NLP and you start reading papers there, in machine translation, for example, this is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=819" target="_blank">00:13:39.120</a></span> | <span class="t">sequence-to-sequence paper, which we'll come back to in a bit. You start to read those papers, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=823" target="_blank">00:13:43.360</a></span> | <span class="t">you're like, OK, I can recognize these words, like there's a neural network, there's a parameter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=826" target="_blank">00:13:46.960</a></span> | <span class="t">there's an optimizer, and it starts to read things that you know of. So that decreased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=832" target="_blank">00:13:52.240</a></span> | <span class="t">tremendously the barrier to entry across the different areas. And then I think the big deal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=837" target="_blank">00:13:57.840</a></span> | <span class="t">is that when the transformer came out in 2017, it's not even that just the toolkits and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=842" target="_blank">00:14:02.480</a></span> | <span class="t">neural networks were similar, it's that literally the architectures converged to one architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=846" target="_blank">00:14:06.960</a></span> | <span class="t">that you copy-paste across everything seemingly. So this was kind of an unassuming machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=852" target="_blank">00:14:12.560</a></span> | <span class="t">translation paper at the time proposing the transformer architecture, but what we found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=855" target="_blank">00:14:15.680</a></span> | <span class="t">since then is that you can just basically copy-paste this architecture and use it everywhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=861" target="_blank">00:14:21.520</a></span> | <span class="t">and what's changing is the details of the data and the chunking of the data and how you feed it in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=866" target="_blank">00:14:26.560</a></span> | <span class="t">And that's a caricature, but it's kind of like a correct first-order statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=869" target="_blank">00:14:29.920</a></span> | <span class="t">And so now papers are even more similar looking because everyone's just using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=873" target="_blank">00:14:33.760</a></span> | <span class="t">transformer. And so this convergence was remarkable to watch and unfolded over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=879" target="_blank">00:14:39.440</a></span> | <span class="t">last decade, and it's pretty crazy to me. What I find kind of interesting is I think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=884" target="_blank">00:14:44.560</a></span> | <span class="t">some kind of a hint that we're maybe converging to something that maybe the brain is doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=887" target="_blank">00:14:47.760</a></span> | <span class="t">because the brain is very homogeneous and uniform across the entire sheet of your cortex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=892" target="_blank">00:14:52.880</a></span> | <span class="t">And okay, maybe some of the details are changing, but those feel like hyperparameters of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=896" target="_blank">00:14:56.640</a></span> | <span class="t">transformer, but your auditory cortex and your visual cortex and everything else looks very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=900" target="_blank">00:15:00.320</a></span> | <span class="t">similar. And so maybe we're converging to some kind of a uniform, powerful learning algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=904" target="_blank">00:15:04.880</a></span> | <span class="t">here, something like that, I think is kind of interesting and exciting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=908" target="_blank">00:15:08.000</a></span> | <span class="t">Okay, so I want to talk about where the transformer came from briefly, historically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=912" target="_blank">00:15:12.880</a></span> | <span class="t">So I want to start in 2003. I like this paper quite a bit. It was the first sort of popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=920" target="_blank">00:15:20.000</a></span> | <span class="t">application of neural networks to the problem of language modeling. So predicting, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=924" target="_blank">00:15:24.160</a></span> | <span class="t">the next word in a sequence, which allows you to build generative models over text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=927" target="_blank">00:15:27.680</a></span> | <span class="t">And in this case, they were using multi-layer perceptron, so a very simple neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=931" target="_blank">00:15:31.040</a></span> | <span class="t">The neural nets took three words and predicted the probability distribution for the fourth word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=934" target="_blank">00:15:34.720</a></span> | <span class="t">in a sequence. So this was well and good at this point. Now, over time, people started to apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=941" target="_blank">00:15:41.280</a></span> | <span class="t">this to machine translation. So that brings us to sequence-to-sequence paper from 2014 that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=946" target="_blank">00:15:46.960</a></span> | <span class="t">pretty influential. And the big problem here was, okay, we don't just want to take three words and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=951" target="_blank">00:15:51.360</a></span> | <span class="t">predict the fourth. We want to predict how to go from an English sentence to a French sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=956" target="_blank">00:15:56.720</a></span> | <span class="t">And the key problem was, okay, you can have arbitrary number of words in English and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=960" target="_blank">00:16:00.400</a></span> | <span class="t">arbitrary number of words in French, so how do you get an architecture that can process this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=965" target="_blank">00:16:05.120</a></span> | <span class="t">variably-sized input? And so here, they used a LSTM. And there's basically two chunks of this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=971" target="_blank">00:16:11.120</a></span> | <span class="t">which are covered by the Slack, by this. But basically, you have an encoder LSTM on the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=978" target="_blank">00:16:18.960</a></span> | <span class="t">and it just consumes one word at a time and builds up a context of what it has read. And then that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=985" target="_blank">00:16:25.280</a></span> | <span class="t">acts as a conditioning vector to the decoder RNN or LSTM that basically goes chunk, chunk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=990" target="_blank">00:16:30.320</a></span> | <span class="t">chunk for the next word in the sequence, translating the English to French or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=994" target="_blank">00:16:34.640</a></span> | <span class="t">like that. Now, the big problem with this that people identified, I think, very quickly and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=999" target="_blank">00:16:39.040</a></span> | <span class="t">tried to resolve is that there's what's called this encoded bottleneck. So this entire English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1005" target="_blank">00:16:45.280</a></span> | <span class="t">sentence that we are trying to condition on is packed into a single vector that goes from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1009" target="_blank">00:16:49.600</a></span> | <span class="t">encoder to the decoder. And so this is just too much information to potentially maintain in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1013" target="_blank">00:16:53.440</a></span> | <span class="t">single vector, and that didn't seem correct. And so people were looking around for ways to alleviate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1017" target="_blank">00:16:57.680</a></span> | <span class="t">the attention of sort of the encoded bottleneck, as it was called at the time. And so that brings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1022" target="_blank">00:17:02.800</a></span> | <span class="t">us to this paper, Neural Machine Translation by Jointly Learning to Align and Translate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1026" target="_blank">00:17:06.960</a></span> | <span class="t">And here, just going from the abstract, in this paper, we conjectured that use of a fixed-length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1033" target="_blank">00:17:13.200</a></span> | <span class="t">vector is a bottleneck in improving the performance of the basic encoded-decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1036" target="_blank">00:17:16.720</a></span> | <span class="t">architecture, and proposed to extend this by allowing the model to automatically soft-search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1041" target="_blank">00:17:21.680</a></span> | <span class="t">for parts of the source sentence that are relevant to predicting a target word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1046" target="_blank">00:17:26.880</a></span> | <span class="t">without having to form these parts or hard segments exclusively. So this was a way to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1052" target="_blank">00:17:32.880</a></span> | <span class="t">back to the words that are coming from the encoder, and it was achieved using this soft-search. So as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1058" target="_blank">00:17:38.720</a></span> | <span class="t">you are decoding the words here, while you are decoding them, you are allowed to look back at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1065" target="_blank">00:17:45.360</a></span> | <span class="t">the words at the encoder via this soft attention mechanism proposed in this paper. And so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1070" target="_blank">00:17:50.960</a></span> | <span class="t">paper, I think, is the first time that I saw, basically, attention. So your context vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1077" target="_blank">00:17:57.760</a></span> | <span class="t">that comes from the encoder is a weighted sum of the hidden states of the words in the encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1084" target="_blank">00:18:04.480</a></span> | <span class="t">and then the weights of this sum come from a softmax that is based on these compatibilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1091" target="_blank">00:18:11.200</a></span> | <span class="t">between the current state, as you're decoding, and the hidden states generated by the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1095" target="_blank">00:18:15.760</a></span> | <span class="t">And so this is the first time that really you start to look at it, and this is the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1100" target="_blank">00:18:20.800</a></span> | <span class="t">modern equations of the attention. And I think this was the first paper that I saw it in. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1105" target="_blank">00:18:25.520</a></span> | <span class="t">the first time that there's a word "attention" used, as far as I know, to call this mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1111" target="_blank">00:18:31.360</a></span> | <span class="t">So I actually tried to dig into the details of the history of the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1115" target="_blank">00:18:35.600</a></span> | <span class="t">So the first author here, Dimitri, I had an email correspondence with him,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1120" target="_blank">00:18:40.000</a></span> | <span class="t">and I basically sent him an email. I'm like, "Dimitri, this is really interesting. Transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1123" target="_blank">00:18:43.360</a></span> | <span class="t">have taken over. Where did you come up with the soft attention mechanism that ends up being the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1126" target="_blank">00:18:46.880</a></span> | <span class="t">heart of the transformer?" And to my surprise, he wrote me back this massive email, which was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1132" target="_blank">00:18:52.160</a></span> | <span class="t">really fascinating. So this is an excerpt from that email. So basically, he talks about how he</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1138" target="_blank">00:18:58.560</a></span> | <span class="t">was looking for a way to avoid this bottleneck between the encoder and decoder. He had some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1142" target="_blank">00:19:02.720</a></span> | <span class="t">ideas about cursors that traversed the sequences that didn't quite work out. And then here - so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1148" target="_blank">00:19:08.000</a></span> | <span class="t">one day, I had this thought that it would be nice to enable the decoder RNN to learn how to search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1151" target="_blank">00:19:11.840</a></span> | <span class="t">where to put the cursor in the source sequence. This was sort of inspired by translation exercises</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1156" target="_blank">00:19:16.400</a></span> | <span class="t">that learning English in my middle school involved. You gaze shifts back and forth between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1162" target="_blank">00:19:22.720</a></span> | <span class="t">source and target sequence as you translate. So literally, I thought this was kind of interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1166" target="_blank">00:19:26.960</a></span> | <span class="t">that he's not a native English speaker. And here, that gave him an edge in this machine translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1171" target="_blank">00:19:31.200</a></span> | <span class="t">that led to attention and then led to transformer. So that's really fascinating. I expressed a soft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1177" target="_blank">00:19:37.680</a></span> | <span class="t">search as softmax and then weighted averaging of the binary states. And basically, to my great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1182" target="_blank">00:19:42.560</a></span> | <span class="t">excitement, this worked from the very first try. So really, I think, interesting piece of history.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1188" target="_blank">00:19:48.320</a></span> | <span class="t">And as it later turned out that the name of RNN search was kind of lame. So the better name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1193" target="_blank">00:19:53.520</a></span> | <span class="t">attention came from Yoshua on one of the final passes as they went over the paper. So maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1200" target="_blank">00:20:00.000</a></span> | <span class="t">attention is all I need would have been called like RNN searches. But we have Yoshua Bengio to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1204" target="_blank">00:20:04.960</a></span> | <span class="t">thank for a little bit of better name, I would say. So apparently, that's the history of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1211" target="_blank">00:20:11.360</a></span> | <span class="t">OK, so that brings us to 2017, which is attention is all you need. So this attention component,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1216" target="_blank">00:20:16.240</a></span> | <span class="t">which in Dimitri's paper was just like one small segment. And there's all this bi-directional RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1220" target="_blank">00:20:20.960</a></span> | <span class="t">RNN and decoder. And this attention-only paper is saying, OK, you can actually delete everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1226" target="_blank">00:20:26.880</a></span> | <span class="t">What's making this work very well is just attention by itself. And so delete everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1231" target="_blank">00:20:31.280</a></span> | <span class="t">keep attention. And then what's remarkable about this paper, actually, is usually you see papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1235" target="_blank">00:20:35.520</a></span> | <span class="t">that are very incremental. They add one thing, and they show that it's better. But I feel like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1240" target="_blank">00:20:40.560</a></span> | <span class="t">attention is all you need with a mix of multiple things at the same time. They were combined in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1244" target="_blank">00:20:44.960</a></span> | <span class="t">very unique way, and then also achieved a very good local minimum in the architecture space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1250" target="_blank">00:20:50.720</a></span> | <span class="t">And so to me, this is really a landmark paper that is quite remarkable, and I think had quite a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1256" target="_blank">00:20:56.480</a></span> | <span class="t">of work behind the scenes. So delete all the RNN, just keep attention. Because attention operates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1263" target="_blank">00:21:03.040</a></span> | <span class="t">over sets, and I'm going to go into this in a second, you now need to positionally encode your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1266" target="_blank">00:21:06.640</a></span> | <span class="t">inputs, because attention doesn't have the notion of space by itself. They - oops, I have to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1275" target="_blank">00:21:15.280</a></span> | <span class="t">very careful - they adopted this residual network structure from ResNets. They interspersed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1282" target="_blank">00:21:22.160</a></span> | <span class="t">attention with multi-layer perceptrons. They used layer norms, which came from a different paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1287" target="_blank">00:21:27.680</a></span> | <span class="t">They introduced the concept of multiple heads of attention that were applied in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1290" target="_blank">00:21:30.800</a></span> | <span class="t">And they gave us, I think, like a fairly good set of hyperparameters that to this day are used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1295" target="_blank">00:21:35.440</a></span> | <span class="t">So the expansion factor in the multi-layer perceptron goes up by 4x, and we'll go into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1300" target="_blank">00:21:40.800</a></span> | <span class="t">like a bit more detail, and this 4x has stuck around. And I believe there's a number of papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1305" target="_blank">00:21:45.280</a></span> | <span class="t">that try to play with all kinds of little details of the transformer, and nothing sticks, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1310" target="_blank">00:21:50.000</a></span> | <span class="t">this is actually quite good. The only thing to my knowledge that stuck, that didn't stick, was this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1315" target="_blank">00:21:55.840</a></span> | <span class="t">reshuffling of the layer norms to go into the pre-norm version, where here you see the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1319" target="_blank">00:21:59.920</a></span> | <span class="t">norms are after the multi-headed attention repeat forward, but they just put them before instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1324" target="_blank">00:22:04.400</a></span> | <span class="t">So just reshuffling of layer norms, but otherwise the GPTs and everything else that you're seeing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1328" target="_blank">00:22:08.160</a></span> | <span class="t">today is basically the 2017 architecture from five years ago. And even though everyone is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1333" target="_blank">00:22:13.120</a></span> | <span class="t">working on it, it's proven remarkably resilient, which I think is real interesting. There are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1338" target="_blank">00:22:18.000</a></span> | <span class="t">innovations that I think have been adopted also in positional encodings. It's more common to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1342" target="_blank">00:22:22.640</a></span> | <span class="t">different rotary and relative positional encodings and so on. So I think there have been changes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1347" target="_blank">00:22:27.600</a></span> | <span class="t">but for the most part it's proven very resilient. So really quite an interesting paper. Now I wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1353" target="_blank">00:22:33.360</a></span> | <span class="t">to go into the attention mechanism, and I think, I sort of like, the way I interpret it is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1360" target="_blank">00:22:40.160</a></span> | <span class="t">similar to the ways that I've seen it presented before. So let me try a different way of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1367" target="_blank">00:22:47.280</a></span> | <span class="t">how I see it. Basically to me, attention is kind of like the communication phase of the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1371" target="_blank">00:22:51.440</a></span> | <span class="t">and the transformer interleaves two phases. The communication phase, which is the multi-headed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1376" target="_blank">00:22:56.560</a></span> | <span class="t">attention, and the computation stage, which is this multilayer perceptron, or P12. So in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1381" target="_blank">00:23:01.760</a></span> | <span class="t">communication phase, it's really just a data-dependent message passing on directed graphs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1386" target="_blank">00:23:06.400</a></span> | <span class="t">And you can think of it as, okay, forget everything with machine translation and everything. Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1391" target="_blank">00:23:11.280</a></span> | <span class="t">just, we have directed graphs at each node. You are storing a vector. And then let me talk now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1398" target="_blank">00:23:18.080</a></span> | <span class="t">about the communication phase of how these vectors talk to each other in this directed graph. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1401" target="_blank">00:23:21.760</a></span> | <span class="t">then the compute phase later is just a multilayer perceptron, which now, which then basically acts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1407" target="_blank">00:23:27.200</a></span> | <span class="t">on every node individually. But how do these nodes talk to each other in this directed graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1412" target="_blank">00:23:32.240</a></span> | <span class="t">So I wrote like some simple Python, like I wrote this in Python basically to create one round of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1420" target="_blank">00:23:40.080</a></span> | <span class="t">communication of using attention as the direct, as the message passing scheme. So here, a node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1428" target="_blank">00:23:48.560</a></span> | <span class="t">has this private data vector, as you can think of it as private information to this node. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1434" target="_blank">00:23:54.720</a></span> | <span class="t">it can also emit a key, a query, and a value. And simply that's done by linear transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1439" target="_blank">00:23:59.440</a></span> | <span class="t">from this node. So the key is, what are the things that I am, sorry, the query is, what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1448" target="_blank">00:24:08.960</a></span> | <span class="t">things that I'm looking for? The key is, what are the things that I have? And the value is, what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1453" target="_blank">00:24:13.040</a></span> | <span class="t">the things that I will communicate? And so then when you have your graph that's made up of nodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1457" target="_blank">00:24:17.600</a></span> | <span class="t">and some random edges, when you actually have these nodes communicating, what's happening is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1461" target="_blank">00:24:21.360</a></span> | <span class="t">you loop over all the nodes individually in some random order, and you are at some node,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1467" target="_blank">00:24:27.040</a></span> | <span class="t">and you get the query vector q, which is, I'm a node in some graph, and this is what I'm looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1473" target="_blank">00:24:33.200</a></span> | <span class="t">for. And so that's just achieved via this linear transformation here. And then we look at all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1477" target="_blank">00:24:37.920</a></span> | <span class="t">inputs that point to this node, and then they broadcast, what are the things that I have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1482" target="_blank">00:24:42.320</a></span> | <span class="t">which is their keys. So they broadcast the keys, I have the query, then those interact by dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1489" target="_blank">00:24:49.600</a></span> | <span class="t">to get scores. So basically, simply by doing dot product, you get some kind of an unnormalized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1495" target="_blank">00:24:55.520</a></span> | <span class="t">weighting of the interestingness of all of the information in the nodes that point to me and to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1500" target="_blank">00:25:00.960</a></span> | <span class="t">the things I'm looking for. And then when you normalize that with a submax, so it just sums to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1504" target="_blank">00:25:04.800</a></span> | <span class="t">one, you basically just end up using those scores, which now sum to one and are a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1509" target="_blank">00:25:09.600</a></span> | <span class="t">distribution, and you do a weighted sum of the values to get your update. So I have a query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1517" target="_blank">00:25:17.280</a></span> | <span class="t">they have keys, dot product to get interestingness, or like affinity, submax to normalize it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1523" target="_blank">00:25:23.840</a></span> | <span class="t">and then weighted sum of those values flow to me and update me. And this is happening for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1528" target="_blank">00:25:28.720</a></span> | <span class="t">node individually, and then we update at the end. And so this kind of a message passing scheme is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1532" target="_blank">00:25:32.800</a></span> | <span class="t">kind of like at the heart of the transformer, and happens in a more vectorized, batched way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1540" target="_blank">00:25:40.240</a></span> | <span class="t">that is more confusing, and is also interspersed with layer norms and things like that to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1545" target="_blank">00:25:45.760</a></span> | <span class="t">training behave better. But that's roughly what's happening in the attention mechanism, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1550" target="_blank">00:25:50.560</a></span> | <span class="t">on a high level. So yeah, so in the communication phase of the transformer, then this message</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1559" target="_blank">00:25:59.680</a></span> | <span class="t">passing scheme happens in every head in parallel, and then in every layer in series, and with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1566" target="_blank">00:26:06.800</a></span> | <span class="t">different weights each time. And that's it as far as the multi-headed attention goes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1573" target="_blank">00:26:13.120</a></span> | <span class="t">And so if you look at these encoder-decoder models, you can sort of think of it then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1577" target="_blank">00:26:17.040</a></span> | <span class="t">in terms of the connectivity of these nodes in the graph, you can kind of think of it as like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1580" target="_blank">00:26:20.560</a></span> | <span class="t">okay, all these tokens that are in the encoder that we want to condition on, they are fully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1584" target="_blank">00:26:24.880</a></span> | <span class="t">connected to each other. So when they communicate, they communicate fully when you calculate their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1589" target="_blank">00:26:29.520</a></span> | <span class="t">features. But in the decoder, because we are trying to have a language model, we don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1594" target="_blank">00:26:34.240</a></span> | <span class="t">to have communication from future tokens, because they give away the answer at this step. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1598" target="_blank">00:26:38.720</a></span> | <span class="t">tokens in the decoder are fully connected from all the encoder states, and then they are also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1603" target="_blank">00:26:43.840</a></span> | <span class="t">fully connected from everything that is before them. And so you end up with this, like, triangular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1608" target="_blank">00:26:48.320</a></span> | <span class="t">structure in the directed graph. But that's the message passing scheme that this basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1613" target="_blank">00:26:53.280</a></span> | <span class="t">implements. And then you have to be also a little bit careful, because in the cross-attention here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1618" target="_blank">00:26:58.320</a></span> | <span class="t">with the decoder, you consume the features from the top of the encoder. So think of it as, in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1623" target="_blank">00:27:03.600</a></span> | <span class="t">encoder, all the nodes are looking at each other, all the tokens are looking at each other, many,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1627" target="_blank">00:27:07.280</a></span> | <span class="t">many times. And they really figure out what's in there. And then the decoder, when it's looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1631" target="_blank">00:27:11.520</a></span> | <span class="t">only at the top nodes. So that's roughly the message passing scheme. I was going to go into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1637" target="_blank">00:27:17.360</a></span> | <span class="t">more of an implementation of the transformer. I don't know if there's any questions about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1641" target="_blank">00:27:21.680</a></span> | <span class="t">Can you explain a little bit about self-attention and multi-headed attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1646" target="_blank">00:27:26.480</a></span> | <span class="t">Yeah, so self-attention and multi-headed attention. So the multi-headed attention is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1656" target="_blank">00:27:36.880</a></span> | <span class="t">just this attention scheme, but it's just applied multiple times in parallel. Multiple heads just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1661" target="_blank">00:27:41.600</a></span> | <span class="t">means independent applications of the same attention. So this message passing scheme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1667" target="_blank">00:27:47.360</a></span> | <span class="t">basically just happens in parallel multiple times with different weights for the query key and value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1673" target="_blank">00:27:53.200</a></span> | <span class="t">So you can almost look at it like, in parallel, I'm looking for, I'm seeking different kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1676" target="_blank">00:27:56.880</a></span> | <span class="t">information from different nodes, and I'm collecting it all in the same node. It's all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1681" target="_blank">00:28:01.360</a></span> | <span class="t">done in parallel. So heads is really just like copy-paste in parallel. And layers are copy-paste,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1689" target="_blank">00:28:09.680</a></span> | <span class="t">but in series. Maybe that makes sense. And self-attention, when it's self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1698" target="_blank">00:28:18.800</a></span> | <span class="t">what it's referring to is that the node here produces each node here. So as I described it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1704" target="_blank">00:28:24.000</a></span> | <span class="t">here, this is really self-attention. Because every one of these nodes produces a key query and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1708" target="_blank">00:28:28.240</a></span> | <span class="t">value from this individual node. When you have cross-attention, you have one cross-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1714" target="_blank">00:28:34.160</a></span> | <span class="t">here coming from the encoder. That just means that the queries are still produced from this node,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1720" target="_blank">00:28:40.720</a></span> | <span class="t">but the keys and the values are produced as a function of nodes that are coming from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1726" target="_blank">00:28:46.480</a></span> | <span class="t">encoder. So I have my queries because I'm trying to decode the fifth word in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1733" target="_blank">00:28:53.760</a></span> | <span class="t">and I'm looking for certain things because I'm the fifth word. And then the keys and the values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1738" target="_blank">00:28:58.240</a></span> | <span class="t">in terms of the source of information that could answer my queries, can come from the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1742" target="_blank">00:29:02.720</a></span> | <span class="t">nodes in the current decoding sequence, or from the top of the encoder. So all the nodes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1747" target="_blank">00:29:07.280</a></span> | <span class="t">have already seen all of the encoding tokens many, many times can now broadcast what they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1752" target="_blank">00:29:12.320</a></span> | <span class="t">contain in terms of information. So I guess to summarize, the self-attention is kind of like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1759" target="_blank">00:29:19.040</a></span> | <span class="t">sorry, cross-attention and self-attention only differ in where the keys and the values come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1763" target="_blank">00:29:23.680</a></span> | <span class="t">from. Either the keys and values are produced from this node, or they are produced from some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1769" target="_blank">00:29:29.120</a></span> | <span class="t">external source, like an encoder and the nodes over there. But algorithmically, it's the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1775" target="_blank">00:29:35.520</a></span> | <span class="t">Michael operations. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1780" target="_blank">00:29:40.400</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1788" target="_blank">00:29:48.640</a></span> | <span class="t">So, yeah, so [INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1796" target="_blank">00:29:56.640</a></span> | <span class="t">So think of - so each one of these nodes is a token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1800" target="_blank">00:30:00.560</a></span> | <span class="t">I guess, like, they don't have a very good picture of it in the transformer, but like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1809" target="_blank">00:30:09.840</a></span> | <span class="t">this node here could represent the third word in the output, in the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1816" target="_blank">00:30:16.800</a></span> | <span class="t">And in the beginning, it is just the embedding of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1821" target="_blank">00:30:21.440</a></span> | <span class="t">And then, okay, I have to think through this analogy a little bit more. I came up with it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1831" target="_blank">00:30:31.840</a></span> | <span class="t">this morning. Actually, I came up with it yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1836" target="_blank">00:30:36.800</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1850" target="_blank">00:30:50.240</a></span> | <span class="t">These nodes are basically the factors. I'll go to an implementation - I'll go to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1854" target="_blank">00:30:54.720</a></span> | <span class="t">implementation, and then maybe I'll make the connections to the graph. So let me try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1859" target="_blank">00:30:59.680</a></span> | <span class="t">first go to - let me now go to, with this intuition in mind at least, to nanoGPT, which is a concrete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1864" target="_blank">00:31:04.480</a></span> | <span class="t">implementation of a transformer that is very minimal. So I worked on this over the last few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1868" target="_blank">00:31:08.240</a></span> | <span class="t">days, and here it is reproducing GPT-2 on open web text. So it's a pretty serious implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1873" target="_blank">00:31:13.760</a></span> | <span class="t">that reproduces GPT-2, I would say, and provided enough compute. This was one node of eight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1879" target="_blank">00:31:19.040</a></span> | <span class="t">GPUs for 38 hours or something like that, and it's very readable at 300 lives, so everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1885" target="_blank">00:31:25.040</a></span> | <span class="t">can take a look at it. And yeah, let me basically briefly step through it. So let's try to have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1892" target="_blank">00:31:32.400</a></span> | <span class="t">decoder-only transformer. So what that means is that it's a language model. It tries to model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1896" target="_blank">00:31:36.880</a></span> | <span class="t">the next word in a sequence or the next character in a sequence. So the data that we train on is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1903" target="_blank">00:31:43.120</a></span> | <span class="t">always some kind of text. So here's some fake Shakespeare. Sorry, this is real Shakespeare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1907" target="_blank">00:31:47.040</a></span> | <span class="t">We're going to produce fake Shakespeare. So this is called the tiny Shakespeare data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1910" target="_blank">00:31:50.480</a></span> | <span class="t">which is one of my favorite toy data sets. You take all of Shakespeare, concatenate it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1914" target="_blank">00:31:54.160</a></span> | <span class="t">and it's one megabyte file, and then you can train language models on it and get infinite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1917" target="_blank">00:31:57.600</a></span> | <span class="t">Shakespeare if you like, which I think is kind of cool. So we have a text. The first thing we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1921" target="_blank">00:32:01.600</a></span> | <span class="t">need to do is we need to convert it to a sequence of integers, because transformers natively process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1926" target="_blank">00:32:06.960</a></span> | <span class="t">you know, you can't plug text into transformer. You need to somehow encode it. So the way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1932" target="_blank">00:32:12.400</a></span> | <span class="t">encoding is done is we convert, for example, in the simplest case, every character gets an integer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1936" target="_blank">00:32:16.560</a></span> | <span class="t">and then instead of "hi" there, we would have this sequence of integers. So then you can encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1942" target="_blank">00:32:22.720</a></span> | <span class="t">every single character as an integer and get a massive sequence of integers. You just concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1948" target="_blank">00:32:28.400</a></span> | <span class="t">it all into one large, long, one-dimensional sequence, and then you can train on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1952" target="_blank">00:32:32.560</a></span> | <span class="t">Now, here we only have a single document. In some cases, if you have multiple independent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1956" target="_blank">00:32:36.800</a></span> | <span class="t">documents, what people like to do is create special tokens, and they intersperse those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1960" target="_blank">00:32:40.160</a></span> | <span class="t">documents with those special end-of-text tokens that they splice in between to create boundaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1965" target="_blank">00:32:45.040</a></span> | <span class="t">But those boundaries actually don't have any modeling impact. It's just that the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1972" target="_blank">00:32:52.080</a></span> | <span class="t">is supposed to learn via backpropagation that the end-of-document sequence means that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1977" target="_blank">00:32:57.040</a></span> | <span class="t">should wipe the memory. Okay, so then we produce batches. So these batches of data just mean that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1984" target="_blank">00:33:04.640</a></span> | <span class="t">we go back to the one-dimensional sequence, and we take out chunks of this sequence. So say if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1989" target="_blank">00:33:09.840</a></span> | <span class="t">block size is 8, then the block size indicates the maximum length of context that your transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=1997" target="_blank">00:33:17.600</a></span> | <span class="t">will process. So if our block size is 8, that means that we are going to have up to 8 characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2002" target="_blank">00:33:22.960</a></span> | <span class="t">of context to predict the 9th character in the sequence. And the batch size indicates how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2008" target="_blank">00:33:28.160</a></span> | <span class="t">sequences in parallel we're going to process. And we want this to be as large as possible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2011" target="_blank">00:33:31.760</a></span> | <span class="t">so we're fully taking advantage of the GPU and the parallels on the boards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2014" target="_blank">00:33:34.640</a></span> | <span class="t">So in this example, we're doing 4 by 8 batches. So every row here is independent example, sort of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2021" target="_blank">00:33:41.440</a></span> | <span class="t">And then every row here is a small chunk of the sequence that we're going to train on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2028" target="_blank">00:33:48.640</a></span> | <span class="t">And then we have both the inputs and the targets at every single point here. So to fully spell out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2033" target="_blank">00:33:53.680</a></span> | <span class="t">what's contained in a single 4 by 8 batch to the transformer, I sort of compact it here. So when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2039" target="_blank">00:33:59.920</a></span> | <span class="t">the input is 47 by itself, the target is 58. And when the input is the sequence 47, 58, the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2047" target="_blank">00:34:07.760</a></span> | <span class="t">is 1. And when it's 47, 58, 1, the target is 51, and so on. So actually the single batch of examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2054" target="_blank">00:34:14.960</a></span> | <span class="t">that's 4 by 8 actually has a ton of individual examples that we are expecting the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2059" target="_blank">00:34:19.200</a></span> | <span class="t">to learn on in parallel. And so you'll see that the batches are learned on completely independently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2065" target="_blank">00:34:25.040</a></span> | <span class="t">but the time dimension sort of here along horizontally is also trained on in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2070" target="_blank">00:34:30.880</a></span> | <span class="t">So sort of your real batch size is more like b times t. It's just that the context grows linearly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2077" target="_blank">00:34:37.280</a></span> | <span class="t">for the predictions that you make along the t direction in the model. So this is all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2084" target="_blank">00:34:44.000</a></span> | <span class="t">examples that the model will learn from this single batch. So now this is the GPT class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2091" target="_blank">00:34:51.760</a></span> | <span class="t">And because this is a decoder-only model, so we're not going to have an encoder because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2097" target="_blank">00:34:57.760</a></span> | <span class="t">there's no, like, English we're translating from. We're not trying to condition on some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2101" target="_blank">00:35:01.280</a></span> | <span class="t">other external information. We're just trying to produce a sequence of words that follow each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2106" target="_blank">00:35:06.080</a></span> | <span class="t">or are likely to. So this is all PyTorch. And I'm going slightly faster because I'm assuming people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2111" target="_blank">00:35:11.360</a></span> | <span class="t">have taken 231n or something along those lines. But here in the forward pass, we take these indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2118" target="_blank">00:35:18.080</a></span> | <span class="t">and then we both encode the identity of the indices just via an embedding lookup table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2126" target="_blank">00:35:26.720</a></span> | <span class="t">So every single integer has a - we index into a lookup table of vectors in this nn.embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2133" target="_blank">00:35:33.680</a></span> | <span class="t">and pull out the word vector for that token. And then because the message - because transformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2140" target="_blank">00:35:40.800</a></span> | <span class="t">by itself doesn't actually - it processes sets natively, so we need to also positionally encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2145" target="_blank">00:35:45.280</a></span> | <span class="t">these vectors so that we basically have both the information about the token identity and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2149" target="_blank">00:35:49.680</a></span> | <span class="t">its place in the sequence from one to block size. Now those - the information about what and where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2156" target="_blank">00:35:56.960</a></span> | <span class="t">is combined additively. So the token embeddings and the positional embeddings are just added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2160" target="_blank">00:36:00.720</a></span> | <span class="t">exactly as here. So this x here, then there's optional dropout. This x here basically just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2167" target="_blank">00:36:07.920</a></span> | <span class="t">contains the set of words and their positions, and that feeds into the blocks of transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2176" target="_blank">00:36:16.960</a></span> | <span class="t">And we're going to look into what's blocked here. But for here, for now, this is just a series of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2180" target="_blank">00:36:20.560</a></span> | <span class="t">blocks in the transformer. And then in the end, there's a layer norm, and then you're decoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2185" target="_blank">00:36:25.760</a></span> | <span class="t">the logits for the next word or next integer in the sequence using a linear projection of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2192" target="_blank">00:36:32.080</a></span> | <span class="t">the output of this transformer. So lm_head here, short for language model head, is just a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2197" target="_blank">00:36:37.600</a></span> | <span class="t">function. So basically, positionally encode all the words, feed them into a sequence of blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2205" target="_blank">00:36:45.200</a></span> | <span class="t">and then apply a linear layer to get the probability distribution for the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2208" target="_blank">00:36:48.640</a></span> | <span class="t">character. And then if we have the targets, which we produced in the data loader, and you'll notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2214" target="_blank">00:36:54.960</a></span> | <span class="t">that the targets are just the inputs offset by one in time, then those targets feed into a cross</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2220" target="_blank">00:37:00.560</a></span> | <span class="t">entropy loss. So this is just a negative one likelihood typical classification loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2224" target="_blank">00:37:04.000</a></span> | <span class="t">So now let's drill into what's here in the blocks. So these blocks that are applied sequentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2230" target="_blank">00:37:10.320</a></span> | <span class="t">there's again, as I mentioned, this communicate phase and the compute phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2234" target="_blank">00:37:14.880</a></span> | <span class="t">So in the communicate phase, all the nodes get to talk to each other, and so these nodes are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2239" target="_blank">00:37:19.760</a></span> | <span class="t">basically - if our block size is eight, then we are going to have eight nodes in this graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2246" target="_blank">00:37:26.560</a></span> | <span class="t">There's eight nodes in this graph, the first node is pointed to only by itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2250" target="_blank">00:37:30.240</a></span> | <span class="t">the second node is pointed to by the first node and itself, the third node is pointed to by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2255" target="_blank">00:37:35.040</a></span> | <span class="t">first two nodes and itself, etc. So there's eight nodes here. So you apply - there's a residual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2261" target="_blank">00:37:41.200</a></span> | <span class="t">pathway in x, you take it out, you apply a layer norm, and then the self-attention so that these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2265" target="_blank">00:37:45.840</a></span> | <span class="t">communicate, these eight nodes communicate, but you have to keep in mind that the batch is four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2270" target="_blank">00:37:50.240</a></span> | <span class="t">So because batch is four, this is also applied - so we have eight nodes communicating, but there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2276" target="_blank">00:37:56.080</a></span> | <span class="t">a batch of four of them all individually communicating among those eight nodes. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2279" target="_blank">00:37:59.920</a></span> | <span class="t">no crisscross across the batch dimension, of course. There's no batch normalization anywhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2282" target="_blank">00:38:02.960</a></span> | <span class="t">luckily. And then once they've changed information, they are processed using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2288" target="_blank">00:38:08.480</a></span> | <span class="t">multilayer perceptron, and that's the compute phase. And then also here, we are missing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2294" target="_blank">00:38:14.160</a></span> | <span class="t">the cross-attention, because this is a decoder-only model. So all we have is this step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2301" target="_blank">00:38:21.280</a></span> | <span class="t">here, the multi-headed attention, and that's this line, the communicate phase, and then we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2305" target="_blank">00:38:25.040</a></span> | <span class="t">feedforward, which is the MLP, and that's the compute phase. I'll take questions a bit later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2310" target="_blank">00:38:30.640</a></span> | <span class="t">Then the MLP here is fairly straightforward. The MLP is just individual processing on each node,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2317" target="_blank">00:38:37.360</a></span> | <span class="t">just transforming the feature representation sort of at that node. So applying a two-layer neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2325" target="_blank">00:38:45.040</a></span> | <span class="t">net with a GELU non-linearity, which is - just think of it as a RELU or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2329" target="_blank">00:38:49.360</a></span> | <span class="t">It's just a non-linearity. And then MLP is straightforward. I don't think there's anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2334" target="_blank">00:38:54.480</a></span> | <span class="t">too crazy there. And then this is the causal self-attention part, the communication phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2338" target="_blank">00:38:58.800</a></span> | <span class="t">So this is kind of like the meat of things and the most complicated part. It's only complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2344" target="_blank">00:39:04.560</a></span> | <span class="t">because of the batching and the implementation detail of how you mask the connectivity in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2350" target="_blank">00:39:10.800</a></span> | <span class="t">graph so that you can't obtain any information from the future when you're predicting your token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2356" target="_blank">00:39:16.240</a></span> | <span class="t">Otherwise, it gives away the information. So if I'm the fifth token, and if I'm the fifth position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2363" target="_blank">00:39:23.120</a></span> | <span class="t">then I'm getting the fourth token coming into the input, and I'm attending to the third,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2367" target="_blank">00:39:27.760</a></span> | <span class="t">second, and first, and I'm trying to figure out what is the next token, well then in this batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2373" target="_blank">00:39:33.440</a></span> | <span class="t">in the next element over in the time dimension, the answer is at the input. So I can't get any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2378" target="_blank">00:39:38.960</a></span> | <span class="t">information from there. So that's why this is all tricky. But basically in the forward pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2382" target="_blank">00:39:42.800</a></span> | <span class="t">we are calculating the queries, keys, and values based on x. So these are the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2391" target="_blank">00:39:51.360</a></span> | <span class="t">queries, and values. Here, when I'm computing the attention, I have the queries matrix multiplying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2397" target="_blank">00:39:57.520</a></span> | <span class="t">the keys. So this is the dot product in parallel for all the queries and all the keys, and all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2402" target="_blank">00:40:02.400</a></span> | <span class="t">heads. So I failed to mention that there's also the aspect of the heads, which is also done all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2408" target="_blank">00:40:08.080</a></span> | <span class="t">in parallel here. So we have the batch dimension, the time dimension, and the head dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2411" target="_blank">00:40:11.920</a></span> | <span class="t">and you end up with five-dimensional tensors, and it's all really confusing. So I invite you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2415" target="_blank">00:40:15.200</a></span> | <span class="t">to step through it later and convince yourself that this is actually doing the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2419" target="_blank">00:40:19.040</a></span> | <span class="t">But basically, you have the batch dimension, the head dimension, and the time dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2423" target="_blank">00:40:23.360</a></span> | <span class="t">and then you have features at them. And so this is evaluating for all the batch elements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2428" target="_blank">00:40:28.320</a></span> | <span class="t">for all the head elements, and all the time elements, the simple Python that I gave you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2432" target="_blank">00:40:32.800</a></span> | <span class="t">earlier, which is query dot product p. Then here, we do a masked fill. And what this is doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2439" target="_blank">00:40:39.280</a></span> | <span class="t">it's basically clamping the attention between the nodes that are not supposed to communicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2445" target="_blank">00:40:45.600</a></span> | <span class="t">to be negative infinity. And we're doing negative infinity because we're about to softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2450" target="_blank">00:40:50.080</a></span> | <span class="t">and so negative infinity will make basically the attention of those elements be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2453" target="_blank">00:40:53.680</a></span> | <span class="t">And so here, we are going to basically end up with the weights, the sort of affinities between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2461" target="_blank">00:41:01.520</a></span> | <span class="t">these nodes, optional dropout, and then here, attention matrix multiply v is basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2467" target="_blank">00:41:07.680</a></span> | <span class="t">gathering of the information according to the affinities we've calculated. And this is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2472" target="_blank">00:41:12.560</a></span> | <span class="t">weighted sum of the values at all those nodes. So this matrix multipliers is doing that weighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2477" target="_blank">00:41:17.840</a></span> | <span class="t">sum. And then transpose contiguous view, because it's all complicated and bashed in five-dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2483" target="_blank">00:41:23.440</a></span> | <span class="t">tensors, but it's really not doing anything, optional dropout, and then a linear projection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2488" target="_blank">00:41:28.560</a></span> | <span class="t">back to the residual pathway. So this is implementing the communication phase here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2492" target="_blank">00:41:32.960</a></span> | <span class="t">Then you can train this transformer, and then you can generate infinite Shakespeare,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2501" target="_blank">00:41:41.120</a></span> | <span class="t">and you will simply do this by - because our block size is eight, we start with a sum token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2506" target="_blank">00:41:46.480</a></span> | <span class="t">say like, I use in this case, you can use something like a muon as the start token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2512" target="_blank">00:41:52.400</a></span> | <span class="t">and then you communicate only to yourself because there's a single node, and you get the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2517" target="_blank">00:41:57.440</a></span> | <span class="t">distribution for the first word in the sequence, and then you decode it, or the first character in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2523" target="_blank">00:42:03.680</a></span> | <span class="t">the sequence, you decode the character, and then you bring back the character, and you re-encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2527" target="_blank">00:42:07.600</a></span> | <span class="t">it as an integer, and now you have the second thing. And so you get, okay, we're at the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2533" target="_blank">00:42:13.360</a></span> | <span class="t">position, and this is whatever integer it is, add the positional encodings, goes into the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2538" target="_blank">00:42:18.800</a></span> | <span class="t">goes into transformer, and again, this token now communicates with the first token and its identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2546" target="_blank">00:42:26.640</a></span> | <span class="t">And so you just keep plugging it back, and once you run out of the block size, which is eight,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2550" target="_blank">00:42:30.880</a></span> | <span class="t">you start to crop, because you can never have block size more than eight in the way you've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2554" target="_blank">00:42:34.560</a></span> | <span class="t">trained this transformer. So we have more and more context until eight, and then if you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2558" target="_blank">00:42:38.240</a></span> | <span class="t">to generate beyond eight, you have to start cropping, because the transformer only works for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2561" target="_blank">00:42:41.920</a></span> | <span class="t">eight elements in time dimension. And so all of these transformers in the naive setting have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2568" target="_blank">00:42:48.000</a></span> | <span class="t">finite block size, or context length, and in typical models, this will be 1024 tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2573" target="_blank">00:42:53.760</a></span> | <span class="t">or 2048 tokens, something like that, but these tokens are usually like DPE tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2578" target="_blank">00:42:58.480</a></span> | <span class="t">or sentence piece tokens, or workpiece tokens, there's many different encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2582" target="_blank">00:43:02.480</a></span> | <span class="t">So it's not like that long, and so that's why I think I did mention, we really want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2585" target="_blank">00:43:05.680</a></span> | <span class="t">to expand the context size, and it gets gnarly, because the attention is quadratic in many cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2589" target="_blank">00:43:09.920</a></span> | <span class="t">Now, if you want to implement an encoder instead of a decoder attention, then all you have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2599" target="_blank">00:43:19.680</a></span> | <span class="t">is this mask node, and you just delete that line. So if you don't mask the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2605" target="_blank">00:43:25.360</a></span> | <span class="t">then all the nodes communicate to each other, and everything is allowed, and information flows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2609" target="_blank">00:43:29.840</a></span> | <span class="t">between all the nodes. So if you want to have the encoder here, just delete all the encoder blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2616" target="_blank">00:43:36.960</a></span> | <span class="t">we'll use attention, where this line is deleted, that's it. So you're allowing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2621" target="_blank">00:43:41.440</a></span> | <span class="t">whatever this encoder might store, say 10 tokens, like 10 nodes, and they are all allowed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2627" target="_blank">00:43:47.040</a></span> | <span class="t">communicate to each other, going up the transformer. And then if you want to implement cross attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2633" target="_blank">00:43:53.360</a></span> | <span class="t">so you have a full encoder decoder transformer, not just a decoder only transformer, or GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2639" target="_blank">00:43:59.280</a></span> | <span class="t">then we need to also add cross attention in the middle. So here, there's a self attention piece,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2645" target="_blank">00:44:05.520</a></span> | <span class="t">where all the, there's a self attention piece, a cross attention piece, and this MLP. And in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2650" target="_blank">00:44:10.160</a></span> | <span class="t">cross attention, we need to take the features from the top of the encoder, we need to add one more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2655" target="_blank">00:44:15.760</a></span> | <span class="t">line here, and this would be the cross attention, instead of, I should have implemented it, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2661" target="_blank">00:44:21.680</a></span> | <span class="t">of just pointing, I think. But there'll be a cross attention line here, so we'll have three lines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2666" target="_blank">00:44:26.480</a></span> | <span class="t">because we need to add another block. And the queries will come from x, but the keys and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2671" target="_blank">00:44:31.520</a></span> | <span class="t">values will come from the top of the encoder. And there will be basically information flowing from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2677" target="_blank">00:44:37.040</a></span> | <span class="t">the encoder strictly to all the nodes inside x. And then that's it. So it's very simple sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2683" target="_blank">00:44:43.760</a></span> | <span class="t">modifications on the decoder attention. So you'll hear people talk that you kind of have a decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2689" target="_blank">00:44:49.920</a></span> | <span class="t">only model, like GPT, you can have an encoder only model, like BERT, or you can have an encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2694" target="_blank">00:44:54.800</a></span> | <span class="t">decoder model, like say T5, doing things like machine translation. So, and in BERT, you can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2701" target="_blank">00:45:01.440</a></span> | <span class="t">train it using sort of this language modeling setup that's autoregressive, and you're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2706" target="_blank">00:45:06.080</a></span> | <span class="t">trying to predict the next element in the sequence, you're training it with slightly different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2708" target="_blank">00:45:08.880</a></span> | <span class="t">objectives, you're putting in like the full sentence, and the full sentence is allowed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2713" target="_blank">00:45:13.040</a></span> | <span class="t">communicate fully, and then you're trying to classify sentiment or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2717" target="_blank">00:45:17.360</a></span> | <span class="t">So you're not trying to model like the next token in the sequence. So these are trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2721" target="_blank">00:45:21.840</a></span> | <span class="t">slightly different with mask, with using masking and other denoising techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2730" target="_blank">00:45:30.000</a></span> | <span class="t">Okay, so that's kind of like the transformer. I'm going to continue. So yeah, maybe more questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2738" target="_blank">00:45:38.320</a></span> | <span class="t">These are excellent questions. So when we're employing information,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2743" target="_blank">00:45:43.760</a></span> | <span class="t">for instance, like the graph that we all did, and when we were like, something like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2748" target="_blank">00:45:48.480</a></span> | <span class="t">you know, this transformer still performs, like it's a dynamic graph, that the connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2756" target="_blank">00:45:56.880</a></span> | <span class="t">change in every instance, and you also have some feature information. So just like, we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2762" target="_blank">00:46:02.720</a></span> | <span class="t">enforcing these constraints on it by just masking, but it is aware of the work that it tends to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2769" target="_blank">00:46:09.840</a></span> | <span class="t">So I'm not sure if I fully followed. So there's different ways to look at this analogy, but one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2776" target="_blank">00:46:16.880</a></span> | <span class="t">analogy is you can interpret this graph as really fixed. It's just that every time we do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2781" target="_blank">00:46:21.120</a></span> | <span class="t">communicate, we are using different weights. You can look at it that way. So if we have block size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2785" target="_blank">00:46:25.440</a></span> | <span class="t">of eight in my example, we would have eight nodes. Here we have two, four, six, okay, so we'd have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2790" target="_blank">00:46:30.080</a></span> | <span class="t">eight nodes. They would be connected in, you lay them out, and you only connect from left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2795" target="_blank">00:46:35.200</a></span> | <span class="t">But for a different problem, that might not be the case, but you have a graph where the connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2800" target="_blank">00:46:40.160</a></span> | <span class="t">might change. Why would the connection, usually the connections don't change as a function of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2806" target="_blank">00:46:46.160</a></span> | <span class="t">the data or something like that. That means like the molecules look like an actual graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2810" target="_blank">00:46:50.880</a></span> | <span class="t">and look like that. I don't think I've seen a single example where the connectivity changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2822" target="_blank">00:47:02.960</a></span> | <span class="t">dynamically in function of data. Usually the connectivity is fixed. If you have an encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2826" target="_blank">00:47:06.560</a></span> | <span class="t">and you're training a BERT, you have how many tokens you want, and they are fully connected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2830" target="_blank">00:47:10.720</a></span> | <span class="t">And if you have a decoder only model, you have this triangular thing. And if you have encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2835" target="_blank">00:47:15.600</a></span> | <span class="t">decoder, then you have awkwardly sort of like two pools of nodes. Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2844" target="_blank">00:47:24.800</a></span> | <span class="t">[inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2868" target="_blank">00:47:48.880</a></span> | <span class="t">Yeah, it's really hard to say. So that's why I think this paper is so interesting is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2896" target="_blank">00:48:16.880</a></span> | <span class="t">yeah, usually you'd see like a path, and maybe they had path internally. They just didn't publish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2900" target="_blank">00:48:20.320</a></span> | <span class="t">it. All you can see is sort of things that didn't look like a transformer. I mean, you have ResNets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2904" target="_blank">00:48:24.800</a></span> | <span class="t">which have lots of this. But a ResNet would be kind of like this, but there's no self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2910" target="_blank">00:48:30.480</a></span> | <span class="t">component. But the MLP is there kind of in a ResNet. So a ResNet looks very much like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2917" target="_blank">00:48:37.600</a></span> | <span class="t">except there's no - you can use layer norms in ResNets, I believe, as well. Typically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2921" target="_blank">00:48:41.600</a></span> | <span class="t">sometimes they can be batch norms. So it is kind of like a ResNet. It is kind of like they took a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2926" target="_blank">00:48:46.000</a></span> | <span class="t">ResNet and they put in a self-attentionary block in addition to the pre-existing MLP block, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2932" target="_blank">00:48:52.160</a></span> | <span class="t">is kind of like convolutions. And MLP would, strictly speaking, be convolution, one-by-one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2936" target="_blank">00:48:56.240</a></span> | <span class="t">convolution. But I think the idea is similar in that MLP is just kind of like typical weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2942" target="_blank">00:49:02.960</a></span> | <span class="t">non-linearity weights or operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2951" target="_blank">00:49:11.120</a></span> | <span class="t">But I will say, yeah, it's kind of interesting because a lot of work is not there, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2956" target="_blank">00:49:16.800</a></span> | <span class="t">they give you this transformer, and then it turns out five years later, it's not changed, even though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2959" target="_blank">00:49:19.840</a></span> | <span class="t">everyone's trying to change it. So it's kind of interesting to me that it's kind of like a package,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2963" target="_blank">00:49:23.280</a></span> | <span class="t">in like a package, which I think is really interesting historically. And I also talked to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2968" target="_blank">00:49:28.160</a></span> | <span class="t">paper authors, and they were unaware of the impact that the transformer would have at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2973" target="_blank">00:49:33.840</a></span> | <span class="t">So when you read this paper, actually, it's kind of unfortunate because this is like the paper that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2979" target="_blank">00:49:39.280</a></span> | <span class="t">changed everything. But when people read it, it's like question marks, because it reads like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2983" target="_blank">00:49:43.200</a></span> | <span class="t">pretty random machine translation paper. Like, oh, we're doing machine translation. Oh, here's a cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2988" target="_blank">00:49:48.000</a></span> | <span class="t">architecture. OK, great, good results. It doesn't sort of know what's going to happen. And so when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=2995" target="_blank">00:49:55.440</a></span> | <span class="t">people read it today, I think they're kind of confused, potentially. I will have some tweets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3001" target="_blank">00:50:01.360</a></span> | <span class="t">at the end, but I think I would have renamed it with the benefit of hindsight of like, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3005" target="_blank">00:50:05.440</a></span> | <span class="t">I'll get to it. Yeah, I think that's a good question as well. Currently, I mean, I certainly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3024" target="_blank">00:50:24.560</a></span> | <span class="t">don't love the autoregressive modeling approach. I think it's kind of weird to sample a token and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3029" target="_blank">00:50:29.120</a></span> | <span class="t">then commit to it. So maybe there's some ways-- some hybrids with diffusion, as an example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3038" target="_blank">00:50:38.000</a></span> | <span class="t">which I think would be really cool. Or we'll find some other ways to edit the sequences later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3043" target="_blank">00:50:43.760</a></span> | <span class="t">but still in the autoregressive framework. But I think diffusion is kind of like an up-and-coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3048" target="_blank">00:50:48.960</a></span> | <span class="t">modeling approach that I personally find much more appealing. When I sample text, I don't go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3053" target="_blank">00:50:53.360</a></span> | <span class="t">chunk, chunk, chunk, and commit. I do a draft one, and then I do a better draft two. And that feels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3058" target="_blank">00:50:58.800</a></span> | <span class="t">like a diffusion process. So that would be my hope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3062" target="_blank">00:51:02.320</a></span> | <span class="t">OK, also a question. So yeah, I use like the Gartner logic where it takes a weight which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3070" target="_blank">00:51:10.560</a></span> | <span class="t">is like a graph. Will you say like the self-attention is sort of like computing like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3076" target="_blank">00:51:16.720</a></span> | <span class="t">an edge weight using the dot product on the node similarity, and then once we have the edge weight,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3081" target="_blank">00:51:21.680</a></span> | <span class="t">we just multiply it by the values, and then we just propagate it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3085" target="_blank">00:51:25.200</a></span> | <span class="t">Yes, that's right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3086" target="_blank">00:51:26.960</a></span> | <span class="t">And do you think there's like analogy between graph neural networks and self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3092" target="_blank">00:51:32.320</a></span> | <span class="t">I find the graph neural networks kind of like a confusing term, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3095" target="_blank">00:51:35.200</a></span> | <span class="t">I mean, yeah, previously there was this notion of-- I kind of feel like maybe today everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3101" target="_blank">00:51:41.920</a></span> | <span class="t">is a graph neural network, because the transformer is a graph neural network processor. The native</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3106" target="_blank">00:51:46.080</a></span> | <span class="t">representation that the transformer operates over is sets that are connected by edges in a directed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3110" target="_blank">00:51:50.880</a></span> | <span class="t">way. And so that's the native representation. And then, yeah. OK, I should go on, because I still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3116" target="_blank">00:51:56.640</a></span> | <span class="t">have like 30 slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3117" target="_blank">00:51:57.440</a></span> | <span class="t">Sorry, sorry, sorry. There's a question I want to say about this. [INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3127" target="_blank">00:52:07.920</a></span> | <span class="t">Oh, yeah. Yeah, the root D, I think, basically like if you're initializing with random weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3134" target="_blank">00:52:14.320</a></span> | <span class="t">separate from a Gaussian, as your dimension size grows, so does your values, the variance grows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3139" target="_blank">00:52:19.600</a></span> | <span class="t">and then your softmax will just become the one-half vector. So it's just a way to control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3144" target="_blank">00:52:24.720</a></span> | <span class="t">the variance and bring it to always be in a good range for softmax and nice diffuse distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3149" target="_blank">00:52:29.200</a></span> | <span class="t">OK, so it's almost like an initialization thing. OK, so transformers have been applied to all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3161" target="_blank">00:52:41.760</a></span> | <span class="t">other fields. And the way this was done is, in my opinion, kind of ridiculous ways, honestly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3167" target="_blank">00:52:47.680</a></span> | <span class="t">because I was a computer vision person, and you have comm nets, and they kind of make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3171" target="_blank">00:52:51.680</a></span> | <span class="t">So what we're doing now with bits, as an example, is you take an image, and you chop it up into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3175" target="_blank">00:52:55.520</a></span> | <span class="t">little squares. And then those squares literally feed into a transformer, and that's it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3179" target="_blank">00:52:59.440</a></span> | <span class="t">which is kind of ridiculous. And so, I mean, yeah. And so the transformer doesn't even,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3187" target="_blank">00:53:07.040</a></span> | <span class="t">in the simplest case, like really know where these patches might come from. They are usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3190" target="_blank">00:53:10.800</a></span> | <span class="t">positionally encoded, but it has to sort of like rediscover a lot of the structure, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3196" target="_blank">00:53:16.960</a></span> | <span class="t">of them in some ways. And it's kind of weird to approach it that way. But it's just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3204" target="_blank">00:53:24.000</a></span> | <span class="t">the simplest baseline of the chomping up big images into small squares and feeding them in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3208" target="_blank">00:53:28.560</a></span> | <span class="t">as like the individual nodes actually works fairly well. And then this is in the transformer encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3212" target="_blank">00:53:32.640</a></span> | <span class="t">So all the patches are talking to each other throughout the entire transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3215" target="_blank">00:53:35.920</a></span> | <span class="t">And the number of nodes here would be sort of like nine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3219" target="_blank">00:53:39.440</a></span> | <span class="t">Also, in speech recognition, you just take your MEL spectrogram, and you chop it up into little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3226" target="_blank">00:53:46.080</a></span> | <span class="t">slices and feed them into a transformer. So there was paper like this, but also Whisper. Whisper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3230" target="_blank">00:53:50.320</a></span> | <span class="t">a copy-based transformer. If you saw Whisper from OpenAI, you just chop up a MEL spectrogram and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3235" target="_blank">00:53:55.600</a></span> | <span class="t">feed it into a transformer, and then pretend you're dealing with text, and it works very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3239" target="_blank">00:53:59.760</a></span> | <span class="t">Decision transformer in RL, you take your states, actions, and reward that you experience in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3244" target="_blank">00:54:04.880</a></span> | <span class="t">environment, and you just pretend it's a language, and you start to model the sequences of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3249" target="_blank">00:54:09.520</a></span> | <span class="t">And then you can use that for planning later. That works pretty well. Even things like alpha</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3253" target="_blank">00:54:13.920</a></span> | <span class="t">folds. So we're frequently talking about molecules and how you can plug them in. So at the heart of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3258" target="_blank">00:54:18.240</a></span> | <span class="t">alpha fold computationally is also a transformer. One thing I wanted to also say about transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3263" target="_blank">00:54:23.840</a></span> | <span class="t">is I find that they're super flexible, and I really enjoy that. I'll give you an example from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3268" target="_blank">00:54:28.960</a></span> | <span class="t">Tesla. You have a ComNet that takes an image and makes predictions about the image. And then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3274" target="_blank">00:54:34.880</a></span> | <span class="t">big question is, how do you feed in extra information? And it's not always trivial. Say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3278" target="_blank">00:54:38.640</a></span> | <span class="t">I have additional information that I want to inform, that I want the outputs to be informed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3283" target="_blank">00:54:43.040</a></span> | <span class="t">by. Maybe I have other sensors, like radar. Maybe I have some map information, or a vehicle type,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3287" target="_blank">00:54:47.520</a></span> | <span class="t">or some audio. And the question is, how do you feed information into a ComNet? Where do you feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3292" target="_blank">00:54:52.000</a></span> | <span class="t">it in? Do you concatenate it? Do you add it? At what stage? And so with a transformer, it's much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3298" target="_blank">00:54:58.080</a></span> | <span class="t">easier, because you just take whatever you want, you chop it up into pieces, and you feed it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3301" target="_blank">00:55:01.760</a></span> | <span class="t">with a set of what you had before. And you let the self-attention figure out how everything should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3305" target="_blank">00:55:05.200</a></span> | <span class="t">communicate. And that actually, frankly, works. So just chop up everything and throw it into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3310" target="_blank">00:55:10.000</a></span> | <span class="t">mix is kind of the way. And it frees neural nets from this burden of Euclidean space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3316" target="_blank">00:55:16.960</a></span> | <span class="t">where previously you had to arrange your computation to conform to the Euclidean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3322" target="_blank">00:55:22.080</a></span> | <span class="t">space of three dimensions of how you're laying out the compute. The compute actually kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3326" target="_blank">00:55:26.560</a></span> | <span class="t">happens in almost 3D space, if you think about it. But in attention, everything is just sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3331" target="_blank">00:55:31.920</a></span> | <span class="t">So it's a very flexible framework, and you can just throw in stuff into your conditioning set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3335" target="_blank">00:55:35.680</a></span> | <span class="t">and everything just self-attended over. So it's quite beautiful from that perspective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3339" target="_blank">00:55:39.760</a></span> | <span class="t">OK. So now, what exactly makes transformers so effective? I think a good example of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3344" target="_blank">00:55:44.560</a></span> | <span class="t">comes from the GPT-3 paper, which I encourage people to read. Language models are two-shot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3349" target="_blank">00:55:49.440</a></span> | <span class="t">learners. I would have probably renamed this a little bit. I would have said something like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3354" target="_blank">00:55:54.000</a></span> | <span class="t">transformers are capable of in-context learning, or like meta-learning. That's kind of what makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3358" target="_blank">00:55:58.960</a></span> | <span class="t">them really special. So basically, the setting that they're working with is, OK, I have some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3363" target="_blank">00:56:03.040</a></span> | <span class="t">context, and I'm trying to, let's say, passage. This is just one example of many. I have a passage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3367" target="_blank">00:56:07.120</a></span> | <span class="t">and I'm asking questions about it. And then I'm giving, as part of the context, in the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3372" target="_blank">00:56:12.960</a></span> | <span class="t">I'm giving the questions and the answers. So I'm giving one example of question-answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3376" target="_blank">00:56:16.240</a></span> | <span class="t">another example of question-answer, another example of question-answer, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3379" target="_blank">00:56:19.120</a></span> | <span class="t">And this becomes, oh yeah, people are going to have to leave soon now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3383" target="_blank">00:56:23.040</a></span> | <span class="t">OK. This is really important, let me think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3390" target="_blank">00:56:30.160</a></span> | <span class="t">OK, so what's really interesting is basically like, with more examples given in the context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3395" target="_blank">00:56:35.440</a></span> | <span class="t">the accuracy improves. And so what that hints at is that the transformer is able to somehow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3400" target="_blank">00:56:40.000</a></span> | <span class="t">learn in the activations without doing any gradient descent in a typical fine-tuning fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3405" target="_blank">00:56:45.120</a></span> | <span class="t">So if you fine-tune, you have to give an example and the answer, and you do fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3410" target="_blank">00:56:50.160</a></span> | <span class="t">using gradient descent. But it looks like the transformer, internally in its weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3413" target="_blank">00:56:53.760</a></span> | <span class="t">is doing something that looks like potential gradient descent, some kind of a meta-learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3416" target="_blank">00:56:56.720</a></span> | <span class="t">in the weights of the transformer as it is reading the prompt. And so in this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3420" target="_blank">00:57:00.640</a></span> | <span class="t">they go into, OK, distinguishing this outer loop with stochastic gradient descent and this inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3424" target="_blank">00:57:04.880</a></span> | <span class="t">loop of the in-context learning. So the inner loop is, as the transformer, sort of like reading the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3429" target="_blank">00:57:09.040</a></span> | <span class="t">sequence almost, and the outer loop is the training by gradient descent. So basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3434" target="_blank">00:57:14.560</a></span> | <span class="t">there's some training happening in the activations of the transformer as it is consuming a sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3438" target="_blank">00:57:18.640</a></span> | <span class="t">that maybe very much looks like gradient descent. And so there's some recent papers that kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3442" target="_blank">00:57:22.320</a></span> | <span class="t">hint at this and study it. And so as an example, in this paper here, they propose something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3447" target="_blank">00:57:27.440</a></span> | <span class="t">the raw operator. And they argue that the raw operator is implemented by a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3452" target="_blank">00:57:32.880</a></span> | <span class="t">and then they show that you can implement things like ridge regression on top of a raw operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3456" target="_blank">00:57:36.800</a></span> | <span class="t">And so this is kind of giving - their paper is hinting that maybe there is some thing that looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3461" target="_blank">00:57:41.680</a></span> | <span class="t">like gradient-based learning inside the activations of the transformer. And I think this is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3466" target="_blank">00:57:46.720</a></span> | <span class="t">impossible to think through, because what is gradient-based learning? Forward pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3470" target="_blank">00:57:50.320</a></span> | <span class="t">backward pass, and then update. Well, that looks like a resonant, right, because you're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3474" target="_blank">00:57:54.800</a></span> | <span class="t">changing - you're adding to the weights. So you start with initial random set of weights, forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3480" target="_blank">00:58:00.160</a></span> | <span class="t">pass, backward pass, and update your weights, and then forward pass, backward pass, update weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3483" target="_blank">00:58:03.840</a></span> | <span class="t">Looks like a resonant. Transformer is a resonant. So much more hand-wavy, but basically some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3491" target="_blank">00:58:11.280</a></span> | <span class="t">papers trying to hint at why that could be potentially possible. And then I have a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3495" target="_blank">00:58:15.760</a></span> | <span class="t">of tweets. I just got them pasted here in the end. This was kind of meant for general consumption,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3500" target="_blank">00:58:20.560</a></span> | <span class="t">so they're a bit more high-level and hype-y a little bit. But I'm talking about why this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3504" target="_blank">00:58:24.960</a></span> | <span class="t">architecture is so interesting and why it potentially became so popular. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3508" target="_blank">00:58:28.720</a></span> | <span class="t">it simultaneously optimizes three properties that I think are very desirable. Number one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3512" target="_blank">00:58:32.320</a></span> | <span class="t">the transformer is very expressive in the forward pass. It's able to implement very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3517" target="_blank">00:58:37.600</a></span> | <span class="t">interesting functions, potentially functions that can even do meta-learning. Number two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3522" target="_blank">00:58:42.480</a></span> | <span class="t">it is very optimizable, thanks to things like residual connections, layer knowns, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3526" target="_blank">00:58:46.240</a></span> | <span class="t">And number three, it's extremely efficient. This is not always appreciated, but the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3530" target="_blank">00:58:50.080</a></span> | <span class="t">if you look at the computational graph, is a shallow wide network, which is perfect to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3534" target="_blank">00:58:54.720</a></span> | <span class="t">advantage of the parallelism of GPUs. So I think the transformer was designed very deliberately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3538" target="_blank">00:58:58.560</a></span> | <span class="t">to run efficiently on GPUs. There's previous work like neural GPU that I really enjoy as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3544" target="_blank">00:59:04.960</a></span> | <span class="t">which is really just like how do we design neural nets that are efficient on GPUs, and thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3549" target="_blank">00:59:09.040</a></span> | <span class="t">backwards from the constraints of the hardware, which I think is a very interesting way to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3552" target="_blank">00:59:12.000</a></span> | <span class="t">about it. Oh yeah, so here I'm saying I probably would have called the transformer a general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3564" target="_blank">00:59:24.000</a></span> | <span class="t">purpose efficient optimizable computer instead of attention is all you need. That's what I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3568" target="_blank">00:59:28.960</a></span> | <span class="t">have maybe in hindsight called that paper. It's proposing a model that is very general purpose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3576" target="_blank">00:59:36.960</a></span> | <span class="t">so forward pass is expressive. It's very efficient in terms of GPU usage, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3582" target="_blank">00:59:42.000</a></span> | <span class="t">easily optimizable by gradient descent, and trains very nicely. Then I have some other hype tweets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3588" target="_blank">00:59:48.160</a></span> | <span class="t">here. Anyway, so you can read them later, but I think this one is maybe interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3594" target="_blank">00:59:54.960</a></span> | <span class="t">So if previous neural nets are special purpose computers designed for a specific task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3600" target="_blank">01:00:00.160</a></span> | <span class="t">GPT is a general purpose computer reconfigurable at runtime to run natural language programs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3605" target="_blank">01:00:05.920</a></span> | <span class="t">So the programs are given as prompts, and then GPT runs the program by completing the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3611" target="_blank">01:00:11.200</a></span> | <span class="t">So I really like these analogies personally to computer. It's just like a powerful computer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3618" target="_blank">01:00:18.080</a></span> | <span class="t">and it's optimizable by gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3630" target="_blank">01:00:30.000</a></span> | <span class="t">I don't know. Okay, you can read this later, but for now I'll just leave this up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3636" target="_blank">01:00:36.720</a></span> | <span class="t">So sorry, I just found this tweet. So it turns out that if you scale up the training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3649" target="_blank">01:00:49.520</a></span> | <span class="t">and use a powerful enough neural net like a transformer, the network becomes a kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3652" target="_blank">01:00:52.880</a></span> | <span class="t">general purpose computer over text. So I think that's a kind of like nice way to look at it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3656" target="_blank">01:00:56.640</a></span> | <span class="t">and instead of performing a single text sequence, you can design the sequence in the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3660" target="_blank">01:01:00.400</a></span> | <span class="t">and because the transformer is both powerful but also is trained on a large enough,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3664" target="_blank">01:01:04.000</a></span> | <span class="t">very hard data set, it kind of becomes a general purpose text computer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3667" target="_blank">01:01:07.440</a></span> | <span class="t">and so I think that's kind of interesting way to look at it. Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3672" target="_blank">01:01:12.320</a></span> | <span class="t">Um, you have three points to the vote. Yeah. Um, so I guess, like, for me, I learned about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3686" target="_blank">01:01:26.800</a></span> | <span class="t">a number of things, but now we've seen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3716" target="_blank">01:01:56.720</a></span> | <span class="t">kind of, like, the idea that, like, you think there's really no harm from gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3721" target="_blank">01:02:01.280</a></span> | <span class="t">and I guess my question is, how much do you think it's, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3725" target="_blank">01:02:05.040</a></span> | <span class="t">it's pretty, really, like, most of it, you know, like, do they really think that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3730" target="_blank">01:02:10.800</a></span> | <span class="t">mostly more efficient, or do you think it's very, sort of, like, something that you have that, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3737" target="_blank">01:02:17.120</a></span> | <span class="t">you need the equivalent value of specific [inaudible] or do you [inaudible]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3744" target="_blank">01:02:24.240</a></span> | <span class="t">Yeah. So I think there's a bit of that, yeah. So I would say RNNs, like, in principle, yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3749" target="_blank">01:02:29.680</a></span> | <span class="t">they can implement arbitrary programs. I think it's kind of, like, a useless statement to some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3753" target="_blank">01:02:33.280</a></span> | <span class="t">extent, because they are not - they're probably - I'm not sure that they're probably expressive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3758" target="_blank">01:02:38.080</a></span> | <span class="t">because in a sense of, like, power, in that they can implement these arbitrary functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3762" target="_blank">01:02:42.160</a></span> | <span class="t">but they're not optimizable, and they're certainly not efficient, because they are serial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3766" target="_blank">01:02:46.960</a></span> | <span class="t">computing devices. So I think - so if you look at it as a compute graph, RNNs are very long,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3773" target="_blank">01:02:53.680</a></span> | <span class="t">thin compute graph. Like, if you stretched out the neurons, and you look, like, take all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3781" target="_blank">01:03:01.200</a></span> | <span class="t">individual neurons in our connectivity, and stretch them out, and try to visualize them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3784" target="_blank">01:03:04.400</a></span> | <span class="t">RNNs would be, like, a very long graph, and it's bad, and it's bad also for optimizability, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3789" target="_blank">01:03:09.440</a></span> | <span class="t">I don't exactly know why, but just the rough intuition is when you're backpropagating,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3793" target="_blank">01:03:13.680</a></span> | <span class="t">you don't want to make too many steps. And so transformers are a shallow, wide graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3798" target="_blank">01:03:18.240</a></span> | <span class="t">and so from supervision to inputs is a very small number of hops, and it's along residual pathways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3805" target="_blank">01:03:25.760</a></span> | <span class="t">which make gradients flow very easily, and there's all these layer norms to control the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3808" target="_blank">01:03:28.960</a></span> | <span class="t">scales of all of those activations. And so there's not too many hops, and you're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3815" target="_blank">01:03:35.760</a></span> | <span class="t">from supervision to input very quickly, and this flows through the graph. And it can all be done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3821" target="_blank">01:03:41.920</a></span> | <span class="t">in parallel, so you don't need to do this encoder-decoder RNNs, you have to go from first word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3826" target="_blank">01:03:46.080</a></span> | <span class="t">then second word, then third word, but here in transformer, every single word was processed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3830" target="_blank">01:03:50.400</a></span> | <span class="t">completely as sort of in parallel, which is kind of - so I think all these are really important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3836" target="_blank">01:03:56.080</a></span> | <span class="t">because all these are really important, and I think number three is less talked about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3840" target="_blank">01:04:00.080</a></span> | <span class="t">but extremely important, because in deep learning, scale matters, and so the size of the network that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3844" target="_blank">01:04:04.800</a></span> | <span class="t">you can train gives you - is extremely important, and so if it's efficient on the current hardware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3850" target="_blank">01:04:10.320</a></span> | <span class="t">then you can make it bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3856" target="_blank">01:04:16.080</a></span> | <span class="t">No, so yeah, so you take your image, and you apparently chop them up into patches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3873" target="_blank">01:04:33.040</a></span> | <span class="t">so there's the first thousand tokens or whatever, and now I have a special - so radar could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3878" target="_blank">01:04:38.880</a></span> | <span class="t">also - but I don't actually know the native representation of radar, so - but you could - you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3884" target="_blank">01:04:44.720</a></span> | <span class="t">just need to chop it up and enter it, and then you have to encode it somehow. Like, the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3888" target="_blank">01:04:48.160</a></span> | <span class="t">needs to know that they're coming from radar, so you create a special - you have some kind of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3892" target="_blank">01:04:52.800</a></span> | <span class="t">special token that you - like, these radar tokens are slightly different in representation, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3898" target="_blank">01:04:58.720</a></span> | <span class="t">learnable by gradient descent, and like, vehicle information would also come in with a special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3904" target="_blank">01:05:04.320</a></span> | <span class="t">embedding token that can be learned. So have you learned those, like, orally?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3911" target="_blank">01:05:11.280</a></span> | <span class="t">You don't, it's all just a set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3913" target="_blank">01:05:13.440</a></span> | <span class="t">Yeah, it's all just a set, but you can positionally encode these sets if you want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3923" target="_blank">01:05:23.120</a></span> | <span class="t">so - but positional encoding means you can hardwire, for example, the coordinates, like using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3928" target="_blank">01:05:28.320</a></span> | <span class="t">sinusoids and cosines, you can hardwire that, but it's better if you don't hardwire the position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3933" target="_blank">01:05:33.120</a></span> | <span class="t">you just - it's just a vector that is always hanging out at this location,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3936" target="_blank">01:05:36.240</a></span> | <span class="t">whatever content is there just adds on it, and this vector is trainable by background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3939" target="_blank">01:05:39.520</a></span> | <span class="t">that's how you do it. Yeah, go for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3943" target="_blank">01:05:43.760</a></span> | <span class="t">I'm not sure if I understand the question. So I mean, the positional encoder is like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3972" target="_blank">01:06:12.800</a></span> | <span class="t">they're actually like, not - they have - okay, so they have very little inductive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3976" target="_blank">01:06:16.560</a></span> | <span class="t">bias or something like that, they're just vectors hanging out in location always,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3979" target="_blank">01:06:19.520</a></span> | <span class="t">and you're trying to help the network in some way, and I think the intuition is good, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3987" target="_blank">01:06:27.360</a></span> | <span class="t">like, if you have enough data, usually trying to mess with it is like a bad thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3991" target="_blank">01:06:31.920</a></span> | <span class="t">Like, trying to enter knowledge when you have enough knowledge in the data set itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=3996" target="_blank">01:06:36.800</a></span> | <span class="t">is not usually productive, so it really depends on what scale you are. If you have infinity data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4001" target="_blank">01:06:41.280</a></span> | <span class="t">then you actually want to encode less and less, that turns out to work better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4004" target="_blank">01:06:44.160</a></span> | <span class="t">and if you have very little data, then actually you do want to encode some biases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4007" target="_blank">01:06:47.520</a></span> | <span class="t">and maybe if you have a much smaller data set, then maybe convolutions are a good idea,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4010" target="_blank">01:06:50.640</a></span> | <span class="t">because you actually have this bias coming from more filters. And so - but I think - so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4017" target="_blank">01:06:57.920</a></span> | <span class="t">transformer is extremely general, but there are ways to mess with the encodings to put in more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4021" target="_blank">01:07:01.600</a></span> | <span class="t">structure, like you could, for example, encode sinuses and cosines and fix it, or you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4025" target="_blank">01:07:05.840</a></span> | <span class="t">actually go to the attention mechanism and say, okay, if my image is chopped up into patches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4031" target="_blank">01:07:11.040</a></span> | <span class="t">this patch can only communicate to this neighborhood, and you can - you just do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4034" target="_blank">01:07:14.080</a></span> | <span class="t">in the attention matrix, just mask out whatever you don't want to communicate. And so people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4038" target="_blank">01:07:18.720</a></span> | <span class="t">really play with this, because the full attention is inefficient, so they will intersperse,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4043" target="_blank">01:07:23.840</a></span> | <span class="t">for example, layers that only communicate in little patches, and then layers that communicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4047" target="_blank">01:07:27.920</a></span> | <span class="t">globally, and they will sort of do all kinds of tricks like that. So you can slowly bring in more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4052" target="_blank">01:07:32.960</a></span> | <span class="t">inductive bias, you would do it - but the inductive biases are sort of like, they're factored out from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4057" target="_blank">01:07:37.920</a></span> | <span class="t">the core transformer, and they are factored out in the connectivity of the nodes, and they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4062" target="_blank">01:07:42.960</a></span> | <span class="t">factored out in the positional encodings, and you can mess with this for computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4082" target="_blank">01:08:02.480</a></span> | <span class="t">So there's probably about 200 papers on this now, if not more. They're kind of hard to track up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4087" target="_blank">01:08:07.920</a></span> | <span class="t">honestly, like my Safari browser, which is - oh, it's on my computer, like 200 open tabs. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4093" target="_blank">01:08:13.840</a></span> | <span class="t">yes, I'm not even sure if I want to pick my favorite, honestly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4102" target="_blank">01:08:22.000</a></span> | <span class="t">Yeah, I think it was a very interesting talk from you this year, and you can think of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4108" target="_blank">01:08:28.160</a></span> | <span class="t">transformer as like a CPU. I think the first test was to take five instructions out of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4112" target="_blank">01:08:32.400</a></span> | <span class="t">4,000 programs, and then now, at the beginning of the CPU, what you have is like you store variables,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4117" target="_blank">01:08:37.440</a></span> | <span class="t">you have memory, so it's like, if I want to do a debugger program of the CPU, I just do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4121" target="_blank">01:08:41.120</a></span> | <span class="t">multiple times. So maybe you can use a transformer like that. The other one that I actually like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4126" target="_blank">01:08:46.640</a></span> | <span class="t">even more is potentially keep the context length fixed, but allow the network to somehow use a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4131" target="_blank">01:08:51.600</a></span> | <span class="t">scratchpad. And so the way this works is you will teach the transformer somehow, via examples in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4137" target="_blank">01:08:57.040</a></span> | <span class="t">the prompt, that hey, you actually have a scratchpad. Hey, basically, you can't remember too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4141" target="_blank">01:09:01.440</a></span> | <span class="t">much. Your context line is finite. But you can use a scratchpad, and you do that by emitting a start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4146" target="_blank">01:09:06.080</a></span> | <span class="t">scratchpad, and then writing whatever you want to remember, and then end scratchpad. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4150" target="_blank">01:09:10.480</a></span> | <span class="t">you continue with whatever you want. And then later, when it's decoding, you actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4155" target="_blank">01:09:15.200</a></span> | <span class="t">special logic that when you detect start scratchpad, you will sort of like save whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4159" target="_blank">01:09:19.520</a></span> | <span class="t">it puts in there in like external thing, and allow it to attend over it. So basically, you can teach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4163" target="_blank">01:09:23.920</a></span> | <span class="t">the transformer just dynamically, because it's so meta-learned. You can teach it dynamically to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4168" target="_blank">01:09:28.800</a></span> | <span class="t">other gizmos and gadgets, and allow it to expand its memory that way, if that makes sense. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4172" target="_blank">01:09:32.800</a></span> | <span class="t">just like human learning to use a notepad, right? You don't have to keep it in your brain. So keeping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4177" target="_blank">01:09:37.440</a></span> | <span class="t">things in your brain is kind of like the context length of the transformer. But maybe we can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4180" target="_blank">01:09:40.720</a></span> | <span class="t">give it a notebook. And then it can query the notebook, and read from it, and write to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4185" target="_blank">01:09:45.600</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4208" target="_blank">01:10:08.960</a></span> | <span class="t">I don't know if I detected that. I kind of feel like-- did you feel like it was more than just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4212" target="_blank">01:10:12.800</a></span> | <span class="t">a long prompt that's unfolding? [INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4219" target="_blank">01:10:19.680</a></span> | <span class="t">I didn't try extensively, but I did see a forgetting event. And I kind of felt like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4224" target="_blank">01:10:24.160</a></span> | <span class="t">the block size was just moved. Maybe I'm wrong. I don't actually know about the internals of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4229" target="_blank">01:10:29.840</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4251" target="_blank">01:10:51.600</a></span> | <span class="t">I mean, so right now, I'm working on things like nano-GPT. Where's nano-GPT?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4259" target="_blank">01:10:59.040</a></span> | <span class="t">I mean, I'm going basically slightly from computer vision and kind of computer vision-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4263" target="_blank">01:11:03.360</a></span> | <span class="t">products to a little bit in the language domain. Where's chat-GPT? OK, nano-GPT. So originally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4268" target="_blank">01:11:08.160</a></span> | <span class="t">I had min-GPT, which I rewrote to nano-GPT. And I'm working on this. I'm trying to reproduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4272" target="_blank">01:11:12.320</a></span> | <span class="t">GPTs. And I mean, I think something like chat-GPT, I think, incrementally improved in a product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4277" target="_blank">01:11:17.840</a></span> | <span class="t">fashion would be extremely interesting. And I think a lot of people feel it. And that's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4283" target="_blank">01:11:23.440</a></span> | <span class="t">it went so wide. So I think there's something like a Google plus, plus, plus to build that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4289" target="_blank">01:11:29.040</a></span> | <span class="t">I think is really interesting. So we did our speed around the clock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4294" target="_blank">01:11:34.800</a></span> | <span class="t">[END PLAYBACK]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4295" target="_blank">01:11:35.380</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XfpMkf4rD6E&t=4295" target="_blank">01:11:35.880</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>