<html><head><title>Can synthetic data unlock AI recursive self-improvement? — Mark Zuckerberg</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Can synthetic data unlock AI recursive self-improvement? — Mark Zuckerberg</h2><a href="https://www.youtube.com/watch?v=9TU0XjJqpOg" target="_blank"><img src="https://i.ytimg.com/vi_webp/9TU0XjJqpOg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>One of the interesting things about it that we saw even with the 70 billion is we, we thought it would get more saturated, you know, it's like we train it on around 15 trillion tokens. And I guess our prediction going in was that it was going to asymptote more, but even by the end it was still learning, right?</p><p>It's like, we probably could have fed it more tokens and it would have gotten somewhat better. But I mean, at some point, you know, you're running a company, you need to do these meta reasoning questions of like, all right, how do I want to spend our GPUs on like training the 70 billion model further?</p><p>Do we want to kind of get on with it so we can start testing hypotheses for LLAMA4? So we kind of needed to, to make, to make that call. And I think we got it, I think we got to a reasonable balance for, for this version of the 70 billion.</p><p>That was, that was fascinating that you could just, that it's the architectures at this point can just take so much data. And I do think in the future, it seems quite possible that more of what we call training for these big models is actually more along the lines of inference generating synthetic data to then go feed into the model.</p><p>So I don't know what that ratio is going to be, but I consider the generation of synthetic data to be more inference than training today. But obviously if you're doing it in order to train a model, it's part of the broader training process. So I don't know, that's an, that's a, an open question is to, to kind of where, what the balance of that and how that plays out.</p><p>If that's the case, would that potentially also be the case with LLAMA3? And maybe like LLAMA4 onwards where you put this out and if somebody has a ton of compute, then using the models that you've put out, you can just keep making these things arbitrarily smarter. Like some Kuwait or UAE or some random country has a ton of compute and they can just actually just use LLAMA4 to just make something much smarter.</p><p>I do think that there are going to be dynamics like that. Actually I found the synthetic data thing really curious. I'm actually interested in why you don't think, like current models, it makes sense why there might be an asymptote with just doing the synthetic data again and again, if they get smarter and use the kind of techniques you talk about in the paper or the blog post that's coming out on the day this will be released, where it, it goes to the thought chain that is the most correct.</p><p>Why this wouldn't like lead to a loop that, of course it wouldn't be overnight, but over many months or years of training, potentially with a smarter model, it gets smarter, makes better output, gets smarter and so forth. Well, I think it could within the parameter of whatever the model architecture is.</p><p>It's just that like at some level, I don't know. I think like today is 8 billion parameter models. I just don't think you're going to be able to get to be as good as the state of the art multi hundred billion parameter models that are incorporating new research into the architecture itself.</p><p>When you're building software, there's like a ton of stuff that you can do with software, but then at some level you're constrained by the chips that it's running on, right? So there are always going to be different physical constraints and it's like how big are the models is going to be constrained by how much energy you can get and use for inference.</p><p>So I guess I'm simultaneously very optimistic that this stuff will continue to improve quickly and also a little more measured than I think some people are about. I just don't think the runaway case is like a particularly likely one. I think it makes sense to keep your options open.</p><p>Like there's so much we don't know. There's a case in which it's really important to keep the balance of power so nobody becomes like a totalitarian dictator. There's a case in which you don't want to open source the architecture because like China can use it to catch up to America's AIs and there is an intelligence explosion and they win that.</p><p>A lot of things seem possible, just keeping your options open, considering all of them seems reasonable. 1 you</p></div></div></body></html>