<html><head><title>Running AI Application in Minutes w/ AI Templates: Gabriela de Queiroz, Pamela Fox, Harald Kirschner</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Running AI Application in Minutes w/ AI Templates: Gabriela de Queiroz, Pamela Fox, Harald Kirschner</h2><a href="https://www.youtube.com/watch?v=1t-9-s1brcg" target="_blank"><img src="https://i.ytimg.com/vi_webp/1t-9-s1brcg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Thank you so much for coming to the workshop. My name is Gabriela de Queiroz and I'm Director of AI at Microsoft. I have Pamela here. I'm Pamela and I'm a Python cloud advocate, so well done on those people who said Python, but I also I worked in JavaScript for them for quite a long time and I generally like lots of languages.</p><p>Hi, I'm Harold. I'm a PM on VS Code and GitHub Copilot chat. Awesome. So today we are going to be talking or showing you how to run an AI application in minutes. So we are going to have a lot of like hands on. So be ready to do like some coding, not coding, but like going through some coding using different tools, GitHub code spaces, Azure, and other tools that we are going to be talking about.</p><p>But just to give an overview of like the agenda, I'm going to be talking about Microsoft for startups a little bit, some of the partnerships, some of the pain points, and then we go through the AI templates and hands on. So Microsoft has a program for startups. So if you have an idea, if you have a startup, you can apply to this program.</p><p>And what I always tell people is you don't have to have a startup per se. But if you have an idea, that's enough to apply for this program. And you get a lot of benefits and benefits that can be I'll just skip I can be like credits. So you get up to $150,000 in Azure credits.</p><p>You also have third party benefits like a lot of like different tools that you can use. And then of course, GitHub, Microsoft 365, LinkedIn Premium, and more. You can use all the different models from OpenAI, but also like LAMA, models from Kohari, Mistral, and so on. And the piece that I like the most is about the sessions that you can get one on one sessions with people like me, or Pamela, that we volunteer our time to share our knowledge with founders.</p><p>We can talk about maybe like, I don't know, you are hiring and then I'm an expert in hiring. So you come and talk to me and I say, hey, these are some of the best practice for you when you are building your team, or you can go to technical sessions and ask more like technical pieces as well.</p><p>And inside this platform, we have like several things other than the benefits that I mentioned and the guidance that I just mentioned is what we call build with AI. And inside we have some AI templates that the idea is like we can help you accelerate the AI application piece with some kind of like skeleton in a way.</p><p>So you have something up and running in like few minutes. So again, you get cloud credits, you have access to dev tools, you have the AI templates, you have the one-on-one guidance. And no matter where you are in your journey, if you have an idea, if you are already building or if you're scaling, this program is for you.</p><p>You have access to all the cutting edge AI tools, so you can innovate and streamline your AI development. And on top of like the founders of this program that we have, you also have like different programs that it kind of like, it's like the next step. Like let's say you are now scaling, growing, and then you use all the credits.</p><p>What is next? There is a next. Like, you know, we try to guide you through the whole process. So there is something called the Pegasus program, where we help you to co-sell, go to market and so on. And then there are some like strategic VC partners and like accelerators that we partner with.</p><p>So we have partnership with Y Combinator, Neo, The Alchemist, etc. Pain points for startups, there are a bunch of them. One of them is like, you don't have time. You cannot wait to go to markets. You have to go like as fast as you can. You have a lot of like resource constraints.</p><p>We have some issues with scalability. You have, you don't have the support and guidance. And that's where we are trying to help you with. So now we are going to go to the fun part. It's like the AI template. So that's where Pamela is going to show you all the amazing things that you can do with all the different tools.</p><p>All right. So our goal today is potentially having you deploy maybe even three different templates. Okay. So we have three different ones all like just, you know, show in the, in the browser, which ones we're going to be deploying. Right. So we have starting, we're going to start simple with this chat application here, just to make sure everything's up and working.</p><p>And then we've got two different rag applications. One of them is a rag on a Postgres database, like rag on a Postgres table that does a SQL filter building. And then we have rag on a unstructured document. So here I've got a rag on my personal blog or like a rag on, you know, internal company documents, whatever it is that you're going to, whatever kind of documents you're going to rag on.</p><p>So those are the three templates we're going to be looking at today. And we have it all set up so that you should be able to deploy those templates without spending any of your own money and doing it all through our credits, which is yay. Yay. All right. So, um, the first thing you need to do is get this URL.</p><p>So everybody open this URL on your computer. So it's aka.ms/aie-workshop. It should open up a, a word document in the browser that looks like the screenshot you see here. So you can either type in the URL or scan that QR code and get that open on your machine. So let's make sure everyone's got it open.</p><p>Welcome, welcome. So go ahead. Once you've got your computer ready, put this, uh, put this URL in your browser. Harold, maybe you can just memorize it and then help anyone who doesn't have it. Yeah. Aie-workshop. Uh, okay. So then let me go to that actual doc here. So the first thing you need is a GitHub account.</p><p>Does anybody here not have a GitHub account? Okay. So everyone here has a GitHub account. Great. If you don't have a GitHub account, you can sign up for one for free right now. And that's it. Um, and, um, and that should be fine. Um, the next thing you need is an Azure pass.</p><p>So this is something that we've got for this workshop for this conference. And this is going to let you deploy stuff on Azure without spending any of your own money. So we got a passes for 50 bucks and they're valid for seven days. So if you do want to keep hacking after the workshop, you can keep using your pass.</p><p>And, uh, after seven days, it'll disappear just like Cinderella and the pumpkin. Uh, so in order to get that as your pass, you do need to have some sort of Microsoft account. Microsoft account. So you can use your, like, uh, you can use a personal Microsoft account if you have one.</p><p>Uh, so if you're, if you like, how do you tell which one you're logged into right now? I guess if you just go to outlook.office.com, maybe you know what Microsoft account you're currently logged into. Um, and then you can see some people in the last workshop were like logged into their kid's Minecraft account.</p><p>So just, uh, just, uh, just, you, you need a Microsoft account and you might want to double check to see which one you're currently signed into. If you are signed into a Microsoft account, if you don't have a Microsoft account, no big deal. You can make one on the spot.</p><p>I made one this morning. So, uh, if you do need to make one, you can just make up a new outlook address and set it up that way. Um, so you can also make it as part of this project. So we're going to go to this easy check in URL and that's linked from this doc here.</p><p>So if you don't have this doc, if you just came in, we can help you get the stock open. So we can get this URL and, uh, we're going to spend 10 minutes making sure we get through this step since it can be a little, a little tricky. So when you go to this check in URL, right, we put this in the browser, it loads, this is what you're going to see.</p><p>And it says I can either create a GitHub account or log in with GitHub. So I'm going to log in with GitHub. Cause I already have a GitHub account and I'm logged into this browser with it already. So I'm just going to click on that. And so what's that's going to do is create a pass for my GitHub account.</p><p>And so we get a pass. So each of us will get a different code based off our GitHub account. So this is my, you know, basically my Azure pass promo code. So I can copy that. And then there's this button here that says, get on board with Azure. This is the next step is to click this.</p><p>And then we get this screen, which says, okay, this is, you can start. And when I click this here, it says what my currently logged in account is. So this is where you should check to make sure you're happy with what account you're logged in with. And you don't want to switch.</p><p>Um, I don't recommend using a corporate account. If you do have a corporate account, like don't just don't use it. It's going to be problematic for various reasons. Cause corporate accounts may have restrictions that won't let you deploy things. So we do recommend using some sort of personal account or making up a new account.</p><p>So that's why you see I'm using my Gmail instead of my Microsoft. Uh, so I'll confirm my account and then I can enter the promo code. And that was from this screen. So I still have this screen open. So I just go there. I paste it in and then we go S six X Y Y K.</p><p>I think it's case insensitive submit. And then it's going to actually fail for me because I've already set this up on, on this thing here. Um, and this, if you see this, it's because you've already actually gone through this stage. Uh, so for you, it should work the first time and then, uh, it'll create the Azure account for you.</p><p>And if it works, then what we can do is go to portal.azure.com. So portal.azure.com and we'll see what, how it loads in. Does a bunch of redirects. And then we can click on subscriptions. And what we should see is there should be at least one subscription that says as your past dash sponsorship.</p><p>So that's our key that we have done this correctly. And as long as we use this subscription, when we're doing our deploys, we will not get charged any money. Well, Microsoft will, but you won't. That's the important part. Okay. So we're going to spend 10 minutes to make sure that we can get everyone through this, this stage so that we're all on the same page going forward.</p><p>So if you already got it, that's awesome. You can, um, you know, like look at Harold's, like, uh, basic profile or something. So once you have that set up, the next step is the proxy. Um, so I'll just show that, uh, so that you can start playing with that.</p><p>Uh, so here's, it's the next link in here. So the reason we have a proxy is because normally when you're using Azure open AI, you actually have to fill out a form and say how you're going to use Azure open AI. And then somebody says, Oh, okay. Yeah. That's a good use of open AI because Microsoft doesn't want people to use AI willy nilly.</p><p>So we, you know, check to make sure that something adheres to our responsible AI principles. Uh, we don't have enough time for you to go through that process while we're in a workshop. So we've set up a, an Azure open AI proxy that you can use during the workshop with the repos.</p><p>And we've have special instructions for how you can use this proxy with the repos, since you can't use the actual Azure open AI. Uh, so this, you can follow the link from the doc and log in with your GitHub account. Uh, I'll log out so I can show that.</p><p>log in with GitHub. Okay. And so as I'm logged in and then we have an API key and a proxy endpoint, and that's all we need to be able to, uh, to use it as your open AI instance. Now, normally I don't like to use keys and I tell everybody to avoid them, but, uh, in this situation, we are going to be using keys and, uh, yeah.</p><p>And these keys will expire at a certain point. So we don't have to worry about them being exposed. Uh, typically with keys, we'd have to protect them very fiercely so that nobody was using them. So you can go ahead and log into this and see your registration details. And then you can even play around with the playground.</p><p>This is really similar to the Azure open AI playground or the open AI.com playground. If any of you played around with this, uh, you can see here, you can play with the system message. That's how you like say like, Oh, you're an AI assistant that constantly makes pirate jokes.</p><p>Yeah. Uh, and then we update the system message. Oh, private. I wonder what it'll do. Uh, there we go. And then, um, let's see. Oh, enter my API key. Okay. So we need to enter the key. I actually have never used this before. Uh, so we're going to enter the key, not save it, select a model.</p><p>Okay. Okay. So we select the model over here. Uh, so we've got three, five turbo. Oh, I didn't know we had four too. You set up four as well. Cool. I can use four. Four is better. All right. And four is slower, but better. Uh, okay. And then, uh, please, uh, tell audience about open AI.</p><p>Open AI. Okay. All right. And you can see different parameters that we send. And these are all getting sent to the open AI SDK. So we say the model right here, we've set up two models. GPT three, five turbo, GPT four. Those are often the ones you're picking between with open AI.</p><p>Although now you've got GPT four. Oh, that's a good choice. If you're doing something with vision, something multimodal, I wouldn't use it. Otherwise just based off of some experience we've had with it. Um, but it is a great one. GP four. Oh, is good for vision as here. You can see, you know, with the combination of the system message and the user message.</p><p>So this is what we call a user message. This is what we call a system message. Those combined together. We get back a response like this where it describes open AI with lots of ours and mateys and stuff. Uh, we can, you know, change different parameters here. Like how many tokens it should send back.</p><p>The temperature is roughly the creativity. Um, top P is also roughly about creativity and there's some more advanced stuff there. And you can see how many tokens you used on the way out and how many tokens you got on the response. So you can play around with this playground to, uh, you know, to try stuff out and make sure that, uh, that you're able to, to use the key.</p><p>So this is just linked off of, um, off of this workshop, right? So if you go to the workshop proxy, you log in, you'll get your key in your end point. You can go to that playground and you can play around with the playground to check that that's working.</p><p>But we just want to make sure everybody now has an Azure pass and is logged in to the proxy so that you have a key in an endpoint. So we'll just check to see if anyone had anything to do that. Okay. All right. So here's the, like he, these are, if you're looking for the models, this is generally the page to check.</p><p>Um, so, you know, GP four O, GP four and going down. Those are the GP four models. GP five. You're saying there's a GP three, five that supports vision. Oh, four tuber with vision. This one. Yeah. Yeah. So we were using that one, but it's a lot slower. Yeah.</p><p>So that's why we, I've started using four O or this one. This is a GA. Um, um, okay. All right. Yep. So you just want to compare those. So we'll just be using the basic GP three, five and GP, uh, just GP three, five today, actually. And then also the embedding models.</p><p>Okay. So is everybody set up with the proxy? Okay. All right. So now we're going to actually get something working. So we have this repo here, so you can follow the link from the doc. And it has read me's for the three different projects that we can deploy. And these read me's are specific to using them with the Azure open AI proxy.</p><p>Uh, so normally you can just use the, the read me's that are on the repos itself. But because we are using this Azure open AI proxy, we do have to use a slightly different setup. So we've made read me specific, uh, for this, for this workshop. Uh, so we can start off on this, uh, open AI chat app quick start and make sure that that's all working.</p><p>So the first step is to open in GitHub code spaces. So you can do that by clicking this button here. Have any of you use code spaces before? Okay. A couple of people. So code spaces will open a VS code in your browser with a developer environment for that repo.</p><p>So you can actually use code spaces on any GitHub repo. So you go to any GitHub repo, you click on code and you can make a code space for it. So it's a way that you can start hacking on any repo, uh, very quickly. So you can open this button here to open in code spaces.</p><p>And, uh, I'll just go ahead and make a new one. And I'll say create code space. So this is going to take a few minutes to load. Cause what it's doing is that it's creating the environment for this repository. It's opening VS code in the browser. And it's also just setting up VS code.</p><p>So if you actually have like, if you use VS code locally and you've got like extensions that you use locally, it's actually potentially sinking those extensions and, uh, enabling them, them here. I should probably just not do that. Cause then it would load faster for me. Um, but yeah, you can see in the bottom here as it's setting up and we'll just wait for it.</p><p>So this is, you know, the slowest part of using code spaces is just the loading. You have a tip. If you want faster code spaces, there's pre-builds available as well. Yeah. And I do have them on the third repo, but I think I don't have it on this one.</p><p>So I, I should have remembered to do pre-builds for all the repos. Right. And the slowest part is probably installing all the dependencies and the builds. It's basically it's doing all the things you would do when you install it locally, just automate it and with a progress bar. And at some point it would just light up.</p><p>Yeah. Let's see what the, you can even watch, can we watch the logs for this one? building code space. Cool. Whoop. Here we go. So you, if you like this sort of thing, like if you like watching Docker containers build, cause that's what it's actually doing. Everything's a Docker container.</p><p>So you can actually watch it as it, um, builds everything here. And now it's downloading all the requirements. So these are all the Python requirements. So all the examples that we're going through today have a Python backend and then some sort of JavaScript front end. Uh, this one has what we call like a vanilla JavaScript front end.</p><p>As in, I just wrote some JavaScript in a script tag. Uh, but then the other ones are much fancier. So they've got a full type script and a build system and react components, uh, using the Microsoft fluent UI, uh, you know, web framework. So you can kind of see the range of front ends there.</p><p>Okay. So you can see it's, you know, it's still going through the process, but at least now, uh, we can see the file Explorer has loaded so we can, uh, explore the files here. And, uh, and I'll show, I'll go ahead and show the, the code. If you're interested in the code, uh, it is in the source folder.</p><p>Uh, we are using a court application and I think nobody has heard of court, but, uh, has anyone here heard of flask or used flask? Great. So core is just the async version of flask. So it's literally built on top of flask. And one day it might be brought back into flask.</p><p>And it just, you just take your flask code and you put a sinks in it. And then you've got, you've got court. Uh, that's really, uh, how it goes. So, uh, if you haven't done async before in Python, async is a way that I've used async with your functions, they become co-routines and then they can be paused and waited on.</p><p>And it's important to use async when we're building applications with AI, because we have these really long blocking calls to an AI API, right? So we make a call to an LLM and we send off our requests. And these LLMs, they can take like two seconds, five seconds, 10 seconds, right?</p><p>Depending on what we're doing. And while that's happening, we ideally want to be able to handle other user requests coming in. Uh, so that's why we use async framework. So if we use an async framework, then while we're making IO calls, we can handle other user requests that are coming in.</p><p>So all of the ones that we see today have an async backend, either court or fast API. Anyone heard of fast API? It's very, very popular these days. Yeah. So fast API is the one most people know of as the async framework. Um, so I, you know, I, I like both of them fairly equally.</p><p>Uh, so I, I use a mix of both. Um, but I just want to make sure people know about the value of async frameworks. Okay. So that's all in the core app folder. Uh, if you want to look at the code there, so it is now finished. Okay. Anybody else get their code space loaded?</p><p>Get a couple. Okay. Great. Finished configuring. So I can. Yeah, we are going to be using the terminal. And if for some reason your terminal like goes away, sometimes this happens to the code space. Just click that plus right here. Sometimes my terminal kind of blinks out. So I just click the plus and that'll give me a new terminal, right?</p><p>Boop. New terminal. Okay. So here we are in the terminal. Um, but actually the first thing we're going to do is that there's a dot EMV dot sample. We're going to make a dot EMV file based off of that. So I'm going to make a new file and I can do that using this little new file button up here.</p><p>So I'll just click that say new file and I'll type dot EMV. Uh, you could also like copy and paste. Um, and then I'm just going to paste the dot EMV in there. You can even rename dot EMV dot sample to dot EMV. I think that's another way. Um, and then we need to fill in these values to match the values of the proxy.</p><p>So we'll go to the proxy and let's see, where's my proxy open here. So here's my proxy. So I'm going to go ahead and fill in this one. That's the end point. So the end point should start with HTTP and end with slash V1 and look like that in the middle.</p><p>Uh, so that's the end point. That's where we'll be sending our opening requests. Then we need the key. So we'll copy that and it'll look like that or slightly different for you. And then the deployment is going to be the name of the deployment is GPT-35 turbo. Uh, and that's also the name of the model in this case.</p><p>So if you, has anybody used open AI.com? A few people. Okay. So on open AI.com, you just pick what model you're going to use and that's all you need. With Azure open AI, you have to make deployments based off of the model. So you actually have a bunch of deployments and you could actually have multiple deployments of a GPT-35 turbo model that have different names.</p><p>So when you're working with Azure open AI, you have to know the deployment name, not just the model name. So that's one of the complexities of, of using Azure open AI, but it does give you more flexibility. Cause you can say, Oh, this deployment is going to have 20 tokens per minute.</p><p>And this was going to have 30 tokens per minute. Right. And then you can like say, which of your colleagues can use what? Like if they're all trying to like use up your deployment or whatever. Uh, so as it's more flexibility, but you do have to specify it. Okay.</p><p>So now my dot EMV is set up. So this is just so that I can run a, a local server and I'm putting local server in quotes. Cause I'm going to run a local server inside GitHub code spaces. So it's actually running a local server, not on my actual machine, but inside the GitHub code spaces development environment.</p><p>Uh, so to do that, I grab, I'll grab the command here. That's going to run the court app and just give it. And when with code spaces, you do have to allow, so you'll see this little thing that pops up. So if you ever want to copy paste, you have to allow for the terminal.</p><p>Uh, and then I have, okay. And then I paste it and then you can see that it says it's running on this URL. Now you can't just paste this URL in the browser. I'll show what will happen. So if I paste in the browser, I'm going to get an error because this is not running on my local machine.</p><p>This is running inside GitHub code spaces. So you have two ways to get to it. One way is that if you just click on it, uh, like, uh, option click, at least on my Mac. So I mouse over, it'll tell me what to do mouse over option. Click. So code spaces will actually detect that you're clicking on a local URL and it'll turn it into a code space port URL.</p><p>And it's this funky URL up here, improved disco for me. Um, and, uh, and that's actually, you know, like local for that GitHub machine. And, uh, that's one way of doing it. Another way that you might like more is you go to your ports tab and you're going to find it listed here.</p><p>And, uh, we'll see the, you know, the forwarded address and we can click on that, or we can even click the glue globe icon and we get to the same URL. So there's many ways you can get to this locally running URL, uh, and, uh, and get to the special code space URL bit.</p><p>And you can even change your port visibility. If you want to like share it with a colleague, if you were just, or in a class, you can change it to public. And then you could actually ping this URL to someone else. Now, this is not a deployed URL. Like you're not going to use this for like, you know, your deployed URL, but it's fun.</p><p>It's good for development. So now I've got this running locally and now we can type stuff and be like, what's the weather in San Francisco? See if it's going to lie. Uh, then we can see. Oh, good. That was a good answer. I think this has been trained. It refused to answer.</p><p>It's always good when it refuses to answer something it shouldn't know. Um, so we could go ahead and like, you know, I could change this now and change the system message. And let's see, where's our system message in here. Right. So right now my system message is just, you are a helpful assistant.</p><p>I'll be like, you are a assistant that cannot resist a good pasta joke. I don't know what it's going to be. I don't know what it's going to be. I love LLMs. Okay. Oh, I love LLMs. Okay. So what's the weather today? Is it going to make a pasta joke?</p><p>Okay. All right. It looks like you might have been quite saucy today. Don't forget your umbrella. You might end up feeling like a soggy middle. So good. Uh, so, uh, so that works. Uh, so here we go. So now this is running locally. Um, and so this is a good one.</p><p>Like when we're developing, we can just test things, test things locally here. The next thing we're going to do, once we're happy with it, we're like, this is the best app. It makes pasta jokes. We're going to deploy it. Uh, so then we move on to the deployment instructions.</p><p>So the first step of that is AZD auth login. So this is going to log in to our Azure account that we made earlier. So I'll do AZD auth login. And, uh, this is going to give us a device code that we're going to paste into this OAuth browser flow.</p><p>So let me go and open it up. Maybe over here. I think that's my Azure account that I'm using for this. And then I go and I take this and I paste it in and I'm going to pick my account. I'm going to use this one continue and, uh, okay.</p><p>And then we're logged in. Okay. All right. So that was the device code flow. So you just want to make sure that you log into the account that you got with the past, right? Whatever account you use for the past, that's what you want to log into. The next step is to create, or Gabriella, should I pause?</p><p>Like, should we get through the local step first? Or should we keep going with AZD deploy? Yeah. We can pause and see if everyone's got the local one running, actually. I think that might be good to do. Okay. So let's just pause and see if there's any questions with getting the local one running.</p><p>Yeah. So yeah, someone asked, like, can we just run this locally? You can totally run these locally as well. We like to use GitHub code spaces in workshops because that reduces the number of potential development environment issues. If you want to run it locally, you can either run it, you know, just with a Python virtual environment and you just have to install all the requirements.</p><p>Or you can run it with VS code using the dev containers extension and that will do the Dockerize environment for you. If you want kind of the benefit of the Dockerize environment without, you know, being in the browser and having to pay, potentially pay for code spaces. So we should know also that GitHub code spaces, you have a limit of some number of hours a month, either 60 or 120.</p><p>It's 60. Okay. I must've paid more. So it's 60. So you're not going to go over that today, but eventually you could go over that if you use code spaces a lot. So if you're local, right, I think I have mine open locally as well. And I'm just, yeah, locally, I'm just using a Python virtual M.</p><p>So you're also welcome to try these things out locally. If you like local environments, just, you know, be a good person and make a Python virtual M to manage your Python dependencies. Right. Yeah. Okay. So I saw a lot of local things. So I think we can move on to the AZD.</p><p>So, yeah, I did the login. So you saw me do, you saw me do the login here. Right. And that's using the device code flow. So you should see something like this happen from inside code spaces. And the next step is to make a new AZD environment. So AZD is this tool we're using for deployment.</p><p>So we make a new environment name. You can just call it like chat app, whatever you want to call it. And then what that does is it actually makes this dot as your folder and it makes this chat app folder inside. And that's where it's going to store all of our deployment environment variables.</p><p>So we need to set all and configure anything we want to customize about our deployment. We're going to configure that now and it's going to get update. It's going to update this file here. So the next thing we're going to do is set all these AZD environment variables. So the AZD environment variables are different from the ones we just saw in the .env.</p><p>The .env is just for the local server. AZD environment variables are for deployment. Sometimes we use the same, but a lot of times we want our local environment to be slightly different from our deployed environment. So we have two different ways of setting those variables. All right, so I first set these commands.</p><p>So this is just going to tell it to not create an Azure OpenAI because we're using the proxy. And then we're going to set the name of the deployment to GPT-35 Turbo. Then we need to set the key. So I'm going to paste this and then I'm going to delete, delete, delete.</p><p>Gosh, that's what happens when you have Wi-Fi issues actually, is you see it with the typing. And then I got to find my key again. And there we go. So that sets the key. And then I'm going to set the end point. I'm going to delete how we're going to do this.</p><p>There we go. And get the end point. All right. So now I've set all these things. Now, if I've done it correctly, if I look at my dot Azure folder for that environment I created, I should see a dot EMV that looks like this. So this is a dot EMV that's inside the dot Azure folder.</p><p>So this is what is going to be used for the deployment. And it's going to tell it, you know, this is how it's going to set up the Azure open AI connection. Okay. And now I'm just going to do, I'm just going to type type. Thank you. Okay. All right.</p><p>And then I'm going to do AZD up. Here we go. Here we go. So what AZD up is doing is that it's actually deciding, it's doing several stages. Okay. So I have to select an Azure subscription. In this case, I only have one subscription. So you just press enter.</p><p>Uh, if you had two subscriptions, you would want to pick the sponsorship one. Uh, then I select an Azure location to use. Typically you just choose one that's close to you. So central us is pretty good. Uh, now what AZD is doing. The first step is that it's actually packaging up the code that it's going to deploy later.</p><p>Uh, in this case, we're deploying to Azure container apps. So it's packaging up a Docker container file. So it's actually literally building a Docker container right now. So if you do like working with Docker, Azure container apps is a great fit. And a lot of people like Docker. So we deploy a lot of stuff there, but we also are going to be using Azure app service for one of the later templates.</p><p>Uh, so we've got lots of ways to deploy on Azure. So you can see it building up that Docker. The step after this is where it's actually going to create Azure resources. So it's going to create the container apps can create a container registry, create a container apps environment and create a log analytics workspace.</p><p>So these are all the components of a containerized app on Azure. And, uh, you know, it's multiple components and we have to stitch them together. The way we stitch them together is using infrastructure as code. Uh, has anyone used Terraform here before? Okay. So we have our own version of Terraform.</p><p>It's called bicep and it is, uh, infrastructure as code, which means we're declaring what resources we want to make. Right. So we say, oh, we want to make log analytics. We want to make container apps. We want to make, you know, the actual container apps image, and then we're going to assign some roles.</p><p>Right. So all of that is declared in this bicep file. So that way you have repeatable repeatable processes for provisioning. And this is really helpful when you're making complex applications on Azure. Cause you might have like 10 different things you're using, right? Uh, you might have a Postgres and a key vault and a Redis cache and, uh, log analytics and app service.</p><p>And you want them all to tie together. So you can declare what that, you know, what that infrastructure looks like. And then, uh, and then put that in a bicep file and then deploy it. You can also use Terraform. So if you were really into Terraform and very comfortable with it, you could totally use Terraform tier as well.</p><p>I don't know Terraform. I haven't used it personally. So all of my examples do use bicep. But if you want to send a PR with Terraform, I'll, I'll review it and just stamp it. Cause I don't know how to reason about it. So what you can see here is that it is actually creating, uh, the resources right now.</p><p>So you can watch it here. You can also watch it in the portal. It's not really super exciting to watch. So this is the point where I usually fold my laundry. Um, uh, because it can take some amount of time, uh, or you can even get an error. Oh, I used.</p><p>Okay. So I already made one in central us for the earlier demo. So I should have picked a different region. So for this Azure pass, there is a constraint of one container app per region, which is why we said in the readme that you should pick a region that you haven't picked before.</p><p>And I didn't pay attention to my readme. Uh, so that one won't deploy. So, uh, what I can do is I'm just going to make a, I'll just make a new environment. I'll just, uh, I'll just copy it. Everything over. Um, now what you shouldn't run into this cause this would be your first, uh, your first environment.</p><p>Right. Uh, so chat app two, and I'll just copy and paste. Uh, we'll change the chat app two. And then West us seems like a good region. Okay. And then I'll AZDM select chat app two. Uh, there we go. And then AZD up. Yeah. Okay. So then it'll do the up again, but I have one of these already, already deployed.</p><p>So I'll just open up the deployed ones. You can see deployed. Deployed is going to look pretty darn similar to what it looks like locally. Where's the one deployed? Okay. So this one's deployed. It looks pretty much the same as what it looks like running locally. Right? Is that the URL is a container apps URL.</p><p>And you'll see this URL displayed in the terminal. Once it finishes successfully deploying, you'll see this displayed. Let me see if I have that in my history anywhere from earlier today. Uh, dah, dah, dah, dah, dah. No, you never know. Okay. So let's see how it's going here. Two, two, two.</p><p>Yeah. I lovingly handcrafted them. Yeah. So, um, I, yeah, I write the bicep files. Uh, some of them, all the ones in core are actually from a shared repo that we just copy and paste from. We're trying to move towards something called AVM as your verified modules, which are bicep files that are maintained and have security best practices in them.</p><p>So we'll, we'll gradually be moving over. But basically with bicep files, like you can use ones from a central registry. You can use ones from your own private registry if you're doing a lot of them. Uh, or you can just use, you know, ones inside the folder. Um, so there's a lot of techniques you can use depending on how much bicep you're using.</p><p>Okay. So now it's starting over and deploying again. All right. So let's walk around and, or any questions on what I showed here? All right. So I saw a lot of AZD deployments are going. I saw some issues with like naming, which I run into all the time. Azure has very obscure naming rules.</p><p>The safest thing is do short names with no symbols in them and nothing fancy. Uh, if you do have a naming rule, you can just always do AZM new and make a new environment and, you know, and start over. Uh, and that should be okay. Uh, but generally the issues you run into with deployment are usually related to naming, region constraints, um, account constraints.</p><p>And that's probably, yeah. The ones you might run into. All right. So, um, we just give me a 45 minutes left. I'm going to show you the other ones. And, uh, and these are ones that you can, uh, that you can also start trying to deploy now. And following very similar read me's, right?</p><p>So the first one, um, actually the, the, these two are both about rag. So first I'll talk briefly about rag. Right. So let me first motivate it. Right. Uh, let's see. Like, uh, tell me what Pamela Fox, uh, likes to code on. I don't know. Let's try this. I'm trying to get it to lie.</p><p>Um, this is the pasta one. Okay. So this one clearly lied. Spaghetti Phi-a-thon. Great. All right. So then, but then if I go to, um, this one right here, tell me what Pamela Fox likes to code on. This one will hopefully be more accurate. At least have less pasta jokes here.</p><p>Um, and this is, so basically what we're trying to show is that if we just ask an LLM to answer a question, it is, it's very possible that it's just going to make something up. Um, if that one, it seems like there. Oh, good. I mean, in this case, it says it doesn't know what I like to code in.</p><p>I think I should have said like code in, um, you know, like here, like what Python frameworks as Pamela used, try this one. Um, so, you know, if it doesn't know the answer, it'll say, uh, in this case, yeah, in this case, it doesn't know the answer because this is actually using the rag technique in order to answer questions based off a knowledge source.</p><p>Right. Um, so those are our last two samples are about rag. rag. Uh, so the general approach of rag is that we get a user question. We use that user question to search some sort of database or search engine. We get back matching search results for that user question.</p><p>And then we send those, uh, to the large language model and say, here's a user question. Here are the sources. Now answer the question according to the sources. And so now we can make customized applications that can actually synthesize and answer questions for any domain. So we've got two rag samples here.</p><p>So one of them is rag on Postgres. So this is for the use case. If you've got an existing database and you want to be able to ask questions about that database and have the LLM answer accurately based on that. So for the example, uh, you know, database that I'm using, I have product, right?</p><p>So these, this is a chat on products. So, uh, you know, our table is storing all the products for this website. So I can say, okay, what is the best shoe for hiking? So then it's going to go and search the database rows and get back matching rows and then come back and say, okay, this blah, blah, blah, blah, blah, blah, blah, blah.</p><p>And it's going to include citations. So one of the key points of rag is to have citations so that users can verify where the information come from and see that it's actually legit information. And we can also look at the, uh, the process for this rag flow here. When we look on the, the thought process here.</p><p>And as, as we'll let you see is that this rag flow is a multi-step process. So the first process is actually what we call like the query rewriting phrase or the query cleanup phrase. So that's where we take the user's question. And we ask the LLM like, Hey, here's a user question.</p><p>Turn this into a good search query. Cause a lot, a user question may not be that well formula, right? Like, uh, please tell me about the best shoes for hiking now. Okay. So, you know, there's like a user query and, uh, you know, that's probably not the optimal search query for, uh, for a search.</p><p>So if we look now at the thought process, we can see that the LLM actually turned that whole long thing into best shoes for hiking. So that's a better query. Uh, so that's our query rewrite phase. So that's an LLM call. Then we get back the resulting rows from the database.</p><p>And then this is our call to the model that says, Hey, you need to answer questions. According to the sources. Here's how you should cite your sources. Here's the user question. Here's the user question. And then here is all the sources. So this is basically rag. And then, you know, we're able to use it with different sorts of, uh, data, data sources.</p><p>So that's rag on Postgres. So you can get that set up following really similar steps to the, to the other one. And you can even run that one locally first as well on just on a local Postgres database. Uh, so here you can run the app locally. Uh, this one is a little more fancy cause you've got a react front in there.</p><p>Then you can deploy to Azure. You're going to set similar variables and, uh, run it up. So if you're interested in that, you can start, uh, going through those steps and then you can customize it. The other kind of rag that we have is rag on documents. So if you're trying to ask questions about unstructured documents, like you've got a bunch of PDFs or Word docs, Excel files, uh, anything like that, you can actually put those into a search index and then search that.</p><p>So the example we have for that is rag with Azure AI search. And, uh, it's a really, really full featured example. We've had it for the last like more than a year now, and we've had thousands of developers deploy with it and put it into production. And so it's been used for a ton of use cases and it's got a lot of features, uh, speech, voice, vision, user access control, lots of, lots of cool things in it.</p><p>Uh, so let me show, that was the one I was actually showing earlier with my blog, right? So here's, you know, I made a version of it that's just based off my blog posts and, uh, you know, it can cite my blog posts. I've also got this one here, which is for an internal company handbook, which is a very popular way of using it as well.</p><p>And so you can see for each of them, we can, you know, click on the citations and, uh, and yeah. So now this is a bit more complicated because here we have a multi-page document. So we've got a 31 page PDF. We can't just send an entire 31 page PDF to the LLM.</p><p>Cause for a lot of our LLMs, it's going to go beyond the context window, right? A lot of our LLMs have a context window limit. So typically that's around 8K, 8,000 tokens, uh, can go up to 32K, even 128K we're seeing. Um, but typically they do have some sort of context window.</p><p>And even if they don't have some sort of context window, LLMs can get lost. If you give them too information, too much information, there's a research paper called lost in the middle, where they did a study to see if they throw too much information. information at LLM, like at what point it stops paying attention.</p><p>So we generally want to send the LLM the most relevant chunks. So what we do is that we first have this data ingestion phase that will take a PDF or whatever document takes a document. It extracts all the text from it. And we do that with Azure document intelligence, which is very good at extracting text from all sorts of documents.</p><p>So we extract the text from it. We chunk up the text into like good sized chunks, usually around 500 tokens each. Then we store each of those chunks in the search index along with their embeddings. And that's what we actually search on and send. And then we send, right?</p><p>So if we look at the search results here, we can actually see that the search results are just chunks from the PDF, where we say, here's the chunk. Here's the embedding. Here's the embedding. This is the page it came from. And this is the file it came from. And we just send back those chunks.</p><p>So this is the most complicated of our architectures because we do have to have that data ingestion phase. And that means we have to have, you know, a script or a process that does that ingestion stage. And, you know, here we can do it locally or in the cloud.</p><p>So those are the two rag samples. So we have, you know, we have another 40 minutes. So, and we have like a good ratio here of helpers to y'all. So if either of those sound compelling to you, like sound like a use case that you're interested in, then you can try to deploy them now and see how they work.</p><p>So once again, you just go to the app templates workshop repo, and you can either pick rag on Postgres or rag with AI search, and then start going through the steps to try it out. These will take longer to deploy. So it's good to start to deploy, you know, now, because they take, they got a lot more infrastructure to set up.</p><p>And then for the AI search, it's got to do the whole ingestion step. And that ingestion step takes a certain amount of time as well. So, yeah, any questions before? For the ingestion step, are you using any libraries for the jumping and all that stuff? Yeah, that's a great question.</p><p>Are we using libraries? So when this sample was first created, it was like last April. It was before there was like really good established libraries. We kind of use link chain, but not heavily. So all of ours, it's actually custom coded. Now, if you're going to use a library, the big thing I would make sure you're doing is using a token based chunker.</p><p>A lot of the splitters out there are doing character based splitting, which is probably fine if you're doing English only documents, but we do have lots of international customers. And as soon as you start doing non English documents, then you really want to do stuff based off of tokens and not characters.</p><p>Because imagine you take like a Chinese document and you you'd say like, oh, my chunks are a thousand characters long. Like that's a lot of tokens. You can like go over the context window really fast. So we have token based chunking that we've implemented here. There are there is token based chunking available in link chain.</p><p>So if you're going to use link chain, the thing to do is find my colleague's blog post where he talked about it. OK, yeah, where can we see, especially if you're doing anything non English. He basically analyzed all the splitters from link chain to figure out which of them properly worked with token based splitting and with CJK languages in particular.</p><p>So we've implemented this ourself. He actually told my manager Anthony, he worked on it. But link chain and llama index both do a lot of this stuff. They just, you know, they take care of behind the scenes. So what you need is you need the splitting and you can get that from basically from link chain because llama index uses link chain.</p><p>So I would just say use use link chain probably with this one so you can specify the chunk size. And then then you just have to vectorize. So that that's easy. You just use the OpenAI SDK and we do the batch embeddings with that so that we can do a bunch at a time.</p><p>And then you just store it in AI search. So the hard part is really the extracting the text. So there we either use Azure document intelligence in the cloud or we do have some local parsers, too. If somebody doesn't want to use document intelligence, we use like PI PDF.</p><p>We use our own CSV parser because that's straightforward HTML for my blog. I just use beautiful soup, which is the Python package that does HTML parsing, right? Because I thought I could do a better job at it. So this one I just use beautiful soup to extract the text.</p><p>So you can do that as well. And we've got beautiful soup in there. So, yeah, there is actually a surprising amount of things that we've written ourselves for the AI search repo. If we were going to do it today, we'd probably use the link chain splitter at least. Yeah.</p><p>Good question. Sorry, long answer. Other questions? So the ABM. So generally with BICEP, what it does is that it tries to figure out, and BICEP is really compiled down to ARM, and ARM is just JSON. So what you're actually doing is what's called an ARM-based deployment. So with ARM-based deployments, what they try to do is figure out what does your resource currently look like?</p><p>What are you saying you want it to look like? And what changes does it need to make happen? So, yeah, we'll probably switch over to ABM in a lot of our samples. And we're probably just going to make sure, like, we're trying to make it not have a change.</p><p>But if you want it to change, then that's fine. So you should totally be able to switch between ABM, not ABM, as you decide, as you see fit. And the important thing is just -- it'll figure out the difference and just make sure you are on board with any changes that come up.</p><p>There is, like -- so if you're doing -- there's this AZ deployment command that does what if, and that tells you, like, actually tells you what resources will change. I want to figure out how we can do that with AZD. I think AZD maybe has a dry run command.</p><p>So that might be what we try when we consider switching to ABM. Because we want to switch to ABM so that we don't have to maintain our own modules. But we just want to make sure that we are aware of any configuration changes that could happen. They're just different things.</p><p>Yeah, AZD is a command line tool that, you know, does the Arm-based deployment and also does code deployment, code upload, right? So I have this azure.yaml here. I didn't show this. So azure.yaml says this is the code that you're going to deploy to this host. So AZD does multiple things.</p><p>It does provisioning, which is basically doing an Arm-based deployment, which is equivalent to if you're doing AZ -- AZ Deployment, if you know the Azure CLI, it's this AZ Deployment command. So it's doing that. And then it's also doing packaging and code deployment. So if you've ever done like, I don't know, if you've ever done like Web App Up, that's where you deploy code up to App Service.</p><p>AZD will also do that for us. So AZD is trying to do the whole workflow of you need to provision your resources and you need to deploy your code. And we're trying to make this central way of doing it across all of our offerings. Because right now with Azure -- you know Azure -- but we've got like a billion different ways of doing things across all the different things.</p><p>And AZD is trying to make a more common way of doing it. So if you look at my GitHub repo, I'm kind of a huge AZD fangirl. So you can see all of these repos are all AZD-ified, almost all. That's what this AZD column is. Because to me it's the best way to deploy because it's repeatable.</p><p>So if you are looking for examples, I have quite a few here. But yeah, so we're -- you know, we should be able to do it on different hosts, container apps, functions, app service, Kubernetes, et cetera. And, you know, with all this -- all the different possible bicep. Other questions?</p><p>So what happens after you go to production, like observability and all that? Oh yeah, great question. So we do have a -- like generally there's lots of docs under Azure Search OpenAI demo. So we do actually have a productionizing guide. You also asked specifically about observability. We do integrate with Application Insights with OpenTelemetry.</p><p>So that's what we use by default. If you want, you could use LangFuse. I actually like to use LangFuse. I don't know if you've seen it, but it's an observability platform. So you could use LangFuse. But by default we're using Azure Application Insights with the OpenTelemetry packages to bring everything in there.</p><p>But we do have a whole productionizing guide that talks about, you know, how are you going to scale things? You know, if you need a load balance, your OpenAI capacity. If you want to do VNet deployment. If you want to do user auth. How to do load testing. So I've run quite a few load tests for this one.</p><p>And then how to do evaluation. So I -- you know, I like to do evaluation. Everyone should do it. It's basically like the new form of testing for this world. Let me see. I think I closed my evaluation report. I can open it. But basically like you want to be running evaluations to see if you are getting quality results from your LLM.</p><p>Because a lot of times you might run -- here's the thing. You know, I show those sample questions all the time and they perform great. But you cannot trust your sample questions. And you can't even trust them. Like you might make a prompt tweak and be like, oh, this prompt tweak was so good.</p><p>I'm getting such good results. You cannot trust it. You have to run an evaluation across a huge number of samples to make sure that it's actually running. Like I run it across like 200 samples is probably like the minimum of what you should do. But you have to run evaluations in order to see -- do you -- I'm assuming you run evaluations for co-pilot chat, right?</p><p>Yeah. How many samples do you run off? Oh. It's cool. You want to come up and like talk about evaluation? Because you're like -- you're -- I mean, Harold is like running an actual -- because basically you're making co-pilot chat. Here we go. Co-pilot chat. Which you can see I use it a lot.</p><p>So you're like running a real rack. Yeah. Yeah. So it's different racks though. So there's not just one rack. So if you do add workspace, if you ever try that in co-pilot chat, we actually run a local sparse index. So that's basically your classic how Google works, just looking up words on the internet and documents.</p><p>And it's faster and we can do it locally. So that will always work. We also do a semantic index against index that GitHub.com maintains and ranking those in using another alarm call. So rack basically becomes a series of indexes. Like you do Postgres. Yeah. You showed that. You created some keywords up front based on the search.</p><p>So that's what we also do locally. But yeah. Anytime we have changes, we have one test set that can run on each PR and then a larger test that we run daily that has a lot more repositories from across different languages. So you run the evals in the PR?</p><p>Some. Yeah. So we have a subset that's more unit test driven. Where it's like, can it answer questions for this? Does it hit any issues we've seen in the past? So it's more unit test style where it's like, does it behave as it did before? So yeah. It's important though.</p><p>I mean, that's the first, the biggest thing we invested on early on because we found it's so easy to get lost in prompt crafting and assume how reg works and assume how it works in the wild. So it's TF IDF. Yeah. Yeah. Yeah. So yeah. Cool. My version. I don't know if you can show your vowels, but here, like I can show vowels for, um, on the Azure AI search.</p><p>Um, so let's see. Um, I'll summary summary. Okay. All right. So we'll look at them for like my blog. Uh, let's see. So these are ones I've run before. Uh, probably Pamela's blog. Pamela's blog. Yeah. Pamela's blog results. Okay. All right. So these are a bunch of valuations that, um, uh, I'm going to show you how this is.</p><p>So let's see. Um, I'll summary summary summary. Okay. All right. So we'll look at them for like my blog. Uh, let's see. So these are ones I've run before. Uh, probably Pamela's blog. Pamela's blog. Yeah. Pamela's blog results. Okay. So these are a bunch of valuations that, uh, run fairly recently.</p><p>Right. So, um, with these evaluations, I do GPT metrics and then I also do basically like regular expression metrics. How are your metrics usually GPT metrics or code tests? Okay. Okay. So with these GPT metrics, what they're actually doing is, um, sending the original answer, uh, sending, sending the ground truth answer, uh, which is generated synthetically.</p><p>And then also sending the new answer to a LLM and saying, Hey, rate this from one to five. And then we can see the results. And this is, this is the actual prompt that gets sent is like, okay, you know, rate this, you know, from one to five. Here's some examples.</p><p>So we do that for groundedness. We do that for relevance. And then I also check whether citations match across ground truth and not ground truth. And that's actually my favorite metric. And that's just a regex. So my favorite is just this one, this, uh, citation match here. So I'm just making sure that the answer contains the, at least the citations that were in the ground truth.</p><p>So I run metrics like this. A lot of times I'm looking at retrieval parameters. Cause for rag, the retrieval is, makes a big difference. So here I was comparing stuff like, what if I use text only? What if I do vector only? What if I do hybrid? What if I do hybrid with rancor?</p><p>Uh, and that's super interesting. I was trying with different retrieval amounts. Like if I retrieve five versus 10 versus three, what do I get out? Uh, what if I change a prompt? I just say like, I've tried so many like tweaks on our prompt and I've never managed to actually get improvement in the overall stats.</p><p>Uh, so we still haven't ever, ever changed the prompt because I, I haven't really proved that anything is sufficiently better or I'm just a really bad prompt engineer. I don't know. I've none of my prompt engineering ever moves the needle. For me, the only thing that moves the needle is retrieval parameters.</p><p>Like how you're working with your search engine or changing the model entirely. Changing a GPT four has a big difference than GPT three, five. Uh, so that should be part of your, uh, you're putting in production for sure is to make sure that you've got some sort of valuation set up.</p><p>Uh, other questions. Is anyone trying to get one of the rags up? I'm getting rags up. I try to like take a lot of things to production, but they were all like early projects. Yeah. I had a hard time evaluating which vector store to use and how much to challenge.</p><p>Yeah. I mean, it's hard. That's part of why I, I do it as well. I've also run all those on our sample data too. But I think what I've discovered is really helps to run the evaluations on stuff that, you know, because then like, cause this is the summary.</p><p>Like you can kind of look at the summary and be like, okay, I guess like things better. But then what I usually look at is like, I actually look at the changes between, um, between two runs and be like, okay, well, what was the difference between, uh, the baseline?</p><p>And then, uh, you know, the, maybe, what was it? Vector only vector, no ranker. Okay. And then I'll just look at things that changed on citation match. Okay. Okay. So this is what I usually do is I look at the overall stuff and then I look and I compare the answers across my ground truth and the, and the new one with the parameters.</p><p>And so then I can better reason about it. But you really have to know your domain in order to be able to evaluate, evaluate your evaluations. Um, but it also helps if other people have run it for you. So this is a really good blog post from the AI search team that I always reference where they ran massive queries, looking at hybrid search versus vector search versus tech search.</p><p>And, um, you know, and they found that hybrid retrieval was semantic ranking outperforms vector only search. So I ran my own versions of that and, um, and recently, uh, blogged about it, but it's basically the stats that I was just showing where, uh, what I found actually for my use case vector on its own.</p><p>Did horribly like really, really, really, really badly. Uh, where is it? Um, so vector only got a groundedness of 2.79, which is really, really low text only got 4.87. So part of that is because as your AI search is really good at full text search, like incredibly good at it.</p><p>It does all spell check, stemming, everything you could imagine. Um, hybrid, which is where you take vector and text and then you merge them using this algorithm called reciprocal rank fusion, which you can actually do this. See that the algorithm is just this. It's just, uh, you're just doing a little math here to combine, uh, rank scores.</p><p>Um, so just a basic hybrid like that, the groundedness is only 3.26. So you can see hybrid on its own is worse than text only. And that's because vector results can add so much noise. You accidentally grab the wrong, like distracting things. Uh, what I found actually is like, if I ever accidentally vectorize like an empty string or something close to an empty string is similar to everything.</p><p>I don't know what this is about the open AI embedding space, but if you accidentally vectorize an empty string or even like, uh, we have vision as a feature in the Azure open ice search demo. If I was helping a customer this, this week and they were finding that so many of the results were getting this blank blue page because apparently this blank blue page, the vector for it.</p><p>And this is a vector via a different model that as your computer vision model, the vector for it was just matching everything. So you gotta like, you gotta be really careful with vector spaces. Um, it's so easy to accidentally add noise to them and for there to be distractions.</p><p>So hybrid on its own only, you know, got like 3.26. Once I use hybrid with semantic ranker, then I got the best results, but only by a couple percentage points. Now hybrid of semantic ranker is semantic ranker. That's a feature of Azure AI search, which is actually another machine learning model.</p><p>It's a, it's called a cross encoder model, but basically they actually had humans rank results according to queries. They use it for Bing. So they said, Hey humans, here's 10 search results for a query, rank these from one to 10 and tell us what's the best. So they train a whole model based off a bunch of human data.</p><p>And then they get back like this, this, uh, this model that they can then use for any arbitrary, uh, ranking of user query along with results. So basically hybrid with being ranker gets you the best. But if I was going to have to, like, if I was on a desert island and like I, I could pick between vector and text, uh, I would use text at least for Azure AI search.</p><p>It's going to depend how good your full text search, right? If you're doing full text search with like SQL light, which I don't even know if it's the ports. We'll take search. It's not going to do very well. Yeah. Um, you, so you're using tipative for your co-pilot chat.</p><p>You said, right. For this one, for this, for the at workspace, right? Yeah. Um, for, for Azure AI search, they're using, uh, they're using several things, but they're using one of the things they use is Lucene, which is, um, a search library. And it's got stuff like spell checking and tokenization and stuff like that.</p><p>Um, so they're doing a lot, but I guess, and they're also using BM 25, which I think is basically tipative. Right. Okay. Yeah. So BM 25, that's what you want to look for. Um, is, uh, we got a search result here. Yeah. So if it's, if something is using BM 25, I think that's basically the best full text.</p><p>I think that's the best full text right now. So, um, that's what you want to look for is you just want to look for a good full text option. Yeah. Yeah. It's overwhelming. That's why I love when, you know, people put out research. So we'd be like, okay, great.</p><p>Cause this also has the optimal, um, chunk size. That's why I was saying we do 500 tokens because they did the, they did the, the work here and said, okay, the optimal is 512 tokens. Great. That's what we're going to use. Now, obviously for your particular use case, it can be different, but we can't all run like 20 hundred different tests to see what the optimal, uh, you know, thing is.</p><p>So it's really nice when people, you know, document what worked well for them. Cool. Yeah. Well, we only need to update the vectors if the data changes or if we're changing our embedding model. So if we change our embedding model, we have to update everything to use a new embedding model.</p><p>Right. Cause now opening. I has these new embedding models. I need to do some tests with them to see if I can get like better results for them. Um, so in that case I would, I would rerun everything. So probably what I want to do is set up like an, uh, a separate AI search index, like for this one, which has uses one of the new embedding models, uh, embedding three.</p><p>And I have to decide how many dimensions to use, um, and then compare it to see how much better results are. I'm told that generally the results are better, but I'm trying, have you tried any of them? Oh, you're switching to the new one. What dimension are you going to use?</p><p>So you can use small two 56. Wow. Yeah. You can do five 12 too. Yeah. Yeah. So you can, that's the thing is it's so, it's so many options now. Yeah. You get a test. Yeah. Oh, and you can run any V tests. You have customers. Yeah. Um, so yeah, but you're going to have to re-index everything.</p><p>Um, so that's, that's when you would, uh, have to update stuff. Is it the content changes or if the model changes? Um, Um, and then, and then test that. Yeah, I do want to try out the new ones. They should redo this one too. It's too many decisions. Cool.</p><p>Any other questions? Harold, do you want to show stuff in, uh, workspace? Okay. You can close out in this. Let's just see a rag in action. Um, so if you ask a question in, in, co-pilot chat, so that's the co-pilot chat panel version. There's also another one that's in line.</p><p>So if you open up, this is a natural input, uh, we call in line chat in, in the, in your code. Basically letting you apply code directly or natural language directly to your code, which is always nice. You don't have to think about the response. You have to think about just, you know what you want and you want to AI to do it for you.</p><p>But in, in the side panel, most of the time, what you will run into is this, you're going to run things. Let's pick a function. These are tests. So compare the tests. Now I've code selected on the right and on the left. I can ask things about the code I have selected.</p><p>That's the surefire way to get good results, have code selected and talk about it. And you already see that, that we do some magic in our responses. So everything is code highlighted. So actually you jump to the different aspects, um, that, that are being used and even to dependencies.</p><p>So it found that there's a dependency, so you can also jump to that. So now going back to here, let's, let's see which tests actually are defined in a repository. And because they're, I want to talk basically about the whole workspace. And that's why I can't just say which tests are defined or how are benchmarks being run.</p><p>So a general question that you would go otherwise to a colleague who hopefully knows this and hopefully they're in the same time zone and they know this. But now I can actually send this to add workspace. And that's where we kicking in this, this whole rag agents scheme. So this repo is probably not indexed on the github.com site.</p><p>So if you're in code for enterprise, you will get a semantic index that github keeps updating for you. They also have a few open source repos indexed. But in this case, this is all happening now in VS code itself. So this is mostly sparse indexing. And actually we see that sparse indexing is usually on par similar to what you see with the text based retrieval that this works really well.</p><p>Yeah, so first we do same as you have an Azure search where it does find more words for what you're potentially looking for that are fitting with the repository. So we also do stemming and that's one that's the first LLM call. Then the T5DF will find all those results and then we do the re-ranking on top.</p><p>So and that actually gets us actually mostly better than doing a full vector search on the same topic. How do you use the re-ranking? The different ways. So that's where we experiment a lot. But it's another GPT 3.5 call, I think. Oh, okay. So use the LLM as a re-ranking?</p><p>Yeah. And you see what's being pulled in. So these are all the things it found and the chunks it found it in. So what we do, what you'll see is we actually do semantic chunking. So for most languages we look at function segments, we look at specific blocks of code and that's where we found the most impact as well.</p><p>So people brought up chunking as a big, big area of improvements and that's what we also have in our code that it's the chunking is the biggest impact finger from what you see. Do you write your semantic chunkers? Yeah. Which helps that we have all the languages, the knowledge around the team like Python, right?</p><p>So, but yeah, so that's, that's the basics. And you'll see that it works everywhere. That's the nice part. It works locally and it works slightly faster if you already have an online index where we can retrieve the semantic index from in action. Questions? What is the dot prompty? Oh, so dot prompty is a, it's a new prompt format.</p><p>You can show the dot prompt you file. Yeah, all those dot ones, those ones, yeah. So this was announced at, what's the top, build? Build, build, yeah. Yeah, so now it's a build. Scroll up to the top of it, yeah. There you go. So, it's a way of, it's like an artifact for prompt.</p><p>Because right now, like, you might store your prompt as a multi-line string variable. And it's like, we store them in all kinds of formats across the repo. So this is like a standard way. So it's actually a Jinja template plus the YAML at the top. So the YAML describes the metadata of the prompts.</p><p>And then the Jinja template, you know, it's a template that you can pass things into. So this is used by Prompt Flow, but it's also used by Azure AI Studio. And the goal is, and I think maybe Linkchain might have support for it, now or soon. But the goal is just to have a common way of representing prompts.</p><p>So we'll probably try to use this in more of our steps on the board. There you go. Just ask Copilot. Yeah, so I'm using, this is using the Prompt Flow evals package, which has a bunch more things as well, other kinds of evaluation. They actually, I wrote my own CLI as UI on top of this, but they have one too.</p><p>Yeah. Prompt. Do you run the evals in your CI pipeline somewhere? Yeah. If you look at Azure devs on this one, it does actually run them. So I run them right now. I'm just running them as a smoke test for this repo. But you can see what I've done is that I have a target URL.</p><p>So that's generally what you'd want to do is you need to run the eval, and then you can run an eval against your live, or like for you, you're doing a VR build. So there you want to run it against your VR build. So the tricky thing is just making sure you have a way of contacting your thing with everything, all the production setup, all that has your stuff in it.</p><p>So yeah, I would ideally have it as a CI step for every one of our repos, and I'm just figuring out the right way of setting up the target URL and all that stuff. Especially because most people aren't making public-facing apps. Most people are either putting it behind user-off or putting it in a v-net.</p><p>So we need evaluation flows that both can use your production resources because that's how you know it's working. But then also work with however your app is deployed. So I think you can certainly figure out how to set it up for your situation. I'm still figuring out how to set it up in the general case.</p><p>But the thing to keep in mind is that evaluations are slow if you're doing GP metrics, right? I mean, generally they're slow because all of these calls are slow. You saw how much time it took to get back a response, right? So generally they're slow. They're much slower than the traditional unit tests.</p><p>So you do not want to casually run an evaluation. They're also expensive, especially if you're doing, well, first because the LLM calls happening behind the scenes. And if you're using GP metrics, because I'm doing all these GP metrics like relevance and readiness, that's another LLM call. So you want to have like a higher barrier to running than with normal unit tests, right?</p><p>And caching. Caching? Oh, you cache? Yeah. How do you cache? How do you know that something hasn't changed? Based on a prompt and the test. Yeah. I guess, yeah. It's all within one repo. This one is like a repo that works with other repos. You don't know if the app has changed behind the scenes.</p><p>Yeah. But yeah. So caching. You can do caching. Cache, what exactly? So we look at each test and we only rerun them when any of the prompts, when the input basically change. So if you imagine like an OpenAI proxy that you could set up, if it's the same similar to what they do, I think OpenAI has like the seed variable, which is basically caching, but they don't tell you.</p><p>And it's basically if nothing changes in a prompt, it just sends back the old response. Oh, so you implement caching in compiler chat, you mean? Or? Not in compiler, in our testing infrastructure. Okay. Some people also implement caching in the RAG application itself. Yeah. I still don't know how often you're going to get the same question.</p><p>For tests it helps. For tests it helps. For tests it helps. Yeah. Yeah. Yeah. I haven't clicked. So can this also, like is this just for OpenAI or can this work with Mestrel and all the other ones mentioned in the beginning? Yeah. Yeah. I mean mine just, this one I just hit up the URL and get back the answer.</p><p>So the URL is just of your deployed app. Oh, sorry. I meant like the starter templates. Oh, yeah. Good question. So with the starter templates, right now they're all configured with OpenAI. And so you can swap out like different OpenAI models so you can do before. But they don't work with the new non-OpenAI models because we can't necessarily use the OpenAI SDK with them.</p><p>I think there is actually a way to use OpenAI SDK with them, but we're supposed to pretend we can't. So there is this new SDK and I haven't, I haven't messed with it yet. I don't know if you have, but as your AI inference, have you seen it? I think this is the new unified SDK.</p><p>And yeah, so this is, this is what to use for everything that's not OpenAI. Oh, it says it can even do OpenAI. So we might have to port to this. The thing I don't love about this is that this is Azure specific. Because right now we use the opening SDK, which is like not Azure specific exactly.</p><p>And so it works with like a llama and stuff. I don't know. So, but we might end up porting for this. So if we ported to this, then probably we would just, it would just work with everything. So this is really new. Like this came out at build. So we just have to decide whether to port everything up to this, out to this so that we can use all the, all the modules, all the models.</p><p>Yeah. Yeah. Everything changes all the time. Yeah. But we would also need to make the bicep for it. That's the other thing I haven't done is I haven't, cause I try to set a bicep for everything. So typically bicep creates your Azure open AI instance. If you're using mistral or llama, you would want bicep to, you'd probably want bicep to create that as well.</p><p>Um, and so that would be different bicep as an addition. So we'll probably end up adding it. Cause basically what you do is you go to the issue tracker, you file a request. And then if enough people ask for it, we're like, okay, guess we're going to do it then.</p><p>We can. Yeah. Yeah. Um, but that's how we figure out what, you know, what it is that people are looking for. Cause it is really nice to be able to swap out models. Cause right now all of the samples do work with OLAMA. So if you have like, uh, OLAMA running locally, here's my little OLAMA up there.</p><p>Um, you know, you can run like five, three and stuff like that. Uh, you just go, you know, you go to your terminal and you're like OLAMA. Is this it? I think, I don't know if I typed by three correct, but, um, and so they do all run with, uh, OLAMA things, but none of the OLAMA models have really been sufficient for RAG in my experience.</p><p>Like I, I run them just to check, but they, they all fail to get, um, follow directions. In my experience. Uh, cause I just think they're not, they don't have enough parameters. Like these are like three B seven B et cetera. So, um, they don't provide citations correctly. I don't know.</p><p>Have you had more success with like five, three mini. You see every model, of course, prompt changes. Yeah. I'm bad at prompt engineering. Yeah. So, so out of the gate, like I haven't had success using any of the small language models for RAG. I'm sure the big versions of them would work much better.</p><p>So I do want to try out like the 70 B I've done, I've done up to seven B cause that's like how far I can go up locally. I can't go much more than that just from space reasons. So for them, what happens is that like they'll answer the questions fine.</p><p>The issue is that we need citations to be in good format. Cause these are actually come back as bracketed square brackets. And they just don't reliably come back with square bracketed citations, which doesn't sound like a big deal, but like we're trying to make clickable citations here. Uh, so that's the issue I've had is that I, I think they're fine at synthesizing the information, but they don't follow the syntax directions in terms of the citations.</p><p>Um, and they're kind of more, maybe more likely to, uh, make stuff up if I ask an off topic question. That's been my experience there. Um, yeah. Good for re-ranking. I think finding. Oh, yeah. Or like the GPT judging. I think that that's where like finding that this one thing, maybe not like the expert full answers that, that follows the format, but like one of the smaller paths.</p><p>But you can't, so most of them don't support function calling off the bat. So you're doing, would you do re-ranking with just a simple, you'd have to figure out what syntax they come back with. But they're all good at coding. Yeah, that's right. That's true. Everyone's good at coding.</p><p>If you can turn something into a coding task, you're good. Yeah. That's another form of rag is like, uh, I was telling some last time, like, uh, these are, these are all like doing like kind of rag on just a few documents at a time. If you're trying to like analyze a whole database or like a huge number of documents, then you really want to like actually, uh, use like a SQL query, like with like aggregate functions or do like a pandas query.</p><p>Right. So at PyCon, we did a demo where you like upload a CSV and then you, you say like, oh, I want to count the top restaurants in it. And then it just comes up with the pandas code and then it runs the pandas code in a sandbox environment.</p><p>So that's, that's another like increasingly common, uh, form of rag where if you want to like come up with insights and analysis and, and that sort of thing, then you want to consider a different architecture where you're actually going to have the LLM generate pandas code or SQL code.</p><p>It's very good at both of those and then run those in a safe way. Yeah. Are you actually using type chat? Cause that's basically what type chat does. Okay. Okay. So yeah, what you're describing, that's the same, the same approach. Yeah. So that would be, yeah, we, we're trying, um, I know Daniel's actually experimenting with type, Daniel's the creator of type chat.</p><p>He is experimenting with type chat with the local models. Um, so try it. Cause we tried, we did also try type chat with five, three locally to see if we could use it instead of function calling with open AI. And we were having a hard time with it. Um, but I think Daniel like maybe has to like beat the prompts and maybe they'll end up working better.</p><p>Like, uh, like three has a response format. Maybe the bigger one, but not, not the smaller one. Not the smaller one. Yeah. Yeah. Yeah. Yeah. And that would work. Yeah. Yeah. That makes sense. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. With Ron, like probably hearing like that.</p><p>Cool. Good work. Nice. Cool. All right. Well, you have the passes for seven days, so feel free to keep deploying. Uh, if you have any feedback for the workshop, tell us, or we have surveyed in which I assume is anonymous. Yeah. It's anonymous. And so you can fill out that one as well.</p><p>Like, picture. Fill out later. And that would be perfect. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. We'll see you next time.</p></div></div></body></html>