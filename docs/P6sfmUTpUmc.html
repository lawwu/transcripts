<html><head><title>Building makemore Part 3: Activations & Gradients, BatchNorm</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Building makemore Part 3: Activations & Gradients, BatchNorm</h2><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc"><img src="https://i.ytimg.com/vi_webp/P6sfmUTpUmc/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=0">0:0</a> intro<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=82">1:22</a> starter code<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=259">4:19</a> fixing the initial loss<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=779">12:59</a> fixing the saturated tanh<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1673">27:53</a> calculating the init scale: “Kaiming init”<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2440">40:40</a> batch normalization<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3787">63:7</a> batch normalization: summary<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3890">64:50</a> real example: resnet50 walkthrough<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4450">74:10</a> summary of the lecture<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4715">78:35</a> just kidding: part2: PyTorch-ifying the code<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5211">86:51</a> viz #1: forward pass activations statistics<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5454">90:54</a> viz #2: backward pass gradient statistics<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5527">92:7</a> the fully linear case of no non-linearities<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5775">96:15</a> viz #3: parameter activation and gradient statistics<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5995">99:55</a> viz #4: update:data ratio over time<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6364">106:4</a> bringing back batchnorm, looking at the visualizations<br><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6694">111:34</a> summary of the lecture for real this time<br><br><div style="text-align: left;"><a href="./P6sfmUTpUmc.html">Whisper Transcript</a> | <a href="./transcript_P6sfmUTpUmc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone. Today we are continuing our implementation of Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3" target="_blank">00:00:03.600</a></span> | <span class="t">Now in the last lecture we implemented the multilayer perceptron along the lines of Benji</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=7" target="_blank">00:00:07.680</a></span> | <span class="t">Hotel 2003 for character level language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=10" target="_blank">00:00:10.840</a></span> | <span class="t">So we followed this paper, took in a few characters in the past, and used an MLP to predict the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=15" target="_blank">00:00:15.240</a></span> | <span class="t">next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=17" target="_blank">00:00:17.420</a></span> | <span class="t">So what we'd like to do now is we'd like to move on to more complex and larger neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=20" target="_blank">00:00:20.880</a></span> | <span class="t">networks, like recurrent neural networks and their variations like the GRU, LSTM, and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=25" target="_blank">00:00:25.200</a></span> | <span class="t">on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=26" target="_blank">00:00:26.200</a></span> | <span class="t">Now, before we do that though, we have to stick around the level of multilayer perceptron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=30" target="_blank">00:00:30.320</a></span> | <span class="t">for a bit longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=31" target="_blank">00:00:31.760</a></span> | <span class="t">And I'd like to do this because I would like us to have a very good intuitive understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=35" target="_blank">00:00:35.360</a></span> | <span class="t">of the activations in the neural net during training, and especially the gradients that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=39" target="_blank">00:00:39.680</a></span> | <span class="t">are flowing backwards, and how they behave and what they look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=43" target="_blank">00:00:43.120</a></span> | <span class="t">This is going to be very important to understand the history of the development of these architectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=48" target="_blank">00:00:48.320</a></span> | <span class="t">because we'll see that recurrent neural networks, while they are very expressive in that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=52" target="_blank">00:00:52.360</a></span> | <span class="t">are a universal approximator and can in principle implement all the algorithms, we'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=58" target="_blank">00:00:58.400</a></span> | <span class="t">they are not very easily optimizable with the first-order gradient-based techniques</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=62" target="_blank">00:01:02.040</a></span> | <span class="t">that we have available to us and that we use all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=64" target="_blank">00:01:04.960</a></span> | <span class="t">And the key to understanding why they are not optimizable easily is to understand the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=70" target="_blank">00:01:10.040</a></span> | <span class="t">activations and the gradients and how they behave during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=72" target="_blank">00:01:12.920</a></span> | <span class="t">And we'll see that a lot of the variants since recurrent neural networks have tried to improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=77" target="_blank">00:01:17.880</a></span> | <span class="t">that situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=79" target="_blank">00:01:19.280</a></span> | <span class="t">And so that's the path that we have to take, and let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=83" target="_blank">00:01:23.000</a></span> | <span class="t">So the starting code for this lecture is largely the code from before, but I've cleaned it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=86" target="_blank">00:01:26.960</a></span> | <span class="t">up a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=87" target="_blank">00:01:27.960</a></span> | <span class="t">So you'll see that we are importing all the Torch and Mathplotlib utilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=93" target="_blank">00:01:33.640</a></span> | <span class="t">We're reading in the words just like before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=95" target="_blank">00:01:35.700</a></span> | <span class="t">These are eight example words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=97" target="_blank">00:01:37.360</a></span> | <span class="t">There's a total of 32,000 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=99" target="_blank">00:01:39.440</a></span> | <span class="t">Here's a vocabulary of all the lowercase letters and the special dot token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=104" target="_blank">00:01:44.500</a></span> | <span class="t">Here we are reading the dataset and processing it and creating three splits, the train, dev,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=111" target="_blank">00:01:51.900</a></span> | <span class="t">and the test split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=112" target="_blank">00:01:52.900</a></span> | <span class="t">Now in the MLP, this is the identical same MLP, except you see that I removed a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=117" target="_blank">00:01:57.940</a></span> | <span class="t">of magic numbers that we had here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=119" target="_blank">00:01:59.880</a></span> | <span class="t">And instead we have the dimensionality of the embedding space of the characters and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=123" target="_blank">00:02:03.740</a></span> | <span class="t">the number of hidden units in the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=126" target="_blank">00:02:06.260</a></span> | <span class="t">And so I've pulled them outside here so that we don't have to go and change all these magic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=130" target="_blank">00:02:10.100</a></span> | <span class="t">numbers all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=131" target="_blank">00:02:11.820</a></span> | <span class="t">With the same neural net with 11,000 parameters that we optimize now over 200,000 steps with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=136" target="_blank">00:02:16.740</a></span> | <span class="t">a batch size of 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=138" target="_blank">00:02:18.460</a></span> | <span class="t">And you'll see that I refactored the code here a little bit, but there are no functional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=143" target="_blank">00:02:23.140</a></span> | <span class="t">changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=144" target="_blank">00:02:24.140</a></span> | <span class="t">I just created a few extra variables, a few more comments, and I removed all the magic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=148" target="_blank">00:02:28.340</a></span> | <span class="t">numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=149" target="_blank">00:02:29.540</a></span> | <span class="t">And otherwise it's the exact same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=152" target="_blank">00:02:32.100</a></span> | <span class="t">Then when we optimize, we saw that our loss looked something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=156" target="_blank">00:02:36.060</a></span> | <span class="t">We saw that the train and val loss were about 2.16 and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=161" target="_blank">00:02:41.900</a></span> | <span class="t">Here I refactored the code a little bit for the evaluation of arbitrary splits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=167" target="_blank">00:02:47.220</a></span> | <span class="t">So you pass in a string of which split you'd like to evaluate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=170" target="_blank">00:02:50.220</a></span> | <span class="t">And then here, depending on train, val, or test, I index in and I get the correct split.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=175" target="_blank">00:02:55.700</a></span> | <span class="t">And then this is the forward pass of the network and evaluation of the loss and printing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=180" target="_blank">00:03:00.160</a></span> | <span class="t">So just making it nicer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=183" target="_blank">00:03:03.100</a></span> | <span class="t">One thing that you'll notice here is I'm using a decorator torch.nograd, which you can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=189" target="_blank">00:03:09.260</a></span> | <span class="t">look up and read documentation of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=191" target="_blank">00:03:11.700</a></span> | <span class="t">Basically what this decorator does on top of a function is that whatever happens in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=196" target="_blank">00:03:16.060</a></span> | <span class="t">this function is assumed by Torch to never require any gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=202" target="_blank">00:03:22.100</a></span> | <span class="t">So it will not do any of the bookkeeping that it does to keep track of all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=206" target="_blank">00:03:26.700</a></span> | <span class="t">in anticipation of an eventual backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=209" target="_blank">00:03:29.760</a></span> | <span class="t">It's almost as if all the tensors that get created here have a requires grad of false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=214" target="_blank">00:03:34.660</a></span> | <span class="t">And so it just makes everything much more efficient because you're telling Torch that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=217" target="_blank">00:03:37.420</a></span> | <span class="t">I will not call .backward on any of this computation and you don't need to maintain the graph under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=222" target="_blank">00:03:42.180</a></span> | <span class="t">the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=223" target="_blank">00:03:43.860</a></span> | <span class="t">So that's what this does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=225" target="_blank">00:03:45.740</a></span> | <span class="t">And you can also use a context manager with torch.nograd and you can look those up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=233" target="_blank">00:03:53.180</a></span> | <span class="t">Then here we have the sampling from a model just as before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=237" target="_blank">00:03:57.100</a></span> | <span class="t">So for passive neural net, getting the distribution, sampling from it, adjusting the context window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=242" target="_blank">00:04:02.100</a></span> | <span class="t">and repeating until we get the special end token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=244" target="_blank">00:04:04.980</a></span> | <span class="t">And we see that we are starting to get much nicer looking words sampled from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=249" target="_blank">00:04:09.980</a></span> | <span class="t">It's still not amazing and they're still not fully name-like, but it's much better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=254" target="_blank">00:04:14.260</a></span> | <span class="t">when we had to do it with the bigram model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=257" target="_blank">00:04:17.860</a></span> | <span class="t">So that's our starting point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=259" target="_blank">00:04:19.240</a></span> | <span class="t">Now the first thing I would like to scrutinize is the initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=262" target="_blank">00:04:22.620</a></span> | <span class="t">I can tell that our network is very improperly configured at initialization and there's multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=268" target="_blank">00:04:28.260</a></span> | <span class="t">things wrong with it, but let's just start with the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=271" target="_blank">00:04:31.300</a></span> | <span class="t">Look here on the zeroth iteration, the very first iteration, we are recording a loss of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=276" target="_blank">00:04:36.220</a></span> | <span class="t">27 and this rapidly comes down to roughly one or two or so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=280" target="_blank">00:04:40.380</a></span> | <span class="t">So I can tell that the initialization is all messed up because this is way too high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=284" target="_blank">00:04:44.500</a></span> | <span class="t">In training of neural nets, it is almost always the case that you will have a rough idea for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=288" target="_blank">00:04:48.060</a></span> | <span class="t">what loss to expect at initialization, and that just depends on the loss function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=293" target="_blank">00:04:53.020</a></span> | <span class="t">the problem setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=294" target="_blank">00:04:54.820</a></span> | <span class="t">In this case, I do not expect 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=297" target="_blank">00:04:57.060</a></span> | <span class="t">I expect a much lower number and we can calculate it together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=300" target="_blank">00:05:00.900</a></span> | <span class="t">Basically at initialization, what we'd like is that there's 27 characters that could come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=306" target="_blank">00:05:06.500</a></span> | <span class="t">next for any one training example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=309" target="_blank">00:05:09.140</a></span> | <span class="t">At initialization, we have no reason to believe any characters to be much more likely than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=312" target="_blank">00:05:12.620</a></span> | <span class="t">others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=313" target="_blank">00:05:13.900</a></span> | <span class="t">And so we'd expect that the probability distribution that comes out initially is a uniform distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=319" target="_blank">00:05:19.100</a></span> | <span class="t">assigning about equal probability to all the 27 characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=323" target="_blank">00:05:23.540</a></span> | <span class="t">So basically what we'd like is the probability for any character would be roughly one over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=329" target="_blank">00:05:29.140</a></span> | <span class="t">27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=330" target="_blank">00:05:30.140</a></span> | <span class="t">That is the probability we should record.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=334" target="_blank">00:05:34.020</a></span> | <span class="t">And then the loss is the negative log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=336" target="_blank">00:05:36.740</a></span> | <span class="t">So let's wrap this in a tensor and then that one can take the log of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=342" target="_blank">00:05:42.160</a></span> | <span class="t">And then the negative log probability is the loss we would expect, which is 3.29, much,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=347" target="_blank">00:05:47.820</a></span> | <span class="t">much lower than 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=350" target="_blank">00:05:50.020</a></span> | <span class="t">And so what's happening right now is that at initialization, the neural net is creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=353" target="_blank">00:05:53.740</a></span> | <span class="t">probability distributions that are all messed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=356" target="_blank">00:05:56.420</a></span> | <span class="t">Some characters are very confident and some characters are very not confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=360" target="_blank">00:06:00.780</a></span> | <span class="t">And then basically what's happening is that the network is very confidently wrong and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=367" target="_blank">00:06:07.100</a></span> | <span class="t">that's what makes it record very high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=370" target="_blank">00:06:10.680</a></span> | <span class="t">So here's a smaller four-dimensional example of the issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=373" target="_blank">00:06:13.520</a></span> | <span class="t">Let's say we only have four characters and then we have logits that come out of the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=378" target="_blank">00:06:18.180</a></span> | <span class="t">net and they are very, very close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=381" target="_blank">00:06:21.000</a></span> | <span class="t">Then when we take the softmax of all zeros, we get probabilities that are a diffuse distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=387" target="_blank">00:06:27.500</a></span> | <span class="t">So sums to one and is exactly uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=391" target="_blank">00:06:31.220</a></span> | <span class="t">And then in this case, if the label is say two, it doesn't actually matter if the label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=396" target="_blank">00:06:36.080</a></span> | <span class="t">is two or three or one or zero because it's a uniform distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=400" target="_blank">00:06:40.000</a></span> | <span class="t">We're recording the exact same loss in this case, 1.38.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=403" target="_blank">00:06:43.200</a></span> | <span class="t">So this is the loss we would expect for a four-dimensional example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=406" target="_blank">00:06:46.360</a></span> | <span class="t">And I can see of course that as we start to manipulate these logits, we're going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=410" target="_blank">00:06:50.920</a></span> | <span class="t">changing the loss here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=412" target="_blank">00:06:52.560</a></span> | <span class="t">So it could be that we luck out and by chance this could be a very high number like five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=418" target="_blank">00:06:58.040</a></span> | <span class="t">or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=419" target="_blank">00:06:59.040</a></span> | <span class="t">Then in that case, we'll record a very low loss because we're assigning the correct probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=422" target="_blank">00:07:02.740</a></span> | <span class="t">at initialization by chance to the correct label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=426" target="_blank">00:07:06.840</a></span> | <span class="t">Much more likely it is that some other dimension will have a high logit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=434" target="_blank">00:07:14.120</a></span> | <span class="t">And then what will happen is we start to record a much higher loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=437" target="_blank">00:07:17.240</a></span> | <span class="t">And what can happen is basically the logits come out like something like this, and they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=442" target="_blank">00:07:22.360</a></span> | <span class="t">take on extreme values and we record really high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=448" target="_blank">00:07:28.640</a></span> | <span class="t">For example, if we have torq.random of four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=451" target="_blank">00:07:31.760</a></span> | <span class="t">So these are normally distributed numbers, four of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=460" target="_blank">00:07:40.560</a></span> | <span class="t">Then here we can also print the logits, probabilities that come out of it and loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=467" target="_blank">00:07:47.160</a></span> | <span class="t">And so because these logits are near zero, for the most part, the loss that comes out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=471" target="_blank">00:07:51.800</a></span> | <span class="t">is okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=474" target="_blank">00:07:54.040</a></span> | <span class="t">But suppose this is like times 10 now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=478" target="_blank">00:07:58.880</a></span> | <span class="t">You see how because these are more extreme values, it's very unlikely that you're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=483" target="_blank">00:08:03.040</a></span> | <span class="t">to be guessing the correct bucket and then you're confidently wrong and recording very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=488" target="_blank">00:08:08.160</a></span> | <span class="t">high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=489" target="_blank">00:08:09.780</a></span> | <span class="t">If your logits are coming out even more extreme, you might get extremely insane losses like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=495" target="_blank">00:08:15.520</a></span> | <span class="t">infinity even at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=500" target="_blank">00:08:20.560</a></span> | <span class="t">So basically this is not good and we want the logits to be roughly zero when the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=506" target="_blank">00:08:26.640</a></span> | <span class="t">is initialized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=507" target="_blank">00:08:27.640</a></span> | <span class="t">In fact, the logits don't have to be just zero, they just have to be equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=511" target="_blank">00:08:31.360</a></span> | <span class="t">So for example, if all the logits are one, then because of the normalization inside the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=515" target="_blank">00:08:35.520</a></span> | <span class="t">softmax, this will actually come out okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=518" target="_blank">00:08:38.640</a></span> | <span class="t">But by symmetry, we don't want it to be any arbitrary positive or negative number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=522" target="_blank">00:08:42.180</a></span> | <span class="t">We just want it to be all zeros and record the loss that we expect at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=526" target="_blank">00:08:46.520</a></span> | <span class="t">So let's now concretely see where things go wrong in our example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=529" target="_blank">00:08:49.040</a></span> | <span class="t">Here we have the initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=531" target="_blank">00:08:51.720</a></span> | <span class="t">Let me reinitialize the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=533" target="_blank">00:08:53.640</a></span> | <span class="t">And here let me break after the very first iteration so we only see the initial loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=538" target="_blank">00:08:58.080</a></span> | <span class="t">which is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=540" target="_blank">00:09:00.280</a></span> | <span class="t">So that's way too high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=541" target="_blank">00:09:01.280</a></span> | <span class="t">And intuitively now we can expect the variables involved and we see that the logits here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=546" target="_blank">00:09:06.040</a></span> | <span class="t">if we just print some of these, if we just print the first row, we see that the logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=551" target="_blank">00:09:11.740</a></span> | <span class="t">take on quite extreme values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=554" target="_blank">00:09:14.040</a></span> | <span class="t">And that's what's creating the fake confidence in incorrect answers and makes the loss get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=560" target="_blank">00:09:20.880</a></span> | <span class="t">very, very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=562" target="_blank">00:09:22.220</a></span> | <span class="t">So these logits should be much, much closer to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=565" target="_blank">00:09:25.520</a></span> | <span class="t">So now let's think through how we can achieve logits coming out of this neural net to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=570" target="_blank">00:09:30.120</a></span> | <span class="t">more closer to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=572" target="_blank">00:09:32.680</a></span> | <span class="t">You see here that logits are calculated as the hidden states multiplied by W2 plus B2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=577" target="_blank">00:09:37.800</a></span> | <span class="t">So first of all, currently we're initializing B2 as random values of the right size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=584" target="_blank">00:09:44.500</a></span> | <span class="t">But because we want roughly zero, we don't actually want to be adding a bias of random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=588" target="_blank">00:09:48.840</a></span> | <span class="t">numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=589" target="_blank">00:09:49.840</a></span> | <span class="t">So I'm going to add a times a zero here to make sure that B2 is just basically zero at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=595" target="_blank">00:09:55.760</a></span> | <span class="t">initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=597" target="_blank">00:09:57.620</a></span> | <span class="t">And second, this is H multiplied by W2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=600" target="_blank">00:10:00.500</a></span> | <span class="t">So if we want logits to be very, very small, then we would be multiplying W2 and making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=605" target="_blank">00:10:05.000</a></span> | <span class="t">that smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=607" target="_blank">00:10:07.140</a></span> | <span class="t">So for example, if we scale down W2 by 0.1, all the elements, then if I do again just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=613" target="_blank">00:10:13.260</a></span> | <span class="t">the very first iteration, you see that we are getting much closer to what we expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=617" target="_blank">00:10:17.480</a></span> | <span class="t">So roughly what we want is about 3.29.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=620" target="_blank">00:10:20.600</a></span> | <span class="t">This is 4.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=621" target="_blank">00:10:21.600</a></span> | <span class="t">I can make this maybe even smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=624" target="_blank">00:10:24.600</a></span> | <span class="t">3.32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=625" target="_blank">00:10:25.600</a></span> | <span class="t">Okay, so we're getting closer and closer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=628" target="_blank">00:10:28.760</a></span> | <span class="t">Now you're probably wondering, can we just set this to zero?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=633" target="_blank">00:10:33.320</a></span> | <span class="t">Then we get, of course, exactly what we're looking for at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=638" target="_blank">00:10:38.360</a></span> | <span class="t">And the reason I don't usually do this is because I'm very nervous, and I'll show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=642" target="_blank">00:10:42.840</a></span> | <span class="t">in a second why you don't want to be setting W's or weights of a neural net exactly to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=647" target="_blank">00:10:47.680</a></span> | <span class="t">zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=648" target="_blank">00:10:48.680</a></span> | <span class="t">You usually want it to be small numbers instead of exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=653" target="_blank">00:10:53.480</a></span> | <span class="t">For this output layer in this specific case, I think it would be fine, but I'll show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=657" target="_blank">00:10:57.840</a></span> | <span class="t">in a second where things go wrong very quickly if you do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=660" target="_blank">00:11:00.840</a></span> | <span class="t">So let's just go with 0.01.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=663" target="_blank">00:11:03.040</a></span> | <span class="t">In that case, our loss is close enough, but has some entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=666" target="_blank">00:11:06.760</a></span> | <span class="t">It's not exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=668" target="_blank">00:11:08.580</a></span> | <span class="t">It's got some little entropy, and that's used for symmetry breaking, as we'll see in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=672" target="_blank">00:11:12.800</a></span> | <span class="t">The logits are now coming out much closer to zero, and everything is well and good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=678" target="_blank">00:11:18.320</a></span> | <span class="t">So if I just erase these, and I now take away the break statement, we can run the optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=686" target="_blank">00:11:26.520</a></span> | <span class="t">with this new initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=688" target="_blank">00:11:28.560</a></span> | <span class="t">And let's just see what losses we record.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=692" target="_blank">00:11:32.120</a></span> | <span class="t">Okay, so I let it run, and you see that we started off good, and then we came down a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=696" target="_blank">00:11:36.520</a></span> | <span class="t">bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=698" target="_blank">00:11:38.400</a></span> | <span class="t">The plot of the loss now doesn't have this hockey-shape appearance, because basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=704" target="_blank">00:11:44.040</a></span> | <span class="t">what's happening in the hockey stick, the very first few iterations of the loss, what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=708" target="_blank">00:11:48.200</a></span> | <span class="t">happening during the optimization is the optimization is just squashing down the logits, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=713" target="_blank">00:11:53.160</a></span> | <span class="t">it's rearranging the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=715" target="_blank">00:11:55.180</a></span> | <span class="t">So basically, we took away this easy part of the loss function where just the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=719" target="_blank">00:11:59.880</a></span> | <span class="t">were just being shrunk down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=722" target="_blank">00:12:02.120</a></span> | <span class="t">And so therefore, we don't get these easy gains in the beginning, and we're just getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=726" target="_blank">00:12:06.500</a></span> | <span class="t">some of the hard gains of training the actual neural net, and so there's no hockey stick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=730" target="_blank">00:12:10.080</a></span> | <span class="t">appearance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=731" target="_blank">00:12:11.560</a></span> | <span class="t">So good things are happening in that both, number one, loss at initialization is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=735" target="_blank">00:12:15.680</a></span> | <span class="t">we expect, and the loss doesn't look like a hockey stick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=740" target="_blank">00:12:20.880</a></span> | <span class="t">And this is true for any neural net you might train, and something to look out for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=745" target="_blank">00:12:25.720</a></span> | <span class="t">And second, the loss that came out is actually quite a bit improved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=749" target="_blank">00:12:29.600</a></span> | <span class="t">Unfortunately, I erased what we had here before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=751" target="_blank">00:12:31.920</a></span> | <span class="t">I believe this was 2.12, and this was 2.16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=757" target="_blank">00:12:37.400</a></span> | <span class="t">So we get a slightly improved result, and the reason for that is because we're spending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=762" target="_blank">00:12:42.240</a></span> | <span class="t">more cycles, more time, optimizing the neural net actually, instead of just spending the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=768" target="_blank">00:12:48.180</a></span> | <span class="t">first several thousand iterations probably just squashing down the weights, because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=773" target="_blank">00:12:53.560</a></span> | <span class="t">are so way too high in the beginning of the initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=776" target="_blank">00:12:56.940</a></span> | <span class="t">So something to look out for, and that's number one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=780" target="_blank">00:13:00.120</a></span> | <span class="t">Now let's look at the second problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=781" target="_blank">00:13:01.880</a></span> | <span class="t">Let me reinitialize our neural net, and let me reintroduce the break statement, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=786" target="_blank">00:13:06.680</a></span> | <span class="t">have a reasonable initial loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=788" target="_blank">00:13:08.680</a></span> | <span class="t">So even though everything is looking good on the level of the loss, and we get something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=791" target="_blank">00:13:11.480</a></span> | <span class="t">that we expect, there's still a deeper problem lurking inside this neural net and its initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=797" target="_blank">00:13:17.560</a></span> | <span class="t">So the logits are now okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=799" target="_blank">00:13:19.960</a></span> | <span class="t">The problem now is with the values of h, the activations of the hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=805" target="_blank">00:13:25.440</a></span> | <span class="t">Now if we just visualize this vector, sorry, this tensor h, it's kind of hard to see, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=810" target="_blank">00:13:30.480</a></span> | <span class="t">the problem here, roughly speaking, is you see how many of the elements are 1 or -1?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=816" target="_blank">00:13:36.080</a></span> | <span class="t">Now recall that torch.10h, the 10h function, is a squashing function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=820" target="_blank">00:13:40.600</a></span> | <span class="t">It takes arbitrary numbers and it squashes them into a range of -1 and 1, and it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=824" target="_blank">00:13:44.600</a></span> | <span class="t">so smoothly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=826" target="_blank">00:13:46.260</a></span> | <span class="t">So let's look at the histogram of h to get a better idea of the distribution of the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=830" target="_blank">00:13:50.720</a></span> | <span class="t">inside this tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=832" target="_blank">00:13:52.420</a></span> | <span class="t">We can do this first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=835" target="_blank">00:13:55.120</a></span> | <span class="t">Well we can see that h is 32 examples and 200 activations in each example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=840" target="_blank">00:14:00.920</a></span> | <span class="t">We can view it as -1, stretch it out into one large vector, and we can then call toList</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=848" target="_blank">00:14:08.600</a></span> | <span class="t">to convert this into one large Python list of floats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=853" target="_blank">00:14:13.800</a></span> | <span class="t">And then we can pass this into plt.hist for histogram, and we say we want 50 bins, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=860" target="_blank">00:14:20.160</a></span> | <span class="t">a semicolon to suppress a bunch of output we don't want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=864" target="_blank">00:14:24.440</a></span> | <span class="t">So we see this histogram, and we see that most of the values by far take on value of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=868" target="_blank">00:14:28.760</a></span> | <span class="t">-1 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=870" target="_blank">00:14:30.160</a></span> | <span class="t">So this 10h is very, very active.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=873" target="_blank">00:14:33.360</a></span> | <span class="t">And we can also look at basically why that is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=877" target="_blank">00:14:37.940</a></span> | <span class="t">We can look at the preactivations that feed into the 10h, and we can see that the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=884" target="_blank">00:14:44.080</a></span> | <span class="t">of the preactivations is very, very broad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=887" target="_blank">00:14:47.480</a></span> | <span class="t">These take numbers between -15 and 15, and that's why in a torch.10h everything is being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=892" target="_blank">00:14:52.440</a></span> | <span class="t">squashed and capped to be in the range of -1 and 1, and lots of numbers here take on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=896" target="_blank">00:14:56.960</a></span> | <span class="t">very extreme values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=899" target="_blank">00:14:59.200</a></span> | <span class="t">Now if you are new to neural networks, you might not actually see this as an issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=903" target="_blank">00:15:03.480</a></span> | <span class="t">But if you're well-versed in the dark arts of backpropagation and have an intuitive sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=907" target="_blank">00:15:07.840</a></span> | <span class="t">of how these gradients flow through a neural net, you are looking at your distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=911" target="_blank">00:15:11.680</a></span> | <span class="t">of 10h activations here, and you are sweating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=915" target="_blank">00:15:15.040</a></span> | <span class="t">So let me show you why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=916" target="_blank">00:15:16.440</a></span> | <span class="t">We have to keep in mind that during backpropagation, just like we saw in micrograd, we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=920" target="_blank">00:15:20.320</a></span> | <span class="t">backward pass starting at the loss and flowing through the network backwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=924" target="_blank">00:15:24.800</a></span> | <span class="t">In particular, we're going to backpropagate through this torch.10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=928" target="_blank">00:15:28.920</a></span> | <span class="t">And this layer here is made up of 200 neurons for each one of these examples, and it implements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=934" target="_blank">00:15:34.960</a></span> | <span class="t">an element-wise 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=936" target="_blank">00:15:36.720</a></span> | <span class="t">So let's look at what happens in 10h in the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=939" target="_blank">00:15:39.860</a></span> | <span class="t">We can actually go back to our previous micrograd code in the very first lecture and see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=944" target="_blank">00:15:44.840</a></span> | <span class="t">we implemented 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=946" target="_blank">00:15:46.960</a></span> | <span class="t">We saw that the input here was x, and then we calculate t, which is the 10h of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=952" target="_blank">00:15:52.320</a></span> | <span class="t">So that's t.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=953" target="_blank">00:15:53.320</a></span> | <span class="t">And t is between -1 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=954" target="_blank">00:15:54.760</a></span> | <span class="t">It's the output of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=956" target="_blank">00:15:56.600</a></span> | <span class="t">And then in the backward pass, how do we backpropagate through a 10h?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=960" target="_blank">00:16:00.200</a></span> | <span class="t">We take out.grad, and then we multiply it, this is the chain rule, with the local gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=966" target="_blank">00:16:06.280</a></span> | <span class="t">which took the form of 1 - t^2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=969" target="_blank">00:16:09.120</a></span> | <span class="t">So what happens if the outputs of your 10h are very close to -1 or 1?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=974" target="_blank">00:16:14.160</a></span> | <span class="t">If you plug in t = 1 here, you're going to get a 0, multiplying out.grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=979" target="_blank">00:16:19.800</a></span> | <span class="t">No matter what out.grad is, we are killing the gradient, and we're stopping, effectively,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=984" target="_blank">00:16:24.880</a></span> | <span class="t">the backpropagation through this 10h unit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=987" target="_blank">00:16:27.360</a></span> | <span class="t">Similarly, when t is -1, this will again become 0, and out.grad just stops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=993" target="_blank">00:16:33.140</a></span> | <span class="t">And intuitively, this makes sense, because this is a 10h neuron, and what's happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=998" target="_blank">00:16:38.440</a></span> | <span class="t">is if its output is very close to 1, then we are in the tail of this 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1004" target="_blank">00:16:44.120</a></span> | <span class="t">And so changing, basically, the input is not going to impact the output of the 10h too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1011" target="_blank">00:16:51.400</a></span> | <span class="t">much, because it's in a flat region of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1015" target="_blank">00:16:55.840</a></span> | <span class="t">And so therefore, there's no impact on the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1018" target="_blank">00:16:58.660</a></span> | <span class="t">And so indeed, the weights and the biases along with this 10h neuron do not impact the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1024" target="_blank">00:17:04.720</a></span> | <span class="t">loss, because the output of this 10h unit is in the flat region of the 10h, and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1029" target="_blank">00:17:09.000</a></span> | <span class="t">no influence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1030" target="_blank">00:17:10.280</a></span> | <span class="t">We can be changing them however we want, and the loss is not impacted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1034" target="_blank">00:17:14.960</a></span> | <span class="t">That's another way to justify that, indeed, the gradient would be basically 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1039" target="_blank">00:17:19.200</a></span> | <span class="t">It vanishes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1040" target="_blank">00:17:20.960</a></span> | <span class="t">Indeed, when t equals 0, we get 1 times out.grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1047" target="_blank">00:17:27.520</a></span> | <span class="t">So when the 10h takes on exactly value of 0, then out.grad is just passed through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1055" target="_blank">00:17:35.040</a></span> | <span class="t">So basically what this is doing is if t is equal to 0, then the 10h unit is inactive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1062" target="_blank">00:17:42.640</a></span> | <span class="t">and a gradient just passes through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1064" target="_blank">00:17:44.960</a></span> | <span class="t">But the more you are in the flat tails, the more the gradient is squashed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1069" target="_blank">00:17:49.620</a></span> | <span class="t">So in fact, you'll see that the gradient flowing through 10h can only ever decrease,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1074" target="_blank">00:17:54.680</a></span> | <span class="t">and the amount that it decreases is proportional through a square here, depending on how far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1082" target="_blank">00:18:02.120</a></span> | <span class="t">you are in the flat tails of this 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1085" target="_blank">00:18:05.200</a></span> | <span class="t">And so that's kind of what's happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1087" target="_blank">00:18:07.440</a></span> | <span class="t">And the concern here is that if all of these outputs h are in the flat regions of negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1094" target="_blank">00:18:14.040</a></span> | <span class="t">1 and 1, then the gradients that are flowing through the network will just get destroyed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1098" target="_blank">00:18:18.960</a></span> | <span class="t">at this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1101" target="_blank">00:18:21.200</a></span> | <span class="t">Now there is some redeeming quality here, and that we can actually get a sense of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1105" target="_blank">00:18:25.520</a></span> | <span class="t">problem here as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1106" target="_blank">00:18:26.920</a></span> | <span class="t">I wrote some code here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1109" target="_blank">00:18:29.400</a></span> | <span class="t">Basically what we want to do here is we want to take a look at h, take the absolute value,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1114" target="_blank">00:18:34.920</a></span> | <span class="t">and see how often it is in the flat region, so say greater than 0.99.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1122" target="_blank">00:18:42.520</a></span> | <span class="t">And what you get is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1124" target="_blank">00:18:44.500</a></span> | <span class="t">And this is a Boolean tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1125" target="_blank">00:18:45.960</a></span> | <span class="t">So in the Boolean tensor, you get a white if this is true and a black if this is false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1132" target="_blank">00:18:52.640</a></span> | <span class="t">And so basically what we have here is the 32 examples and the 200 hidden neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1137" target="_blank">00:18:57.560</a></span> | <span class="t">And we see that a lot of this is white.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1140" target="_blank">00:19:00.460</a></span> | <span class="t">And what that's telling us is that all these 10h neurons were very, very active, and they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1147" target="_blank">00:19:07.080</a></span> | <span class="t">in the flat tail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1149" target="_blank">00:19:09.160</a></span> | <span class="t">And so in all these cases, the backward gradient would get destroyed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1156" target="_blank">00:19:16.600</a></span> | <span class="t">Now we would be in a lot of trouble if for any one of these 200 neurons, if it was the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1162" target="_blank">00:19:22.960</a></span> | <span class="t">case that the entire column is white, because in that case, we have what's called a dead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1167" target="_blank">00:19:27.920</a></span> | <span class="t">neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1168" target="_blank">00:19:28.920</a></span> | <span class="t">And this could be a 10h neuron where the initialization of the weights and the biases could be such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1172" target="_blank">00:19:32.640</a></span> | <span class="t">that no single example ever activates this 10h in the sort of active part of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1180" target="_blank">00:19:40.040</a></span> | <span class="t">If all the examples land in the tail, then this neuron will never learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1184" target="_blank">00:19:44.800</a></span> | <span class="t">It is a dead neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1186" target="_blank">00:19:46.840</a></span> | <span class="t">And so just scrutinizing this and looking for columns of completely white, we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1192" target="_blank">00:19:52.800</a></span> | <span class="t">this is not the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1194" target="_blank">00:19:54.280</a></span> | <span class="t">So I don't see a single neuron that is all of white.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1199" target="_blank">00:19:59.520</a></span> | <span class="t">And so therefore, it is the case that for every one of these 10h neurons, we do have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1204" target="_blank">00:20:04.640</a></span> | <span class="t">some examples that activate them in the active part of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1209" target="_blank">00:20:09.080</a></span> | <span class="t">And so some gradients will flow through, and this neuron will learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1212" target="_blank">00:20:12.520</a></span> | <span class="t">And the neuron will change, and it will move, and it will do something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1216" target="_blank">00:20:16.520</a></span> | <span class="t">But you can sometimes get yourself in cases where you have dead neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1220" target="_blank">00:20:20.440</a></span> | <span class="t">And the way this manifests is that for a 10h neuron, this would be when no matter what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1225" target="_blank">00:20:25.600</a></span> | <span class="t">inputs you plug in from your data set, this 10h neuron always fires completely one or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1230" target="_blank">00:20:30.000</a></span> | <span class="t">completely negative one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1231" target="_blank">00:20:31.460</a></span> | <span class="t">And then it will just not learn, because all the gradients will be just zeroed out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1236" target="_blank">00:20:36.800</a></span> | <span class="t">This is true not just for 10h, but for a lot of other nonlinearities that people use in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1240" target="_blank">00:20:40.240</a></span> | <span class="t">neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1241" target="_blank">00:20:41.280</a></span> | <span class="t">So we certainly use 10h a lot, but sigmoid will have the exact same issue, because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1245" target="_blank">00:20:45.600</a></span> | <span class="t">is a squashing neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1247" target="_blank">00:20:47.640</a></span> | <span class="t">And so the same will be true for sigmoid, but basically the same will actually apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1255" target="_blank">00:20:55.920</a></span> | <span class="t">to sigmoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1257" target="_blank">00:20:57.160</a></span> | <span class="t">The same will also apply to a ReLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1259" target="_blank">00:20:59.300</a></span> | <span class="t">So ReLU has a completely flat region here below zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1263" target="_blank">00:21:03.580</a></span> | <span class="t">So if you have a ReLU neuron, then it is a pass-through if it is positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1268" target="_blank">00:21:08.760</a></span> | <span class="t">And if the pre-activation is negative, it will just shut it off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1272" target="_blank">00:21:12.840</a></span> | <span class="t">Since the region here is completely flat, then during backpropagation, this would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1278" target="_blank">00:21:18.120</a></span> | <span class="t">exactly zeroing out the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1281" target="_blank">00:21:21.160</a></span> | <span class="t">All of the gradient would be set exactly to zero instead of just a very, very small number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1284" target="_blank">00:21:24.560</a></span> | <span class="t">depending on how positive or negative t is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1288" target="_blank">00:21:28.660</a></span> | <span class="t">And so you can get, for example, a dead ReLU neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1291" target="_blank">00:21:31.680</a></span> | <span class="t">And a dead ReLU neuron would basically look like-- basically what it is is if a neuron</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1297" target="_blank">00:21:37.520</a></span> | <span class="t">with a ReLU nonlinearity never activates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1301" target="_blank">00:21:41.320</a></span> | <span class="t">So for any examples that you plug in in the dataset, it never turns on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1305" target="_blank">00:21:45.360</a></span> | <span class="t">It's always in this flat region.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1307" target="_blank">00:21:47.600</a></span> | <span class="t">Then this ReLU neuron is a dead neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1309" target="_blank">00:21:49.640</a></span> | <span class="t">Its weights and bias will never learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1312" target="_blank">00:21:52.160</a></span> | <span class="t">They will never get a gradient because the neuron never activated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1315" target="_blank">00:21:55.880</a></span> | <span class="t">And this can sometimes happen at initialization because the weights and the biases just make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1319" target="_blank">00:21:59.440</a></span> | <span class="t">it so that by chance some neurons are just forever dead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1323" target="_blank">00:22:03.000</a></span> | <span class="t">But it can also happen during optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1325" target="_blank">00:22:05.020</a></span> | <span class="t">If you have like a too high of a learning rate, for example, sometimes you have these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1328" target="_blank">00:22:08.280</a></span> | <span class="t">neurons that get too much of a gradient and they get knocked out of the data manifold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1333" target="_blank">00:22:13.820</a></span> | <span class="t">And what happens is that from then on, no example ever activates this neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1338" target="_blank">00:22:18.000</a></span> | <span class="t">So this neuron remains dead forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1339" target="_blank">00:22:19.640</a></span> | <span class="t">So it's kind of like a permanent brain damage in a mind of a network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1344" target="_blank">00:22:24.040</a></span> | <span class="t">And so sometimes what can happen is if your learning rate is very high, for example, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1347" target="_blank">00:22:27.520</a></span> | <span class="t">you have a neural net with ReLU neurons, you train the neural net and you get some last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1351" target="_blank">00:22:31.920</a></span> | <span class="t">loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1352" target="_blank">00:22:32.920</a></span> | <span class="t">And then actually what you do is you go through the entire training set and you forward your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1358" target="_blank">00:22:38.440</a></span> | <span class="t">examples and you can find neurons that never activate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1362" target="_blank">00:22:42.160</a></span> | <span class="t">They are dead neurons in your network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1364" target="_blank">00:22:44.200</a></span> | <span class="t">And so those neurons will never turn on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1366" target="_blank">00:22:46.580</a></span> | <span class="t">And usually what happens is that during training, these ReLU neurons are changing, moving, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1370" target="_blank">00:22:50.820</a></span> | <span class="t">And then because of a high gradient somewhere by chance, they get knocked off and then nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1375" target="_blank">00:22:55.300</a></span> | <span class="t">ever activates them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1376" target="_blank">00:22:56.500</a></span> | <span class="t">And from then on, they are just dead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1379" target="_blank">00:22:59.120</a></span> | <span class="t">So that's kind of like a permanent brain damage that can happen to some of these neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1383" target="_blank">00:23:03.320</a></span> | <span class="t">These other nonlinearities like Leaky ReLU will not suffer from this issue as much because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1387" target="_blank">00:23:07.560</a></span> | <span class="t">you can see that it doesn't have flat tails.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1390" target="_blank">00:23:10.720</a></span> | <span class="t">You'll almost always get gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1393" target="_blank">00:23:13.200</a></span> | <span class="t">And ELU is also fairly frequently used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1396" target="_blank">00:23:16.600</a></span> | <span class="t">It also might suffer from this issue because it has flat parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1400" target="_blank">00:23:20.400</a></span> | <span class="t">So that's just something to be aware of and something to be concerned about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1404" target="_blank">00:23:24.240</a></span> | <span class="t">And in this case, we have way too many activations h that take on extreme values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1410" target="_blank">00:23:30.640</a></span> | <span class="t">And because there's no column of white, I think we will be okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1414" target="_blank">00:23:34.440</a></span> | <span class="t">And indeed, the network optimizes and gives us a pretty decent loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1417" target="_blank">00:23:37.740</a></span> | <span class="t">But it's just not optimal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1419" target="_blank">00:23:39.040</a></span> | <span class="t">And this is not something you want, especially during initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1422" target="_blank">00:23:42.400</a></span> | <span class="t">And so basically what's happening is that this h pre-activation that's flowing to 10h,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1429" target="_blank">00:23:49.080</a></span> | <span class="t">it's too extreme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1430" target="_blank">00:23:50.080</a></span> | <span class="t">It's too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1431" target="_blank">00:23:51.080</a></span> | <span class="t">It's creating a distribution that is too saturated in both sides of the 10h.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1437" target="_blank">00:23:57.400</a></span> | <span class="t">And it's not something you want because it means that there's less training for these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1441" target="_blank">00:24:01.600</a></span> | <span class="t">neurons because they update less frequently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1445" target="_blank">00:24:05.820</a></span> | <span class="t">So how do we fix this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1446" target="_blank">00:24:06.820</a></span> | <span class="t">Well, h pre-activation is mcat, which comes from c.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1452" target="_blank">00:24:12.660</a></span> | <span class="t">So these are uniform Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1455" target="_blank">00:24:15.040</a></span> | <span class="t">But then it's multiplied by w1 plus b1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1457" target="_blank">00:24:17.680</a></span> | <span class="t">And h pre-act is too far off from 0, and that's causing the issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1461" target="_blank">00:24:21.600</a></span> | <span class="t">So we want this pre-activation to be closer to 0, very similar to what we had with logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1467" target="_blank">00:24:27.440</a></span> | <span class="t">So here, we want actually something very, very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1471" target="_blank">00:24:31.480</a></span> | <span class="t">Now it's okay to set the biases to a very small number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1475" target="_blank">00:24:35.080</a></span> | <span class="t">We can either multiply by 001 to get a little bit of entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1479" target="_blank">00:24:39.540</a></span> | <span class="t">I sometimes like to do that just so that there's a little bit of variation and diversity in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1485" target="_blank">00:24:45.920</a></span> | <span class="t">the original initialization of these 10h neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1489" target="_blank">00:24:49.600</a></span> | <span class="t">And I find in practice that that can help optimization a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1494" target="_blank">00:24:54.000</a></span> | <span class="t">And then the weights, we can also just squash.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1496" target="_blank">00:24:56.360</a></span> | <span class="t">So let's multiply everything by 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1499" target="_blank">00:24:59.400</a></span> | <span class="t">Let's rerun the first batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1501" target="_blank">00:25:01.680</a></span> | <span class="t">And now let's look at this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1503" target="_blank">00:25:03.360</a></span> | <span class="t">And well, first let's look at here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1507" target="_blank">00:25:07.120</a></span> | <span class="t">You see now, because we multiplied w by 0.1, we have a much better histogram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1511" target="_blank">00:25:11.280</a></span> | <span class="t">And that's because the pre-activations are now between -1.5 and 1.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1515" target="_blank">00:25:15.160</a></span> | <span class="t">And this, we expect much, much less white.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1517" target="_blank">00:25:17.520</a></span> | <span class="t">Okay, there's no white.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1521" target="_blank">00:25:21.000</a></span> | <span class="t">So basically, that's because there are no neurons that saturated above 0.99 in either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1526" target="_blank">00:25:26.760</a></span> | <span class="t">direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1527" target="_blank">00:25:27.760</a></span> | <span class="t">So it's actually a pretty decent place to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1532" target="_blank">00:25:32.200</a></span> | <span class="t">Maybe we can go up a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1535" target="_blank">00:25:35.160</a></span> | <span class="t">Sorry, am I changing w1 here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1538" target="_blank">00:25:38.880</a></span> | <span class="t">So maybe we can go to 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1542" target="_blank">00:25:42.200</a></span> | <span class="t">Okay, so maybe something like this is a nice distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1546" target="_blank">00:25:46.560</a></span> | <span class="t">So maybe this is what our initialization should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1549" target="_blank">00:25:49.280</a></span> | <span class="t">So let me now erase these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1553" target="_blank">00:25:53.640</a></span> | <span class="t">And let me, starting with initialization, let me run the full optimization without the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1559" target="_blank">00:25:59.360</a></span> | <span class="t">break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1560" target="_blank">00:26:00.720</a></span> | <span class="t">And let's see what we get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1562" target="_blank">00:26:02.840</a></span> | <span class="t">Okay, so the optimization finished.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1564" target="_blank">00:26:04.920</a></span> | <span class="t">And I rerun the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1566" target="_blank">00:26:06.300</a></span> | <span class="t">And this is the result that we get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1568" target="_blank">00:26:08.200</a></span> | <span class="t">And then just as a reminder, I put down all the losses that we saw previously in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1571" target="_blank">00:26:11.120</a></span> | <span class="t">lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1572" target="_blank">00:26:12.560</a></span> | <span class="t">So we see that we actually do get an improvement here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1575" target="_blank">00:26:15.280</a></span> | <span class="t">And just as a reminder, we started off with a validation loss of 2.17 when we started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1580" target="_blank">00:26:20.160</a></span> | <span class="t">By fixing the softmax being confidently wrong, we came down to 2.13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1584" target="_blank">00:26:24.200</a></span> | <span class="t">And by fixing the 10H layer being way too saturated, we came down to 2.10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1589" target="_blank">00:26:29.000</a></span> | <span class="t">And the reason this is happening, of course, is because our initialization is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1592" target="_blank">00:26:32.040</a></span> | <span class="t">And so we're spending more time doing productive training instead of not very productive training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1597" target="_blank">00:26:37.920</a></span> | <span class="t">because our gradients are set to zero, and we have to learn very simple things like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1603" target="_blank">00:26:43.400</a></span> | <span class="t">overconfidence of the softmax in the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1605" target="_blank">00:26:45.600</a></span> | <span class="t">And we're spending cycles just like squashing down the weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1609" target="_blank">00:26:49.120</a></span> | <span class="t">So this is illustrating basically initialization and its impact on performance just by being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1616" target="_blank">00:26:56.360</a></span> | <span class="t">aware of the internals of these neural nets and their activations and their gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1620" target="_blank">00:27:00.640</a></span> | <span class="t">Now we're working with a very small network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1623" target="_blank">00:27:03.000</a></span> | <span class="t">This is just one layer multilayer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1625" target="_blank">00:27:05.640</a></span> | <span class="t">So because the network is so shallow, the optimization problem is actually quite easy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1629" target="_blank">00:27:09.920</a></span> | <span class="t">and very forgiving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1631" target="_blank">00:27:11.600</a></span> | <span class="t">So even though our initialization was terrible, the network still learned eventually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1635" target="_blank">00:27:15.520</a></span> | <span class="t">It just got a bit worse result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1637" target="_blank">00:27:17.480</a></span> | <span class="t">This is not the case in general, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1639" target="_blank">00:27:19.560</a></span> | <span class="t">Once we actually start working with much deeper networks that have, say, 50 layers, things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1644" target="_blank">00:27:24.880</a></span> | <span class="t">can get much more complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1647" target="_blank">00:27:27.400</a></span> | <span class="t">And these problems stack up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1650" target="_blank">00:27:30.600</a></span> | <span class="t">And so you can actually get into a place where the network is basically not training at all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1655" target="_blank">00:27:35.080</a></span> | <span class="t">if your initialization is bad enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1657" target="_blank">00:27:37.520</a></span> | <span class="t">And the deeper your network is and the more complex it is, the less forgiving it is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1661" target="_blank">00:27:41.600</a></span> | <span class="t">some of these errors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1663" target="_blank">00:27:43.200</a></span> | <span class="t">And so something to definitely be aware of and something to scrutinize, something to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1668" target="_blank">00:27:48.760</a></span> | <span class="t">plot and something to be careful with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1671" target="_blank">00:27:51.400</a></span> | <span class="t">And yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1672" target="_blank">00:27:52.400</a></span> | <span class="t">Okay, so that's great that that worked for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1675" target="_blank">00:27:55.860</a></span> | <span class="t">But what we have here now is all these magic numbers like 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1679" target="_blank">00:27:59.200</a></span> | <span class="t">Like where do I come up with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1680" target="_blank">00:28:00.800</a></span> | <span class="t">And how am I supposed to set these if I have a large neural network with lots and lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1683" target="_blank">00:28:03.840</a></span> | <span class="t">of layers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1685" target="_blank">00:28:05.440</a></span> | <span class="t">And so obviously no one does this by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1687" target="_blank">00:28:07.660</a></span> | <span class="t">There's actually some relatively principled ways of setting these scales that I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1692" target="_blank">00:28:12.200</a></span> | <span class="t">like to introduce to you now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1694" target="_blank">00:28:14.160</a></span> | <span class="t">So let me paste some code here that I prepared just to motivate the discussion of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1699" target="_blank">00:28:19.660</a></span> | <span class="t">So what I'm doing here is we have some random input here, X, that is drawn from a Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1705" target="_blank">00:28:25.520</a></span> | <span class="t">And there's 1,000 examples that are 10-dimensional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1708" target="_blank">00:28:28.920</a></span> | <span class="t">And then we have a weighting layer here that is also initialized using Gaussian, just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1713" target="_blank">00:28:33.360</a></span> | <span class="t">we did here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1715" target="_blank">00:28:35.000</a></span> | <span class="t">And these neurons in the hidden layer look at 10 inputs, and there are 200 neurons in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1720" target="_blank">00:28:40.280</a></span> | <span class="t">this hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1721" target="_blank">00:28:41.800</a></span> | <span class="t">And then we have here, just like here in this case, the multiplication, X multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1726" target="_blank">00:28:46.760</a></span> | <span class="t">W to get the pre-activations of these neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1731" target="_blank">00:28:51.000</a></span> | <span class="t">And basically the analysis here looks at, okay, suppose these are uniform Gaussian and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1735" target="_blank">00:28:55.560</a></span> | <span class="t">these weights are uniform Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1737" target="_blank">00:28:57.300</a></span> | <span class="t">If I do X times W, and we forget for now the bias and the nonlinearity, then what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1744" target="_blank">00:29:04.040</a></span> | <span class="t">mean and the standard deviation of these Gaussians?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1747" target="_blank">00:29:07.080</a></span> | <span class="t">So in the beginning here, the input is just a normal Gaussian distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1751" target="_blank">00:29:11.160</a></span> | <span class="t">Mean is zero, and the standard deviation is one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1753" target="_blank">00:29:13.720</a></span> | <span class="t">And the standard deviation, again, is just the measure of a spread of this Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1758" target="_blank">00:29:18.680</a></span> | <span class="t">But then once we multiply here and we look at the histogram of Y, we see that the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1764" target="_blank">00:29:24.360</a></span> | <span class="t">of course, stays the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1765" target="_blank">00:29:25.800</a></span> | <span class="t">It's about zero, because this is a symmetric operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1768" target="_blank">00:29:28.980</a></span> | <span class="t">But we see here that the standard deviation has expanded to three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1772" target="_blank">00:29:32.680</a></span> | <span class="t">So the input standard deviation was one, but now we've grown to three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1775" target="_blank">00:29:35.840</a></span> | <span class="t">And so what you're seeing in the histogram is that this Gaussian is expanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1781" target="_blank">00:29:41.240</a></span> | <span class="t">And so we're expanding this Gaussian from the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1785" target="_blank">00:29:45.920</a></span> | <span class="t">And we don't want that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1786" target="_blank">00:29:46.920</a></span> | <span class="t">We want most of the neural net to have relatively similar activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1790" target="_blank">00:29:50.680</a></span> | <span class="t">So unit Gaussian, roughly, throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1793" target="_blank">00:29:53.480</a></span> | <span class="t">And so the question is, how do we scale these Ws to preserve this distribution to remain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1802" target="_blank">00:30:02.160</a></span> | <span class="t">a Gaussian?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1803" target="_blank">00:30:03.900</a></span> | <span class="t">And so intuitively, if I multiply here these elements of W by a larger number, let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1809" target="_blank">00:30:09.680</a></span> | <span class="t">by five, then this Gaussian grows and grows in standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1816" target="_blank">00:30:16.040</a></span> | <span class="t">So now we're at 15.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1817" target="_blank">00:30:17.440</a></span> | <span class="t">So basically, these numbers here in the output, Y, take on more and more extreme values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1822" target="_blank">00:30:22.680</a></span> | <span class="t">But if we scale it down, let's say 0.2, then conversely, this Gaussian is getting smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1828" target="_blank">00:30:28.800</a></span> | <span class="t">and smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1829" target="_blank">00:30:29.800</a></span> | <span class="t">And it's shrinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1830" target="_blank">00:30:30.800</a></span> | <span class="t">And you can see that the standard deviation is 0.6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1834" target="_blank">00:30:34.080</a></span> | <span class="t">And so the question is, what do I multiply by here to exactly preserve the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1839" target="_blank">00:30:39.440</a></span> | <span class="t">to be one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1841" target="_blank">00:30:41.160</a></span> | <span class="t">And it turns out that the correct answer mathematically, when you work out through the variance of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1845" target="_blank">00:30:45.880</a></span> | <span class="t">multiplication here, is that you are supposed to divide by the square root of the fan in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1852" target="_blank">00:30:52.840</a></span> | <span class="t">The fan in is basically the number of input elements here, 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1858" target="_blank">00:30:58.180</a></span> | <span class="t">So we are supposed to divide by 10 square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1861" target="_blank">00:31:01.000</a></span> | <span class="t">And this is one way to do the square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1862" target="_blank">00:31:02.560</a></span> | <span class="t">You raise it to a power of 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1864" target="_blank">00:31:04.600</a></span> | <span class="t">That's the same as doing a square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1867" target="_blank">00:31:07.300</a></span> | <span class="t">So when you divide by the square root of 10, then we see that the output Gaussian, it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1874" target="_blank">00:31:14.500</a></span> | <span class="t">exactly standard deviation of 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1877" target="_blank">00:31:17.560</a></span> | <span class="t">Now unsurprisingly, a number of papers have looked into how to best initialize neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1882" target="_blank">00:31:22.360</a></span> | <span class="t">networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1883" target="_blank">00:31:23.640</a></span> | <span class="t">And in the case of multilayer perceptrons, we can have fairly deep networks that have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1887" target="_blank">00:31:27.080</a></span> | <span class="t">these nonlinearities in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1889" target="_blank">00:31:29.400</a></span> | <span class="t">And we want to make sure that the activations are well-behaved and they don't expand to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1892" target="_blank">00:31:32.880</a></span> | <span class="t">infinity or shrink all the way to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1895" target="_blank">00:31:35.420</a></span> | <span class="t">And the question is, how do we initialize the weights so that these activations take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1898" target="_blank">00:31:38.280</a></span> | <span class="t">on reasonable values throughout the network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1901" target="_blank">00:31:41.040</a></span> | <span class="t">Now one paper that has studied this in quite a bit of detail that is often referenced is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1905" target="_blank">00:31:45.100</a></span> | <span class="t">this paper by Kaiming et al. called Delving Deep Interactive Fires.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1909" target="_blank">00:31:49.440</a></span> | <span class="t">Now in this case, they actually studied convolutional neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1912" target="_blank">00:31:52.520</a></span> | <span class="t">And they studied especially the ReLU nonlinearity and the pReLU nonlinearity instead of a 10H</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1919" target="_blank">00:31:59.440</a></span> | <span class="t">nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1920" target="_blank">00:32:00.440</a></span> | <span class="t">But the analysis is very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1921" target="_blank">00:32:01.940</a></span> | <span class="t">And basically what happens here is for them, the ReLU nonlinearity that they care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1927" target="_blank">00:32:07.980</a></span> | <span class="t">quite a bit here is a squashing function where all the negative numbers are simply clamped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1934" target="_blank">00:32:14.740</a></span> | <span class="t">to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1936" target="_blank">00:32:16.000</a></span> | <span class="t">So the positive numbers are a path through, but everything negative is just set to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1940" target="_blank">00:32:20.740</a></span> | <span class="t">And because you're basically throwing away half of the distribution, they find in their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1945" target="_blank">00:32:25.300</a></span> | <span class="t">analysis of the forward activations in the neural net that you have to compensate for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1949" target="_blank">00:32:29.500</a></span> | <span class="t">that with a gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1952" target="_blank">00:32:32.220</a></span> | <span class="t">And so here, they find that basically when they initialize their weights, they have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1957" target="_blank">00:32:37.340</a></span> | <span class="t">do it with a zero-mean Gaussian whose standard deviation is square root of 2 over the Fannin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1963" target="_blank">00:32:43.540</a></span> | <span class="t">What we have here is we are initializing the Gaussian with the square root of Fannin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1969" target="_blank">00:32:49.140</a></span> | <span class="t">This NL here is the Fannin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1970" target="_blank">00:32:50.700</a></span> | <span class="t">So what we have is square root of 1 over the Fannin because we have the division here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1978" target="_blank">00:32:58.200</a></span> | <span class="t">Now they have to add this factor of 2 because of the ReLU, which basically discards half</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1982" target="_blank">00:33:02.860</a></span> | <span class="t">of the distribution and clamps it at 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1985" target="_blank">00:33:05.580</a></span> | <span class="t">And so that's where you get an initial factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1988" target="_blank">00:33:08.060</a></span> | <span class="t">Now in addition to that, this paper also studies not just the behavior of the activations in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1993" target="_blank">00:33:13.540</a></span> | <span class="t">the forward pass of the neural net, but it also studies the backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=1997" target="_blank">00:33:17.860</a></span> | <span class="t">And we have to make sure that the gradients also are well-behaved because ultimately,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2003" target="_blank">00:33:23.580</a></span> | <span class="t">they end up updating our parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2005" target="_blank">00:33:25.900</a></span> | <span class="t">And what they find here through a lot of the analysis that I invite you to read through,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2009" target="_blank">00:33:29.620</a></span> | <span class="t">but it's not exactly approachable, what they find is basically if you properly initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2015" target="_blank">00:33:35.140</a></span> | <span class="t">the forward pass, the backward pass is also approximately initialized up to a constant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2020" target="_blank">00:33:40.380</a></span> | <span class="t">factor that has to do with the size of the number of hidden neurons in an early and late</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2026" target="_blank">00:33:46.980</a></span> | <span class="t">layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2029" target="_blank">00:33:49.940</a></span> | <span class="t">But basically they find empirically that this is not a choice that matters too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2034" target="_blank">00:33:54.100</a></span> | <span class="t">Now this kyming initialization is also implemented in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2038" target="_blank">00:33:58.160</a></span> | <span class="t">So if you go to torch.nn.init documentation, you'll find kyming normal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2042" target="_blank">00:34:02.620</a></span> | <span class="t">And in my opinion, this is probably the most common way of initializing neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2046" target="_blank">00:34:06.140</a></span> | <span class="t">now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2047" target="_blank">00:34:07.140</a></span> | <span class="t">And it takes a few keyword arguments here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2049" target="_blank">00:34:09.900</a></span> | <span class="t">So number one, it wants to know the mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2053" target="_blank">00:34:13.020</a></span> | <span class="t">Would you like to normalize the activations or would you like to normalize the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2057" target="_blank">00:34:17.220</a></span> | <span class="t">to be always Gaussian with zero mean and a unit or one standard deviation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2062" target="_blank">00:34:22.820</a></span> | <span class="t">And because they find in the paper that this doesn't matter too much, most of the people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2065" target="_blank">00:34:25.680</a></span> | <span class="t">just leave it as the default, which is fan-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2068" target="_blank">00:34:28.380</a></span> | <span class="t">And then second, passing the nonlinearity that you are using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2071" target="_blank">00:34:31.600</a></span> | <span class="t">Because depending on the nonlinearity, we need to calculate a slightly different gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2076" target="_blank">00:34:36.060</a></span> | <span class="t">And so if your nonlinearity is just linear, so there's no nonlinearity, then the gain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2081" target="_blank">00:34:41.140</a></span> | <span class="t">here will be one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2082" target="_blank">00:34:42.140</a></span> | <span class="t">And we have the exact same kind of formula that we've got up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2086" target="_blank">00:34:46.420</a></span> | <span class="t">But if the nonlinearity is something else, we're going to get a slightly different gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2089" target="_blank">00:34:49.920</a></span> | <span class="t">And so if we come up here to the top, we see that, for example, in the case of ReLU, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2094" target="_blank">00:34:54.420</a></span> | <span class="t">gain is a square root of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2096" target="_blank">00:34:56.420</a></span> | <span class="t">And the reason it's a square root is because in this paper, you see how the two is inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2105" target="_blank">00:35:05.060</a></span> | <span class="t">of the square root, so the gain is a square root of two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2109" target="_blank">00:35:09.120</a></span> | <span class="t">In the case of linear or identity, we just get a gain of one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2113" target="_blank">00:35:13.860</a></span> | <span class="t">In the case of 10H, which is what we're using here, the advised gain is a five over three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2119" target="_blank">00:35:19.000</a></span> | <span class="t">And intuitively, why do we need a gain on top of the initialization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2122" target="_blank">00:35:22.720</a></span> | <span class="t">It's because 10H, just like ReLU, is a contractive transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2127" target="_blank">00:35:27.520</a></span> | <span class="t">So what that means is you're taking the output distribution from this matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2131" target="_blank">00:35:31.740</a></span> | <span class="t">and then you are squashing it in some way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2133" target="_blank">00:35:33.720</a></span> | <span class="t">Now ReLU squashes it by taking everything below zero and clamping it to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2137" target="_blank">00:35:37.560</a></span> | <span class="t">10H also squashes it because it's a contractive operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2140" target="_blank">00:35:40.360</a></span> | <span class="t">It will take the tails and it will squeeze them in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2144" target="_blank">00:35:44.360</a></span> | <span class="t">And so in order to fight the squeezing in, we need to boost the weights a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2148" target="_blank">00:35:48.940</a></span> | <span class="t">so that we renormalize everything back to unit standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2153" target="_blank">00:35:53.520</a></span> | <span class="t">So that's why there's a little bit of a gain that comes out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2156" target="_blank">00:35:56.640</a></span> | <span class="t">Now I'm skipping through this section a little bit quickly, and I'm doing that actually intentionally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2161" target="_blank">00:36:01.060</a></span> | <span class="t">And the reason for that is because about seven years ago when this paper was written, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2166" target="_blank">00:36:06.280</a></span> | <span class="t">had to actually be extremely careful with the activations and the gradients and their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2169" target="_blank">00:36:09.820</a></span> | <span class="t">ranges and their histograms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2171" target="_blank">00:36:11.860</a></span> | <span class="t">And you had to be very careful with the precise setting of gains and the scrutinizing of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2175" target="_blank">00:36:15.160</a></span> | <span class="t">nonlinearities used and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2177" target="_blank">00:36:17.260</a></span> | <span class="t">And everything was very finicky and very fragile and to be very properly arranged for the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2181" target="_blank">00:36:21.680</a></span> | <span class="t">net to train, especially if your neural net was very deep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2184" target="_blank">00:36:24.960</a></span> | <span class="t">But there are a number of modern innovations that have made everything significantly more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2187" target="_blank">00:36:27.760</a></span> | <span class="t">stable and more well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2189" target="_blank">00:36:29.720</a></span> | <span class="t">And it's become less important to initialize these networks exactly right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2194" target="_blank">00:36:34.120</a></span> | <span class="t">And some of those modern innovations, for example, are residual connections, which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2197" target="_blank">00:36:37.600</a></span> | <span class="t">will cover in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2199" target="_blank">00:36:39.200</a></span> | <span class="t">The use of a number of normalization layers, like for example, batch normalization, layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2205" target="_blank">00:36:45.080</a></span> | <span class="t">normalization, group normalization, we're going to go into a lot of these as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2209" target="_blank">00:36:49.080</a></span> | <span class="t">And number three, much better optimizers, not just stochastic gradient descent, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2212" target="_blank">00:36:52.480</a></span> | <span class="t">simple optimizer we're basically using here, but slightly more complex optimizers like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2217" target="_blank">00:36:57.360</a></span> | <span class="t">RMSProp and especially Adam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2219" target="_blank">00:36:59.760</a></span> | <span class="t">And so all of these modern innovations make it less important for you to precisely calibrate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2223" target="_blank">00:37:03.960</a></span> | <span class="t">the initialization of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2226" target="_blank">00:37:06.180</a></span> | <span class="t">All that being said, in practice, what should we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2229" target="_blank">00:37:09.740</a></span> | <span class="t">In practice, when I initialize these neural nets, I basically just normalize my weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2233" target="_blank">00:37:13.720</a></span> | <span class="t">by the square root of the fan-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2236" target="_blank">00:37:16.000</a></span> | <span class="t">So basically, roughly what we did here is what I do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2240" target="_blank">00:37:20.320</a></span> | <span class="t">Now, if we want to be exactly accurate here, and go back in it of timing normal, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2248" target="_blank">00:37:28.000</a></span> | <span class="t">how we would implement it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2249" target="_blank">00:37:29.720</a></span> | <span class="t">We want to set the standard deviation to be gain over the square root of fan-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2255" target="_blank">00:37:35.500</a></span> | <span class="t">So to set the standard deviation of our weights, we will proceed as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2261" target="_blank">00:37:41.560</a></span> | <span class="t">Basically when we have a torsion type random, and let's say I just create a thousand numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2266" target="_blank">00:37:46.000</a></span> | <span class="t">we can look at the standard deviation of this, and of course that's one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2268" target="_blank">00:37:48.840</a></span> | <span class="t">That's the amount of spread.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2269" target="_blank">00:37:49.840</a></span> | <span class="t">Let's make this a bit bigger so it's closer to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2272" target="_blank">00:37:52.480</a></span> | <span class="t">So that's the spread of the Gaussian of zero mean and unit standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2278" target="_blank">00:37:58.120</a></span> | <span class="t">Now basically when you take these and you multiply by say 0.2, that basically scales</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2283" target="_blank">00:38:03.320</a></span> | <span class="t">down the Gaussian and that makes its standard deviation 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2287" target="_blank">00:38:07.120</a></span> | <span class="t">So basically the number that you multiply by here ends up being the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2290" target="_blank">00:38:10.320</a></span> | <span class="t">of this Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2292" target="_blank">00:38:12.280</a></span> | <span class="t">So here this is a standard deviation 0.2 Gaussian here when we sample RW1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2299" target="_blank">00:38:19.400</a></span> | <span class="t">But we want to set the standard deviation to gain over square root of fan-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2306" target="_blank">00:38:26.160</a></span> | <span class="t">So in other words, we want to multiply by gain, which for 10H is 5/3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2312" target="_blank">00:38:32.400</a></span> | <span class="t">5/3 is the gain, and then times, or I guess sorry, divide square root of the fan-in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2331" target="_blank">00:38:51.720</a></span> | <span class="t">In this example here the fan-in was 10, and I just noticed that actually here the fan-in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2336" target="_blank">00:38:56.200</a></span> | <span class="t">for W1 is actually an embed times block size, which as you will recall is actually 30.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2341" target="_blank">00:39:01.960</a></span> | <span class="t">And that's because each character is 10-dimensional, but then we have three of them and we concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2345" target="_blank">00:39:05.480</a></span> | <span class="t">them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2346" target="_blank">00:39:06.480</a></span> | <span class="t">So actually the fan-in here was 30, and I should have used 30 here probably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2350" target="_blank">00:39:10.240</a></span> | <span class="t">But basically we want 30 square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2353" target="_blank">00:39:13.320</a></span> | <span class="t">So this is the number, this is what our standard deviation we want to be, and this number turns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2357" target="_blank">00:39:17.680</a></span> | <span class="t">out to be 0.3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2359" target="_blank">00:39:19.680</a></span> | <span class="t">Whereas here just by fiddling with it and looking at the distribution and making sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2362" target="_blank">00:39:22.880</a></span> | <span class="t">it looks okay, we came up with 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2366" target="_blank">00:39:26.040</a></span> | <span class="t">And so instead what we want to do here is we want to make the standard deviation be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2369" target="_blank">00:39:29.960</a></span> | <span class="t">5/3, which is our gain, divide this amount times 0.2 square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2381" target="_blank">00:39:41.400</a></span> | <span class="t">And these brackets here are not that necessary, but I'll just put them here for clarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2386" target="_blank">00:39:46.220</a></span> | <span class="t">This is basically what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2387" target="_blank">00:39:47.580</a></span> | <span class="t">This is the kyming init in our case for 10H nonlinearity, and this is how we would initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2393" target="_blank">00:39:53.400</a></span> | <span class="t">the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2394" target="_blank">00:39:54.880</a></span> | <span class="t">And so we're multiplying by 0.3 instead of multiplying by 0.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2401" target="_blank">00:40:01.120</a></span> | <span class="t">And so we can initialize this way, and then we can train the neural net and see what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2407" target="_blank">00:40:07.160</a></span> | <span class="t">get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2408" target="_blank">00:40:08.160</a></span> | <span class="t">Okay, so I trained the neural net and we end up in roughly the same spot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2412" target="_blank">00:40:12.340</a></span> | <span class="t">So looking at the validation loss, we now get 2.10, and previously we also had 2.10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2417" target="_blank">00:40:17.680</a></span> | <span class="t">There's a little bit of a difference, but that's just the randomness of the process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2420" target="_blank">00:40:20.240</a></span> | <span class="t">I suspect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2421" target="_blank">00:40:21.600</a></span> | <span class="t">But the big deal, of course, is we get to the same spot, but we did not have to introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2426" target="_blank">00:40:26.080</a></span> | <span class="t">any magic numbers that we got from just looking at histograms and guessing, checking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2432" target="_blank">00:40:32.540</a></span> | <span class="t">We have something that is semi-principled and will scale us to much bigger networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2437" target="_blank">00:40:37.080</a></span> | <span class="t">and something that we can sort of use as a guide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2440" target="_blank">00:40:40.260</a></span> | <span class="t">So I mentioned that the precise setting of these initializations is not as important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2444" target="_blank">00:40:44.160</a></span> | <span class="t">today due to some modern innovations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2446" target="_blank">00:40:46.080</a></span> | <span class="t">And I think now is a pretty good time to introduce one of those modern innovations, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2449" target="_blank">00:40:49.520</a></span> | <span class="t">is batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2451" target="_blank">00:40:51.320</a></span> | <span class="t">So batch normalization came out in 2015 from a team at Google, and it was an extremely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2457" target="_blank">00:40:57.000</a></span> | <span class="t">impactful paper because it made it possible to train very deep neural nets quite reliably,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2463" target="_blank">00:41:03.080</a></span> | <span class="t">and it basically just worked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2465" target="_blank">00:41:05.240</a></span> | <span class="t">So here's what batch normalization does and what's implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2470" target="_blank">00:41:10.320</a></span> | <span class="t">Basically we have these hidden states, H_preact, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2473" target="_blank">00:41:13.840</a></span> | <span class="t">And we were talking about how we don't want these preactivation states to be way too small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2480" target="_blank">00:41:20.400</a></span> | <span class="t">because then the tanh is not doing anything, but we don't want them to be too large because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2485" target="_blank">00:41:25.320</a></span> | <span class="t">then the tanh is saturated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2487" target="_blank">00:41:27.640</a></span> | <span class="t">In fact, we want them to be roughly Gaussian, so zero mean and a unit or one standard deviation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2494" target="_blank">00:41:34.160</a></span> | <span class="t">at least at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2496" target="_blank">00:41:36.200</a></span> | <span class="t">So the insight from the batch normalization paper is, okay, you have these hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2501" target="_blank">00:41:41.160</a></span> | <span class="t">and you'd like them to be roughly Gaussian, then why not take the hidden states and just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2506" target="_blank">00:41:46.440</a></span> | <span class="t">normalize them to be Gaussian?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2509" target="_blank">00:41:49.000</a></span> | <span class="t">And it sounds kind of crazy, but you can just do that because standardizing hidden states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2515" target="_blank">00:41:55.280</a></span> | <span class="t">so that they're unit Gaussian is a perfectly differentiable operation, as we'll soon see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2519" target="_blank">00:41:59.760</a></span> | <span class="t">And so that was kind of like the big insight in this paper, and when I first read it, my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2523" target="_blank">00:42:03.280</a></span> | <span class="t">mind was blown because you can just normalize these hidden states, and if you'd like unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2527" target="_blank">00:42:07.480</a></span> | <span class="t">Gaussian states in your network, at least initialization, you can just normalize them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2532" target="_blank">00:42:12.680</a></span> | <span class="t">to be unit Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2534" target="_blank">00:42:14.360</a></span> | <span class="t">So let's see how that works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2536" target="_blank">00:42:16.600</a></span> | <span class="t">So we're going to scroll to our pre-activations here just before they enter into the tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2541" target="_blank">00:42:21.560</a></span> | <span class="t">Now the idea again is, remember, we're trying to make these roughly Gaussian, and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2545" target="_blank">00:42:25.240</a></span> | <span class="t">because if these are way too small numbers, then the tanh here is kind of inactive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2550" target="_blank">00:42:30.560</a></span> | <span class="t">But if these are very large numbers, then the tanh is way too saturated and gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2555" target="_blank">00:42:35.360</a></span> | <span class="t">is no flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2556" target="_blank">00:42:36.600</a></span> | <span class="t">So we'd like this to be roughly Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2559" target="_blank">00:42:39.280</a></span> | <span class="t">So the insight in batch normalization again is that we can just standardize these activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2564" target="_blank">00:42:44.280</a></span> | <span class="t">so they are exactly Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2567" target="_blank">00:42:47.000</a></span> | <span class="t">So here, H_preact has a shape of 32 by 200, 32 examples by 200 neurons in the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2576" target="_blank">00:42:56.120</a></span> | <span class="t">So basically what we can do is we can take H_preact and we can just calculate the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2581" target="_blank">00:43:01.480</a></span> | <span class="t">and the mean we want to calculate across the 0th dimension, and we want to also keep the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2586" target="_blank">00:43:06.800</a></span> | <span class="t">missed true so that we can easily broadcast this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2591" target="_blank">00:43:11.760</a></span> | <span class="t">So the shape of this is 1 by 200.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2594" target="_blank">00:43:14.880</a></span> | <span class="t">In other words, we are doing the mean over all the elements in the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2601" target="_blank">00:43:21.040</a></span> | <span class="t">And similarly, we can calculate the standard deviation of these activations, and that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2607" target="_blank">00:43:27.280</a></span> | <span class="t">also be 1 by 200.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2609" target="_blank">00:43:29.560</a></span> | <span class="t">Now in this paper, they have the sort of prescription here, and see here we are calculating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2616" target="_blank">00:43:36.280</a></span> | <span class="t">mean, which is just taking the average value of any neuron's activation, and then the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2624" target="_blank">00:43:44.480</a></span> | <span class="t">deviation is basically kind of like the measure of the spread that we've been using, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2630" target="_blank">00:43:50.360</a></span> | <span class="t">is the distance of every one of these values away from the mean, and that squared and averaged.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2640" target="_blank">00:44:00.320</a></span> | <span class="t">That's the variance, and then if you want to take the standard deviation, you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2643" target="_blank">00:44:03.680</a></span> | <span class="t">square root the variance to get the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2647" target="_blank">00:44:07.880</a></span> | <span class="t">So these are the two that we're calculating, and now we're going to normalize or standardize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2652" target="_blank">00:44:12.640</a></span> | <span class="t">these x's by subtracting the mean and dividing by the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2657" target="_blank">00:44:17.980</a></span> | <span class="t">So basically, we're taking H_preact and we subtract the mean, and then we divide by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2671" target="_blank">00:44:31.600</a></span> | <span class="t">standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2674" target="_blank">00:44:34.520</a></span> | <span class="t">This is exactly what these two, std and mean, are calculating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2681" target="_blank">00:44:41.080</a></span> | <span class="t">This is the mean and this is the variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2683" target="_blank">00:44:43.160</a></span> | <span class="t">You see how the sigma is the standard deviation usually, so this is sigma squared, which the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2687" target="_blank">00:44:47.040</a></span> | <span class="t">variance is the square of the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2691" target="_blank">00:44:51.040</a></span> | <span class="t">So this is how you standardize these values, and what this will do is that every single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2694" target="_blank">00:44:54.920</a></span> | <span class="t">neuron now, and its firing rate, will be exactly unit Gaussian on these 32 examples at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2700" target="_blank">00:45:00.920</a></span> | <span class="t">of this batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2701" target="_blank">00:45:01.920</a></span> | <span class="t">That's why it's called batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2703" target="_blank">00:45:03.400</a></span> | <span class="t">We are normalizing these batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2706" target="_blank">00:45:06.920</a></span> | <span class="t">And then we could, in principle, train this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2709" target="_blank">00:45:09.720</a></span> | <span class="t">Notice that calculating the mean and your standard deviation, these are just mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2713" target="_blank">00:45:13.360</a></span> | <span class="t">formulas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2714" target="_blank">00:45:14.360</a></span> | <span class="t">They're perfectly differentiable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2715" target="_blank">00:45:15.360</a></span> | <span class="t">All of this is perfectly differentiable, and we can just train this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2718" target="_blank">00:45:18.860</a></span> | <span class="t">The problem is you actually won't achieve a very good result with this, and the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2723" target="_blank">00:45:23.520</a></span> | <span class="t">for that is we want these to be roughly Gaussian, but only at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2729" target="_blank">00:45:29.840</a></span> | <span class="t">But we don't want these to be forced to be Gaussian always.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2734" target="_blank">00:45:34.520</a></span> | <span class="t">We'd like to allow the neural net to move this around to potentially make it more diffuse,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2739" target="_blank">00:45:39.360</a></span> | <span class="t">to make it more sharp, to make some 10H neurons maybe be more trigger happy or less trigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2744" target="_blank">00:45:44.640</a></span> | <span class="t">happy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2745" target="_blank">00:45:45.640</a></span> | <span class="t">So we'd like this distribution to move around, and we'd like the backpropagation to tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2749" target="_blank">00:45:49.000</a></span> | <span class="t">us how the distribution should move around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2752" target="_blank">00:45:52.540</a></span> | <span class="t">And so in addition to this idea of standardizing the activations at any point in the network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2759" target="_blank">00:45:59.400</a></span> | <span class="t">we have to also introduce this additional component in the paper here described as scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2764" target="_blank">00:46:04.480</a></span> | <span class="t">and shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2765" target="_blank">00:46:05.480</a></span> | <span class="t">And so basically what we're doing is we're taking these normalized inputs, and we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2769" target="_blank">00:46:09.640</a></span> | <span class="t">additionally scaling them by some gain and offsetting them by some bias to get our final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2775" target="_blank">00:46:15.480</a></span> | <span class="t">output from this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2777" target="_blank">00:46:17.920</a></span> | <span class="t">And so what that amounts to is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2780" target="_blank">00:46:20.520</a></span> | <span class="t">We are going to allow a batch normalization gain to be initialized at just a once, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2787" target="_blank">00:46:27.760</a></span> | <span class="t">the once will be in the shape of 1 by n hidden.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2792" target="_blank">00:46:32.560</a></span> | <span class="t">And then we also will have a bn_bias, which will be torched at zeros, and it will also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2798" target="_blank">00:46:38.400</a></span> | <span class="t">be of the shape 1 by n hidden.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2802" target="_blank">00:46:42.400</a></span> | <span class="t">And then here, the bn_gain will multiply this, and the bn_bias will offset it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2811" target="_blank">00:46:51.280</a></span> | <span class="t">So because this is initialized to 1 and this to 0, at initialization, each neuron's firing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2818" target="_blank">00:46:58.080</a></span> | <span class="t">values in this batch will be exactly unit Gaussian and will have nice numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2823" target="_blank">00:47:03.680</a></span> | <span class="t">No matter what the distribution of the H_preact is coming in, coming out, it will be unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2828" target="_blank">00:47:08.240</a></span> | <span class="t">Gaussian for each neuron, and that's roughly what we want, at least at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2834" target="_blank">00:47:14.040</a></span> | <span class="t">And then during optimization, we'll be able to backpropagate to bn_gain and bn_bias and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2838" target="_blank">00:47:18.600</a></span> | <span class="t">change them so the network is given the full ability to do with this whatever it wants</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2843" target="_blank">00:47:23.360</a></span> | <span class="t">internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2845" target="_blank">00:47:25.960</a></span> | <span class="t">Here we just have to make sure that we include these in the parameters of the neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2852" target="_blank">00:47:32.160</a></span> | <span class="t">because they will be trained with backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2855" target="_blank">00:47:35.860</a></span> | <span class="t">So let's initialize this, and then we should be able to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2865" target="_blank">00:47:45.800</a></span> | <span class="t">And then we're going to also copy this line, which is the batch normalization layer here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2872" target="_blank">00:47:52.040</a></span> | <span class="t">on a single line of code, and we're going to swing down here, and we're also going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2876" target="_blank">00:47:56.440</a></span> | <span class="t">do the exact same thing at test time here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2881" target="_blank">00:48:01.840</a></span> | <span class="t">So similar to train time, we're going to normalize and then scale, and that's going to give us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2887" target="_blank">00:48:07.520</a></span> | <span class="t">our train and validation loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2890" target="_blank">00:48:10.880</a></span> | <span class="t">And we'll see in a second that we're actually going to change this a little bit, but for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2893" target="_blank">00:48:13.240</a></span> | <span class="t">now I'm going to keep it this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2895" target="_blank">00:48:15.720</a></span> | <span class="t">So I'm just going to wait for this to converge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2897" target="_blank">00:48:17.400</a></span> | <span class="t">Okay, so I allowed the neural nets to converge here, and when we scroll down, we see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2901" target="_blank">00:48:21.400</a></span> | <span class="t">our validation loss here is 2.10, roughly, which I wrote down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2906" target="_blank">00:48:26.500</a></span> | <span class="t">And we see that this is actually kind of comparable to some of the results that we've achieved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2909" target="_blank">00:48:29.560</a></span> | <span class="t">previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2910" target="_blank">00:48:30.560</a></span> | <span class="t">Now, I'm not actually expecting an improvement in this case, and that's because we are dealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2915" target="_blank">00:48:35.800</a></span> | <span class="t">with a very simple neural net that has just a single hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2919" target="_blank">00:48:39.520</a></span> | <span class="t">So in fact, in this very simple case of just one hidden layer, we were able to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2923" target="_blank">00:48:43.800</a></span> | <span class="t">calculate what the scale of W should be to make these preactivations already have a roughly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2929" target="_blank">00:48:49.240</a></span> | <span class="t">Gaussian shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2930" target="_blank">00:48:50.240</a></span> | <span class="t">So the batch normalization is not doing much here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2933" target="_blank">00:48:53.360</a></span> | <span class="t">But you might imagine that once you have a much deeper neural net that has lots of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2937" target="_blank">00:48:57.000</a></span> | <span class="t">types of operations, and there's also, for example, residual connections, which we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2941" target="_blank">00:49:01.240</a></span> | <span class="t">cover, and so on, it will become basically very, very difficult to tune the scales of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2947" target="_blank">00:49:07.120</a></span> | <span class="t">your weight matrices such that all the activations throughout the neural net are roughly Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2953" target="_blank">00:49:13.140</a></span> | <span class="t">And so that's going to become very quickly intractable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2956" target="_blank">00:49:16.160</a></span> | <span class="t">But compared to that, it's going to be much, much easier to sprinkle batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2960" target="_blank">00:49:20.160</a></span> | <span class="t">layers throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2962" target="_blank">00:49:22.240</a></span> | <span class="t">So in particular, it's common to look at every single linear layer like this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2967" target="_blank">00:49:27.060</a></span> | <span class="t">This is a linear layer multiplying by a weight matrix and adding a bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2971" target="_blank">00:49:31.000</a></span> | <span class="t">Or for example, convolutions, which we'll cover later and also perform basically a multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2976" target="_blank">00:49:36.240</a></span> | <span class="t">with a weight matrix, but in a more spatially structured format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2981" target="_blank">00:49:41.080</a></span> | <span class="t">It's customary to take this linear layer or convolutional layer and append a batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2986" target="_blank">00:49:46.040</a></span> | <span class="t">layer right after it to control the scale of these activations at every point in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2990" target="_blank">00:49:50.760</a></span> | <span class="t">neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2991" target="_blank">00:49:51.960</a></span> | <span class="t">So we'd be adding these batch normal layers throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2994" target="_blank">00:49:54.960</a></span> | <span class="t">And then this controls the scale of these activations throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=2998" target="_blank">00:49:58.720</a></span> | <span class="t">It doesn't require us to do perfect mathematics and care about the activation distributions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3003" target="_blank">00:50:03.920</a></span> | <span class="t">for all these different types of neural network Lego building blocks that you might want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3007" target="_blank">00:50:07.760</a></span> | <span class="t">introduce into your neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3009" target="_blank">00:50:09.560</a></span> | <span class="t">And it significantly stabilizes the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3012" target="_blank">00:50:12.400</a></span> | <span class="t">And that's why these layers are quite popular.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3014" target="_blank">00:50:14.960</a></span> | <span class="t">Now the stability offered by batch normalization actually comes at a terrible cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3019" target="_blank">00:50:19.160</a></span> | <span class="t">And that cost is that if you think about what's happening here, something terribly strange</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3024" target="_blank">00:50:24.120</a></span> | <span class="t">and unnatural is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3026" target="_blank">00:50:26.580</a></span> | <span class="t">It used to be that we have a single example feeding into a neural net, and then we calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3032" target="_blank">00:50:32.040</a></span> | <span class="t">its activations and its logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3034" target="_blank">00:50:34.560</a></span> | <span class="t">And this is a deterministic sort of process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3037" target="_blank">00:50:37.560</a></span> | <span class="t">So you arrive at some logits for this example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3040" target="_blank">00:50:40.260</a></span> | <span class="t">And then because of efficiency of training, we suddenly started to use batches of examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3044" target="_blank">00:50:44.940</a></span> | <span class="t">But those batches of examples were processed independently, and it was just an efficiency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3048" target="_blank">00:50:48.720</a></span> | <span class="t">thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3050" target="_blank">00:50:50.000</a></span> | <span class="t">But now suddenly in batch normalization, because of the normalization through the batch, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3053" target="_blank">00:50:53.760</a></span> | <span class="t">are coupling these examples mathematically and in the forward pass and the backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3058" target="_blank">00:50:58.040</a></span> | <span class="t">of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3059" target="_blank">00:50:59.640</a></span> | <span class="t">So now the hidden state activations, HPREACT, and your logits for any one input example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3065" target="_blank">00:51:05.560</a></span> | <span class="t">are not just a function of that example and its input, but they're also a function of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3069" target="_blank">00:51:09.760</a></span> | <span class="t">all the other examples that happen to come for a ride in that batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3074" target="_blank">00:51:14.640</a></span> | <span class="t">And these examples are sampled randomly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3076" target="_blank">00:51:16.620</a></span> | <span class="t">And so what's happening is, for example, when you look at HPREACT, that's going to feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3079" target="_blank">00:51:19.420</a></span> | <span class="t">into H, the hidden state activations, for example, for any one of these input examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3085" target="_blank">00:51:25.400</a></span> | <span class="t">is going to actually change slightly depending on what other examples there are in the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3091" target="_blank">00:51:31.040</a></span> | <span class="t">And depending on what other examples happen to come for a ride, H is going to change suddenly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3096" target="_blank">00:51:36.300</a></span> | <span class="t">and is going to jitter if you imagine sampling different examples, because the statistics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3100" target="_blank">00:51:40.840</a></span> | <span class="t">of the mean and the standard deviation are going to be impacted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3104" target="_blank">00:51:44.120</a></span> | <span class="t">And so you'll get a jitter for H, and you'll get a jitter for logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3108" target="_blank">00:51:48.760</a></span> | <span class="t">And you'd think that this would be a bug or something undesirable, but in a very strange</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3113" target="_blank">00:51:53.840</a></span> | <span class="t">way, this actually turns out to be good in neural network training as a side effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3119" target="_blank">00:51:59.840</a></span> | <span class="t">And the reason for that is that you can think of this as kind of like a regularizer, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3124" target="_blank">00:52:04.200</a></span> | <span class="t">what's happening is you have your input and you get your H, and then depending on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3127" target="_blank">00:52:07.200</a></span> | <span class="t">other examples, this is jittering a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3130" target="_blank">00:52:10.080</a></span> | <span class="t">And so what that does is that it's effectively padding out any one of these input examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3134" target="_blank">00:52:14.440</a></span> | <span class="t">and it's introducing a little bit of entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3136" target="_blank">00:52:16.720</a></span> | <span class="t">And because of the padding out, it's actually kind of like a form of data augmentation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3141" target="_blank">00:52:21.480</a></span> | <span class="t">which we'll cover in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3142" target="_blank">00:52:22.480</a></span> | <span class="t">And it's kind of like augmenting the input a little bit and jittering it, and that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3147" target="_blank">00:52:27.360</a></span> | <span class="t">it harder for the neural nets to overfit to these concrete specific examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3152" target="_blank">00:52:32.120</a></span> | <span class="t">So by introducing all this noise, it actually like pads out the examples and it regularizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3156" target="_blank">00:52:36.800</a></span> | <span class="t">the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3157" target="_blank">00:52:37.960</a></span> | <span class="t">And that's one of the reasons why deceivingly as a second order effect, this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3162" target="_blank">00:52:42.680</a></span> | <span class="t">a regularizer, and that has made it harder for us to remove the use of batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3168" target="_blank">00:52:48.920</a></span> | <span class="t">Because basically no one likes this property that the examples in the batch are coupled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3173" target="_blank">00:52:53.240</a></span> | <span class="t">mathematically and in the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3175" target="_blank">00:52:55.960</a></span> | <span class="t">And it leads to all kinds of like strange results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3178" target="_blank">00:52:58.840</a></span> | <span class="t">We'll go into some of that in a second as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3181" target="_blank">00:53:01.960</a></span> | <span class="t">And it leads to a lot of bugs and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3185" target="_blank">00:53:05.000</a></span> | <span class="t">And so no one likes this property.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3187" target="_blank">00:53:07.180</a></span> | <span class="t">And so people have tried to deprecate the use of batch normalization and move to other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3191" target="_blank">00:53:11.720</a></span> | <span class="t">normalization techniques that do not couple the examples of a batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3195" target="_blank">00:53:15.280</a></span> | <span class="t">Examples are layer normalization, instance normalization, group normalization, and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3199" target="_blank">00:53:19.440</a></span> | <span class="t">on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3200" target="_blank">00:53:20.440</a></span> | <span class="t">And we'll come or we'll come or some of these later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3204" target="_blank">00:53:24.400</a></span> | <span class="t">But basically long story short, batch normalization was the first kind of normalization layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3208" target="_blank">00:53:28.120</a></span> | <span class="t">to be introduced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3209" target="_blank">00:53:29.300</a></span> | <span class="t">It worked extremely well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3211" target="_blank">00:53:31.080</a></span> | <span class="t">It happens to have this regularizing effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3213" target="_blank">00:53:33.600</a></span> | <span class="t">It stabilized training and people have been trying to remove it and move to some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3218" target="_blank">00:53:38.880</a></span> | <span class="t">other normalization techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3221" target="_blank">00:53:41.080</a></span> | <span class="t">But it's been hard because it just works quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3224" target="_blank">00:53:44.480</a></span> | <span class="t">And some of the reason that it works quite well is again because of this regularizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3227" target="_blank">00:53:47.560</a></span> | <span class="t">effect and because it is quite effective at controlling the activations and their distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3234" target="_blank">00:53:54.640</a></span> | <span class="t">So that's kind of like the brief story of batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3237" target="_blank">00:53:57.700</a></span> | <span class="t">And I'd like to show you one of the other weird sort of outcomes of this coupling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3243" target="_blank">00:54:03.880</a></span> | <span class="t">So here's one of the strange outcomes that I only glossed over previously when I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3248" target="_blank">00:54:08.120</a></span> | <span class="t">evaluating the loss on the validation set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3251" target="_blank">00:54:11.160</a></span> | <span class="t">Basically once we've trained a neural net, we'd like to deploy it in some kind of a setting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3255" target="_blank">00:54:15.920</a></span> | <span class="t">and we'd like to be able to feed in a single individual example and get a prediction out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3259" target="_blank">00:54:19.840</a></span> | <span class="t">from our neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3261" target="_blank">00:54:21.560</a></span> | <span class="t">But how do we do that when our neural net now in a forward pass estimates the statistics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3265" target="_blank">00:54:25.840</a></span> | <span class="t">of the mean understated deviation of a batch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3268" target="_blank">00:54:28.040</a></span> | <span class="t">The neural net expects batches as an input now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3270" target="_blank">00:54:30.640</a></span> | <span class="t">So how do we feed in a single example and get sensible results out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3274" target="_blank">00:54:34.600</a></span> | <span class="t">And so the proposal in the batch normalization paper is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3279" target="_blank">00:54:39.040</a></span> | <span class="t">What we would like to do here is we would like to basically have a step after training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3284" target="_blank">00:54:44.800</a></span> | <span class="t">that calculates and sets the batch norm mean and standard deviation a single time over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3290" target="_blank">00:54:50.720</a></span> | <span class="t">the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3292" target="_blank">00:54:52.360</a></span> | <span class="t">And so I wrote this code here in the interest of time and we're going to call what's called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3296" target="_blank">00:54:56.720</a></span> | <span class="t">calibrate the batch norm statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3299" target="_blank">00:54:59.280</a></span> | <span class="t">And basically what we do is Torch.nograd telling PyTorch that none of this we will call a dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3305" target="_blank">00:55:05.360</a></span> | <span class="t">backward on and it's going to be a bit more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3309" target="_blank">00:55:09.000</a></span> | <span class="t">We're going to take the training set, get the preactivations for every single training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3312" target="_blank">00:55:12.600</a></span> | <span class="t">example and then one single time estimate the mean and standard deviation over the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3316" target="_blank">00:55:16.760</a></span> | <span class="t">training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3318" target="_blank">00:55:18.360</a></span> | <span class="t">And then we're going to get B and mean and B and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3321" target="_blank">00:55:21.100</a></span> | <span class="t">And now these are fixed numbers estimating over the entire training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3325" target="_blank">00:55:25.440</a></span> | <span class="t">And here instead of estimating it dynamically, we are going to instead here use B and mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3334" target="_blank">00:55:34.460</a></span> | <span class="t">and here we're just going to use B and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3338" target="_blank">00:55:38.220</a></span> | <span class="t">And so at test time, we are going to fix these, clamp them and use them during inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3343" target="_blank">00:55:43.280</a></span> | <span class="t">And now you see that we get basically identical result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3349" target="_blank">00:55:49.120</a></span> | <span class="t">But the benefit that we've gained is that we can now also forward a single example because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3353" target="_blank">00:55:53.480</a></span> | <span class="t">the mean and standard deviation are now fixed sort of tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3357" target="_blank">00:55:57.560</a></span> | <span class="t">That said, nobody actually wants to estimate this mean and standard deviation as a second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3361" target="_blank">00:56:01.800</a></span> | <span class="t">stage after neural network training because everyone is lazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3365" target="_blank">00:56:05.820</a></span> | <span class="t">And so this batch normalization paper actually introduced one more idea, which is that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3370" target="_blank">00:56:10.680</a></span> | <span class="t">can estimate the mean and standard deviation in a running manner during training of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3375" target="_blank">00:56:15.960</a></span> | <span class="t">neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3377" target="_blank">00:56:17.280</a></span> | <span class="t">And then we can simply just have a single stage of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3380" target="_blank">00:56:20.240</a></span> | <span class="t">And on the side of that training, we are estimating the running mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3384" target="_blank">00:56:24.640</a></span> | <span class="t">So let's see what that would look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3386" target="_blank">00:56:26.160</a></span> | <span class="t">Let me basically take the mean here that we are estimating on the batch and let me call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3390" target="_blank">00:56:30.600</a></span> | <span class="t">this B and mean on the i-th iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3395" target="_blank">00:56:35.520</a></span> | <span class="t">And then here, this is B and STD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3401" target="_blank">00:56:41.180</a></span> | <span class="t">B and STD at i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3407" target="_blank">00:56:47.300</a></span> | <span class="t">And the mean comes here and the STD comes here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3413" target="_blank">00:56:53.100</a></span> | <span class="t">So so far, I've done nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3414" target="_blank">00:56:54.180</a></span> | <span class="t">I've just moved around and I created these extra variables for the mean and standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3417" target="_blank">00:56:57.500</a></span> | <span class="t">deviation and I've put them here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3419" target="_blank">00:56:59.860</a></span> | <span class="t">So so far, nothing has changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3421" target="_blank">00:57:01.840</a></span> | <span class="t">But what we're going to do now is we're going to keep a running mean of both of these values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3425" target="_blank">00:57:05.400</a></span> | <span class="t">during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3426" target="_blank">00:57:06.400</a></span> | <span class="t">So let me swing up here and let me create a B and mean underscore running.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3432" target="_blank">00:57:12.020</a></span> | <span class="t">And I'm going to initialize it at zeros and then B and STD running, which I'll initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3439" target="_blank">00:57:19.580</a></span> | <span class="t">at ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3443" target="_blank">00:57:23.540</a></span> | <span class="t">Because in the beginning, because of the way we initialized W1 and B1, HPREACT will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3450" target="_blank">00:57:30.000</a></span> | <span class="t">roughly unit Gaussian, so the mean will be roughly zero and the standard deviation roughly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3453" target="_blank">00:57:33.920</a></span> | <span class="t">one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3454" target="_blank">00:57:34.920</a></span> | <span class="t">So I'm going to initialize these that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3457" target="_blank">00:57:37.280</a></span> | <span class="t">But then here, I'm going to update these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3459" target="_blank">00:57:39.560</a></span> | <span class="t">And in PyTorch, these mean and standard deviation that are running, they're not actually part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3466" target="_blank">00:57:46.220</a></span> | <span class="t">of the gradient based optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3467" target="_blank">00:57:47.800</a></span> | <span class="t">We're never going to derive gradients with respect to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3470" target="_blank">00:57:50.800</a></span> | <span class="t">They're updated on the side of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3473" target="_blank">00:57:53.740</a></span> | <span class="t">And so what we're going to do here is we're going to say with torch.nograd, telling PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3478" target="_blank">00:57:58.720</a></span> | <span class="t">that the update here is not supposed to be building out a graph because there will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3483" target="_blank">00:58:03.360</a></span> | <span class="t">no dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3485" target="_blank">00:58:05.480</a></span> | <span class="t">But this running mean is basically going to be 0.999 times the current value plus 0.001</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3495" target="_blank">00:58:15.160</a></span> | <span class="t">times this value, this new mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3500" target="_blank">00:58:20.640</a></span> | <span class="t">And in the same way, BNSTDRunning will be mostly what it used to be, but it will receive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3509" target="_blank">00:58:29.400</a></span> | <span class="t">a small update in the direction of what the current standard deviation is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3515" target="_blank">00:58:35.180</a></span> | <span class="t">And as you're seeing here, this update is outside and on the side of the gradient based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3519" target="_blank">00:58:39.940</a></span> | <span class="t">optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3521" target="_blank">00:58:41.480</a></span> | <span class="t">And it's simply being updated not using gradient descent, it's just being updated using a janky,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3526" target="_blank">00:58:46.640</a></span> | <span class="t">like smooth, sort of running mean manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3533" target="_blank">00:58:53.360</a></span> | <span class="t">And so while the network is training and these preactivations are sort of changing and shifting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3538" target="_blank">00:58:58.120</a></span> | <span class="t">around during backpropagation, we are keeping track of the typical mean and standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3543" target="_blank">00:59:03.680</a></span> | <span class="t">and we're estimating them once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3545" target="_blank">00:59:05.640</a></span> | <span class="t">And when I run this, now I'm keeping track of this in a running manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3552" target="_blank">00:59:12.160</a></span> | <span class="t">And what we're hoping for, of course, is that the BNMean_running and BNMean_backpropagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3556" target="_blank">00:59:16.520</a></span> | <span class="t">or STD are going to be very similar to the ones that we calculated here before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3562" target="_blank">00:59:22.480</a></span> | <span class="t">And that way, we don't need a second stage because we've sort of combined the two stages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3566" target="_blank">00:59:26.800</a></span> | <span class="t">and we've put them on the side of each other, if you want to look at it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3570" target="_blank">00:59:30.800</a></span> | <span class="t">And this is how this is also implemented in the batch normalization layer in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3575" target="_blank">00:59:35.100</a></span> | <span class="t">So during training, the exact same thing will happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3579" target="_blank">00:59:39.120</a></span> | <span class="t">And then later when you're using inference, it will use the estimated running mean of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3583" target="_blank">00:59:43.720</a></span> | <span class="t">both the mean and standard deviation of those hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3587" target="_blank">00:59:47.960</a></span> | <span class="t">So let's wait for the optimization to converge and hopefully the running mean and standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3591" target="_blank">00:59:51.760</a></span> | <span class="t">deviation are roughly equal to these two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3593" target="_blank">00:59:53.960</a></span> | <span class="t">And then we can simply use it here and we don't need this stage of explicit calibration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3598" target="_blank">00:59:58.360</a></span> | <span class="t">at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3599" target="_blank">00:59:59.360</a></span> | <span class="t">Okay, so the optimization finished.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3601" target="_blank">01:00:01.280</a></span> | <span class="t">I'll rerun the explicit estimation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3603" target="_blank">01:00:03.980</a></span> | <span class="t">And then the BNMean from the explicit estimation is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3607" target="_blank">01:00:07.880</a></span> | <span class="t">And BNMean from the running estimation during the optimization, you can see is very, very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3614" target="_blank">01:00:14.760</a></span> | <span class="t">similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3615" target="_blank">01:00:15.760</a></span> | <span class="t">It's not identical, but it's pretty close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3619" target="_blank">01:00:19.720</a></span> | <span class="t">And in the same way, BNSTD is this and BNSTDRunning is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3626" target="_blank">01:00:26.440</a></span> | <span class="t">As you can see that once again, they are fairly similar values, not identical, but pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3630" target="_blank">01:00:30.600</a></span> | <span class="t">close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3631" target="_blank">01:00:31.960</a></span> | <span class="t">And so then here, instead of BNMean, we can use the BNMean running.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3636" target="_blank">01:00:36.160</a></span> | <span class="t">Instead of BNSTD, we can use BNSTDRunning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3640" target="_blank">01:00:40.120</a></span> | <span class="t">And hopefully the validation loss will not be impacted too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3643" target="_blank">01:00:43.720</a></span> | <span class="t">Okay, so basically identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3646" target="_blank">01:00:46.880</a></span> | <span class="t">And this way, we've eliminated the need for this explicit stage of calibration because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3651" target="_blank">01:00:51.800</a></span> | <span class="t">we are doing it in line over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3653" target="_blank">01:00:53.760</a></span> | <span class="t">Okay, so we're almost done with batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3656" target="_blank">01:00:56.160</a></span> | <span class="t">There are only two more notes that I'd like to make.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3658" target="_blank">01:00:58.600</a></span> | <span class="t">Number one, I've skipped a discussion over what is this plus epsilon doing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3662" target="_blank">01:01:02.280</a></span> | <span class="t">This epsilon is usually like some small fixed number, for example, 1E negative 5 by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3667" target="_blank">01:01:07.360</a></span> | <span class="t">And what it's doing is that it's basically preventing a division by zero in the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3671" target="_blank">01:01:11.120</a></span> | <span class="t">that the variance over your batch is exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3675" target="_blank">01:01:15.940</a></span> | <span class="t">In that case, here we'd normally have a division by zero, but because of the plus epsilon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3680" target="_blank">01:01:20.880</a></span> | <span class="t">this is going to become a small number in the denominator instead, and things will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3684" target="_blank">01:01:24.080</a></span> | <span class="t">more well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3685" target="_blank">01:01:25.680</a></span> | <span class="t">So feel free to also add a plus epsilon here of a very small number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3689" target="_blank">01:01:29.200</a></span> | <span class="t">It doesn't actually substantially change the result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3691" target="_blank">01:01:31.200</a></span> | <span class="t">I'm going to skip it in our case just because this is unlikely to happen in our very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3694" target="_blank">01:01:34.840</a></span> | <span class="t">example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3695" target="_blank">01:01:35.840</a></span> | <span class="t">And the second thing I want you to notice is that we're being wasteful here, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3699" target="_blank">01:01:39.920</a></span> | <span class="t">very subtle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3701" target="_blank">01:01:41.400</a></span> | <span class="t">But right here where we are adding the bias into HPREACT, these biases now are actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3707" target="_blank">01:01:47.160</a></span> | <span class="t">useless because we're adding them to the HPREACT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3710" target="_blank">01:01:50.600</a></span> | <span class="t">But then we are calculating the mean for every one of these neurons and subtracting it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3716" target="_blank">01:01:56.080</a></span> | <span class="t">So whatever bias you add here is going to get subtracted right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3720" target="_blank">01:02:00.940</a></span> | <span class="t">And so these biases are not doing anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3722" target="_blank">01:02:02.920</a></span> | <span class="t">In fact, they're being subtracted out, and they don't impact the rest of the calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3727" target="_blank">01:02:07.360</a></span> | <span class="t">So if you look at B1.grad, it's actually going to be zero because it's being subtracted out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3731" target="_blank">01:02:11.620</a></span> | <span class="t">and doesn't actually have any effect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3733" target="_blank">01:02:13.720</a></span> | <span class="t">And so whenever you're using batch normalization layers, then if you have any weight layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3737" target="_blank">01:02:17.640</a></span> | <span class="t">before, like a linear or a conv or something like that, you're better off coming here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3742" target="_blank">01:02:22.380</a></span> | <span class="t">just not using bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3744" target="_blank">01:02:24.400</a></span> | <span class="t">So you don't want to use bias, and then here you don't want to add it because that's spurious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3750" target="_blank">01:02:30.720</a></span> | <span class="t">Instead we have this batch normalization bias here, and that batch normalization bias is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3755" target="_blank">01:02:35.320</a></span> | <span class="t">now in charge of the biasing of this distribution instead of this B1 that we had here originally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3762" target="_blank">01:02:42.320</a></span> | <span class="t">And so basically the batch normalization layer has its own bias, and there's no need to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3767" target="_blank">01:02:47.560</a></span> | <span class="t">a bias in the layer before it because that bias is going to be subtracted out anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3772" target="_blank">01:02:52.080</a></span> | <span class="t">So that's the other small detail to be careful with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3774" target="_blank">01:02:54.160</a></span> | <span class="t">Sometimes it's not going to do anything catastrophic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3776" target="_blank">01:02:56.760</a></span> | <span class="t">This B1 will just be useless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3778" target="_blank">01:02:58.600</a></span> | <span class="t">It will never get any gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3780" target="_blank">01:03:00.440</a></span> | <span class="t">It will not learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3781" target="_blank">01:03:01.440</a></span> | <span class="t">It will stay constant, and it's just wasteful, but it doesn't actually really impact anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3786" target="_blank">01:03:06.200</a></span> | <span class="t">otherwise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3787" target="_blank">01:03:07.200</a></span> | <span class="t">Okay, so I rearranged the code a little bit with comments, and I just wanted to give a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3790" target="_blank">01:03:10.640</a></span> | <span class="t">very quick summary of the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3793" target="_blank">01:03:13.800</a></span> | <span class="t">We are using batch normalization to control the statistics of activations in the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3798" target="_blank">01:03:18.600</a></span> | <span class="t">net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3799" target="_blank">01:03:19.600</a></span> | <span class="t">It is common to sprinkle batch normalization layer across the neural net, and usually we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3803" target="_blank">01:03:23.880</a></span> | <span class="t">will place it after layers that have multiplications, like for example a linear layer or a convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3810" target="_blank">01:03:30.200</a></span> | <span class="t">layer, which we may cover in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3813" target="_blank">01:03:33.240</a></span> | <span class="t">Now the batch normalization internally has parameters for the gain and the bias, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3819" target="_blank">01:03:39.520</a></span> | <span class="t">these are trained using backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3821" target="_blank">01:03:41.820</a></span> | <span class="t">It also has two buffers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3824" target="_blank">01:03:44.460</a></span> | <span class="t">The buffers are the mean and the standard deviation, the running mean and the running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3828" target="_blank">01:03:48.680</a></span> | <span class="t">mean of the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3831" target="_blank">01:03:51.080</a></span> | <span class="t">And these are not trained using backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3833" target="_blank">01:03:53.040</a></span> | <span class="t">These are trained using this janky update of kind of like a running mean update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3839" target="_blank">01:03:59.000</a></span> | <span class="t">So these are sort of the parameters and the buffers of batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3845" target="_blank">01:04:05.320</a></span> | <span class="t">And then really what it's doing is it's calculating the mean and the standard deviation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3849" target="_blank">01:04:09.040</a></span> | <span class="t">activations that are feeding into the batch normalization layer over that batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3855" target="_blank">01:04:15.080</a></span> | <span class="t">Then it's centering that batch to be unit Gaussian, and then it's offsetting and scaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3860" target="_blank">01:04:20.040</a></span> | <span class="t">it by the learned bias and gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3864" target="_blank">01:04:24.240</a></span> | <span class="t">And then on top of that, it's keeping track of the mean and standard deviation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3867" target="_blank">01:04:27.640</a></span> | <span class="t">inputs, and it's maintaining this running mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3872" target="_blank">01:04:32.920</a></span> | <span class="t">And this will later be used at inference so that we don't have to re-estimate the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3876" target="_blank">01:04:36.920</a></span> | <span class="t">and standard deviation all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3879" target="_blank">01:04:39.200</a></span> | <span class="t">And in addition, that allows us to basically forward individual examples at test time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3883" target="_blank">01:04:43.520</a></span> | <span class="t">So that's the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3885" target="_blank">01:04:45.960</a></span> | <span class="t">It's a fairly complicated layer, but this is what it's doing internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3890" target="_blank">01:04:50.560</a></span> | <span class="t">Now I wanted to show you a little bit of a real example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3893" target="_blank">01:04:53.360</a></span> | <span class="t">So you can search ResNet, which is a residual neural network, and these are contacts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3899" target="_blank">01:04:59.120</a></span> | <span class="t">neural networks used for image classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3902" target="_blank">01:05:02.320</a></span> | <span class="t">And of course, we haven't covered ResNets in detail, so I'm not going to explain all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3906" target="_blank">01:05:06.520</a></span> | <span class="t">the pieces of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3907" target="_blank">01:05:07.520</a></span> | <span class="t">But for now, just note that the image feeds into a ResNet on the top here, and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3912" target="_blank">01:05:12.440</a></span> | <span class="t">many, many layers with repeating structure all the way to predictions of what's inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3916" target="_blank">01:05:16.880</a></span> | <span class="t">that image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3918" target="_blank">01:05:18.440</a></span> | <span class="t">This repeating structure is made up of these blocks, and these blocks are just sequentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3922" target="_blank">01:05:22.360</a></span> | <span class="t">stacked up in this deep neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3925" target="_blank">01:05:25.680</a></span> | <span class="t">Now the code for this, the block basically that's used and repeated sequentially in series,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3932" target="_blank">01:05:32.480</a></span> | <span class="t">is called this bottleneck block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3936" target="_blank">01:05:36.360</a></span> | <span class="t">And there's a lot here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3937" target="_blank">01:05:37.480</a></span> | <span class="t">This is all PyTorch, and of course we haven't covered all of it, but I want to point out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3940" target="_blank">01:05:40.960</a></span> | <span class="t">some small pieces of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3943" target="_blank">01:05:43.320</a></span> | <span class="t">Here in the init is where we initialize the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3945" target="_blank">01:05:45.760</a></span> | <span class="t">So this code of block here is basically the kind of stuff we're doing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3949" target="_blank">01:05:49.040</a></span> | <span class="t">We're initializing all the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3951" target="_blank">01:05:51.200</a></span> | <span class="t">And in the forward, we are specifying how the neural net acts once you actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3954" target="_blank">01:05:54.840</a></span> | <span class="t">the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3955" target="_blank">01:05:55.840</a></span> | <span class="t">So this code here is along the lines of what we're doing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3961" target="_blank">01:06:01.760</a></span> | <span class="t">And now these blocks are replicated and stacked up serially, and that's what a residual network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3967" target="_blank">01:06:07.440</a></span> | <span class="t">would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3969" target="_blank">01:06:09.040</a></span> | <span class="t">And so notice what's happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3971" target="_blank">01:06:11.160</a></span> | <span class="t">Conv1, these are convolution layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3975" target="_blank">01:06:15.040</a></span> | <span class="t">And these convolution layers basically, they're the same thing as a linear layer, except convolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3980" target="_blank">01:06:20.340</a></span> | <span class="t">layers don't apply, convolution layers are used for images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3984" target="_blank">01:06:24.880</a></span> | <span class="t">And so they have spatial structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3986" target="_blank">01:06:26.640</a></span> | <span class="t">And basically this linear multiplication and bias offset are done on patches instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3992" target="_blank">01:06:32.480</a></span> | <span class="t">a map, instead of the full input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3994" target="_blank">01:06:34.840</a></span> | <span class="t">So because these images have structure, spatial structure, convolutions just basically do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=3999" target="_blank">01:06:39.520</a></span> | <span class="t">WX plus B, but they do it on overlapping patches of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4004" target="_blank">01:06:44.040</a></span> | <span class="t">But otherwise it's WX plus B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4006" target="_blank">01:06:46.920</a></span> | <span class="t">Then we have the normal layer, which by default here is initialized to be a batch norm in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4010" target="_blank">01:06:50.800</a></span> | <span class="t">2D, so two-dimensional batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4014" target="_blank">01:06:54.360</a></span> | <span class="t">And then we have a nonlinearity like ReLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4016" target="_blank">01:06:56.780</a></span> | <span class="t">So instead of, here they use ReLU, we are using tanh in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4022" target="_blank">01:07:02.760</a></span> | <span class="t">But both are just nonlinearities and you can just use them relatively interchangeably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4027" target="_blank">01:07:07.440</a></span> | <span class="t">For very deep networks, ReLUs typically empirically work a bit better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4031" target="_blank">01:07:11.840</a></span> | <span class="t">So see the motif that's being repeated here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4034" target="_blank">01:07:14.200</a></span> | <span class="t">We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4038" target="_blank">01:07:18.800</a></span> | <span class="t">etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4039" target="_blank">01:07:19.800</a></span> | <span class="t">And then here, this is a residual connection that we haven't covered yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4043" target="_blank">01:07:23.120</a></span> | <span class="t">But basically that's the exact same pattern we have here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4045" target="_blank">01:07:25.480</a></span> | <span class="t">We have a weight layer, like a convolution or like a linear layer, batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4052" target="_blank">01:07:32.600</a></span> | <span class="t">and then tanh, which is nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4055" target="_blank">01:07:35.680</a></span> | <span class="t">But basically a weight layer, a normalization layer, and nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4059" target="_blank">01:07:39.700</a></span> | <span class="t">And that's the motif that you would be stacking up when you create these deep neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4063" target="_blank">01:07:43.600</a></span> | <span class="t">exactly as is done here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4065" target="_blank">01:07:45.760</a></span> | <span class="t">And one more thing I'd like you to notice is that here when they are initializing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4069" target="_blank">01:07:49.280</a></span> | <span class="t">conv layers, like conv1x1, the depth for that is right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4073" target="_blank">01:07:53.920</a></span> | <span class="t">And so it's initializing an nn.conv2d, which is a convolution layer in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4079" target="_blank">01:07:59.160</a></span> | <span class="t">And there's a bunch of keyword arguments here that I'm not going to explain yet, but you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4082" target="_blank">01:08:02.680</a></span> | <span class="t">see how there's bias equals false?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4084" target="_blank">01:08:04.920</a></span> | <span class="t">The bias equals false is exactly for the same reason as bias is not used in our case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4090" target="_blank">01:08:10.240</a></span> | <span class="t">You see how I erased the use of bias?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4092" target="_blank">01:08:12.320</a></span> | <span class="t">And the use of bias is spurious because after this weight layer, there's a batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4096" target="_blank">01:08:16.920</a></span> | <span class="t">And the batch normalization subtracts that bias and then has its own bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4100" target="_blank">01:08:20.400</a></span> | <span class="t">So there's no need to introduce these spurious parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4103" target="_blank">01:08:23.280</a></span> | <span class="t">It wouldn't hurt performance, it's just useless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4105" target="_blank">01:08:25.960</a></span> | <span class="t">And so because they have this motif of conv, batch, and relu, they don't need a bias here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4111" target="_blank">01:08:31.080</a></span> | <span class="t">because there's a bias inside here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4113" target="_blank">01:08:33.640</a></span> | <span class="t">So by the way, this example here is very easy to find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4117" target="_blank">01:08:37.120</a></span> | <span class="t">Just do ResNetPyTorch, and it's this example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4121" target="_blank">01:08:41.920</a></span> | <span class="t">So this is kind of like the stock implementation of a residual neural network in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4126" target="_blank">01:08:46.600</a></span> | <span class="t">And you can find that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4128" target="_blank">01:08:48.320</a></span> | <span class="t">But of course, I haven't covered many of these parts yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4130" target="_blank">01:08:50.840</a></span> | <span class="t">And I would also like to briefly descend into the definitions of these PyTorch layers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4135" target="_blank">01:08:55.400</a></span> | <span class="t">the parameters that they take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4137" target="_blank">01:08:57.120</a></span> | <span class="t">Now instead of a convolutional layer, we're going to look at a linear layer because that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4141" target="_blank">01:09:01.760</a></span> | <span class="t">the one that we're using here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4142" target="_blank">01:09:02.920</a></span> | <span class="t">This is a linear layer, and I haven't covered convolutions yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4146" target="_blank">01:09:06.280</a></span> | <span class="t">But as I mentioned, convolutions are basically linear layers except on patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4151" target="_blank">01:09:11.400</a></span> | <span class="t">So a linear layer performs a WX+B, except here they're calling the W a transpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4158" target="_blank">01:09:18.920</a></span> | <span class="t">So it calculates WX+B very much like we did here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4161" target="_blank">01:09:21.620</a></span> | <span class="t">To initialize this layer, you need to know the fan in, the fan out, and that's so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4167" target="_blank">01:09:27.200</a></span> | <span class="t">they can initialize this W. This is the fan in and the fan out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4172" target="_blank">01:09:32.120</a></span> | <span class="t">So they know how big the weight matrix should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4175" target="_blank">01:09:35.680</a></span> | <span class="t">You need to also pass in whether or not you want a bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4179" target="_blank">01:09:39.240</a></span> | <span class="t">And if you set it to false, then no bias will be inside this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4184" target="_blank">01:09:44.600</a></span> | <span class="t">And you may want to do that exactly like in our case, if your layer is followed by a normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4189" target="_blank">01:09:49.180</a></span> | <span class="t">layer such as batch norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4191" target="_blank">01:09:51.900</a></span> | <span class="t">So this allows you to basically disable bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4194" target="_blank">01:09:54.720</a></span> | <span class="t">In terms of the initialization, if we swing down here, this is reporting the variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4198" target="_blank">01:09:58.240</a></span> | <span class="t">used inside this linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4201" target="_blank">01:10:01.200</a></span> | <span class="t">And our linear layer here has two parameters, the weight and the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4205" target="_blank">01:10:05.960</a></span> | <span class="t">In the same way, they have a weight and a bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4208" target="_blank">01:10:08.840</a></span> | <span class="t">And they're talking about how they initialize it by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4211" target="_blank">01:10:11.880</a></span> | <span class="t">So by default, PyTorch will initialize your weights by taking the fan in and then doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4218" target="_blank">01:10:18.120</a></span> | <span class="t">1/fanin square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4221" target="_blank">01:10:21.040</a></span> | <span class="t">And then instead of a normal distribution, they are using a uniform distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4225" target="_blank">01:10:25.900</a></span> | <span class="t">So it's very much the same thing, but they are using a 1 instead of 5/3, so there's no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4230" target="_blank">01:10:30.940</a></span> | <span class="t">gain being calculated here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4232" target="_blank">01:10:32.640</a></span> | <span class="t">The gain is just 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4233" target="_blank">01:10:33.860</a></span> | <span class="t">But otherwise, it's exactly 1/the square root of fanin, exactly as we have here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4240" target="_blank">01:10:40.600</a></span> | <span class="t">So 1/the square root of k is the scale of the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4245" target="_blank">01:10:45.440</a></span> | <span class="t">But when they are drawing the numbers, they're not using a Gaussian by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4248" target="_blank">01:10:48.960</a></span> | <span class="t">They're using a uniform distribution by default.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4251" target="_blank">01:10:51.600</a></span> | <span class="t">And so they draw uniformly from negative square root of k to square root of k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4256" target="_blank">01:10:56.080</a></span> | <span class="t">But it's the exact same thing and the same motivation with respect to what we've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4261" target="_blank">01:11:01.360</a></span> | <span class="t">in this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4263" target="_blank">01:11:03.260</a></span> | <span class="t">And the reason they're doing this is if you have a roughly Gaussian input, this will ensure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4267" target="_blank">01:11:07.900</a></span> | <span class="t">that out of this layer, you will have a roughly Gaussian output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4272" target="_blank">01:11:12.020</a></span> | <span class="t">And you basically achieve that by scaling the weights by 1/the square root of fanin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4277" target="_blank">01:11:17.900</a></span> | <span class="t">So that's what this is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4280" target="_blank">01:11:20.260</a></span> | <span class="t">And then the second thing is the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4283" target="_blank">01:11:23.340</a></span> | <span class="t">So let's look at what that looks like in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4286" target="_blank">01:11:26.300</a></span> | <span class="t">So here we have a one-dimensional batch normalization layer, exactly as we are using here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4291" target="_blank">01:11:31.060</a></span> | <span class="t">And there are a number of keyword arguments going into it as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4293" target="_blank">01:11:33.660</a></span> | <span class="t">So we need to know the number of features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4295" target="_blank">01:11:35.820</a></span> | <span class="t">For us, that is 200.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4297" target="_blank">01:11:37.520</a></span> | <span class="t">And that is needed so that we can initialize these parameters here, the gain, the bias,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4302" target="_blank">01:11:42.580</a></span> | <span class="t">and the buffers for the running mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4307" target="_blank">01:11:47.140</a></span> | <span class="t">Then they need to know the value of epsilon here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4310" target="_blank">01:11:50.140</a></span> | <span class="t">And by default, this is 1.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4311" target="_blank">01:11:51.820</a></span> | <span class="t">You don't typically change this too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4314" target="_blank">01:11:54.080</a></span> | <span class="t">Then they need to know the momentum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4316" target="_blank">01:11:56.140</a></span> | <span class="t">And the momentum here, as they explain, is basically used for these running mean and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4321" target="_blank">01:12:01.160</a></span> | <span class="t">running standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4322" target="_blank">01:12:02.900</a></span> | <span class="t">So by default, the momentum here is 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4325" target="_blank">01:12:05.140</a></span> | <span class="t">The momentum we are using here in this example is 0.001.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4329" target="_blank">01:12:09.940</a></span> | <span class="t">And basically, you may want to change this sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4333" target="_blank">01:12:13.820</a></span> | <span class="t">And roughly speaking, if you have a very large batch size, then typically what you'll see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4338" target="_blank">01:12:18.740</a></span> | <span class="t">is that when you estimate the mean and standard deviation for every single batch size, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4342" target="_blank">01:12:22.780</a></span> | <span class="t">it's large enough, you're going to get roughly the same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4346" target="_blank">01:12:26.220</a></span> | <span class="t">And so therefore, you can use slightly higher momentum, like 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4351" target="_blank">01:12:31.240</a></span> | <span class="t">But for a batch size as small as 32, the mean and standard deviation here might take on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4356" target="_blank">01:12:36.700</a></span> | <span class="t">slightly different numbers, because there's only 32 examples we are using to estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4360" target="_blank">01:12:40.460</a></span> | <span class="t">the mean and standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4361" target="_blank">01:12:41.960</a></span> | <span class="t">So the value is changing around a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4364" target="_blank">01:12:44.340</a></span> | <span class="t">And if your momentum is 0.1, that might not be good enough for this value to settle and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4370" target="_blank">01:12:50.820</a></span> | <span class="t">converge to the actual mean and standard deviation over the entire training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4375" target="_blank">01:12:55.260</a></span> | <span class="t">And so basically, if your batch size is very small, momentum of 0.1 is potentially dangerous,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4379" target="_blank">01:12:59.920</a></span> | <span class="t">and it might make it so that the running mean and standard deviation is thrashing too much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4384" target="_blank">01:13:04.380</a></span> | <span class="t">during training, and it's not actually converging properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4389" target="_blank">01:13:09.740</a></span> | <span class="t">Affine equals true determines whether this batch normalization layer has these learnable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4393" target="_blank">01:13:13.980</a></span> | <span class="t">affine parameters, the gain and the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4398" target="_blank">01:13:18.700</a></span> | <span class="t">And this is almost always kept to true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4400" target="_blank">01:13:20.700</a></span> | <span class="t">I'm not actually sure why you would want to change this to false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4406" target="_blank">01:13:26.620</a></span> | <span class="t">Then track running stats is determining whether or not batch normalization layer of PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4410" target="_blank">01:13:30.780</a></span> | <span class="t">will be doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4413" target="_blank">01:13:33.060</a></span> | <span class="t">And one reason you may want to skip the running stats is because you may want to, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4419" target="_blank">01:13:39.540</a></span> | <span class="t">estimate them at the end as a stage two, like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4422" target="_blank">01:13:42.860</a></span> | <span class="t">And in that case, you don't want the batch normalization layer to be doing all this extra</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4426" target="_blank">01:13:46.100</a></span> | <span class="t">compute that you're not going to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4429" target="_blank">01:13:49.180</a></span> | <span class="t">And finally, we need to know which device we're going to run this batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4432" target="_blank">01:13:52.940</a></span> | <span class="t">on, a CPU or a GPU, and what the data type should be, half precision, single precision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4438" target="_blank">01:13:58.220</a></span> | <span class="t">double precision, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4441" target="_blank">01:14:01.220</a></span> | <span class="t">So that's the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4442" target="_blank">01:14:02.580</a></span> | <span class="t">Otherwise, they link to the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4443" target="_blank">01:14:03.820</a></span> | <span class="t">It's the same formula we've implemented, and everything is the same, exactly as we've done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4448" target="_blank">01:14:08.820</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4449" target="_blank">01:14:09.820</a></span> | <span class="t">Okay, so that's everything that I wanted to cover for this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4454" target="_blank">01:14:14.020</a></span> | <span class="t">Really what I wanted to talk about is the importance of understanding the activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4457" target="_blank">01:14:17.260</a></span> | <span class="t">and the gradients and their statistics in neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4460" target="_blank">01:14:20.720</a></span> | <span class="t">And this becomes increasingly important, especially as you make your neural networks bigger, larger,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4464" target="_blank">01:14:24.460</a></span> | <span class="t">and deeper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4465" target="_blank">01:14:25.460</a></span> | <span class="t">We looked at the distributions basically at the output layer, and we saw that if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4470" target="_blank">01:14:30.160</a></span> | <span class="t">two confident mispredictions because the activations are too messed up at the last layer, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4475" target="_blank">01:14:35.260</a></span> | <span class="t">end up with these hockey stick losses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4477" target="_blank">01:14:37.780</a></span> | <span class="t">And if you fix this, you get a better loss at the end of training because your training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4481" target="_blank">01:14:41.460</a></span> | <span class="t">is not doing wasteful work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4483" target="_blank">01:14:43.460</a></span> | <span class="t">Then we also saw that we need to control the activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4486" target="_blank">01:14:46.060</a></span> | <span class="t">We don't want them to squash to zero or explode to infinity, because that you can run into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4492" target="_blank">01:14:52.260</a></span> | <span class="t">a lot of trouble with all of these nonlinearities in these neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4496" target="_blank">01:14:56.020</a></span> | <span class="t">And basically you want everything to be fairly homogeneous throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4498" target="_blank">01:14:58.980</a></span> | <span class="t">You want roughly Gaussian activations throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4502" target="_blank">01:15:02.620</a></span> | <span class="t">Then we talked about, okay, if we want roughly Gaussian activations, how do we scale these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4507" target="_blank">01:15:07.780</a></span> | <span class="t">weight matrices and biases during initialization of the neural net so that we don't get, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4513" target="_blank">01:15:13.180</a></span> | <span class="t">know, so everything is as controlled as possible?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4517" target="_blank">01:15:17.420</a></span> | <span class="t">So that gave us a large boost and improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4520" target="_blank">01:15:20.100</a></span> | <span class="t">And then I talked about how that strategy is not actually possible for much, much deeper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4526" target="_blank">01:15:26.220</a></span> | <span class="t">neural nets, because when you have much deeper neural nets with lots of different types of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4531" target="_blank">01:15:31.260</a></span> | <span class="t">layers, it becomes really, really hard to precisely set the weights and the biases in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4536" target="_blank">01:15:36.460</a></span> | <span class="t">such a way that the activations are roughly uniform throughout the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4541" target="_blank">01:15:41.420</a></span> | <span class="t">So then I introduced the notion of a normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4544" target="_blank">01:15:44.580</a></span> | <span class="t">Now there are many normalization layers that people use in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4548" target="_blank">01:15:48.020</a></span> | <span class="t">Batch normalization, layer normalization, instance normalization, group normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4552" target="_blank">01:15:52.740</a></span> | <span class="t">We haven't covered most of them, but I've introduced the first one and also the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4556" target="_blank">01:15:56.580</a></span> | <span class="t">that I believe came out first, and that's called batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4560" target="_blank">01:16:00.820</a></span> | <span class="t">And we saw how batch normalization works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4563" target="_blank">01:16:03.060</a></span> | <span class="t">This is a layer that you can sprinkle throughout your deep neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4566" target="_blank">01:16:06.540</a></span> | <span class="t">And the basic idea is if you want roughly Gaussian activations, well then take your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4571" target="_blank">01:16:11.060</a></span> | <span class="t">activations and take the mean and the standard deviation and center your data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4576" target="_blank">01:16:16.740</a></span> | <span class="t">And you can do that because the centering operation is differentiable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4581" target="_blank">01:16:21.500</a></span> | <span class="t">But on top of that, we actually had to add a lot of bells and whistles, and that gave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4585" target="_blank">01:16:25.620</a></span> | <span class="t">you a sense of the complexities of the batch normalization layer, because now we're centering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4589" target="_blank">01:16:29.780</a></span> | <span class="t">the data, that's great, but suddenly we need the gain and the bias, and now those are trainable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4595" target="_blank">01:16:35.940</a></span> | <span class="t">And then because we are coupling all the training examples, now suddenly the question is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4599" target="_blank">01:16:39.740</a></span> | <span class="t">do you do the inference?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4600" target="_blank">01:16:40.740</a></span> | <span class="t">Well, to do the inference, we need to now estimate these mean and standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4607" target="_blank">01:16:47.300</a></span> | <span class="t">once over the entire training set, and then use those at inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4611" target="_blank">01:16:51.980</a></span> | <span class="t">But then no one likes to do stage two, so instead we fold everything into the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4616" target="_blank">01:16:56.140</a></span> | <span class="t">normalization layer during training and try to estimate these in a running manner so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4620" target="_blank">01:17:00.540</a></span> | <span class="t">everything is a bit simpler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4622" target="_blank">01:17:02.860</a></span> | <span class="t">And that gives us the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4626" target="_blank">01:17:06.580</a></span> | <span class="t">And as I mentioned, no one likes this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4629" target="_blank">01:17:09.540</a></span> | <span class="t">It causes a huge amount of bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4632" target="_blank">01:17:12.740</a></span> | <span class="t">And intuitively it's because it is coupling examples in the forward pass of the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4637" target="_blank">01:17:17.860</a></span> | <span class="t">net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4638" target="_blank">01:17:18.860</a></span> | <span class="t">And I've shot myself in the foot with this layer over and over again in my life, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4645" target="_blank">01:17:25.300</a></span> | <span class="t">I don't want you to suffer the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4648" target="_blank">01:17:28.460</a></span> | <span class="t">So basically try to avoid it as much as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4652" target="_blank">01:17:32.140</a></span> | <span class="t">Some of the other alternatives to these layers are, for example, group normalization or layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4655" target="_blank">01:17:35.580</a></span> | <span class="t">normalization, and those have become more common in more recent deep learning, but we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4661" target="_blank">01:17:41.420</a></span> | <span class="t">haven't covered those yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4663" target="_blank">01:17:43.340</a></span> | <span class="t">But definitely batch normalization was very influential at the time when it came out in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4666" target="_blank">01:17:46.900</a></span> | <span class="t">roughly 2015, because it was kind of the first time that you could train reliably much deeper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4674" target="_blank">01:17:54.180</a></span> | <span class="t">neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4675" target="_blank">01:17:55.460</a></span> | <span class="t">And fundamentally the reason for that is because this layer was very effective at controlling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4679" target="_blank">01:17:59.780</a></span> | <span class="t">the statistics of the activations in the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4683" target="_blank">01:18:03.260</a></span> | <span class="t">So that's the story so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4685" target="_blank">01:18:05.580</a></span> | <span class="t">And that's all I wanted to cover.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4688" target="_blank">01:18:08.000</a></span> | <span class="t">And in the future lectures, hopefully we can start going into recurrent neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4691" target="_blank">01:18:11.860</a></span> | <span class="t">And recurrent neural nets, as we'll see, are just very, very deep networks, because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4697" target="_blank">01:18:17.460</a></span> | <span class="t">unroll the loop when you actually optimize these neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4701" target="_blank">01:18:21.660</a></span> | <span class="t">And that's where a lot of this analysis around the activation statistics and all these normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4708" target="_blank">01:18:28.620</a></span> | <span class="t">layers will become very, very important for good performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4712" target="_blank">01:18:32.820</a></span> | <span class="t">So we'll see that next time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4713" target="_blank">01:18:33.820</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4714" target="_blank">01:18:34.820</a></span> | <span class="t">Okay, so I lied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4715" target="_blank">01:18:35.820</a></span> | <span class="t">I would like us to do one more summary here as a bonus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4719" target="_blank">01:18:39.420</a></span> | <span class="t">And I think it's useful as to have one more summary of everything I've presented in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4723" target="_blank">01:18:43.060</a></span> | <span class="t">lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4724" target="_blank">01:18:44.060</a></span> | <span class="t">But also I would like us to start PyTorchifying our code a little bit, so it looks much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4727" target="_blank">01:18:47.940</a></span> | <span class="t">like what you would encounter in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4730" target="_blank">01:18:50.420</a></span> | <span class="t">So you'll see that I will structure our code into these modules, like a linear module and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4737" target="_blank">01:18:57.060</a></span> | <span class="t">a batch form module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4738" target="_blank">01:18:58.740</a></span> | <span class="t">And I'm putting the code inside these modules so that we can construct neural networks very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4742" target="_blank">01:19:02.920</a></span> | <span class="t">much like we would construct them in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4744" target="_blank">01:19:04.820</a></span> | <span class="t">And I will go through this in detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4746" target="_blank">01:19:06.780</a></span> | <span class="t">So we'll create our neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4748" target="_blank">01:19:08.940</a></span> | <span class="t">Then we will do the optimization loop, as we did before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4752" target="_blank">01:19:12.740</a></span> | <span class="t">And then the one more thing that I want to do here is I want to look at the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4755" target="_blank">01:19:15.500</a></span> | <span class="t">statistics, both in the forward pass and in the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4759" target="_blank">01:19:19.460</a></span> | <span class="t">And then here we have the evaluation and sampling just like before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4763" target="_blank">01:19:23.060</a></span> | <span class="t">So let me rewind all the way up here and go a little bit slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4766" target="_blank">01:19:26.920</a></span> | <span class="t">So here I am creating a linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4769" target="_blank">01:19:29.420</a></span> | <span class="t">You'll notice that torch.nn has lots of different types of layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4772" target="_blank">01:19:32.820</a></span> | <span class="t">And one of those layers is the linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4774" target="_blank">01:19:34.700</a></span> | <span class="t">torch.nn.linear takes a number of input features, output features, whether or not we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4778" target="_blank">01:19:38.860</a></span> | <span class="t">have a bias, and then the device that we want to place this layer on, and the data type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4784" target="_blank">01:19:44.020</a></span> | <span class="t">So I will omit these two, but otherwise we have the exact same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4788" target="_blank">01:19:48.460</a></span> | <span class="t">We have the fan_in, which is the number of inputs, fan_out, the number of outputs, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4793" target="_blank">01:19:53.580</a></span> | <span class="t">whether or not we want to use a bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4795" target="_blank">01:19:55.420</a></span> | <span class="t">And internally inside this layer, there's a weight and a bias, if you'd like it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4799" target="_blank">01:19:59.940</a></span> | <span class="t">It is typical to initialize the weight using, say, random numbers drawn from a Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4806" target="_blank">01:20:06.080</a></span> | <span class="t">And then here's the kyming initialization that we discussed already in this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4810" target="_blank">01:20:10.720</a></span> | <span class="t">And that's a good default, and also the default that I believe PyTorch uses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4814" target="_blank">01:20:14.900</a></span> | <span class="t">And by default, the bias is usually initialized to zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4818" target="_blank">01:20:18.420</a></span> | <span class="t">Now when you call this module, this will basically calculate w times x plus b, if you have nb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4824" target="_blank">01:20:24.940</a></span> | <span class="t">And then when you also call .parameters on this module, it will return the tensors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4830" target="_blank">01:20:30.020</a></span> | <span class="t">are the parameters of this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4832" target="_blank">01:20:32.260</a></span> | <span class="t">Now next, we have the batch normalization layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4834" target="_blank">01:20:34.620</a></span> | <span class="t">So I've written that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4837" target="_blank">01:20:37.220</a></span> | <span class="t">And this is very similar to PyTorch's nn.batchnorm1d layer, as shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4844" target="_blank">01:20:44.500</a></span> | <span class="t">So I'm kind of taking these three parameters here, the dimensionality, the epsilon that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4849" target="_blank">01:20:49.900</a></span> | <span class="t">we'll use in the division, and the momentum that we will use in keeping track of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4854" target="_blank">01:20:54.180</a></span> | <span class="t">running stats, the running mean and the running variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4858" target="_blank">01:20:58.180</a></span> | <span class="t">Now PyTorch actually takes quite a few more things, but I'm assuming some of their settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4862" target="_blank">01:21:02.340</a></span> | <span class="t">So for us, alphan will be true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4863" target="_blank">01:21:03.980</a></span> | <span class="t">That means that we will be using a gamma and beta after the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4868" target="_blank">01:21:08.020</a></span> | <span class="t">The track running stats will be true, so we will be keeping track of the running mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4871" target="_blank">01:21:11.300</a></span> | <span class="t">and the running variance in the batch norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4874" target="_blank">01:21:14.660</a></span> | <span class="t">Our device by default is the CPU, and the data type by default is float, float32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4882" target="_blank">01:21:22.260</a></span> | <span class="t">So those are the defaults.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4883" target="_blank">01:21:23.660</a></span> | <span class="t">Otherwise, we are taking all the same parameters in this batch norm layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4887" target="_blank">01:21:27.500</a></span> | <span class="t">So first, I'm just saving them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4889" target="_blank">01:21:29.980</a></span> | <span class="t">Now here's something new.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4890" target="_blank">01:21:30.980</a></span> | <span class="t">There's a .training, which by default is true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4893" target="_blank">01:21:33.620</a></span> | <span class="t">And PyTorch nn modules also have this attribute, .training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4897" target="_blank">01:21:37.140</a></span> | <span class="t">And that's because many modules, and batch norm is included in that, have a different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4902" target="_blank">01:21:42.100</a></span> | <span class="t">behavior whether you are training your neural net or whether you are running it in an evaluation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4906" target="_blank">01:21:46.940</a></span> | <span class="t">mode and calculating your evaluation loss or using it for inference on some test examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4913" target="_blank">01:21:53.060</a></span> | <span class="t">And batch norm is an example of this, because when we are training, we are going to be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4916" target="_blank">01:21:56.860</a></span> | <span class="t">the mean and the variance estimated from the current batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4919" target="_blank">01:21:59.820</a></span> | <span class="t">But during inference, we are using the running mean and running variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4924" target="_blank">01:22:04.140</a></span> | <span class="t">And so also, if we are training, we are updating mean and variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4927" target="_blank">01:22:07.980</a></span> | <span class="t">But if we are testing, then these are not being updated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4930" target="_blank">01:22:10.100</a></span> | <span class="t">They're kept fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4931" target="_blank">01:22:11.880</a></span> | <span class="t">And so this flag is necessary and by default true, just like in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4936" target="_blank">01:22:16.460</a></span> | <span class="t">Now the parameters of batch norm 1D are the gamma and the beta here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4941" target="_blank">01:22:21.940</a></span> | <span class="t">And then the running mean and the running variance are called buffers in PyTorch nomenclature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4947" target="_blank">01:22:27.780</a></span> | <span class="t">And these buffers are trained using exponential moving average here explicitly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4953" target="_blank">01:22:33.580</a></span> | <span class="t">And they are not part of the backpropagation and stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4957" target="_blank">01:22:37.100</a></span> | <span class="t">So they are not parameters of this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4959" target="_blank">01:22:39.960</a></span> | <span class="t">And that's why when we have parameters here, we only return gamma and beta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4964" target="_blank">01:22:44.700</a></span> | <span class="t">We do not return the mean and the variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4966" target="_blank">01:22:46.740</a></span> | <span class="t">This is trained internally here, every forward pass, using exponential moving average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4974" target="_blank">01:22:54.700</a></span> | <span class="t">So that's the initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4975" target="_blank">01:22:55.700</a></span> | <span class="t">Now in a forward pass, if we are training, then we use the mean and the variance estimated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4981" target="_blank">01:23:01.900</a></span> | <span class="t">by the batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4982" target="_blank">01:23:02.900</a></span> | <span class="t">Let me pull up the paper here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4985" target="_blank">01:23:05.980</a></span> | <span class="t">We calculate the mean and the variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4988" target="_blank">01:23:08.940</a></span> | <span class="t">Now up above, I was estimating the standard deviation and keeping track of the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4993" target="_blank">01:23:13.260</a></span> | <span class="t">deviation here in the running standard deviation instead of running variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=4998" target="_blank">01:23:18.220</a></span> | <span class="t">But let's follow the paper exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5000" target="_blank">01:23:20.300</a></span> | <span class="t">Here they calculate the variance, which is the standard deviation squared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5004" target="_blank">01:23:24.000</a></span> | <span class="t">And that's what's kept track of in the running variance instead of the running standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5009" target="_blank">01:23:29.900</a></span> | <span class="t">But those two would be very, very similar, I believe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5013" target="_blank">01:23:33.980</a></span> | <span class="t">If we are not training, then we use the running mean and variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5016" target="_blank">01:23:36.860</a></span> | <span class="t">We normalize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5019" target="_blank">01:23:39.180</a></span> | <span class="t">And then here, I'm calculating the output of this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5022" target="_blank">01:23:42.140</a></span> | <span class="t">And I'm also assigning it to an attribute called dot out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5025" target="_blank">01:23:45.540</a></span> | <span class="t">Now dot out is something that I'm using in our modules here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5029" target="_blank">01:23:49.660</a></span> | <span class="t">This is not what you would find in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5031" target="_blank">01:23:51.420</a></span> | <span class="t">We are slightly deviating from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5033" target="_blank">01:23:53.100</a></span> | <span class="t">I'm creating a dot out because I would like to very easily maintain all those variables</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5038" target="_blank">01:23:58.660</a></span> | <span class="t">so that we can create statistics of them and plot them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5041" target="_blank">01:24:01.500</a></span> | <span class="t">But PyTorch and modules will not have a dot out attribute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5045" target="_blank">01:24:05.500</a></span> | <span class="t">And finally here, we are updating the buffers using, again, as I mentioned, exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5049" target="_blank">01:24:09.020</a></span> | <span class="t">moving average given the provided momentum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5053" target="_blank">01:24:13.100</a></span> | <span class="t">And importantly, you'll notice that I'm using the torch.nograd context manager.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5057" target="_blank">01:24:17.420</a></span> | <span class="t">And I'm doing this because if we don't use this, then PyTorch will start building out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5061" target="_blank">01:24:21.580</a></span> | <span class="t">an entire computational graph out of these tensors because it is expecting that we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5065" target="_blank">01:24:25.940</a></span> | <span class="t">eventually call dot backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5068" target="_blank">01:24:28.120</a></span> | <span class="t">But we are never going to be calling dot backward on anything that includes running mean and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5071" target="_blank">01:24:31.380</a></span> | <span class="t">running variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5072" target="_blank">01:24:32.700</a></span> | <span class="t">So that's why we need to use this context manager so that we are not sort of maintaining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5077" target="_blank">01:24:37.240</a></span> | <span class="t">and using all this additional memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5080" target="_blank">01:24:40.500</a></span> | <span class="t">So this will make it more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5081" target="_blank">01:24:41.620</a></span> | <span class="t">And it's just telling PyTorch that there will be no backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5084" target="_blank">01:24:44.180</a></span> | <span class="t">We just have a bunch of tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5085" target="_blank">01:24:45.380</a></span> | <span class="t">We want to update them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5086" target="_blank">01:24:46.500</a></span> | <span class="t">That's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5088" target="_blank">01:24:48.000</a></span> | <span class="t">And then we return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5089" target="_blank">01:24:49.340</a></span> | <span class="t">OK, now scrolling down, we have the 10H layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5092" target="_blank">01:24:52.940</a></span> | <span class="t">This is very, very similar to torch.10H.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5096" target="_blank">01:24:56.180</a></span> | <span class="t">And it doesn't do too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5097" target="_blank">01:24:57.820</a></span> | <span class="t">It just calculates 10H, as you might expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5100" target="_blank">01:25:00.580</a></span> | <span class="t">So that's torch.10H.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5102" target="_blank">01:25:02.860</a></span> | <span class="t">And there's no parameters in this layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5105" target="_blank">01:25:05.380</a></span> | <span class="t">But because these are layers, it now becomes very easy to sort of stack them up into basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5111" target="_blank">01:25:11.180</a></span> | <span class="t">just a list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5113" target="_blank">01:25:13.580</a></span> | <span class="t">And we can do all the initializations that we're used to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5116" target="_blank">01:25:16.400</a></span> | <span class="t">So we have the initial sort of embedding matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5119" target="_blank">01:25:19.580</a></span> | <span class="t">We have our layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5120" target="_blank">01:25:20.580</a></span> | <span class="t">And we can call them sequentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5122" target="_blank">01:25:22.380</a></span> | <span class="t">And then again, with torch.nograd, there's some initializations here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5126" target="_blank">01:25:26.260</a></span> | <span class="t">So we want to make the outputs of max a bit less confident, like we saw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5130" target="_blank">01:25:30.500</a></span> | <span class="t">And in addition to that, because we are using a six-layer multilayer perceptron here-- so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5134" target="_blank">01:25:34.900</a></span> | <span class="t">you see how I'm stacking linear, 10H, linear, 10H, et cetera-- I'm going to be using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5140" target="_blank">01:25:40.200</a></span> | <span class="t">gain here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5141" target="_blank">01:25:41.460</a></span> | <span class="t">And I'm going to play with this in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5142" target="_blank">01:25:42.940</a></span> | <span class="t">So you'll see how when we change this, what happens to the statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5146" target="_blank">01:25:46.780</a></span> | <span class="t">Finally, the parameters are basically the embedding matrix and all the parameters in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5151" target="_blank">01:25:51.300</a></span> | <span class="t">all the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5152" target="_blank">01:25:52.540</a></span> | <span class="t">And notice here, I'm using a double list comprehension, if you want to call it that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5156" target="_blank">01:25:56.220</a></span> | <span class="t">But for every layer in layers and for every parameter in each of those layers, we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5160" target="_blank">01:26:00.940</a></span> | <span class="t">just stacking up all those p's, all those parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5165" target="_blank">01:26:05.100</a></span> | <span class="t">Now in total, we have 46,000 parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5169" target="_blank">01:26:09.520</a></span> | <span class="t">And I'm telling PyTorch that all of them require gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5176" target="_blank">01:26:16.140</a></span> | <span class="t">Then here, we have everything here we are actually mostly used to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5180" target="_blank">01:26:20.780</a></span> | <span class="t">We are sampling batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5182" target="_blank">01:26:22.140</a></span> | <span class="t">We are doing forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5183" target="_blank">01:26:23.580</a></span> | <span class="t">The forward pass now is just a linear application of all the layers in order, followed by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5187" target="_blank">01:26:27.900</a></span> | <span class="t">cross entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5189" target="_blank">01:26:29.500</a></span> | <span class="t">And then in the backward pass, you'll notice that for every single layer, I now iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5192" target="_blank">01:26:32.740</a></span> | <span class="t">over all the outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5194" target="_blank">01:26:34.220</a></span> | <span class="t">And I'm telling PyTorch to retain the gradient of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5197" target="_blank">01:26:37.540</a></span> | <span class="t">And then here, we are already used to all the gradients set to none, do the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5202" target="_blank">01:26:42.260</a></span> | <span class="t">to fill in the gradients, do an update using stochastic gradient send, and then track some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5207" target="_blank">01:26:47.340</a></span> | <span class="t">statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5208" target="_blank">01:26:48.860</a></span> | <span class="t">And then I am going to break after a single iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5212" target="_blank">01:26:52.100</a></span> | <span class="t">Now here in this cell, in this diagram, I am visualizing the histograms of the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5217" target="_blank">01:26:57.220</a></span> | <span class="t">pass activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5218" target="_blank">01:26:58.780</a></span> | <span class="t">And I am specifically doing it at the 10-H layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5221" target="_blank">01:27:01.920</a></span> | <span class="t">So iterating over all the layers, except for the very last one, which is basically just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5226" target="_blank">01:27:06.260</a></span> | <span class="t">the softmax layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5230" target="_blank">01:27:10.260</a></span> | <span class="t">If it is a 10-H layer, and I'm using a 10-H layer just because they have a finite output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5234" target="_blank">01:27:14.500</a></span> | <span class="t">negative one to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5235" target="_blank">01:27:15.500</a></span> | <span class="t">And so it's very easy to visualize here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5237" target="_blank">01:27:17.180</a></span> | <span class="t">So you see negative one to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5238" target="_blank">01:27:18.700</a></span> | <span class="t">And it's a finite range and easy to work with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5241" target="_blank">01:27:21.740</a></span> | <span class="t">I take the out tensor from that layer into T. And then I'm calculating the mean, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5247" target="_blank">01:27:27.020</a></span> | <span class="t">standard deviation, and the percent saturation of T. And the way I define the percent saturation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5252" target="_blank">01:27:32.260</a></span> | <span class="t">is that T dot absolute value is greater than 0.97.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5255" target="_blank">01:27:35.540</a></span> | <span class="t">So that means we are here at the tails of the 10-H. And remember that when we are in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5259" target="_blank">01:27:39.660</a></span> | <span class="t">the tails of the 10-H, that will actually stop gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5262" target="_blank">01:27:42.920</a></span> | <span class="t">So we don't want this to be too high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5265" target="_blank">01:27:45.660</a></span> | <span class="t">Now here, I'm calling torch dot histogram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5269" target="_blank">01:27:49.180</a></span> | <span class="t">And then I am plotting this histogram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5271" target="_blank">01:27:51.300</a></span> | <span class="t">So basically what this is doing is that every different type of layer-- and they all have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5274" target="_blank">01:27:54.340</a></span> | <span class="t">a different color-- we are looking at how many values in these tensors take on any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5280" target="_blank">01:28:00.860</a></span> | <span class="t">the values below on this axis here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5284" target="_blank">01:28:04.280</a></span> | <span class="t">So the first layer is fairly saturated here at 20%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5288" target="_blank">01:28:08.060</a></span> | <span class="t">So you can see that it's got tails here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5290" target="_blank">01:28:10.580</a></span> | <span class="t">But then everything sort of stabilizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5292" target="_blank">01:28:12.620</a></span> | <span class="t">And if we had more layers here, it would actually just stabilize at around the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5295" target="_blank">01:28:15.980</a></span> | <span class="t">of about 0.65.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5298" target="_blank">01:28:18.140</a></span> | <span class="t">And the saturation would be roughly 5%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5300" target="_blank">01:28:20.860</a></span> | <span class="t">And the reason that this stabilizes and gives us a nice distribution here is because gain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5305" target="_blank">01:28:25.300</a></span> | <span class="t">is set to 5/3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5307" target="_blank">01:28:27.820</a></span> | <span class="t">Now here, this gain, you see that by default, we initialize with 1 over square root of fan</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5314" target="_blank">01:28:34.420</a></span> | <span class="t">in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5315" target="_blank">01:28:35.420</a></span> | <span class="t">But then here during initialization, I come in and I iterate over all the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5318" target="_blank">01:28:38.860</a></span> | <span class="t">And if it's a linear layer, I boost that by the gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5322" target="_blank">01:28:42.500</a></span> | <span class="t">Now we saw that 1-- so basically, if we just do not use a gain, then what happens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5328" target="_blank">01:28:48.860</a></span> | <span class="t">If I redraw this, you will see that the standard deviation is shrinking and the saturation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5335" target="_blank">01:28:55.340</a></span> | <span class="t">is coming to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5337" target="_blank">01:28:57.180</a></span> | <span class="t">And basically what's happening is the first layer is pretty decent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5341" target="_blank">01:29:01.060</a></span> | <span class="t">But then further layers are just kind of like shrinking down to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5345" target="_blank">01:29:05.060</a></span> | <span class="t">And it's happening slowly, but it's shrinking to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5347" target="_blank">01:29:07.760</a></span> | <span class="t">And the reason for that is when you just have a sandwich of linear layers alone, then initializing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5355" target="_blank">01:29:15.880</a></span> | <span class="t">our weights in this manner we saw previously would have conserved the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5361" target="_blank">01:29:21.020</a></span> | <span class="t">of 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5362" target="_blank">01:29:22.260</a></span> | <span class="t">But because we have this interspersed tanh layers in there, these tanh layers are squashing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5368" target="_blank">01:29:28.620</a></span> | <span class="t">functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5369" target="_blank">01:29:29.620</a></span> | <span class="t">And so they take your distribution and they slightly squash it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5373" target="_blank">01:29:33.020</a></span> | <span class="t">And so some gain is necessary to keep expanding it to fight the squashing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5380" target="_blank">01:29:40.080</a></span> | <span class="t">So it just turns out that 5/3 is a good value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5383" target="_blank">01:29:43.620</a></span> | <span class="t">So if we have something too small like 1, we saw that things will come towards 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5389" target="_blank">01:29:49.140</a></span> | <span class="t">But if it's something too high, let's do 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5392" target="_blank">01:29:52.540</a></span> | <span class="t">Then here we see that-- well, let me do something a bit more extreme so it's a bit more visible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5400" target="_blank">01:30:00.560</a></span> | <span class="t">Let's try 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5401" target="_blank">01:30:01.560</a></span> | <span class="t">OK, so we see here that the saturations are starting to be way too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5407" target="_blank">01:30:07.140</a></span> | <span class="t">So 3 would create way too saturated activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5410" target="_blank">01:30:10.980</a></span> | <span class="t">So 5/3 is a good setting for a sandwich of linear layers with tanh activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5417" target="_blank">01:30:17.940</a></span> | <span class="t">And it roughly stabilizes the standard deviation at a reasonable point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5422" target="_blank">01:30:22.060</a></span> | <span class="t">Now, honestly, I have no idea where 5/3 came from in PyTorch when we were looking at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5427" target="_blank">01:30:27.980</a></span> | <span class="t">counting initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5430" target="_blank">01:30:30.020</a></span> | <span class="t">I see empirically that it stabilizes this sandwich of linear and tanh and that the saturation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5435" target="_blank">01:30:35.180</a></span> | <span class="t">is in a good range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5436" target="_blank">01:30:36.940</a></span> | <span class="t">But I don't actually know if this came out of some math formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5439" target="_blank">01:30:39.500</a></span> | <span class="t">I tried searching briefly for where this comes from, but I wasn't able to find anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5444" target="_blank">01:30:44.940</a></span> | <span class="t">But certainly we see that empirically these are very nice ranges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5447" target="_blank">01:30:47.460</a></span> | <span class="t">Our saturation is roughly 5%, which is a pretty good number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5451" target="_blank">01:30:51.100</a></span> | <span class="t">And this is a good setting of the gain in this context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5455" target="_blank">01:30:55.260</a></span> | <span class="t">Similarly, we can do the exact same thing with the gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5458" target="_blank">01:30:58.380</a></span> | <span class="t">So here is a very same loop if it's a tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5461" target="_blank">01:31:01.540</a></span> | <span class="t">But instead of taking the layer dot out, I'm taking the grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5464" target="_blank">01:31:04.500</a></span> | <span class="t">And then I'm also showing the mean and the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5467" target="_blank">01:31:07.340</a></span> | <span class="t">And I'm plotting the histogram of these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5470" target="_blank">01:31:10.060</a></span> | <span class="t">And so you'll see that the gradient distribution is fairly reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5473" target="_blank">01:31:13.640</a></span> | <span class="t">And in particular, what we're looking for is that all the different layers in this sandwich</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5477" target="_blank">01:31:17.700</a></span> | <span class="t">has roughly the same gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5479" target="_blank">01:31:19.700</a></span> | <span class="t">Things are not shrinking or exploding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5482" target="_blank">01:31:22.060</a></span> | <span class="t">So we can, for example, come here and we can take a look at what happens if this gain was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5486" target="_blank">01:31:26.140</a></span> | <span class="t">way too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5487" target="_blank">01:31:27.520</a></span> | <span class="t">So this was 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5490" target="_blank">01:31:30.740</a></span> | <span class="t">Then you see the first of all, the activations are shrinking to zero, but also the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5495" target="_blank">01:31:35.140</a></span> | <span class="t">are doing something weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5496" target="_blank">01:31:36.460</a></span> | <span class="t">The gradient started off here, and then now they're expanding out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5501" target="_blank">01:31:41.460</a></span> | <span class="t">And similarly, if we, for example, have a too high of a gain, so like 3, then we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5506" target="_blank">01:31:46.900</a></span> | <span class="t">that also the gradients have-- there's some asymmetry going on where as you go into deeper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5510" target="_blank">01:31:50.900</a></span> | <span class="t">and deeper layers, the activations are also changing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5514" target="_blank">01:31:54.180</a></span> | <span class="t">And so that's not what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5515" target="_blank">01:31:55.540</a></span> | <span class="t">And in this case, we saw that without the use of BatchNorm, as we are going through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5519" target="_blank">01:31:59.340</a></span> | <span class="t">right now, we have to very carefully set those gains to get nice activations in both the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5525" target="_blank">01:32:05.580</a></span> | <span class="t">forward pass and the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5527" target="_blank">01:32:07.620</a></span> | <span class="t">Now before we move on to BatchNormalization, I would also like to take a look at what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5531" target="_blank">01:32:11.900</a></span> | <span class="t">when we have no 10H units here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5534" target="_blank">01:32:14.040</a></span> | <span class="t">So erasing all the 10H nonlinearities, but keeping the gain at 5/3, we now have just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5540" target="_blank">01:32:20.000</a></span> | <span class="t">a giant linear sandwich.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5542" target="_blank">01:32:22.160</a></span> | <span class="t">So let's see what happens to the activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5544" target="_blank">01:32:24.380</a></span> | <span class="t">As we saw before, the correct gain here is 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5547" target="_blank">01:32:27.580</a></span> | <span class="t">That is the standard deviation preserving gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5549" target="_blank">01:32:29.760</a></span> | <span class="t">So 1.667 is too high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5553" target="_blank">01:32:33.740</a></span> | <span class="t">And so what's going to happen now is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5557" target="_blank">01:32:37.020</a></span> | <span class="t">I have to change this to be linear, because there's no more 10H layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5563" target="_blank">01:32:43.140</a></span> | <span class="t">And let me change this to linear as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5566" target="_blank">01:32:46.140</a></span> | <span class="t">So what we're seeing is the activations started out on the blue and have, by layer four, become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5574" target="_blank">01:32:54.140</a></span> | <span class="t">very diffuse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5575" target="_blank">01:32:55.220</a></span> | <span class="t">So what's happening to the activations is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5577" target="_blank">01:32:57.980</a></span> | <span class="t">And with the gradients on the top layer, the activation, the gradient statistics are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5583" target="_blank">01:33:03.580</a></span> | <span class="t">purple, and then they diminish as you go down deeper in the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5587" target="_blank">01:33:07.740</a></span> | <span class="t">And so basically you have an asymmetry in the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5590" target="_blank">01:33:10.980</a></span> | <span class="t">And you might imagine that if you have very deep neural networks, say like 50 layers or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5593" target="_blank">01:33:13.960</a></span> | <span class="t">something like that, this is not a good place to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5598" target="_blank">01:33:18.900</a></span> | <span class="t">So that's why before BatchNormalization, this was incredibly tricky to set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5604" target="_blank">01:33:24.260</a></span> | <span class="t">In particular, if this is too large of a gain, this happens, and if it's too little of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5608" target="_blank">01:33:28.060</a></span> | <span class="t">gain, then this happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5611" target="_blank">01:33:31.540</a></span> | <span class="t">So the opposite of that basically happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5613" target="_blank">01:33:33.580</a></span> | <span class="t">Here we have a shrinking and a diffusion, depending on which direction you look at it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5620" target="_blank">01:33:40.980</a></span> | <span class="t">from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5621" target="_blank">01:33:41.980</a></span> | <span class="t">And so certainly this is not what you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5624" target="_blank">01:33:44.260</a></span> | <span class="t">And in this case, the correct setting of the gain is exactly 1, just like we're doing at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5629" target="_blank">01:33:49.100</a></span> | <span class="t">initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5630" target="_blank">01:33:50.300</a></span> | <span class="t">And then we see that the statistics for the forward and the backward paths are well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5636" target="_blank">01:33:56.300</a></span> | <span class="t">And so the reason I want to show you this is that basically getting neural nets to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5642" target="_blank">01:34:02.540</a></span> | <span class="t">before these normalization layers and before the use of advanced optimizers like Adam,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5647" target="_blank">01:34:07.020</a></span> | <span class="t">which we still have to cover, and residual connections and so on, training neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5651" target="_blank">01:34:11.460</a></span> | <span class="t">basically looked like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5652" target="_blank">01:34:12.460</a></span> | <span class="t">It's like a total balancing act.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5655" target="_blank">01:34:15.020</a></span> | <span class="t">You have to make sure that everything is precisely orchestrated, and you have to care about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5658" target="_blank">01:34:18.940</a></span> | <span class="t">activations and the gradients and their statistics, and then maybe you can train something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5663" target="_blank">01:34:23.620</a></span> | <span class="t">But it was basically impossible to train very deep networks, and this is fundamentally the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5666" target="_blank">01:34:26.900</a></span> | <span class="t">reason for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5668" target="_blank">01:34:28.300</a></span> | <span class="t">You'd have to be very, very careful with your initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5672" target="_blank">01:34:32.300</a></span> | <span class="t">The other point here is, you might be asking yourself, by the way, I'm not sure if I covered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5676" target="_blank">01:34:36.340</a></span> | <span class="t">this, why do we need these 10H layers at all?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5680" target="_blank">01:34:40.860</a></span> | <span class="t">Why do we include them and then have to worry about the gain?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5683" target="_blank">01:34:43.820</a></span> | <span class="t">And the reason for that, of course, is that if you just have a stack of linear layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5687" target="_blank">01:34:47.980</a></span> | <span class="t">then certainly we're getting very easily nice activations and so on, but this is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5693" target="_blank">01:34:53.300</a></span> | <span class="t">massive linear sandwich, and it turns out that it collapses to a single linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5697" target="_blank">01:34:57.260</a></span> | <span class="t">in terms of its representation power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5699" target="_blank">01:34:59.820</a></span> | <span class="t">So if you were to plot the output as a function of the input, you're just getting a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5703" target="_blank">01:35:03.660</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5704" target="_blank">01:35:04.660</a></span> | <span class="t">No matter how many linear layers you stack up, you still just end up with a linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5709" target="_blank">01:35:09.100</a></span> | <span class="t">All the WX plus Bs just collapse into a large WX plus B with slightly different Ws and slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5715" target="_blank">01:35:15.380</a></span> | <span class="t">different B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5716" target="_blank">01:35:16.380</a></span> | <span class="t">But interestingly, even though the forward pass collapses to just a linear layer, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5721" target="_blank">01:35:21.740</a></span> | <span class="t">of back propagation and the dynamics of the backward pass, the optimization actually is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5727" target="_blank">01:35:27.140</a></span> | <span class="t">not identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5728" target="_blank">01:35:28.700</a></span> | <span class="t">You actually end up with all kinds of interesting dynamics in the backward pass because of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5735" target="_blank">01:35:35.780</a></span> | <span class="t">way the chain rule is calculating it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5737" target="_blank">01:35:37.980</a></span> | <span class="t">And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5743" target="_blank">01:35:43.920</a></span> | <span class="t">in both cases those are just a linear transformation in the forward pass, but the training dynamics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5747" target="_blank">01:35:47.620</a></span> | <span class="t">would be different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5748" target="_blank">01:35:48.680</a></span> | <span class="t">And there's entire papers that analyze, in fact, infinitely layered linear layers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5753" target="_blank">01:35:53.660</a></span> | <span class="t">so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5754" target="_blank">01:35:54.660</a></span> | <span class="t">And so there's a lot of things that you can play with there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5758" target="_blank">01:35:58.820</a></span> | <span class="t">But basically the 10-H nonlinearities allow us to turn this sandwich from just a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5768" target="_blank">01:36:08.780</a></span> | <span class="t">chain into a neural network that can, in principle, approximate any arbitrary function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5774" target="_blank">01:36:14.820</a></span> | <span class="t">Okay, so now I've reset the code to use the linear 10-H sandwich like before, and I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5780" target="_blank">01:36:20.860</a></span> | <span class="t">reset everything so the gain is 5 over 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5784" target="_blank">01:36:24.020</a></span> | <span class="t">We can run a single step of optimization and we can look at the activation statistics of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5788" target="_blank">01:36:28.340</a></span> | <span class="t">the forward pass and the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5790" target="_blank">01:36:30.660</a></span> | <span class="t">But I've added one more plot here that I think is really important to look at when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5793" target="_blank">01:36:33.900</a></span> | <span class="t">training your neural nets and to consider.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5796" target="_blank">01:36:36.400</a></span> | <span class="t">And ultimately what we're doing is we're updating the parameters of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5800" target="_blank">01:36:40.240</a></span> | <span class="t">So we care about the parameters and their values and their gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5804" target="_blank">01:36:44.560</a></span> | <span class="t">So here what I'm doing is I'm actually iterating over all the parameters available and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5808" target="_blank">01:36:48.240</a></span> | <span class="t">I'm only restricting it to the two-dimensional parameters, which are basically the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5813" target="_blank">01:36:53.360</a></span> | <span class="t">of these linear layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5814" target="_blank">01:36:54.880</a></span> | <span class="t">And I'm skipping the biases and I'm skipping the gammas and the betas and the bastrom just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5820" target="_blank">01:37:00.480</a></span> | <span class="t">for simplicity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5821" target="_blank">01:37:01.480</a></span> | <span class="t">But you can also take a look at those as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5824" target="_blank">01:37:04.280</a></span> | <span class="t">But what's happening with the weights is instructive by itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5829" target="_blank">01:37:09.080</a></span> | <span class="t">So here we have all the different weights, their shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5832" target="_blank">01:37:12.940</a></span> | <span class="t">So this is the embedding layer, the first linear layer, all the way to the very last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5836" target="_blank">01:37:16.600</a></span> | <span class="t">linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5837" target="_blank">01:37:17.600</a></span> | <span class="t">And then we have the mean, the standard deviation of all these parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5842" target="_blank">01:37:22.120</a></span> | <span class="t">The histogram, and you can see that it actually doesn't look that amazing, so there's some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5845" target="_blank">01:37:25.320</a></span> | <span class="t">trouble in paradise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5846" target="_blank">01:37:26.860</a></span> | <span class="t">Even though these gradients looked okay, there's something weird going on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5850" target="_blank">01:37:30.360</a></span> | <span class="t">I'll get to that in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5852" target="_blank">01:37:32.280</a></span> | <span class="t">And the last thing here is the gradient to data ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5856" target="_blank">01:37:36.000</a></span> | <span class="t">So sometimes I like to visualize this as well because what this gives you a sense of is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5860" target="_blank">01:37:40.440</a></span> | <span class="t">what is the scale of the gradient compared to the scale of the actual values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5865" target="_blank">01:37:45.840</a></span> | <span class="t">And this is important because we're going to end up taking a step update that is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5871" target="_blank">01:37:51.000</a></span> | <span class="t">learning rate times the gradient onto the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5874" target="_blank">01:37:54.280</a></span> | <span class="t">And so if the gradient has too large of a magnitude, if the numbers in there are too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5877" target="_blank">01:37:57.680</a></span> | <span class="t">large compared to the numbers in data, then you'd be in trouble.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5881" target="_blank">01:38:01.860</a></span> | <span class="t">But in this case, the gradient to data is our low numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5885" target="_blank">01:38:05.480</a></span> | <span class="t">So the values inside grad are 1000 times smaller than the values inside data in these weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5892" target="_blank">01:38:12.600</a></span> | <span class="t">most of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5894" target="_blank">01:38:14.000</a></span> | <span class="t">Now notably, that is not true about the last layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5897" target="_blank">01:38:17.340</a></span> | <span class="t">And so the last layer actually here, the output layer, is a bit of a troublemaker in the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5900" target="_blank">01:38:20.920</a></span> | <span class="t">that this is currently arranged.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5902" target="_blank">01:38:22.320</a></span> | <span class="t">Because you can see that the last layer here in pink takes on values that are much larger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5910" target="_blank">01:38:30.720</a></span> | <span class="t">than some of the values inside the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5916" target="_blank">01:38:36.020</a></span> | <span class="t">So the standard deviations are roughly 1 and -3 throughout, except for the last layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5921" target="_blank">01:38:41.700</a></span> | <span class="t">which actually has roughly 1 and -2 standard deviation of gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5926" target="_blank">01:38:46.000</a></span> | <span class="t">And so the gradients on the last layer are currently about 100 times greater, sorry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5930" target="_blank">01:38:50.860</a></span> | <span class="t">10 times greater than all the other weights inside the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5936" target="_blank">01:38:56.020</a></span> | <span class="t">And so that's problematic because in the simple stochastic gradient descent setup, you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5940" target="_blank">01:39:00.560</a></span> | <span class="t">be training this last layer about 10 times faster than you would be training the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5944" target="_blank">01:39:04.820</a></span> | <span class="t">layers at initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5947" target="_blank">01:39:07.300</a></span> | <span class="t">Now this actually kind of fixes itself a little bit if you train for a bit longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5951" target="_blank">01:39:11.240</a></span> | <span class="t">So for example, if I greater than 1000, only then do a break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5956" target="_blank">01:39:16.340</a></span> | <span class="t">Let me reinitialize, and then let me do it 1000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5960" target="_blank">01:39:20.200</a></span> | <span class="t">And after 1000 steps, we can look at the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5964" target="_blank">01:39:24.460</a></span> | <span class="t">So you see how the neurons are saturating a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5967" target="_blank">01:39:27.940</a></span> | <span class="t">And we can also look at the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5970" target="_blank">01:39:30.180</a></span> | <span class="t">But otherwise they look good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5971" target="_blank">01:39:31.180</a></span> | <span class="t">They're about equal, and there's no shrinking to zero or exploding to infinities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5975" target="_blank">01:39:35.500</a></span> | <span class="t">And you can see that here in the weights, things are also stabilizing a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5980" target="_blank">01:39:40.460</a></span> | <span class="t">So the tails of the last pink layer are actually coming in during the optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5986" target="_blank">01:39:46.460</a></span> | <span class="t">But certainly this is a little bit troubling, especially if you are using a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5990" target="_blank">01:39:50.420</a></span> | <span class="t">update rule like stochastic gradient descent instead of a modern optimizer like Atom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5995" target="_blank">01:39:55.380</a></span> | <span class="t">Now I'd like to show you one more plot that I usually look at when I train neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=5999" target="_blank">01:39:59.300</a></span> | <span class="t">And basically the gradient to data ratio is not actually that informative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6003" target="_blank">01:40:03.500</a></span> | <span class="t">Because what matters at the end is not the gradient to data ratio, but the update to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6007" target="_blank">01:40:07.380</a></span> | <span class="t">the data ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6008" target="_blank">01:40:08.620</a></span> | <span class="t">Because that is the amount by which we will actually change the data in these tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6013" target="_blank">01:40:13.060</a></span> | <span class="t">So coming up here, what I'd like to do is I'd like to introduce a new update to data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6017" target="_blank">01:40:17.420</a></span> | <span class="t">ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6018" target="_blank">01:40:18.420</a></span> | <span class="t">It's going to be a list, and we're going to build it out every single iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6023" target="_blank">01:40:23.300</a></span> | <span class="t">And here I'd like to keep track of basically the ratio every single iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6030" target="_blank">01:40:30.180</a></span> | <span class="t">So without any gradients, I'm comparing the update, which is learning rate times the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6039" target="_blank">01:40:39.100</a></span> | <span class="t">That is the update that we're going to apply to every parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6042" target="_blank">01:40:42.740</a></span> | <span class="t">So see I'm iterating over all the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6044" target="_blank">01:40:44.660</a></span> | <span class="t">And then I'm taking the basically standard deviation of the update we're going to apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6048" target="_blank">01:40:48.180</a></span> | <span class="t">and divide it by the actual content, the data of that parameter and its standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6056" target="_blank">01:40:56.220</a></span> | <span class="t">So this is the ratio of basically how great are the updates to the values in these tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6061" target="_blank">01:41:01.500</a></span> | <span class="t">Then we're going to take a log of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6063" target="_blank">01:41:03.580</a></span> | <span class="t">And actually I'd like to take a log 10 just so it's a nicer visualization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6070" target="_blank">01:41:10.500</a></span> | <span class="t">So we're going to be basically looking at the exponents of this division here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6076" target="_blank">01:41:16.940</a></span> | <span class="t">And then that item to pop out the float.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6079" target="_blank">01:41:19.460</a></span> | <span class="t">And we're going to be keeping track of this for all the parameters and adding it to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6082" target="_blank">01:41:22.460</a></span> | <span class="t">UD tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6084" target="_blank">01:41:24.300</a></span> | <span class="t">So now let me reinitialize and run a thousand iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6087" target="_blank">01:41:27.700</a></span> | <span class="t">We can look at the activations, the gradients, and the parameter gradients as we did before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6094" target="_blank">01:41:34.340</a></span> | <span class="t">But now I have one more plot here to introduce.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6096" target="_blank">01:41:36.660</a></span> | <span class="t">And what's happening here is we're iterating over all the parameters, and I'm constraining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6101" target="_blank">01:41:41.140</a></span> | <span class="t">it again like I did here to just the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6104" target="_blank">01:41:44.780</a></span> | <span class="t">So the number of dimensions in these sensors is two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6107" target="_blank">01:41:47.940</a></span> | <span class="t">And then I'm basically plotting all of these update ratios over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6114" target="_blank">01:41:54.580</a></span> | <span class="t">So when I plot this, I plot those ratios and you can see that they evolve over time during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6119" target="_blank">01:41:59.620</a></span> | <span class="t">initialization to take on certain values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6122" target="_blank">01:42:02.060</a></span> | <span class="t">And then these updates are like start stabilizing usually during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6126" target="_blank">01:42:06.020</a></span> | <span class="t">Then the other thing that I'm plotting here is I'm plotting here like an approximate value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6129" target="_blank">01:42:09.300</a></span> | <span class="t">that is a rough guide for what it roughly should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6132" target="_blank">01:42:12.580</a></span> | <span class="t">And it should be like roughly 1 and -3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6135" target="_blank">01:42:15.580</a></span> | <span class="t">And so that means that basically there's some values in this tensor and they take on certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6140" target="_blank">01:42:20.980</a></span> | <span class="t">values and the updates to them at every single iteration are no more than roughly one thousandth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6146" target="_blank">01:42:26.980</a></span> | <span class="t">of the actual magnitude in those tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6151" target="_blank">01:42:31.060</a></span> | <span class="t">If this was much larger, like for example, if the log of this was like say -1, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6157" target="_blank">01:42:37.900</a></span> | <span class="t">actually updating those values quite a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6160" target="_blank">01:42:40.180</a></span> | <span class="t">They're undergoing a lot of change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6162" target="_blank">01:42:42.300</a></span> | <span class="t">But the reason that the final layer here is an outlier is because this layer was artificially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6169" target="_blank">01:42:49.100</a></span> | <span class="t">shrunk down to keep the softmax unconfident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6174" target="_blank">01:42:54.580</a></span> | <span class="t">So here you see how we multiply the weight by 0.1 in the initialization to make the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6181" target="_blank">01:43:01.180</a></span> | <span class="t">layer prediction less confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6184" target="_blank">01:43:04.380</a></span> | <span class="t">That artificially made the values inside that tensor way too low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6189" target="_blank">01:43:09.460</a></span> | <span class="t">And that's why we're getting temporarily a very high ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6192" target="_blank">01:43:12.260</a></span> | <span class="t">But you see that that stabilizes over time once that weight starts to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6198" target="_blank">01:43:18.100</a></span> | <span class="t">But basically I like to look at the evolution of this update ratio for all my parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6202" target="_blank">01:43:22.420</a></span> | <span class="t">usually and I like to make sure that it's not too much above 1 and -3 roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6209" target="_blank">01:43:29.820</a></span> | <span class="t">So around -3 on this log plot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6213" target="_blank">01:43:33.160</a></span> | <span class="t">If it's below -3, usually that means that the parameters are not training fast enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6217" target="_blank">01:43:37.520</a></span> | <span class="t">So if our learning rate was very low, let's do that experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6221" target="_blank">01:43:41.940</a></span> | <span class="t">Let's initialize and then let's actually do a learning rate of say 1 and -3 here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6227" target="_blank">01:43:47.620</a></span> | <span class="t">So 0.001.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6229" target="_blank">01:43:49.700</a></span> | <span class="t">If your learning rate is way too low, this plot will typically reveal it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6236" target="_blank">01:43:56.500</a></span> | <span class="t">So you see how all of these updates are way too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6240" target="_blank">01:44:00.460</a></span> | <span class="t">So the size of the update is basically 10,000 times in magnitude to the size of the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6249" target="_blank">01:44:09.080</a></span> | <span class="t">in that tensor in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6250" target="_blank">01:44:10.740</a></span> | <span class="t">So this is a symptom of training way too slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6254" target="_blank">01:44:14.700</a></span> | <span class="t">So this is another way to sometimes set the learning rate and to get a sense of what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6257" target="_blank">01:44:17.720</a></span> | <span class="t">learning rate should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6259" target="_blank">01:44:19.280</a></span> | <span class="t">And ultimately this is something that you would keep track of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6265" target="_blank">01:44:25.120</a></span> | <span class="t">If anything, the learning rate here is a little bit on the higher side because you see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6271" target="_blank">01:44:31.920</a></span> | <span class="t">we're above the black line of -3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6274" target="_blank">01:44:34.020</a></span> | <span class="t">We're somewhere around -2.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6276" target="_blank">01:44:36.080</a></span> | <span class="t">It's like okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6277" target="_blank">01:44:37.960</a></span> | <span class="t">But everything is somewhat stabilizing and so this looks like a pretty decent setting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6281" target="_blank">01:44:41.340</a></span> | <span class="t">of learning rates and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6283" target="_blank">01:44:43.680</a></span> | <span class="t">But this is something to look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6285" target="_blank">01:44:45.160</a></span> | <span class="t">And when things are miscalibrated, you will see very quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6288" target="_blank">01:44:48.520</a></span> | <span class="t">So for example, everything looks pretty well behaved, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6292" target="_blank">01:44:52.380</a></span> | <span class="t">But just as a comparison, when things are not properly calibrated, what does that look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6295" target="_blank">01:44:55.500</a></span> | <span class="t">like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6296" target="_blank">01:44:56.580</a></span> | <span class="t">Let me come up here and let's say that for example, what do we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6301" target="_blank">01:45:01.920</a></span> | <span class="t">Let's say that we forgot to apply this fan-in normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6305" target="_blank">01:45:05.900</a></span> | <span class="t">So the weights inside the linear layers are just a sample from a Gaussian in all those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6310" target="_blank">01:45:10.180</a></span> | <span class="t">stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6311" target="_blank">01:45:11.180</a></span> | <span class="t">What happens to our - how do we notice that something's off?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6314" target="_blank">01:45:14.620</a></span> | <span class="t">Well the activation plot will tell you, whoa, your neurons are way too saturated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6318" target="_blank">01:45:18.940</a></span> | <span class="t">The gradients are going to be all messed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6321" target="_blank">01:45:21.460</a></span> | <span class="t">And the histogram for these weights are going to be all messed up as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6325" target="_blank">01:45:25.420</a></span> | <span class="t">And there's a lot of asymmetry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6327" target="_blank">01:45:27.260</a></span> | <span class="t">And then if we look here, I suspect it's all going to be also pretty messed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6330" target="_blank">01:45:30.780</a></span> | <span class="t">So you see there's a lot of discrepancy in how fast these layers are learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6336" target="_blank">01:45:36.540</a></span> | <span class="t">And some of them are learning way too fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6338" target="_blank">01:45:38.700</a></span> | <span class="t">So -1, -1.5, those are very large numbers in terms of this ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6343" target="_blank">01:45:43.780</a></span> | <span class="t">Again, you should be somewhere around -3 and not much more above that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6348" target="_blank">01:45:48.640</a></span> | <span class="t">So this is how miscalibrations of your neural nets are going to manifest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6353" target="_blank">01:45:53.020</a></span> | <span class="t">And these kinds of plots here are a good way of sort of bringing those miscalibrations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6361" target="_blank">01:46:01.820</a></span> | <span class="t">to your attention and so you can address them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6364" target="_blank">01:46:04.100</a></span> | <span class="t">Okay, so so far we've seen that when we have this linear tanh sandwich, we can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6368" target="_blank">01:46:08.780</a></span> | <span class="t">precisely calibrate the gains and make the activations, the gradients, and the parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6373" target="_blank">01:46:13.460</a></span> | <span class="t">and the updates all look pretty decent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6375" target="_blank">01:46:15.960</a></span> | <span class="t">But it definitely feels a little bit like balancing of a pencil on your finger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6381" target="_blank">01:46:21.340</a></span> | <span class="t">And that's because this gain has to be very precisely calibrated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6386" target="_blank">01:46:26.040</a></span> | <span class="t">So now let's introduce batch normalization layers into the mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6390" target="_blank">01:46:30.720</a></span> | <span class="t">Let's see how that helps fix the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6394" target="_blank">01:46:34.020</a></span> | <span class="t">So here, I'm going to take the BatchNorm1D class, and I'm going to start placing it inside.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6401" target="_blank">01:46:41.220</a></span> | <span class="t">And as I mentioned before, the standard typical place you would place it is between the linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6406" target="_blank">01:46:46.780</a></span> | <span class="t">layer, so right after it, but before the nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6409" target="_blank">01:46:49.320</a></span> | <span class="t">But people have definitely played with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6411" target="_blank">01:46:51.480</a></span> | <span class="t">And in fact, you can get very similar results, even if you place it after the nonlinearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6417" target="_blank">01:46:57.960</a></span> | <span class="t">And the other thing that I wanted to mention is it's totally fine to also place it at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6420" target="_blank">01:47:00.960</a></span> | <span class="t">end, after the last linear layer and before the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6424" target="_blank">01:47:04.880</a></span> | <span class="t">So this is potentially fine as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6428" target="_blank">01:47:08.940</a></span> | <span class="t">And in this case, this would be output, would be vocab size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6434" target="_blank">01:47:14.260</a></span> | <span class="t">Now because the last layer is BatchNorm, we would not be changing the weight to make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6439" target="_blank">01:47:19.200</a></span> | <span class="t">softmax less confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6440" target="_blank">01:47:20.720</a></span> | <span class="t">We'd be changing the gamma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6443" target="_blank">01:47:23.240</a></span> | <span class="t">Because gamma, remember, in the BatchNorm, is the variable that multiplicatively interacts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6448" target="_blank">01:47:28.120</a></span> | <span class="t">with the output of that normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6452" target="_blank">01:47:32.800</a></span> | <span class="t">So we can initialize this sandwich now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6455" target="_blank">01:47:35.920</a></span> | <span class="t">We can train, and we can see that the activations are going to of course look very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6461" target="_blank">01:47:41.920</a></span> | <span class="t">And they are going to necessarily look good, because now before every single tanh layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6466" target="_blank">01:47:46.920</a></span> | <span class="t">there is a normalization in the BatchNorm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6469" target="_blank">01:47:49.380</a></span> | <span class="t">So this is unsurprisingly all looks pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6473" target="_blank">01:47:53.080</a></span> | <span class="t">It's going to be standard deviation of roughly 0.65, 2%, and roughly equal standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6478" target="_blank">01:47:58.120</a></span> | <span class="t">throughout the entire layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6479" target="_blank">01:47:59.840</a></span> | <span class="t">So everything looks very homogeneous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6482" target="_blank">01:48:02.800</a></span> | <span class="t">The gradients look good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6484" target="_blank">01:48:04.800</a></span> | <span class="t">The weights look good in their distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6489" target="_blank">01:48:09.400</a></span> | <span class="t">And then the updates also look pretty reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6494" target="_blank">01:48:14.360</a></span> | <span class="t">We're going above -3 a little bit, but not by too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6498" target="_blank">01:48:18.020</a></span> | <span class="t">So all the parameters are training at roughly the same rate here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6504" target="_blank">01:48:24.840</a></span> | <span class="t">But now what we've gained is we are going to be slightly less brittle with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6512" target="_blank">01:48:32.800</a></span> | <span class="t">the gain of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6514" target="_blank">01:48:34.300</a></span> | <span class="t">So for example, I can make the gain be, say, 0.2 here, which is much slower than what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6520" target="_blank">01:48:40.760</a></span> | <span class="t">had with the tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6523" target="_blank">01:48:43.120</a></span> | <span class="t">But as we'll see, the activations will actually be exactly unaffected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6527" target="_blank">01:48:47.000</a></span> | <span class="t">And that's because of, again, this explicit normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6529" target="_blank">01:48:49.780</a></span> | <span class="t">The gradients are going to look okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6531" target="_blank">01:48:51.520</a></span> | <span class="t">The weight gradients are going to look okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6534" target="_blank">01:48:54.120</a></span> | <span class="t">But actually the updates will change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6537" target="_blank">01:48:57.100</a></span> | <span class="t">And so even though the forward and backward paths to a very large extent look okay, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6541" target="_blank">01:49:01.920</a></span> | <span class="t">of the backward paths of the Bash norm and how the scale of the incoming activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6546" target="_blank">01:49:06.040</a></span> | <span class="t">interacts in the Bash norm and its backward paths, this is actually changing the scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6553" target="_blank">01:49:13.960</a></span> | <span class="t">of the updates on these parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6556" target="_blank">01:49:16.420</a></span> | <span class="t">So the gradients of these weights are affected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6559" target="_blank">01:49:19.760</a></span> | <span class="t">So we still don't get a completely free path to pass in arbitrary weights here, but everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6566" target="_blank">01:49:26.500</a></span> | <span class="t">else is significantly more robust in terms of the forward, backward, and the weight gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6573" target="_blank">01:49:33.160</a></span> | <span class="t">It's just that you may have to retune your learning rate if you are changing sufficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6577" target="_blank">01:49:37.460</a></span> | <span class="t">the scale of the activations that are coming into the Bash norms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6581" target="_blank">01:49:41.620</a></span> | <span class="t">So here, for example, we changed the gains of these linear layers to be greater, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6587" target="_blank">01:49:47.380</a></span> | <span class="t">we're seeing that the updates are coming out lower as a result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6591" target="_blank">01:49:51.880</a></span> | <span class="t">And then finally, if we are using Bash norms, we don't actually need to necessarily—let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6596" target="_blank">01:49:56.280</a></span> | <span class="t">me reset this to 1 so there's no gain—we don't necessarily even have to normalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6602" target="_blank">01:50:02.080</a></span> | <span class="t">backfan_in sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6603" target="_blank">01:50:03.640</a></span> | <span class="t">So if I take out the fan_in, so these are just now random Gaussian, we'll see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6608" target="_blank">01:50:08.640</a></span> | <span class="t">because of Bash norm, this will actually be relatively well-behaved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6611" target="_blank">01:50:11.920</a></span> | <span class="t">So this will of course in the forward path look good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6617" target="_blank">01:50:17.680</a></span> | <span class="t">The gradients look good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6620" target="_blank">01:50:20.000</a></span> | <span class="t">The backward weight updates look okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6623" target="_blank">01:50:23.720</a></span> | <span class="t">A little bit of fat tails on some of the layers, and this looks okay as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6629" target="_blank">01:50:29.040</a></span> | <span class="t">But as you can see, we're significantly below -3, so we'd have to bump up the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6635" target="_blank">01:50:35.000</a></span> | <span class="t">rate of this Bash norm so that we are training more properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6638" target="_blank">01:50:38.760</a></span> | <span class="t">And in particular, looking at this, roughly looks like we have to 10x the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6643" target="_blank">01:50:43.240</a></span> | <span class="t">to get to about 1e-3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6646" target="_blank">01:50:46.860</a></span> | <span class="t">So we'd come here and we would change this to be update of 1.0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6651" target="_blank">01:50:51.520</a></span> | <span class="t">And if I reinitialize, then we'll see that everything still of course looks good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6662" target="_blank">01:51:02.600</a></span> | <span class="t">And now we are roughly here, and we expect this to be an okay training run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6667" target="_blank">01:51:07.280</a></span> | <span class="t">So long story short, we are significantly more robust to the gain of these linear layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6671" target="_blank">01:51:11.840</a></span> | <span class="t">whether or not we have to apply the fan_in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6674" target="_blank">01:51:14.240</a></span> | <span class="t">And then we can change the gain, but we actually do have to worry a little bit about the update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6680" target="_blank">01:51:20.040</a></span> | <span class="t">scales and making sure that the learning rate is properly calibrated here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6684" target="_blank">01:51:24.320</a></span> | <span class="t">But the activations of the forward, backward paths and the updates are looking significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6689" target="_blank">01:51:29.120</a></span> | <span class="t">more well-behaved, except for the global scale that is potentially being adjusted here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6693" target="_blank">01:51:33.760</a></span> | <span class="t">Okay, so now let me summarize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6696" target="_blank">01:51:36.660</a></span> | <span class="t">There are three things I was hoping to achieve with this section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6699" target="_blank">01:51:39.640</a></span> | <span class="t">Number one, I wanted to introduce you to Bash normalization, which is one of the first modern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6703" target="_blank">01:51:43.840</a></span> | <span class="t">innovations that we're looking into that helped stabilize very deep neural networks and their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6708" target="_blank">01:51:48.600</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6710" target="_blank">01:51:50.000</a></span> | <span class="t">And I hope you understand how the Bash normalization works and how it would be used in a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6714" target="_blank">01:51:54.920</a></span> | <span class="t">network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6716" target="_blank">01:51:56.280</a></span> | <span class="t">Number two, I was hoping to PyTorchify some of our code and wrap it up into these modules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6721" target="_blank">01:52:01.960</a></span> | <span class="t">So like linear, Bash normalization 1D, 10H, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6724" target="_blank">01:52:04.800</a></span> | <span class="t">These are layers or modules, and they can be stacked up into neural nets like Lego building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6729" target="_blank">01:52:09.800</a></span> | <span class="t">blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6731" target="_blank">01:52:11.160</a></span> | <span class="t">And these layers actually exist in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6735" target="_blank">01:52:15.080</a></span> | <span class="t">And if you import torch-nn, then you can actually, the way I've constructed it, you can simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6739" target="_blank">01:52:19.800</a></span> | <span class="t">just use PyTorch by prepending nn. to all these different layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6745" target="_blank">01:52:25.440</a></span> | <span class="t">And actually everything will just work because the API that I've developed here is identical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6750" target="_blank">01:52:30.460</a></span> | <span class="t">to the API that PyTorch uses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6752" target="_blank">01:52:32.880</a></span> | <span class="t">And the implementation also is basically, as far as I'm aware, identical to the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6756" target="_blank">01:52:36.920</a></span> | <span class="t">in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6757" target="_blank">01:52:37.920</a></span> | <span class="t">And number three, I tried to introduce you to the diagnostic tools that you would use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6762" target="_blank">01:52:42.440</a></span> | <span class="t">to understand whether your neural network is in a good state dynamically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6766" target="_blank">01:52:46.420</a></span> | <span class="t">So we are looking at the statistics and histograms and activation of the forward pass activations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6772" target="_blank">01:52:52.320</a></span> | <span class="t">the backward pass gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6774" target="_blank">01:52:54.200</a></span> | <span class="t">And then also we're looking at the weights that are going to be updated as part of stochastic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6777" target="_blank">01:52:57.920</a></span> | <span class="t">gradient ascent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6779" target="_blank">01:52:59.120</a></span> | <span class="t">And we're looking at their means, standard deviations, and also the ratio of gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6783" target="_blank">01:53:03.520</a></span> | <span class="t">to data, or even better, the updates to data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6788" target="_blank">01:53:08.040</a></span> | <span class="t">And we saw that typically we don't actually look at it as a single snapshot frozen in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6791" target="_blank">01:53:11.680</a></span> | <span class="t">time at some particular iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6794" target="_blank">01:53:14.080</a></span> | <span class="t">Typically people look at this as over time, just like I've done here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6797" target="_blank">01:53:17.900</a></span> | <span class="t">And they look at these update to data ratios and they make sure everything looks okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6801" target="_blank">01:53:21.680</a></span> | <span class="t">And in particular, I said that 1e-3, or basically negative 3 on the log scale, is a good rough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6809" target="_blank">01:53:29.120</a></span> | <span class="t">heuristic for what you want this ratio to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6811" target="_blank">01:53:31.920</a></span> | <span class="t">And if it's way too high, then probably the learning rate or the updates are a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6815" target="_blank">01:53:35.560</a></span> | <span class="t">too big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6816" target="_blank">01:53:36.560</a></span> | <span class="t">And if it's way too small, then the learning rate is probably too small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6819" target="_blank">01:53:39.840</a></span> | <span class="t">So that's just some of the things that you may want to play with when you try to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6823" target="_blank">01:53:43.080</a></span> | <span class="t">your neural network to work very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6825" target="_blank">01:53:45.960</a></span> | <span class="t">Now, there's a number of things I did not try to achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6829" target="_blank">01:53:49.200</a></span> | <span class="t">I did not try to beat our previous performance, as an example, by introducing the BatchNorm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6833" target="_blank">01:53:53.080</a></span> | <span class="t">layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6834" target="_blank">01:53:54.080</a></span> | <span class="t">Actually, I did try, and I found that I used the learning rate finding mechanism that I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6838" target="_blank">01:53:58.840</a></span> | <span class="t">described before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6839" target="_blank">01:53:59.840</a></span> | <span class="t">I tried to train the BatchNorm layer, a BatchNorm neural net, and I actually ended up with results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6845" target="_blank">01:54:05.400</a></span> | <span class="t">that are very, very similar to what we've obtained before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6848" target="_blank">01:54:08.440</a></span> | <span class="t">And that's because our performance now is not bottlenecked by the optimization, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6853" target="_blank">01:54:13.120</a></span> | <span class="t">is what BatchNorm is helping with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6855" target="_blank">01:54:15.220</a></span> | <span class="t">The performance at this stage is bottlenecked by what I suspect is the context length of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6860" target="_blank">01:54:20.080</a></span> | <span class="t">our context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6862" target="_blank">01:54:22.080</a></span> | <span class="t">So currently, we are taking three characters to predict the fourth one, and I think we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6865" target="_blank">01:54:25.140</a></span> | <span class="t">need to go beyond that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6866" target="_blank">01:54:26.240</a></span> | <span class="t">And we need to look at more powerful architectures, like recurrent neural networks and transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6871" target="_blank">01:54:31.000</a></span> | <span class="t">in order to further push the like probabilities that we're achieving on this dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6876" target="_blank">01:54:36.840</a></span> | <span class="t">And I also did not try to have a full explanation of all of these activations, the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6882" target="_blank">01:54:42.400</a></span> | <span class="t">and the backward pass, and the statistics of all these gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6885" target="_blank">01:54:45.640</a></span> | <span class="t">And so you may have found some of the parts here unintuitive, and maybe you're slightly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6888" target="_blank">01:54:48.640</a></span> | <span class="t">confused about, okay, if I change the gain here, how come that we need a different learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6893" target="_blank">01:54:53.600</a></span> | <span class="t">rate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6894" target="_blank">01:54:54.600</a></span> | <span class="t">But I didn't go into the full detail, because you'd have to actually look at the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6897" target="_blank">01:54:57.120</a></span> | <span class="t">pass of all these different layers and get an intuitive understanding of how that works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6901" target="_blank">01:55:01.480</a></span> | <span class="t">And I did not go into that in this lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6904" target="_blank">01:55:04.040</a></span> | <span class="t">The purpose really was just to introduce you to the diagnostic tools and what they look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6907" target="_blank">01:55:07.560</a></span> | <span class="t">like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6908" target="_blank">01:55:08.560</a></span> | <span class="t">But there's still a lot of work remaining on the intuitive level to understand the initialization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6912" target="_blank">01:55:12.720</a></span> | <span class="t">the backward pass, and how all of that interacts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6915" target="_blank">01:55:15.880</a></span> | <span class="t">But you shouldn't feel too bad, because honestly, we are getting to the cutting edge of where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6921" target="_blank">01:55:21.480</a></span> | <span class="t">the field is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6922" target="_blank">01:55:22.960</a></span> | <span class="t">We certainly haven't, I would say, solved initialization, and we haven't solved backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6928" target="_blank">01:55:28.360</a></span> | <span class="t">And these are still very much an active area of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6930" target="_blank">01:55:30.960</a></span> | <span class="t">People are still trying to figure out what is the best way to initialize these networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6933" target="_blank">01:55:33.960</a></span> | <span class="t">what is the best update rule to use, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6937" target="_blank">01:55:37.440</a></span> | <span class="t">So none of this is really solved, and we don't really have all the answers to all these cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6944" target="_blank">01:55:44.240</a></span> | <span class="t">But at least we're making progress, and at least we have some tools to tell us whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6948" target="_blank">01:55:48.440</a></span> | <span class="t">or not things are on the right track for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6951" target="_blank">01:55:51.800</a></span> | <span class="t">So I think we've made positive progress in this lecture, and I hope you enjoyed that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=P6sfmUTpUmc&t=6956" target="_blank">01:55:56.120</a></span> | <span class="t">And I will see you next time.</span></div></div></body></html>