<html><head><title>Qwen Image: SOTA text rendering + 4o-imagegen-level Editing Open Weights MMDiT</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Qwen Image: SOTA text rendering + 4o-imagegen-level Editing Open Weights MMDiT</h2><a href="https://www.youtube.com/watch?v=J79Jqs_7820"><img src="https://i.ytimg.com/vi/J79Jqs_7820/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=0">0:0</a> Introduction to the Qwen Image Paper Review<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=40">0:40</a> Introduction to Qwen Image<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=87">1:27</a> Progressive Training Strategy<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=108">1:48</a> Model Architecture<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=643">10:43</a> Diffusion Process<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=734">12:14</a> Data Collection and Curation<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=891">14:51</a> Seven-Stage Filtering Process<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1467">24:27</a> Data Annotation with Qwen<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1671">27:51</a> Data Synthesis<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2043">34:3</a> Pre-training and Optimization<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2446">40:46</a> Post-training and Reinforcement Learning (RL)<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2618">43:38</a> Multimodal Image Generation<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2646">44:6</a> Performance and Benchmarks<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2959">49:19</a> Live Testing in Other Languages<br><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3463">57:43</a> Conclusion and Practical Applications<br><br><div style="text-align: left;"><a href="./J79Jqs_7820.html">Whisper Transcript</a> | <a href="./transcript_J79Jqs_7820.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=0" target="_blank">00:00:00.960</a></span> | <span class="t">Yeah, okay. So, hello everyone. This is the Quint image paper, or technical report rather.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=6" target="_blank">00:00:06.640</a></span> | <span class="t">So, let's get started. So, wait, wait, wait. We have a volunteer for next week, Venki.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=13" target="_blank">00:00:13.600</a></span> | <span class="t">Yeah, Venki. Awesome. Let's go. Are you on the Discord?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=20" target="_blank">00:00:20.480</a></span> | <span class="t">I think I'm on the Discord. If not, I'll ask my friend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=25" target="_blank">00:00:25.840</a></span> | <span class="t">Yeah, I mean, just announce what paper you're going through next week, and just drop the link</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=30" target="_blank">00:00:30.800</a></span> | <span class="t">to the paper so that people can pre-read this. Super excited for you to share, Venki. Looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=36" target="_blank">00:00:36.720</a></span> | <span class="t">forward to it. Thank you. Thank you, YouTube. Yeah, cool. Okay. So, yeah. So, we have Quint image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=45" target="_blank">00:00:45.680</a></span> | <span class="t">So, it's a diffusion image generation model. That's pretty nice. And throughout the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=55" target="_blank">00:00:55.120</a></span> | <span class="t">they talk about how it's a diffusion model, obviously, but they kind of put special emphasis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=58" target="_blank">00:00:58.640</a></span> | <span class="t">on how the model does well in Chinese. Or, like, logographic languages, including Chinese and others.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=66" target="_blank">00:01:06.960</a></span> | <span class="t">And also, we kind of get to see how they curate their data, and also how they label it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=72" target="_blank">00:01:12.720</a></span> | <span class="t">and the different techniques that they use. Yeah, I find that pretty interesting. So, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=77" target="_blank">00:01:17.680</a></span> | <span class="t">So, it's Quint image. So, it achieves significant advances in complex text rendering and precise image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=83" target="_blank">00:01:23.680</a></span> | <span class="t">editing. That's pretty cool. They use a progressive training strategy. So, yeah. So, with this, they use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=88" target="_blank">00:01:28.640</a></span> | <span class="t">essentially, curriculum learning. Like, for a stage, they train the model on, like, very, very low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=93" target="_blank">00:01:33.440</a></span> | <span class="t">resolution/quality images, like 256 pixel, or, like, 256 by 256 images. And they kind of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=100" target="_blank">00:01:40.240</a></span> | <span class="t">progress. Like, they make the images, like, sharper and sharper, which I find interesting. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=106" target="_blank">00:01:46.800</a></span> | <span class="t">that's a form of curriculum learning. Let's see. English, Chinese. And they also use VAE, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=113" target="_blank">00:01:53.280</a></span> | <span class="t">a variational autoencoder, and also a language model, or, like, a visual language model, to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=118" target="_blank">00:01:58.800</a></span> | <span class="t">like, to kind of have, like, a system one, system two thing, where, like, they use the VAE encoder to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=123" target="_blank">00:02:03.520</a></span> | <span class="t">encode low-level, like, physical or, like, spatial details within the image. And they use their language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=129" target="_blank">00:02:09.600</a></span> | <span class="t">model, like, when 2.5 VL to encode, like, the more, like, semantic part of the image. So, I kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=137" target="_blank">00:02:17.600</a></span> | <span class="t">found that interesting also. So, let's see. There's nothing new here. These are a bunch of benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=142" target="_blank">00:02:22.720</a></span> | <span class="t">You guys can read it if you want. But I'll kind of skip them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=145" target="_blank">00:02:25.440</a></span> | <span class="t">A bunch of images. Pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=151" target="_blank">00:02:31.520</a></span> | <span class="t">Yeah, the images are worth looking at at some point later. You can refer to them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=159" target="_blank">00:02:39.040</a></span> | <span class="t">or pull it up yourself separately, you know? But, like, it's editing and generation that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=165" target="_blank">00:02:45.760</a></span> | <span class="t">and both are, like, to a level I've never seen before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=169" target="_blank">00:02:49.360</a></span> | <span class="t">Yeah, and if people are interested, I can just, like, go back after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=173" target="_blank">00:02:53.120</a></span> | <span class="t">I think editing and stuff, it's been done before just in separate models, right? Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=180" target="_blank">00:03:00.560</a></span> | <span class="t">you need, like, a context consistency model. But, yes, very cool. I thought the first page of images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=188" target="_blank">00:03:08.240</a></span> | <span class="t">was very cool. Like, this one with the whiteboard, you see how, like, the whiteboard is at an angle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=194" target="_blank">00:03:14.000</a></span> | <span class="t">and then it has reflections, and the text is at an angle and not directly straight? I was like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=198" target="_blank">00:03:18.640</a></span> | <span class="t">oh, shit, this is pretty cool. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=200" target="_blank">00:03:20.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=201" target="_blank">00:03:21.520</a></span> | <span class="t">Quen dropped a teaser about edit today. This is an editing model, by the way. It's very heavy on handwriting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=208" target="_blank">00:03:28.800</a></span> | <span class="t">Okay, let's just continue to. I think you should read the descriptions of all these as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=216" target="_blank">00:03:36.240</a></span> | <span class="t">for what they show, by the way, for people following along. Oh, shit. I'll drop the paper as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=220" target="_blank">00:03:40.400</a></span> | <span class="t">Yeah, so let's see. Intro. So here they just talk about the challenges. They talk about, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=230" target="_blank">00:03:50.560</a></span> | <span class="t">allotting model outputs with complex prompts, and also, like, just essentially making the text look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=238" target="_blank">00:03:58.160</a></span> | <span class="t">nice. That's kind of what they're talking about here. And, like, sometimes they talk about how people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=243" target="_blank">00:04:03.600</a></span> | <span class="t">have difficulty modifying images. Like, if someone has, like, a pose, or if they want someone to strike,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=249" target="_blank">00:04:09.120</a></span> | <span class="t">like, a different pose, they want to edit the image. Like, the person will kind of strike the pose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=253" target="_blank">00:04:13.520</a></span> | <span class="t">but the background will kind of, like, lose coherence. So they kind of, like, they talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=257" target="_blank">00:04:17.600</a></span> | <span class="t">that as, like, some of the problems that they're trying to solve with the making of Quen. So they also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=263" target="_blank">00:04:23.200</a></span> | <span class="t">put, like, a ton of effort into, like, data engineering right here, which we'll kind of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=267" target="_blank">00:04:27.600</a></span> | <span class="t">like, see in the paper. So progressive learning, I already talked about that. Multitask training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=272" target="_blank">00:04:32.880</a></span> | <span class="t">paradigms, we'll talk about that. And they also talk about, like, how they use GPUs. They have, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=277" target="_blank">00:04:37.200</a></span> | <span class="t">a really interesting structure there. So let's see. Yeah, we'll get into all of this. Yeah. Okay. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=287" target="_blank">00:04:47.200</a></span> | <span class="t">here's the architecture. So for this, this is a diffusion model. So, like, they'll inject noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=293" target="_blank">00:04:53.200</a></span> | <span class="t">into it, like, when they're training. When they're training, they'll, like, they'll patch some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=297" target="_blank">00:04:57.280</a></span> | <span class="t">images, and they'll train the model to, like, essentially reconstruct it. And what they do here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=305" target="_blank">00:05:05.440</a></span> | <span class="t">is they use, like, well, we'll talk about it here, but they'll use, like, the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=311" target="_blank">00:05:11.360</a></span> | <span class="t">to maintain, like, global coherence. Like, they'll say, like, oh, this image is supposed to be about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=316" target="_blank">00:05:16.080</a></span> | <span class="t">this image. And their autoencoder kind of encodes the lower-level details. So they have, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=323" target="_blank">00:05:23.600</a></span> | <span class="t">it's composed of 60 of these, like, transformer blocks. I mean, not transformer. Yeah, the diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=328" target="_blank">00:05:28.080</a></span> | <span class="t">transformer blocks. And let's see. So there's nothing special here. Or you can look at this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=332" target="_blank">00:05:32.400</a></span> | <span class="t">But they use QKNorm. And they also, like, they made their own positional encoding, like, mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=340" target="_blank">00:05:40.720</a></span> | <span class="t">which is msrope, which we'll also get into. Okay, so let's see. So yeah, so they have three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=346" target="_blank">00:05:46.960</a></span> | <span class="t">components. So they have, like, a multimodal language model. That serves as the condition encoder. And it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=351" target="_blank">00:05:51.360</a></span> | <span class="t">extracts the feature from the textual inputs. This is, like, the QN 2.5 part that I was talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=356" target="_blank">00:05:56.080</a></span> | <span class="t">They also have, like, the VAE. So it compresses images. Like, it just extracts the physical features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=362" target="_blank">00:06:02.240</a></span> | <span class="t">quote-unquote, of the image. And they'll also have, like, the multimodal diffusion transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=366" target="_blank">00:06:06.080</a></span> | <span class="t">which is the actual diffusion part of the diffusion model. Let's see. So they use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=371" target="_blank">00:06:11.440</a></span> | <span class="t">they talk about how they use QN 2.5 VL. Let's see. So they say that it has, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=377" target="_blank">00:06:17.440</a></span> | <span class="t">they have three key reasons. So language individual spaces of QN 2.5 VL have already been aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=382" target="_blank">00:06:22.000</a></span> | <span class="t">And 2.5 VL retains strong language modeling capabilities without significant degradation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=387" target="_blank">00:06:27.680</a></span> | <span class="t">compared to other language models. So I tried to, like, look up and, like, I tried to see what they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=392" target="_blank">00:06:32.240</a></span> | <span class="t">talking about with this part. I didn't really, I wasn't really able to find what they were talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=395" target="_blank">00:06:35.760</a></span> | <span class="t">So if anyone knows, like, I would, I would definitely like to know that. But yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=400" target="_blank">00:06:40.160</a></span> | <span class="t">so the third reason is that QN 2.5 VL supports multimodal inputs. So just image and text. Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=406" target="_blank">00:06:46.400</a></span> | <span class="t">Oh, yeah. And they use, they use a latent space of, or latent of the last layer's hidden state from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=414" target="_blank">00:06:54.800</a></span> | <span class="t">QN 2.5 VL as the backbone. So they use that to, like, represent the user's input. So that's pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=421" target="_blank">00:07:01.120</a></span> | <span class="t">interesting. Let's see. So as for the VAE, let's see. So they train an image VAE with 2D convolutions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=428" target="_blank">00:07:08.400</a></span> | <span class="t">or that's usually how this works. They usually train a VAE with 2D convolutions on a massive image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=433" target="_blank">00:07:13.200</a></span> | <span class="t">dataset to obtain a high quality image representation. But what they do differently is they use a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=439" target="_blank">00:07:19.040</a></span> | <span class="t">encoder, dual decoder architecture. So they use a shared encoder. Oh, no. Shared encoder compatible with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=444" target="_blank">00:07:24.320</a></span> | <span class="t">images and videos alongside separate specialized decoders for each modality. So that's what they do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=449" target="_blank">00:07:29.600</a></span> | <span class="t">differently. And let's see what they do. Oh, they also, like, they also collected a lot of text-rich</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=456" target="_blank">00:07:36.160</a></span> | <span class="t">images. Like, they talk about here how it's, like, PDFs, PowerPoints, alongside, like, synthetic graphic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=461" target="_blank">00:07:41.840</a></span> | <span class="t">or synthetic paragraphs, and, like, covering different languages. And they use this to essentially to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=468" target="_blank">00:07:48.800</a></span> | <span class="t">train the model, like, later with post-training on RL. They use that to, like, train the model on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=475" target="_blank">00:07:55.280</a></span> | <span class="t">how to actually follow instructions. Like, oh, if I want to make a PowerPoint with, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=478" target="_blank">00:07:58.160</a></span> | <span class="t">this property and this property, they'll, like, use some of those, like, some of those documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=483" target="_blank">00:08:03.440</a></span> | <span class="t">Let's see. They do that. Let's see. Yeah. And for the diffusion part, they, let's see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=492" target="_blank">00:08:12.160</a></span> | <span class="t">they use a multimodal diffusion transformer. So they talk about how they have, like, their,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=496" target="_blank">00:08:16.320</a></span> | <span class="t">like, their MS rope, which is a multimodal scalable rope. So, like, the reason, well, I'll just read it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=503" target="_blank">00:08:23.920</a></span> | <span class="t">first. But let's see. So in the traditional, like, diffusion block, text tokens are directly concatenated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=509" target="_blank">00:08:29.120</a></span> | <span class="t">after the flattened image positional embedding. So they're talking about, like, this. So, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=514" target="_blank">00:08:34.800</a></span> | <span class="t">if your image is, like, if your image is split into nine pieces, then they'll, like, they'll just,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=519" target="_blank">00:08:39.040</a></span> | <span class="t">like, concatenate the text after. But what they do is, I'm reading from the blue text now. So they,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=526" target="_blank">00:08:46.640</a></span> | <span class="t">text inputs are treated as 2D tensors with identical position IDs applied across both dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=532" target="_blank">00:08:52.400</a></span> | <span class="t">and they're concatenated along the diagonal of the image. What that actually means is that they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=536" target="_blank">00:08:56.800</a></span> | <span class="t">their image here. And they essentially just pretend that the text, like, the text after is, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=542" target="_blank">00:09:02.720</a></span> | <span class="t">concatenated along this dimension. So they pretend the image is, like, in this case, they pretend the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=546" target="_blank">00:09:06.720</a></span> | <span class="t">is, like, a six, a six by six image. So that's what they do. And they say, like, the reason that they use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=556" target="_blank">00:09:16.080</a></span> | <span class="t">this is because, like, previous 2D ropes, like, with other implementations of their positional embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=563" target="_blank">00:09:23.280</a></span> | <span class="t">certain rows of positional encodings for text and image, the 0th middle row in figure 8b becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=568" target="_blank">00:09:28.560</a></span> | <span class="t">isomorphic. So essentially, the model becomes confused. They're talking about this part right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=572" target="_blank">00:09:32.160</a></span> | <span class="t">here. They say, like, with previous positional encodings, the model kind of becomes confused,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=579" target="_blank">00:09:39.520</a></span> | <span class="t">and it can, like, confuse these, like, this middle row. So it can become confused as to which, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=584" target="_blank">00:09:44.960</a></span> | <span class="t">which parts of the image correspond to text and which are actual, like, pieces of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=589" target="_blank">00:09:49.760</a></span> | <span class="t">Let's see. So this is where they talk about the actual, like, the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=596" target="_blank">00:09:56.800</a></span> | <span class="t">They talk about, like, the VLM, the VAE, and the transformer. So there's that. So 20 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=605" target="_blank">00:10:05.760</a></span> | <span class="t">parameters for the diffusion model in total.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=607" target="_blank">00:10:07.920</a></span> | <span class="t">Yeah. So does anyone have any questions so far? I realize I didn't pause for input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=616" target="_blank">00:10:16.160</a></span> | <span class="t">No, we're looking at chat. It seems good. You're doing good on Mark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=623" target="_blank">00:10:23.920</a></span> | <span class="t">I actually have a question. In the architecture diagram, up a little above, I don't see any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=632" target="_blank">00:10:32.000</a></span> | <span class="t">mention of... Maybe I just didn't read it properly. No, sorry. Up in the architectural diagram part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=638" target="_blank">00:10:38.320</a></span> | <span class="t">Oh, yeah. Yeah. Like, up in the previous page, I think. Yeah. So I don't... If I'm not... I don't see...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=645" target="_blank">00:10:45.280</a></span> | <span class="t">So is the diffusion process... Like, how does diffusion come in here? Is it just that they're adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=654" target="_blank">00:10:54.720</a></span> | <span class="t">noise in the autoencoder and then removing the noise progressively on the unpatch, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=666" target="_blank">00:11:06.160</a></span> | <span class="t">using the transformer blocks? Or is there something more complicated there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=669" target="_blank">00:11:09.520</a></span> | <span class="t">Yeah. So someone correct me if I'm wrong here, but, like, I don't see how they actually, like... I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=676" target="_blank">00:11:16.240</a></span> | <span class="t">see which, like, which diffusion objective they use. But, like, to my knowledge, a common diffusion objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=683" target="_blank">00:11:23.040</a></span> | <span class="t">is, like, not to reconstruct the image, but usually they'll estimate how much noise was added. Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=688" target="_blank">00:11:28.400</a></span> | <span class="t">first they'll... Like, with the forward pass, they'll, like, corrupt the data. Like, they'll gradually corrupt the data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=692" target="_blank">00:11:32.800</a></span> | <span class="t">using a specific amount of noise at each step. And, like, with a backward pass, instead of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=698" target="_blank">00:11:38.560</a></span> | <span class="t">actually trying to reconstruct the data or reconstruct the image or whatever modality it is, they'll try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=703" target="_blank">00:11:43.680</a></span> | <span class="t">estimate how much noise was taken away in that step. So I don't know if they put it here, but I wasn't able to find it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=710" target="_blank">00:11:50.400</a></span> | <span class="t">Yeah, okay. So it seems like it's just in the training regime and adding the noise. Okay. Understood. Thank you. That's helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=720" target="_blank">00:12:00.000</a></span> | <span class="t">Yeah. Any other questions? All right. Cool. Yeah. So now we go on to the data collection. So in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=733" target="_blank">00:12:13.120</a></span> | <span class="t">they actually put a lot of effort into data collection and curation. Well, I'm sure all, like, I'm sure all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=738" target="_blank">00:12:18.880</a></span> | <span class="t">companies/labs put effort into this, but, like, we're able to actually see what they do to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=744" target="_blank">00:12:24.800</a></span> | <span class="t">curate their data. So they annotated, they collected annotated billions of text image pairs. That's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=750" target="_blank">00:12:30.480</a></span> | <span class="t">And they prioritize data quality and balanced data distribution. So they try to create, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=755" target="_blank">00:12:35.040</a></span> | <span class="t">a well-balanced and representative data set that closely mirrors real-world scenarios. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=759" target="_blank">00:12:39.600</a></span> | <span class="t">interesting. And they categorize it into, like, four categories. So nature, design, people, and synthetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=764" target="_blank">00:12:44.160</a></span> | <span class="t">data. And, like, it's important to note, they say it, like, somewhere here, but it's very important to note</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=768" target="_blank">00:12:48.480</a></span> | <span class="t">that when they say synthetic data, they mean, like, PowerPoints and stuff like that. They do not mean AI-generated content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=774" target="_blank">00:12:54.240</a></span> | <span class="t">They take extra care to not include AI-generated content, like, in their data mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=780" target="_blank">00:13:00.880</a></span> | <span class="t">Yeah. So we'll just go through each of these. So they have their nature category. So it says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=787" target="_blank">00:13:07.120</a></span> | <span class="t">like, 55% of the data set. They have, like, their objects, landscape, cityscape, plants, animals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=792" target="_blank">00:13:12.160</a></span> | <span class="t">indoor, and food. So also it has, like, content that doesn't clearly belong to the people or design</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=796" target="_blank">00:13:16.960</a></span> | <span class="t">categories. So that's cool. So with the design category, oh, they also have, like, a graphic here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=803" target="_blank">00:13:23.200</a></span> | <span class="t">but I'll look at that later. So they also have, like, their design category. So it's 27% of the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=808" target="_blank">00:13:28.720</a></span> | <span class="t">So it's usually, like, posters, UIs, presentation slides, and, like, paintings, sculptures, arts and crafts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=814" target="_blank">00:13:34.880</a></span> | <span class="t">and digital arts. So, like, this, they say that this helps the model to form, like, to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=821" target="_blank">00:13:41.840</a></span> | <span class="t">emulate/replicate different art styles. That was interesting. So they also have the people data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=827" target="_blank">00:13:47.520</a></span> | <span class="t">That's 13% of the data set. So they pay special attention to this, because that's, like, portraits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=834" target="_blank">00:13:54.400</a></span> | <span class="t">sports and activities, and just, like, humans doing different things. So they say that it helps the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=841" target="_blank">00:14:01.520</a></span> | <span class="t">model to generate realistic and diverse human images. And finally, the synthetic data set. So it's around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=846" target="_blank">00:14:06.320</a></span> | <span class="t">5% of the data set. So, again, like I said before, it does not include images generated by other AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=851" target="_blank">00:14:11.600</a></span> | <span class="t">models. But it's, like, data synthesized to control text rendering techniques. So it includes, like, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=857" target="_blank">00:14:17.440</a></span> | <span class="t">see. Let's see. Oh, yeah. So they adopt a conservative stance towards such data as training on low fidelity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=863" target="_blank">00:14:23.520</a></span> | <span class="t">or misleading images may weaken the model's generalization capabilities. So, yeah. So let's go to the graphic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=869" target="_blank">00:14:29.920</a></span> | <span class="t">So this is, like, a visual representation of, like, of the proportion of, like, the nature of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=875" target="_blank">00:14:35.520</a></span> | <span class="t">different classes and the different, like, subclasses, like objects, cityscape, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=879" target="_blank">00:14:39.280</a></span> | <span class="t">Yeah. So that is that. So they have data filtering. They have, like, I think, like, seven to ten stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=888" target="_blank">00:14:48.880</a></span> | <span class="t">How many stages? They have a lot of stages of, like, filtering and pre-training. So this is kind of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=896" target="_blank">00:14:56.480</a></span> | <span class="t">I get the gist from reading this section that they had, like, a seven to ten stage filtering process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=904" target="_blank">00:15:04.000</a></span> | <span class="t">but they also started doing curriculum learning at the same time while they were filtering the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=908" target="_blank">00:15:08.400</a></span> | <span class="t">I'm not sure if that's actually the case, but that's kind of what it seems like from here. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=912" target="_blank">00:15:12.400</a></span> | <span class="t">let's see. Oh, yeah. They have seven sequential stages. So, yeah, synthetic data is introduced from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=919" target="_blank">00:15:19.680</a></span> | <span class="t">stage four. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=920" target="_blank">00:15:20.480</a></span> | <span class="t">Yeah. They mention it in the abstract that it's kind of important, right? So they have a,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=925" target="_blank">00:15:25.600</a></span> | <span class="t">they adopt a progressive training strategy and then they kind of go through their stages, right? So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=931" target="_blank">00:15:31.760</a></span> | <span class="t">there's non-text-to-text rendering evolves from simple complex textual inputs, gradually scales up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=937" target="_blank">00:15:37.520</a></span> | <span class="t">paragraph level descriptions. So they do have, like, this curriculum learning and it's kind of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=944" target="_blank">00:15:44.960</a></span> | <span class="t">um, it builds from simplicity to more advanced stuff to, like, the most niche little synthetic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=951" target="_blank">00:15:51.920</a></span> | <span class="t">Yeah. They cover it later too, yeah. Okay. Thanks. I appreciate that. Yeah. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=959" target="_blank">00:15:59.200</a></span> | <span class="t">I was like, I'm not going to go, like, super in-depth for each stage, but, like, so let's see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=964" target="_blank">00:16:04.320</a></span> | <span class="t">they have the initial pre-training data. So, like, this is what I talked about or reference earlier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=968" target="_blank">00:16:08.800</a></span> | <span class="t">where they trained on, like, very small images, like 256 by 256 pixels, like, various aspect ratios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=974" target="_blank">00:16:14.240</a></span> | <span class="t">So they kind of list them there. Uh, so they also, like, remove low quality and relevant images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=979" target="_blank">00:16:19.520</a></span> | <span class="t">So they make sure they remove duplicates and they remove, like, really, really low quality images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=986" target="_blank">00:16:26.320</a></span> | <span class="t">right here and also NSFW stuff. Yeah. Let's see. They also, let's see. So onto stage two, they focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=994" target="_blank">00:16:34.720</a></span> | <span class="t">improving the image quality. So they remove images, like, with significant rotation or flipping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1000" target="_blank">00:16:40.240</a></span> | <span class="t">That's this part. Uh, let's see. They describe, like, blurry, out-of-focus images, excessively bright</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1008" target="_blank">00:16:48.880</a></span> | <span class="t">or dark or images, like, with unnaturally high color saturation. They also remove images with, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1015" target="_blank">00:16:55.120</a></span> | <span class="t">low entropy. So that's just, like, only black or only white images or whatever. And they discard images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1021" target="_blank">00:17:01.360</a></span> | <span class="t">with overly complex textures or, yeah, which is associated with noise and non-sematic patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1026" target="_blank">00:17:06.720</a></span> | <span class="t">So this is also a graphic of, um, like, of their filtering process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1031" target="_blank">00:17:11.440</a></span> | <span class="t">Yeah. So stage three. Let's see. So this is actually where they do, uh, or where they talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1039" target="_blank">00:17:19.120</a></span> | <span class="t">about some of their annotation. So here's where they start to focus on text. So they start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1044" target="_blank">00:17:24.000</a></span> | <span class="t">focus on improving the alignment between textual description and visual content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1048" target="_blank">00:17:28.480</a></span> | <span class="t">So they, I'm reading the blue part here, but they, uh, they have, like, two splits. So they have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1055" target="_blank">00:17:35.520</a></span> | <span class="t">like, captions provided by websites as well as metadata, such as titles or tags originally associated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1060" target="_blank">00:17:40.000</a></span> | <span class="t">with the images. Uh, so they also have, let's see, captions generated by Quent. So, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1065" target="_blank">00:17:45.920</a></span> | <span class="t">they also use Quent to help in their, like, data annotation process. Um, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1072" target="_blank">00:17:52.320</a></span> | <span class="t">So this is just talking about the, how they also combine raw captions and synthesized captions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1076" target="_blank">00:17:56.400</a></span> | <span class="t">They also discard, like, trash captions, like, really long captions or, like, generic ones. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1083" target="_blank">00:18:03.040</a></span> | <span class="t">like, sorry, I cannot provide a caption. I mean, indicating that the caption is broken or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1086" target="_blank">00:18:06.800</a></span> | <span class="t">something else is wrong. So they also talk about, let's see, text rendering. So this is H4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1091" target="_blank">00:18:11.760</a></span> | <span class="t">text rendering. They, interestingly, they split their languages into English, Chinese, or other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1097" target="_blank">00:18:17.200</a></span> | <span class="t">which I did not expect. But apparently it worked for them. Uh, let's see. Yeah, they address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1104" target="_blank">00:18:24.720</a></span> | <span class="t">challenges such as low frequency characters, mixed language scenarios, and font diversity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1109" target="_blank">00:18:29.280</a></span> | <span class="t">So they incorporate synthetic text rendering data. And let's see, they also remove images with overly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1115" target="_blank">00:18:35.920</a></span> | <span class="t">dense or excessively small text. So they do that to increase their text quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1124" target="_blank">00:18:44.240</a></span> | <span class="t">Oh, and this is also an interesting graphic where they show, uh, like, they show kind of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1127" target="_blank">00:18:47.920</a></span> | <span class="t">distributions on some of their, like, their filters. And you can kind of see the examples of some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1132" target="_blank">00:18:52.560</a></span> | <span class="t">these images. I kind of found it helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1134" target="_blank">00:18:54.160</a></span> | <span class="t">Yeah, so stage five. So they talk about how the model transitions to training with, uh, training with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1144" target="_blank">00:19:04.000</a></span> | <span class="t">images at 640p resolution. So they're increasing the, like, they're making their images sharper and just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1148" target="_blank">00:19:08.880</a></span> | <span class="t">increasing the resolution, uh, presumably to make the training more stable. So they also apply more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1155" target="_blank">00:19:15.440</a></span> | <span class="t">filters. So let's see, they, they try to remove images that have, like, overexposure, underexposure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1161" target="_blank">00:19:21.520</a></span> | <span class="t">blur, or compression artifacts, or all poor, or, or composition or visual appeal. They also remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1167" target="_blank">00:19:27.600</a></span> | <span class="t">images containing watermarks to our codes and stuff like that. So stage six, they kind of focus more on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1172" target="_blank">00:19:32.960</a></span> | <span class="t">portraits. Uh, let's see. Yeah, so they categorize their data set into three categories. So general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1180" target="_blank">00:19:40.880</a></span> | <span class="t">portrait and text rendering. So that's what it sounds like. Stage seven, this is balanced multi-scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1188" target="_blank">00:19:48.160</a></span> | <span class="t">training. So again, they're increasing their resolution. And they, interestingly, they design a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1194" target="_blank">00:19:54.000</a></span> | <span class="t">hierarchical taxonomy system for image categorization. So within each category, they retain only images with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1200" target="_blank">00:20:00.560</a></span> | <span class="t">highest quality. So here they're, this is essentially, like, data QA. So just making sure that their, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1206" target="_blank">00:20:06.400</a></span> | <span class="t">their model, um, doesn't generate really bad images, like, of a certain, like, of a certain type. So, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1213" target="_blank">00:20:13.440</a></span> | <span class="t">they build a tree of all, you know, of all, like, objects that they have. And they just essentially check,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1218" target="_blank">00:20:18.640</a></span> | <span class="t">like, oh, are, like, like, when the model generates buildings, do the buildings look good? Etc. Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1224" target="_blank">00:20:24.560</a></span> | <span class="t">like, do the landscapes look good? Do the parks look good? Etc. They do that. They also, uh, while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1232" target="_blank">00:20:32.640</a></span> | <span class="t">they're, like, within each category, they retain images with the highest quality. And they also allow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1238" target="_blank">00:20:38.480</a></span> | <span class="t">like, they make sure to balance their data so that they allow the model to retain previously learned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1242" target="_blank">00:20:42.640</a></span> | <span class="t">general knowledge and ensure stable convergence while adapting to higher resolution images. So presumably,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1247" target="_blank">00:20:47.920</a></span> | <span class="t">this is to combat, like, catastrophic forgetting, where your model trains on a specific, uh, subset of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1252" target="_blank">00:20:52.800</a></span> | <span class="t">data, but, like, kind of loses its generality. So that's what they do in stage. Yeah, stage seven?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1258" target="_blank">00:20:58.240</a></span> | <span class="t">Yeah, I think stage one to seven really shows how much care they put to cleaning the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1264" target="_blank">00:21:04.560</a></span> | <span class="t">whereby they had so many simple filters just to check for resolution, check for cropping, check for flipping,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1271" target="_blank">00:21:11.920</a></span> | <span class="t">and everything. Um, and that's what led to this strong model. Actually, then, then the question becomes, hey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1279" target="_blank">00:21:19.040</a></span> | <span class="t">if we had left, left that data in, would the model be just as good? It's unknown. I don't know if we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1284" target="_blank">00:21:24.400</a></span> | <span class="t">actually train such a model on deliberately train a model on bad data. Um, but, I mean, their pipeline is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1291" target="_blank">00:21:31.040</a></span> | <span class="t">quite fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1291" target="_blank">00:21:31.920</a></span> | <span class="t">I think it might agree. Um, they, they share some stuff with, like, why they don't do synthetic data from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1300" target="_blank">00:21:40.480</a></span> | <span class="t">uh, you know, images generated by other models and noisy stuff. And they, they say that that would harm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1307" target="_blank">00:21:47.440</a></span> | <span class="t">the quality, right? Yeah, because there's a lot of artifacts, right? Yeah. Exactly. So if you train on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1312" target="_blank">00:21:52.960</a></span> | <span class="t">poor quality data, you will not get good model, but that's, that's, you know, it seems necessary to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1319" target="_blank">00:21:59.600</a></span> | <span class="t">need to do all this then. Yeah, that seems so. And if we look at, um, figure 10, right, I think like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1326" target="_blank">00:22:06.080</a></span> | <span class="t">they shrank their data set maybe by, by two thirds. So a lot of it is, it's filtering and I, I, I really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1335" target="_blank">00:22:15.120</a></span> | <span class="t">enjoyed figure 10. So it really just shows you how much care needs to be taken for doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1340" target="_blank">00:22:20.640</a></span> | <span class="t">Yeah. Yeah. So does anyone else have any other questions or comments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1351" target="_blank">00:22:31.840</a></span> | <span class="t">Did they mention, uh, if this is a semi-automatic, uh, filtering because, uh, you know, it's, uh, billions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1362" target="_blank">00:22:42.400</a></span> | <span class="t">of images and, uh, it's a huge effort to annotate and filter, uh, based on quality and, uh, all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1371" target="_blank">00:22:51.920</a></span> | <span class="t">Yeah. So they like in the, in the technical, in the technical report itself, they just talk about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1380" target="_blank">00:23:00.480</a></span> | <span class="t">they say like, oh, we applied in entropy filter. So I'm assuming that they programmatically do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1385" target="_blank">00:23:05.760</a></span> | <span class="t">Uh, maybe there's more data in the, what's it called in the appendix. I didn't read the appendix or I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1390" target="_blank">00:23:10.240</a></span> | <span class="t">don't even know if there is an appendix. Yeah. It just sounds like they, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1394" target="_blank">00:23:14.000</a></span> | <span class="t">No, go for it. Oh yeah. No, I was also, I was just saying like, yeah, I apply, like, I imagine that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1399" target="_blank">00:23:19.920</a></span> | <span class="t">they just apply, um, they just say like, oh, if the entropy is greater than this in this image, then they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1404" target="_blank">00:23:24.160</a></span> | <span class="t">just like discard it and do something similar or attempt to do something similar with the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1408" target="_blank">00:23:28.000</a></span> | <span class="t">filters. Yeah. I imagine that pipeline is completely automatic. Developing the filters is not automatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1414" target="_blank">00:23:34.400</a></span> | <span class="t">in a sense. They probably need to figure out what the, what, what the right threshold is so that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1418" target="_blank">00:23:38.960</a></span> | <span class="t">exclude most of the defects without losing out too much, too much good data. But you can imagine that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1424" target="_blank">00:23:44.240</a></span> | <span class="t">you know, you have a team of 20, everyone just takes one of these filters and then you be your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1428" target="_blank">00:23:48.960</a></span> | <span class="t">evals and figure out how to cut it, get high precision and recall. And then once it's there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1433" target="_blank">00:23:53.200</a></span> | <span class="t">it's just, it's just a very simple, uh, CPU intensive task. Right. That open CV probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1438" target="_blank">00:23:58.880</a></span> | <span class="t">has quite a few of these as well. And then once it, I just pass everything through. And I think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1444" target="_blank">00:24:04.160</a></span> | <span class="t">will work very well. Yeah. But I'm sure that they also did some quality, uh, control checks on a small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1451" target="_blank">00:24:11.200</a></span> | <span class="t">sample subsets. Oh yeah. Yeah. I'm also sure. Yeah. Okay. So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1462" target="_blank">00:24:22.240</a></span> | <span class="t">Data annotation. Oh yeah. So they go on to talk about their data annotation, which I think is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1466" target="_blank">00:24:26.480</a></span> | <span class="t">really interesting because they, they essentially have Quen generate, uh, a JSON. So I'll actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1472" target="_blank">00:24:32.160</a></span> | <span class="t">read some of this. So say we use a capable image captioner. So they're talking about Quen 2.5 VL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1476" target="_blank">00:24:36.480</a></span> | <span class="t">that generate, uh, comprehensive image descriptions, but also structured metadata that captures essential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1481" target="_blank">00:24:41.920</a></span> | <span class="t">image properties and attributes. So like, instead of treating captioning and metadata extraction as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1487" target="_blank">00:24:47.200</a></span> | <span class="t">independent tasks, the captioner concurrently describes visual content and generates detailed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1492" target="_blank">00:24:52.320</a></span> | <span class="t">information in a structured format, such as JSON. So critical details such as like object attributes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1497" target="_blank">00:24:57.440</a></span> | <span class="t">spatial relationships, environmental context, and verbatim, uh, translations of visible text are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1501" target="_blank">00:25:01.920</a></span> | <span class="t">captured. So they capture key image properties and report it in a structured format. So I think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1508" target="_blank">00:25:08.080</a></span> | <span class="t">that's really interesting. I don't know how many other like labs do this, but yeah, I just think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1514" target="_blank">00:25:14.560</a></span> | <span class="t">really interesting that like, they, they treated them both as the same like image or is it, uh, image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1519" target="_blank">00:25:19.200</a></span> | <span class="t">captioning and like metadata extraction to like capture a bunch of different relationships from the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1525" target="_blank">00:25:25.280</a></span> | <span class="t">that might not just be captured with the image caption. Yeah. I also, I thought this was really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1533" target="_blank">00:25:33.120</a></span> | <span class="t">interesting and that they're actually using the, the vision language model in two ways. One is they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1538" target="_blank">00:25:38.960</a></span> | <span class="t">doing it, using it to annotate like this, but then they're also using it to embed the language into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1545" target="_blank">00:25:45.840</a></span> | <span class="t">vision, uh, the vision, uh, the vision embedding space, right. And then there, and then like sort of losing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1553" target="_blank">00:25:53.440</a></span> | <span class="t">across, uh, across attention to align. I thought that was really interesting that they were sort of using it in two ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1562" target="_blank">00:26:02.560</a></span> | <span class="t">Yeah, I agree. I thought it was pretty cool. The two papers they reference on this, um, siglip 2 and Chinese clip, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1575" target="_blank">00:26:15.280</a></span> | <span class="t">the siglip 2, I looked into that paper. It's kind of, it's kind of interesting. It's like a, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1579" target="_blank">00:26:19.760</a></span> | <span class="t">a very recent 2025 iteration of clip, but it's cool to see how much stuff keeps pushing there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1587" target="_blank">00:26:27.600</a></span> | <span class="t">And then, uh, you know, they even mentioned like in that stage three, after you do captioning, how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1595" target="_blank">00:26:35.680</a></span> | <span class="t">they, you know, how do they still filter itself? So like there's token length, you gotta remove stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1601" target="_blank">00:26:41.280</a></span> | <span class="t">you gotta filter stuff that says I can't caption this image. Um, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1604" target="_blank">00:26:44.960</a></span> | <span class="t">there's even more filters in this stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1607" target="_blank">00:26:47.280</a></span> | <span class="t">Yeah. Wait, do you know what Chinese clip did differently than like regular clip?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1612" target="_blank">00:26:52.640</a></span> | <span class="t">I'm pretty sure that the captions are in Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1615" target="_blank">00:26:55.920</a></span> | <span class="t">I haven't checked, but you know, I would assume Chinese clip is clip in Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1622" target="_blank">00:27:02.480</a></span> | <span class="t">I meant like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1624" target="_blank">00:27:04.080</a></span> | <span class="t">I meant like they do anything differently in terms of like diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1629" target="_blank">00:27:09.600</a></span> | <span class="t">So they, they have two, two, two things that they reference. Uh, sig clip is not Chinese clip. It's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1635" target="_blank">00:27:15.040</a></span> | <span class="t">it's, uh, it's a variation that builds on top of clip and that's not specific. That's not like specific to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1642" target="_blank">00:27:22.320</a></span> | <span class="t">Chinese. That's from deep mind in like February, 2025. It's, it's just an improvement on clip,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1648" target="_blank">00:27:28.800</a></span> | <span class="t">but you know, they probably merged that with the Chinese clip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1652" target="_blank">00:27:32.080</a></span> | <span class="t">Okay. Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1653" target="_blank">00:27:33.120</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1654" target="_blank">00:27:34.240</a></span> | <span class="t">Right. Okay. So let's say we did that. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1658" target="_blank">00:27:38.880</a></span> | <span class="t">Okay. Data synthesis. Uh, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1665" target="_blank">00:27:45.360</a></span> | <span class="t">Uh, oh yeah. So they talk about how apparently this is a problem with, uh, like with Chinese where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1674" target="_blank">00:27:54.480</a></span> | <span class="t">like there are some characters that are just really, really rare, but are still important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1678" target="_blank">00:27:58.080</a></span> | <span class="t">So I say given the long tail distribution of textual context, relying solely on, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1682" target="_blank">00:28:02.640</a></span> | <span class="t">naturally occurring text is insufficient to ensure adequate exposure to these rare characters during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1686" target="_blank">00:28:06.720</a></span> | <span class="t">model training. So to address this, they use like a multi-stage text-aware image synthesis pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1692" target="_blank">00:28:12.800</a></span> | <span class="t">It's like, they have three stages and they kind of describe it. Uh, so the most straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1699" target="_blank">00:28:19.520</a></span> | <span class="t">way it's like to train the model to recognize and generate characters. So like they make text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1703" target="_blank">00:28:23.920</a></span> | <span class="t">or they extract text paragraphs from large scale, high quality corpora, and they render it onto clean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1708" target="_blank">00:28:28.320</a></span> | <span class="t">backgrounds. And so they also like implement QA or quality control. So if any character within a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1714" target="_blank">00:28:34.800</a></span> | <span class="t">paragraph cannot be rendered due to limitations, the entire paragraph is discarded. So again, like they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1719" target="_blank">00:28:39.760</a></span> | <span class="t">really, really care about having like clean data. Uh, yeah. So they maintain a high fidelity. Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1727" target="_blank">00:28:47.360</a></span> | <span class="t">So this is an example of the first one, the paragraph that I just talked about. So they do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1733" target="_blank">00:28:53.360</a></span> | <span class="t">So they also do compositional rendering and contextual scenes. So they just embed a embed synthetic text into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1741" target="_blank">00:29:01.520</a></span> | <span class="t">realistic visual context. So they use QNVL captioner to generate descriptive captions for each synthesized image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1747" target="_blank">00:29:07.840</a></span> | <span class="t">capturing contextual relationships between the text and surrounding visual elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1750" target="_blank">00:29:10.880</a></span> | <span class="t">So an example of this is like this right here. And the third is, let's see, complex rendering and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1757" target="_blank">00:29:17.920</a></span> | <span class="t">structured templates. So they follow complex structured prompts involving layout sensitive content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1763" target="_blank">00:29:23.280</a></span> | <span class="t">So they propose a synthesis strategy based on programmatic editing of predefined templates,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1767" target="_blank">00:29:27.520</a></span> | <span class="t">such as PowerPoint slides or user interface mockups. This is kind of what I was talking about at the very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1770" target="_blank">00:29:30.960</a></span> | <span class="t">beginning where they, like, they kind of use some of their PowerPoint such like synthetic, quote unquote, synthetic images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1777" target="_blank">00:29:37.840</a></span> | <span class="t">Uh, that kind of like teach it to follow instructions or teach it to like how to place different, like how to use graphics or like manipulate graphics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1786" target="_blank">00:29:46.880</a></span> | <span class="t">So like, again, this is an example of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1790" target="_blank">00:29:50.160</a></span> | <span class="t">I thought this section was very interesting, right? Because what they're doing here is they're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1800" target="_blank">00:30:00.080</a></span> | <span class="t">they're generating a very different form of synthetic data, right? This is not synthetic data from a diffusion model or like an LLM that's just added. Like you don't, you're not really doing distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1812" target="_blank">00:30:12.480</a></span> | <span class="t">Uh, you know, the first one, uh, you know, the first one, the pure rendering is very interesting, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1816" target="_blank">00:30:16.720</a></span> | <span class="t">They're, they're basically writing paragraphs, like they're extracting text and just rendering it onto clean backgrounds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1825" target="_blank">00:30:25.680</a></span> | <span class="t">Like, you know, in Photoshop where you can like have text and just paste it into different backgrounds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1830" target="_blank">00:30:30.400</a></span> | <span class="t">Uh, that's, that's a form of a synthetic image, but it's not a generated image, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1836" target="_blank">00:30:36.080</a></span> | <span class="t">So they mentioned this more earlier in the paper as well, where their synthetic data gen is like very different, it's, it's not generated images, it's text that's written and then rendered in synthetic sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1849" target="_blank">00:30:49.280</a></span> | <span class="t">But yeah, it's like pure, pure text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1852" target="_blank">00:30:52.080</a></span> | <span class="t">And then the thing that you mentioned with the random characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1856" target="_blank">00:30:56.960</a></span> | <span class="t">So the, the, the part of that is actually that in, in languages like Chinese, there's tail end characters that don't show up a lot, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1865" target="_blank">00:31:05.920</a></span> | <span class="t">Like you, you just won't see these that often, but you still need to be able to understand them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1872" target="_blank">00:31:12.000</a></span> | <span class="t">So this synthetic text properly brings back in the tail end of distribution that you don't see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1878" target="_blank">00:31:18.560</a></span> | <span class="t">So it's like, you know, in English, we have like only so many characters, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1882" target="_blank">00:31:22.080</a></span> | <span class="t">26 letters, they're pretty distributed, but in, in Chinese there are characters that don't show up and they, they render them and then they synthetically add them back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1891" target="_blank">00:31:31.600</a></span> | <span class="t">But it's a very different type of, uh, synthetic data gen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1895" target="_blank">00:31:35.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1897" target="_blank">00:31:37.760</a></span> | <span class="t">I'm pretty surprised it works as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1901" target="_blank">00:31:41.840</a></span> | <span class="t">Like, yeah, they just have templates of PowerPoints and then they just add in words, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1907" target="_blank">00:31:47.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1909" target="_blank">00:31:49.120</a></span> | <span class="t">I'm also kind of surprised because like, I thought that if there were rare characters and they would rarely show up in, um, like in these like pure rendering, like these types of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1918" target="_blank">00:31:58.720</a></span> | <span class="t">But I mean, maybe they artificially, like maybe they just like manually found which characters are rare and just like ensured that there are more of those available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1928" target="_blank">00:32:08.400</a></span> | <span class="t">So that's exactly what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1930" target="_blank">00:32:10.400</a></span> | <span class="t">Yeah, I can imagine that's what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1932" target="_blank">00:32:12.560</a></span> | <span class="t">That's, that's not a real image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1934" target="_blank">00:32:14.480</a></span> | <span class="t">That's a paragraph of text that exists, and then they just paste that onto a green background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1940" target="_blank">00:32:20.560</a></span> | <span class="t">Oh yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1940" target="_blank">00:32:20.560</a></span> | <span class="t">But this is not an image that exists, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1942" target="_blank">00:32:22.560</a></span> | <span class="t">That's why it's synthetic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1943" target="_blank">00:32:23.760</a></span> | <span class="t">Oh yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1944" target="_blank">00:32:24.160</a></span> | <span class="t">It's not telling, it's not telling a model to generate something with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1947" target="_blank">00:32:27.760</a></span> | <span class="t">They actually just purely printed out this paragraph and then overlay it on a green background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1953" target="_blank">00:32:33.200</a></span> | <span class="t">And same thing with the ones below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1954" target="_blank">00:32:34.720</a></span> | <span class="t">That's what makes this unique form of synthetic data gen, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1957" target="_blank">00:32:37.680</a></span> | <span class="t">Because it's, yeah, it's just image editing, but it's, it's not generated per se.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1962" target="_blank">00:32:42.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1964" target="_blank">00:32:44.320</a></span> | <span class="t">And then they do this, they adjust font, sizing, spacing, all that stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1968" target="_blank">00:32:48.880</a></span> | <span class="t">And then, you know, when you type out this paragraph, if one character is off, they discard it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1974" target="_blank">00:32:54.320</a></span> | <span class="t">So it's, it's pure clean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1975" target="_blank">00:32:55.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1978" target="_blank">00:32:58.080</a></span> | <span class="t">And this helps with like tiny text too, I think they said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1981" target="_blank">00:33:01.440</a></span> | <span class="t">So like, they can super scale it down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1983" target="_blank">00:33:03.200</a></span> | <span class="t">And as long as it's correct, you know, synthetic small text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1985" target="_blank">00:33:05.760</a></span> | <span class="t">Yeah, that's really useful then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1988" target="_blank">00:33:08.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1989" target="_blank">00:33:09.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1989" target="_blank">00:33:09.600</a></span> | <span class="t">But the, the, the cool thing is after you do this, like in the second blob there of text being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=1999" target="_blank">00:33:19.840</a></span> | <span class="t">overlaid, sorry, if you go up a little bit, the second one, like, you know, this, I love you too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2005" target="_blank">00:33:25.280</a></span> | <span class="t">So this is text that was thrown on a piece of paper in a background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2008" target="_blank">00:33:28.720</a></span> | <span class="t">They still have to pass this back in through their captioner, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2012" target="_blank">00:33:32.800</a></span> | <span class="t">So even though it's like fake image, not synthetically generated, they, they have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2018" target="_blank">00:33:38.960</a></span> | <span class="t">still throw it in the captioner and have descriptions without metadata and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2022" target="_blank">00:33:42.880</a></span> | <span class="t">So it's, it's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2023" target="_blank">00:33:43.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2025" target="_blank">00:33:45.600</a></span> | <span class="t">Does anyone else want to say anything?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2028" target="_blank">00:33:48.880</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2031" target="_blank">00:33:51.600</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2034" target="_blank">00:33:54.800</a></span> | <span class="t">So this is, so this is a pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2037" target="_blank">00:33:57.680</a></span> | <span class="t">So, so like, first I'll just read some of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2040" target="_blank">00:34:00.880</a></span> | <span class="t">So they adopt a flow matching training objective to pre-train Quen image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2044" target="_blank">00:34:04.320</a></span> | <span class="t">So it facilitates stable learning dynamics via ordinary differential equations while preserving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2048" target="_blank">00:34:08.640</a></span> | <span class="t">occultence to the maximum likelihood objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2050" target="_blank">00:34:10.400</a></span> | <span class="t">Uh, so this is, this is essentially like the diffusion part of their model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2056" target="_blank">00:34:16.560</a></span> | <span class="t">So like, let's see, they have, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2060" target="_blank">00:34:20.400</a></span> | <span class="t">So yeah, you can just, you can throw it into chat.tpt if you want, but like essentially they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2065" target="_blank">00:34:25.120</a></span> | <span class="t">train the data to point towards, I think like that, or they start with noise and they train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2071" target="_blank">00:34:31.360</a></span> | <span class="t">like the noise to point towards like the actual Bonavilla data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2077" target="_blank">00:34:37.200</a></span> | <span class="t">Uh, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2077" target="_blank">00:34:37.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2079" target="_blank">00:34:39.280</a></span> | <span class="t">Then the model trained to predict the target velocity and the loss function is defined as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2082" target="_blank">00:34:42.400</a></span> | <span class="t">the mean squared error between the predicted output and the ground truth velocity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2087" target="_blank">00:34:47.120</a></span> | <span class="t">So I use like a mean squared error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2088" target="_blank">00:34:48.400</a></span> | <span class="t">So I try to get the velocity to match.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2092" target="_blank">00:34:52.320</a></span> | <span class="t">So essentially you get it to like point in the direction of the real data, uh, like in this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2097" target="_blank">00:34:57.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2100" target="_blank">00:35:00.560</a></span> | <span class="t">So they also talk about how they optimize, uh, their models for like GPU usage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2106" target="_blank">00:35:06.560</a></span> | <span class="t">So they use something called like what they call a producer consumer framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2110" target="_blank">00:35:10.320</a></span> | <span class="t">It decouples data pre-processing from model training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2112" target="_blank">00:35:12.800</a></span> | <span class="t">So this design enables both stages to operate asynchronously and add optimal efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2117" target="_blank">00:35:17.280</a></span> | <span class="t">So on the producer side, the selected data is encoded into latent representation using MLM models and VAE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2124" target="_blank">00:35:24.640</a></span> | <span class="t">So like they have, uh, two types of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2127" target="_blank">00:35:27.760</a></span> | <span class="t">Like they dedicate some GPUs to like do the producer, like do the producer's work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2132" target="_blank">00:35:32.400</a></span> | <span class="t">And they dedicate some GPUs to do the consumer's work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2134" target="_blank">00:35:34.800</a></span> | <span class="t">So the consumer GPUs are dedicated exclusively to model training and every data parallel group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2139" target="_blank">00:35:39.680</a></span> | <span class="t">asynchronously pulled pre-pulls pre-processed batches directly from the producer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2143" target="_blank">00:35:43.920</a></span> | <span class="t">It's like, again, like they have, uh, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2146" target="_blank">00:35:46.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2146" target="_blank">00:35:46.960</a></span> | <span class="t">So they have like their pre-produced GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2148" target="_blank">00:35:48.720</a></span> | <span class="t">They encode like the data to relate in representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2151" target="_blank">00:35:51.840</a></span> | <span class="t">And they just like stack them up somewhere in memory or in storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2155" target="_blank">00:35:55.280</a></span> | <span class="t">And when, like whenever the consumer GPUs, uh, like whenever they finish with their previous batch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2160" target="_blank">00:36:00.560</a></span> | <span class="t">they're ready, they just like asynchronously pull the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2162" target="_blank">00:36:02.800</a></span> | <span class="t">Uh, so I think that's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2166" target="_blank">00:36:06.480</a></span> | <span class="t">So let's see, so distributed training optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2169" target="_blank">00:36:09.440</a></span> | <span class="t">So they use like hybrid parallelism strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2171" target="_blank">00:36:11.440</a></span> | <span class="t">So they combined data parallelism and tensor parallelism to efficiently scale training across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2176" target="_blank">00:36:16.800</a></span> | <span class="t">large GPU clusters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2177" target="_blank">00:36:17.920</a></span> | <span class="t">So this is like less, it's like, I'm less confident, uh, of like what this, or like of the specifics of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2183" target="_blank">00:36:23.680</a></span> | <span class="t">this part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2184" target="_blank">00:36:24.320</a></span> | <span class="t">So if anyone wants to like jump in, they're like more than welcome to, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2188" target="_blank">00:36:28.240</a></span> | <span class="t">they also talk about like distributed optimizer and activation checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2191" target="_blank">00:36:31.360</a></span> | <span class="t">Like to alleviate GPU memory pressure with minimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2194" target="_blank">00:36:34.080</a></span> | <span class="t">recomputation overhead during that prop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2196" target="_blank">00:36:36.560</a></span> | <span class="t">So we experimented with both distributed optimizers and activation checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2201" target="_blank">00:36:41.040</a></span> | <span class="t">However, activation checkpointing introduces substantial computational overhead and backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2204" target="_blank">00:36:44.800</a></span> | <span class="t">paths, which can significantly degrade training speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2207" target="_blank">00:36:47.040</a></span> | <span class="t">So they observed that enabling activation checkpointing reduced per GPU memory consumption by 11%,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2213" target="_blank">00:36:53.200</a></span> | <span class="t">but like at the cost of increasing per iteration time by essentially almost four times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2218" target="_blank">00:36:58.480</a></span> | <span class="t">So from two to seven and a half seconds per iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2221" target="_blank">00:37:01.200</a></span> | <span class="t">And like they say that based on the trade-off, they ultimately opted to disable activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2225" target="_blank">00:37:05.040</a></span> | <span class="t">checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2225" target="_blank">00:37:05.600</a></span> | <span class="t">So I think that was pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2227" target="_blank">00:37:07.840</a></span> | <span class="t">I think there's an important point here, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2230" target="_blank">00:37:10.400</a></span> | <span class="t">In the sense that, um, with activation checkpointing, you're like 75% slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2235" target="_blank">00:37:15.840</a></span> | <span class="t">Um, but you only save like 11% GPU memory in, in a sense you could sort of reduce your batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2241" target="_blank">00:37:21.520</a></span> | <span class="t">and actually go away faster and you would actually make up for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2243" target="_blank">00:37:23.840</a></span> | <span class="t">Um, in some of my experiments, I mean, the default, if you ask Claude to write some code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2249" target="_blank">00:37:29.040</a></span> | <span class="t">the default is to enable activation checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2250" target="_blank">00:37:30.880</a></span> | <span class="t">I think because everyone's was fine tuning on very small LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2254" target="_blank">00:37:34.160</a></span> | <span class="t">Uh, and you know, I think by in hugging phase, the default is just to enable activation checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2259" target="_blank">00:37:39.600</a></span> | <span class="t">But when you turn it off, uh, at least I was able to see a 20% speed up with not very much increase in memory either.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2265" target="_blank">00:37:45.920</a></span> | <span class="t">So I think it's something to observe, uh, something, if you do train your own models or finding your own models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2270" target="_blank">00:37:50.880</a></span> | <span class="t">like do consider not using activation checkpointing, uh, just for your, uh, training loop to run faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2278" target="_blank">00:37:58.240</a></span> | <span class="t">Um, so it's, it's nice to see another data point here in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2282" target="_blank">00:38:02.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2283" target="_blank">00:38:03.680</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2284" target="_blank">00:38:04.320</a></span> | <span class="t">Would it like, would a middle point in that trade off just be to have activation checkpointing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2288" target="_blank">00:38:08.800</a></span> | <span class="t">but just to like checkpoint it less frequently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2290" target="_blank">00:38:10.640</a></span> | <span class="t">Like wouldn't that work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2292" target="_blank">00:38:12.240</a></span> | <span class="t">I, I don't think that that works that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2294" target="_blank">00:38:14.800</a></span> | <span class="t">I think activation checkpointing is needed every time you do a backward pass and optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2298" target="_blank">00:38:18.800</a></span> | <span class="t">So you, you, you definitely need to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2300" target="_blank">00:38:20.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2301" target="_blank">00:38:21.120</a></span> | <span class="t">Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2302" target="_blank">00:38:22.640</a></span> | <span class="t">Interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2303" target="_blank">00:38:23.120</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2303" target="_blank">00:38:23.280</a></span> | <span class="t">So, uh, I mean, try it for what's worth if you have extra memory on your GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2311" target="_blank">00:38:31.040</a></span> | <span class="t">or you can afford to go over a smaller batch size and just do gradient accumulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2314" target="_blank">00:38:34.480</a></span> | <span class="t">I think it's definitely, I mean, in this case, it's definitely worth the trade off not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2318" target="_blank">00:38:38.720</a></span> | <span class="t">doing the activation checkpointing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2320" target="_blank">00:38:40.640</a></span> | <span class="t">Uh, I found the same thing in my own use case as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2323" target="_blank">00:38:43.200</a></span> | <span class="t">So yeah, something to think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2326" target="_blank">00:38:46.080</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2328" target="_blank">00:38:48.160</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2329" target="_blank">00:38:49.520</a></span> | <span class="t">Keep that in mind then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2330" target="_blank">00:38:50.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2332" target="_blank">00:38:52.640</a></span> | <span class="t">So we talked about this kind of earlier with, oh no, there we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2336" target="_blank">00:38:56.880</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2337" target="_blank">00:38:57.200</a></span> | <span class="t">So I kind of talked about this earlier, uh, with their data synthesis or their data curation pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2343" target="_blank">00:39:03.280</a></span> | <span class="t">But so onto a training strategy, they adopt a multi-stage re-training strategy aimed at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2347" target="_blank">00:39:07.520</a></span> | <span class="t">progressively enhancing data quality, image resolution, and model performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2350" target="_blank">00:39:10.720</a></span> | <span class="t">So again, they enhance their resolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2354" target="_blank">00:39:14.880</a></span> | <span class="t">They go from low to high resolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2357" target="_blank">00:39:17.120</a></span> | <span class="t">So they go from 256 to 256 pixels up to 1328 by 1328.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2365" target="_blank">00:39:25.440</a></span> | <span class="t">So they enhance the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2366" target="_blank">00:39:26.800</a></span> | <span class="t">Oh, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2367" target="_blank">00:39:27.360</a></span> | <span class="t">Where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2367" target="_blank">00:39:27.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2369" target="_blank">00:39:29.040</a></span> | <span class="t">They let the model capture more detailed features leading to better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2372" target="_blank">00:39:32.160</a></span> | <span class="t">So I'm reading like right here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2373" target="_blank">00:39:33.680</a></span> | <span class="t">Richer features, specs, or spaces facilitate improved generalization to unseen data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2378" target="_blank">00:39:38.720</a></span> | <span class="t">So transitioning from low resolution to high-res on flower images allows the model to discern finer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2384" target="_blank">00:39:44.000</a></span> | <span class="t">details such as petal textures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2385" target="_blank">00:39:45.440</a></span> | <span class="t">So that's interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2386" target="_blank">00:39:46.480</a></span> | <span class="t">So they also go from non-text to text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2390" target="_blank">00:39:50.000</a></span> | <span class="t">So they progressively introduced images containing like render text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2392" target="_blank">00:39:52.960</a></span> | <span class="t">So the model can like learn visual representations and subsequently acquire text rendering capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2399" target="_blank">00:39:59.760</a></span> | <span class="t">So they also go from massive to refined data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2401" target="_blank">00:40:01.760</a></span> | <span class="t">They gradually employ increasingly stringent data filtering mechanisms to select higher quality data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2407" target="_blank">00:40:07.040</a></span> | <span class="t">So it ensures that only the most relevant and high quality samples are leveraged to ensure training efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2412" target="_blank">00:40:12.880</a></span> | <span class="t">It's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2413" target="_blank">00:40:13.520</a></span> | <span class="t">They also have balanced data distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2416" target="_blank">00:40:16.080</a></span> | <span class="t">So from unbalanced to balanced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2417" target="_blank">00:40:17.280</a></span> | <span class="t">So it mitigates the risk of the model overfitting to particular domains or resolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2421" target="_blank">00:40:21.440</a></span> | <span class="t">And like just let the model generalize better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2424" target="_blank">00:40:24.080</a></span> | <span class="t">And lastly, they have synthetic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2427" target="_blank">00:40:27.040</a></span> | <span class="t">So from real world synthetic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2428" target="_blank">00:40:28.640</a></span> | <span class="t">So here they generate supplementary samples enriching the data set and ensuring more comprehensive coverage of diverse visual domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2437" target="_blank">00:40:37.280</a></span> | <span class="t">They say it enhances the model's ability to generalize and perform robustly across a wider range of scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2442" target="_blank">00:40:42.240</a></span> | <span class="t">So it seems pretty useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2444" target="_blank">00:40:44.240</a></span> | <span class="t">So on to post-training and RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2446" target="_blank">00:40:46.800</a></span> | <span class="t">So they have supervised fine-tuning, RL, and DPO, or and GRPR2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2456" target="_blank">00:40:56.320</a></span> | <span class="t">So they use hemean annotations to address specific shortcomings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2461" target="_blank">00:41:01.120</a></span> | <span class="t">This is in the supervised fine-tuning phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2464" target="_blank">00:41:04.240</a></span> | <span class="t">So they make the selected images clear, rich in detail, bright, and photorealistic, and like all that good stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2469" target="_blank">00:41:09.440</a></span> | <span class="t">And they guide the model towards producing content with creative realism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2471" target="_blank">00:41:11.680</a></span> | <span class="t">So with RL, they use DPO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2475" target="_blank">00:41:15.040</a></span> | <span class="t">DPO, like it excels at the flow matching, which is the diffusion part of the image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2480" target="_blank">00:41:20.400</a></span> | <span class="t">And GRPO performs on-policy sampling during training and evaluates each trajectory with a reward model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2486" target="_blank">00:41:26.960</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2490" target="_blank">00:41:30.240</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2490" target="_blank">00:41:30.560</a></span> | <span class="t">So for DPO, it says given the same prompt, multiple images are generated with different random initialization seeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2496" target="_blank">00:41:36.240</a></span> | <span class="t">So like with prompts without reference images, annotators are asked to select the best and worst</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2502" target="_blank">00:41:42.960</a></span> | <span class="t">samples along to gender-oriented images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2505" target="_blank">00:41:45.040</a></span> | <span class="t">So I don't know if selecting the best and worst samples, I don't know if this is new or if people have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2510" target="_blank">00:41:50.080</a></span> | <span class="t">previously used this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2511" target="_blank">00:41:51.280</a></span> | <span class="t">But I thought that was interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2512" target="_blank">00:41:52.400</a></span> | <span class="t">Because like usually with DPO, I just hear people selecting the best images or like the best,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2516" target="_blank">00:41:56.560</a></span> | <span class="t">like whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2517" target="_blank">00:41:57.280</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2519" target="_blank">00:41:59.200</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2520" target="_blank">00:42:00.080</a></span> | <span class="t">So DPO, they use that for flow matching and for GRPO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2523" target="_blank">00:42:03.760</a></span> | <span class="t">They use it for, what's it called?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2527" target="_blank">00:42:07.520</a></span> | <span class="t">They use it for like the reverse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2528" target="_blank">00:42:08.880</a></span> | <span class="t">I think they also use it for flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2531" target="_blank">00:42:11.120</a></span> | <span class="t">I'm not sure of this part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2532" target="_blank">00:42:12.000</a></span> | <span class="t">But they use it for like, I'm pretty sure they use it for reconstructing the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2535" target="_blank">00:42:15.440</a></span> | <span class="t">Like finding the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2537" target="_blank">00:42:17.040</a></span> | <span class="t">So if anyone wants to like comment, they can jump in here too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2542" target="_blank">00:42:22.480</a></span> | <span class="t">But if no one has anything to say, then I'll go on to the next section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2547" target="_blank">00:42:27.840</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2551" target="_blank">00:42:31.440</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2551" target="_blank">00:42:31.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2556" target="_blank">00:42:36.640</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2557" target="_blank">00:42:37.280</a></span> | <span class="t">So in addition to text image, so they let the model explore like multimodal image generation tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2562" target="_blank">00:42:42.960</a></span> | <span class="t">So it's not only like giving the model a prompt, which is text only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2566" target="_blank">00:42:46.480</a></span> | <span class="t">They also let the user like give it a prompt in an image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2569" target="_blank">00:42:49.040</a></span> | <span class="t">So that's cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2570" target="_blank">00:42:50.640</a></span> | <span class="t">So they also find that providing the visual semantic embeddings from the MLM enables better instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2577" target="_blank">00:42:57.840</a></span> | <span class="t">following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2578" target="_blank">00:42:58.320</a></span> | <span class="t">So this is kind of like what they talked about like at the very beginning where, or not at the very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2582" target="_blank">00:43:02.080</a></span> | <span class="t">beginning, but like what they talked about previously where they, they took the last latent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2586" target="_blank">00:43:06.400</a></span> | <span class="t">representation of Quen, like of their, like their multimodal language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2591" target="_blank">00:43:11.600</a></span> | <span class="t">They took the last image and they kind of like fed that into the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2594" target="_blank">00:43:14.480</a></span> | <span class="t">And they say here that it helped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2596" target="_blank">00:43:16.240</a></span> | <span class="t">So it's pretty good for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2597" target="_blank">00:43:17.680</a></span> | <span class="t">And they also use like the, like they talked about how pixel level VAE embeddings further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2603" target="_blank">00:43:23.760</a></span> | <span class="t">enhances the model's ability to preserve visual fidelity and maintain structural consistency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2608" target="_blank">00:43:28.240</a></span> | <span class="t">So I was kind of talking about that earlier too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2609" target="_blank">00:43:29.920</a></span> | <span class="t">So yeah, so I think like the, I don't know, this really reminds you of like system one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2614" target="_blank">00:43:34.080</a></span> | <span class="t">system two and how like different labs have been using that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2616" target="_blank">00:43:36.960</a></span> | <span class="t">Like I remember DeepMind used system one and system two for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2619" target="_blank">00:43:39.680</a></span> | <span class="t">And I think like figure AI use that for like, they also use that for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2623" target="_blank">00:43:43.840</a></span> | <span class="t">So I don't really have a lot of notes on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2627" target="_blank">00:43:47.600</a></span> | <span class="t">So human evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2628" target="_blank">00:43:48.800</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2631" target="_blank">00:43:51.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2632" target="_blank">00:43:52.960</a></span> | <span class="t">These are just evaluation benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2634" target="_blank">00:43:54.720</a></span> | <span class="t">I kind of like put more emphasis on how the model was trained in like that data creation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2638" target="_blank">00:43:58.240</a></span> | <span class="t">than like the human eval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2641" target="_blank">00:44:01.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2642" target="_blank">00:44:02.000</a></span> | <span class="t">So they have like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2643" target="_blank">00:44:03.200</a></span> | <span class="t">So there are different benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2644" target="_blank">00:44:04.960</a></span> | <span class="t">So you can read these if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2647" target="_blank">00:44:07.360</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2652" target="_blank">00:44:12.400</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2652" target="_blank">00:44:12.960</a></span> | <span class="t">Formants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2653" target="_blank">00:44:13.520</a></span> | <span class="t">Formants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2655" target="_blank">00:44:15.200</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2655" target="_blank">00:44:15.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2656" target="_blank">00:44:16.880</a></span> | <span class="t">I really just like left the benchmarks alone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2658" target="_blank">00:44:18.800</a></span> | <span class="t">And like if people want to read them, I just let, like you can read them if you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2663" target="_blank">00:44:23.520</a></span> | <span class="t">So they also, let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2669" target="_blank">00:44:29.600</a></span> | <span class="t">They also have like image editing benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2671" target="_blank">00:44:31.280</a></span> | <span class="t">And they also have like, so in some of their editing benchmarks, or in one of them rather.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2680" target="_blank">00:44:40.640</a></span> | <span class="t">So they have like very dense PDFs and they like essentially just let the model see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2686" target="_blank">00:44:46.240</a></span> | <span class="t">Or like they let the model like try to reconstruct some of the images to like see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2690" target="_blank">00:44:50.320</a></span> | <span class="t">if it could generate some of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2692" target="_blank">00:44:52.880</a></span> | <span class="t">And apparently it does pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2697" target="_blank">00:44:57.920</a></span> | <span class="t">So like, again, they focus on like English, Chinese, and like multi-object generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2701" target="_blank">00:45:01.760</a></span> | <span class="t">and spatial relationship generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2704" target="_blank">00:45:04.960</a></span> | <span class="t">So that's kind of, that's kind of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2709" target="_blank">00:45:09.840</a></span> | <span class="t">That's, that's most of the thing that I, or that's most of what I focused on like in my annotation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2714" target="_blank">00:45:14.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2716" target="_blank">00:45:16.160</a></span> | <span class="t">So does anyone have any, any questions or anything else or any comments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2719" target="_blank">00:45:19.280</a></span> | <span class="t">I think the conclusion is also quite eye opening in the sense that they try to make, they make this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2732" target="_blank">00:45:32.720</a></span> | <span class="t">claim that a generative model can effectively perform classical understanding tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2738" target="_blank">00:45:38.400</a></span> | <span class="t">And they say that, uh, uh, the current image model is deliberately does not optimize for photo realism and, or aesthetic quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2748" target="_blank">00:45:48.000</a></span> | <span class="t">but really tries to optimize more for aligning text and image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2752" target="_blank">00:45:52.000</a></span> | <span class="t">I think you sort of tells you where they are trying to bring this model towards, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2758" target="_blank">00:45:58.000</a></span> | <span class="t">Like essentially like creating posters, creating PowerPoints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2760" target="_blank">00:46:00.560</a></span> | <span class="t">I essentially, it's more practical instead of just generating images, but images with text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2765" target="_blank">00:46:05.120</a></span> | <span class="t">Uh, so I thought the conclusion was quite a worth trying to read and understanding what they mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2773" target="_blank">00:46:13.280</a></span> | <span class="t">what they mean by it as well, uh, especially the last paragraph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2777" target="_blank">00:46:17.280</a></span> | <span class="t">So that's pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2778" target="_blank">00:46:18.240</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2779" target="_blank">00:46:19.200</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2782" target="_blank">00:46:22.160</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2782" target="_blank">00:46:22.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2787" target="_blank">00:46:27.680</a></span> | <span class="t">I'll just like read it, but well, like other people can ask questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2790" target="_blank">00:46:30.720</a></span> | <span class="t">Anyone have any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2805" target="_blank">00:46:45.920</a></span> | <span class="t">So yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2808" target="_blank">00:46:48.240</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2808" target="_blank">00:46:48.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2809" target="_blank">00:46:49.440</a></span> | <span class="t">So they mentioned that, uh, the, uh, model streaming data, uh, pipeline, uh, does include other languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2816" target="_blank">00:46:56.960</a></span> | <span class="t">but, uh, I see that, uh, that most of the benchmarks, uh, were focusing on, uh, English and Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2828" target="_blank">00:47:08.160</a></span> | <span class="t">So I don't know if, uh, uh, uh, in the conclusion that they,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2834" target="_blank">00:47:14.720</a></span> | <span class="t">they, uh, specified that they can, uh, handle other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2840" target="_blank">00:47:20.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2846" target="_blank">00:47:26.000</a></span> | <span class="t">So actually, I'm not sure they don't really, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2848" target="_blank">00:47:28.400</a></span> | <span class="t">They don't really talk about like other languages besides English and Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2852" target="_blank">00:47:32.000</a></span> | <span class="t">So I'm not really sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2854" target="_blank">00:47:34.640</a></span> | <span class="t">I kind of like, uh, I tested quite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2856" target="_blank">00:47:36.400</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2856" target="_blank">00:47:36.960</a></span> | <span class="t">They had a category for other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2860" target="_blank">00:47:40.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2860" target="_blank">00:47:40.560</a></span> | <span class="t">They had, uh, English, Chinese, and other, but I don't think this thing will do, do other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2872" target="_blank">00:47:52.240</a></span> | <span class="t">Maybe others to exclude them or to further them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2875" target="_blank">00:47:55.920</a></span> | <span class="t">No, no, they weren't filtered out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2877" target="_blank">00:47:57.360</a></span> | <span class="t">They were filtered in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2878" target="_blank">00:47:58.240</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2878" target="_blank">00:47:58.560</a></span> | <span class="t">But, um, yeah, it's, it's primarily English and Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2883" target="_blank">00:48:03.520</a></span> | <span class="t">So I don't know if they did evals on the other texts, maybe they're just not equipped to do evals on other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2896" target="_blank">00:48:16.400</a></span> | <span class="t">Um, they had, they had four categories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2900" target="_blank">00:48:20.240</a></span> | <span class="t">They had English, Chinese, other language, and no text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2903" target="_blank">00:48:23.360</a></span> | <span class="t">We definitely had a other language, but I don't think it's significant, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2908" target="_blank">00:48:28.560</a></span> | <span class="t">And then some of this is also like, um, you know, there's already a vision encoder in there that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2914" target="_blank">00:48:34.080</a></span> | <span class="t">do multilingual, but I don't think it can specialize in output for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2919" target="_blank">00:48:39.360</a></span> | <span class="t">Uh, uh, I mean, actually, I tested, like, I tested some other prompts with the, I didn't text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2931" target="_blank">00:48:51.840</a></span> | <span class="t">or test any, like, non-English or Chinese languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2934" target="_blank">00:48:54.880</a></span> | <span class="t">Yeah, I mean, like, Quen, I tested them in Sora, like, Quen and Sora.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2938" target="_blank">00:48:58.800</a></span> | <span class="t">And like, actually, I think the quality was, like, pretty similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2941" target="_blank">00:49:01.280</a></span> | <span class="t">Or Quen's, like, Quen's text generation was, like, uh, it was a little more clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2946" target="_blank">00:49:06.080</a></span> | <span class="t">Like, the characters were a lot clearer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2947" target="_blank">00:49:07.760</a></span> | <span class="t">You could kind of see them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2948" target="_blank">00:49:08.720</a></span> | <span class="t">It was a lot easier to look at them and just, like, see the background,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2951" target="_blank">00:49:11.360</a></span> | <span class="t">or, like, distinguish between the characters and the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2953" target="_blank">00:49:13.920</a></span> | <span class="t">But, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2956" target="_blank">00:49:16.400</a></span> | <span class="t">Live coding, I'm testing it in Spanish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2964" target="_blank">00:49:24.560</a></span> | <span class="t">Yeah, I should've.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2965" target="_blank">00:49:25.520</a></span> | <span class="t">Should we do a different language?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2972" target="_blank">00:49:32.640</a></span> | <span class="t">Like, should we do Korean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2974" target="_blank">00:49:34.480</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2976" target="_blank">00:49:36.480</a></span> | <span class="t">Yeah, maybe something that's not Latin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2980" target="_blank">00:49:40.080</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2981" target="_blank">00:49:41.600</a></span> | <span class="t">Arabic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2983" target="_blank">00:49:43.040</a></span> | <span class="t">I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2984" target="_blank">00:49:44.000</a></span> | <span class="t">I don't know if we would even be able to evaluate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2990" target="_blank">00:49:50.640</a></span> | <span class="t">I can try that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2991" target="_blank">00:49:51.520</a></span> | <span class="t">May I have?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=2992" target="_blank">00:49:52.000</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3003" target="_blank">00:50:03.840</a></span> | <span class="t">I just shut off a generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3006" target="_blank">00:50:06.560</a></span> | <span class="t">We'll see what it outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3007" target="_blank">00:50:07.760</a></span> | <span class="t">Oh, shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3008" target="_blank">00:50:08.240</a></span> | <span class="t">It's not correct, but it has Korean-looking stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3015" target="_blank">00:50:15.680</a></span> | <span class="t">Wait, what's your prompt?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3016" target="_blank">00:50:16.640</a></span> | <span class="t">Uh, literally in Korean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3022" target="_blank">00:50:22.080</a></span> | <span class="t">I went to Google Translate, and I translated the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3026" target="_blank">00:50:26.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3029" target="_blank">00:50:29.280</a></span> | <span class="t">You might want to-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3030" target="_blank">00:50:30.160</a></span> | <span class="t">You want to share the images I shared in Zoom chat?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3032" target="_blank">00:50:32.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3035" target="_blank">00:50:35.120</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3035" target="_blank">00:50:35.200</a></span> | <span class="t">Maybe just change it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3037" target="_blank">00:50:37.200</a></span> | <span class="t">Well, I mean, it's off, but it does look a little more Korean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3040" target="_blank">00:50:40.160</a></span> | <span class="t">Wow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3040" target="_blank">00:50:40.480</a></span> | <span class="t">Why did you say it's not correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3041" target="_blank">00:50:41.840</a></span> | <span class="t">Because look at the-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3044" target="_blank">00:50:44.720</a></span> | <span class="t">Look at what it's supposed to write.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3046" target="_blank">00:50:46.880</a></span> | <span class="t">The first character, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3047" target="_blank">00:50:47.920</a></span> | <span class="t">Uh, actually-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3050" target="_blank">00:50:50.160</a></span> | <span class="t">Maybe it fixed it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3051" target="_blank">00:50:51.760</a></span> | <span class="t">I actually don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3052" target="_blank">00:50:52.640</a></span> | <span class="t">Wow, this is really-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3054" target="_blank">00:50:54.640</a></span> | <span class="t">Oh, I guess the second line is kind of there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3056" target="_blank">00:50:56.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3057" target="_blank">00:50:57.440</a></span> | <span class="t">Maybe the first line actually-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3058" target="_blank">00:50:58.560</a></span> | <span class="t">I actually don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3059" target="_blank">00:50:59.120</a></span> | <span class="t">Maybe it's a ha-ha.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3059" target="_blank">00:50:59.840</a></span> | <span class="t">Kind of there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3060" target="_blank">00:51:00.160</a></span> | <span class="t">Kind of there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3060" target="_blank">00:51:00.720</a></span> | <span class="t">And they actually-</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3062" target="_blank">00:51:02.000</a></span> | <span class="t">They actually fixed it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3062" target="_blank">00:51:02.880</a></span> | <span class="t">Oh, it's actually coffee.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3066" target="_blank">00:51:06.640</a></span> | <span class="t">Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3069" target="_blank">00:51:09.600</a></span> | <span class="t">But okay, it does something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3071" target="_blank">00:51:11.360</a></span> | <span class="t">Wow, this is actually pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3076" target="_blank">00:51:16.880</a></span> | <span class="t">Yeah, I don't know if that's right or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3087" target="_blank">00:51:27.520</a></span> | <span class="t">I don't see Korean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3088" target="_blank">00:51:28.880</a></span> | <span class="t">It looks similar, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3090" target="_blank">00:51:30.080</a></span> | <span class="t">Oh my god, it does Korean work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3092" target="_blank">00:51:32.000</a></span> | <span class="t">Okay, I'll do another one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3099" target="_blank">00:51:39.040</a></span> | <span class="t">I'm now trying to translate the text it generated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3105" target="_blank">00:51:45.680</a></span> | <span class="t">I showed another image if you want to swap to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3108" target="_blank">00:51:48.480</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3111" target="_blank">00:51:51.120</a></span> | <span class="t">And then I'm translating this with Korean model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3115" target="_blank">00:51:55.440</a></span> | <span class="t">And see if it's coherent or if it's just random stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3121" target="_blank">00:52:01.360</a></span> | <span class="t">Yep, so it put Korean on the board.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3127" target="_blank">00:52:07.840</a></span> | <span class="t">I'm trying to have it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3129" target="_blank">00:52:09.360</a></span> | <span class="t">Oh my god.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3131" target="_blank">00:52:11.840</a></span> | <span class="t">Translate this Korean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3133" target="_blank">00:52:13.040</a></span> | <span class="t">It's struggling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3133" target="_blank">00:52:13.760</a></span> | <span class="t">Okay, I think this is enough live coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3148" target="_blank">00:52:28.480</a></span> | <span class="t">Oh yeah, as long as the fuzzy background characters don't look Korean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3151" target="_blank">00:52:31.680</a></span> | <span class="t">I mean, it could also just be style, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3156" target="_blank">00:52:36.320</a></span> | <span class="t">Like it is also a monkey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3157" target="_blank">00:52:37.680</a></span> | <span class="t">That's true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3160" target="_blank">00:52:40.640</a></span> | <span class="t">I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3161" target="_blank">00:52:41.120</a></span> | <span class="t">So the second image is nonsense, gibberish, according to translations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3168" target="_blank">00:52:48.560</a></span> | <span class="t">I guess I did ask it for random text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3176" target="_blank">00:52:56.720</a></span> | <span class="t">Yeah, but the font of the main text and the background text do look different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3181" target="_blank">00:53:01.440</a></span> | <span class="t">But oh well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3182" target="_blank">00:53:02.000</a></span> | <span class="t">Yeah, it's cooked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3187" target="_blank">00:53:07.120</a></span> | <span class="t">Oh well, they tried.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3195" target="_blank">00:53:15.040</a></span> | <span class="t">I'm doing another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3196" target="_blank">00:53:16.240</a></span> | <span class="t">Alright, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3196" target="_blank">00:53:16.880</a></span> | <span class="t">I think next week we have, was it Venki again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3201" target="_blank">00:53:21.280</a></span> | <span class="t">Yeah, Venki will talk us through GLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3203" target="_blank">00:53:23.760</a></span> | <span class="t">Wow, this chalk text actually looks so much like chalk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3210" target="_blank">00:53:30.480</a></span> | <span class="t">I'm actually impressed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3212" target="_blank">00:53:32.720</a></span> | <span class="t">But I don't deal with a lot of image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3217" target="_blank">00:53:37.200</a></span> | <span class="t">I thought the chalk was mid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3218" target="_blank">00:53:38.640</a></span> | <span class="t">No, the one that Steve just posted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3224" target="_blank">00:53:44.160</a></span> | <span class="t">At least the first character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3225" target="_blank">00:53:45.120</a></span> | <span class="t">Yeah, it's pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3227" target="_blank">00:53:47.360</a></span> | <span class="t">It's pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3227" target="_blank">00:53:47.920</a></span> | <span class="t">I mean, it's trained specifically on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3229" target="_blank">00:53:49.680</a></span> | <span class="t">I'm impressed for the first character at least.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3234" target="_blank">00:53:54.480</a></span> | <span class="t">Yeah, so sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3238" target="_blank">00:53:58.560</a></span> | <span class="t">Maybe a late question here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3240" target="_blank">00:54:00.080</a></span> | <span class="t">I was going over that the flow and diffusion model that they had in 4.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3246" target="_blank">00:54:06.640</a></span> | <span class="t">And I just want to make sure that I understand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3249" target="_blank">00:54:09.920</a></span> | <span class="t">So I think that they have a joint latent space of image and text, correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3255" target="_blank">00:54:15.760</a></span> | <span class="t">And so they're applying a noise model on top of that space, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3263" target="_blank">00:54:23.760</a></span> | <span class="t">And I was wondering, typically when you add noise to say latent space of text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3270" target="_blank">00:54:30.000</a></span> | <span class="t">it doesn't work very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3271" target="_blank">00:54:31.040</a></span> | <span class="t">That's why we don't have very good diffusion models for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3274" target="_blank">00:54:34.160</a></span> | <span class="t">So I'm kind of surprised that this kind of works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3278" target="_blank">00:54:38.000</a></span> | <span class="t">So I'm not sure what you guys feel about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3280" target="_blank">00:54:40.560</a></span> | <span class="t">So what I'm saying is that their latent space is kind of like a multimodal one, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3286" target="_blank">00:54:46.480</a></span> | <span class="t">It contains both image and text related stuff together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3291" target="_blank">00:54:51.200</a></span> | <span class="t">And typically diffusion, as you have here, which is adding noise to that, normally doesn't work well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3298" target="_blank">00:54:58.080</a></span> | <span class="t">on text-based latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3301" target="_blank">00:55:01.360</a></span> | <span class="t">So I was wondering why this would do any better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3308" target="_blank">00:55:08.160</a></span> | <span class="t">So one thing that I noticed was that if you look at the architectural diagram, the noise is only added on the image side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3315" target="_blank">00:55:15.680</a></span> | <span class="t">Oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3317" target="_blank">00:55:17.440</a></span> | <span class="t">So if you scroll up to the architectural diagram, I might be wrong about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3324" target="_blank">00:55:24.400</a></span> | <span class="t">It's just from memory, but I think that's what I saw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3328" target="_blank">00:55:28.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3329" target="_blank">00:55:29.040</a></span> | <span class="t">So the noise is only added on the image side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3332" target="_blank">00:55:32.240</a></span> | <span class="t">And then you have this cross entropy thing that combines them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3338" target="_blank">00:55:38.400</a></span> | <span class="t">So it looks like it's more like conditioning on the text, building the common latent space inside of the transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3351" target="_blank">00:55:51.360</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3352" target="_blank">00:55:52.880</a></span> | <span class="t">So it's like normal diffusion then, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3354" target="_blank">00:55:54.640</a></span> | <span class="t">Like normal image diffusion?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3356" target="_blank">00:55:56.080</a></span> | <span class="t">Yeah, it seems like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3357" target="_blank">00:55:57.440</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3358" target="_blank">00:55:58.480</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3359" target="_blank">00:55:59.280</a></span> | <span class="t">I also think it's because the resulting product of the model will be an image, which is continuous in diffusion space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3367" target="_blank">00:56:07.520</a></span> | <span class="t">If the resulting product of the model would be text or something, or something that's inherently discrete, then I feel it could be a lot different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3375" target="_blank">00:56:15.200</a></span> | <span class="t">Like you might have to use a decoder or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3378" target="_blank">00:56:18.400</a></span> | <span class="t">But like, yeah, like with the prompt and the image, both of those get like injected into latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3385" target="_blank">00:56:25.760</a></span> | <span class="t">And then you can like perform diffusion on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3387" target="_blank">00:56:27.840</a></span> | <span class="t">And like the resulting process will be an image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3390" target="_blank">00:56:30.080</a></span> | <span class="t">Well, I think RJ said that it doesn't get injected right on the combined embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3397" target="_blank">00:56:37.120</a></span> | <span class="t">It's just on the image embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3398" target="_blank">00:56:38.800</a></span> | <span class="t">It gets added like on this figure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3402" target="_blank">00:56:42.400</a></span> | <span class="t">The noise only added to the image and image latent space, not to the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3407" target="_blank">00:56:47.760</a></span> | <span class="t">They get combined later, but the noise is only added to the image side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3412" target="_blank">00:56:52.800</a></span> | <span class="t">Oh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3413" target="_blank">00:56:53.680</a></span> | <span class="t">Oh, yeah, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3414" target="_blank">00:56:54.400</a></span> | <span class="t">You're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3414" target="_blank">00:56:54.720</a></span> | <span class="t">You're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3415" target="_blank">00:56:55.360</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3416" target="_blank">00:56:56.640</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3417" target="_blank">00:56:57.520</a></span> | <span class="t">Sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3418" target="_blank">00:56:58.080</a></span> | <span class="t">Thanks, RJ.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3418" target="_blank">00:56:58.640</a></span> | <span class="t">Yeah, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3423" target="_blank">00:57:03.760</a></span> | <span class="t">So, oh, it's 402 right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3426" target="_blank">00:57:06.320</a></span> | <span class="t">So is that it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3428" target="_blank">00:57:08.480</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3429" target="_blank">00:57:09.200</a></span> | <span class="t">Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3429" target="_blank">00:57:09.760</a></span> | <span class="t">Thanks for guiding us through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3432" target="_blank">00:57:12.400</a></span> | <span class="t">Next week, we have another volunteer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3435" target="_blank">00:57:15.680</a></span> | <span class="t">We'll share the Luma and stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3437" target="_blank">00:57:17.280</a></span> | <span class="t">But cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3437" target="_blank">00:57:17.920</a></span> | <span class="t">Thanks so much for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3439" target="_blank">00:57:19.120</a></span> | <span class="t">Yeah, I tested in Arabic as you requested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3442" target="_blank">00:57:22.800</a></span> | <span class="t">So, yeah, of course, it's not right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3446" target="_blank">00:57:26.880</a></span> | <span class="t">But there is like the same number of letters and going from right to left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3454" target="_blank">00:57:34.880</a></span> | <span class="t">But it's missing a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3457" target="_blank">00:57:37.280</a></span> | <span class="t">So, completely rubbish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3459" target="_blank">00:57:39.280</a></span> | <span class="t">Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3461" target="_blank">00:57:41.920</a></span> | <span class="t">Well, I guess it's English and Chinese then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3463" target="_blank">00:57:43.760</a></span> | <span class="t">Cool, guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3464" target="_blank">00:57:44.400</a></span> | <span class="t">Take care.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3464" target="_blank">00:57:44.800</a></span> | <span class="t">See you guys next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3465" target="_blank">00:57:45.520</a></span> | <span class="t">Okay, bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3466" target="_blank">00:57:46.720</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3467" target="_blank">00:57:47.600</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3468" target="_blank">00:57:48.160</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3468" target="_blank">00:57:48.240</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3468" target="_blank">00:57:48.320</a></span> | <span class="t">Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=J79Jqs_7820&t=3468" target="_blank">00:57:48.820</a></span> | <span class="t">Any questions, Mr. Calder?</span></div></div></body></html>