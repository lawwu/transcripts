<html><head><title>Strategies for LLM Evals (GuideLLM, lm-eval-harness, OpenAI Evals Workshop) — Taylor Jordan Smith</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Strategies for LLM Evals (GuideLLM, lm-eval-harness, OpenAI Evals Workshop) — Taylor Jordan Smith</h2><a href="https://www.youtube.com/watch?v=89NuzmKokIk" target="_blank"><img src="https://i.ytimg.com/vi_webp/89NuzmKokIk/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>TAYLOR SMITH: Hello, everybody. My name is Taylor Smith. I am an AI developer advocate/evangelist/technical marketing and other titles, depending on where I am. I work at Red Hat in our AI business unit, because, yes, we do also do AI, too, now. Great. I'm really happy to be here. This is my first time at this conference.</p><p>So I'm new to the conference, new to speaking at this conference, ready to have a good time and also relax in about an hour and a half and just learn stuff. Stoked that this was at 9:00 AM. My presentation clicker isn't working, so I'm going to be a little bit glued over here.</p><p>But to kind of overview what we're going to do today, I'm going to talk a little bit about kind of the issues of setting up a large language model in production and the reason why you need evaluations and benchmarks and all of these things and why that is so critical.</p><p>And then I do have some hands-on activities that we'll get to do to use some evaluation methods and benchmark tools to kind of get a sense of what is out there to use, how we can use those tools, what that might look like to put it all together for an actual production system.</p><p>So ready? Are you excited? Yay. First thing in the morning. Love that. I'm glad it's 9:00 AM because that feels like a reasonable start time. I don't like that 8:00 AM stuff that happens. Setting up generative AI tech in production, if you think about it, crazy pants, OK? There's so many things that could go wrong.</p><p>This is such a complex type of technology that is very creative. Setting this up to be scalable and reliable and safe is challenging, OK? That's why we need evaluations. That's why we need all of these tests. And we need to be careful. We need to understand the technology we're working with when we're looking to implement this as well.</p><p>So hard to do. Most organizations-- so they don't typically start off with a multi-agent framework and go crazy. Typically, they're going to have a kind of standard repeatable path to maturity. They might start out with, OK, how can I automate some things with AI? How can I have like a chatbot type of situation?</p><p>That's kind of the standard that everybody starts out with. OK, I maybe want to implement a RAG setup. All right, let's start to look at agents. On the Red Hat side, we're still dealing with probably the first three phases of this maturity with our customers. And that's where a lot of people still are.</p><p>But we get to talk about all the cool advanced stuff at this conference so we can plan ahead and think about what we want to implement. But enterprises, they need to take an incremental approach to do this successfully. Gen AI models, they have a number of drawbacks, right? And we all know about these things.</p><p>Policy restrictions-- typically, if you're a developer at a company or have whatever type of role, you're restricted in the type of AI that you can use. Red Hat was just allowed to use-- Gemini was just made available to us. But before that, we weren't really allowed to use anything officially.</p><p>So the company policy restrictions of the tools you can use and the AI you can use is pretty locked down, typically. The legal exposures and risks of these models-- there was the glue incident. They're going to say crazy stuff sometimes. How do we guard rail against that? And how do we protect our customers against that and account for that when that does happen?</p><p>Because it's hard to completely avoid. There's the bias and discrimination issues. Most of the internet data is still largely Eurocentric and US-based. So the models trained on this public internet data, of course, is going to be a little skewed, right? So we need to be aware of it. Just like in regular everyday life, we know that bias and discrimination exists.</p><p>How do we account and provide guardrails to make sure that we adjust and prevent what we can? Cost and performance-- that's probably a baseline big one. How much these Gen AI models cost to run at scale and production, and the performance, throughput, latency, et cetera, that we need to account for when we have these production systems set up?</p><p>And then the knowledge cutoff as well is another limitation. These large frontier models have a knowledge cutoff because they're not consistently trained. So you might be working with a model that was cut off a year ago, so it's not going to have that up-to-date information, which is why they implement RAG systems and agent systems to look out into the internet for more up-to-date info.</p><p>These are just some of the kind of drawbacks we need to be aware of and account for. Inference at scale. No matter how-- I'm going to go into these categories a little more in-depth. No matter how good your model is, if it's not fast, if it's not reliable, if it's not affordable, you're screwed a little bit from the get-go.</p><p>So this graphic, it shows a classic bottleneck type of the scenario you've got concurrent user requests, which might be represented by those green, yellow, and orange dots flowing into a system. But then traditional inference runtimes typically can't handle that kind of load officially. To serve real-world traffic, whether you're powering a customer support agent, a developer type of copilot system, or maybe a RAG pipeline, you need an inference engine that's purpose-built for scale.</p><p>So that's where inference runtimes like TRT, SGLang, which I know we have a session on, or at least a couple sessions on. VLLM, that's where Red Hat's kind of focusing. That's where those type of production-grade inference runtimes really need to be utilized. There's a lot of pain points with inference that I just want to double down on.</p><p>And some of the activity will be benchmarking and evaluating these types of metrics. Model inference performance evaluation under enterprise-level workload scenarios, it's very complicated to actually evaluate this appropriately. It requires manual setup of evaluation runs with various parameters you have to test. The compute load, just for performance evaluations as well, is also pretty taxing.</p><p>You have to make sure the data sets that you're using for benchmarking are compatible with the models that you're using. The resource optimization and identifying sizing so that you're efficiently using your hardware appropriately for whatever model size you're using, that's a big challenge for enterprises today, making sure they're efficiently using their GPU investments.</p><p>And then actually, cost estimating is a little bit of a black magic thing. It's really hard to do that appropriately. You have to backwards math map inference performance to tokens, tokens, and the whole thing. So these are what enterprises are trying to achieve. And it's hard. Just a little kind of more examples of the challenges.</p><p>So we have-- this is just an example of stable diffusion bias. Like I said, most of our data is Eurocentric. This is going to be there. So how do we-- what tools can we use to provide guardrails against this type of behavior? The glue incident-- this was because there was something on Reddit was like a joke satire.</p><p>And this AI overview tool used that information and didn't have the right mitigation techniques in place to identify that satire. So then it came out in that AI overview suggestion. And then we have this like mad situation where a lot of the-- we're getting into a lot of synthetic data on the internet.</p><p>And each generation of these AI models that come out are consuming more and more AI-generated data, which over time is going to get you further-- oh, there's music happening-- further away from that original human-anchored data. So this is going to lead to a loss of output diversity, a loss of precision.</p><p>This would be an area where you would need to use those kind of accuracy evals to mitigate again and identify that this is occurring. So just to cover, of course, Google and the Stable Diffusion Project. They have introduced additional evaluation frameworks and mitigation techniques to address this. Just like any time we have any story like that, right?</p><p>They are certainly working to make sure that that does not happen again. We don't know with the closed source AI offerings how exactly they're doing that, but we can speculate. You know, the AI overview technology maybe introduced more RAG mitigations to where it helps to identify that that was satire or whatever the case is.</p><p>It has more safeguarding triggers. The Stable Diffusion Model likely introduced some level of bias mitigation guardrails. So we need--so, of course, they've been working to fix this, but ideally, we don't run into this, and we prevent it ahead of time before a model release or before an application release.</p><p>So how do we prevent these kinds of issues at scale in production environments? I want to just look at a couple of definitions, because sometimes evaluation and benchmarking are terms used, kind of--they kind of conflate a little bit, and people kind of use them for whatever they want. So benchmarking is just a subcategory of evaluation.</p><p>Evaluation is a comprehensive process to assess a model end-to-end, and it could include a lot of different kinds of evaluations about a lot of different components. Benchmarking is very specifically controlled specific data sets and specific tasks, typically used to compare models against one another. So this would be like a latency score that compares different hardware setups and different models, or like the MMLU benchmark scoring, things like that.</p><p>We'll look at both custom evals that aren't benchmarking and also some benchmarks in their hands-on. These are just some examples of what is typically considered a model evaluation versus a benchmarking specific test, just to kind of get a little bit of a sense. But again, like, there's so many types of evaluations, so many tools.</p><p>You can customize it in so many different ways, but this kind of helps, hopefully, a little bit with the definitions. So, hopefully, kind of seeing all these challenges with Gen AI, we understand that this is a critical process. You need to manage risk for customers. Like, there's less of a concern, right, when I'm tinkering on my laptop, whatever, you know, I see something weird, who cares.</p><p>But when we're talking about a production-level environment, we're serving thousands, whatever, customers, we need to think about these things, obviously, more in these types of scenarios. The credibility of the company, when those stories come out, that takes a hit for a good chunk of time. And you obviously also need to continuously improve your evaluation frameworks as well, because you're not going to catch everything.</p><p>You need that CI process to make sure you are continuously improving your evaluations and benchmark setups. It's also going to very much depend on the type of system that you have, what you set up. So again, it's very much, there's tons of tools, this could look a lot of different ways, we'll get a sense of it today.</p><p>If you have a RAG set up, you're going to be maybe focused on the RAGAS benchmark, or evaluation tool, agents, you need to look at function, tool calling, capabilities, et cetera. You can kind of get a sense, there's going to be specific metrics that you need to set up and look for, depending on the system that you have, which requires a lot of planning in advance and kind of architecture scoping.</p><p>I'll give an example of a RAG use case, and also it's incremental too, because you could, and I'll talk about this a bit, like you could literally evaluate every single part of things, but that's going to be time and resource extensive to set up immediately. So you likely want to take an incremental approach with these types of setups.</p><p>So you might start out with, okay, I'm just going to evaluate the chunk retrieval, my retrieval application in a RAG system. I want to set up some kind of evaluation test there. I might just want to set up a latency throughput of a benchmark test for my LLM output.</p><p>You can start with those kind of incremental approaches for specific components, and then from there, based on priority levels as well, branch out into a full system eval that covers all the components, the integration layer of components, the integration layer of how the components work together, the UI end-to-end experience, and kind of have a software engineering kind of test period approach to this, where you have that unit test layer kind of approach at the bottom, integration layer in the middle, and that UI end-to-end at the top.</p><p>And you can take that layer by layer as you're building this evaluation framework for your systems. So there is this pyramid also for model evaluation that represents the same kind of setup for that software engineering pyramid represents as well. So the base layer and very base layer is the system performance, because like I said, no matter how good your model is, if you don't have fast throughput, you aren't able to handle concurrent users, you're going to be in a bit of a pickle.</p><p>So GPU utilization, etc. You need to make sure kind of the basics are handled as the main kind of event. And we'll talk about, that'll be the first hands-on activity is evaluating the system performance. Formatting might be making sure it's religiously giving you JSON output that you need for your application, something like that.</p><p>The factual accuracy, which we'll also talk about in one of our hands-on, that would be like the MMLU benchmark. So evaluating that it's performing well on various subjects, kind of standard large language model accuracy, as well as if you've fine-tuned a model, potentially making sure that it's accurate based on the information that you fine-tuned that model on.</p><p>And you kind of go up from there into, you know, safety, bias. There might be specific custom evaluations that are very specific to your application. So it gives you kind of a sense of the tiered approach that can be taken here. So we're going to talk first, system performance, and we're going to have our first hands-on around this.</p><p>We're going to be looking at Guide LLM, which is kind of a new project that is associated with the VLLM inference runtime project. We're going to use that for system performance benchmarks, like latency, throughput, and you'll get a little bit of hands-on there. The general user flow there is like, you know, you select your model, you select your particular data set that you want to use to test throughput and to test inter-token latency, time-to-first token, those types of metrics.</p><p>And then Guide LLM allows, gives you a nice kind of in-terminal UI to visualize the results of that. And then once you get kind of the results that you want based on your use case again, then you're ready to deploy. You'll see this in the hands-on as well, but you want to test based on the use case.</p><p>And one of the primary ways you test via Guide LLM is adjusting the input and output tokens. So if you have a chat bot use case, a RAG use case, you can adjust the input and output token levels based on your use case. And you'll see that in the hands-on and have an opportunity to kind of play around with that, depending on what you're most interested in.</p><p>So we're going to start with the hands-on. That is the link that red.ht/evals is the link to the workshop. You will be signing in with your email. I have no marketing game. It just requires you to do that. So I'm not going to haunt you after this. You'll put in your email and that is the password.</p><p>And, hold on. I just want to show you, well, I don't want to take up one of the systems. Otherwise I would show you, but you're going to get, once you are in the workshop, you're going to get your instructions on the left-hand side. And you're going to have two terminals available, two terminal sessions available to you on the right-hand side to the same system.</p><p>It's a REL system. The instructions will overview what the system includes a little bit for you so you get a sense. They each have an L4 GPU as an example. Anything else I want to call out before we get started. That'll be primarily what you use. It has Tmux enabled if you like to use that to open up different things and gives you some flexibility.</p><p>We have three different activities. I'm going to pause after each activity so we can have a little bit of a discussion. in between. This is my first time running this particular activity, so we'll gauge kind of the time it takes, but I'm going to give it about 15-20 minutes for this first one.</p><p>Okay, so I'm also pulling up a system and I'm just going to like walk through some of the stuff. The initial page is just going to give you, of course if it loads, Jesus, the internet. The initial page is just going to give you a little bit of background and preparing your system instructions.</p><p>And then my terminals are on this second tab. And everything is going to be glacial pace. So the first thing I have to do, these systems don't have the container toolkit installed. So I just got to do that. Some system logistics because I'm going to be running VLLM inference runtime in a container and I need that to work.</p><p>So that's what that's doing. And then I'm going to deploy a model with VLLM. So you're going to have to grab a hugging face token. Probably have gotten there by now. Probably most of us have a hugging face token, but just disclaimer there. So incognito window, if you're hitting the hugging face rate limit to grab a new token.</p><p>So VLLM you can also install locally, like if you have a Mac or whatever Linux machine, but we're deploying it as a container here. So I'm going to get that to play and I'm, you can see the VLLM serve command at the end. So that's just the VLLM CLI tool.</p><p>And I'm using an IBM granite model because you know, red hat IBM. So we'll be working with that for a chunk of the activity and it takes a bit for VLLM to load the model. So you're going to be waiting for info, the words info like four times in green, and then it's deployed.</p><p>What's nice about VLLM is that it is, it's compatible with the safe tensor format. So has anybody used TRT to load up a model? Okay. Anyway, it's crazy. It requires you to also convert the model formats initially. So there's less kind of configuration steps with VLLM. It takes up less space too.</p><p>So when you do these kinds of system and performance benchmarks, you can make a lot of adjustments like the input and output tokens that I mentioned for the guide LLM configurations. But there's a lot of also configuration opportunities for the inference runtime itself, depending on what you're trying to do.</p><p>So sometimes we'll reduce the max, the context window of the model. So it runs more quickly because if it's a big context window, it's going to be pretty beefy. There's a lot of knobs you can use for VLLM. We're not really going to touch that this particular time, but just so you're aware.</p><p>So I have my three green infos. So that means it's working and the model is successfully deployed. So I'm going to get into my virtual environment, which is already in place. which is already in place. And then I already have guide LLM installed, but I'm going to pip install guide LLM.</p><p>And these are copy buttons, by the way. So you can just easily copy paste things over. So once I have that up, then this command is set up to just work with the model deployed by VLLM and that I'm just keeping up here in the top terminal and can run this in the background, but I'm just not doing that.</p><p>And this is so I have my target. The rate type is a sweep of various benchmarks like inter token latency. There's various types of benchmarks that it'll run that you'll see in the output. But these are all things that can be adjusted. You can run one particular benchmark at a time.</p><p>For instance, you can take a look at and gets guide LLM dash dash help typical type of commands. You can kind of see what the where all the knobs are and the documentation is pretty good, too. So that'll take a couple of minutes to run because I have it set at a rate of five to reduce the amount of time that it takes to process.</p><p>So you can kind of get a sense of the output here once it all processes. And I have explainers on the left hand side of kind of how to read some of this. The mean performance for each is we have the the benchmark info on the top and then benchmark stats on the bottom.</p><p>So the for the constant rate the on the very left hand side. So those are the number of requests sent to the model per second at that particular rate. So three the constant at three dot six three six point nine three if I did written the guide LLM command rate and it was five if I did rate 10 you would see more lines of that at more progressive rates.</p><p>And like whether or not these numbers are good also totally depend on your use case as well. And I would be comparing I would have a better hardware configuration obviously in production as well. So again, I'm just I'm running an eight billion parameter side or two to two billion parameter size model on an L4, which is like, okay, but obviously if you're doing anything concurrently and at scale like that's going to go bonkers pretty quickly.</p><p>So you get like the mean performance the median performance and P99 which is like the the extreme level which matters for SLOs and things like that. And you can also output this into JSON format as well to take a closer look. So once you reach this you can try also with an additional like tweak the parameters and then maybe compare the.</p><p>results and kind of see what what that changed if you do a rag set up. So I think we had I forget what we had what the initial command said but you can adjust those based on a different use case and kind of compare and contrast what the stats look like after it just takes a couple minutes to run.</p><p>who's done with this first exercise. Okay, good. We're a few minutes away from the additional 10 to 15 systems being ready. for the sake of time I am not going to do breaks for kind of discussion in between if everybody's okay with that and we can kind of just converse independently and I'll just awkwardly walk around the room.</p><p>So if you're done with activity one move on to activity two because there are three activities and I want to make sure everybody has appropriate time for each that they would like. But we will also have the systems up until probably about noon early afternoon. We'll keep them up if you want it to also go back and look at it after.</p><p>So. Is that good with everybody? We just kind of steamroll power through. Okay. Yes. So I have a new URL where we have three more available so far but others are provisioning. And I just wanted to go ahead and put the URL up. It'll come. They'll appear here. And it's the same password right you said?</p><p>Sorry. It's the same password. It's the same password you said? Yeah. Okay. Okay. So that's the new URL. Three more systems but the more will appear. I put descriptions on the left to explain that but I just wanted to heads up we're kind of moving from system performance to now that kind of factual accuracy part of the pyramid.</p><p>So you get a sense of so we're going to be doing the MMLU pro in the second activity and then the third activity is going to be focused on safety and bias and more custom evals. So that's the trajectory of activities feel free if one is also more interesting than the other feel free to skip around totally fine.</p><p>You can cut you can basically customize everything because I like all these things are open source so you can create a similar type of eval in that multiple choice format that MMLU does with your own data set. There's different ways to do custom accuracy evals with your fine tuned data so we kind of do it in a we have so as part of one of the products we incorporate an eval for our fine tuned models on your proprietary data and we do like a branch of MMLU.</p><p>Essentially so there's kind of a lot of ways to skin a cat you know I love cats in regards to how to set up the evals and a lot of tools available. Yeah, so yeah, yes. You can just like fork it and change the data sources. Yeah, yeah. Put that link back up.</p><p>The instructions don't some of the instructions don't look updated as I expected so I'm going what I'm going to do is everybody in the slack for engineer world fair. I created a slack channel called work workshop beyond benchmarks and I'm going to put some of these activity three does anybody see activity three.</p><p>Okay, I'm putting content in this in this slack. If people can be it's a public channel if people can go there my you'll see starting I'm the link is activity two but then you'll also see at the page for activity three. You're approaching activity three and are looking for that the systems for some reason didn't render my latest changes yesterday where I improved on activity three, but I put the link to it to my repo in the slack channel.</p><p>In this channel if you search for it, but there's information about the slack in the emails that we got about the event, but also on the back of our badges as well to navigate there. And this will be good for after if anybody has any questions and I can send more info about any particular tool here as well.</p><p>I wanted to have a wrap up moment because we have we have about eight minutes. So please feel free to continue working. I put the link to the activities in the slack channel. These environments will be available until the end of the day today. So you have time also to tinker around with whatever you want.</p><p>So you would just to kind of recap we went through the furnace sounds funny. The first was at that system performance latency throughput level. Did everybody kind of get through that successfully? Of course, there's I tried to include reading material and stuff to kind of look more into things after because it is it is a very big topic and there's a lot going on and there's a lot of terms and it is very complicated.</p><p>So hopefully you can use it as a learning reason my GitHub repository as a learning resource to kind of poke around after. But we started there and then we moved into the MMLU pro with MLE Val harness, which also allows you to do a lot of other evaluation benchmarks as a part of that MLE Val harness framework.</p><p>I happen to choose MMLU pro because it took the least amount of time, even though it took still 10 minutes. But there's other ones also that you can play around with within that MLE Val harness repository. You can see the different evals you can run there. And then we ended with a safety evaluation with prompt, which with prompt foo, which is a tool that allows you to do a lot of customizing and your own evals.</p><p>Like you can do all kinds of custom tests with prompt foo. So I wanted to get you exposed to that tool so that you can start looking around there as well. That repository on GitHub also has a lot of different examples. So we use that particular safety focused example.</p><p>But if you look at the prompt foo repository, it's very easy to play around with other types of examples as well. So we kind of moved up the pyramid throughout the activity. So hopefully you get a sense of kind of how you can layer this approach when you're looking at and trying to plan for how to strategically implement evals across your entire system.</p><p>Does anybody have any kind of questions or general what they experienced notes of import? Does this feel like valuable? I'm curious also about use cases and happy to talk to you after too. Yeah. So I'm like super new to evals and stuff. So like when I'm doing evals, it's like testing my prompts and like my data science or if I want to switch like models out and stuff right now.</p><p>I'm actually thinking about like those evals connecting those actually to like my production like running like use cases to like track that like my real performance matches kind of like the drug scene. Yeah. What is like, is there like a word for that concept or like? So for me, what I hear is the kind of CI/CD automation implementation of an evaluation framework just like with software engineering testing is kind of what I hear from that.</p><p>You know? Yeah. So I don't know. It's evaluation, but in a more CI/CD format. When it's actually running for customers and stuff. Yeah. You should have a CI/CD framework that includes these evaluation tests just like for unit testing setups. Yeah. Anybody else? Okay. Thank you, everybody. I really appreciate it.</p><p>Of course, feel free to. Thank you. Feel free to continue to use the repo. Message me on Slack. Again, the environments will be up until about 6:00 PM, 5:00 PM tonight. So thank you, everybody. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. We'll see you next time.</p></div></div></body></html>