<html><head><title>Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</h2><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc"><img src="https://i.ytimg.com/vi/XMnxKGVnEUc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./XMnxKGVnEUc.html">Whisper Transcript</a> | <a href="./transcript_XMnxKGVnEUc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">perfect wonderful uh do you also are you also handy inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=6" target="_blank">00:00:06.720</a></span> | <span class="t">yes yeah wonderful so we can have like three streams or two streams only i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=10" target="_blank">00:00:10.480</a></span> | <span class="t">don't know how that works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=13" target="_blank">00:00:13.520</a></span> | <span class="t">okay let's start guys um so the goal of today's paper reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=24" target="_blank">00:00:24.960</a></span> | <span class="t">is to go through the deep seek r1 paper and what we will be seeing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=32" target="_blank">00:00:32.320</a></span> | <span class="t">first of all the biggest difficulty people have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=34" target="_blank">00:00:34.560</a></span> | <span class="t">in order to understand this paper is the reinforcement learning part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">and what why it is difficult because we are you they use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=42" target="_blank">00:00:42.880</a></span> | <span class="t">another algorithm called the grpo and the goal of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=46" target="_blank">00:00:46.080</a></span> | <span class="t">initial part of this paper reading is actually to give you the background</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=49" target="_blank">00:00:49.440</a></span> | <span class="t">knowledge that is is needed to understand the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=53" target="_blank">00:00:53.840</a></span> | <span class="t">so i will be using the slides that i used for making my video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=58" target="_blank">00:00:58.400</a></span> | <span class="t">reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=60" target="_blank">00:01:00.560</a></span> | <span class="t">so to understand what is the connection between language models and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=64" target="_blank">00:01:04.160</a></span> | <span class="t">reinforcement learning so let's go to that slides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=68" target="_blank">00:01:08.480</a></span> | <span class="t">so let's review very fast very very fast language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=72" target="_blank">00:01:12.320</a></span> | <span class="t">so as you know language models are generative models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=75" target="_blank">00:01:15.600</a></span> | <span class="t">which only have one simple function which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=78" target="_blank">00:01:18.560</a></span> | <span class="t">only have one simple objective which is to tell us what is the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=82" target="_blank">00:01:22.400</a></span> | <span class="t">likely token given an input prompt so if we are given for example the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=86" target="_blank">00:01:26.640</a></span> | <span class="t">prompt shanghai is a city in and we feed it to the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=89" target="_blank">00:01:29.920</a></span> | <span class="t">the language model will generate a probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=93" target="_blank">00:01:33.040</a></span> | <span class="t">over what it thinks means the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=98" target="_blank">00:01:38.000</a></span> | <span class="t">what it thinks is the next likely token that is coherent with the prompt that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=103" target="_blank">00:01:43.680</a></span> | <span class="t">have given the language model and usually we sample the for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=108" target="_blank">00:01:48.960</a></span> | <span class="t">if we feed this prompt to the language model it will give us that maybe the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=111" target="_blank">00:01:51.840</a></span> | <span class="t">next likely token is the word china or beijing or cat or pizza</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=115" target="_blank">00:01:55.920</a></span> | <span class="t">and then we choose what we believe is the most likely based on its probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=120" target="_blank">00:02:00.160</a></span> | <span class="t">or probability score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=123" target="_blank">00:02:03.120</a></span> | <span class="t">then we take this word we put it back in the prompt and we ask again the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=128" target="_blank">00:02:08.480</a></span> | <span class="t">model what is the next next word and we do this this job</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=132" target="_blank">00:02:12.720</a></span> | <span class="t">iteratively to generate a full text for example to generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=140" target="_blank">00:02:20.160</a></span> | <span class="t">response to where is shanghai we first ask the language model where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=143" target="_blank">00:02:23.200</a></span> | <span class="t">it will tell us okay the next likely word is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=146" target="_blank">00:02:26.240</a></span> | <span class="t">shanghai then we put it back in the input of the language model and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=149" target="_blank">00:02:29.600</a></span> | <span class="t">tell us the next likely token is etc etc i'm also making a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=153" target="_blank">00:02:33.360</a></span> | <span class="t">simplified assumption here that says that each token is a word and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=158" target="_blank">00:02:38.080</a></span> | <span class="t">each word is a token this is not the case in language models but for our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=161" target="_blank">00:02:41.920</a></span> | <span class="t">explanation we will think like in this way okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=166" target="_blank">00:02:46.560</a></span> | <span class="t">now we know what is language model what is reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=169" target="_blank">00:02:49.840</a></span> | <span class="t">we go very very simple through what is reinforcement learning and then we find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=173" target="_blank">00:02:53.520</a></span> | <span class="t">the connection between language models and reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=176" target="_blank">00:02:56.720</a></span> | <span class="t">now reinforcement learning is an area of artificial intelligence i don't remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=180" target="_blank">00:03:00.560</a></span> | <span class="t">if it belongs to machine learning in particular but it's an area of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=183" target="_blank">00:03:03.280</a></span> | <span class="t">artificial intelligence that is tasked with optimizing the behavior of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=188" target="_blank">00:03:08.720</a></span> | <span class="t">an agent and the behavior of an agent is called a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=192" target="_blank">00:03:12.560</a></span> | <span class="t">which is the decision making of an agent in such a way that the agent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=196" target="_blank">00:03:16.960</a></span> | <span class="t">performs actions that maximize the reward it gets from performing these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=201" target="_blank">00:03:21.920</a></span> | <span class="t">actions in an environment for example for example i have a cat so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=207" target="_blank">00:03:27.440</a></span> | <span class="t">this cat likes to eat meat like most cats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=211" target="_blank">00:03:31.200</a></span> | <span class="t">and the cat is in my house and the cat will be considered of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=215" target="_blank">00:03:35.760</a></span> | <span class="t">reinforcement learning agent the cat can make some decisions on how it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=220" target="_blank">00:03:40.720</a></span> | <span class="t">wants to move in the house and this will be the policy of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=224" target="_blank">00:03:44.960</a></span> | <span class="t">so the policy tell the cat if the cat should move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=228" target="_blank">00:03:48.160</a></span> | <span class="t">up down left or right in the house now this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=231" target="_blank">00:03:51.840</a></span> | <span class="t">my house you cannot see the border so i will draw them because i don't know why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=235" target="_blank">00:03:55.440</a></span> | <span class="t">the the borders are not drawn here and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=239" target="_blank">00:03:59.600</a></span> | <span class="t">should think of this environment as being a grid environment made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=243" target="_blank">00:04:03.760</a></span> | <span class="t">cells so like the following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=249" target="_blank">00:04:09.120</a></span> | <span class="t">like this what is the goal of the cat the goal of the cat is to arrive to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=255" target="_blank">00:04:15.920</a></span> | <span class="t">meat so the the cat can at each position in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=260" target="_blank">00:04:20.000</a></span> | <span class="t">the house it can make some choices some perform some actions we say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=264" target="_blank">00:04:24.320</a></span> | <span class="t">technically and the actions that the cat can can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=268" target="_blank">00:04:28.240</a></span> | <span class="t">choose at each position in the house is a move up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=271" target="_blank">00:04:31.200</a></span> | <span class="t">down left or right what we want we want to make sure that the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=276" target="_blank">00:04:36.400</a></span> | <span class="t">learns to perform the series of actions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=279" target="_blank">00:04:39.840</a></span> | <span class="t">lead him with very likely to the meat while avoiding the things that the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=286" target="_blank">00:04:46.320</a></span> | <span class="t">is scared about which is the broom and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=288" target="_blank">00:04:48.800</a></span> | <span class="t">bathtub because no cat likes to take shower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=292" target="_blank">00:04:52.400</a></span> | <span class="t">so we designed first of all a reward system for this reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=296" target="_blank">00:04:56.640</a></span> | <span class="t">agent because we want to train this reinforcement learning agent to choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=299" target="_blank">00:04:59.840</a></span> | <span class="t">which actions to perform in each position in the house</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=302" target="_blank">00:05:02.560</a></span> | <span class="t">based on some reward so one reward that we could do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=306" target="_blank">00:05:06.880</a></span> | <span class="t">one reward model could be this one for example if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=310" target="_blank">00:05:10.080</a></span> | <span class="t">cat moves to an empty cell it receives a reward of zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=313" target="_blank">00:05:13.200</a></span> | <span class="t">if the cat moves to the broom it receives a reward of minus one if it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=317" target="_blank">00:05:17.680</a></span> | <span class="t">moves to the bathtub it receives a reward of minus 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=321" target="_blank">00:05:21.440</a></span> | <span class="t">however if after performing a series of actions the cat arrives to the meat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=325" target="_blank">00:05:25.920</a></span> | <span class="t">then the cat receives a big reward of plus 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=330" target="_blank">00:05:30.320</a></span> | <span class="t">the decision making of this cat is governed by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=333" target="_blank">00:05:33.600</a></span> | <span class="t">a model that we will call the policy of this cat and the goal of the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=337" target="_blank">00:05:37.920</a></span> | <span class="t">is to choose an action given the current state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=341" target="_blank">00:05:41.600</a></span> | <span class="t">and the action that the cat can choose is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=344" target="_blank">00:05:44.880</a></span> | <span class="t">stochastic it means that this policy gives us a distribution over all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=349" target="_blank">00:05:49.600</a></span> | <span class="t">possible action that the cat can take so if if we have imagine we have a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=354" target="_blank">00:05:54.480</a></span> | <span class="t">optimized policy if the cat is here the good policy should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=358" target="_blank">00:05:58.800</a></span> | <span class="t">tell us that with very high probability score we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=362" target="_blank">00:06:02.240</a></span> | <span class="t">move down because that's one way to maximize the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=365" target="_blank">00:06:05.280</a></span> | <span class="t">and with very low probability we should move left because that will take us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=370" target="_blank">00:06:10.000</a></span> | <span class="t">towards the the bathtub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=373" target="_blank">00:06:13.600</a></span> | <span class="t">the another thing for example that this policy should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=377" target="_blank">00:06:17.840</a></span> | <span class="t">do is if the cat is here for example it should not move right so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=382" target="_blank">00:06:22.960</a></span> | <span class="t">probability associated to the action move right should be low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=386" target="_blank">00:06:26.240</a></span> | <span class="t">and maybe the probability associated with the action move down should be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=389" target="_blank">00:06:29.920</a></span> | <span class="t">little higher this is what the policy is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=394" target="_blank">00:06:34.320</a></span> | <span class="t">now what is the connection between language models and reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=397" target="_blank">00:06:37.680</a></span> | <span class="t">learning so first of all what is the goal in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=400" target="_blank">00:06:40.080</a></span> | <span class="t">reinforcement learning is to train this policy so to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=403" target="_blank">00:06:43.680</a></span> | <span class="t">this decision making of this cat of this agent in order to choose the proper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=408" target="_blank">00:06:48.320</a></span> | <span class="t">actions at every possible state in the environment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=412" target="_blank">00:06:52.080</a></span> | <span class="t">such that it maximizes the reward that the agent can get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=417" target="_blank">00:06:57.360</a></span> | <span class="t">so a good policy for the cat would be a policy that always leads the cat to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=421" target="_blank">00:07:01.440</a></span> | <span class="t">meat no matter where the cat is now let's go let's connect the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=426" target="_blank">00:07:06.320</a></span> | <span class="t">reinforcement learning with language models so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=428" target="_blank">00:07:08.640</a></span> | <span class="t">language model is also kind of a policy because the language model every time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=433" target="_blank">00:07:13.040</a></span> | <span class="t">you feed a prompt to the language model the language model has to choose an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=436" target="_blank">00:07:16.400</a></span> | <span class="t">action to perform which is what token should come after this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=440" target="_blank">00:07:20.080</a></span> | <span class="t">prompt so in this case we talk about the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=445" target="_blank">00:07:25.600</a></span> | <span class="t">and action the state is the state in which the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=448" target="_blank">00:07:28.320</a></span> | <span class="t">reinforcement agent is in the case of the cat is the position of the cat inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=452" target="_blank">00:07:32.560</a></span> | <span class="t">of the house in case of the language model the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=455" target="_blank">00:07:35.120</a></span> | <span class="t">is the prompt itself that you feed and the action is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=458" target="_blank">00:07:38.080</a></span> | <span class="t">distribution over all the next token that the language model can choose from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=462" target="_blank">00:07:42.960</a></span> | <span class="t">in the case of language models so we also want to train the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=466" target="_blank">00:07:46.240</a></span> | <span class="t">perform its action actions or to choose the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=470" target="_blank">00:07:50.480</a></span> | <span class="t">tokens in a particular way according to some reward that we that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=475" target="_blank">00:07:55.040</a></span> | <span class="t">can build a reward model that we can build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=477" target="_blank">00:07:57.360</a></span> | <span class="t">in the case specifically case of reinforcement learning from human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=480" target="_blank">00:08:00.400</a></span> | <span class="t">feedback we want the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=484" target="_blank">00:08:04.560</a></span> | <span class="t">generate text using particular rules for example when we do language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=490" target="_blank">00:08:10.160</a></span> | <span class="t">alignment we are first of all how language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=492" target="_blank">00:08:12.960</a></span> | <span class="t">are trained usually we have a pre-training part where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=496" target="_blank">00:08:16.000</a></span> | <span class="t">we feed a lot of information to the language model so we throw a lot of data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=499" target="_blank">00:08:19.840</a></span> | <span class="t">like the entire wikipedia the entire web the entire i don't know stack overflow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=503" target="_blank">00:08:23.840</a></span> | <span class="t">and the leet code everything and the language model learns how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=507" target="_blank">00:08:27.920</a></span> | <span class="t">kind of the structure of the language it learns a little bit of chinese a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=511" target="_blank">00:08:31.920</a></span> | <span class="t">bit of english a little bit of japanese because we throw every data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=515" target="_blank">00:08:35.120</a></span> | <span class="t">possible that we have at the language model then we do a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=518" target="_blank">00:08:38.960</a></span> | <span class="t">fine-tuning so we train the language model to generate high quality data so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=522" target="_blank">00:08:42.160</a></span> | <span class="t">we increase the likelihood of generating high quality outputs instead of just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=525" target="_blank">00:08:45.920</a></span> | <span class="t">throwing whatever is on the internet but then we do also an alignment part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=530" target="_blank">00:08:50.480</a></span> | <span class="t">in which we want the language model to follow instructions so we want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=533" target="_blank">00:08:53.440</a></span> | <span class="t">language model to adhere to some standards for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=536" target="_blank">00:08:56.960</a></span> | <span class="t">what makes the language model conversational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=539" target="_blank">00:08:59.040</a></span> | <span class="t">is the conversation is the instruction fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=542" target="_blank">00:09:02.320</a></span> | <span class="t">which means that we train the language model to follow a particular format so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=546" target="_blank">00:09:06.400</a></span> | <span class="t">always for example greet the user always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=550" target="_blank">00:09:10.160</a></span> | <span class="t">be helpful always never use a curse word etc etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=554" target="_blank">00:09:14.640</a></span> | <span class="t">and this job is done through the reinforcement learning from human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=557" target="_blank">00:09:17.040</a></span> | <span class="t">feedback which includes many kind of algorithms like ppo dpo etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=560" target="_blank">00:09:20.960</a></span> | <span class="t">and grpo is one of them what we do usually in language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=565" target="_blank">00:09:25.920</a></span> | <span class="t">to train the language model to follow instructions is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=570" target="_blank">00:09:30.400</a></span> | <span class="t">generate a data set of instructions in which we we first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=577" target="_blank">00:09:37.840</a></span> | <span class="t">have some list of questions and then we ask the language model to generate some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=581" target="_blank">00:09:41.600</a></span> | <span class="t">answers and then we ask some professional annotators to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=585" target="_blank">00:09:45.360</a></span> | <span class="t">choose which answer they would like the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=588" target="_blank">00:09:48.480</a></span> | <span class="t">generate more and which one they don't like the language model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=593" target="_blank">00:09:53.200</a></span> | <span class="t">and the the goal of reinforcement learning from human feedback is to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=597" target="_blank">00:09:57.200</a></span> | <span class="t">sure that the language model will generate more answers like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=601" target="_blank">00:10:01.920</a></span> | <span class="t">ones that are chosen by the annotators and the less likely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=605" target="_blank">00:10:05.200</a></span> | <span class="t">generate answers that are not chosen by the annotators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=610" target="_blank">00:10:10.080</a></span> | <span class="t">this is called the reward model of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=615" target="_blank">00:10:15.680</a></span> | <span class="t">of the reinforcement learning from human feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=620" target="_blank">00:10:20.320</a></span> | <span class="t">okay now that we have understood a little bit the connection between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=624" target="_blank">00:10:24.400</a></span> | <span class="t">language models and the reinforcement learning framework let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=628" target="_blank">00:10:28.400</a></span> | <span class="t">move on to the paper i just want to do a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=631" target="_blank">00:10:31.520</a></span> | <span class="t">review of what we have seen so far so now we know what are language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=635" target="_blank">00:10:35.360</a></span> | <span class="t">they are models that generate the probability over what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=639" target="_blank">00:10:39.680</a></span> | <span class="t">next likely what is the next token the probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=645" target="_blank">00:10:45.680</a></span> | <span class="t">of what is the what should be the next token based on the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=649" target="_blank">00:10:49.040</a></span> | <span class="t">we know what is reinforcement learning which is a framework for training the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=652" target="_blank">00:10:52.240</a></span> | <span class="t">policy of an agent in order to choose actions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=655" target="_blank">00:10:55.760</a></span> | <span class="t">that maximize its reward what is the connection between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=659" target="_blank">00:10:59.440</a></span> | <span class="t">reinforcement learning and language model is that the language model itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=662" target="_blank">00:11:02.480</a></span> | <span class="t">is a policy because it makes decisions it takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=665" target="_blank">00:11:05.600</a></span> | <span class="t">actions in choosing what is the next token and we want the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=669" target="_blank">00:11:09.600</a></span> | <span class="t">choose tokens so the next token in such a way that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=673" target="_blank">00:11:13.120</a></span> | <span class="t">it follows some standards which are according to a data set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=677" target="_blank">00:11:17.360</a></span> | <span class="t">preferences that we usually build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=681" target="_blank">00:11:21.040</a></span> | <span class="t">this data set of preferences is converted into a reward model but we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=685" target="_blank">00:11:25.280</a></span> | <span class="t">will not be covering the reward model for now at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=688" target="_blank">00:11:28.320</a></span> | <span class="t">so now let's go to the deepseq paper now in the deepseq</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=691" target="_blank">00:11:31.760</a></span> | <span class="t">in the deepseq r1 paper what they do is they start with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=696" target="_blank">00:11:36.240</a></span> | <span class="t">a pre-trained model they they use the deepseq v3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=699" target="_blank">00:11:39.760</a></span> | <span class="t">base model which i believe is a 600 billion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=702" target="_blank">00:11:42.720</a></span> | <span class="t">parameter model and then they want this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=706" target="_blank">00:11:46.400</a></span> | <span class="t">this model to perform better at reasoning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=711" target="_blank">00:11:51.280</a></span> | <span class="t">what does it mean to perform better at reasoning it we want the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=715" target="_blank">00:11:55.120</a></span> | <span class="t">to find a way to solve complex problems by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=719" target="_blank">00:11:59.040</a></span> | <span class="t">breaking them into smaller steps that can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=724" target="_blank">00:12:04.240</a></span> | <span class="t">that are easier for the language model to manage and the way they do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=728" target="_blank">00:12:08.160</a></span> | <span class="t">is also through reinforcement learning let's go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=732" target="_blank">00:12:12.240</a></span> | <span class="t">to the paper so let's go here let's go here let's go here okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=738" target="_blank">00:12:18.640</a></span> | <span class="t">first of all they say that in this section we explore the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=742" target="_blank">00:12:22.400</a></span> | <span class="t">so let me use the in this section we explore the potential of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=745" target="_blank">00:12:25.840</a></span> | <span class="t">language models to develop reasoning capabilities without any supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=749" target="_blank">00:12:29.920</a></span> | <span class="t">data focusing on their self-evolution through a pure reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=754" target="_blank">00:12:34.080</a></span> | <span class="t">process so what is supervised data when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=756" target="_blank">00:12:36.800</a></span> | <span class="t">train language models we also try to build some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=759" target="_blank">00:12:39.680</a></span> | <span class="t">very high quality data set of what the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=763" target="_blank">00:12:43.200</a></span> | <span class="t">should be generating because as we said before we have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=766" target="_blank">00:12:46.720</a></span> | <span class="t">multiple stages of training one is the pre-training in which we just throw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=770" target="_blank">00:12:50.400</a></span> | <span class="t">random data from the web to the language model but then we want the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=773" target="_blank">00:12:53.520</a></span> | <span class="t">model to generate high quality data so we have this kind of the supervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=777" target="_blank">00:12:57.840</a></span> | <span class="t">fine-tuning part and they skip this part here they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=781" target="_blank">00:13:01.920</a></span> | <span class="t">take the base model and then they want the base model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=785" target="_blank">00:13:05.840</a></span> | <span class="t">develop the reasoning capability just by using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=789" target="_blank">00:13:09.680</a></span> | <span class="t">reinforcement learning which means that we want to incentivize the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=794" target="_blank">00:13:14.080</a></span> | <span class="t">model through a reward system to develop by itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=798" target="_blank">00:13:18.400</a></span> | <span class="t">what is the sequence of token that should lead to so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=802" target="_blank">00:13:22.320</a></span> | <span class="t">for the language model to acquire as much reward as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=805" target="_blank">00:13:25.840</a></span> | <span class="t">it's like i take my cat and i want the cat to solve math problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=812" target="_blank">00:13:32.560</a></span> | <span class="t">and i what i can do i can just play with how many biscuits i can give to the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=817" target="_blank">00:13:37.440</a></span> | <span class="t">so if i build my reward model in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=820" target="_blank">00:13:40.960</a></span> | <span class="t">that the cat is incentivized to solve math problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=826" target="_blank">00:13:46.560</a></span> | <span class="t">then by the because the cat wants to get the biscuit the cat will develop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=831" target="_blank">00:13:51.120</a></span> | <span class="t">whatever skill it needs to develop in order to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=834" target="_blank">00:13:54.240</a></span> | <span class="t">maximize the number of biscuits it gets now of course the cat will never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=837" target="_blank">00:13:57.440</a></span> | <span class="t">develop it because the underlying kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=839" target="_blank">00:13:59.680</a></span> | <span class="t">lacks the capability of learning certain things but that's not the problem in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=843" target="_blank">00:14:03.600</a></span> | <span class="t">language models because big language models have a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=847" target="_blank">00:14:07.600</a></span> | <span class="t">capabilities in developing novel skills</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=851" target="_blank">00:14:11.840</a></span> | <span class="t">now the algorithm that they use in DeepSeq R1 is called the GRPO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=858" target="_blank">00:14:18.320</a></span> | <span class="t">algorithm if you look at my video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=861" target="_blank">00:14:21.520</a></span> | <span class="t">reinforcement learning from feedback historically we have always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=865" target="_blank">00:14:25.440</a></span> | <span class="t">used the PPO algorithm more recently the DPO algorithm there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=869" target="_blank">00:14:29.760</a></span> | <span class="t">also other algorithms like the ORPO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=873" target="_blank">00:14:33.360</a></span> | <span class="t">what they use here is called the GRPO algorithm and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=877" target="_blank">00:14:37.040</a></span> | <span class="t">very similar to the PPO but slightly different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=880" target="_blank">00:14:40.160</a></span> | <span class="t">and we will see how let's see what does the GRPO algorithm does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=887" target="_blank">00:14:47.280</a></span> | <span class="t">well as we saw before when we do the when we do reinforcement learning on a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=894" target="_blank">00:14:54.720</a></span> | <span class="t">language model we have a data set of preferences so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=898" target="_blank">00:14:58.160</a></span> | <span class="t">have some questions and then we ask the language model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=901" target="_blank">00:15:01.120</a></span> | <span class="t">multiple answers and then we ask annotators to choose which answer they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=904" target="_blank">00:15:04.720</a></span> | <span class="t">like and then we train a language a reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=907" target="_blank">00:15:07.760</a></span> | <span class="t">model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=911" target="_blank">00:15:11.120</a></span> | <span class="t">that gives the signal to the language model to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=916" target="_blank">00:15:16.000</a></span> | <span class="t">which if the answer that the language model is generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=920" target="_blank">00:15:20.400</a></span> | <span class="t">is good or bad in this case with the GRPO we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=926" target="_blank">00:15:26.240</a></span> | <span class="t">following objective now if you have never seen clip or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=931" target="_blank">00:15:31.120</a></span> | <span class="t">if you are sorry if you have never seen PPO this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=934" target="_blank">00:15:34.320</a></span> | <span class="t">sounds quite scary but let's try to break it down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=938" target="_blank">00:15:38.000</a></span> | <span class="t">step by step what are we doing here is we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=941" target="_blank">00:15:41.920</a></span> | <span class="t">to optimize a policy so the policy is the language model itself and the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=947" target="_blank">00:15:47.040</a></span> | <span class="t">is always denoted with the letter pi like the greek letter pi so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=951" target="_blank">00:15:51.840</a></span> | <span class="t">pi of theta is the policy so it is the language model that we are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=955" target="_blank">00:15:55.280</a></span> | <span class="t">optimize we want this policy to be trained to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=959" target="_blank">00:15:59.120</a></span> | <span class="t">maximize the following objective what is this objective and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=964" target="_blank">00:16:04.160</a></span> | <span class="t">objective is saying that if i have a list of questions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=968" target="_blank">00:16:08.560</a></span> | <span class="t">belong to some database of questions and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=973" target="_blank">00:16:13.840</a></span> | <span class="t">we sample some output from our policy using these questions then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=981" target="_blank">00:16:21.680</a></span> | <span class="t">based on some reward that this output that this output of the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=988" target="_blank">00:16:28.880</a></span> | <span class="t">model get from our reward system we should train the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=994" target="_blank">00:16:34.080</a></span> | <span class="t">give more weight to those actions that result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=998" target="_blank">00:16:38.080</a></span> | <span class="t">in good reward and to give less weight so are less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1002" target="_blank">00:16:42.480</a></span> | <span class="t">likely the language model should be less likely to take those actions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1006" target="_blank">00:16:46.560</a></span> | <span class="t">lead to bad reward and the way we do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1011" target="_blank">00:16:51.120</a></span> | <span class="t">is as follows so we take basically what we are doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1016" target="_blank">00:16:56.400</a></span> | <span class="t">okay here you see all the policy and the new policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1020" target="_blank">00:17:00.240</a></span> | <span class="t">for now let's ignore that that's because we do something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1023" target="_blank">00:17:03.760</a></span> | <span class="t">offline learning so we will not be covering that at least now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1028" target="_blank">00:17:08.160</a></span> | <span class="t">for now what we want to do ignore the the the denominator here so the pi old</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1035" target="_blank">00:17:15.600</a></span> | <span class="t">just concentrate on the um on the term pi what we are saying is that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1041" target="_blank">00:17:21.840</a></span> | <span class="t">generate the log probabilities of the output so what is the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1046" target="_blank">00:17:26.080</a></span> | <span class="t">probability of the output let's do it step by step actually okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1052" target="_blank">00:17:32.320</a></span> | <span class="t">imagine we have the following question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1056" target="_blank">00:17:36.560</a></span> | <span class="t">for example imagine we ask the language model where is shanghai so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1062" target="_blank">00:17:42.160</a></span> | <span class="t">and the language model generates because as we saw we sample a few questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1073" target="_blank">00:17:53.120</a></span> | <span class="t">from a database of questions and then language model and then we generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1077" target="_blank">00:17:57.760</a></span> | <span class="t">multiple outputs using this this question using our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1080" target="_blank">00:18:00.720</a></span> | <span class="t">language model so these outputs are called o and there are g of them this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1085" target="_blank">00:18:05.680</a></span> | <span class="t">the group in grpo maybe the language model the first time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1090" target="_blank">00:18:10.320</a></span> | <span class="t">will generate let's say uh shanghai is in china so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1095" target="_blank">00:18:15.360</a></span> | <span class="t">let me just write shanghai is in china another output could be for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1102" target="_blank">00:18:22.240</a></span> | <span class="t">example the sky is blue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1109" target="_blank">00:18:29.200</a></span> | <span class="t">and another output could be shanghai is beautiful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1116" target="_blank">00:18:36.640</a></span> | <span class="t">imagine we have some magic reward system that assigns or imagine that our reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1125" target="_blank">00:18:45.120</a></span> | <span class="t">system is a human being this human being will very likely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1130" target="_blank">00:18:50.000</a></span> | <span class="t">give a very high reward to this answer zero reward to this answer and maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1136" target="_blank">00:18:56.000</a></span> | <span class="t">not completely zero but nearly zero score to this answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1141" target="_blank">00:19:01.040</a></span> | <span class="t">why because this at least talks about shanghai this doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1144" target="_blank">00:19:04.400</a></span> | <span class="t">talk about anything related to shanghai and this actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1147" target="_blank">00:19:07.520</a></span> | <span class="t">answers the questions now when we have a language model we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1153" target="_blank">00:19:13.440</a></span> | <span class="t">a question and the generated answer we have what is known as a trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1159" target="_blank">00:19:19.600</a></span> | <span class="t">the trajectory is a list of actions that the language model has taken</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1164" target="_blank">00:19:24.000</a></span> | <span class="t">why because the language model was given this question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1167" target="_blank">00:19:27.280</a></span> | <span class="t">as input and the language model took an action chose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1171" target="_blank">00:19:31.600</a></span> | <span class="t">an action to generate the first token which is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1174" target="_blank">00:19:34.640</a></span> | <span class="t">then this shanghai was put back into the language model and then the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1178" target="_blank">00:19:38.080</a></span> | <span class="t">model choose another action which is the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1180" target="_blank">00:19:40.480</a></span> | <span class="t">is and then this is was put back into the language model and then the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1184" target="_blank">00:19:44.480</a></span> | <span class="t">model chose another action which is in etc etc this is a trajectory at each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1189" target="_blank">00:19:49.840</a></span> | <span class="t">step of the generation process the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1192" target="_blank">00:19:52.240</a></span> | <span class="t">model chose an action based on the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1195" target="_blank">00:19:55.600</a></span> | <span class="t">distribution that it generated actually it's not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1198" target="_blank">00:19:58.240</a></span> | <span class="t">language model that chooses it's our sampling strategy that chooses the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1201" target="_blank">00:20:01.760</a></span> | <span class="t">particular token so usually we can use the greedy strategy or the top of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1205" target="_blank">00:20:05.120</a></span> | <span class="t">strategy or whatever so at each step we have chosen some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1210" target="_blank">00:20:10.960</a></span> | <span class="t">action based on the distribution of the language that the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1214" target="_blank">00:20:14.800</a></span> | <span class="t">generated for each state so we have a log probability a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1220" target="_blank">00:20:20.560</a></span> | <span class="t">probability associated with the word shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1224" target="_blank">00:20:24.560</a></span> | <span class="t">on conditioned on the question where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1227" target="_blank">00:20:27.680</a></span> | <span class="t">we have a probability associated with the word is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1230" target="_blank">00:20:30.640</a></span> | <span class="t">conditioned on the input where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1233" target="_blank">00:20:33.680</a></span> | <span class="t">question mark shanghai then we have a probability associated with the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1237" target="_blank">00:20:37.840</a></span> | <span class="t">in conditioned on the input where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1241" target="_blank">00:20:41.200</a></span> | <span class="t">question mark shanghai is blah blah blah etc etc so we have a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1245" target="_blank">00:20:45.280</a></span> | <span class="t">probabilities what we are doing here is that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1250" target="_blank">00:20:50.400</a></span> | <span class="t">the um this is the product of all the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1255" target="_blank">00:20:55.600</a></span> | <span class="t">that are generated at each step of the particular output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1259" target="_blank">00:20:59.280</a></span> | <span class="t">o i here which is maybe the first answer here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1263" target="_blank">00:21:03.440</a></span> | <span class="t">then we ask the language model again the same question and the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1266" target="_blank">00:21:06.560</a></span> | <span class="t">will come up with another output and this output will also have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1270" target="_blank">00:21:10.480</a></span> | <span class="t">associated with it a list of log probabilities and these are the log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1274" target="_blank">00:21:14.160</a></span> | <span class="t">probability that you see here each log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1277" target="_blank">00:21:17.520</a></span> | <span class="t">furthermore is weighted by an advantage term the advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1284" target="_blank">00:21:24.960</a></span> | <span class="t">is basically telling me how better is choosing a particular token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1291" target="_blank">00:21:31.040</a></span> | <span class="t">given a particular input over all the tokens that are available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1294" target="_blank">00:21:34.800</a></span> | <span class="t">for example imagine we have the input where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1299" target="_blank">00:21:39.360</a></span> | <span class="t">is it better to choose the word shanghai or is it better to as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1303" target="_blank">00:21:43.200</a></span> | <span class="t">first word of the response or the word pizza</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1307" target="_blank">00:21:47.520</a></span> | <span class="t">i believe it's better to choose the word shanghai so the advantage of choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1311" target="_blank">00:21:51.520</a></span> | <span class="t">the word shanghai would result in a better long-term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1315" target="_blank">00:21:55.680</a></span> | <span class="t">reward for the policy so for the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1318" target="_blank">00:21:58.480</a></span> | <span class="t">model because it will result in a good answer so it will result in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1322" target="_blank">00:22:02.720</a></span> | <span class="t">high reward from our reward model for now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1325" target="_blank">00:22:05.840</a></span> | <span class="t">just have modeled the reward model as a human being who tells that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1329" target="_blank">00:22:09.600</a></span> | <span class="t">language model okay this is a good answer this is a bad answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1332" target="_blank">00:22:12.960</a></span> | <span class="t">but actually later we will see that the reward is actually also a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1336" target="_blank">00:22:16.480</a></span> | <span class="t">and in the case of grpo they actually used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1339" target="_blank">00:22:19.440</a></span> | <span class="t">in the case of the deep seek r1 they use actually a rule-based reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1343" target="_blank">00:22:23.120</a></span> | <span class="t">model so let's go back we have a list of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1350" target="_blank">00:22:30.240</a></span> | <span class="t">we generate multiple answers with each of these questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1354" target="_blank">00:22:34.320</a></span> | <span class="t">using our language model and then we for each of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1358" target="_blank">00:22:38.400</a></span> | <span class="t">answers we have the log probabilities associated with this answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1362" target="_blank">00:22:42.720</a></span> | <span class="t">which is just the product of all the probabilities of choosing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1366" target="_blank">00:22:46.160</a></span> | <span class="t">particular token given that particular input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1369" target="_blank">00:22:49.600</a></span> | <span class="t">we weight each of this log probability by an advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1374" target="_blank">00:22:54.320</a></span> | <span class="t">which basically tells us how good is choosing this particular token over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1379" target="_blank">00:22:59.680</a></span> | <span class="t">all the other that are available for this particular input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1383" target="_blank">00:23:03.120</a></span> | <span class="t">and we train our language model to maximize this objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1387" target="_blank">00:23:07.120</a></span> | <span class="t">let's see what does it mean to maximize this objective and now let's also see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1390" target="_blank">00:23:10.640</a></span> | <span class="t">what is this old here the language model that we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1395" target="_blank">00:23:15.360</a></span> | <span class="t">training is our deep seek base right so at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1399" target="_blank">00:23:19.760</a></span> | <span class="t">beginning suppose that this pi old is the base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1403" target="_blank">00:23:23.200</a></span> | <span class="t">version of deep seek what we do is basically we want to refine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1408" target="_blank">00:23:28.560</a></span> | <span class="t">it iteratively by generate keep generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1412" target="_blank">00:23:32.480</a></span> | <span class="t">output from it and then through the reward model we want to tell it okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1416" target="_blank">00:23:36.480</a></span> | <span class="t">this was a good this was a good output so do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1420" target="_blank">00:23:40.480</a></span> | <span class="t">more of this or this was a bad output so do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1424" target="_blank">00:23:44.240</a></span> | <span class="t">less of this this is one of the advantage of using reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1428" target="_blank">00:23:48.000</a></span> | <span class="t">because if you do supervised fine tuning you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1430" target="_blank">00:23:50.560</a></span> | <span class="t">just telling the language model to i want this so generate this if you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1434" target="_blank">00:23:54.560</a></span> | <span class="t">doing reinforcement learning you have the ability to tell the language model i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1437" target="_blank">00:23:57.600</a></span> | <span class="t">want more of this and i want less of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1440" target="_blank">00:24:00.800</a></span> | <span class="t">what we are doing is the the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1444" target="_blank">00:24:04.320</a></span> | <span class="t">at each iteration the language model should be optimized in the following way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1452" target="_blank">00:24:12.080</a></span> | <span class="t">if the language model at the current iteration is giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1455" target="_blank">00:24:15.120</a></span> | <span class="t">more probability more likelihood to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1459" target="_blank">00:24:19.120</a></span> | <span class="t">a response that resulted in a good reward and the advantage term will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1465" target="_blank">00:24:25.040</a></span> | <span class="t">high in that case then this policy is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1470" target="_blank">00:24:30.160</a></span> | <span class="t">objective is telling the language model do more of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1473" target="_blank">00:24:33.440</a></span> | <span class="t">however if right now the language model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1476" target="_blank">00:24:36.800</a></span> | <span class="t">giving less probability to an action that also resulted in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1481" target="_blank">00:24:41.360</a></span> | <span class="t">a low than a bad reward and the advantage term will be negative in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1486" target="_blank">00:24:46.560</a></span> | <span class="t">that case then by optimizing this objective here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1490" target="_blank">00:24:50.400</a></span> | <span class="t">by maximizing the objective here the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1493" target="_blank">00:24:53.040</a></span> | <span class="t">will learn to do less of that i know that i didn't explain very well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1498" target="_blank">00:24:58.640</a></span> | <span class="t">the pi theta and the pi old theta i believe i can do that later when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1503" target="_blank">00:25:03.200</a></span> | <span class="t">if we have time by talking about offline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1508" target="_blank">00:25:08.960</a></span> | <span class="t">offline learning in the case of in the case of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1515" target="_blank">00:25:15.440</a></span> | <span class="t">moreover in the grpo you find this kehl divergence term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1519" target="_blank">00:25:19.760</a></span> | <span class="t">now for people who already know what is the kehl divergence that would be super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1522" target="_blank">00:25:22.960</a></span> | <span class="t">easy but for people who don't know basically the kehl divergence is a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1526" target="_blank">00:25:26.480</a></span> | <span class="t">of measuring how to distributions are different so how far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1530" target="_blank">00:25:30.240</a></span> | <span class="t">they are what we want is we want the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1533" target="_blank">00:25:33.360</a></span> | <span class="t">model to be fine tuned to generate more of things that lead to a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1538" target="_blank">00:25:38.000</a></span> | <span class="t">reward to do less of things that result in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1541" target="_blank">00:25:41.600</a></span> | <span class="t">low reward but at the same time we don't want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1544" target="_blank">00:25:44.240</a></span> | <span class="t">language model to change too much its behavior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1546" target="_blank">00:25:46.880</a></span> | <span class="t">for example imagine we have a reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1549" target="_blank">00:25:49.920</a></span> | <span class="t">that tells the language model to be more polite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1553" target="_blank">00:25:53.760</a></span> | <span class="t">what the language model could do and imagine that by saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1556" target="_blank">00:25:56.880</a></span> | <span class="t">being polite for example in my case means that i always say thank you right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1560" target="_blank">00:26:00.880</a></span> | <span class="t">so what could happen is if we don't enforce the kehl divergence is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1564" target="_blank">00:26:04.160</a></span> | <span class="t">language model could just cheat and always generates thank you thank you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1567" target="_blank">00:26:07.440</a></span> | <span class="t">thank you thank you thank you thank you at every response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1569" target="_blank">00:26:09.760</a></span> | <span class="t">like a list of thank yous because that results obviously in a high reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1573" target="_blank">00:26:13.920</a></span> | <span class="t">but the language model would stop doing its main job which is to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1577" target="_blank">00:26:17.360</a></span> | <span class="t">something factual and useful so we want the language model to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1581" target="_blank">00:26:21.280</a></span> | <span class="t">a little bit but to change so to be more polite but not just be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1587" target="_blank">00:26:27.840</a></span> | <span class="t">just generate a bunch of thank yous so change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1590" target="_blank">00:26:30.880</a></span> | <span class="t">change but change a little bit so that's why we add the scale divergence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1595" target="_blank">00:26:35.360</a></span> | <span class="t">otherwise what the language model will do it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1597" target="_blank">00:26:37.440</a></span> | <span class="t">do what is known as a reward hacking which is it will try to find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1601" target="_blank">00:26:41.200</a></span> | <span class="t">a way to just learn what is a trick to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1605" target="_blank">00:26:45.440</a></span> | <span class="t">maximize its reward without actually being useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1610" target="_blank">00:26:50.240</a></span> | <span class="t">this is also for example if you want a parallel like example it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1614" target="_blank">00:26:54.240</a></span> | <span class="t">you have a text code and it's very complex people will always find a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1618" target="_blank">00:26:58.880</a></span> | <span class="t">cheat on it and if you have a text code that is very simple made up of few rules</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1623" target="_blank">00:27:03.120</a></span> | <span class="t">then it's very unlikely that people will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1625" target="_blank">00:27:05.200</a></span> | <span class="t">able to cheat on it so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1628" target="_blank">00:27:08.880</a></span> | <span class="t">and so so you cannot do like in that case you cannot cheat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1634" target="_blank">00:27:14.480</a></span> | <span class="t">okay what do we miss here so first of all i didn't explain the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1643" target="_blank">00:27:23.520</a></span> | <span class="t">i didn't explain the offline policy learning and i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1650" target="_blank">00:27:30.800</a></span> | <span class="t">explain the clip part why we are clipping here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1656" target="_blank">00:27:36.400</a></span> | <span class="t">the clipping part basically we are saying if the language model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1659" target="_blank">00:27:39.680</a></span> | <span class="t">trying to change its log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1664" target="_blank">00:27:44.400</a></span> | <span class="t">if the language model is trying to change its log probabilities by being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1668" target="_blank">00:27:48.160</a></span> | <span class="t">too confident about its change then we don't want we don't want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1673" target="_blank">00:27:53.680</a></span> | <span class="t">let the model be overly confident basically what means that if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1678" target="_blank">00:27:58.160</a></span> | <span class="t">this is the language model at the current iteration and this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1681" target="_blank">00:28:01.520</a></span> | <span class="t">you can think of it at the previous iteration in the training process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1686" target="_blank">00:28:06.080</a></span> | <span class="t">if the language model at the current iteration is very confident that by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1690" target="_blank">00:28:10.160</a></span> | <span class="t">saying the word shanghai will result in a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1693" target="_blank">00:28:13.600</a></span> | <span class="t">reward even if it's a good choice we don't want the language model to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1698" target="_blank">00:28:18.880</a></span> | <span class="t">overly confident so we clip this ratio between the log probabilities up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1704" target="_blank">00:28:24.960</a></span> | <span class="t">one plus epsilon or or in the lower case in one minus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1710" target="_blank">00:28:30.800</a></span> | <span class="t">because if the language model will choose the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1714" target="_blank">00:28:34.320</a></span> | <span class="t">word as a shanghai given this question then okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1717" target="_blank">00:28:37.440</a></span> | <span class="t">we are lucky and it's it's good but imagine the language model is overly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1721" target="_blank">00:28:41.040</a></span> | <span class="t">confident then the next word is i don't know coffee then we don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1725" target="_blank">00:28:45.920</a></span> | <span class="t">the language model to make too big step we want the model to learn as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1729" target="_blank">00:28:49.760</a></span> | <span class="t">slow as possible by choosing the accordingly this epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1734" target="_blank">00:28:54.960</a></span> | <span class="t">term which is something that we can choose and this beta term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1739" target="_blank">00:28:59.280</a></span> | <span class="t">okay i believe i have covered a little bit of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1743" target="_blank">00:29:03.760</a></span> | <span class="t">so last last review guys so we are trying to optimize the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1747" target="_blank">00:29:07.840</a></span> | <span class="t">iteratively by telling it to make more of something that we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1751" target="_blank">00:29:11.840</a></span> | <span class="t">and to do less of what we don't want how does the model know what we want and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1755" target="_blank">00:29:15.920</a></span> | <span class="t">what we don't want it's the reward that we give it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1758" target="_blank">00:29:18.720</a></span> | <span class="t">according to our reward model now let's talk about the reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1762" target="_blank">00:29:22.960</a></span> | <span class="t">the reward model historically in ppo let's go to the other here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1768" target="_blank">00:29:28.720</a></span> | <span class="t">was a model which was of the same structure as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1773" target="_blank">00:29:33.120</a></span> | <span class="t">language model that we are trying to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1775" target="_blank">00:29:35.680</a></span> | <span class="t">in which we add a linear layer on top that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1778" target="_blank">00:29:38.960</a></span> | <span class="t">gives a reward to each answer how can we assign a numeric reward to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1785" target="_blank">00:29:45.680</a></span> | <span class="t">a particular answer so if you remember when we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1792" target="_blank">00:29:52.160</a></span> | <span class="t">about the bpo i said that usually we start with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1795" target="_blank">00:29:55.680</a></span> | <span class="t">questions then we generate a few answers then we ask some annotators to choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1799" target="_blank">00:29:59.360</a></span> | <span class="t">which answers we like and they and to also tell us which answers they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1802" target="_blank">00:30:02.800</a></span> | <span class="t">don't like how do we convert this data set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1805" target="_blank">00:30:05.840</a></span> | <span class="t">preferences into a number we do that by training a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1810" target="_blank">00:30:10.720</a></span> | <span class="t">language model which has the same architecture as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1814" target="_blank">00:30:14.160</a></span> | <span class="t">policy that we are trying to train just a different head on top that instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1818" target="_blank">00:30:18.160</a></span> | <span class="t">of generating the log probabilities of the next token generates a numeric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1821" target="_blank">00:30:21.600</a></span> | <span class="t">reward and we do it with what is known as the bradley terry model which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1827" target="_blank">00:30:27.040</a></span> | <span class="t">basically this loss here you don't have to understand this law</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1830" target="_blank">00:30:30.240</a></span> | <span class="t">doesn't matter it basically means that we want the if we train the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1834" target="_blank">00:30:34.400</a></span> | <span class="t">model on this loss it will generate a very high reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1837" target="_blank">00:30:37.840</a></span> | <span class="t">for the quest bradley terry model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1842" target="_blank">00:30:42.000</a></span> | <span class="t">so if we train our language model on this loss it will basically result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1849" target="_blank">00:30:49.520</a></span> | <span class="t">in this head here this linear head on top of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1853" target="_blank">00:30:53.360</a></span> | <span class="t">to generate a very high value for the questions that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1857" target="_blank">00:30:57.200</a></span> | <span class="t">for the answers that were chosen in the data set of preferences and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1860" target="_blank">00:31:00.800</a></span> | <span class="t">a very low or low basically value for the answers that were not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1866" target="_blank">00:31:06.800</a></span> | <span class="t">chosen by the professional annotators this is how it was done with ppo so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1873" target="_blank">00:31:13.120</a></span> | <span class="t">we have an objective which increases the likelihood of the things that give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1878" target="_blank">00:31:18.000</a></span> | <span class="t">high reward we also have a reward model which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1880" target="_blank">00:31:20.960</a></span> | <span class="t">generates a numeric reward as a signal for the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1884" target="_blank">00:31:24.480</a></span> | <span class="t">model to understand what it should do more and what it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1887" target="_blank">00:31:27.520</a></span> | <span class="t">do less in deepseek r1 they do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1890" target="_blank">00:31:30.800</a></span> | <span class="t">different they instead of using a model as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1895" target="_blank">00:31:35.200</a></span> | <span class="t">reward they use a rule-based model system so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1898" target="_blank">00:31:38.640</a></span> | <span class="t">they don't train a neural network to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1901" target="_blank">00:31:41.440</a></span> | <span class="t">generate a number that gives a signal to the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1904" target="_blank">00:31:44.800</a></span> | <span class="t">understand what we like and what we don't like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1908" target="_blank">00:31:48.960</a></span> | <span class="t">they used a rule-based system you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1912" target="_blank">00:31:52.800</a></span> | <span class="t">and this rule-based system is you can do it for all the tasks that you can kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1917" target="_blank">00:31:57.600</a></span> | <span class="t">of verify so for example for the lead code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1920" target="_blank">00:32:00.960</a></span> | <span class="t">problems they ask the how can you check if the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1925" target="_blank">00:32:05.200</a></span> | <span class="t">generated by the model is good well you just run it and if it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1928" target="_blank">00:32:08.400</a></span> | <span class="t">performs if it first compiles and secondly it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1932" target="_blank">00:32:12.000</a></span> | <span class="t">runs in a predefined time limit then it is a good answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1936" target="_blank">00:32:16.000</a></span> | <span class="t">it doesn't matter how it came to be if it's if it works it's a good answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1940" target="_blank">00:32:20.480</a></span> | <span class="t">just like also for example math some for most of math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1944" target="_blank">00:32:24.000</a></span> | <span class="t">math problems we do have the answer that we expect the model to generate so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1948" target="_blank">00:32:28.160</a></span> | <span class="t">can compare what is the actual generated answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1950" target="_blank">00:32:30.800</a></span> | <span class="t">and what is the we expect the model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1955" target="_blank">00:32:35.920</a></span> | <span class="t">so they create a rule-based reward system in a way that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1960" target="_blank">00:32:40.480</a></span> | <span class="t">ask the language model to generate some output for a given problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1964" target="_blank">00:32:44.160</a></span> | <span class="t">and then they can assign by just following rules so check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1967" target="_blank">00:32:47.600</a></span> | <span class="t">if okay if it's a lead code problem just run it and check if it runs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1971" target="_blank">00:32:51.280</a></span> | <span class="t">okay good reward doesn't run okay zero and if it's a math problem they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1976" target="_blank">00:32:56.480</a></span> | <span class="t">take the the output of the model compare it with the expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1979" target="_blank">00:32:59.520</a></span> | <span class="t">result and assign reward based on that so if it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1982" target="_blank">00:33:02.800</a></span> | <span class="t">the answer matches what we expected good reward otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1986" target="_blank">00:33:06.400</a></span> | <span class="t">zero etc etc they also assign a reward for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1991" target="_blank">00:33:11.200</a></span> | <span class="t">for the language model if it formats the output in a certain way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=1997" target="_blank">00:33:17.120</a></span> | <span class="t">by for example forcing the they give a reward for the language model if it uses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2001" target="_blank">00:33:21.840</a></span> | <span class="t">for example if it follows the format of putting all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2006" target="_blank">00:33:26.320</a></span> | <span class="t">the thought process in the tags think and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2010" target="_blank">00:33:30.160</a></span> | <span class="t">slash think etc so basically just with this they train the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2017" target="_blank">00:33:37.040</a></span> | <span class="t">language model so they train the language model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2020" target="_blank">00:33:40.240</a></span> | <span class="t">generate answers and they reward these answers to a reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2026" target="_blank">00:33:46.960</a></span> | <span class="t">system which is rule-based and they keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2030" target="_blank">00:33:50.960</a></span> | <span class="t">training it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2033" target="_blank">00:33:53.600</a></span> | <span class="t">and the language models basically automatically by itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2037" target="_blank">00:33:57.760</a></span> | <span class="t">learns to generate the thought process that is necessary to perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2043" target="_blank">00:34:03.200</a></span> | <span class="t">the the tasks that it is being asked so the language model learns by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2049" target="_blank">00:34:09.520</a></span> | <span class="t">itself by just with the reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2052" target="_blank">00:34:12.800</a></span> | <span class="t">learning but with this reward system to generate the thought process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2057" target="_blank">00:34:17.920</a></span> | <span class="t">that leads to the generating the right code for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2061" target="_blank">00:34:21.920</a></span> | <span class="t">lead code problems to generate the right thought process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2065" target="_blank">00:34:25.920</a></span> | <span class="t">that is necessary for solving math problems etc etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2071" target="_blank">00:34:31.440</a></span> | <span class="t">let me check what else we need to know from here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2076" target="_blank">00:34:36.400</a></span> | <span class="t">okay here they show some results i think the results you can check by yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2081" target="_blank">00:34:41.600</a></span> | <span class="t">what is very interesting i think it's this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2085" target="_blank">00:34:45.040</a></span> | <span class="t">during the training of r1 just with reinforcement learning so as i as i want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2090" target="_blank">00:34:50.480</a></span> | <span class="t">to remind you is they took deep seek v3 base</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2094" target="_blank">00:34:54.800</a></span> | <span class="t">and added this reinforcement learning step on top of it which is a massive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2100" target="_blank">00:35:00.560</a></span> | <span class="t">reinforcement learning step usually the alignment part is not so big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2105" target="_blank">00:35:05.120</a></span> | <span class="t">and the more they find you they they run this reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2109" target="_blank">00:35:09.680</a></span> | <span class="t">step they saw that the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2114" target="_blank">00:35:14.080</a></span> | <span class="t">models automatically learns to generate longer responses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2119" target="_blank">00:35:19.520</a></span> | <span class="t">because to solve problems you need to generate a longer chain of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2123" target="_blank">00:35:23.200</a></span> | <span class="t">thought so the language model because of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2125" target="_blank">00:35:25.680</a></span> | <span class="t">reward system learns that in order to get reward it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2129" target="_blank">00:35:29.600</a></span> | <span class="t">it should generate longer responses so they didn't tell the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2135" target="_blank">00:35:35.040</a></span> | <span class="t">with supervised fine tuning to generate that kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2138" target="_blank">00:35:38.240</a></span> | <span class="t">data with that particular format with that kind of thought process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2142" target="_blank">00:35:42.720</a></span> | <span class="t">just with reinforcement learning with the right incentives the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2146" target="_blank">00:35:46.320</a></span> | <span class="t">learned to do that how did it do that it learned that at a particular input it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2151" target="_blank">00:35:51.600</a></span> | <span class="t">should generate that particular token which in the long term results in a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2156" target="_blank">00:35:56.320</a></span> | <span class="t">reward so the language model the beauty of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2158" target="_blank">00:35:58.960</a></span> | <span class="t">reinforcement learning is that you not only learn to do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2163" target="_blank">00:36:03.360</a></span> | <span class="t">that based on the immediate reward that you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2166" target="_blank">00:36:06.640</a></span> | <span class="t">but also on the long-term reward that you will get because sometimes the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2171" target="_blank">00:36:11.280</a></span> | <span class="t">as you can see here the reward model only applies when the entire answer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2177" target="_blank">00:36:17.680</a></span> | <span class="t">produced so the model will only know the signal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2181" target="_blank">00:36:21.360</a></span> | <span class="t">that the model gets is only for the entire output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2186" target="_blank">00:36:26.240</a></span> | <span class="t">actually okay through the advantage term this signal is propagated back to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2190" target="_blank">00:36:30.000</a></span> | <span class="t">single token but okay we can skip that in the in the reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2194" target="_blank">00:36:34.160</a></span> | <span class="t">actually the beauty is that sometimes the reward you get is not for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2197" target="_blank">00:36:37.840</a></span> | <span class="t">the single action you take so in the case of my cat for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2201" target="_blank">00:36:41.200</a></span> | <span class="t">let's say here so let's go back to my cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2205" target="_blank">00:36:45.280</a></span> | <span class="t">the cat will receive reward only after taking many steps that lead it to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2210" target="_blank">00:36:50.080</a></span> | <span class="t">meat so only when the cat is here it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2212" target="_blank">00:36:52.960</a></span> | <span class="t">know okay all this sequence of actions was a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2216" target="_blank">00:36:56.480</a></span> | <span class="t">choice but this signal is propagated back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2220" target="_blank">00:37:00.880</a></span> | <span class="t">each single action in such a way that the cat when it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2224" target="_blank">00:37:04.480</a></span> | <span class="t">be here it will very likely to choose i need to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2228" target="_blank">00:37:08.080</a></span> | <span class="t">down and less likely to choose i need to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2231" target="_blank">00:37:11.040</a></span> | <span class="t">right this also happens in the case of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2234" target="_blank">00:37:14.080</a></span> | <span class="t">models in this case through the advantage term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2238" target="_blank">00:37:18.160</a></span> | <span class="t">where is it through this advantage term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2242" target="_blank">00:37:22.080</a></span> | <span class="t">which is actually done for each token in the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2245" target="_blank">00:37:25.120</a></span> | <span class="t">okay now we can go a little bit more technical details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2249" target="_blank">00:37:29.520</a></span> | <span class="t">in the case of why they choose grpo over ppo first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2253" target="_blank">00:37:33.840</a></span> | <span class="t">well with ppo basically this advantage term here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2257" target="_blank">00:37:37.600</a></span> | <span class="t">to compute requires another function that is called the value function and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2262" target="_blank">00:37:42.320</a></span> | <span class="t">this value function basically to be computed requires the training of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2265" target="_blank">00:37:45.600</a></span> | <span class="t">another model by using the advantage term in grpo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2269" target="_blank">00:37:49.920</a></span> | <span class="t">this advantage term is calculated without the value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2273" target="_blank">00:37:53.680</a></span> | <span class="t">function but by the following formula here which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2276" target="_blank">00:37:56.240</a></span> | <span class="t">basically just based on the rewards which is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2279" target="_blank">00:37:59.680</a></span> | <span class="t">given by the reward model which we have which in the case of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2282" target="_blank">00:38:02.720</a></span> | <span class="t">deepseek r1 is rule-based so they don't need to train this other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2287" target="_blank">00:38:07.760</a></span> | <span class="t">uh model to generate the the value function which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2291" target="_blank">00:38:11.280</a></span> | <span class="t">adds more complexity to the to the system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2295" target="_blank">00:38:15.680</a></span> | <span class="t">okay so now we have seen what is the reinforcement learning we have seen what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2299" target="_blank">00:38:19.520</a></span> | <span class="t">is the connection between language models and reinforcement learning we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2301" target="_blank">00:38:21.760</a></span> | <span class="t">have seen a little bit what is the the grpo objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2309" target="_blank">00:38:29.280</a></span> | <span class="t">okay i believe let's actually do a poll guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2313" target="_blank">00:38:33.120</a></span> | <span class="t">do you want me to go deeper in the grpo like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2317" target="_blank">00:38:37.120</a></span> | <span class="t">let's explore exactly this loss because actually the the rest of the paper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2323" target="_blank">00:38:43.920</a></span> | <span class="t">okay we tried just reinforcement learning okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2327" target="_blank">00:38:47.200</a></span> | <span class="t">so people like deep so let's go deep okay all right first of all the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2332" target="_blank">00:38:52.240</a></span> | <span class="t">interesting thing about reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2335" target="_blank">00:38:55.760</a></span> | <span class="t">especially in the case of ppo and the first learn from human feedback is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2340" target="_blank">00:39:00.480</a></span> | <span class="t">is this thing called the gradient policy optimization so let's go back to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2343" target="_blank">00:39:03.360</a></span> | <span class="t">other slide and then we go back to the dpo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2345" target="_blank">00:39:05.920</a></span> | <span class="t">because the rest of the paper in dpo deepseek is basically they they took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2349" target="_blank">00:39:09.680</a></span> | <span class="t">this um r1 and then said okay instead of just doing reinforcement learning let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2353" target="_blank">00:39:13.920</a></span> | <span class="t">do maybe multiple step of reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2356" target="_blank">00:39:16.240</a></span> | <span class="t">learning and supervised fine tuning and then reinforcement learning again then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2359" target="_blank">00:39:19.200</a></span> | <span class="t">super and it leads to better outcomes but that is not technically difficult to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2363" target="_blank">00:39:23.360</a></span> | <span class="t">understand i think because most of you already kind of have backgrounds in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2367" target="_blank">00:39:27.680</a></span> | <span class="t">so if you're here it's because you kind of understand what we are talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2371" target="_blank">00:39:31.840</a></span> | <span class="t">so let's do all the things that maybe some people have difficulties with which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2375" target="_blank">00:39:35.200</a></span> | <span class="t">is i believe this and this part okay so let's go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2377" target="_blank">00:39:37.840</a></span> | <span class="t">deeper so um okay so let's go back to my cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2382" target="_blank">00:39:42.560</a></span> | <span class="t">and imagine i want to train my cat so i want to train my cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2386" target="_blank">00:39:46.080</a></span> | <span class="t">to um follow to to reach the meat so as we saw before my cat is just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2392" target="_blank">00:39:52.320</a></span> | <span class="t">an agent with a policy a policy is what it tells the cat what action to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2396" target="_blank">00:39:56.240</a></span> | <span class="t">given the position of the cat inside of the house</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2398" target="_blank">00:39:58.800</a></span> | <span class="t">you need to think of this house as a grid so like like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2401" target="_blank">00:40:01.920</a></span> | <span class="t">the following i didn't uh you cannot see the the grid lines because i don't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2405" target="_blank">00:40:05.840</a></span> | <span class="t">my it's not showing but okay it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2408" target="_blank">00:40:08.240</a></span> | <span class="t">matter what is the policy the policy as we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2412" target="_blank">00:40:12.080</a></span> | <span class="t">before is it tells the cat what action to take given a particular position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2415" target="_blank">00:40:15.280</a></span> | <span class="t">what is our goal in reinforcement learning is to select a policy that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2418" target="_blank">00:40:18.800</a></span> | <span class="t">maximizes the reward that the agent gets when using this policy so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2425" target="_blank">00:40:25.040</a></span> | <span class="t">this is the um the objective that if you have we want to select among</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2429" target="_blank">00:40:29.040</a></span> | <span class="t">all the possible policies that we can can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2432" target="_blank">00:40:32.560</a></span> | <span class="t">the one that maximizes an objective what is this objective is the expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2438" target="_blank">00:40:38.320</a></span> | <span class="t">reward that we can get when using this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2441" target="_blank">00:40:41.040</a></span> | <span class="t">means that if i apply in the side or in the brain of my cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2444" target="_blank">00:40:44.080</a></span> | <span class="t">this policy which is the decision making stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2448" target="_blank">00:40:48.080</a></span> | <span class="t">then my cat if this best policy will tell my cat to move down here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2452" target="_blank">00:40:52.960</a></span> | <span class="t">and move right here and move right right down down down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2457" target="_blank">00:40:57.040</a></span> | <span class="t">and until it arrives to the needle this should be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2460" target="_blank">00:41:00.800</a></span> | <span class="t">um the policy how do we actually train this policy we do what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2469" target="_blank">00:41:09.360</a></span> | <span class="t">known as a policy gradient optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2474" target="_blank">00:41:14.160</a></span> | <span class="t">but before we understand policy we need to understand a few terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2477" target="_blank">00:41:17.920</a></span> | <span class="t">so uh we want to um we want to first of all learn what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2483" target="_blank">00:41:23.520</a></span> | <span class="t">trajectories the trajectory is basically a list of state and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2486" target="_blank">00:41:26.800</a></span> | <span class="t">actions so um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2490" target="_blank">00:41:30.560</a></span> | <span class="t">okay yeah okay the trajectories are a list of states of action so if the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2495" target="_blank">00:41:35.520</a></span> | <span class="t">is here it's in the state uh let me draw the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2497" target="_blank">00:41:37.920</a></span> | <span class="t">lines otherwise it's too bad for you guys if you cannot see it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2501" target="_blank">00:41:41.920</a></span> | <span class="t">here here here here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2505" target="_blank">00:41:45.840</a></span> | <span class="t">okay this is the the cat is initially here so this let's call it the state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2511" target="_blank">00:41:51.040</a></span> | <span class="t">number zero and the cat can choose some action and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2513" target="_blank">00:41:53.440</a></span> | <span class="t">let's call it the action number zero when the cat takes the action number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2516" target="_blank">00:41:56.960</a></span> | <span class="t">zero it will arrive in a new state so maybe the cat will arrive here and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2520" target="_blank">00:42:00.560</a></span> | <span class="t">will become the state number one of the cat and then the cat here we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2523" target="_blank">00:42:03.680</a></span> | <span class="t">another action according to its policy let's call it action number one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2527" target="_blank">00:42:07.520</a></span> | <span class="t">which will lead the cat into a new state so we'll call it s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2530" target="_blank">00:42:10.480</a></span> | <span class="t">s2 which in which it will in which it can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2535" target="_blank">00:42:15.040</a></span> | <span class="t">another action and let's call it the action number two etc etc so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2538" target="_blank">00:42:18.240</a></span> | <span class="t">trajectory is a list of states and action that the agent can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2541" target="_blank">00:42:21.600</a></span> | <span class="t">inside of the environment uh what we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2545" target="_blank">00:42:25.040</a></span> | <span class="t">is if we take sample a trajectory according to our policy we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2549" target="_blank">00:42:29.840</a></span> | <span class="t">maximize the reward that we get from each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2552" target="_blank">00:42:32.960</a></span> | <span class="t">possible trajectories that we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2556" target="_blank">00:42:36.800</a></span> | <span class="t">how do we do that let's do it here so basically what we do is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2563" target="_blank">00:42:43.680</a></span> | <span class="t">following we this is our objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2568" target="_blank">00:42:48.240</a></span> | <span class="t">so if we can find as you know when we have in deep learning what we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2573" target="_blank">00:42:53.520</a></span> | <span class="t">we are trying to either maximize something or we are going to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2577" target="_blank">00:42:57.280</a></span> | <span class="t">something in the case of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2580" target="_blank">00:43:00.960</a></span> | <span class="t">model training we usually always minimize a cost function in this case we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2586" target="_blank">00:43:06.000</a></span> | <span class="t">want to maximize the expected rewards when the agent acts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2590" target="_blank">00:43:10.480</a></span> | <span class="t">according to this policy this policy however is not just any policy it is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2595" target="_blank">00:43:15.040</a></span> | <span class="t">particular policy made up of some parameters that we will call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2598" target="_blank">00:43:18.160</a></span> | <span class="t">theta to give you a parallel on what is happening here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2603" target="_blank">00:43:23.120</a></span> | <span class="t">is the following imagine you have a company</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2606" target="_blank">00:43:26.800</a></span> | <span class="t">and you are like the ceo of the company and this company is made up of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2614" target="_blank">00:43:34.720</a></span> | <span class="t">many actors and many functions and many departments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2618" target="_blank">00:43:38.480</a></span> | <span class="t">so each of these things are let's say they are parameters of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2623" target="_blank">00:43:43.440</a></span> | <span class="t">your company they define your pump because you can tune them and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2626" target="_blank">00:43:46.800</a></span> | <span class="t">company function will change so how people talk to each other how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2631" target="_blank">00:43:51.200</a></span> | <span class="t">people work how the departments work how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2634" target="_blank">00:43:54.560</a></span> | <span class="t">logistics work how the office works etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2637" target="_blank">00:43:57.040</a></span> | <span class="t">they are all parameters of your company which define the outcome of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2640" target="_blank">00:44:00.640</a></span> | <span class="t">company and imagine you want to maximize the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2644" target="_blank">00:44:04.320</a></span> | <span class="t">profit of your company so what you do you learn to tune all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2648" target="_blank">00:44:08.160</a></span> | <span class="t">these parameters so you learn to for example tell people to behave in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2651" target="_blank">00:44:11.760</a></span> | <span class="t">particular way or you tell the people to collaborate in a particular way or to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2655" target="_blank">00:44:15.760</a></span> | <span class="t">work on some projects and not work on some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2657" target="_blank">00:44:17.760</a></span> | <span class="t">other projects this is what we do in the gradient policy optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2661" target="_blank">00:44:21.120</a></span> | <span class="t">policy gradient optimization we calculate the gradient with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2666" target="_blank">00:44:26.480</a></span> | <span class="t">to the parameters of this particular objective function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2671" target="_blank">00:44:31.280</a></span> | <span class="t">which what is the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2675" target="_blank">00:44:35.120</a></span> | <span class="t">yes later we talk about the discount factor in the word sum okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2678" target="_blank">00:44:38.560</a></span> | <span class="t">so what is the gradient the gradient basically tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2684" target="_blank">00:44:44.400</a></span> | <span class="t">how the how the objective will change if we change the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2691" target="_blank">00:44:51.040</a></span> | <span class="t">parameter a little bit the gradient always tell us how it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2697" target="_blank">00:44:57.040</a></span> | <span class="t">increase so the gradient tells us what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2700" target="_blank">00:45:00.080</a></span> | <span class="t">the direction of max the maximum ascent of a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2703" target="_blank">00:45:03.680</a></span> | <span class="t">function with respect to the variable in to which you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2706" target="_blank">00:45:06.960</a></span> | <span class="t">calculate it so in this case we are computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2709" target="_blank">00:45:09.440</a></span> | <span class="t">gradient with respect to the parameters of the objective function which tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2717" target="_blank">00:45:17.280</a></span> | <span class="t">how should i change the parameters to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2720" target="_blank">00:45:20.480</a></span> | <span class="t">this objective function which is exactly the expected reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2724" target="_blank">00:45:24.480</a></span> | <span class="t">that we want we can get from this policy so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2728" target="_blank">00:45:28.640</a></span> | <span class="t">because the the the gradient tells us how we should change the parameters to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2734" target="_blank">00:45:34.240</a></span> | <span class="t">increase the objective then we change the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2737" target="_blank">00:45:37.600</a></span> | <span class="t">according to the direction of the gradient and this is what we do here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2741" target="_blank">00:45:41.600</a></span> | <span class="t">so we have an objective which is tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2744" target="_blank">00:45:44.960</a></span> | <span class="t">the expected reward when acting according to this policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2748" target="_blank">00:45:48.480</a></span> | <span class="t">we calculate its gradient with respect to the parameters which tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2752" target="_blank">00:45:52.320</a></span> | <span class="t">how we should change these parameters to increase this objective so to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2756" target="_blank">00:45:56.640</a></span> | <span class="t">the expected reward and then we change the parameters in the same direction of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2761" target="_blank">00:46:01.120</a></span> | <span class="t">the gradient and we do it iteratively this is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2765" target="_blank">00:46:05.680</a></span> | <span class="t">policy gradient optimization and actually this is a beautiful result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2769" target="_blank">00:46:09.280</a></span> | <span class="t">because it means that i can just use my cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2773" target="_blank">00:46:13.600</a></span> | <span class="t">whatever policies my cat has right now i can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2778" target="_blank">00:46:18.560</a></span> | <span class="t">sample some trajectories from these policies when i ask my cat to move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2782" target="_blank">00:46:22.080</a></span> | <span class="t">around check what kind of reward i get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2785" target="_blank">00:46:25.840</a></span> | <span class="t">calculate the gradient with respect to the expected reward according to these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2790" target="_blank">00:46:30.240</a></span> | <span class="t">trajectories and then tell the cat hey you should do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2793" target="_blank">00:46:33.920</a></span> | <span class="t">more of this because this led to a better reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2797" target="_blank">00:46:37.120</a></span> | <span class="t">or you should do less of this because it leads to a bad reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2801" target="_blank">00:46:41.680</a></span> | <span class="t">this is policy gradient optimization now it has some problems because policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2807" target="_blank">00:46:47.440</a></span> | <span class="t">gradient optimization basically okay as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2810" target="_blank">00:46:50.400</a></span> | <span class="t">let's skip the math because if you want the math i i made a video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2813" target="_blank">00:46:53.760</a></span> | <span class="t">it's on youtube so you can watch it tomorrow but it has some problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2817" target="_blank">00:46:57.840</a></span> | <span class="t">because as you can see the search space of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2821" target="_blank">00:47:01.840</a></span> | <span class="t">cat is enormous because at the possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2825" target="_blank">00:47:05.280</a></span> | <span class="t">trajectory that the cat can take to go from here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2828" target="_blank">00:47:08.080</a></span> | <span class="t">to the meat there are a lot because the cat can go like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2831" target="_blank">00:47:11.840</a></span> | <span class="t">it can go like this it can go like this it can go here then come back then go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2837" target="_blank">00:47:17.200</a></span> | <span class="t">down etc etc so there is many many many many many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2841" target="_blank">00:47:21.120</a></span> | <span class="t">trajectories that the cat can take to go to the meat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2846" target="_blank">00:47:26.400</a></span> | <span class="t">however to compute this objective we should actually check all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2852" target="_blank">00:47:32.320</a></span> | <span class="t">possible trajectories to get the direction of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2857" target="_blank">00:47:37.760</a></span> | <span class="t">however this is intractable means that in the case of language model we should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2862" target="_blank">00:47:42.000</a></span> | <span class="t">ask the language model to generate all the possible output ever possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2866" target="_blank">00:47:46.320</a></span> | <span class="t">given a particular question which is intractable because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2869" target="_blank">00:47:49.520</a></span> | <span class="t">at each token the the language model can choose what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2874" target="_blank">00:47:54.000</a></span> | <span class="t">let's say the vocabulary size is 30 000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2877" target="_blank">00:47:57.040</a></span> | <span class="t">then the language model can choose 30 000 possibilities for the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2880" target="_blank">00:48:00.560</a></span> | <span class="t">token then 30 000 for the second 30 000 for the third etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2885" target="_blank">00:48:05.200</a></span> | <span class="t">and to check all of them it's computationally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2888" target="_blank">00:48:08.400</a></span> | <span class="t">impossible so we can always approximate this with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2892" target="_blank">00:48:12.160</a></span> | <span class="t">a sample with a sample this is called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2898" target="_blank">00:48:18.880</a></span> | <span class="t">monte carlo estimation however this results in because we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2903" target="_blank">00:48:23.840</a></span> | <span class="t">not checking all the possible trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2905" target="_blank">00:48:25.680</a></span> | <span class="t">but we are making the decision of optimizing our cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2908" target="_blank">00:48:28.880</a></span> | <span class="t">using only a few trajectories of course as you can see it's a risky situation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2913" target="_blank">00:48:33.440</a></span> | <span class="t">so it means that we are making a hard decision on how we should change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2917" target="_blank">00:48:37.520</a></span> | <span class="t">a policy without checking all the possible search space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2921" target="_blank">00:48:41.920</a></span> | <span class="t">this basically means that we have high variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2925" target="_blank">00:48:45.200</a></span> | <span class="t">and there are many ways to reduce this variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2928" target="_blank">00:48:48.880</a></span> | <span class="t">so when you read the term baseline in the deep seek paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2933" target="_blank">00:48:53.040</a></span> | <span class="t">this is one of the ways to reduce this variance because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2936" target="_blank">00:48:56.160</a></span> | <span class="t">are trying to optimize the language model into choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2939" target="_blank">00:48:59.920</a></span> | <span class="t">certain patterns into choosing certain chain of thoughts into choosing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2944" target="_blank">00:49:04.000</a></span> | <span class="t">certain sequence of tokens without exploring all the possible generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2948" target="_blank">00:49:08.160</a></span> | <span class="t">that the language model can have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2951" target="_blank">00:49:11.280</a></span> | <span class="t">um so so let me see how can we simplify this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2958" target="_blank">00:49:18.720</a></span> | <span class="t">we blah blah okay so in order to reduce this variance so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2965" target="_blank">00:49:25.760</a></span> | <span class="t">in order to make sure that we optimize the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2969" target="_blank">00:49:29.600</a></span> | <span class="t">model even without checking all the possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2973" target="_blank">00:49:33.040</a></span> | <span class="t">generations but still making sure that we make the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2977" target="_blank">00:49:37.040</a></span> | <span class="t">gradient that we get so the direction that tells us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2979" target="_blank">00:49:39.680</a></span> | <span class="t">what parameter we should change and in which direction in order to increase the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2983" target="_blank">00:49:43.920</a></span> | <span class="t">expected reward we can introduce this advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2987" target="_blank">00:49:47.680</a></span> | <span class="t">here this advantage term basically for each token tells the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2992" target="_blank">00:49:52.480</a></span> | <span class="t">how better is choosing this token over all the other tokens that i can choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=2997" target="_blank">00:49:57.520</a></span> | <span class="t">in this position for example in the example in the example that we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3001" target="_blank">00:50:01.680</a></span> | <span class="t">before so where is shanghai is should the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3006" target="_blank">00:50:06.000</a></span> | <span class="t">model choose the word shanghai or it should choose the word coffee or should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3009" target="_blank">00:50:09.680</a></span> | <span class="t">it choose the word pizza well it's very more it's much more advantageous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3014" target="_blank">00:50:14.000</a></span> | <span class="t">to to to choose the word shanghai because it's very likely that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3018" target="_blank">00:50:18.160</a></span> | <span class="t">language model will then complete it as shanghai is in china or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3021" target="_blank">00:50:21.440</a></span> | <span class="t">shanghai is a city in china or shanghai is located in china etc etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3025" target="_blank">00:50:25.840</a></span> | <span class="t">so the choosing the language the the word shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3030" target="_blank">00:50:30.400</a></span> | <span class="t">results on the long term in a much better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3034" target="_blank">00:50:34.640</a></span> | <span class="t">reward so the advantage of choosing shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3038" target="_blank">00:50:38.160</a></span> | <span class="t">is higher compared to all the other tokens in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3041" target="_blank">00:50:41.280</a></span> | <span class="t">condition in the case of the cat it means that the cat when is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3046" target="_blank">00:50:46.640</a></span> | <span class="t">here it should very high it's very advantageous to choose go down because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3051" target="_blank">00:50:51.840</a></span> | <span class="t">it will result in going to the meat and over all the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3057" target="_blank">00:50:57.040</a></span> | <span class="t">all the over all the other actions this doesn't mean that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3060" target="_blank">00:51:00.080</a></span> | <span class="t">choosing up will lead you to die or to get no reward because you can always go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3064" target="_blank">00:51:04.240</a></span> | <span class="t">up and then change direction and go down but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3067" target="_blank">00:51:07.200</a></span> | <span class="t">much more advantageous to just go down this is the this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3073" target="_blank">00:51:13.440</a></span> | <span class="t">uh this is the meaning of the advantage term</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3076" target="_blank">00:51:16.480</a></span> | <span class="t">and this is the same advantage term that you see in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3080" target="_blank">00:51:20.880</a></span> | <span class="t">grpo loss now what is the difference between the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3084" target="_blank">00:51:24.800</a></span> | <span class="t">advantage term that you see in the ppo and the grpo is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3088" target="_blank">00:51:28.240</a></span> | <span class="t">the advantage term in the ppo requires the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3091" target="_blank">00:51:31.600</a></span> | <span class="t">what is known as the value function in the grpo they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3096" target="_blank">00:51:36.560</a></span> | <span class="t">they they compute this advantage in a different way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3100" target="_blank">00:51:40.560</a></span> | <span class="t">which still results in in variance reduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3105" target="_blank">00:51:45.120</a></span> | <span class="t">but without having this value function estimation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3108" target="_blank">00:51:48.240</a></span> | <span class="t">so grpo is a computationally more um advanced i would say efficient in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3114" target="_blank">00:51:54.480</a></span> | <span class="t">case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3116" target="_blank">00:51:56.800</a></span> | <span class="t">okay let me see what else we need we've skipped also the part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3121" target="_blank">00:52:01.360</a></span> | <span class="t">let me see um off policy learning right so offline policy learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3128" target="_blank">00:52:08.320</a></span> | <span class="t">so what is off policy learning uh imagine we have the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3133" target="_blank">00:52:13.760</a></span> | <span class="t">let's go to the cat actually okay to compute this the the the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3141" target="_blank">00:52:21.760</a></span> | <span class="t">to to okay the the gradient policy optimization we saw that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3145" target="_blank">00:52:25.840</a></span> | <span class="t">sample some trajectories right and we don't have to sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3148" target="_blank">00:52:28.480</a></span> | <span class="t">all the possible trajectories right because we are trying to approximate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3153" target="_blank">00:52:33.280</a></span> | <span class="t">um so when we sample these trajectories we are sampling from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3157" target="_blank">00:52:37.440</a></span> | <span class="t">a policy which is the current brain of the cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3160" target="_blank">00:52:40.800</a></span> | <span class="t">so we ask the current brain of the cat to choose some actions and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3164" target="_blank">00:52:44.880</a></span> | <span class="t">generate some trajectories means that we ask the language the cat to just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3168" target="_blank">00:52:48.160</a></span> | <span class="t">navigate the house and let's see what it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3172" target="_blank">00:52:52.720</a></span> | <span class="t">okay so we ask the the cat to just navigate the house and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3179" target="_blank">00:52:59.920</a></span> | <span class="t">see what it does and then after the cat has navigated the house</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3184" target="_blank">00:53:04.160</a></span> | <span class="t">we look at what they are the trajectory that the cat has taken and then we give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3188" target="_blank">00:53:08.400</a></span> | <span class="t">reward to the cat based on the trajectory it has taken</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3192" target="_blank">00:53:12.160</a></span> | <span class="t">and then we optimize the policy which means that we optimize the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3195" target="_blank">00:53:15.600</a></span> | <span class="t">brain of the cat uh with the whatever it has learned with the with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3202" target="_blank">00:53:22.000</a></span> | <span class="t">the direction of the gradient based on the reward it has received</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3205" target="_blank">00:53:25.200</a></span> | <span class="t">but now the brain of the cat is a new brain because it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3209" target="_blank">00:53:29.120</a></span> | <span class="t">changed compared to the past which means that the next step of iteration of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3214" target="_blank">00:53:34.000</a></span> | <span class="t">optimizing optimizing the brain of the cat or its decision making skills</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3218" target="_blank">00:53:38.320</a></span> | <span class="t">we need to sample new trajectories so we need to ask the cat again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3222" target="_blank">00:53:42.160</a></span> | <span class="t">to go through all the house make a few choices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3225" target="_blank">00:53:45.520</a></span> | <span class="t">and then we check these choices and again we tell the cat hey you did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3229" target="_blank">00:53:49.120</a></span> | <span class="t">here you did well here you didn't well so the cat will learn to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3232" target="_blank">00:53:52.880</a></span> | <span class="t">to optimize its policy which will result in a new policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3239" target="_blank">00:53:59.360</a></span> | <span class="t">and then again we need to sample from this policy but as you can see every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3242" target="_blank">00:54:02.400</a></span> | <span class="t">time we do an optimization step we need to sample again these trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3246" target="_blank">00:54:06.240</a></span> | <span class="t">and in the case of language models this means that first you need to sample some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3250" target="_blank">00:54:10.720</a></span> | <span class="t">responses then you reward these responses based on your reward model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3256" target="_blank">00:54:16.240</a></span> | <span class="t">and then you need to sample new responses because now the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3261" target="_blank">00:54:21.360</a></span> | <span class="t">has changed because we updated the language model in order to avoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3266" target="_blank">00:54:26.560</a></span> | <span class="t">this sampling process which is expensive we introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3270" target="_blank">00:54:30.560</a></span> | <span class="t">of policy learning in which let me show you here where is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3276" target="_blank">00:54:36.880</a></span> | <span class="t">okay in which basically we take the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3281" target="_blank">00:54:41.200</a></span> | <span class="t">we ask it to generate a lot of trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3284" target="_blank">00:54:44.480</a></span> | <span class="t">and we do it once then we sample some of these trajectories which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3288" target="_blank">00:54:48.720</a></span> | <span class="t">basically we ask it to generate some responses then we sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3291" target="_blank">00:54:51.760</a></span> | <span class="t">some of these responses and we fine-tune the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3295" target="_blank">00:54:55.040</a></span> | <span class="t">based on the reward we got on these responses then we don't sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3298" target="_blank">00:54:58.800</a></span> | <span class="t">new we don't sample new trajectories or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3301" target="_blank">00:55:01.680</a></span> | <span class="t">responses we just take another mini batch of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3304" target="_blank">00:55:04.720</a></span> | <span class="t">trajectories that we sampled initially and again we do another step of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3308" target="_blank">00:55:08.320</a></span> | <span class="t">optimization and we keep doing it for n steps only then we sample new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3314" target="_blank">00:55:14.080</a></span> | <span class="t">trajectories this results in a much more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3318" target="_blank">00:55:18.320</a></span> | <span class="t">training so it's not like we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3322" target="_blank">00:55:22.960</a></span> | <span class="t">let's say do change a little bit the policy and then sample new trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3326" target="_blank">00:55:26.960</a></span> | <span class="t">from this policy no we sample a lot of trajectories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3329" target="_blank">00:55:29.600</a></span> | <span class="t">initially we keep them in some database in the memory or whatever you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3334" target="_blank">00:55:34.080</a></span> | <span class="t">then we do some continuously optimizing the policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3337" target="_blank">00:55:37.200</a></span> | <span class="t">using the trajectories that we have sampled initially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3341" target="_blank">00:55:41.040</a></span> | <span class="t">and this basically results in a much efficient training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3347" target="_blank">00:55:47.920</a></span> | <span class="t">and now we can go back to the DPO sorry the DeepSeq R1 paper here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3354" target="_blank">00:55:54.640</a></span> | <span class="t">to understand finally the loss in its entirety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3358" target="_blank">00:55:58.480</a></span> | <span class="t">so what we are doing here is we have a policy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3362" target="_blank">00:56:02.560</a></span> | <span class="t">at the current step of sampling and then we have the policy from which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3367" target="_blank">00:56:07.680</a></span> | <span class="t">we sampled the trajectories so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3371" target="_blank">00:56:11.840</a></span> | <span class="t">as you can see it's written here so we sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3374" target="_blank">00:56:14.960</a></span> | <span class="t">first a question from our database of questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3378" target="_blank">00:56:18.160</a></span> | <span class="t">then for each question we generate a list of outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3381" target="_blank">00:56:21.680</a></span> | <span class="t">of responses so we prompt the model basically with the question and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3386" target="_blank">00:56:26.320</a></span> | <span class="t">ask it to generate multiple responses and you can generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3389" target="_blank">00:56:29.200</a></span> | <span class="t">multiple responses like we saw here like we ask the language model where is Shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3392" target="_blank">00:56:32.240</a></span> | <span class="t">and the language model will generate one response then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3394" target="_blank">00:56:34.560</a></span> | <span class="t">we ask it again where is Shanghai and maybe this time the language model will say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3397" target="_blank">00:56:37.440</a></span> | <span class="t">something else and then we ask it again etc etc so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3400" target="_blank">00:56:40.000</a></span> | <span class="t">generate a list of outputs for the same question then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3404" target="_blank">00:56:44.480</a></span> | <span class="t">compute the following loss which is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3407" target="_blank">00:56:47.440</a></span> | <span class="t">the ratio of the log probability the ratio of the probabilities or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3412" target="_blank">00:56:52.240</a></span> | <span class="t">difference of the log probabilities so the ratio of the probabilities of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3417" target="_blank">00:56:57.360</a></span> | <span class="t">the assigned to the output by the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3422" target="_blank">00:57:02.400</a></span> | <span class="t">iteration so the current iteration at which we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3427" target="_blank">00:57:07.120</a></span> | <span class="t">optimizing the language model with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3431" target="_blank">00:57:11.120</a></span> | <span class="t">the language model from which we sampled so the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3434" target="_blank">00:57:14.400</a></span> | <span class="t">basically when we initially sample the trajectories we already pre-compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3437" target="_blank">00:57:17.920</a></span> | <span class="t">probabilities so we we can compute the probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3442" target="_blank">00:57:22.080</a></span> | <span class="t">while while sampling we can save them so we have always available this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3446" target="_blank">00:57:26.640</a></span> | <span class="t">pi pi of old now this ratio means what means that if at the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3453" target="_blank">00:57:33.040</a></span> | <span class="t">iteration the language imagine this ratio is more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3455" target="_blank">00:57:35.360</a></span> | <span class="t">than one it means that at the current iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3457" target="_blank">00:57:37.920</a></span> | <span class="t">the language model is telling me that i want to choose this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3461" target="_blank">00:57:41.680</a></span> | <span class="t">output more because i am more likely to choose this one now what we want is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3468" target="_blank">00:57:48.240</a></span> | <span class="t">if the advantage of doing this choosing this action is good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3472" target="_blank">00:57:52.960</a></span> | <span class="t">because it leads advantage positive means that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3476" target="_blank">00:57:56.160</a></span> | <span class="t">good to choose it's advantageous to choose this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3479" target="_blank">00:57:59.120</a></span> | <span class="t">action because it results in a good reward so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3482" target="_blank">00:58:02.400</a></span> | <span class="t">if the language model is more likely to choose something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3485" target="_blank">00:58:05.840</a></span> | <span class="t">and at the same time this something also results in good advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3490" target="_blank">00:58:10.640</a></span> | <span class="t">then this stuff here will be positive and it will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3494" target="_blank">00:58:14.800</a></span> | <span class="t">big and because we are maximizing this will contribute positively to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3499" target="_blank">00:58:19.840</a></span> | <span class="t">objective because we are trying to maximize it so we want to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3502" target="_blank">00:58:22.720</a></span> | <span class="t">more of this however if the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3506" target="_blank">00:58:26.160</a></span> | <span class="t">on the other hand is trying to do less of something so means that this ratio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3510" target="_blank">00:58:30.960</a></span> | <span class="t">will be less than one and at the same time this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3514" target="_blank">00:58:34.640</a></span> | <span class="t">results in a something that is disadvantageous so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3519" target="_blank">00:58:39.200</a></span> | <span class="t">means that it's not advantageous to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3522" target="_blank">00:58:42.640</a></span> | <span class="t">then the language model is also um okay let me check what so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3531" target="_blank">00:58:51.120</a></span> | <span class="t">i got a little lost if i have if this ratio is a positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3535" target="_blank">00:58:55.680</a></span> | <span class="t">and i am i get a good advantage it means that the language model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3541" target="_blank">00:59:01.680</a></span> | <span class="t">be more likely to do it if something that results in bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3546" target="_blank">00:59:06.240</a></span> | <span class="t">advantage and the language model is still doing it then it will be a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3550" target="_blank">00:59:10.880</a></span> | <span class="t">negative reward which will contribute negatively to our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3554" target="_blank">00:59:14.960</a></span> | <span class="t">objective so the language model will be less likely to do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3558" target="_blank">00:59:18.640</a></span> | <span class="t">at the same time we don't want the language model to make big decisions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3562" target="_blank">00:59:22.560</a></span> | <span class="t">at every step we want to clip limit the decision making of the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3568" target="_blank">00:59:28.880</a></span> | <span class="t">model at each step so it means that at each step of it of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3572" target="_blank">00:59:32.480</a></span> | <span class="t">optimization even if the language model is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3575" target="_blank">00:59:35.200</a></span> | <span class="t">confident that something is good then we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3578" target="_blank">00:59:38.800</a></span> | <span class="t">care how confident language is we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3580" target="_blank">00:59:40.880</a></span> | <span class="t">limit its confidence by clipping this ratio here between one minus epsilon and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3586" target="_blank">00:59:46.880</a></span> | <span class="t">one plus epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3589" target="_blank">00:59:49.920</a></span> | <span class="t">at the same time we also don't want the language model to change too much so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3594" target="_blank">00:59:54.320</a></span> | <span class="t">have initial frozen model here which is a pyref</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3597" target="_blank">00:59:57.680</a></span> | <span class="t">pyref basically means the original model so in the case of r1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3602" target="_blank">01:00:02.320</a></span> | <span class="t">it is the deep seek v3 base so which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3607" target="_blank">01:00:07.760</a></span> | <span class="t">language model that has never been trained with reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3611" target="_blank">01:00:11.920</a></span> | <span class="t">so we want because we want reinforcement learning framework we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3615" target="_blank">01:00:15.920</a></span> | <span class="t">change the language model a little bit to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3619" target="_blank">01:00:19.120</a></span> | <span class="t">reasoning but we don't want the language model to forget</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3622" target="_blank">01:00:22.320</a></span> | <span class="t">everything or to just not behave like a language model anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3626" target="_blank">01:00:26.560</a></span> | <span class="t">otherwise the language model could just do like a reward hacking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3629" target="_blank">01:00:29.680</a></span> | <span class="t">so we want this is basically pyref is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3634" target="_blank">01:00:34.400</a></span> | <span class="t">the deep seek v3 base so the current policy so the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3641" target="_blank">01:00:41.360</a></span> | <span class="t">language model should try to be as close as possible to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3644" target="_blank">01:00:44.640</a></span> | <span class="t">original model but at the same time it should try to change according to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3648" target="_blank">01:00:48.640</a></span> | <span class="t">reward it gets the advantages of the reward it gets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3653" target="_blank">01:00:53.440</a></span> | <span class="t">now how is the advantage term computed here basically it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3658" target="_blank">01:00:58.160</a></span> | <span class="t">each reward normalized so it's basically it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3663" target="_blank">01:01:03.200</a></span> | <span class="t">each reward coming out from a distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3665" target="_blank">01:01:05.360</a></span> | <span class="t">centered on the mean of zero and the standard division of one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3670" target="_blank">01:01:10.880</a></span> | <span class="t">why do we want this because it means first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3674" target="_blank">01:01:14.800</a></span> | <span class="t">we don't want the numeric value of the reward to affect the training process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3679" target="_blank">01:01:19.760</a></span> | <span class="t">but how better it is compared to the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3683" target="_blank">01:01:23.600</a></span> | <span class="t">rewards to affect the the training process not the magnitude of its value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3690" target="_blank">01:01:30.160</a></span> | <span class="t">okay now that we have seen this i believe you should have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3697" target="_blank">01:01:37.120</a></span> | <span class="t">most of the knowledge i guess to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3700" target="_blank">01:01:40.320</a></span> | <span class="t">all of the paper because the other part that they do is okay instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3707" target="_blank">01:01:47.040</a></span> | <span class="t">just taking the language model and just training with the reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3710" target="_blank">01:01:50.880</a></span> | <span class="t">let's try to introduce some first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3715" target="_blank">01:01:55.120</a></span> | <span class="t">first of all let's try to introduce some very high quality super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3719" target="_blank">01:01:59.280</a></span> | <span class="t">fine tuning data and then we do another step of reinforcement learning and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3722" target="_blank">01:02:02.640</a></span> | <span class="t">we do another step of fine tuning and then do another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3726" target="_blank">01:02:06.000</a></span> | <span class="t">step of reinforcement learning and this actually leads to a better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3730" target="_blank">01:02:10.720</a></span> | <span class="t">better model another actually interesting part of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3734" target="_blank">01:02:14.240</a></span> | <span class="t">the paper is the distillation so the distillation i don't know if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3737" target="_blank">01:02:17.600</a></span> | <span class="t">most people are familiar with what is distillation and how it works so if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3741" target="_blank">01:02:21.440</a></span> | <span class="t">want i can talk about it a little bit otherwise i think let's go to sleep guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3745" target="_blank">01:02:25.840</a></span> | <span class="t">let's see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3748" target="_blank">01:02:28.240</a></span> | <span class="t">so yes means let's talk about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3753" target="_blank">01:02:33.280</a></span> | <span class="t">okay okay okay okay no problem please give big picture overview i mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3759" target="_blank">01:02:39.760</a></span> | <span class="t">bro you can just read the paper yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3762" target="_blank">01:02:42.720</a></span> | <span class="t">okay yes okay more on distillation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3768" target="_blank">01:02:48.000</a></span> | <span class="t">okay so distillation basically means this imagine you have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3773" target="_blank">01:02:53.680</a></span> | <span class="t">imagine you are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3776" target="_blank">01:02:56.720</a></span> | <span class="t">imagine you are trying to teach yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3783" target="_blank">01:03:03.440</a></span> | <span class="t">okay imagine you are trying to teach yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3790" target="_blank">01:03:10.720</a></span> | <span class="t">graduate maths that's one thing right imagine you don't have any background on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3795" target="_blank">01:03:15.600</a></span> | <span class="t">math and imagine you learn it from a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3798" target="_blank">01:03:18.000</a></span> | <span class="t">university teacher it's two different thing right because if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3801" target="_blank">01:03:21.520</a></span> | <span class="t">you try to learn it by yourself you need to come up with all the strategies to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3804" target="_blank">01:03:24.400</a></span> | <span class="t">learn math but if you learn it from a professor then the professor can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3807" target="_blank">01:03:27.520</a></span> | <span class="t">give you hints on how to learn it faster so in the case of models we have usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3813" target="_blank">01:03:33.360</a></span> | <span class="t">a big model so let's call it big brother</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3817" target="_blank">01:03:37.760</a></span> | <span class="t">and then we have a smaller model what we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3821" target="_blank">01:03:41.520</a></span> | <span class="t">is and then we have a data set let's call it a data set so data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3829" target="_blank">01:03:49.360</a></span> | <span class="t">what we do is we prompt the big brother so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3832" target="_blank">01:03:52.800</a></span> | <span class="t">big model let's call it the big model actually i don't want to confuse people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3838" target="_blank">01:03:58.080</a></span> | <span class="t">so this is the big model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3841" target="_blank">01:04:01.920</a></span> | <span class="t">and this is the small model so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3846" target="_blank">01:04:06.480</a></span> | <span class="t">so now when we train language models so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3850" target="_blank">01:04:10.800</a></span> | <span class="t">what how do we train language models first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3853" target="_blank">01:04:13.200</a></span> | <span class="t">when we train language models we do that in a self-supervised way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3857" target="_blank">01:04:17.520</a></span> | <span class="t">what does it mean it means that there is no kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3861" target="_blank">01:04:21.040</a></span> | <span class="t">it means that the the language model is trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3865" target="_blank">01:04:25.200</a></span> | <span class="t">without annotating the data it means that we sample a lot of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3869" target="_blank">01:04:29.680</a></span> | <span class="t">and we force the language model to learn to predict the next token given the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3873" target="_blank">01:04:33.760</a></span> | <span class="t">the context that comes before it it means that imagine you want to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3877" target="_blank">01:04:37.360</a></span> | <span class="t">the language model on the following text for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3882" target="_blank">01:04:42.720</a></span> | <span class="t">the following sentence for distilled model we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3887" target="_blank">01:04:47.120</a></span> | <span class="t">report representative results blah blah imagine we want to train the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3891" target="_blank">01:04:51.600</a></span> | <span class="t">model only on this green text here what we will do is we will give it the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3896" target="_blank">01:04:56.720</a></span> | <span class="t">word for and we ask it to learn to predict the word distilled when it sees</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3900" target="_blank">01:05:00.880</a></span> | <span class="t">for then we give it the word for distilled and we force it to learn to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3905" target="_blank">01:05:05.520</a></span> | <span class="t">predict the word models when it sees for distilled the beauty of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3909" target="_blank">01:05:09.440</a></span> | <span class="t">transformer is that all this process can be done in parallel so we don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3912" target="_blank">01:05:12.640</a></span> | <span class="t">do it one word at the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3915" target="_blank">01:05:15.920</a></span> | <span class="t">if you do the job of training a small model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3920" target="_blank">01:05:20.240</a></span> | <span class="t">on reasoning just by itself it will have much more difficulties however if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3925" target="_blank">01:05:25.360</a></span> | <span class="t">have a big model that has already been trained on a specific task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3929" target="_blank">01:05:29.360</a></span> | <span class="t">then you can use the big model to help the small model learn faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3933" target="_blank">01:05:33.680</a></span> | <span class="t">how when we train a language model on raw data so in this case for example we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3939" target="_blank">01:05:39.680</a></span> | <span class="t">said to the language model when you see four you should choose distilled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3944" target="_blank">01:05:44.480</a></span> | <span class="t">when you see four distilled you should models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3948" target="_blank">01:05:48.080</a></span> | <span class="t">this basically is called the next token prediction task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3952" target="_blank">01:05:52.320</a></span> | <span class="t">and the way we do it is we force the distribution that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3956" target="_blank">01:05:56.000</a></span> | <span class="t">the language model outputs and this distribution is the goal of the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3959" target="_blank">01:05:59.520</a></span> | <span class="t">model so as we saw before the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3961" target="_blank">01:06:01.360</a></span> | <span class="t">generates a distribution over all the possible next words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3964" target="_blank">01:06:04.880</a></span> | <span class="t">so it will assign a probability to the first word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3968" target="_blank">01:06:08.320</a></span> | <span class="t">next word and to the second next word and the third next word and the fourth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3972" target="_blank">01:06:12.400</a></span> | <span class="t">next word and we ask it okay when you see four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3976" target="_blank">01:06:16.400</a></span> | <span class="t">distilled you should exactly this particular word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3979" target="_blank">01:06:19.840</a></span> | <span class="t">which is the word models so you should choose the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3985" target="_blank">01:06:25.200</a></span> | <span class="t">models and all the other words should not be chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3988" target="_blank">01:06:28.880</a></span> | <span class="t">so zero zero zero zero zero this one should be chosen with 100 percent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3995" target="_blank">01:06:35.120</a></span> | <span class="t">score this is how we force the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=3999" target="_blank">01:06:39.520</a></span> | <span class="t">by doing it on many many many many texts the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4002" target="_blank">01:06:42.720</a></span> | <span class="t">learns to generate a distribution that very likely will generate this word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4007" target="_blank">01:06:47.840</a></span> | <span class="t">models when it is four distilled and less likely to generate the others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4013" target="_blank">01:06:53.600</a></span> | <span class="t">if we do the same job with the small model we will see that the small model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4016" target="_blank">01:06:56.880</a></span> | <span class="t">will have a lot of difficulties learning at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4020" target="_blank">01:07:00.800</a></span> | <span class="t">same pace as the big model why because the big model has more parameters so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4024" target="_blank">01:07:04.320</a></span> | <span class="t">has more flexibility in learning complex tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4027" target="_blank">01:07:07.760</a></span> | <span class="t">while the small model does not have this flexibility so it will must be much slow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4031" target="_blank">01:07:11.600</a></span> | <span class="t">learner so how do we how can we distill the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4036" target="_blank">01:07:16.000</a></span> | <span class="t">the knowledge of the big model into the small model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4039" target="_blank">01:07:19.760</a></span> | <span class="t">we do it as follows we take a data set of prompts so a list of prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4046" target="_blank">01:07:26.240</a></span> | <span class="t">we feed it to the big model and then we ask the big model to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4050" target="_blank">01:07:30.080</a></span> | <span class="t">an answer not only the answer we also ask it to generate the log probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4055" target="_blank">01:07:35.440</a></span> | <span class="t">at each step of the generation process so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4058" target="_blank">01:07:38.720</a></span> | <span class="t">before we use the sentence where is shanghai shanghai is in china right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4063" target="_blank">01:07:43.200</a></span> | <span class="t">so imagine we are dealing with the big model where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4067" target="_blank">01:07:47.280</a></span> | <span class="t">the model will generate the shanghai is in china but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4070" target="_blank">01:07:50.720</a></span> | <span class="t">with the because we have the word shanghai we also have the log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4075" target="_blank">01:07:55.280</a></span> | <span class="t">of not only the word shanghai but also all the other words that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4079" target="_blank">01:07:59.360</a></span> | <span class="t">could we could have chosen in that position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4082" target="_blank">01:08:02.640</a></span> | <span class="t">we forced the small model to learn not only to generate shanghai but we force</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4087" target="_blank">01:08:07.520</a></span> | <span class="t">it to learn the same distribution that the big model generated for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4092" target="_blank">01:08:12.800</a></span> | <span class="t">same position so let me do it again so imagine now we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4097" target="_blank">01:08:17.040</a></span> | <span class="t">do a concrete example imagine we have a sentence where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4104" target="_blank">01:08:24.160</a></span> | <span class="t">shanghai we put it give it to the big model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4110" target="_blank">01:08:30.000</a></span> | <span class="t">the big model will generate an answer what does it mean to generate an answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4112" target="_blank">01:08:32.960</a></span> | <span class="t">it means that it will generate first of all a distribution over what is the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4116" target="_blank">01:08:36.400</a></span> | <span class="t">likely next token which will be a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4119" target="_blank">01:08:39.440</a></span> | <span class="t">probabilities where the word shanghai will have a very high score so maybe 0.6</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4125" target="_blank">01:08:45.280</a></span> | <span class="t">and maybe the word pizza will be 0.1 and the word the cat will be 0.05 etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4132" target="_blank">01:08:52.400</a></span> | <span class="t">0.05 etc etc then we choose the word shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4136" target="_blank">01:08:56.480</a></span> | <span class="t">and we ask the language model again where is shanghai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4139" target="_blank">01:08:59.600</a></span> | <span class="t">question mark shanghai then it will choose the word is but it will not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4143" target="_blank">01:09:03.760</a></span> | <span class="t">choose the word is it will actually give us a distribution and we choose the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4147" target="_blank">01:09:07.520</a></span> | <span class="t">is it will give us a distribution that looks like this for example it will say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4151" target="_blank">01:09:11.360</a></span> | <span class="t">okay the word is is very likely so it's maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4155" target="_blank">01:09:15.440</a></span> | <span class="t">70 probability the word i don't know road is unlikely because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4162" target="_blank">01:09:22.400</a></span> | <span class="t">doesn't make sense and the word i don't know hello</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4166" target="_blank">01:09:26.720</a></span> | <span class="t">also doesn't make sense so it is even less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4170" target="_blank">01:09:30.160</a></span> | <span class="t">likely so for each position we can generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4173" target="_blank">01:09:33.600</a></span> | <span class="t">log probabilities from the big model and we force the small model not to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4178" target="_blank">01:09:38.240</a></span> | <span class="t">learn to just generate shanghai but to learn this entire distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4182" target="_blank">01:09:42.160</a></span> | <span class="t">why because this gives a much stronger signal to the small model on what it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4188" target="_blank">01:09:48.080</a></span> | <span class="t">should do what it can do and what it must never do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4192" target="_blank">01:09:52.000</a></span> | <span class="t">instead of just telling you should do this it's a bigger signal for the small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4196" target="_blank">01:09:56.240</a></span> | <span class="t">model to learn faster and what they say in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4201" target="_blank">01:10:01.120</a></span> | <span class="t">in the deep seek r1 paper is that by distilling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4204" target="_blank">01:10:04.880</a></span> | <span class="t">you can create a much stronger model than just by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4208" target="_blank">01:10:08.640</a></span> | <span class="t">um training them from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4212" target="_blank">01:10:12.960</a></span> | <span class="t">a bit more context on the probabilities for each forward pass well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4228" target="_blank">01:10:28.320</a></span> | <span class="t">uh no distillation is used on all the tokens so it's you're telling for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4234" target="_blank">01:10:34.960</a></span> | <span class="t">each possible token you learn the entire distribution for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4238" target="_blank">01:10:38.400</a></span> | <span class="t">each position not only the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4253" target="_blank">01:10:53.040</a></span> | <span class="t">so how we assign reward for each action okay in the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4256" target="_blank">01:10:56.800</a></span> | <span class="t">of um in the case of ppo by the way if you watch my video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4263" target="_blank">01:11:03.840</a></span> | <span class="t">ppo i i explain all of that so i don't want to kind of uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4269" target="_blank">01:11:09.120</a></span> | <span class="t">i don't know why nobody ever watched that video it's my one of my masterpieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4273" target="_blank">01:11:13.200</a></span> | <span class="t">and nobody ever cared about it but all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4275" target="_blank">01:11:15.520</a></span> | <span class="t">slides come from my video that i done in 2023</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4279" target="_blank">01:11:19.040</a></span> | <span class="t">so um the way we generate the rewards is basically we the reward are usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4286" target="_blank">01:11:26.320</a></span> | <span class="t">only generated for the okay there are two kind of rewards that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4290" target="_blank">01:11:30.080</a></span> | <span class="t">generate one is called the outcome based reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4293" target="_blank">01:11:33.680</a></span> | <span class="t">and one is called the process based reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4296" target="_blank">01:11:36.480</a></span> | <span class="t">in the case of ppo we generate the outcome based reward and then through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4301" target="_blank">01:11:41.760</a></span> | <span class="t">the advantage term is it is a kind of distributed on all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4306" target="_blank">01:11:46.800</a></span> | <span class="t">previous tokens because we sample a response and then we judge this response</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4311" target="_blank">01:11:51.600</a></span> | <span class="t">according to our reward model then with the advantage terms that you see in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4316" target="_blank">01:11:56.160</a></span> | <span class="t">loss this reward is distributed on all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4320" target="_blank">01:12:00.880</a></span> | <span class="t">previous tokens so each token carries information on how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4325" target="_blank">01:12:05.280</a></span> | <span class="t">likely it is going to lead to the final reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4338" target="_blank">01:12:18.080</a></span> | <span class="t">okay there is another part of way i think we forgot one part guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4344" target="_blank">01:12:24.800</a></span> | <span class="t">um the unsuccessful attempts this is interesting also so uh they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4351" target="_blank">01:12:31.280</a></span> | <span class="t">tried what is known as the process reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4353" target="_blank">01:12:33.680</a></span> | <span class="t">model basically um okay the reward model that we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4357" target="_blank">01:12:37.360</a></span> | <span class="t">is a rule-based model that is outcome driven means that the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4361" target="_blank">01:12:41.600</a></span> | <span class="t">have to generate the entire process and the for example in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4364" target="_blank">01:12:44.960</a></span> | <span class="t">the case of read code problems the model has to generate the final code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4368" target="_blank">01:12:48.320</a></span> | <span class="t">to make it runnable then we run the code we see how it performs and then we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4373" target="_blank">01:12:53.600</a></span> | <span class="t">signal on the reward however there are other ways of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4377" target="_blank">01:12:57.520</a></span> | <span class="t">generating reward and one of them is called the process reward model where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4381" target="_blank">01:13:01.040</a></span> | <span class="t">you divide the problem in sub-problems and then you have a model which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4388" target="_blank">01:13:08.080</a></span> | <span class="t">called the process reward model that assigns a reward to each single step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4392" target="_blank">01:13:12.400</a></span> | <span class="t">however they in the paper they say that assigning first of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4396" target="_blank">01:13:16.480</a></span> | <span class="t">dividing a problem into sub-steps it's difficult because every problem is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4400" target="_blank">01:13:20.880</a></span> | <span class="t">different and we don't want to kind of tell the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4403" target="_blank">01:13:23.920</a></span> | <span class="t">model you should follow this pattern it should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4406" target="_blank">01:13:26.640</a></span> | <span class="t">the model that should come try to come up with this pattern and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4409" target="_blank">01:13:29.440</a></span> | <span class="t">actually it does and secondly they say that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4412" target="_blank">01:13:32.560</a></span> | <span class="t">difficult to judge each single step so with sometimes we don't even know if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4416" target="_blank">01:13:36.880</a></span> | <span class="t">that step is good or not like when you're trying to solve a math proof</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4420" target="_blank">01:13:40.480</a></span> | <span class="t">sometimes you do some intermitted steps that may not lead to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4426" target="_blank">01:13:46.080</a></span> | <span class="t">to the you may not immediately see that they will lead to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4432" target="_blank">01:13:52.160</a></span> | <span class="t">final conclusion but they are necessary right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4435" target="_blank">01:13:55.520</a></span> | <span class="t">so it's difficult to give them a reward until you arrive to the end so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4440" target="_blank">01:14:00.720</a></span> | <span class="t">difficult to kind of give a reward to each single step and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4444" target="_blank">01:14:04.640</a></span> | <span class="t">there is another technique called the multi-carlot research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4447" target="_blank">01:14:07.440</a></span> | <span class="t">which basically it's a tree search in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4450" target="_blank">01:14:10.640</a></span> | <span class="t">each intermediate node is given each intermediate node is also a step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4456" target="_blank">01:14:16.480</a></span> | <span class="t">and each of these steps each of these nodes actually has a kind of a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4460" target="_blank">01:14:20.720</a></span> | <span class="t">associated with it which increases the more time that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4464" target="_blank">01:14:24.720</a></span> | <span class="t">particular step leads to a successful solution so imagine for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4469" target="_blank">01:14:29.360</a></span> | <span class="t">example you are doing a lit code and if you a lit code problem so if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4473" target="_blank">01:14:33.360</a></span> | <span class="t">start your code for example with a typo</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4477" target="_blank">01:14:37.200</a></span> | <span class="t">of course the code will not compile so any style anything that starts with that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4481" target="_blank">01:14:41.280</a></span> | <span class="t">type typo will never lead to a successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4483" target="_blank">01:14:43.520</a></span> | <span class="t">solution so that node will never be explored further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4486" target="_blank">01:14:46.480</a></span> | <span class="t">so multi-carlot tree search basically forces the language model to explore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4490" target="_blank">01:14:50.480</a></span> | <span class="t">more of the solutions that lead to successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4494" target="_blank">01:14:54.080</a></span> | <span class="t">attempts and less to the one that don't succeed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4498" target="_blank">01:14:58.400</a></span> | <span class="t">however even by doing multi-carlot research they they get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4501" target="_blank">01:15:01.760</a></span> | <span class="t">like sub results that are not as good as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4506" target="_blank">01:15:06.400</a></span> | <span class="t">reinforcement learning driven one and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4509" target="_blank">01:15:09.520</a></span> | <span class="t">the beauty of this actually is also in this sentence which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4512" target="_blank">01:15:12.880</a></span> | <span class="t">rather than explicitly teaching the model on how to first of all divide the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4516" target="_blank">01:15:16.720</a></span> | <span class="t">problem into sub problems and how to solve the problem or tell the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4520" target="_blank">01:15:20.880</a></span> | <span class="t">model what is the format it should follow to solve the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4524" target="_blank">01:15:24.080</a></span> | <span class="t">the model just learns to solve to just learns to come up with the right format</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4529" target="_blank">01:15:29.520</a></span> | <span class="t">with the right chain of thought to solve the problem of course they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4533" target="_blank">01:15:33.280</a></span> | <span class="t">apply they play with the reward because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4535" target="_blank">01:15:35.920</a></span> | <span class="t">tell the reward that the language model should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4539" target="_blank">01:15:39.920</a></span> | <span class="t">follow a particular format and the language model should also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4543" target="_blank">01:15:43.200</a></span> | <span class="t">what is it should be accurate etc etc so you can always play with a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4547" target="_blank">01:15:47.680</a></span> | <span class="t">the reward and another thing that they notice is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4549" target="_blank">01:15:49.920</a></span> | <span class="t">actually if you train the language model only on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4552" target="_blank">01:15:52.560</a></span> | <span class="t">reinforcement learning then it leads to kind of the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4557" target="_blank">01:15:57.120</a></span> | <span class="t">with some side effects for example the fact that the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4561" target="_blank">01:16:01.760</a></span> | <span class="t">model mixes languages because nobody told the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4565" target="_blank">01:16:05.440</a></span> | <span class="t">that it has to only speak english to solve a problem that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4569" target="_blank">01:16:09.040</a></span> | <span class="t">stated in english so imagine i ask you to solve a problem like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4572" target="_blank">01:16:12.160</a></span> | <span class="t">a math problem and i say it in english</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4575" target="_blank">01:16:15.920</a></span> | <span class="t">and imagine you are like a mixed kid and you can speak chinese and english or you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4581" target="_blank">01:16:21.040</a></span> | <span class="t">are just a chinese who knows also english etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4585" target="_blank">01:16:25.520</a></span> | <span class="t">maybe in your head you you speak english and the chinese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4589" target="_blank">01:16:29.040</a></span> | <span class="t">you think in english and in chinese and then you come up with the solution and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4592" target="_blank">01:16:32.400</a></span> | <span class="t">that's totally correct right so unless i tell you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4595" target="_blank">01:16:35.280</a></span> | <span class="t">explicitly that you should never think in chinese</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4598" target="_blank">01:16:38.720</a></span> | <span class="t">then you you why you should not take the freedom of doing it right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4602" target="_blank">01:16:42.480</a></span> | <span class="t">so if you never tell the model to not do something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4606" target="_blank">01:16:46.160</a></span> | <span class="t">the model probably will do it and because in the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4609" target="_blank">01:16:49.520</a></span> | <span class="t">here they never told the language model to never think in other languages the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4612" target="_blank">01:16:52.880</a></span> | <span class="t">language actually started thinking in other languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4616" target="_blank">01:16:56.000</a></span> | <span class="t">anything to get the job done that's the beauty of reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4620" target="_blank">01:17:00.640</a></span> | <span class="t">so you give the right incentive and the language model will come up with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4624" target="_blank">01:17:04.000</a></span> | <span class="t">right way of reaching that goal as long as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4627" target="_blank">01:17:07.840</a></span> | <span class="t">signal is strong enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4630" target="_blank">01:17:10.880</a></span> | <span class="t">yes you need a massive cluster of gpus i think i believe so because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4639" target="_blank">01:17:19.600</a></span> | <span class="t">you still have a like you're still running gradient descent on the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4643" target="_blank">01:17:23.120</a></span> | <span class="t">model itself so every time you are kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4646" target="_blank">01:17:26.240</a></span> | <span class="t">fine-tuning the language model you are changing its weights so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4650" target="_blank">01:17:30.640</a></span> | <span class="t">okay guys um before we talk about other questions okay uh all this lecture was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4665" target="_blank">01:17:45.200</a></span> | <span class="t">totally uh kind of um improvised so i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4668" target="_blank">01:17:48.320</a></span> | <span class="t">prepare it um so i hope i didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4673" target="_blank">01:17:53.440</a></span> | <span class="t">make big mistakes because the problem is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4677" target="_blank">01:17:57.280</a></span> | <span class="t">reinforcement learning from human feedback is quite a complicated topic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4679" target="_blank">01:17:59.840</a></span> | <span class="t">and you need to kind of derive everything step by step this is what i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4682" target="_blank">01:18:02.400</a></span> | <span class="t">do in my video on reinforcement learning from human feedback here i kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4685" target="_blank">01:18:05.600</a></span> | <span class="t">sometimes skipped some steps and went back etc so i hope i didn't create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4689" target="_blank">01:18:09.600</a></span> | <span class="t">confusion so if there is kind of some parts about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4694" target="_blank">01:18:14.480</a></span> | <span class="t">the rl that you didn't understand then i</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4697" target="_blank">01:18:17.440</a></span> | <span class="t">think we can talk about it otherwise let's call it a day guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4702" target="_blank">01:18:22.960</a></span> | <span class="t">maybe you could record for youtube more prepared i could but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4706" target="_blank">01:18:26.800</a></span> | <span class="t">i have a daughter guys now i'm super busy and i also have a full-time job and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4710" target="_blank">01:18:30.400</a></span> | <span class="t">i also have a wife so have some mercy on me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4715" target="_blank">01:18:35.040</a></span> | <span class="t">any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4725" target="_blank">01:18:45.040</a></span> | <span class="t">thank you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4731" target="_blank">01:18:51.680</a></span> | <span class="t">yeah i think everyone now should have at least the basic understanding on how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4735" target="_blank">01:18:55.760</a></span> | <span class="t">read the paper like when you read the paper you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4738" target="_blank">01:18:58.960</a></span> | <span class="t">have a clear idea of what's happening and if you kind of need more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4742" target="_blank">01:19:02.720</a></span> | <span class="t">background you are uh you can check like my previous works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4749" target="_blank">01:19:09.440</a></span> | <span class="t">i think let's stop the recording and have a good night guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4755" target="_blank">01:19:15.520</a></span> | <span class="t">and now if you want you can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4760" target="_blank">01:19:20.800</a></span> | <span class="t">how to stop the recording</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4764" target="_blank">01:19:24.160</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4766" target="_blank">01:19:26.240</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4768" target="_blank">01:19:28.400</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4770" target="_blank">01:19:30.480</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4771" target="_blank">01:19:31.480</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4772" target="_blank">01:19:32.480</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4773" target="_blank">01:19:33.480</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4774" target="_blank">01:19:34.480</a></span> | <span class="t">you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc&t=4775" target="_blank">01:19:35.480</a></span> | <span class="t">you</span></div></div></body></html>