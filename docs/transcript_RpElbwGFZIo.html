<html><head><title>Is Alignment Solvable? – Dario Amodei (Anthropic CEO)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Is Alignment Solvable? – Dario Amodei (Anthropic CEO)</h2><a href="https://www.youtube.com/watch?v=RpElbwGFZIo" target="_blank"><img src="https://i.ytimg.com/vi/RpElbwGFZIo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>what is the timescale on which you think alignment is solvable? I think this is a really difficult question because I actually think often people are thinking about kind of alignment in the wrong way. I think there's a general feeling that it's like models are misaligned or like there's like an alignment problem to solve, kind of like the Riemann hypothesis or something like someday we'll crack the Riemann hypothesis.</p><p>I don't quite think it's like that, not in a way that's worse or better. It might be just as bad or just as unpredictable. I think this frame of like, man, we haven't cracked the problem yet. We haven't solved the Riemann hypothesis isn't quite right. I think of it more as.</p><p>Already with today's systems, we are not very good at controlling them, and the consequences of that could be could be could be very bad. We just need to get more ways of like increasing the likelihood that are that are that, you know, that we can control our models and understand what's going on in them.</p><p>And like we have some of them so far, they aren't that good yet. But, you know, I don't think of this as binary of like works and not works. We're going to develop more. And I do think that over over the next two to three years, we're going to start eating that probability mass of ways things can go wrong.</p><p>When I think of like, you know, why am I why am I scared? Few things I think of. One is look like I think the thing that's really hard to argue with is like there will be powerful models. They will be agentic. We're getting towards them. If such a model wanted to wreak havoc and destroy humanity or whatever, I think we have basically no ability to stop it.</p><p>Like that's that's I think just just if that's not true at some point, it'll it will reach the point where it's true as we scale the models. So that definitely seems the case. And I think a second thing that seems the case is that we seem to be bad at controlling the models, not in any particular way, but just their statistical systems.</p><p>And you can ask a million things and they can say a million things in reply. And, you know, you might not have thought of a millionth of one thing that does something crazy. Or when you train them, you train them in this very abstract way. And you might not understand all the consequences of of what they do in response to that.</p><p>I mean, I think the best example we've seen of that is like being and being in Sydney, right, where it's like, I don't know how they train that model. I don't know what they did to make it do all this weird stuff like, you know, threaten threaten people and, you know, have this kind of weird, obsessive personality.</p><p>But but what it shows is that we can get something very different from and maybe opposite to what we intended. And so I actually think facts, number one, in fact, number two are like enough to be really worried. Like you don't need all this detailed stuff about, you know, convergent instrumental goals or, you know, analogies to evolution.</p><p>Like actually one and two for me are pretty motivated. I'm like, OK, this thing's going to be powerful. It could destroy us. And like all the ones built so far, like, you know, are at pretty decent risk of doing some random shit we don't understand. If we take our current understanding and move that to two very powerful models, you might just be in this world where it's like, OK, you make something and depending on the details, maybe it's totally fine.</p><p>You know, not really alignment by default, but but just kind of like it depends on a lot of the details. And like if you if you're very careful about all those details and you know what you're doing, you're getting it right. But we have a high susceptibility to you mess something up in a way that you didn't really understand was connected to.</p><p>Actually, instead of making all the humans happy, it wants to, you know, turn them into pumpkins. Yeah, I just some weird shit, right? Because the models are so powerful, you know, they're like these kind of giants that are, you know, they're they're like, you know, they're standing in the landscape.</p><p>And if they start to move their arms around randomly, they could just break everything. I don't think we're aligned by default. I don't think we're doomed by default and have some problem we need to solve. It has some kind of different character. Now, what I do think is that hopefully within a timescale of two to three years, we get better at diagnosing when the models are good and when they're bad.</p><p>We get better at increasing our repertoire of methods to train the model that they're less likely to do bad things and more likely to do good things in a way that isn't just relevant to the current models, but scales. And we can help develop that with interpretability as the test set.</p><p>You know, it's almost like right now, if I try and, you know, juggle five balls or something, I can juggle three balls, right? I actually can. But but I can't juggle five balls at all, right? You have to practice a lot to do that. If I were to do that, I would mostly try.</p><p>I would I would almost certainly drop them. And then just just over time, you just get better at the task of controlling the balls. People love to kind of propose these broad plans and say, like, oh, this is the way we should do it. This is the way we should do it.</p><p>I think the honest fact is that we're figuring this out as we go along. The way we discover new things and understand the structure of what's going to work and what's what's not is by playing around with things. Not that we should just kind of blindly say, oh, this worked here.</p><p>And so we'll work there. But you you really you really start to understand the patterns. I think right now there's just there's there's way there's way too many assumptions. There's way too much overconfidence about how all this is going to go. I have a substantial probability mass on this all goes wrong.</p><p>It's a complete disaster, but in a completely different way than anyone had anticipated.</p></div></div></body></html>