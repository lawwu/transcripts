<html><head><title>Transfer learning proves LLMs aren’t stochastic parrots – Trenton Bricken & Sholto Douglas</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Transfer learning proves LLMs aren’t stochastic parrots – Trenton Bricken & Sholto Douglas</h2><a href="https://www.youtube.com/watch?v=3Fyv3VIgeS4" target="_blank"><img src="https://i.ytimg.com/vi_webp/3Fyv3VIgeS4/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>A book that Trenton recommended, The Symbolic Species, has this really interesting argument. When we just think of language as this contingent and maybe suboptimal way to represent ideas. Actually, maybe one of the reasons that LLMs have succeeded is because language has evolved for tens of thousands of years to be this sort of cast in which young minds can develop.</p><p>Certainly when you talk to multimodal or computer vision researchers versus when you talk to language model researchers, people who work in other modalities have to put enormous amounts of thought into exactly what the right representation space for the images is. Understanding the right level of representation there, really hard.</p><p>In language, people are just like, "Well, I guess you just predict the next token." The case for a multimodal being a way to bridge the data wall or get past the data wall is based on the idea that the things you would have learned from more language tokens anyway, you can just get from YouTube.</p><p>Has that actually been the case? How much positive transfer do you see between different modalities where actually the images are helping you be better at writing code or something? Just because the model is learning a latent capability is just from trying to understand the image. Yeah, I'm the wrong person to ask, but there are interesting interpretability pieces where if we fine tune on math problems, the model just gets better at entity recognition.</p><p>Whoa, really? Yeah, so there's a paper from David Bow's lab recently where they investigate what actually changes in a model when I fine tune it with respect to the attention heads and these sorts of things. Fascinating. And they have this synthetic problem of box A has this object in it, box B has this other object in it.</p><p>What was in this box? And it makes sense, right? That's beautiful. You're better at attending to the positions of different things, which you need for coding and manipulating math equations. I love this kind of research. One of the things you mentioned to me a long time ago is the evidence that when you train LLMs on code, they get better at reasoning in language.</p><p>Which, unless it's the case that the comments in the code are just really high quality tokens or something, implies that to be able to think through how to code better, it makes you a better reasoner. That's crazy, right? I think that's one of the strongest pieces of evidence for scaling, just making the thing smart, that kind of positive transfer.</p><p>I think this is true in two senses. One is just that modeling code obviously implies modeling a difficult reasoning process used to create it. But two, that code is a nice explicit structure of composed reasoning, I guess. If this, then that. Code's a lot of structure in that way that you could imagine transferring to other types of types of reasoning problem.</p><p>Right. And crucially, the thing that makes it significant is that it's not just stochastically predicting the next token of words or whatever, because it's learned that Sally corresponds to murderer at the end of a Sherlock Holmes story. No, if there is some shared thing between code and language, it must be at a deeper level than the model has learned.</p><p>Yeah, I think we have a lot of evidence that actual reasoning is occurring in these models and that they're not just stochastic parrots. It just feels very hard for me to believe that I haven't worked and played with these models. Yeah, my two immediate cash responses to this are one, the work on Othello and now other games where it's like, I give you a sequence of moves in the game and it turns out if you apply some pretty straightforward interpretability techniques, then you can get a board that the model has learned.</p><p>And it's never seen the game board before anything, right? Like that's generalization. The other is Anthropic's influence functions paper that came out last year, where they look at the model outputs, like, please don't turn me off. I want to be helpful. And then they scan, like, what was the data that led to that?</p><p>And like one of the data points that was very influential was someone dying of dehydration in the desert and like having like a will to keep surviving. And to me, that just seems like very clear generalization of motive rather than regurgitating, don't turn me off.</p></div></div></body></html>