<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 3: Positional Encoding I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 3: Positional Encoding I Spring 2023</h2><a href="https://www.youtube.com/watch?v=JERXX2Byr90"><img src="https://i.ytimg.com/vi/JERXX2Byr90/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./JERXX2Byr90.html">Whisper Transcript</a> | <a href="./transcript_JERXX2Byr90.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=6" target="_blank">00:00:06.160</a></span> | <span class="t">This is part 3 in our series on contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=9" target="_blank">00:00:09.600</a></span> | <span class="t">We have a bunch of famous transformer-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=12" target="_blank">00:00:12.040</a></span> | <span class="t">architectures that we're going to talk about a bit later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=14" target="_blank">00:00:14.480</a></span> | <span class="t">But before doing that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=15" target="_blank">00:00:15.800</a></span> | <span class="t">I thought it would be good to pause and just reflect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=18" target="_blank">00:00:18.600</a></span> | <span class="t">a little bit on this important notion of positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=21" target="_blank">00:00:21.480</a></span> | <span class="t">This is an idea that I feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=23" target="_blank">00:00:23.200</a></span> | <span class="t">the field took for granted for too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=25" target="_blank">00:00:25.320</a></span> | <span class="t">I certainly took it for granted for too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=27" target="_blank">00:00:27.440</a></span> | <span class="t">I think we now see that this is a crucial factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=31" target="_blank">00:00:31.000</a></span> | <span class="t">in shaping the performance of transformer-based models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=34" target="_blank">00:00:34.360</a></span> | <span class="t">Let's start by reflecting on the role of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=36" target="_blank">00:00:36.840</a></span> | <span class="t">positional encoding in the context of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=39" target="_blank">00:00:39.520</a></span> | <span class="t">I think the central observation is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=41" target="_blank">00:00:41.760</a></span> | <span class="t">the transformer itself has only a very limited capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=45" target="_blank">00:00:45.460</a></span> | <span class="t">to keep track of word order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=47" target="_blank">00:00:47.380</a></span> | <span class="t">The attention mechanisms are themselves not directional,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=50" target="_blank">00:00:50.720</a></span> | <span class="t">it's just a bunch of dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=52" target="_blank">00:00:52.720</a></span> | <span class="t">There are no other interactions between the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=56" target="_blank">00:00:56.560</a></span> | <span class="t">We are in grave danger of losing track of the fact that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=60" target="_blank">00:01:00.680</a></span> | <span class="t">the input sequence ABC is different from the input sequence CBA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=66" target="_blank">00:01:06.120</a></span> | <span class="t">Positional encodings will ensure that we retain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=69" target="_blank">00:01:09.280</a></span> | <span class="t">a difference between those two sequences no matter what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=72" target="_blank">00:01:12.040</a></span> | <span class="t">do with the representations that come from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=75" target="_blank">00:01:15.880</a></span> | <span class="t">Secondarily, there's another purpose that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=78" target="_blank">00:01:18.400</a></span> | <span class="t">positional encodings play which is hierarchical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=81" target="_blank">00:01:21.300</a></span> | <span class="t">They've been used to keep track of things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=83" target="_blank">00:01:23.280</a></span> | <span class="t">premise hypothesis in natural language inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=86" target="_blank">00:01:26.380</a></span> | <span class="t">That was an important feature of the BERT model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=88" target="_blank">00:01:28.740</a></span> | <span class="t">that we'll talk about a bit later in the series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=91" target="_blank">00:01:31.960</a></span> | <span class="t">I think there are a lot of perspectives that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=94" target="_blank">00:01:34.400</a></span> | <span class="t">could take on positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=96" target="_blank">00:01:36.580</a></span> | <span class="t">To keep things simple, I thought I would center</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=98" target="_blank">00:01:38.680</a></span> | <span class="t">our discussion around two crucial questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=101" target="_blank">00:01:41.900</a></span> | <span class="t">The first is, does the set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=104" target="_blank">00:01:44.360</a></span> | <span class="t">positions need to be decided ahead of time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=107" target="_blank">00:01:47.860</a></span> | <span class="t">The second is, does the positional encoding scheme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=111" target="_blank">00:01:51.640</a></span> | <span class="t">hinder generalization to new positions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=115" target="_blank">00:01:55.200</a></span> | <span class="t">I think those are good questions to guide us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=117" target="_blank">00:01:57.960</a></span> | <span class="t">One other rule that I wanted to introduce is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=122" target="_blank">00:02:02.080</a></span> | <span class="t">Modern transformer architectures might impose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=124" target="_blank">00:02:04.880</a></span> | <span class="t">a max length on sequences for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=126" target="_blank">00:02:06.900</a></span> | <span class="t">many reasons related to how they were designed and optimized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=130" target="_blank">00:02:10.520</a></span> | <span class="t">I would like to set all of that aside and just ask whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=134" target="_blank">00:02:14.220</a></span> | <span class="t">the positional encoding scheme itself is imposing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=138" target="_blank">00:02:18.060</a></span> | <span class="t">anything about length generalization separately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=140" target="_blank">00:02:20.360</a></span> | <span class="t">from all that other stuff that might be happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=143" target="_blank">00:02:23.280</a></span> | <span class="t">Let's start with absolute positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=146" target="_blank">00:02:26.300</a></span> | <span class="t">This is the scheme that we have talked about so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=149" target="_blank">00:02:29.520</a></span> | <span class="t">On this scheme, we have word representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=152" target="_blank">00:02:32.760</a></span> | <span class="t">and we also have positional representations that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=155" target="_blank">00:02:35.560</a></span> | <span class="t">learned corresponding to some fixed number of dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=159" target="_blank">00:02:39.100</a></span> | <span class="t">To get our position-sensitive word representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=163" target="_blank">00:02:43.220</a></span> | <span class="t">we simply add together the word vector with the position vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=167" target="_blank">00:02:47.780</a></span> | <span class="t">How is this scheme doing for our two crucial questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=171" target="_blank">00:02:51.580</a></span> | <span class="t">Well, not so well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=173" target="_blank">00:02:53.160</a></span> | <span class="t">First, obviously, the set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=175" target="_blank">00:02:55.280</a></span> | <span class="t">positions needs to be decided ahead of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=178" target="_blank">00:02:58.160</a></span> | <span class="t">When we set up our model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=179" target="_blank">00:02:59.700</a></span> | <span class="t">we will have some embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=181" target="_blank">00:03:01.640</a></span> | <span class="t">maybe up to 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=184" target="_blank">00:03:04.180</a></span> | <span class="t">If we picked 512,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">when we hit position 513,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=188" target="_blank">00:03:08.440</a></span> | <span class="t">we will not have a positional representation for that position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=193" target="_blank">00:03:13.860</a></span> | <span class="t">I also think it's clear that this scheme can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=197" target="_blank">00:03:17.520</a></span> | <span class="t">hinder generalization to new positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=200" target="_blank">00:03:20.400</a></span> | <span class="t">even for familiar phenomena.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=202" target="_blank">00:03:22.900</a></span> | <span class="t">Just consider the fact that the rock as a phrase,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=206" target="_blank">00:03:26.400</a></span> | <span class="t">if it occurs early in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=208" target="_blank">00:03:28.620</a></span> | <span class="t">is simply a different representation than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=211" target="_blank">00:03:31.240</a></span> | <span class="t">the rock if it appears later in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=214" target="_blank">00:03:34.520</a></span> | <span class="t">There will be some shared features across these two as a result of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=218" target="_blank">00:03:38.440</a></span> | <span class="t">the fact that we have two word vectors involved in both places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=222" target="_blank">00:03:42.520</a></span> | <span class="t">But we add in those positional representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=225" target="_blank">00:03:45.480</a></span> | <span class="t">as equal partners in this representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=228" target="_blank">00:03:48.000</a></span> | <span class="t">and I think the result is very heavy-handed when it comes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=231" target="_blank">00:03:51.880</a></span> | <span class="t">learning representations that are heavily position-dependent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=235" target="_blank">00:03:55.600</a></span> | <span class="t">That could make it hard for the model to see that in some sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=239" target="_blank">00:03:59.040</a></span> | <span class="t">the rock is the same phrase whether it's at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=241" target="_blank">00:04:01.840</a></span> | <span class="t">the start of the sequence or the middle or the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=245" target="_blank">00:04:05.920</a></span> | <span class="t">Another scheme we could consider actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=248" target="_blank">00:04:08.680</a></span> | <span class="t">goes all the way back to the Transformers paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=251" target="_blank">00:04:11.000</a></span> | <span class="t">I've called this frequency-based positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=254" target="_blank">00:04:14.220</a></span> | <span class="t">There are lots of ways we could set this up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=257" target="_blank">00:04:17.040</a></span> | <span class="t">but the essential idea here is that we'll define</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=259" target="_blank">00:04:19.880</a></span> | <span class="t">a mathematical function that given a position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=263" target="_blank">00:04:23.240</a></span> | <span class="t">will give us back a vector that encodes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=265" target="_blank">00:04:25.960</a></span> | <span class="t">information about that position semantically in its structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=270" target="_blank">00:04:30.160</a></span> | <span class="t">In the Transformer paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=271" target="_blank">00:04:31.840</a></span> | <span class="t">they picked a scheme that's based in frequency oscillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=275" target="_blank">00:04:35.400</a></span> | <span class="t">Essentially based in sine and cosine frequencies for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=278" target="_blank">00:04:38.920</a></span> | <span class="t">these vectors where higher positions oscillate more frequently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=283" target="_blank">00:04:43.560</a></span> | <span class="t">and that information is encoded in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=285" target="_blank">00:04:45.840</a></span> | <span class="t">the position vector that we create.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=288" target="_blank">00:04:48.020</a></span> | <span class="t">I think there are lots of other schemes that we could use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=290" target="_blank">00:04:50.480</a></span> | <span class="t">The essential feature of this is this argument pause here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=294" target="_blank">00:04:54.320</a></span> | <span class="t">If you give this function position 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=296" target="_blank">00:04:56.960</a></span> | <span class="t">it gives you a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=298" target="_blank">00:04:58.240</a></span> | <span class="t">If you give it 513,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=299" target="_blank">00:04:59.920</a></span> | <span class="t">it gives you a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=300" target="_blank">00:05:00.840</a></span> | <span class="t">If you give it a million,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=302" target="_blank">00:05:02.100</a></span> | <span class="t">it gives you a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=303" target="_blank">00:05:03.400</a></span> | <span class="t">All of those vectors manifestly do encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=307" target="_blank">00:05:07.160</a></span> | <span class="t">information about the relative position of that input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=312" target="_blank">00:05:12.320</a></span> | <span class="t">We have definitely overcome the first limitation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=315" target="_blank">00:05:15.600</a></span> | <span class="t">the set of positions does not need to be decided ahead of time in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=318" target="_blank">00:05:18.880</a></span> | <span class="t">this scheme because we can fire off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=320" target="_blank">00:05:20.480</a></span> | <span class="t">a new vector for any position that you give us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=323" target="_blank">00:05:23.800</a></span> | <span class="t">But I think our second question remains pressing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=326" target="_blank">00:05:26.880</a></span> | <span class="t">Just as before, this scheme can hinder generalization to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=330" target="_blank">00:05:30.600</a></span> | <span class="t">new positions even for familiar phenomena in virtue of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=334" target="_blank">00:05:34.160</a></span> | <span class="t">the fact that we are taking those word representations and adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=337" target="_blank">00:05:37.440</a></span> | <span class="t">in these positional ones for different positions as equal partners,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=342" target="_blank">00:05:42.200</a></span> | <span class="t">as I said, and I think that makes it hard for models to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=345" target="_blank">00:05:45.200</a></span> | <span class="t">see that the same phrase could appear in multiple places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=349" target="_blank">00:05:49.680</a></span> | <span class="t">The third scheme is the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=351" target="_blank">00:05:51.840</a></span> | <span class="t">promising of the three that we're going to discuss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=354" target="_blank">00:05:54.080</a></span> | <span class="t">This is relative positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=356" target="_blank">00:05:56.440</a></span> | <span class="t">We're going to take a few steps to build up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=358" target="_blank">00:05:58.460</a></span> | <span class="t">an understanding of how the scheme works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=361" target="_blank">00:06:01.000</a></span> | <span class="t">Let's start with a reminder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=362" target="_blank">00:06:02.560</a></span> | <span class="t">This is a picture of the attention layer of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=366" target="_blank">00:06:06.440</a></span> | <span class="t">We have our three position sensitive inputs here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=369" target="_blank">00:06:09.600</a></span> | <span class="t">A input, B input, and C input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=371" target="_blank">00:06:11.840</a></span> | <span class="t">Remember, it's crucial that they be position sensitive because of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=376" target="_blank">00:06:16.080</a></span> | <span class="t">how much symmetry there is in these dot product attention mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=381" target="_blank">00:06:21.000</a></span> | <span class="t">Here's a reminder about how that calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=383" target="_blank">00:06:23.800</a></span> | <span class="t">works with respect to position C over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=387" target="_blank">00:06:27.040</a></span> | <span class="t">For positional encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=389" target="_blank">00:06:29.180</a></span> | <span class="t">we really just add in some new parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=391" target="_blank">00:06:31.360</a></span> | <span class="t">What I've depicted at the bottom of the slide here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=394" target="_blank">00:06:34.160</a></span> | <span class="t">the same calculation that's at the top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=396" target="_blank">00:06:36.600</a></span> | <span class="t">except now in two crucial places,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=398" target="_blank">00:06:38.980</a></span> | <span class="t">I have added in some new vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=401" target="_blank">00:06:41.760</a></span> | <span class="t">that we're going to learn representations for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=404" target="_blank">00:06:44.100</a></span> | <span class="t">Down in blue here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=405" target="_blank">00:06:45.560</a></span> | <span class="t">we have key representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=407" target="_blank">00:06:47.720</a></span> | <span class="t">which get added into this dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=410" target="_blank">00:06:50.300</a></span> | <span class="t">We up here in the final step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=412" target="_blank">00:06:52.320</a></span> | <span class="t">we have value representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=414" target="_blank">00:06:54.120</a></span> | <span class="t">which get added in to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=415" target="_blank">00:06:55.640</a></span> | <span class="t">this multiplied attention mechanism plus the thing we're attending to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=420" target="_blank">00:07:00.200</a></span> | <span class="t">Those are the new crucial parameters that we're adding in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=425" target="_blank">00:07:05.520</a></span> | <span class="t">The essential idea is that having done this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=428" target="_blank">00:07:08.640</a></span> | <span class="t">with all the position sensitivity that's going to be encoded in these vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=432" target="_blank">00:07:12.640</a></span> | <span class="t">we don't need these green representations here anymore to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=436" target="_blank">00:07:16.520</a></span> | <span class="t">positional information in them because that positional information is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=440" target="_blank">00:07:20.100</a></span> | <span class="t">now being introduced in the attention layer because we're going to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=444" target="_blank">00:07:24.320</a></span> | <span class="t">potentially new vectors for every combination of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=447" target="_blank">00:07:27.160</a></span> | <span class="t">position as indicated by these subscripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=450" target="_blank">00:07:30.680</a></span> | <span class="t">But that's only part of the story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=452" target="_blank">00:07:32.920</a></span> | <span class="t">I think the really powerful thing about this method is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=456" target="_blank">00:07:36.920</a></span> | <span class="t">the notion of having a positional encoding window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=460" target="_blank">00:07:40.440</a></span> | <span class="t">To illustrate that, I've repeated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=462" target="_blank">00:07:42.720</a></span> | <span class="t">the core calculation at the top here as a reminder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=465" target="_blank">00:07:45.840</a></span> | <span class="t">Now for my illustration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=467" target="_blank">00:07:47.440</a></span> | <span class="t">I'm going to set the window size to two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=470" target="_blank">00:07:50.080</a></span> | <span class="t">Here's the input sequence that we'll use as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=474" target="_blank">00:07:54.280</a></span> | <span class="t">Above that, I'm going to show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=476" target="_blank">00:07:56.160</a></span> | <span class="t">just integers corresponding to the positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=478" target="_blank">00:07:58.760</a></span> | <span class="t">Those aren't directly ingredients into the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=481" target="_blank">00:08:01.600</a></span> | <span class="t">but they will help us keep track of where we are in the calculations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=486" target="_blank">00:08:06.200</a></span> | <span class="t">To start the illustration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=487" target="_blank">00:08:07.800</a></span> | <span class="t">let's zoom in on position 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=490" target="_blank">00:08:10.640</a></span> | <span class="t">If we follow the letter of the definitions that I've offered so far for the key values here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=496" target="_blank">00:08:16.680</a></span> | <span class="t">we're going to have a vector A_44 corresponding to us attending from position 4 to position 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=504" target="_blank">00:08:24.280</a></span> | <span class="t">As part of creating this more limited window-based version of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=509" target="_blank">00:08:29.320</a></span> | <span class="t">we're actually going to map that into a single vector W_0 for the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">Now we travel to the position 1 to the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=518" target="_blank">00:08:38.400</a></span> | <span class="t">In this case, we would have a vector A_43 for the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=522" target="_blank">00:08:42.960</a></span> | <span class="t">But what we're going to do is map that into a single vector W_-1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=527" target="_blank">00:08:47.480</a></span> | <span class="t">corresponding to taking 3 minus 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=531" target="_blank">00:08:51.120</a></span> | <span class="t">When we travel one more to the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=533" target="_blank">00:08:53.400</a></span> | <span class="t">we get a position 4, 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=535" target="_blank">00:08:55.960</a></span> | <span class="t">but now we're going to map that to vector W_-2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=539" target="_blank">00:08:59.240</a></span> | <span class="t">again for the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=540" target="_blank">00:09:00.880</a></span> | <span class="t">Then because we set our window size to 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=544" target="_blank">00:09:04.080</a></span> | <span class="t">when we get all the way to that leftmost position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=547" target="_blank">00:09:07.040</a></span> | <span class="t">that's also just W_-2 again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=550" target="_blank">00:09:10.000</a></span> | <span class="t">4 minus 1, given the window size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=552" target="_blank">00:09:12.560</a></span> | <span class="t">takes us just to the maximum of this window,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=554" target="_blank">00:09:14.960</a></span> | <span class="t">in this case, minus 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=557" target="_blank">00:09:17.160</a></span> | <span class="t">Then a parallel thing happens when we travel to the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=560" target="_blank">00:09:20.200</a></span> | <span class="t">We go from 4 to 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=561" target="_blank">00:09:21.960</a></span> | <span class="t">that gives us vector W_1 for the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=565" target="_blank">00:09:25.000</a></span> | <span class="t">Then 4, 6 gives us W_2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=567" target="_blank">00:09:27.640</a></span> | <span class="t">Then when we get to the third position from our starting point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=571" target="_blank">00:09:31.240</a></span> | <span class="t">that again just flattens out to W_2 because of our window size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=576" target="_blank">00:09:36.480</a></span> | <span class="t">Actually represented in blue here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=579" target="_blank">00:09:39.240</a></span> | <span class="t">we have just a few vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=581" target="_blank">00:09:41.520</a></span> | <span class="t">the 0, 1, the minus 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=583" target="_blank">00:09:43.320</a></span> | <span class="t">and the minus 2, 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=584" target="_blank">00:09:44.800</a></span> | <span class="t">and then the 1, 2 vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=588" target="_blank">00:09:48.080</a></span> | <span class="t">as opposed to all the distinctions that are made with those alpha,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=591" target="_blank">00:09:51.880</a></span> | <span class="t">sub 4, 3, and 4, 2, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=595" target="_blank">00:09:55.160</a></span> | <span class="t">We're collapsing those down into a smaller number of vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=599" target="_blank">00:09:59.000</a></span> | <span class="t">corresponding to the window size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=601" target="_blank">00:10:01.320</a></span> | <span class="t">Then to continue the illustration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=603" target="_blank">00:10:03.480</a></span> | <span class="t">if we zoom in on position 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=605" target="_blank">00:10:05.680</a></span> | <span class="t">that would be vector A_3, 3 for the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=609" target="_blank">00:10:09.120</a></span> | <span class="t">but now that gets mapped to W_0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=611" target="_blank">00:10:11.520</a></span> | <span class="t">k, which is the same vector that we have up here in that 4, 4 position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=616" target="_blank">00:10:16.680</a></span> | <span class="t">A similar collapsing is going to happen down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=619" target="_blank">00:10:19.240</a></span> | <span class="t">When we move one to the left of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=620" target="_blank">00:10:20.920</a></span> | <span class="t">we get minus 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=621" target="_blank">00:10:21.960</a></span> | <span class="t">which is the same vector as we had up here just to the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=627" target="_blank">00:10:27.000</a></span> | <span class="t">Then we have the same thing over here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=629" target="_blank">00:10:29.240</a></span> | <span class="t">minus 2 corresponding to the same vector that we had above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=633" target="_blank">00:10:33.560</a></span> | <span class="t">That would continue and we have a parallel calculation for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=637" target="_blank">00:10:37.360</a></span> | <span class="t">the value parameters that you see in purple up here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=640" target="_blank">00:10:40.440</a></span> | <span class="t">the same notions of relative position and window size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=644" target="_blank">00:10:44.360</a></span> | <span class="t">We actually learn a relatively small number of position vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=649" target="_blank">00:10:49.040</a></span> | <span class="t">What we're doing is essentially giving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=652" target="_blank">00:10:52.200</a></span> | <span class="t">a small window relative notion of position that's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=656" target="_blank">00:10:56.240</a></span> | <span class="t">slide around and give us a lot of ability to generalize to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=660" target="_blank">00:11:00.040</a></span> | <span class="t">new positions based on combinations that we've seen before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=663" target="_blank">00:11:03.440</a></span> | <span class="t">possibly in other parts of these inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=667" target="_blank">00:11:07.280</a></span> | <span class="t">A final thing I'll say is that this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=670" target="_blank">00:11:10.800</a></span> | <span class="t">embedded in that full theory of attention that might have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=673" target="_blank">00:11:13.400</a></span> | <span class="t">a lot of learned parameters and might even be multi-headed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=676" target="_blank">00:11:16.320</a></span> | <span class="t">What I've depicted here is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=678" target="_blank">00:11:18.200</a></span> | <span class="t">the full calculation just to really give you all the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=681" target="_blank">00:11:21.760</a></span> | <span class="t">But again, the cognitive shortcut is that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=684" target="_blank">00:11:24.880</a></span> | <span class="t">the previous attention calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=687" target="_blank">00:11:27.740</a></span> | <span class="t">with these new positional elements added in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=690" target="_blank">00:11:30.480</a></span> | <span class="t">Again, a reminder, in this new mode,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=692" target="_blank">00:11:32.960</a></span> | <span class="t">we introduce position relativity in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=694" target="_blank">00:11:34.800</a></span> | <span class="t">the attention layer, not in the embedding layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=698" target="_blank">00:11:38.360</a></span> | <span class="t">Let's think about our two crucial questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=701" target="_blank">00:11:41.080</a></span> | <span class="t">First, we don't need to decide the set of positions ahead of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=704" target="_blank">00:11:44.240</a></span> | <span class="t">we just need to decide on the window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=706" target="_blank">00:11:46.480</a></span> | <span class="t">Then for a potentially extremely long string,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=710" target="_blank">00:11:50.000</a></span> | <span class="t">we're just sliding it around in it using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=712" target="_blank">00:11:52.320</a></span> | <span class="t">a relatively few number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=714" target="_blank">00:11:54.520</a></span> | <span class="t">positional vectors to keep track of relative position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=718" target="_blank">00:11:58.280</a></span> | <span class="t">I think we have also largely overcome the concern that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=722" target="_blank">00:12:02.200</a></span> | <span class="t">positional embeddings might hinder generalization to new positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=726" target="_blank">00:12:06.420</a></span> | <span class="t">After all, if you consider a phrase like the rock,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=730" target="_blank">00:12:10.220</a></span> | <span class="t">the core position vectors that are involved there are 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=735" target="_blank">00:12:15.160</a></span> | <span class="t">1, and minus 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=736" target="_blank">00:12:16.700</a></span> | <span class="t">no matter where this appears in the string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=739" target="_blank">00:12:19.280</a></span> | <span class="t">Now, depending on where it appears,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=741" target="_blank">00:12:21.240</a></span> | <span class="t">there will be other positional things that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=743" target="_blank">00:12:23.440</a></span> | <span class="t">happening and other information will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=745" target="_blank">00:12:25.280</a></span> | <span class="t">brought in as part of the calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=747" target="_blank">00:12:27.400</a></span> | <span class="t">But we do have this sense of constancy that will allow the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=751" target="_blank">00:12:31.180</a></span> | <span class="t">see that the rock is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=753" target="_blank">00:12:33.720</a></span> | <span class="t">essentially wherever it appears in the string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=756" target="_blank">00:12:36.920</a></span> | <span class="t">My hypothesis is that because we have overcome these two crucial limitations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=762" target="_blank">00:12:42.440</a></span> | <span class="t">relative positional encoding is a very good bet for how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=766" target="_blank">00:12:46.160</a></span> | <span class="t">do positional encoding in general in the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=768" target="_blank">00:12:48.880</a></span> | <span class="t">I believe that that is now well-supported</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=771" target="_blank">00:12:51.920</a></span> | <span class="t">by results across the field for the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JERXX2Byr90&t=776" target="_blank">00:12:56.600</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>