<html><head><title>AI Frontiers in Trust and Safety  Combatting Multifaceted Harm on Tinder at Scale: Vibhor Kumar</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AI Frontiers in Trust and Safety  Combatting Multifaceted Harm on Tinder at Scale: Vibhor Kumar</h2><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY"><img src="https://i.ytimg.com/vi_webp/kwnCvA9l-TY/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./kwnCvA9l-TY.html">Whisper Transcript</a> | <a href="./transcript_kwnCvA9l-TY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=14" target="_blank">00:00:14.100</a></span> | <span class="t">Hello, AI engineer World's Fair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=16" target="_blank">00:00:16.500</a></span> | <span class="t">My name is Vibor, VB for short, and I'm a senior AI engineer at Tinder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=21" target="_blank">00:00:21.220</a></span> | <span class="t">where I've been working on trust and safety for the last five years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=24" target="_blank">00:00:24.080</a></span> | <span class="t">I also work on and maintain some open-source AI projects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=26" target="_blank">00:00:26.940</a></span> | <span class="t">and I'm an advisor to a few AI startups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=29" target="_blank">00:00:29.700</a></span> | <span class="t">But we only have 15 minutes, so I'll jump right into it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=31" target="_blank">00:00:31.940</a></span> | <span class="t">maybe a little bit less than 15 now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=33" target="_blank">00:00:33.820</a></span> | <span class="t">Today I'll be talking about AI frontiers in trust and safety,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=36" target="_blank">00:00:36.920</a></span> | <span class="t">combating multifaceted harm on Tinder at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=40" target="_blank">00:00:40.420</a></span> | <span class="t">We'll first go over what trust and safety actually is for everyone in the audience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=43" target="_blank">00:00:43.660</a></span> | <span class="t">and more specifically what it means at Tinder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=45" target="_blank">00:00:45.940</a></span> | <span class="t">Then we'll go over the complex interaction between generative AI and trust and safety,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=50" target="_blank">00:00:50.060</a></span> | <span class="t">some of the problems which most people think about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=52" target="_blank">00:00:52.280</a></span> | <span class="t">but also some of the tremendous opportunities which will fundamentally change the space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=56" target="_blank">00:00:56.880</a></span> | <span class="t">Next we'll dive specifically into how to actually use LLMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=59" target="_blank">00:00:59.400</a></span> | <span class="t">for detecting trust and safety violations in text, covering the end-to-end stack from training to fine-tuning to productionization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=66" target="_blank">00:01:06.620</a></span> | <span class="t">and an overview of how we're doing this at Tinder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=69" target="_blank">00:01:09.000</a></span> | <span class="t">Finally we'll cover what the future looks like for this effort, and what we should be most excited about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=75" target="_blank">00:01:15.840</a></span> | <span class="t">So what is trust and safety actually?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=77" target="_blank">00:01:17.840</a></span> | <span class="t">It's not really something that's well defined, and it's more of an art than a science.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=81" target="_blank">00:01:21.860</a></span> | <span class="t">Oftentimes we have to make ethical judgment calls, but it's helpful to look at a breakdown of the goals of TNS by Del Harvey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=87" target="_blank">00:01:27.840</a></span> | <span class="t">who led trust and safety at Twitter for 13 years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=92" target="_blank">00:01:32.900</a></span> | <span class="t">Ultimately TNS is about preventing risk, reducing risk, detecting harm, and mitigating harm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=98" target="_blank">00:01:38.060</a></span> | <span class="t">The ultimate goal is to protect users and also the companies creating the products that they use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=103" target="_blank">00:01:43.220</a></span> | <span class="t">In this presentation we'll focus on the detecting harm part of it, where we devote a lot of our time to at Tinder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=110" target="_blank">00:01:50.060</a></span> | <span class="t">Speaking of which, as the largest dating app in the world, we encounter many, many types of violative behavior on Tinder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=116" target="_blank">00:01:56.000</a></span> | <span class="t">Here are some of the different categories, and a representative, synthetic textual example of each.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=121" target="_blank">00:02:01.280</a></span> | <span class="t">First, we have social media in your profile, a relatively minor, but rather prevalent violation of our private information policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=131" target="_blank">00:02:11.100</a></span> | <span class="t">And it's often done by low intent users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=133" target="_blank">00:02:13.500</a></span> | <span class="t">On the other side of the spectrum, we have things that are low prevalence, but high harm, things like hate speech, harassment, and pig butchering scams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=142" target="_blank">00:02:22.220</a></span> | <span class="t">So now that we have a sense of what TNS is, let's move on to the problems that generative AI is causing for the industry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=150" target="_blank">00:02:30.220</a></span> | <span class="t">One of the biggest problems is that Gen AI enables rapid generation of content, which makes it particularly easy to spread misinformation, propaganda, and low-quality spam by drowning out genuine content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=160" target="_blank">00:02:40.940</a></span> | <span class="t">That's what's known as the content pollution phenomenon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=163" target="_blank">00:02:43.940</a></span> | <span class="t">Additionally, there's some risk that platforms where the content is posted will essentially inherit the known copyright issues plaguing consumer Gen AI tools today, like OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=173" target="_blank">00:02:53.320</a></span> | <span class="t">Another problem is the accessibility of deepfake technology, which lowers the bar of entry to impersonation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=180" target="_blank">00:03:00.160</a></span> | <span class="t">and catfishing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=180" target="_blank">00:03:00.920</a></span> | <span class="t">This also enables malicious interpersonal harm, like in the case of revenge porn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=186" target="_blank">00:03:06.100</a></span> | <span class="t">Lastly, Gen AI can be used to scale up organized spam and scam operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=190" target="_blank">00:03:10.880</a></span> | <span class="t">Bad actors can rapidly create profiles by generating text and images, which means that existing signals, the ones we are using today, rely on similarity matching or hashes, will be increasingly less likely to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=201" target="_blank">00:03:21.880</a></span> | <span class="t">As a side note, this is why TNS teams dealing with automated fraud will need to increasingly take advantage of metadata-type signals associated with physical bottlenecks in meatspace, things like IP address, ISP information, and phone numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=218" target="_blank">00:03:38.640</a></span> | <span class="t">Now that we've covered some of the major problems that Gen AI is causing for the trust and safety industry, but there's actually some big reasons to be hopeful as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=235" target="_blank">00:03:55.400</a></span> | <span class="t">The first is that AI labs at both startups and big companies have already done the hard work of pre-training and open sourcing LLMs for everyone's benefit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=243" target="_blank">00:04:03.280</a></span> | <span class="t">Out of the box, these are really powerful in that they have latent semantic capability and often global language coverage as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=250" target="_blank">00:04:10.120</a></span> | <span class="t">Just try doing a few-shot example with prompt engineering LLM3 or Mist rule to detect any kind of violation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=257" target="_blank">00:04:17.000</a></span> | <span class="t">It usually works pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=258" target="_blank">00:04:18.840</a></span> | <span class="t">By fine-tuning these models, we can actually achieve state-of-the-art performance, in some cases better than few-shot GPT-4 performance on downstream textual detection tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=268" target="_blank">00:04:28.880</a></span> | <span class="t">And the act of fine-tuning has been made a lot easier because the open source community has produced libraries and tools that are relatively mature and maintained now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=277" target="_blank">00:04:37.880</a></span> | <span class="t">It's easier than ever to do fine-tuning, with the low-level details being abstracted away, and there's libraries built for every stage of model development and productionization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=285" target="_blank">00:04:45.520</a></span> | <span class="t">The next two opportunities I wanted to bring up are things that every trust and safety organization should be paying attention to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=292" target="_blank">00:04:52.360</a></span> | <span class="t">First is that we're fine-tuning rather than starting from scratch, and because of that, and the strong open source library support, we can actually accelerate the model development process from months to weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=305" target="_blank">00:05:05.960</a></span> | <span class="t">And additionally, one of the major reasons we see such a good performance from the fine-tuned open source LLMs is that, in general, model performance degrades quickly in trust and safety due to the adversarial nature of automated fraud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=317" target="_blank">00:05:17.640</a></span> | <span class="t">For example, whenever we release a rule to block one spam wave, bad actors are incentivized to, and ultimately do change their behavior to get around it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=327" target="_blank">00:05:27.480</a></span> | <span class="t">But, the generalization performance of LLMs slows the model degradation curve significantly, and we basically get this for free when we use LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=335" target="_blank">00:05:35.240</a></span> | <span class="t">Okay, so let's move on to some of the specifics of actually using LLMs for TNS violation detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=342" target="_blank">00:05:42.520</a></span> | <span class="t">The first major step is creating our training dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=345" target="_blank">00:05:45.160</a></span> | <span class="t">This is often the hardest part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=348" target="_blank">00:05:48.040</a></span> | <span class="t">That's due, in part, to how easy some of the later steps are, as we'll see, but it's also because smaller datasets are required for fine-tuning versus training from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=357" target="_blank">00:05:57.560</a></span> | <span class="t">In some cases, hundreds to thousands of examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=360" target="_blank">00:06:00.760</a></span> | <span class="t">And this necessitates creating a pretty high-quality dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=364" target="_blank">00:06:04.440</a></span> | <span class="t">What does this dataset look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=368" target="_blank">00:06:08.760</a></span> | <span class="t">Well, GPT-type LLMs, like the closed-source GPT-4 and Cloud Opus, and also like the open-source LAM and MISTRO models, can be thought of as a text-in to text-out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=378" target="_blank">00:06:18.040</a></span> | <span class="t">This is an approximate mental model, but it helps for understanding what the dataset should look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=382" target="_blank">00:06:22.520</a></span> | <span class="t">In our case, the text-in is the potentially violating text we want the model to be able to make a prediction on, wrapped by some prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=390" target="_blank">00:06:30.600</a></span> | <span class="t">And the text-out is a classification label or some extracted characters representing the violation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=397" target="_blank">00:06:37.320</a></span> | <span class="t">It's not a very complicated format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=399" target="_blank">00:06:39.320</a></span> | <span class="t">And there's a synthetic example for how we would detect users if they're underage in their written bio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=408" target="_blank">00:06:48.120</a></span> | <span class="t">As for actually assembling this dataset, it's possible to do it entirely by hand, because, again, we only need hundreds to thousands of examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=417" target="_blank">00:06:57.240</a></span> | <span class="t">One thing we've seen work quite well is actually to incorporate the largest LLMs in the data generation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=424" target="_blank">00:07:04.360</a></span> | <span class="t">We could generate purely synthetic training examples with few-shot prompting, but this introduces some risk that the data won't resemble the true data distribution, and it's platform-agnostic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=434" target="_blank">00:07:14.360</a></span> | <span class="t">What we found works better is to actually do a hybrid process where we can use GPT-4 with some clever prompting to ensure we don't run into the alignment built-in to actually make predictions on internal analytics data and to mine examples for our training set that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=449" target="_blank">00:07:29.560</a></span> | <span class="t">The cost of doing this is inversely proportional to the true prevalence of the harm, but that cost is still pretty negligible, and it provides a metric that alone is actually really helpful to track for TNS operations teams, anyways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=462" target="_blank">00:07:42.760</a></span> | <span class="t">It's possible to restrict the LLM calls to more likely candidates, and finally, when we get the mined examples from using GPT-4 effectively as an auto-moderation, we can then do a manual verification, fixing any mislabeled data, and making judgment calls where the label is more ambiguous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=480" target="_blank">00:08:00.760</a></span> | <span class="t">Okay, so I've got the training set, now let's talk about fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=486" target="_blank">00:08:06.760</a></span> | <span class="t">One question you might have is, why don't we just directly use the API LLMs, like GPT-4, to do this detection and production?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=492" target="_blank">00:08:12.760</a></span> | <span class="t">One fundamental reason is scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=494" target="_blank">00:08:14.760</a></span> | <span class="t">One fundamental reason is scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=496" target="_blank">00:08:16.760</a></span> | <span class="t">Tinder has a huge, real-time volume of profile interactions, and hitting OpenAI APIs that often doesn't scale in terms of cost, latency, and throughput.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=504" target="_blank">00:08:24.760</a></span> | <span class="t">The other reason is maintainability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=506" target="_blank">00:08:26.760</a></span> | <span class="t">By fine-tuning our own models, we have full control over the model weights and can re-fine-tune when production performance inevitably degrades, without needing to worry about changes in the underlying base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=517" target="_blank">00:08:37.760</a></span> | <span class="t">One additional benefit for us for classification tasks is that we have access to the output probability distribution, which means we can create, essentially, a confidence of the prediction, like in a traditional machine learning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=532" target="_blank">00:08:52.760</a></span> | <span class="t">As for actually doing the fine-tuning, well, the relatively mature open-source ecosystem makes this really easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=538" target="_blank">00:08:58.760</a></span> | <span class="t">Hugging face libraries make this as simple as writing a few hundred lines of code, without really needing to understand anything about deep learning or transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=545" target="_blank">00:09:05.760</a></span> | <span class="t">We've also had particular success building out training pipelines in notebooks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=550" target="_blank">00:09:10.760</a></span> | <span class="t">There are also libraries which abstract fine-tuning to just config files, like Axolotl, Ludwig, Lama Factory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=556" target="_blank">00:09:16.760</a></span> | <span class="t">And finally, there's managed solutions emerging that provide additional UI and dataset management support for rapid experimentation, like H2LM Studio and Predibase, the latter with whom we've had a lot of success with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=570" target="_blank">00:09:30.760</a></span> | <span class="t">So many of you are probably familiar with parameter efficient fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=576" target="_blank">00:09:36.760</a></span> | <span class="t">This is critical for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=578" target="_blank">00:09:38.760</a></span> | <span class="t">Low-rank adaptation, or LoRa, freezes the weights of the base model and can create a fine-tuned model while only really needing to learn megabytes of additional weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=587" target="_blank">00:09:47.760</a></span> | <span class="t">Accordingly, the fine-tuning can be done quickly and only on one or a few GPUs, which enables rapid experimentation and also unlocks using larger base models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=596" target="_blank">00:09:56.760</a></span> | <span class="t">PEFs of larger base models are more likely to be better than full fine-tunes of smaller base models, especially for classification tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=603" target="_blank">00:10:03.760</a></span> | <span class="t">We've had a lot of success also with QLaura, which unlocks fine-tuning on a single GPU, even the largest models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=610" target="_blank">00:10:10.760</a></span> | <span class="t">Lastly, one of the biggest reasons to use LoRa is that we can take advantage of the massive inference optimizations, as we'll see now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=617" target="_blank">00:10:17.760</a></span> | <span class="t">So, production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=622" target="_blank">00:10:22.760</a></span> | <span class="t">In production, we use LoRaX, which is an open-source framework that allows users to efficiently serve thousands of fine-tuned models on a single GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=629" target="_blank">00:10:29.760</a></span> | <span class="t">It exploits the fact that a fine-tuned LoRa adapter, which is just a single fine-tuned model, is only a few megabytes in size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=636" target="_blank">00:10:36.760</a></span> | <span class="t">Many adapters can be efficiently served jointly by simply shuffling and batching adapters and requests for efficient serving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=643" target="_blank">00:10:43.760</a></span> | <span class="t">In practice, this means that the marginal cost of serving a new adapter on the same base model is virtually zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=648" target="_blank">00:10:48.760</a></span> | <span class="t">I just want to let the implication of that sink in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=651" target="_blank">00:10:51.760</a></span> | <span class="t">It means that we can train adapters for the many, many different types of trust and safety violations possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=656" target="_blank">00:10:56.760</a></span> | <span class="t">Hate speech, promotion, catfishing, pig butchering scams, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=660" target="_blank">00:11:00.760</a></span> | <span class="t">And we can serve all those adapters on one or even a small set of GPUs and not need to worry about horizontal scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=667" target="_blank">00:11:07.760</a></span> | <span class="t">Incorporating a new adapter in production is as simple as storing the megabytes of weights on some file system and modifying a request to the LoRaX client.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=675" target="_blank">00:11:15.760</a></span> | <span class="t">Special thanks to the Predibase team who developed and maintains LoRaX and have provided us a lot of support in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=682" target="_blank">00:11:22.760</a></span> | <span class="t">The optimizations in LoRaX basically enable us to do real-time inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=687" target="_blank">00:11:27.760</a></span> | <span class="t">And at Tinder, that's critical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=689" target="_blank">00:11:29.760</a></span> | <span class="t">We can support on 7 billion parameter models, tens of QPS on 100-ish milliseconds of latency on A10 GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=696" target="_blank">00:11:36.760</a></span> | <span class="t">This is good enough for some use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=699" target="_blank">00:11:39.760</a></span> | <span class="t">And for those other use cases, the high-frequency domains,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=702" target="_blank">00:11:42.760</a></span> | <span class="t">we can further reduce throughput by gating requests with heuristics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=707" target="_blank">00:11:47.760</a></span> | <span class="t">For example, for detecting social media in profiles,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=711" target="_blank">00:11:51.760</a></span> | <span class="t">we can make predictions only on bios that contain some word that's not in a dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=717" target="_blank">00:11:57.760</a></span> | <span class="t">And then we're also exploring doing cascade classification through some distillation process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=721" target="_blank">00:12:01.760</a></span> | <span class="t">where we train adapters on smaller base models optimizing for recall,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=725" target="_blank">00:12:05.760</a></span> | <span class="t">and only call the larger base model adapters when the smaller one gives a high enough score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=730" target="_blank">00:12:10.760</a></span> | <span class="t">Another advantage for us in this TNS space is, in general, LLM outputs are computationally expensive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=737" target="_blank">00:12:17.760</a></span> | <span class="t">because the generation is done autoregressively, one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=741" target="_blank">00:12:21.760</a></span> | <span class="t">But classification or extraction tasks require only exactly one token or a few tokens to output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=748" target="_blank">00:12:28.760</a></span> | <span class="t">which means our time to prediction is low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=750" target="_blank">00:12:30.760</a></span> | <span class="t">And compared to NLP models of the past, we're seeing that we can get massive improvements in precision and recall,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=759" target="_blank">00:12:39.760</a></span> | <span class="t">just due to the much higher latent semantic capability of today's LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=764" target="_blank">00:12:44.760</a></span> | <span class="t">We can achieve near 100% recall in simpler tasks like social handle detection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=768" target="_blank">00:12:48.760</a></span> | <span class="t">and significant improvements over the baseline in more semantically complex tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=774" target="_blank">00:12:54.760</a></span> | <span class="t">The other huge benefit that we get is way better generalization performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=777" target="_blank">00:12:57.760</a></span> | <span class="t">which I've talked about before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=779" target="_blank">00:12:59.760</a></span> | <span class="t">In particular, this is important for TNS because it's an adversarial game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=785" target="_blank">00:13:05.760</a></span> | <span class="t">Bad actors and other violative users always try to avoid detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=788" target="_blank">00:13:08.760</a></span> | <span class="t">For example, with intentional typos, mixing letters and numbers and innuendos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=792" target="_blank">00:13:12.760</a></span> | <span class="t">But LLMs are much better at making sense of these, meaning that these new adapter-based models get stale less quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=798" target="_blank">00:13:18.760</a></span> | <span class="t">than other traditional machine learning models and are a better defense against harm in the long run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=803" target="_blank">00:13:23.760</a></span> | <span class="t">So where do we go from here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=807" target="_blank">00:13:27.760</a></span> | <span class="t">We're interested in the growing work on non-textual modalities and how we can leverage that for detection purposes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=813" target="_blank">00:13:33.760</a></span> | <span class="t">For example, we can use pre-trained visual language models like LAVA to do explicit image detection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=818" target="_blank">00:13:38.760</a></span> | <span class="t">and that's an active area of exploration for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=821" target="_blank">00:13:41.760</a></span> | <span class="t">Overall, we're excited about rapidly training adapters for detecting harm along the long tail of TNS violations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=829" target="_blank">00:13:49.760</a></span> | <span class="t">We can create high-quality data sets with trust and safety operations and policy experts with that AI in the loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=835" target="_blank">00:13:55.760</a></span> | <span class="t">We can automate training and retraining pipelines for fine-tuning adapters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=839" target="_blank">00:13:59.760</a></span> | <span class="t">And we can take advantage of Lorax to slot in new adapters for inference with low marginal cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=845" target="_blank">00:14:05.760</a></span> | <span class="t">Ultimately, we can build a next-generation defensive moat against harm that takes advantage of the Gen.AI landscape today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=851" target="_blank">00:14:11.760</a></span> | <span class="t">ultimately leading to a safer, healthier platform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=855" target="_blank">00:14:15.760</a></span> | <span class="t">Thanks for listening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=856" target="_blank">00:14:16.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=857" target="_blank">00:14:17.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=858" target="_blank">00:14:18.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=859" target="_blank">00:14:19.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=860" target="_blank">00:14:20.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=861" target="_blank">00:14:21.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=862" target="_blank">00:14:22.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=869" target="_blank">00:14:29.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=870" target="_blank">00:14:30.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=871" target="_blank">00:14:31.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=872" target="_blank">00:14:32.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=873" target="_blank">00:14:33.760</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kwnCvA9l-TY&t=874" target="_blank">00:14:34.760</a></span> | <span class="t">.</span></div></div></body></html>