<html><head><title>[Paper Club] BERT: Bidirectional Encoder Representations from Transformers</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>[Paper Club] BERT: Bidirectional Encoder Representations from Transformers</h2><a href="https://www.youtube.com/watch?v=V64q3p7DNjc"><img src="https://i.ytimg.com/vi_webp/V64q3p7DNjc/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./V64q3p7DNjc.html">Whisper Transcript</a> | <a href="./transcript_V64q3p7DNjc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Let's go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=0" target="_blank">00:00:00.840</a></span> | <span class="t">I'll kill time while Eric figures out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=10" target="_blank">00:00:10.600</a></span> | <span class="t">how to share his computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=13" target="_blank">00:00:13.260</a></span> | <span class="t">So why do we pick Bert today?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=17" target="_blank">00:00:17.280</a></span> | <span class="t">I'm kind of curious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=20" target="_blank">00:00:20.360</a></span> | <span class="t">- Yeah, 'cause I'm working on a text classification problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=25" target="_blank">00:00:25.380</a></span> | <span class="t">at work and just wanna get the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=28" target="_blank">00:00:28.440</a></span> | <span class="t">- Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=29" target="_blank">00:00:29.280</a></span> | <span class="t">- Yeah, I wonder if there is a microservice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=35" target="_blank">00:00:35.040</a></span> | <span class="t">that is easy to set up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=37" target="_blank">00:00:37.040</a></span> | <span class="t">I wonder if OpenPipe does this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=39" target="_blank">00:00:39.040</a></span> | <span class="t">that only mirrors a structured output GPT-4-O call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=44" target="_blank">00:00:44.040</a></span> | <span class="t">and just mirrors it until it has enough data for Bert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">and then just switches you to Bert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=51" target="_blank">00:00:51.760</a></span> | <span class="t">- What do you mean by mirrors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=56" target="_blank">00:00:56.880</a></span> | <span class="t">- Shadows it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=58" target="_blank">00:00:58.880</a></span> | <span class="t">- Basically, you just use it at the start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=61" target="_blank">00:01:01.840</a></span> | <span class="t">until you have enough data and then do URLs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=64" target="_blank">00:01:04.080</a></span> | <span class="t">and then just swap it, so it's cheap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=66" target="_blank">00:01:06.360</a></span> | <span class="t">- Yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=67" target="_blank">00:01:07.720</a></span> | <span class="t">- I think OpenPipe actually does that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=69" target="_blank">00:01:09.360</a></span> | <span class="t">but I don't think it's as automatic and as seamless</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=71" target="_blank">00:01:11.520</a></span> | <span class="t">as how your vision is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=74" target="_blank">00:01:14.520</a></span> | <span class="t">- Yeah, I mean, like the amount of times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=76" target="_blank">00:01:16.200</a></span> | <span class="t">people suggest using Bert for classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=78" target="_blank">00:01:18.840</a></span> | <span class="t">it's cheap, like it's a path that is worth taking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=81" target="_blank">00:01:21.600</a></span> | <span class="t">Oh, Eric's rejoining, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=84" target="_blank">00:01:24.440</a></span> | <span class="t">- Bart and T5 are good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=86" target="_blank">00:01:26.680</a></span> | <span class="t">like they also got a host, you need to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=89" target="_blank">00:01:29.640</a></span> | <span class="t">like one thing is just switch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=90" target="_blank">00:01:30.960</a></span> | <span class="t">but like for OpenAI or routing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=92" target="_blank">00:01:32.680</a></span> | <span class="t">you have endpoints and hosted model and all that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=96" target="_blank">00:01:36.520</a></span> | <span class="t">So to do Bert, like you gotta go deploy it somewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=99" target="_blank">00:01:39.840</a></span> | <span class="t">and handle that part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=102" target="_blank">00:01:42.160</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=104" target="_blank">00:01:44.000</a></span> | <span class="t">All right, Eric, we can see you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=107" target="_blank">00:01:47.600</a></span> | <span class="t">- Okay, perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=108" target="_blank">00:01:48.880</a></span> | <span class="t">So we'll start by going through the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=116" target="_blank">00:01:56.200</a></span> | <span class="t">and then there's additional material out there on Bert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=121" target="_blank">00:02:01.200</a></span> | <span class="t">So depending how much time we have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=125" target="_blank">00:02:05.720</a></span> | <span class="t">we can look at some of the other things out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=129" target="_blank">00:02:09.080</a></span> | <span class="t">So first of all, Bert stands for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=135" target="_blank">00:02:15.440</a></span> | <span class="t">bi-directional encoder representations from transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=139" target="_blank">00:02:19.400</a></span> | <span class="t">So this is one of the first transformer papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=144" target="_blank">00:02:24.560</a></span> | <span class="t">like I believe it was about a year after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=148" target="_blank">00:02:28.520</a></span> | <span class="t">the attention is everything you need paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=151" target="_blank">00:02:31.840</a></span> | <span class="t">You can see it's from 2019.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=154" target="_blank">00:02:34.600</a></span> | <span class="t">So ancient history in terms of deep learning and NLP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=159" target="_blank">00:02:39.600</a></span> | <span class="t">but still useful for a lot of use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=166" target="_blank">00:02:46.600</a></span> | <span class="t">And just for a little bit more context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=173" target="_blank">00:02:53.560</a></span> | <span class="t">So after, so this was a Google paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=178" target="_blank">00:02:58.560</a></span> | <span class="t">and after they trained and released Bert,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=182" target="_blank">00:03:02.240</a></span> | <span class="t">they started using it in Google search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=186" target="_blank">00:03:06.480</a></span> | <span class="t">And so it provided context for search results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=191" target="_blank">00:03:11.480</a></span> | <span class="t">so that there's some examples maybe we can look at later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=196" target="_blank">00:03:16.960</a></span> | <span class="t">but it would, if you gave like a query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=200" target="_blank">00:03:20.240</a></span> | <span class="t">that could mean a couple of different things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=202" target="_blank">00:03:22.800</a></span> | <span class="t">they use this model to discriminate between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=206" target="_blank">00:03:26.560</a></span> | <span class="t">So let's, I guess, just walk down through the paper here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=214" target="_blank">00:03:34.240</a></span> | <span class="t">One thing to note is that there is this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=222" target="_blank">00:03:42.840</a></span> | <span class="t">two different kinds of approaches early on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=227" target="_blank">00:03:47.520</a></span> | <span class="t">feature-based and fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=230" target="_blank">00:03:50.600</a></span> | <span class="t">And so the like models like Elmo were feature-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=235" target="_blank">00:03:55.600</a></span> | <span class="t">where they had tasks specific architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=240" target="_blank">00:04:00.600</a></span> | <span class="t">that were built into the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=243" target="_blank">00:04:03.080</a></span> | <span class="t">Whereas models like Bert and GPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=247" target="_blank">00:04:07.520</a></span> | <span class="t">were more used to fine tuning approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=250" target="_blank">00:04:10.720</a></span> | <span class="t">where they just, as more general purpose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=253" target="_blank">00:04:13.960</a></span> | <span class="t">they just trained one model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=255" target="_blank">00:04:15.880</a></span> | <span class="t">and then you could fine tune it after the fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=258" target="_blank">00:04:18.520</a></span> | <span class="t">for your particular tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=262" target="_blank">00:04:22.440</a></span> | <span class="t">They talk a little bit here about the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=269" target="_blank">00:04:29.360</a></span> | <span class="t">one of the limitations of standard language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=273" target="_blank">00:04:33.560</a></span> | <span class="t">is that they're unidirectional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=276" target="_blank">00:04:36.120</a></span> | <span class="t">So what that means is for like GPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=279" target="_blank">00:04:39.600</a></span> | <span class="t">and many other models out there now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=284" target="_blank">00:04:44.600</a></span> | <span class="t">they only look forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=286" target="_blank">00:04:46.640</a></span> | <span class="t">And so they take a sequence of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=289" target="_blank">00:04:49.320</a></span> | <span class="t">and try to predict the next word in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=293" target="_blank">00:04:53.080</a></span> | <span class="t">However, Bert also goes backwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=297" target="_blank">00:04:57.520</a></span> | <span class="t">So it takes like in the training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=302" target="_blank">00:05:02.080</a></span> | <span class="t">it can also start at say the end of a piece of text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=304" target="_blank">00:05:04.960</a></span> | <span class="t">and try to predict the previous word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=307" target="_blank">00:05:07.080</a></span> | <span class="t">And so we'll talk a little bit how they avoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=312" target="_blank">00:05:12.960</a></span> | <span class="t">like contamination of the prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=316" target="_blank">00:05:16.680</a></span> | <span class="t">because obviously if you're training from back to front,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=319" target="_blank">00:05:19.800</a></span> | <span class="t">you get a peek at what the words are coming up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=323" target="_blank">00:05:23.880</a></span> | <span class="t">- So a lot of this section</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=327" target="_blank">00:05:27.920</a></span> | <span class="t">was pre-decoder only transformers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=331" target="_blank">00:05:31.880</a></span> | <span class="t">So a lot of what they reference is like RNNs, GRUs, LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=336" target="_blank">00:05:36.480</a></span> | <span class="t">So like pre-transformer stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=338" target="_blank">00:05:38.400</a></span> | <span class="t">you only get one pass and then they're like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=340" target="_blank">00:05:40.920</a></span> | <span class="t">"Crazy idea if you look at it from front to back."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=343" target="_blank">00:05:43.800</a></span> | <span class="t">Like, so a lot of the training tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=346" target="_blank">00:05:46.440</a></span> | <span class="t">was like classification, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=347" target="_blank">00:05:47.920</a></span> | <span class="t">When humans classify something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=349" target="_blank">00:05:49.720</a></span> | <span class="t">we don't do it token by token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=351" target="_blank">00:05:51.360</a></span> | <span class="t">We listen to the whole sentence, then we classify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=353" target="_blank">00:05:53.880</a></span> | <span class="t">So this was more so like LSTM, RNN era, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=358" target="_blank">00:05:58.880</a></span> | <span class="t">- Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=361" target="_blank">00:06:01.240</a></span> | <span class="t">Yeah, it's a very early paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=369" target="_blank">00:06:09.440</a></span> | <span class="t">So let's see, we talked about bi-directional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=372" target="_blank">00:06:12.720</a></span> | <span class="t">And then, yeah, so it's fine-tuned versus feature-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=382" target="_blank">00:06:22.280</a></span> | <span class="t">And then they show the effectiveness on BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=385" target="_blank">00:06:25.240</a></span> | <span class="t">on 11 different NLP tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=388" target="_blank">00:06:28.360</a></span> | <span class="t">I guess one call out on the related work is just ELMo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=396" target="_blank">00:06:36.600</a></span> | <span class="t">So ELMo was another model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=399" target="_blank">00:06:39.680</a></span> | <span class="t">I believe it was also by Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=401" target="_blank">00:06:41.600</a></span> | <span class="t">If anyone knows for sure, feel free to correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=405" target="_blank">00:06:45.840</a></span> | <span class="t">But they used different representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=410" target="_blank">00:06:50.840</a></span> | <span class="t">for the same word in different contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=415" target="_blank">00:06:55.320</a></span> | <span class="t">And so like the word stick, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=420" target="_blank">00:07:00.320</a></span> | <span class="t">you could say that means I'm going to chase a dog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=425" target="_blank">00:07:05.760</a></span> | <span class="t">with a stick, or it could be like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=427" target="_blank">00:07:07.960</a></span> | <span class="t">hey, let's stick to the material that we're talking about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=431" target="_blank">00:07:11.840</a></span> | <span class="t">or maybe some other words or contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=434" target="_blank">00:07:14.840</a></span> | <span class="t">And so those can mean different things, the same token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=439" target="_blank">00:07:19.760</a></span> | <span class="t">And because of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=440" target="_blank">00:07:20.640</a></span> | <span class="t">they want to use like different representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=443" target="_blank">00:07:23.320</a></span> | <span class="t">even though it's the same word in English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=446" target="_blank">00:07:26.160</a></span> | <span class="t">And so BERT leverages that feature as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=450" target="_blank">00:07:30.120</a></span> | <span class="t">- So slight correction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=453" target="_blank">00:07:33.040</a></span> | <span class="t">ELMo is from Allen Institute and University of Washington.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=457" target="_blank">00:07:37.160</a></span> | <span class="t">And then further context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=458" target="_blank">00:07:38.720</a></span> | <span class="t">So one of the best BERT adaptations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=462" target="_blank">00:07:42.440</a></span> | <span class="t">I think in 2019 was Roberta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=464" target="_blank">00:07:44.680</a></span> | <span class="t">Roberta is like BERT, but make it good and bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=468" target="_blank">00:07:48.440</a></span> | <span class="t">And it was also from the same team</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=471" target="_blank">00:07:51.320</a></span> | <span class="t">at University of Washington and Facebook, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=474" target="_blank">00:07:54.320</a></span> | <span class="t">But ELMo was just, yeah, it wasn't from Google,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=477" target="_blank">00:07:57.480</a></span> | <span class="t">but same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=478" target="_blank">00:07:58.480</a></span> | <span class="t">It went from like one hot encoding bag of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=480" target="_blank">00:08:00.640</a></span> | <span class="t">to ELMo was really popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=482" target="_blank">00:08:02.760</a></span> | <span class="t">for like a bunch of Kaggle competitions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=485" target="_blank">00:08:05.160</a></span> | <span class="t">where you needed good embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=486" target="_blank">00:08:06.920</a></span> | <span class="t">And then for embeddings, it's not Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=489" target="_blank">00:08:09.720</a></span> | <span class="t">- Great, thanks for the additional info.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=495" target="_blank">00:08:15.520</a></span> | <span class="t">So let's take a stop here to look at this diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=507" target="_blank">00:08:27.480</a></span> | <span class="t">So this is the BERT like architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=512" target="_blank">00:08:32.480</a></span> | <span class="t">Essentially, you can see this pink row down here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=518" target="_blank">00:08:38.080</a></span> | <span class="t">is a set of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=519" target="_blank">00:08:39.480</a></span> | <span class="t">Let me see if I can zoom in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=524" target="_blank">00:08:44.320</a></span> | <span class="t">I'm not sure why I can't zoom in while I'm sharing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=536" target="_blank">00:08:56.360</a></span> | <span class="t">but anyway, this token is a classifier token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=541" target="_blank">00:09:01.360</a></span> | <span class="t">And then you see like tokens one through N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=546" target="_blank">00:09:06.800</a></span> | <span class="t">So those are the first sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=550" target="_blank">00:09:10.640</a></span> | <span class="t">And then there's a separator token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=553" target="_blank">00:09:13.160</a></span> | <span class="t">And then there's another token one through M.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=558" target="_blank">00:09:18.000</a></span> | <span class="t">And the reason that BERT has this structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=562" target="_blank">00:09:22.200</a></span> | <span class="t">is because one of the tasks it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=564" target="_blank">00:09:24.520</a></span> | <span class="t">is sentence classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=569" target="_blank">00:09:29.520</a></span> | <span class="t">Basically, it can take two sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=572" target="_blank">00:09:32.600</a></span> | <span class="t">and determine if one of the sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=576" target="_blank">00:09:36.440</a></span> | <span class="t">seems like it follows the other one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=579" target="_blank">00:09:39.120</a></span> | <span class="t">And then you see, so this is a pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=585" target="_blank">00:09:45.240</a></span> | <span class="t">So this is trained on, well, what was at that point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=589" target="_blank">00:09:49.280</a></span> | <span class="t">a large amount of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=591" target="_blank">00:09:51.640</a></span> | <span class="t">And then these are the different fine tunes of that model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=595" target="_blank">00:09:55.120</a></span> | <span class="t">for different benchmarks, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=598" target="_blank">00:09:58.600</a></span> | <span class="t">And FYI, I'm not looking at the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=602" target="_blank">00:10:02.240</a></span> | <span class="t">So if there's anything in there, I'm not seeing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=604" target="_blank">00:10:04.800</a></span> | <span class="t">I can answer questions later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=607" target="_blank">00:10:07.160</a></span> | <span class="t">or if someone just wants to unmute, feel free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=610" target="_blank">00:10:10.040</a></span> | <span class="t">So here's those two tokens I mentioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=614" target="_blank">00:10:14.720</a></span> | <span class="t">that you might not have been able to see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=616" target="_blank">00:10:16.840</a></span> | <span class="t">the classifier token and then the separator token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=620" target="_blank">00:10:20.200</a></span> | <span class="t">The classifier token tells it like what task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=624" target="_blank">00:10:24.360</a></span> | <span class="t">it's supposed to be performing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=626" target="_blank">00:10:26.560</a></span> | <span class="t">So they talk about how there's two steps in the framework,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=631" target="_blank">00:10:31.640</a></span> | <span class="t">retraining and fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=633" target="_blank">00:10:33.840</a></span> | <span class="t">Probably a lot of people are familiar with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=638" target="_blank">00:10:38.000</a></span> | <span class="t">So I won't go into too much detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=641" target="_blank">00:10:41.560</a></span> | <span class="t">This is interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=643" target="_blank">00:10:43.960</a></span> | <span class="t">So this is 2019 numbers of what was at that point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=649" target="_blank">00:10:49.640</a></span> | <span class="t">a large model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=651" target="_blank">00:10:51.080</a></span> | <span class="t">You can see the base model was 110 million parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=656" target="_blank">00:10:56.080</a></span> | <span class="t">and the large model, which was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=658" target="_blank">00:10:58.680</a></span> | <span class="t">I saw someone referred to as like gigantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=662" target="_blank">00:11:02.240</a></span> | <span class="t">or like unbelievably large or something like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=666" target="_blank">00:11:06.040</a></span> | <span class="t">is 340 million parameters, which is, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=670" target="_blank">00:11:10.600</a></span> | <span class="t">like 5% the size of like a LLAMA7B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=675" target="_blank">00:11:15.600</a></span> | <span class="t">or something along those lines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=678" target="_blank">00:11:18.000</a></span> | <span class="t">So what at the time seemed very large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=681" target="_blank">00:11:21.600</a></span> | <span class="t">in retrospect, isn't really.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=685" target="_blank">00:11:25.160</a></span> | <span class="t">And that also goes for the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=689" target="_blank">00:11:29.440</a></span> | <span class="t">Do they have it in here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=694" target="_blank">00:11:34.640</a></span> | <span class="t">Well, there's two training sets that they used for it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=700" target="_blank">00:11:40.800</a></span> | <span class="t">One was a, all of Wikipedia, the English version,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=706" target="_blank">00:11:46.320</a></span> | <span class="t">which was like 800 million words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=708" target="_blank">00:11:48.560</a></span> | <span class="t">And then I think a set of books,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=711" target="_blank">00:11:51.560</a></span> | <span class="t">which was 2.5 billion words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=713" target="_blank">00:11:53.720</a></span> | <span class="t">And again, those data sizes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=718" target="_blank">00:11:58.680</a></span> | <span class="t">while they could have been large for the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=721" target="_blank">00:12:01.160</a></span> | <span class="t">currently are relatively small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=723" target="_blank">00:12:03.920</a></span> | <span class="t">Typically, like at least for frontier models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=728" target="_blank">00:12:08.240</a></span> | <span class="t">you're talking about low trillions of tokens to train them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=733" target="_blank">00:12:13.760</a></span> | <span class="t">So here they talk about what I mentioned earlier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=738" target="_blank">00:12:18.760</a></span> | <span class="t">about having a pair of sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=743" target="_blank">00:12:23.240</a></span> | <span class="t">where you can have a question and answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=745" target="_blank">00:12:25.360</a></span> | <span class="t">in one token sequence with the separator token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=749" target="_blank">00:12:29.280</a></span> | <span class="t">in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=750" target="_blank">00:12:30.200</a></span> | <span class="t">And so let's go down and talk about pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=759" target="_blank">00:12:39.800</a></span> | <span class="t">And here we get to their answer to the left-to-right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=764" target="_blank">00:12:44.800</a></span> | <span class="t">versus right-to-left dilemma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=773" target="_blank">00:12:53.280</a></span> | <span class="t">So because it's bi-directional,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=776" target="_blank">00:12:56.160</a></span> | <span class="t">potentially each word could see itself in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=781" target="_blank">00:13:01.160</a></span> | <span class="t">And so what they ended up doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=784" target="_blank">00:13:04.600</a></span> | <span class="t">was to mask 15% of the words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=790" target="_blank">00:13:10.600</a></span> | <span class="t">so instead of having the actual word in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=795" target="_blank">00:13:15.600</a></span> | <span class="t">they have this mask token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=798" target="_blank">00:13:18.200</a></span> | <span class="t">And then they try to predict that word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=802" target="_blank">00:13:22.720</a></span> | <span class="t">either going forward or backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=804" target="_blank">00:13:24.920</a></span> | <span class="t">and do the, you know, score the training on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=811" target="_blank">00:13:31.760</a></span> | <span class="t">They do have an issue that, let's see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=819" target="_blank">00:13:39.520</a></span> | <span class="t">that since the mask token does not appear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=823" target="_blank">00:13:43.240</a></span> | <span class="t">during fine tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=824" target="_blank">00:13:44.560</a></span> | <span class="t">they have to compensate for that in the pre-training step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=831" target="_blank">00:13:51.200</a></span> | <span class="t">So in order to do that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=832" target="_blank">00:13:52.320</a></span> | <span class="t">they don't always replace the mask words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=835" target="_blank">00:13:55.200</a></span> | <span class="t">with this mask token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=837" target="_blank">00:13:57.320</a></span> | <span class="t">They do 80% of the time, 10% of the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=842" target="_blank">00:14:02.120</a></span> | <span class="t">they just stick in a random token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=844" target="_blank">00:14:04.480</a></span> | <span class="t">or anything in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=846" target="_blank">00:14:06.600</a></span> | <span class="t">And then another 10% of the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=848" target="_blank">00:14:08.800</a></span> | <span class="t">they just leave it unchanged.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=851" target="_blank">00:14:11.160</a></span> | <span class="t">And so that helps during the fine tuning stage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=855" target="_blank">00:14:15.000</a></span> | <span class="t">so that fine tuning doesn't expect these mask tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=859" target="_blank">00:14:19.320</a></span> | <span class="t">like all the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=860" target="_blank">00:14:20.520</a></span> | <span class="t">And then the other task they give it during pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=865" target="_blank">00:14:25.720</a></span> | <span class="t">is this next sentence prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=868" target="_blank">00:14:28.640</a></span> | <span class="t">That is the, you saw the two sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=875" target="_blank">00:14:35.320</a></span> | <span class="t">and whether they are related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=877" target="_blank">00:14:37.080</a></span> | <span class="t">So that is, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=882" target="_blank">00:14:42.080</a></span> | <span class="t">that's the sentence A, separator, sentence B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=887" target="_blank">00:14:47.400</a></span> | <span class="t">And then they have the 50% split of training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=896" target="_blank">00:14:56.120</a></span> | <span class="t">where half of it is labeled as is next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=901" target="_blank">00:15:01.960</a></span> | <span class="t">and half of it is labeled as not next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=904" target="_blank">00:15:04.360</a></span> | <span class="t">So you can imagine a initial sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=909" target="_blank">00:15:09.360</a></span> | <span class="t">of I walked down to the store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=913" target="_blank">00:15:13.040</a></span> | <span class="t">One that might be marked as is next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=917" target="_blank">00:15:17.360</a></span> | <span class="t">is something like I bought a burrito</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=920" target="_blank">00:15:20.840</a></span> | <span class="t">and something that might be not next is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=924" target="_blank">00:15:24.960</a></span> | <span class="t">you know, salamanders have many spots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=927" target="_blank">00:15:27.160</a></span> | <span class="t">or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=928" target="_blank">00:15:28.320</a></span> | <span class="t">And so this part of the pre-training figures out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=932" target="_blank">00:15:32.120</a></span> | <span class="t">which of those sentences follow each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=935" target="_blank">00:15:35.000</a></span> | <span class="t">Here, oh, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=941" target="_blank">00:15:41.080</a></span> | <span class="t">So here's what I was talking about earlier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=942" target="_blank">00:15:42.880</a></span> | <span class="t">the book corpus, 800 million words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=944" target="_blank">00:15:44.600</a></span> | <span class="t">And I guess I had them flipped Wikipedia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=946" target="_blank">00:15:46.680</a></span> | <span class="t">as the 2.5 billion words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=949" target="_blank">00:15:49.360</a></span> | <span class="t">And this shows how the input representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=958" target="_blank">00:15:58.360</a></span> | <span class="t">So there's a couple of different layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=962" target="_blank">00:16:02.800</a></span> | <span class="t">and they just add these layers up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=964" target="_blank">00:16:04.960</a></span> | <span class="t">to come up with the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=967" target="_blank">00:16:07.320</a></span> | <span class="t">So the top layer here is the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=972" target="_blank">00:16:12.320</a></span> | <span class="t">So it's the vector representation of that particular word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=976" target="_blank">00:16:16.960</a></span> | <span class="t">The next is the segment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=980" target="_blank">00:16:20.080</a></span> | <span class="t">So this splits it up between sentence A and sentence B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=984" target="_blank">00:16:24.840</a></span> | <span class="t">Each one of those has a different factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=988" target="_blank">00:16:28.680</a></span> | <span class="t">that gets added into the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=991" target="_blank">00:16:31.840</a></span> | <span class="t">to help the model distinguish between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=994" target="_blank">00:16:34.520</a></span> | <span class="t">And then finally as the positional embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=996" target="_blank">00:16:36.680</a></span> | <span class="t">So each place in the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1001" target="_blank">00:16:41.680</a></span> | <span class="t">it's its own value that when it's added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1006" target="_blank">00:16:46.600</a></span> | <span class="t">to the other layers helps the model to distinguish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1012" target="_blank">00:16:52.600</a></span> | <span class="t">like does my come before dog or vice versa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1017" target="_blank">00:16:57.600</a></span> | <span class="t">Okay, and then they talk about a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1028" target="_blank">00:17:08.520</a></span> | <span class="t">of different benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1032" target="_blank">00:17:12.960</a></span> | <span class="t">We'll just maybe take a look at some of the results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1038" target="_blank">00:17:18.800</a></span> | <span class="t">and then continue forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1041" target="_blank">00:17:21.480</a></span> | <span class="t">I'll also look at the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1042" target="_blank">00:17:22.840</a></span> | <span class="t">let me take a break to look at the chat here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1047" target="_blank">00:17:27.280</a></span> | <span class="t">- Another serious questions in chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1051" target="_blank">00:17:31.040</a></span> | <span class="t">I guess Sam had one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1053" target="_blank">00:17:33.280</a></span> | <span class="t">Have there been revisiting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1055" target="_blank">00:17:35.640</a></span> | <span class="t">since the weird now pre-training tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1057" target="_blank">00:17:37.560</a></span> | <span class="t">that were used to invert?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1059" target="_blank">00:17:39.320</a></span> | <span class="t">Or has this all been done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1060" target="_blank">00:17:40.840</a></span> | <span class="t">and it's somewhat of a shut book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1063" target="_blank">00:17:43.000</a></span> | <span class="t">and next token prediction is all you need?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1065" target="_blank">00:17:45.880</a></span> | <span class="t">Any thoughts, comments?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1069" target="_blank">00:17:49.520</a></span> | <span class="t">- So I didn't look at any of the following BERT papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1073" target="_blank">00:17:53.960</a></span> | <span class="t">like Roberta or the Berta for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1078" target="_blank">00:17:58.440</a></span> | <span class="t">So I'm not sure if there's anyone else on the call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1083" target="_blank">00:18:03.280</a></span> | <span class="t">who knows would be happy for a contribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1086" target="_blank">00:18:06.720</a></span> | <span class="t">- Yeah, we had a slight follow-up in the comments there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1094" target="_blank">00:18:14.880</a></span> | <span class="t">Basically for encoder decoder stuff, it still makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1098" target="_blank">00:18:18.200</a></span> | <span class="t">And a lot of my takeaway from this paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1100" target="_blank">00:18:20.640</a></span> | <span class="t">when I read it like years ago</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1102" target="_blank">00:18:22.200</a></span> | <span class="t">was they had really interesting pre-training tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1106" target="_blank">00:18:26.760</a></span> | <span class="t">And if you look at it from a lens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1108" target="_blank">00:18:28.480</a></span> | <span class="t">of what they're trying to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1110" target="_blank">00:18:30.040</a></span> | <span class="t">it's actually pretty useless, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1111" target="_blank">00:18:31.600</a></span> | <span class="t">Like next token prediction is still useful to generate words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1115" target="_blank">00:18:35.960</a></span> | <span class="t">but something like predicting sentence order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1118" target="_blank">00:18:38.760</a></span> | <span class="t">is sentence one or sentence two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1121" target="_blank">00:18:41.280</a></span> | <span class="t">Should sentence one come before sentence two or whatever?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1124" target="_blank">00:18:44.160</a></span> | <span class="t">It's not really useful, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1125" target="_blank">00:18:45.960</a></span> | <span class="t">There's never a time where there's people trying to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1129" target="_blank">00:18:49.160</a></span> | <span class="t">which sentence came before the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1131" target="_blank">00:18:51.280</a></span> | <span class="t">but it does really teach a model conceptually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1134" target="_blank">00:18:54.880</a></span> | <span class="t">like what word order is, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1137" target="_blank">00:18:57.320</a></span> | <span class="t">So there's these words and there's these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1139" target="_blank">00:18:59.800</a></span> | <span class="t">You have to understand should these set of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1141" target="_blank">00:19:01.800</a></span> | <span class="t">come before these set of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1143" target="_blank">00:19:03.080</a></span> | <span class="t">So that helps in the classification task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1145" target="_blank">00:19:05.640</a></span> | <span class="t">where you have to like group together words together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1148" target="_blank">00:19:08.240</a></span> | <span class="t">So in some sense for the tasks that BERT is trying to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1153" target="_blank">00:19:13.240</a></span> | <span class="t">it's trying to be a small efficient classification model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1156" target="_blank">00:19:16.600</a></span> | <span class="t">as one of the tasks it's trying to do, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1159" target="_blank">00:19:19.320</a></span> | <span class="t">It kind of makes sense to do these weird training objectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1162" target="_blank">00:19:22.160</a></span> | <span class="t">So next token prediction or like masking of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1166" target="_blank">00:19:26.000</a></span> | <span class="t">is still like, there's not many use cases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1168" target="_blank">00:19:28.800</a></span> | <span class="t">where you need to fill in the blank, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1170" target="_blank">00:19:30.600</a></span> | <span class="t">We don't have like 500 words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1172" target="_blank">00:19:32.360</a></span> | <span class="t">and you have to fill in one word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1173" target="_blank">00:19:33.880</a></span> | <span class="t">but it does teach a model like over a billion words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1177" target="_blank">00:19:37.720</a></span> | <span class="t">If you start to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1178" target="_blank">00:19:38.920</a></span> | <span class="t">what words go in between other words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1181" target="_blank">00:19:41.080</a></span> | <span class="t">it's a good pre-training task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1182" target="_blank">00:19:42.800</a></span> | <span class="t">and it helps with classification in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1185" target="_blank">00:19:45.360</a></span> | <span class="t">So it's like, instead of having a very broad task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1188" target="_blank">00:19:48.920</a></span> | <span class="t">of just predict next token and then extract,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1192" target="_blank">00:19:52.600</a></span> | <span class="t">like abstract that out to eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1194" target="_blank">00:19:54.600</a></span> | <span class="t">like get this emergent capability to do classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1197" target="_blank">00:19:57.720</a></span> | <span class="t">This is like, here's 12 tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1199" target="_blank">00:19:59.960</a></span> | <span class="t">that mimic understanding words very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1203" target="_blank">00:20:03.080</a></span> | <span class="t">You're a small model, use all this to do classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1206" target="_blank">00:20:06.200</a></span> | <span class="t">but that concept is still applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1209" target="_blank">00:20:09.040</a></span> | <span class="t">for like current encoder decoder models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1211" target="_blank">00:20:11.800</a></span> | <span class="t">and small models that are not just next token prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1215" target="_blank">00:20:15.360</a></span> | <span class="t">So basically if you dumb down the task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1219" target="_blank">00:20:19.000</a></span> | <span class="t">to subsets of your like main goal, it's still very effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1223" target="_blank">00:20:23.200</a></span> | <span class="t">And that's why like, if you take a step back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1225" target="_blank">00:20:25.440</a></span> | <span class="t">and look at what Bert did, it makes no sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1228" target="_blank">00:20:28.360</a></span> | <span class="t">Like Google doesn't need to spend millions of dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1231" target="_blank">00:20:31.000</a></span> | <span class="t">to predict which sentence comes before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1232" target="_blank">00:20:32.880</a></span> | <span class="t">or after another sentence, but it does help a small model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1236" target="_blank">00:20:36.480</a></span> | <span class="t">that'll better learn embedding per se</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1240" target="_blank">00:20:40.080</a></span> | <span class="t">and abstract that to these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1242" target="_blank">00:20:42.120</a></span> | <span class="t">- Yeah, agreed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1246" target="_blank">00:20:46.240</a></span> | <span class="t">Like the sentence, the sentence prediction didn't seem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1252" target="_blank">00:20:52.120</a></span> | <span class="t">like you're right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1253" target="_blank">00:20:53.960</a></span> | <span class="t">There's not too many use cases where that would be helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1257" target="_blank">00:20:57.760</a></span> | <span class="t">I wonder if it does help the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1260" target="_blank">00:21:00.200</a></span> | <span class="t">to like figure out the relationships</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1263" target="_blank">00:21:03.240</a></span> | <span class="t">between words and ideas to some degree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1269" target="_blank">00:21:09.120</a></span> | <span class="t">- Yeah, that's pretty much the whole point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1271" target="_blank">00:21:11.160</a></span> | <span class="t">It should help the model better understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1273" target="_blank">00:21:13.360</a></span> | <span class="t">like breaking down the problem and understand word order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1277" target="_blank">00:21:17.680</a></span> | <span class="t">I think they also did something where they swapped words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1280" target="_blank">00:21:20.640</a></span> | <span class="t">from different sentences or swap sentences, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1283" target="_blank">00:21:23.360</a></span> | <span class="t">And like, that's even more useless in reality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1287" target="_blank">00:21:27.720</a></span> | <span class="t">Like in reality, there's not many times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1291" target="_blank">00:21:31.080</a></span> | <span class="t">where you've got mixing of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1292" target="_blank">00:21:32.840</a></span> | <span class="t">from different chunks of sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1294" target="_blank">00:21:34.960</a></span> | <span class="t">But once again, it helps the model generalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1297" target="_blank">00:21:37.600</a></span> | <span class="t">towards understanding sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1299" target="_blank">00:21:39.160</a></span> | <span class="t">So stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1301" target="_blank">00:21:41.400</a></span> | <span class="t">It's just, if you look at it in today's sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1303" target="_blank">00:21:43.680</a></span> | <span class="t">if you have a niche topic that can benefit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1306" target="_blank">00:21:46.160</a></span> | <span class="t">from encoder decoder, like small, small,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1309" target="_blank">00:21:49.240</a></span> | <span class="t">like on-device active model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1311" target="_blank">00:21:51.880</a></span> | <span class="t">you wanna start to employ breaking down problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1315" target="_blank">00:21:55.240</a></span> | <span class="t">in this sort of methodology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1316" target="_blank">00:21:56.800</a></span> | <span class="t">But there's examples of papers that do this type of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1325" target="_blank">00:22:05.200</a></span> | <span class="t">- Yeah, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1326" target="_blank">00:22:06.800</a></span> | <span class="t">Would be great to follow on this presentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1329" target="_blank">00:22:09.960</a></span> | <span class="t">with some of the more recent work that kind of builds on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1334" target="_blank">00:22:14.080</a></span> | <span class="t">- There are not many other questions in chat, by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1340" target="_blank">00:22:20.920</a></span> | <span class="t">- Okay, okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1342" target="_blank">00:22:22.880</a></span> | <span class="t">You can see from this, at least when it was released,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1348" target="_blank">00:22:28.360</a></span> | <span class="t">BERT-Large was state-of-the-art, even beating out GPT-1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1354" target="_blank">00:22:34.120</a></span> | <span class="t">And ELMo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1355" target="_blank">00:22:35.960</a></span> | <span class="t">So at least at the time it was released,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1360" target="_blank">00:22:40.360</a></span> | <span class="t">it was a very capable model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1363" target="_blank">00:22:43.000</a></span> | <span class="t">And then, I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1367" target="_blank">00:22:47.240</a></span> | <span class="t">I'm gonna kind of skip through these,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1369" target="_blank">00:22:49.880</a></span> | <span class="t">except maybe just to show the results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1373" target="_blank">00:22:53.760</a></span> | <span class="t">of it being state-of-the-art in a lot of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1377" target="_blank">00:22:57.880</a></span> | <span class="t">So the next section is ablation studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1382" target="_blank">00:23:02.560</a></span> | <span class="t">So removing different parts of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1385" target="_blank">00:23:05.680</a></span> | <span class="t">to see what the effects were.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1389" target="_blank">00:23:09.800</a></span> | <span class="t">And let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1395" target="_blank">00:23:15.720</a></span> | <span class="t">So here's a couple of different ablations they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1400" target="_blank">00:23:20.240</a></span> | <span class="t">was they removed the next sentence prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1405" target="_blank">00:23:25.240</a></span> | <span class="t">So I guess this is something we were just talking about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1410" target="_blank">00:23:30.760</a></span> | <span class="t">but they still kept the mask LM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1414" target="_blank">00:23:34.320</a></span> | <span class="t">And then the next thing they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1416" target="_blank">00:23:36.760</a></span> | <span class="t">is they only made it go left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1420" target="_blank">00:23:40.280</a></span> | <span class="t">And they also have the no sentence prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1424" target="_blank">00:23:44.680</a></span> | <span class="t">And so you can see the results from those attempts up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1430" target="_blank">00:23:50.480</a></span> | <span class="t">The top is the kind of the standard model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1437" target="_blank">00:23:57.280</a></span> | <span class="t">And then if you look at the no next sentence prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1441" target="_blank">00:24:01.200</a></span> | <span class="t">it does lose a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1444" target="_blank">00:24:04.440</a></span> | <span class="t">Oh, actually in QNLI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1446" target="_blank">00:24:06.120</a></span> | <span class="t">it looks like it has a significant loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1448" target="_blank">00:24:08.560</a></span> | <span class="t">maybe in other tasks, much less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1451" target="_blank">00:24:11.280</a></span> | <span class="t">But then as you also take away the bidirectional,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1456" target="_blank">00:24:16.080</a></span> | <span class="t">it becomes less capable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1459" target="_blank">00:24:19.000</a></span> | <span class="t">Looks like it kind of varies between tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1462" target="_blank">00:24:22.200</a></span> | <span class="t">like how much capability it loses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1465" target="_blank">00:24:25.240</a></span> | <span class="t">But this does show that there is some value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1468" target="_blank">00:24:28.640</a></span> | <span class="t">to those components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1470" target="_blank">00:24:30.640</a></span> | <span class="t">Oh yeah, maybe this is what I was talking about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1483" target="_blank">00:24:43.080</a></span> | <span class="t">where they say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1483" target="_blank">00:24:43.920</a></span> | <span class="t">"We believe that this is the first work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1484" target="_blank">00:24:44.760</a></span> | <span class="t">"to demonstrate convincingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1485" target="_blank">00:24:45.920</a></span> | <span class="t">"that scaling to extreme model sizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1488" target="_blank">00:24:48.920</a></span> | <span class="t">"also leads to large improvements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1490" target="_blank">00:24:50.480</a></span> | <span class="t">"on a very small scale task."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1493" target="_blank">00:24:53.440</a></span> | <span class="t">This is kind of like the bidder lesson,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1495" target="_blank">00:24:55.720</a></span> | <span class="t">but maybe a little bit exaggerated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1498" target="_blank">00:24:58.680</a></span> | <span class="t">as far as the extreme model size at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1501" target="_blank">00:25:01.640</a></span> | <span class="t">And then they talk about feature approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1508" target="_blank">00:25:08.640</a></span> | <span class="t">with BERT a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1510" target="_blank">00:25:10.920</a></span> | <span class="t">So if there's questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1514" target="_blank">00:25:14.120</a></span> | <span class="t">feel free to unmute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1516" target="_blank">00:25:16.880</a></span> | <span class="t">Otherwise, let's go over to and look through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1519" target="_blank">00:25:19.600</a></span> | <span class="t">There's Jay Alomar has made some very helpful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1524" target="_blank">00:25:24.600</a></span> | <span class="t">like illustrated BERT and ELMo articles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1530" target="_blank">00:25:30.080</a></span> | <span class="t">that we can go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1531" target="_blank">00:25:31.280</a></span> | <span class="t">to just kind of cement our understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1534" target="_blank">00:25:34.720</a></span> | <span class="t">So that's just kind of a comparison of some models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1538" target="_blank">00:25:38.920</a></span> | <span class="t">that were out at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1541" target="_blank">00:25:41.120</a></span> | <span class="t">And then this is one thing that was a takeaway for me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1547" target="_blank">00:25:47.720</a></span> | <span class="t">is that, okay, this is the pre-training step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1550" target="_blank">00:25:50.960</a></span> | <span class="t">that we talked about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1552" target="_blank">00:25:52.040</a></span> | <span class="t">but then on the supervised learning step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1555" target="_blank">00:25:55.200</a></span> | <span class="t">you basically stick a classifier after the BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1558" target="_blank">00:25:58.560</a></span> | <span class="t">So BERT is in charge of essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1561" target="_blank">00:26:01.240</a></span> | <span class="t">encoding the text into an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1564" target="_blank">00:26:04.960</a></span> | <span class="t">And then you use that classifier to then classify,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1568" target="_blank">00:26:08.440</a></span> | <span class="t">in this case, either spam or not spam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1572" target="_blank">00:26:12.240</a></span> | <span class="t">Let me see if I can find...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1577" target="_blank">00:26:17.000</a></span> | <span class="t">There's one diagram that I thought was especially helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1580" target="_blank">00:26:20.280</a></span> | <span class="t">Oh yeah, this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1581" target="_blank">00:26:21.920</a></span> | <span class="t">So this one shows how BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1586" target="_blank">00:26:26.520</a></span> | <span class="t">takes the entire sequence of tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1589" target="_blank">00:26:29.800</a></span> | <span class="t">and then creates like for each input token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1594" target="_blank">00:26:34.800</a></span> | <span class="t">it has an output vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1599" target="_blank">00:26:39.240</a></span> | <span class="t">However, for the purpose of classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1602" target="_blank">00:26:42.040</a></span> | <span class="t">we only look at the first output vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1606" target="_blank">00:26:46.320</a></span> | <span class="t">That one contains essentially the entire sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1612" target="_blank">00:26:52.720</a></span> | <span class="t">of all of the input tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1615" target="_blank">00:26:55.800</a></span> | <span class="t">And then you can run that through,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1617" target="_blank">00:26:57.880</a></span> | <span class="t">it can be a neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1619" target="_blank">00:26:59.240</a></span> | <span class="t">could be a logistic regression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1621" target="_blank">00:27:01.440</a></span> | <span class="t">And then from the features here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1625" target="_blank">00:27:05.760</a></span> | <span class="t">and I think there's like something like 768 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1630" target="_blank">00:27:10.480</a></span> | <span class="t">in the embedding, from the features there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1633" target="_blank">00:27:13.360</a></span> | <span class="t">you can then predict spam or not spam</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1637" target="_blank">00:27:17.560</a></span> | <span class="t">based on your training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1639" target="_blank">00:27:19.440</a></span> | <span class="t">And let's see, that shows the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1646" target="_blank">00:27:26.880</a></span> | <span class="t">And then here's like some illustration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1653" target="_blank">00:27:33.160</a></span> | <span class="t">of the different encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1659" target="_blank">00:27:39.200</a></span> | <span class="t">So as we mentioned earlier, BERT is encoder only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1663" target="_blank">00:27:43.640</a></span> | <span class="t">So the kind of classical transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1667" target="_blank">00:27:47.520</a></span> | <span class="t">is an encoder and a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1670" target="_blank">00:27:50.600</a></span> | <span class="t">Many modern models are decoder only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1675" target="_blank">00:27:55.720</a></span> | <span class="t">And so encoder is like used mostly these days</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1682" target="_blank">00:28:02.280</a></span> | <span class="t">for text classification or text clustering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1688" target="_blank">00:28:08.200</a></span> | <span class="t">To my knowledge, there's encoder only transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1693" target="_blank">00:28:13.200</a></span> | <span class="t">aren't really used for any kind of next,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1698" target="_blank">00:28:18.320</a></span> | <span class="t">like sequence generation or next token generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1701" target="_blank">00:28:21.680</a></span> | <span class="t">This talks about ELMo and the different context of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1713" target="_blank">00:28:33.440</a></span> | <span class="t">and how ELMo captures that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1716" target="_blank">00:28:36.520</a></span> | <span class="t">(silence)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1718" target="_blank">00:28:38.680</a></span> | <span class="t">GPT, I thought there was something here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1727" target="_blank">00:28:47.400</a></span> | <span class="t">Yeah, so this is just like what we talked about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1735" target="_blank">00:28:55.680</a></span> | <span class="t">about if you have a BERT encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1742" target="_blank">00:29:02.000</a></span> | <span class="t">you can stick another model for training on the end of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1745" target="_blank">00:29:05.960</a></span> | <span class="t">And then go from there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1747" target="_blank">00:29:07.960</a></span> | <span class="t">And then you can also use BERT for embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1754" target="_blank">00:29:14.960</a></span> | <span class="t">So if you have like a certain problem space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1759" target="_blank">00:29:19.960</a></span> | <span class="t">with a lot of texts that you want to embed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1763" target="_blank">00:29:23.880</a></span> | <span class="t">you can continue pre-training or do fine tuning on BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1771" target="_blank">00:29:31.360</a></span> | <span class="t">with your corpus in your industry specific like text corpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1776" target="_blank">00:29:36.360</a></span> | <span class="t">and then create an encoder that's especially built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1782" target="_blank">00:29:42.720</a></span> | <span class="t">for your needs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1787" target="_blank">00:29:47.240</a></span> | <span class="t">So there's one more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1793" target="_blank">00:29:53.000</a></span> | <span class="t">I'll pause any other questions in the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1797" target="_blank">00:29:57.480</a></span> | <span class="t">So some context of how they train BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1801" target="_blank">00:30:01.400</a></span> | <span class="t">they have like those 12 paths, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1803" target="_blank">00:30:03.880</a></span> | <span class="t">They had a BERT based model, a math language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1806" target="_blank">00:30:06.200</a></span> | <span class="t">They had the next sentence prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1808" target="_blank">00:30:08.080</a></span> | <span class="t">token classification, QA, sequence classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1811" target="_blank">00:30:11.960</a></span> | <span class="t">They had all these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1813" target="_blank">00:30:13.360</a></span> | <span class="t">And basically what they did where they were BERT models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1816" target="_blank">00:30:16.360</a></span> | <span class="t">with a layer added on top for a classification head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1821" target="_blank">00:30:21.240</a></span> | <span class="t">Now, in the time of 2019,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1823" target="_blank">00:30:23.720</a></span> | <span class="t">when people started using these models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1825" target="_blank">00:30:25.360</a></span> | <span class="t">what was really common to do was you could either,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1829" target="_blank">00:30:29.000</a></span> | <span class="t">if you had enough data, take the base model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1831" target="_blank">00:30:31.840</a></span> | <span class="t">add in a linear output head for classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1834" target="_blank">00:30:34.920</a></span> | <span class="t">where you basically take all this, there's no output head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1837" target="_blank">00:30:37.520</a></span> | <span class="t">It's just output is the last step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1839" target="_blank">00:30:39.240</a></span> | <span class="t">of processing these tokens or sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1842" target="_blank">00:30:42.160</a></span> | <span class="t">Then you add just a linear head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1843" target="_blank">00:30:43.880</a></span> | <span class="t">with a softmax for classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1846" target="_blank">00:30:46.120</a></span> | <span class="t">Now, then you fine tune it on a lot of your data itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1850" target="_blank">00:30:50.600</a></span> | <span class="t">If you didn't have as much data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1853" target="_blank">00:30:53.280</a></span> | <span class="t">one thing that was popular was you take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1854" target="_blank">00:30:54.920</a></span> | <span class="t">the sequence classification head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1857" target="_blank">00:30:57.920</a></span> | <span class="t">you just continue fine tuning it on your data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1860" target="_blank">00:31:00.200</a></span> | <span class="t">and it's already somewhat good at sequence classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1863" target="_blank">00:31:03.440</a></span> | <span class="t">But there was a whole like series of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1866" target="_blank">00:31:06.400</a></span> | <span class="t">that looked into based on how much data you have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1870" target="_blank">00:31:10.480</a></span> | <span class="t">where you should do your fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1872" target="_blank">00:31:12.480</a></span> | <span class="t">So if you have a lot of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1874" target="_blank">00:31:14.760</a></span> | <span class="t">it was pretty common to not only add a classification head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1878" target="_blank">00:31:18.560</a></span> | <span class="t">but also peel back a few layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1880" target="_blank">00:31:20.640</a></span> | <span class="t">So reset the weights of like the top three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1883" target="_blank">00:31:23.920</a></span> | <span class="t">the top two, the final layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1886" target="_blank">00:31:26.320</a></span> | <span class="t">and then continue training those in as well for your task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1889" target="_blank">00:31:29.280</a></span> | <span class="t">Because at some level, what people started to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1891" target="_blank">00:31:31.600</a></span> | <span class="t">was these pre-training objective tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1894" target="_blank">00:31:34.840</a></span> | <span class="t">of like mask word prediction and sentence ordering or QA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1899" target="_blank">00:31:39.760</a></span> | <span class="t">they were actually affecting the net output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1902" target="_blank">00:31:42.800</a></span> | <span class="t">of sequence classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1904" target="_blank">00:31:44.480</a></span> | <span class="t">And if you wanted better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1906" target="_blank">00:31:46.000</a></span> | <span class="t">you could just train more of your whole model on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1908" target="_blank">00:31:48.520</a></span> | <span class="t">So there was a whole thing of like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1910" target="_blank">00:31:50.400</a></span> | <span class="t">you should remove the top two layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1912" target="_blank">00:31:52.720</a></span> | <span class="t">add a sequence classification head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1914" target="_blank">00:31:54.560</a></span> | <span class="t">train on tens of thousands of examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1916" target="_blank">00:31:56.440</a></span> | <span class="t">and you'll get state-of-the-art results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1919" target="_blank">00:31:59.160</a></span> | <span class="t">You could, if you have less data, freeze layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1922" target="_blank">00:32:02.760</a></span> | <span class="t">you could unfreeze weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1924" target="_blank">00:32:04.080</a></span> | <span class="t">There's like a whole set of this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1925" target="_blank">00:32:05.920</a></span> | <span class="t">but it was pretty common to also just mess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1928" target="_blank">00:32:08.480</a></span> | <span class="t">with the architecture and add classification head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1931" target="_blank">00:32:11.160</a></span> | <span class="t">- Do you know if anyone trained all the layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1938" target="_blank">00:32:18.400</a></span> | <span class="t">or like just use that as a starting point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1940" target="_blank">00:32:20.680</a></span> | <span class="t">to train all the layers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1941" target="_blank">00:32:21.920</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1943" target="_blank">00:32:23.200</a></span> | <span class="t">- So today there's like stuff that came out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1945" target="_blank">00:32:25.520</a></span> | <span class="t">like a year or two ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1947" target="_blank">00:32:27.600</a></span> | <span class="t">where basically you could retrain BERT in 24 hours</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1951" target="_blank">00:32:31.200</a></span> | <span class="t">on like a budget of like sub $500 with regular A100s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1956" target="_blank">00:32:36.200</a></span> | <span class="t">and how you can do this better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1958" target="_blank">00:32:38.920</a></span> | <span class="t">So it was in the realm of like at the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1962" target="_blank">00:32:42.680</a></span> | <span class="t">not as effective to, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1964" target="_blank">00:32:44.440</a></span> | <span class="t">like you don't have Google Compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1965" target="_blank">00:32:45.760</a></span> | <span class="t">to retrain BERT from scratch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1967" target="_blank">00:32:47.480</a></span> | <span class="t">but now there's stuff of like 24 hours</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1970" target="_blank">00:32:50.360</a></span> | <span class="t">and a couple of hundred dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1971" target="_blank">00:32:51.920</a></span> | <span class="t">to retrain your own better BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1974" target="_blank">00:32:54.000</a></span> | <span class="t">There's like an academia paper that came out about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1977" target="_blank">00:32:57.080</a></span> | <span class="t">If people go down the rabbit hole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1978" target="_blank">00:32:58.720</a></span> | <span class="t">of like encoder models and this stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1981" target="_blank">00:33:01.600</a></span> | <span class="t">it's a cool one to look into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1983" target="_blank">00:33:03.000</a></span> | <span class="t">of how they can better objectify these 12 pre-training tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1988" target="_blank">00:33:08.000</a></span> | <span class="t">to a few and a better curated dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1990" target="_blank">00:33:10.560</a></span> | <span class="t">and outperform it on a couple of hundred dollars in 24 hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1993" target="_blank">00:33:13.920</a></span> | <span class="t">But then it was also common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1995" target="_blank">00:33:15.080</a></span> | <span class="t">where like there was a sentence classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=1999" target="_blank">00:33:19.880</a></span> | <span class="t">and sentence extraction tasks that BERT was adapted towards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2003" target="_blank">00:33:23.520</a></span> | <span class="t">So like BERT for sequence classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2005" target="_blank">00:33:25.760</a></span> | <span class="t">or extraction for like abstractive summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2008" target="_blank">00:33:28.400</a></span> | <span class="t">And then companies that took it to production</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2010" target="_blank">00:33:30.320</a></span> | <span class="t">would do like significant retrains or like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2014" target="_blank">00:33:34.560</a></span> | <span class="t">yeah, they train a lot more of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2016" target="_blank">00:33:36.560</a></span> | <span class="t">And then this also just went into like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2021" target="_blank">00:33:41.280</a></span> | <span class="t">at what part do you want to start training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2024" target="_blank">00:33:44.040</a></span> | <span class="t">(mouse clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2026" target="_blank">00:33:46.800</a></span> | <span class="t">Yeah, I mean that sounds like interesting stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2031" target="_blank">00:33:51.640</a></span> | <span class="t">If you have any links,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2035" target="_blank">00:33:55.160</a></span> | <span class="t">please drop in the chat and I'll check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2037" target="_blank">00:33:57.520</a></span> | <span class="t">So maybe the last thing we can go through here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2044" target="_blank">00:34:04.320</a></span> | <span class="t">is the same author Jay Alomar has a notebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2051" target="_blank">00:34:11.400</a></span> | <span class="t">where he shows like hands-on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2054" target="_blank">00:34:14.120</a></span> | <span class="t">how to do this movie review sentiment classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2059" target="_blank">00:34:19.120</a></span> | <span class="t">He uses DistilBERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2064" target="_blank">00:34:24.080</a></span> | <span class="t">So DistilBERT is a hugging face like recreation of BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2069" target="_blank">00:34:29.080</a></span> | <span class="t">that like has very comparable performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2074" target="_blank">00:34:34.920</a></span> | <span class="t">on many fewer parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2079" target="_blank">00:34:39.280</a></span> | <span class="t">And then to do the classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2081" target="_blank">00:34:41.400</a></span> | <span class="t">he just uses a basic logistic regression model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2085" target="_blank">00:34:45.320</a></span> | <span class="t">from scikit-learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2087" target="_blank">00:34:47.720</a></span> | <span class="t">And so then the features that go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2091" target="_blank">00:34:51.120</a></span> | <span class="t">into this logistic regression model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2092" target="_blank">00:34:52.880</a></span> | <span class="t">are just the vector of size 768</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2097" target="_blank">00:34:57.040</a></span> | <span class="t">that comes out of the DistilBERT embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2104" target="_blank">00:35:04.760</a></span> | <span class="t">So a lot of this leans very heavily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2107" target="_blank">00:35:07.920</a></span> | <span class="t">on the HuggingFaceTransformers library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2111" target="_blank">00:35:11.120</a></span> | <span class="t">So let's see, that's just installing it, doing imports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2115" target="_blank">00:35:15.880</a></span> | <span class="t">He uses, he must've mentioned it up above, but a...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2123" target="_blank">00:35:23.080</a></span> | <span class="t">Maybe it's...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2129" target="_blank">00:35:29.240</a></span> | <span class="t">Anyway, there's a particular HuggingFace dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2132" target="_blank">00:35:32.480</a></span> | <span class="t">that he's using that has the movie sentiment training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2136" target="_blank">00:35:36.480</a></span> | <span class="t">Maybe he just uploaded it somewhere, he had it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2143" target="_blank">00:35:43.760</a></span> | <span class="t">- I think it's an IMDb dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2147" target="_blank">00:35:47.160</a></span> | <span class="t">I think it's one of the Kaggle ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2149" target="_blank">00:35:49.360</a></span> | <span class="t">Yeah, it's just a Kaggle IMDb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2150" target="_blank">00:35:50.920</a></span> | <span class="t">You have movie reviews, you classify them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2153" target="_blank">00:35:53.160</a></span> | <span class="t">Oh, and on that actually reminds me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2158" target="_blank">00:35:58.160</a></span> | <span class="t">one of the big things that made BERT somewhat popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2161" target="_blank">00:36:01.560</a></span> | <span class="t">was there was another Kaggle competition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2163" target="_blank">00:36:03.840</a></span> | <span class="t">on tweet classification of sentiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2167" target="_blank">00:36:07.360</a></span> | <span class="t">So in tweets, like with previous embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2170" target="_blank">00:36:10.520</a></span> | <span class="t">like Bag of Words or Glove or Elmo,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2173" target="_blank">00:36:13.880</a></span> | <span class="t">if you have stuff like, "I hate this so much,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2178" target="_blank">00:36:18.880</a></span> | <span class="t">that in some contexts in tweets could still be positive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2182" target="_blank">00:36:22.880</a></span> | <span class="t">even though it's very negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2184" target="_blank">00:36:24.240</a></span> | <span class="t">And when you just look at lexical understanding of words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2190" target="_blank">00:36:30.480</a></span> | <span class="t">it's very negative, but then BERT embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2193" target="_blank">00:36:33.080</a></span> | <span class="t">were what really dominated that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2195" target="_blank">00:36:35.320</a></span> | <span class="t">And then for like a few years,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2196" target="_blank">00:36:36.320</a></span> | <span class="t">they kept doing follow-ups on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2197" target="_blank">00:36:37.640</a></span> | <span class="t">But IMDb and tweet classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2200" target="_blank">00:36:40.520</a></span> | <span class="t">were versions that they used in a lot of these demos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2204" target="_blank">00:36:44.600</a></span> | <span class="t">- So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2213" target="_blank">00:36:53.520</a></span> | <span class="t">So here we're just uploading or downloading BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2218" target="_blank">00:36:58.920</a></span> | <span class="t">DistilBERT from HuggingFace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2222" target="_blank">00:37:02.200</a></span> | <span class="t">and getting the model initialized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2226" target="_blank">00:37:06.240</a></span> | <span class="t">So you can see it's just a few lines of code there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2229" target="_blank">00:37:09.040</a></span> | <span class="t">So we have to do a few things like tokenize it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2236" target="_blank">00:37:16.800</a></span> | <span class="t">and then add padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2239" target="_blank">00:37:19.600</a></span> | <span class="t">So this is so that all of the sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2243" target="_blank">00:37:23.760</a></span> | <span class="t">can be run in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2246" target="_blank">00:37:26.960</a></span> | <span class="t">So we need to pad out so that they're all the same length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2250" target="_blank">00:37:30.320</a></span> | <span class="t">And then we need to mark the padded sections as masked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2257" target="_blank">00:37:37.120</a></span> | <span class="t">so that BERT doesn't get confused at thinking like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2261" target="_blank">00:37:41.680</a></span> | <span class="t">what's empty space is actual sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2265" target="_blank">00:37:45.400</a></span> | <span class="t">that we want it to process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2268" target="_blank">00:37:48.800</a></span> | <span class="t">So then you can see this diagram here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2275" target="_blank">00:37:55.520</a></span> | <span class="t">And again, apologies, I'll try to zoom in again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2278" target="_blank">00:37:58.440</a></span> | <span class="t">Oh, it worked this time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2280" target="_blank">00:38:00.000</a></span> | <span class="t">So this just takes the input text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2284" target="_blank">00:38:04.720</a></span> | <span class="t">runs it through DistilBERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2286" target="_blank">00:38:06.400</a></span> | <span class="t">and comes out with the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2288" target="_blank">00:38:08.200</a></span> | <span class="t">So that's all of this thing does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2295" target="_blank">00:38:15.120</a></span> | <span class="t">And then the one tricky part about all of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2300" target="_blank">00:38:20.120</a></span> | <span class="t">is you need to pick out exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2303" target="_blank">00:38:23.800</a></span> | <span class="t">which values from this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2308" target="_blank">00:38:28.800</a></span> | <span class="t">I guess it's three-dimensional tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2311" target="_blank">00:38:31.720</a></span> | <span class="t">you want to predict on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2314" target="_blank">00:38:34.040</a></span> | <span class="t">So if you remember back from here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2317" target="_blank">00:38:37.040</a></span> | <span class="t">we just want the very first output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2325" target="_blank">00:38:45.120</a></span> | <span class="t">We wanna ignore all of these other ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2331" target="_blank">00:38:51.680</a></span> | <span class="t">So he draws out in detail like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2335" target="_blank">00:38:55.960</a></span> | <span class="t">how exactly you pull just those vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2339" target="_blank">00:38:59.760</a></span> | <span class="t">out of this three-dimensional tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2343" target="_blank">00:39:03.920</a></span> | <span class="t">And then it's pretty straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2346" target="_blank">00:39:06.320</a></span> | <span class="t">machine learning after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2348" target="_blank">00:39:08.080</a></span> | <span class="t">You just turn those 768 dimensions into features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2357" target="_blank">00:39:17.800</a></span> | <span class="t">and do a test train split,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2361" target="_blank">00:39:21.400</a></span> | <span class="t">train your logistic regression model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2365" target="_blank">00:39:25.040</a></span> | <span class="t">and then run, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2371" target="_blank">00:39:31.000</a></span> | <span class="t">once you've got the model trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2372" target="_blank">00:39:32.040</a></span> | <span class="t">you can run a score and it gets 82%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2375" target="_blank">00:39:35.560</a></span> | <span class="t">So assuming it's a 50/50 split,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2379" target="_blank">00:39:39.320</a></span> | <span class="t">then the expected amount just from random chance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2382" target="_blank">00:39:42.760</a></span> | <span class="t">would be 50%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2384" target="_blank">00:39:44.560</a></span> | <span class="t">So there is a significant increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2388" target="_blank">00:39:48.560</a></span> | <span class="t">using BERT to do classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2391" target="_blank">00:39:51.280</a></span> | <span class="t">but obviously still plenty of room for improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2395" target="_blank">00:39:55.680</a></span> | <span class="t">Okay, and down here, it says the highest accuracy score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2400" target="_blank">00:40:00.360</a></span> | <span class="t">for this data set is currently 96.8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2403" target="_blank">00:40:03.880</a></span> | <span class="t">So as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2406" target="_blank">00:40:06.640</a></span> | <span class="t">things have come a long way since 2019,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2410" target="_blank">00:40:10.080</a></span> | <span class="t">but still a useful model to start with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2415" target="_blank">00:40:15.080</a></span> | <span class="t">for any classification tasks or clustering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2423" target="_blank">00:40:23.240</a></span> | <span class="t">If you just wanna see what tech sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2426" target="_blank">00:40:26.800</a></span> | <span class="t">are close to each other in embedding space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2428" target="_blank">00:40:28.760</a></span> | <span class="t">you can use it for that as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2430" target="_blank">00:40:30.920</a></span> | <span class="t">So that's about all I had for prepared stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2439" target="_blank">00:40:39.280</a></span> | <span class="t">I'll stop sharing and then maybe go through the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2444" target="_blank">00:40:44.280</a></span> | <span class="t">If anyone wants to chime in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2448" target="_blank">00:40:48.600</a></span> | <span class="t">add any color commentary, feel free to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2452" target="_blank">00:40:52.800</a></span> | <span class="t">- Someone linked the paper on the academic budget,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2462" target="_blank">00:41:02.680</a></span> | <span class="t">the 24-hour BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2465" target="_blank">00:41:05.280</a></span> | <span class="t">and then I was also trying to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2467" target="_blank">00:41:07.360</a></span> | <span class="t">where apparently MosaicML showed how you can do it for $20</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2470" target="_blank">00:41:10.600</a></span> | <span class="t">pre-trained BERT from scratch now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2472" target="_blank">00:41:12.560</a></span> | <span class="t">So yeah, $20, they did like eight A100s for an hour,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2477" target="_blank">00:41:17.560</a></span> | <span class="t">and they're able to match the glue store of basic BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2481" target="_blank">00:41:21.640</a></span> | <span class="t">with their recipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2483" target="_blank">00:41:23.080</a></span> | <span class="t">Kind of interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2485" target="_blank">00:41:25.480</a></span> | <span class="t">So a note Eugene Cha made is researchers felt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2490" target="_blank">00:41:30.480</a></span> | <span class="t">10K training is expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2492" target="_blank">00:41:32.560</a></span> | <span class="t">So I remember mapping this out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2495" target="_blank">00:41:35.320</a></span> | <span class="t">BERT, all these things that compare like 24-hour,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2499" target="_blank">00:41:39.880</a></span> | <span class="t">one-hour BERT, they trained BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2502" target="_blank">00:41:42.600</a></span> | <span class="t">for four days of TPU v3 equivalent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2506" target="_blank">00:41:46.960</a></span> | <span class="t">which is like at the time, let's say eight to $10 an hour,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2510" target="_blank">00:41:50.760</a></span> | <span class="t">which is like 10 to 15K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2512" target="_blank">00:41:52.720</a></span> | <span class="t">But then there wasn't just BERT-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2514" target="_blank">00:41:54.960</a></span> | <span class="t">There was BERT-based, there was BERT-large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2516" target="_blank">00:41:56.800</a></span> | <span class="t">there was BERT-small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2517" target="_blank">00:41:57.800</a></span> | <span class="t">There's a bunch of experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2519" target="_blank">00:41:59.160</a></span> | <span class="t">The BERT-large was trained for more than four days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2522" target="_blank">00:42:02.240</a></span> | <span class="t">Like the cost equivalent is 50K on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2525" target="_blank">00:42:05.040</a></span> | <span class="t">10K on the regular BERT-based, less on the little one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2528" target="_blank">00:42:08.440</a></span> | <span class="t">And then you got to add in like the time and the R&D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2532" target="_blank">00:42:12.280</a></span> | <span class="t">Oh, it was well more than a 10K project at Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2536" target="_blank">00:42:16.400</a></span> | <span class="t">The BERT-large itself was already a 50K train run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2540" target="_blank">00:42:20.080</a></span> | <span class="t">plus 10 to 15 for BERT-based, plus just experimentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2544" target="_blank">00:42:24.520</a></span> | <span class="t">So expensive, expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2547" target="_blank">00:42:27.680</a></span> | <span class="t">- I think it's more amount of labs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2549" target="_blank">00:42:29.600</a></span> | <span class="t">would love to hear those numbers for SOTA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2552" target="_blank">00:42:32.520</a></span> | <span class="t">- Right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2553" target="_blank">00:42:33.360</a></span> | <span class="t">- True, true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2559" target="_blank">00:42:39.400</a></span> | <span class="t">- I'm just reading through the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2572" target="_blank">00:42:52.280</a></span> | <span class="t">- Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2581" target="_blank">00:43:01.520</a></span> | <span class="t">- Did we get a volunteer for next week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2583" target="_blank">00:43:03.360</a></span> | <span class="t">or still waiting on that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2585" target="_blank">00:43:05.160</a></span> | <span class="t">- Anyone else next week?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2589" target="_blank">00:43:09.760</a></span> | <span class="t">Any other questions on this, by the way?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2592" target="_blank">00:43:12.360</a></span> | <span class="t">- I do have a random one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2593" target="_blank">00:43:13.880</a></span> | <span class="t">Because this is regarding the embedding science, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2598" target="_blank">00:43:18.840</a></span> | <span class="t">Even though I joke about this was the era before the GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2603" target="_blank">00:43:23.840</a></span> | <span class="t">the gaming GPU folks came in and said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2606" target="_blank">00:43:26.400</a></span> | <span class="t">hey, you need to be divisible by 64, 32, or power two, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2613" target="_blank">00:43:33.280</a></span> | <span class="t">Does TPUs not have the divisible by 64 batch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2619" target="_blank">00:43:39.120</a></span> | <span class="t">I mean like optimization when it comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2621" target="_blank">00:43:41.760</a></span> | <span class="t">to embedding size characteristic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2624" target="_blank">00:43:44.440</a></span> | <span class="t">That's why they have all these weird embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2626" target="_blank">00:43:46.640</a></span> | <span class="t">size on TPUs related training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2628" target="_blank">00:43:48.960</a></span> | <span class="t">- I don't think it's TPU based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2634" target="_blank">00:43:54.120</a></span> | <span class="t">I have like old notes that I'm recalling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2636" target="_blank">00:43:56.520</a></span> | <span class="t">where I dug through why they specifically did 768 and 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2642" target="_blank">00:44:02.240</a></span> | <span class="t">And also someone noted in chat that's a limitation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2646" target="_blank">00:44:06.280</a></span> | <span class="t">There's other work that extends this out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2649" target="_blank">00:44:09.760</a></span> | <span class="t">to like SentenceBERT that extends the embedding dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2655" target="_blank">00:44:15.200</a></span> | <span class="t">And they were also pretty small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2658" target="_blank">00:44:18.560</a></span> | <span class="t">But back to Eugene's point of, is it hardware limitation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2662" target="_blank">00:44:22.600</a></span> | <span class="t">It's not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2663" target="_blank">00:44:23.480</a></span> | <span class="t">It was well divisibility between layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2666" target="_blank">00:44:26.520</a></span> | <span class="t">and adding layers and a bunch of stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2668" target="_blank">00:44:28.440</a></span> | <span class="t">I really can't remember the specifics of the reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2671" target="_blank">00:44:31.760</a></span> | <span class="t">I'll dig through some old notes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2673" target="_blank">00:44:33.400</a></span> | <span class="t">but someone broke down the per layer map</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2675" target="_blank">00:44:35.520</a></span> | <span class="t">and sending through inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2678" target="_blank">00:44:38.600</a></span> | <span class="t">And there was a decent range reason for why all this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2683" target="_blank">00:44:43.160</a></span> | <span class="t">It's also like there's 12 layers divisible by 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2688" target="_blank">00:44:48.000</a></span> | <span class="t">It went down that path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2691" target="_blank">00:44:51.040</a></span> | <span class="t">But also it wasn't like someone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2692" target="_blank">00:44:52.800</a></span> | <span class="t">from Google that worked on BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2694" target="_blank">00:44:54.440</a></span> | <span class="t">It was just mapping through the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2696" target="_blank">00:44:56.760</a></span> | <span class="t">through every layer and all of this math working out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2699" target="_blank">00:44:59.200</a></span> | <span class="t">And then a reason for like, oh, here's why this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2701" target="_blank">00:45:01.800</a></span> | <span class="t">why not this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2702" target="_blank">00:45:02.840</a></span> | <span class="t">And I was like, sounds good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2704" target="_blank">00:45:04.280</a></span> | <span class="t">Checks out to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2705" target="_blank">00:45:05.280</a></span> | <span class="t">I can probably find this if I look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2708" target="_blank">00:45:08.520</a></span> | <span class="t">It's in some notes from a couple of years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2711" target="_blank">00:45:11.600</a></span> | <span class="t">(indistinct)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2714" target="_blank">00:45:14.000</a></span> | <span class="t">- Eric, someone has asked,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2720" target="_blank">00:45:20.160</a></span> | <span class="t">does BERT pre-training objective MLM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2723" target="_blank">00:45:23.240</a></span> | <span class="t">like mass language modeling follow the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2725" target="_blank">00:45:25.240</a></span> | <span class="t">LLM scaling laws as GPTs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2727" target="_blank">00:45:27.520</a></span> | <span class="t">- That's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2731" target="_blank">00:45:31.680</a></span> | <span class="t">I don't know if there's been enough research in that area</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2738" target="_blank">00:45:38.080</a></span> | <span class="t">to like come to any conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2741" target="_blank">00:45:41.680</a></span> | <span class="t">So I, like when I was researching this presentation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2746" target="_blank">00:45:46.840</a></span> | <span class="t">I went to, I think it was paperswithcode.com</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2751" target="_blank">00:45:51.840</a></span> | <span class="t">or something like that and looked at all the top papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2757" target="_blank">00:45:57.720</a></span> | <span class="t">for text classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2759" target="_blank">00:45:59.800</a></span> | <span class="t">And like a lot of them were pre or 2021 or previous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2765" target="_blank">00:46:05.880</a></span> | <span class="t">And so it seems like this direction of like encoder only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2770" target="_blank">00:46:10.880</a></span> | <span class="t">or bi-directional has,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2774" target="_blank">00:46:14.320</a></span> | <span class="t">well, I don't know about the bi-directional part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2775" target="_blank">00:46:15.720</a></span> | <span class="t">but at least encoder only,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2777" target="_blank">00:46:17.320</a></span> | <span class="t">research has been pretty sparse recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2780" target="_blank">00:46:20.920</a></span> | <span class="t">So for example, I don't know if anyone's spending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2786" target="_blank">00:46:26.360</a></span> | <span class="t">millions of dollars to train like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2790" target="_blank">00:46:30.440</a></span> | <span class="t">super BERT or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2795" target="_blank">00:46:35.520</a></span> | <span class="t">- I think it's-- - Well, the Rekka AI guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2797" target="_blank">00:46:37.800</a></span> | <span class="t">seems to play.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2798" target="_blank">00:46:38.640</a></span> | <span class="t">- No, the last time I looked at this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2801" target="_blank">00:46:41.840</a></span> | <span class="t">like leaderboard and hugging face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2803" target="_blank">00:46:43.920</a></span> | <span class="t">I think it was all led by these transformed LLMs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2808" target="_blank">00:46:48.360</a></span> | <span class="t">that now get the best performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2811" target="_blank">00:46:51.240</a></span> | <span class="t">like a Mistral 7B turned into embedding model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2815" target="_blank">00:46:55.920</a></span> | <span class="t">- Is that, sorry, I missed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2822" target="_blank">00:47:02.440</a></span> | <span class="t">Is that for text classification?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2825" target="_blank">00:47:05.320</a></span> | <span class="t">- Well, yeah, I guess at the core,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2828" target="_blank">00:47:08.920</a></span> | <span class="t">it's all turning text into an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2831" target="_blank">00:47:11.600</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2834" target="_blank">00:47:14.040</a></span> | <span class="t">- Could you drop a link in the chat?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2838" target="_blank">00:47:18.800</a></span> | <span class="t">- Yeah, sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2840" target="_blank">00:47:20.480</a></span> | <span class="t">- Yeah, it seems like that would lead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2841" target="_blank">00:47:21.920</a></span> | <span class="t">to higher performance with this classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2846" target="_blank">00:47:26.040</a></span> | <span class="t">but I'd have to do a little bit more research on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2852" target="_blank">00:47:32.720</a></span> | <span class="t">- Well, there's two pieces as well, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2855" target="_blank">00:47:35.480</a></span> | <span class="t">So for mass language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2857" target="_blank">00:47:37.880</a></span> | <span class="t">a lot of the scaling law papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2859" target="_blank">00:47:39.920</a></span> | <span class="t">directly showed why decoder only token prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2863" target="_blank">00:47:43.200</a></span> | <span class="t">is better scaling than mass language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2865" target="_blank">00:47:45.880</a></span> | <span class="t">One is purely when you mask 15% of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2868" target="_blank">00:47:48.720</a></span> | <span class="t">you train on the mass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2869" target="_blank">00:47:49.840</a></span> | <span class="t">So you lose a lot of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2871" target="_blank">00:47:51.720</a></span> | <span class="t">You need more quality data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2874" target="_blank">00:47:54.520</a></span> | <span class="t">You're just straight training on less, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2876" target="_blank">00:47:56.680</a></span> | <span class="t">If you have a data set of a trillion tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2879" target="_blank">00:47:59.120</a></span> | <span class="t">you can mask 15% of them and train on learning the 15%,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2883" target="_blank">00:48:03.000</a></span> | <span class="t">or you can train on all trillion of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2885" target="_blank">00:48:05.360</a></span> | <span class="t">That's a straight scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2886" target="_blank">00:48:06.640</a></span> | <span class="t">Now, if you have 15 trillion tokens versus 1 trillion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2889" target="_blank">00:48:09.680</a></span> | <span class="t">that's another question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2891" target="_blank">00:48:11.160</a></span> | <span class="t">but for embedding tasks in smaller models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2895" target="_blank">00:48:15.560</a></span> | <span class="t">the better trade-off scaling curve at the start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2899" target="_blank">00:48:19.360</a></span> | <span class="t">for encoder learns better with less tokens at first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2903" target="_blank">00:48:23.960</a></span> | <span class="t">but then extending this out in pure scaling laws,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2908" target="_blank">00:48:28.760</a></span> | <span class="t">yeah, you lose a lot of your training data, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2911" target="_blank">00:48:31.040</a></span> | <span class="t">And then that was one of the big points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2913" target="_blank">00:48:33.040</a></span> | <span class="t">of why do we do next token prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2915" target="_blank">00:48:35.840</a></span> | <span class="t">because it scales better than other tasks, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2918" target="_blank">00:48:38.760</a></span> | <span class="t">So scaling laws were made to show better objectives,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2921" target="_blank">00:48:41.480</a></span> | <span class="t">so directly against it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2922" target="_blank">00:48:42.760</a></span> | <span class="t">But then at small scale and stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2925" target="_blank">00:48:45.440</a></span> | <span class="t">there's benefit in this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2928" target="_blank">00:48:48.240</a></span> | <span class="t">specifically for like edge models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2931" target="_blank">00:48:51.120</a></span> | <span class="t">like you can deploy a BART as a guardrail live,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2935" target="_blank">00:48:55.800</a></span> | <span class="t">and you can have it intercept every query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2937" target="_blank">00:48:57.840</a></span> | <span class="t">because it can act in milliseconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2939" target="_blank">00:48:59.360</a></span> | <span class="t">versus LLMs will still take longer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2942" target="_blank">00:49:02.280</a></span> | <span class="t">So at a smaller scale, they'll be better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2944" target="_blank">00:49:04.800</a></span> | <span class="t">There was another part to this that I'm blanking on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2949" target="_blank">00:49:09.040</a></span> | <span class="t">Oh, Raika AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2950" target="_blank">00:49:10.120</a></span> | <span class="t">they're doing encoder decoder generation models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2952" target="_blank">00:49:12.880</a></span> | <span class="t">where they're adding decoder heads to encoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2955" target="_blank">00:49:15.200</a></span> | <span class="t">and they're scaling them up to billions of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2958" target="_blank">00:49:18.320</a></span> | <span class="t">They're a case study of spending money</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2961" target="_blank">00:49:21.720</a></span> | <span class="t">to train them up pretty big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2964" target="_blank">00:49:24.120</a></span> | <span class="t">(mouse clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2966" target="_blank">00:49:26.880</a></span> | <span class="t">There's a question from Isaac in the chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2971" target="_blank">00:49:31.960</a></span> | <span class="t">about my use case at work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2975" target="_blank">00:49:35.400</a></span> | <span class="t">So currently where we're at in the project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2978" target="_blank">00:49:38.520</a></span> | <span class="t">is we need to accumulate some good training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2983" target="_blank">00:49:43.520</a></span> | <span class="t">So we don't have enough training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2985" target="_blank">00:49:45.360</a></span> | <span class="t">to like actually train a BART or that type of model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2991" target="_blank">00:49:51.520</a></span> | <span class="t">So to start with, we're just using LLMs and prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=2996" target="_blank">00:49:56.520</a></span> | <span class="t">to like do some logical classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3001" target="_blank">00:50:01.920</a></span> | <span class="t">to like kind of bootstrap until we get enough data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3007" target="_blank">00:50:07.600</a></span> | <span class="t">And then also to create a feedback loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3009" target="_blank">00:50:09.600</a></span> | <span class="t">where we can get feedback from people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3012" target="_blank">00:50:12.920</a></span> | <span class="t">so that we'll have enough like solid training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3015" target="_blank">00:50:15.880</a></span> | <span class="t">so we can actually train a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3018" target="_blank">00:50:18.880</a></span> | <span class="t">The main purpose of it being faster performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3022" target="_blank">00:50:22.640</a></span> | <span class="t">as Vibhu mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3024" target="_blank">00:50:24.320</a></span> | <span class="t">then you can respond in milliseconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3028" target="_blank">00:50:28.400</a></span> | <span class="t">versus multiple seconds or tens of seconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3033" target="_blank">00:50:33.400</a></span> | <span class="t">if you're using an LLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3035" target="_blank">00:50:35.440</a></span> | <span class="t">(mouse clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3038" target="_blank">00:50:38.200</a></span> | <span class="t">- Well, thank you, Eric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3054" target="_blank">00:50:54.760</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3055" target="_blank">00:50:55.600</a></span> | <span class="t">- Always appreciate the OG papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3057" target="_blank">00:50:57.240</a></span> | <span class="t">- Yeah, it's good to--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3062" target="_blank">00:51:02.720</a></span> | <span class="t">- Do you wanna ask one volunteer next week?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3065" target="_blank">00:51:05.640</a></span> | <span class="t">- Any paper, it doesn't have to be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3067" target="_blank">00:51:07.240</a></span> | <span class="t">I'll look back at anything that I'm interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3069" target="_blank">00:51:09.800</a></span> | <span class="t">(mouse clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3075" target="_blank">00:51:15.560</a></span> | <span class="t">- Yeah, I don't know if there's any paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3086" target="_blank">00:51:26.800</a></span> | <span class="t">that's caught my eye recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3088" target="_blank">00:51:28.280</a></span> | <span class="t">I guess like we talked a little bit about embedding papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3093" target="_blank">00:51:33.320</a></span> | <span class="t">people are interested in embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3095" target="_blank">00:51:35.120</a></span> | <span class="t">- Did we ever do the Geno2 embeddings paper?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3099" target="_blank">00:51:39.560</a></span> | <span class="t">- No, there's also NOMIC embed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3103" target="_blank">00:51:43.680</a></span> | <span class="t">I thought like, I didn't see the Geno one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3107" target="_blank">00:51:47.000</a></span> | <span class="t">but the NOMIC one was pretty detailed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3108" target="_blank">00:51:48.640</a></span> | <span class="t">in terms of what their process was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3111" target="_blank">00:51:51.680</a></span> | <span class="t">So, interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3113" target="_blank">00:51:53.520</a></span> | <span class="t">- I might be mixing up the paper thought,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3116" target="_blank">00:51:56.600</a></span> | <span class="t">but I think I remember we went through one embedding paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3121" target="_blank">00:52:01.480</a></span> | <span class="t">Maybe it's NOMIC, maybe it's NOMIC, I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3124" target="_blank">00:52:04.080</a></span> | <span class="t">But yeah, probably it wasn't there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3126" target="_blank">00:52:06.680</a></span> | <span class="t">- The NOMIC compares directly to Geno.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3133" target="_blank">00:52:13.120</a></span> | <span class="t">I have the exact same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3134" target="_blank">00:52:14.880</a></span> | <span class="t">I haven't seen the NOMIC one as much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3136" target="_blank">00:52:16.320</a></span> | <span class="t">I just know Geno was the one open source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3138" target="_blank">00:52:18.920</a></span> | <span class="t">AK context, very detailed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3141" target="_blank">00:52:21.800</a></span> | <span class="t">Here's how to do embeddings from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3143" target="_blank">00:52:23.360</a></span> | <span class="t">and fine tune them paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3144" target="_blank">00:52:24.840</a></span> | <span class="t">But I guess if NOMIC is the same thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3147" target="_blank">00:52:27.000</a></span> | <span class="t">50/50 if anyone wants to take one or both,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3149" target="_blank">00:52:29.680</a></span> | <span class="t">I would love both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3151" target="_blank">00:52:31.600</a></span> | <span class="t">- Oh, there's Geno three now, crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3153" target="_blank">00:52:33.440</a></span> | <span class="t">- Okay, well, I will volunteer for NOMIC or Geno.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3169" target="_blank">00:52:49.720</a></span> | <span class="t">If anyone else has papers they wanna cover in the meantime,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3174" target="_blank">00:52:54.800</a></span> | <span class="t">let's cover them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3175" target="_blank">00:52:55.720</a></span> | <span class="t">but otherwise I don't wanna drag this too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3178" target="_blank">00:52:58.800</a></span> | <span class="t">Yeah, nice chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3181" target="_blank">00:53:01.040</a></span> | <span class="t">- All right, and thanks, Eric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3184" target="_blank">00:53:04.080</a></span> | <span class="t">- Yeah, thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3185" target="_blank">00:53:05.240</a></span> | <span class="t">- Thanks, Eric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3187" target="_blank">00:53:07.000</a></span> | <span class="t">Thanks, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3187" target="_blank">00:53:07.840</a></span> | <span class="t">- Bye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3188" target="_blank">00:53:08.680</a></span> | <span class="t">- See ya.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=V64q3p7DNjc&t=3189" target="_blank">00:53:09.520</a></span> | <span class="t">See ya.</span></div></div></body></html>