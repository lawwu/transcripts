<html><head><title>RL for Autonomous Coding — Aakanksha Chowdhery, Reflection.ai</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>RL for Autonomous Coding — Aakanksha Chowdhery, Reflection.ai</h2><a href="https://www.youtube.com/watch?v=QluDzKVfp6A"><img src="https://i.ytimg.com/vi_webp/QluDzKVfp6A/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=0">0:0</a> Introduction to LLMs and Scaling Laws<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=101">1:41</a> Emergent Behavior in LLMs<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=240">4:0</a> Reinforcement Learning from Human Feedback (RLHF)<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=371">6:11</a> Inference-Time Scaling and Verification<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=633">10:33</a> Challenges with Inference-Time Scaling<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=676">11:16</a> The Next Frontier: Reinforcement Learning for Correct Generation<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=800">13:20</a> Challenges in Scaling RL<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=898">14:58</a> Autonomous Coding as a Prime Domain for RL<br><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=953">15:53</a> Reflection.ai's Mission<br><br><div style="text-align: left;"><a href="./QluDzKVfp6A.html">Whisper Transcript</a> | <a href="./transcript_QluDzKVfp6A.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, everyone. I'm Akhan Shaw. I was at Google for more than six years, and I led the research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=21" target="_blank">00:00:21.360</a></span> | <span class="t">for Palm, and I was a lead researcher in Gemini. These days, I'm working on pushing the frontier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=27" target="_blank">00:00:27.120</a></span> | <span class="t">for Autonomous Coding with Reinforcement Learning. So just to recap the arc of how we have progressed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=35" target="_blank">00:00:35.840</a></span> | <span class="t">in large language models and why Autonomous Coding and why now. So I think everyone here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=44" target="_blank">00:00:44.640</a></span> | <span class="t">or those of you who don't remember, in 2020, there was this breakthrough paper that came out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">which talked about scaling laws for large language models. And if you were to take a 30-second recap,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=56" target="_blank">00:00:56.160</a></span> | <span class="t">all the main thing it said was that there's a power law relationship between the test loss of large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=61" target="_blank">00:01:01.920</a></span> | <span class="t">language models. So if you use more compute, more data, and put more parameters in your machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=68" target="_blank">00:01:08.400</a></span> | <span class="t">learning model, which is a transformer model, you will get more performant models. And it will not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=75" target="_blank">00:01:15.680</a></span> | <span class="t">be performant just in the domain in which you are training the model. It will actually be performant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=80" target="_blank">00:01:20.160</a></span> | <span class="t">and it will generalize to many other domains. And the generalization was pretty much a feature in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=87" target="_blank">00:01:27.360</a></span> | <span class="t">particular case. So as the large language models got bigger, we saw continuous improvement across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=94" target="_blank">00:01:34.320</a></span> | <span class="t">benchmarks to the point that they're starting to get saturated now. And the other interesting thing was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=100" target="_blank">00:01:40.320</a></span> | <span class="t">that we saw emergent behavior where capabilities were emerging in large language models that were not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=106" target="_blank">00:01:46.240</a></span> | <span class="t">present in smaller models. And this is a classic slide that I show for the work that we did in Palm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=114" target="_blank">00:01:54.160</a></span> | <span class="t">So typically when you go about trying to solve math problems and you give the model some examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=120" target="_blank">00:02:00.720</a></span> | <span class="t">on the left you have a math problem around tennis balls, and then you give a second problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=126" target="_blank">00:02:06.160</a></span> | <span class="t">the model output looks wrong. But what Palm and the subsequent set of papers showed was that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=132" target="_blank">00:02:12.880</a></span> | <span class="t">if you ask the model to output its reasoning chains, which has become a very common concept now, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=139" target="_blank">00:02:19.440</a></span> | <span class="t">this is, remember, 2021, so four years ago, if you ask the model to show its reasoning chains, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=146" target="_blank">00:02:26.080</a></span> | <span class="t">the answer actually is correct. So basically by getting the model to output its chain of thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=152" target="_blank">00:02:32.400</a></span> | <span class="t">or reasoning chains, the model performance improves. And this capability particularly emerged in large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=159" target="_blank">00:02:39.920</a></span> | <span class="t">language models. These are all the models. So Lambda and Palm were the state-of-the-art models about three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=165" target="_blank">00:02:45.840</a></span> | <span class="t">years ago. And what I'm showing on x-axis is the increasing number of parameters. Palm was scaled all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=172" target="_blank">00:02:52.160</a></span> | <span class="t">the way up to 540 billion parameters. No one actually publishes the number of parameters these days, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=177" target="_blank">00:02:57.840</a></span> | <span class="t">you have to live with the graphs from three years ago or the open source stuff that's coming out with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=183" target="_blank">00:03:03.040</a></span> | <span class="t">DeepSeq and Quen models. But what y-axis is showing is that the solve rate on middle school math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=189" target="_blank">00:03:09.600</a></span> | <span class="t">word problems was increasing with the number of parameters in the models. And it was essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=196" target="_blank">00:03:16.160</a></span> | <span class="t">increasing mainly when you are prompting the models and asking them to show chain of thought. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=202" target="_blank">00:03:22.560</a></span> | <span class="t">led to all kinds of prompting techniques where you ask the model to think step by step. You even go and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=207" target="_blank">00:03:27.280</a></span> | <span class="t">bribe the model and such, and you ask the model nicely or not. So this was all kinds of fun stuff. And I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=215" target="_blank">00:03:35.120</a></span> | <span class="t">the thing that really stood out from this generation of models few years ago was that this capability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=221" target="_blank">00:03:41.680</a></span> | <span class="t">capability was not just limited to math problems. It was basically generalizing across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=228" target="_blank">00:03:48.240</a></span> | <span class="t">a whole bunch of domains anywhere from question answering in other languages to puzzle problems to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=234" target="_blank">00:03:54.240</a></span> | <span class="t">multitask natural language understanding problems. And what this led to next was that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=242" target="_blank">00:04:02.240</a></span> | <span class="t">now that these models could reason, we could get them to follow instructions. So the first set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=248" target="_blank">00:04:08.800</a></span> | <span class="t">applications that became possible with these large language models were chatbot applications. So everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=254" target="_blank">00:04:14.480</a></span> | <span class="t">remembers that ChatGPT and now Gemini and various other chatbots have become extremely popular. All of us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=261" target="_blank">00:04:21.840</a></span> | <span class="t">use them all the time. But what made them really possible was that when you give instructions to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=267" target="_blank">00:04:27.360</a></span> | <span class="t">to go do something, it's actually able to do it. And the way it learns that is actually based on reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=273" target="_blank">00:04:33.280</a></span> | <span class="t">learning. And the reinforcement learning data that we're giving to the model in this particular case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=278" target="_blank">00:04:38.720</a></span> | <span class="t">is essentially data based on human feedback. So you're basically saying, okay, here is a set of questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=286" target="_blank">00:04:46.960</a></span> | <span class="t">And if I were to give it to a human and it were there were two answers, which one would the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=293" target="_blank">00:04:53.200</a></span> | <span class="t">prefer? And if you have enough of this data and you train your model, you would actually end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=298" target="_blank">00:04:58.960</a></span> | <span class="t">a better performance because you taught the model which set of responses to prefer. And this actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=305" target="_blank">00:05:05.120</a></span> | <span class="t">doesn't only work in chatbot applications, it also works in code. So on the bottom right, I'm showing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=311" target="_blank">00:05:11.360</a></span> | <span class="t">even if you were to do this for applications in code, you start to see some performance improvements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=316" target="_blank">00:05:16.960</a></span> | <span class="t">Now, of course, the question is that last year, there was a whole bunch of debate as to are we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=323" target="_blank">00:05:23.200</a></span> | <span class="t">hitting the wall in terms of performance of large language models, pre-training is not giving any gains,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=328" target="_blank">00:05:28.560</a></span> | <span class="t">or all of these questions were on the horizon. So what is next? And one of the key questions to remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=337" target="_blank">00:05:37.200</a></span> | <span class="t">in all of this is that when you go and pre-train the models, you end up spending a lot of money</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=342" target="_blank">00:05:42.160</a></span> | <span class="t">on training these models. It could be tens of millions of dollars. And when you do inference on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=347" target="_blank">00:05:47.920</a></span> | <span class="t">the models, it's extremely cheap. These numbers are not endorsed by any of the companies I worked at,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=354" target="_blank">00:05:54.800</a></span> | <span class="t">but these are public numbers from public sources. So going back to the main point that I want to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=361" target="_blank">00:06:01.760</a></span> | <span class="t">here is that training is extremely costly. So if you constantly try to scale up the model size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=367" target="_blank">00:06:07.920</a></span> | <span class="t">you end up in this regime of like, if it's not giving performance gains, then can we get performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=375" target="_blank">00:06:15.120</a></span> | <span class="t">gains at inference time because inference calls are so cheap? And a key idea that was extremely useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=384" target="_blank">00:06:24.480</a></span> | <span class="t">here was that if you could get the models to generate multiple responses and then do majority voting. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=392" target="_blank">00:06:32.640</a></span> | <span class="t">in the example above, I'm showing that the prompt doesn't make sense, but you've given a mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=400" target="_blank">00:06:40.160</a></span> | <span class="t">problem to large language model and you're asking it to generate three answers independently. And then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=405" target="_blank">00:06:45.760</a></span> | <span class="t">basically do some voting on top of those answers. And if two answers match, then that's a majority vote. Or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=413" target="_blank">00:06:53.360</a></span> | <span class="t">like if in this room I were to ask a question and all of you said, yes, then that is a majority vote.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=418" target="_blank">00:06:58.720</a></span> | <span class="t">So similarly in large language models, if you can get the model to like generate many, many samples and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=424" target="_blank">00:07:04.240</a></span> | <span class="t">then consistently get it to, uh, like get many of those answers to agree, this notion of majority voting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=430" target="_blank">00:07:10.640</a></span> | <span class="t">or self consistency had shown gains. So this kind of scaling computed inference time was clearly one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=436" target="_blank">00:07:16.240</a></span> | <span class="t">avenue to go push on. Another avenue that emerged and showed substantial value was that you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=442" target="_blank">00:07:22.640</a></span> | <span class="t">sequentially revise your previous response. So as humans, oftentimes we write the first answer and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=449" target="_blank">00:07:29.680</a></span> | <span class="t">we go evaluate our answer and we're like, oh, there's some mistake here. It doesn't quite match. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=454" target="_blank">00:07:34.560</a></span> | <span class="t">you go fix it. So basically can we get LLMs to do the same kind of revision looking at previous set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=461" target="_blank">00:07:41.360</a></span> | <span class="t">revisions? And this was the second. So basically having longer, uh, chains of thought, uh, and getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=466" target="_blank">00:07:46.640</a></span> | <span class="t">the model to improve consistently in inference time based on that. And these kind of techniques, uh, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=472" target="_blank">00:07:52.960</a></span> | <span class="t">you could verify your correct answer. So in math or in programming where you have unit tests showed, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=478" target="_blank">00:07:58.960</a></span> | <span class="t">very clear gains. So what I'm showing you here is an example from, uh, uh, one of my colleagues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=484" target="_blank">00:08:04.400</a></span> | <span class="t">work, uh, at Stanford, uh, which is a publicly, uh, published, uh, paper. And on the y-axis, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=491" target="_blank">00:08:11.760</a></span> | <span class="t">pass at K or coverage score. And on the x-axis, we have a number of samples. So as you basically are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=497" target="_blank">00:08:17.920</a></span> | <span class="t">doing a lot of samples on the x-axis, your accuracy is improving, uh, with open source DeepSeq model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=503" target="_blank">00:08:23.840</a></span> | <span class="t">just taking more samples. So you're getting a very high score on Sweebench verified compared to even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=509" target="_blank">00:08:29.600</a></span> | <span class="t">state of the art back in end of 2024. Uh, of course, now all of these scores have pushed up and we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=516" target="_blank">00:08:36.000</a></span> | <span class="t">roughly somewhere around 80% already. But what we want to take away here is the fact that these lines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=525" target="_blank">00:08:45.200</a></span> | <span class="t">of work, they showed that inference time compute predictably gives us gains, especially in domains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=531" target="_blank">00:08:51.600</a></span> | <span class="t">where we can verify. If we know how to verify the answers, then we actually know how to translate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=538" target="_blank">00:08:58.800</a></span> | <span class="t">that into intelligence. And going back to my talk title, coding is one of those domains where we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=544" target="_blank">00:09:04.720</a></span> | <span class="t">do have the capability to verify. Um, and that gives us tremendous advantage in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=551" target="_blank">00:09:11.200</a></span> | <span class="t">building super intelligence on top of autonomous coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=556" target="_blank">00:09:16.160</a></span> | <span class="t">Of course, now you ask the question of what does automated verification mean here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=560" target="_blank">00:09:20.800</a></span> | <span class="t">So for inference time scaling to work, you need basically some way to say this output is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=568" target="_blank">00:09:28.720</a></span> | <span class="t">Now in math, um, this is a very simple example. If you were to give the input to solve this mathematical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=574" target="_blank">00:09:34.560</a></span> | <span class="t">equation, um, and if you were to do the same calculation on a calculator, you can actually verify that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=580" target="_blank">00:09:40.320</a></span> | <span class="t">that that problem is correct or that solution is correct. Um, and similarly in math, you have formal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=587" target="_blank">00:09:47.920</a></span> | <span class="t">proofs so you can actually verify that things are correct. Uh, in coding, you have unit tests in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=592" target="_blank">00:09:52.960</a></span> | <span class="t">compilers. You can actually generate the code and then use Pytorch as a verifier, uh, Pytorch the compiler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=598" target="_blank">00:09:58.800</a></span> | <span class="t">as a verifier. And in fact, uh, in domains where you don't have, uh, this kind of verification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=605" target="_blank">00:10:05.040</a></span> | <span class="t">then there's a large gap. If you were to generate a lot of solutions and then do majority</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=610" target="_blank">00:10:10.240</a></span> | <span class="t">voting, you actually don't get as much gains. So what this roughly meant was that, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=615" target="_blank">00:10:15.600</a></span> | <span class="t">so inference time scaling would work in scenarios where I have automated verification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=620" target="_blank">00:10:20.640</a></span> | <span class="t">but that doesn't quite solve the problem for it to have real world impact. And the reason for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=626" target="_blank">00:10:26.880</a></span> | <span class="t">is shown in this graph as to typically, uh, if you do majority voting and these, uh, this is across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=633" target="_blank">00:10:33.520</a></span> | <span class="t">multiple different models on GSM 8K, which is middle school math problems and another math benchmark. Uh, if you were to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=640" target="_blank">00:10:40.160</a></span> | <span class="t">sort them by correct fraction, you have to sample a lot. The correct generations could be very rare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=645" target="_blank">00:10:45.920</a></span> | <span class="t">So who has time to sample 10,000 times and then get a correct solution? You would be sitting there waiting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=651" target="_blank">00:10:51.920</a></span> | <span class="t">just finding the correct solution unless you can actually figure out where the correct generation is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=658" target="_blank">00:10:58.960</a></span> | <span class="t">So basically scaling inference time compute with just majority voting or longer reasoning chains is great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=665" target="_blank">00:11:05.840</a></span> | <span class="t">in the sense that there are some correct solutions somewhere there, but it doesn't work well across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=670" target="_blank">00:11:10.560</a></span> | <span class="t">the board. So what will get these models to learn to generate correctly, uh, during training? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=678" target="_blank">00:11:18.480</a></span> | <span class="t">in the chat bot application scenario, we saw that RL with human feedback did work. So can we apply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=685" target="_blank">00:11:25.040</a></span> | <span class="t">same principle here and get the model to generate correctly in, uh, where, where we can automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=691" target="_blank">00:11:31.120</a></span> | <span class="t">verify the outputs? So our belief at, uh, reflection is that the next frontier for scaling is reinforcement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=698" target="_blank">00:11:38.000</a></span> | <span class="t">learning. And we already have proof points from some of the frontier labs as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=702" target="_blank">00:11:42.800</a></span> | <span class="t">And as David Silver, um, uh, sudden published recently, they agree with, uh, or, or rather they, they are the pioneers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=710" target="_blank">00:11:50.800</a></span> | <span class="t">in, uh, in reinforcement learning. They say that we are basically entering the era of experience, like starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=717" target="_blank">00:11:57.280</a></span> | <span class="t">from alpha go and alpha zero, uh, where you had an era of simulation and the next set of large language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=724" target="_blank">00:12:04.800</a></span> | <span class="t">era was where you scaled up with RL using human data. But the next era from this year is really the era of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=732" target="_blank">00:12:12.240</a></span> | <span class="t">experience, which was, which will lead us to super intelligence. So reinforcement learning will be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=736" target="_blank">00:12:16.960</a></span> | <span class="t">fundamental component in building, uh, super intelligent systems, uh, especially in areas where we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=743" target="_blank">00:12:23.280</a></span> | <span class="t">automated, uh, verification. And some, uh, proof point for why this makes sense is that in math, uh, over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=752" target="_blank">00:12:32.000</a></span> | <span class="t">several papers, this is, uh, results from 01, but over several papers, we have already seen examples that if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=759" target="_blank">00:12:39.280</a></span> | <span class="t">give the model on the right side, uh, test time compute, uh, on the Y axis, test time compute the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=765" target="_blank">00:12:45.440</a></span> | <span class="t">inference time scaling and you measure accuracy on the X axis, it should go up. Um, but as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=772" target="_blank">00:12:52.320</a></span> | <span class="t">repeat this process, uh, and with the reinforcement learning, then the training time compute going up on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=778" target="_blank">00:12:58.160</a></span> | <span class="t">X axis also improves the accuracy on Y axis for a challenging benchmark in math called Amy. Uh, most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=784" target="_blank">00:13:04.480</a></span> | <span class="t">of these benchmarks saturate within a year as you probably have learned by now. So, uh, this benchmark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=789" target="_blank">00:13:09.760</a></span> | <span class="t">is already saturated. Um, so now that I've hopefully convinced you that reinforcement learning and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=798" target="_blank">00:13:18.000</a></span> | <span class="t">scaling reinforcement learning is, uh, the next frontier, you'd be like, okay, so why are, why is not everyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=804" target="_blank">00:13:24.960</a></span> | <span class="t">doing it? What's so challenging about it? So as I have built large language models before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=809" target="_blank">00:13:29.920</a></span> | <span class="t">a big part of building, uh, these systems, uh, ends up being that the machine learning plus system stack</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=816" target="_blank">00:13:36.160</a></span> | <span class="t">for these, uh, systems themselves is very challenging. So here is, um, an example of, uh, why scaling up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=824" target="_blank">00:13:44.000</a></span> | <span class="t">reinforcement learning is challenging. So if you are trying to do reinforcement learning with, uh, PPO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=829" target="_blank">00:13:49.440</a></span> | <span class="t">which is, uh, one of the, uh, algorithms used for RL with human feedback, um, then it moved to, uh, DPO,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=836" target="_blank">00:13:56.480</a></span> | <span class="t">you have to keep four copies of, uh, different models. Uh, so if you imagine a really large model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=843" target="_blank">00:14:03.120</a></span> | <span class="t">and then you have to keep four copies, then you have to arrange them somewhere on GPUs in your large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=848" target="_blank">00:14:08.080</a></span> | <span class="t">cluster. You, you can have some fun figuring out the exact layout and, um, it's, it's, it's a fun and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=854" target="_blank">00:14:14.800</a></span> | <span class="t">interesting problem, but it's a hard problem in the sense that, uh, to make maximum utilization of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=860" target="_blank">00:14:20.080</a></span> | <span class="t">these systems and, and arranging them in the right way, just building that system is extremely hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=865" target="_blank">00:14:25.440</a></span> | <span class="t">And, uh, deep seek actually showed, uh, with deep seek math that GRPO, uh, gets rid of the value model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=871" target="_blank">00:14:31.200</a></span> | <span class="t">and it only has three copies of the model, but that doesn't, that's still a very challenging problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=876" target="_blank">00:14:36.080</a></span> | <span class="t">So scaling up RL, uh, is more, even more challenging, um, than scaling up, um, LLMs because you have multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=884" target="_blank">00:14:44.800</a></span> | <span class="t">copies of the model and you have a training loop and an inference loop. And then on the machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=890" target="_blank">00:14:50.080</a></span> | <span class="t">side on the, on the reinforcement learning side, you also suffer a lot from reward hacking. If you're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=895" target="_blank">00:14:55.280</a></span> | <span class="t">uh, the model that is deciding that this is the correct answer is a neural reward model. So you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=900" target="_blank">00:15:00.720</a></span> | <span class="t">uh, as we discussed before in autonomous coding applications, you do have the ability to verify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=906" target="_blank">00:15:06.400</a></span> | <span class="t">your output, uh, which roughly means that you can decide this is the correct answer or not. Uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=912" target="_blank">00:15:12.560</a></span> | <span class="t">that's how sweet bench verified scores, uh, work today. Uh, you have execution feedback,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=918" target="_blank">00:15:18.320</a></span> | <span class="t">you have unit tests. So all of these possibilities, of course, um, this is an ongoing list. All of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=924" target="_blank">00:15:24.800</a></span> | <span class="t">possibilities may mean that you can design better reward functions. Okay. So this means that autonomous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=931" target="_blank">00:15:31.680</a></span> | <span class="t">coding is a great domain for scaling up RL. Then the question becomes, how does this have real world impact?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=937" target="_blank">00:15:37.680</a></span> | <span class="t">So in software engineering applications, generation of code is only one part of the system. If you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=944" target="_blank">00:15:44.240</a></span> | <span class="t">look at end to end workflows for software engineering, there is many more parts to that system. How do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=949" target="_blank">00:15:49.440</a></span> | <span class="t">scale up your system to generalize across all of those domains? So that's the problem we are trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=955" target="_blank">00:15:55.200</a></span> | <span class="t">solve at reflection. Uh, our mission is that we would like to build some super intelligence and we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=960" target="_blank">00:16:00.880</a></span> | <span class="t">starting with autonomous coding as the root node problem for this, um, a mission. And, uh, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=968" target="_blank">00:16:08.240</a></span> | <span class="t">a team of about 35 pioneers, um, who are, who have pioneered various, uh, legendary works in LLMs and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=976" target="_blank">00:16:16.160</a></span> | <span class="t">reinforcement learning. So if you're excited about this mission, uh, you can reach out to, um, one of us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=982" target="_blank">00:16:22.880</a></span> | <span class="t">Um, or my, uh, my emails, uh, my last name at reflection.ai and we would love to work with you. And with that, I can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=989" target="_blank">00:16:29.440</a></span> | <span class="t">questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=997" target="_blank">00:16:37.280</a></span> | <span class="t">All right. Um, same protocol as last time. If you have a question, please come up to one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1001" target="_blank">00:16:41.840</a></span> | <span class="t">three microphones we have distributed throughout. We can probably take one or two questions. So if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1006" target="_blank">00:16:46.400</a></span> | <span class="t">want to ask something, um, feel free. Um, I guess I can, I I'll do the first one while people are coming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1011" target="_blank">00:16:51.440</a></span> | <span class="t">up. So I'm curious, um, it seems like the foundation models are trying to build one model and deploy it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1019" target="_blank">00:16:59.040</a></span> | <span class="t">across everything. Do you have an opinion with the work you're doing right now? If you think that's the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1023" target="_blank">00:17:03.600</a></span> | <span class="t">approach or if you think there'll be more specialization on different languages or even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1027" target="_blank">00:17:07.680</a></span> | <span class="t">like individual code bases, um, or do you feel like the best approach is just to have like one model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1032" target="_blank">00:17:12.160</a></span> | <span class="t">that's trained across the, the greatest diversity of tasks possible? Uh, I think I will answer your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1036" target="_blank">00:17:16.880</a></span> | <span class="t">question, uh, in terms of building coding agents does require, um, multiple capabilities and how you get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1043" target="_blank">00:17:23.680</a></span> | <span class="t">there, you will definitely need multiple LLM calls and then whether that's one model or multiple models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1048" target="_blank">00:17:28.800</a></span> | <span class="t">I think that's the secret sauce right now for most people. Fair enough. All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1053" target="_blank">00:17:33.200</a></span> | <span class="t">Please. Hi. Um, I'm wondering in the slide with the chart of error of simulation, error of something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1060" target="_blank">00:17:40.880</a></span> | <span class="t">and error of experience, uh, they had put in AlphaGo and, um, the previous one where also you, they played</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1070" target="_blank">00:17:50.480</a></span> | <span class="t">Star, Starcraft or something. They all used MCDS, uh, which I mean, maybe it's my unfamiliarity with them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1077" target="_blank">00:17:57.680</a></span> | <span class="t">but it's also data simulation. Uh, so we're using synthetic data for error of experience as well. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1085" target="_blank">00:18:05.600</a></span> | <span class="t">how does, why is that called simulation and why is what we're doing right now not called simulation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1091" target="_blank">00:18:11.520</a></span> | <span class="t">What's the sort of overlap between simulation experience? How does that, how do you think about that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1095" target="_blank">00:18:15.920</a></span> | <span class="t">I can ask Dave that question, you know, but going back to the point, I think, I think the better way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1102" target="_blank">00:18:22.560</a></span> | <span class="t">answer that question is, uh, roughly what Greg covered in the last talk where his comment was that, um, so in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1108" target="_blank">00:18:28.800</a></span> | <span class="t">gaming you can envision what scenarios might happen next and you're basically using that to build your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1115" target="_blank">00:18:35.760</a></span> | <span class="t">reinforcement learning. So you're doing rollouts and you're, you're basically building, uh, based on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1120" target="_blank">00:18:40.320</a></span> | <span class="t">uh, in, in real world, in most scenarios, you have an imperfect rollout. So you don't have full knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1128" target="_blank">00:18:48.720</a></span> | <span class="t">of how the system might works. Um, simulation is possible in certain domains where you do build a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1136" target="_blank">00:18:56.160</a></span> | <span class="t">world model, uh, which is closer to robotics and all the work that's happening in the physical AI space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1142" target="_blank">00:19:02.400</a></span> | <span class="t">right. But in the, in the real world applications, which is what we're targeting, uh, you will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1148" target="_blank">00:19:08.560</a></span> | <span class="t">imperfect things. So you have to actually experience the real world and you have to collect some data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1152" target="_blank">00:19:12.960</a></span> | <span class="t">and that data is not going to be in any way complete, nor will it complete early search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=QluDzKVfp6A&t=1158" target="_blank">00:19:18.320</a></span> | <span class="t">the exponential search space that could exist.</span></div></div></body></html>