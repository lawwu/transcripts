<html><head><title>What We Learned from Using LLMs in Pinterest — Mukuntha Narayanan, Han Wang, Pinterest</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>What We Learned from Using LLMs in Pinterest — Mukuntha Narayanan, Han Wang, Pinterest</h2><a href="https://www.youtube.com/watch?v=XdAWgO11zuk"><img src="https://i.ytimg.com/vi_webp/XdAWgO11zuk/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=0">0:0</a> Introduction to Pinterest and its search functionality.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=112">1:52</a> Overview of the Pinterest search backend architecture.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=149">2:29</a> The search relevance model.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=175">2:55</a> Key learnings from using LLMs for search relevance.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=304">5:4</a> The value of VLM-generated captions and user actions as content annotations.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=436">7:16</a> Productionizing LLMs with knowledge distillation.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=734">12:14</a> The utility of relevance-tuned LLM embeddings as general-purpose semantic representations.<br><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=835">13:55</a> Q&A session.<br><br><div style="text-align: left;"><a href="./XdAWgO11zuk.html">Whisper Transcript</a> | <a href="./transcript_XdAWgO11zuk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=0" target="_blank">00:00:00.600</a></span> | <span class="t">Hi, everyone. Thanks for joining the talk today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=17" target="_blank">00:00:17.240</a></span> | <span class="t">We are super excited to be here and share some of the learnings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=22" target="_blank">00:00:22.100</a></span> | <span class="t">we have from integrating the LM into Pinterest search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=27" target="_blank">00:00:27.400</a></span> | <span class="t">My name is Khan, and today I will be presenting with Mukunda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=30" target="_blank">00:00:30.300</a></span> | <span class="t">And we are both machine learning engineers from search relevance team at Pinterest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=34" target="_blank">00:00:34.440</a></span> | <span class="t">So start with a brief introduction to Pinterest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=39" target="_blank">00:00:39.140</a></span> | <span class="t">Pinterest is a visual discovery platform where pinners can come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=43" target="_blank">00:00:43.900</a></span> | <span class="t">to find inspiration to create a life they love.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=46" target="_blank">00:00:46.600</a></span> | <span class="t">And there are three main discovery services on Pinterest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">The home feed, the related things and search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=53" target="_blank">00:00:53.860</a></span> | <span class="t">And today's talk will be focusing on search and where the user can type in their queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=60" target="_blank">00:01:00.900</a></span> | <span class="t">and find useful, inspiring content based on their information need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=66" target="_blank">00:01:06.360</a></span> | <span class="t">And we will share how we leverage LM to improve the search relevance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=70" target="_blank">00:01:10.960</a></span> | <span class="t">Here are some key statistics for Pinterest search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=77" target="_blank">00:01:17.460</a></span> | <span class="t">Every month we handled over 6 billion searches with billions of pins to search from covering topics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=84" target="_blank">00:01:24.500</a></span> | <span class="t">from recipe, home decor, travel, fashion and beyond.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=88" target="_blank">00:01:28.820</a></span> | <span class="t">And at Pinterest search is remarkably global and multilingual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=93" target="_blank">00:01:33.920</a></span> | <span class="t">We support over 45 languages and reaching pinners in more than 100 countries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=100" target="_blank">00:01:40.500</a></span> | <span class="t">These numbers highlight the importance of search at Pinterest and why we are investing in search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=106" target="_blank">00:01:46.900</a></span> | <span class="t">relevance to improving the search experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=109" target="_blank">00:01:49.240</a></span> | <span class="t">So this is an overview of how Pinterest search work at the back end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=117" target="_blank">00:01:57.540</a></span> | <span class="t">So it's similar to many recommendation system and industry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=122" target="_blank">00:02:02.540</a></span> | <span class="t">it has query understanding, retrieval, re-ranking and the blending stage, and finally produced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=128" target="_blank">00:02:08.700</a></span> | <span class="t">on relevant and engagement search things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=131" target="_blank">00:02:11.400</a></span> | <span class="t">And in today's talk, we'll be focusing on the semantic relevance modeling that happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=138" target="_blank">00:02:18.300</a></span> | <span class="t">at the re-ranking stage and share about how we use LM to improve the search relevance on the search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=149" target="_blank">00:02:29.500</a></span> | <span class="t">Okay, so here's our search relevance model, which is essentially a classification model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=156" target="_blank">00:02:36.460</a></span> | <span class="t">Given a search query and the pin, the model will predict how much the pin is relevant to this search query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=163" target="_blank">00:02:43.740</a></span> | <span class="t">And to measure this, we use a five point scale ranging from the most relevant to most irrelevant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=173" target="_blank">00:02:53.700</a></span> | <span class="t">All right, now we are going to share some key learnings we have from using the LM to improve search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=180" target="_blank">00:03:00.860</a></span> | <span class="t">Pinterest search relevance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=182" target="_blank">00:03:02.700</a></span> | <span class="t">And here are four main takeaways that we would like to go into more details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=188" target="_blank">00:03:08.260</a></span> | <span class="t">Lesson one, LMs are good at relevance prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=197" target="_blank">00:03:17.740</a></span> | <span class="t">So before I present the result, let me first give a quick overview of the model architecture that we are using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=205" target="_blank">00:03:25.180</a></span> | <span class="t">We contain the query and the pin text together and pass them into an LM to get an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=214" target="_blank">00:03:34.940</a></span> | <span class="t">So this is called cross encoder structure where we can better capture the interaction between the query and the pin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=223" target="_blank">00:03:43.780</a></span> | <span class="t">And then we see the embedding from LM into an MLP layer to produce a five dimensional factor, which correspond to the five relevant levels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=234" target="_blank">00:03:54.820</a></span> | <span class="t">And during training, we fine tune some open source LM using Pinterest internal data and to better adapt the model to our Pinterest content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=246" target="_blank">00:04:06.880</a></span> | <span class="t">And here I'd like to share some results to demonstrate that the usefulness of LM and as a baseline, we use search search, which is a Pinterest in house content and the query embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=264" target="_blank">00:04:24.220</a></span> | <span class="t">And so if you look at the table, you can see that the LM has substantially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=270" target="_blank">00:04:30.080</a></span> | <span class="t">improve the performance of the relevance prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=273" target="_blank">00:04:33.980</a></span> | <span class="t">And as we use more advanced LMs and increase the model size, the performance keeps improving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=281" target="_blank">00:04:41.560</a></span> | <span class="t">And for example, the 8 billion landmass remodel gives 12% of improvement over the multilingual bird based model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=290" target="_blank">00:04:50.560</a></span> | <span class="t">and 20% of improvement over the search search search embedding model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=294" target="_blank">00:04:54.520</a></span> | <span class="t">So the lesson here is that LM, they are quite good at relevance prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">Um, lesson two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=305" target="_blank">00:05:05.920</a></span> | <span class="t">The mission language model generated captions and the user actions can be quite useful for content annotations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=316" target="_blank">00:05:16.160</a></span> | <span class="t">So to use LM for search for relevance prediction, we need to view a text representation of each pin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=324" target="_blank">00:05:24.560</a></span> | <span class="t">And here I listed several features that we used in our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=329" target="_blank">00:05:29.500</a></span> | <span class="t">Besides the, um, the title of description of the pin, we also include, um, the VON generated synthetic image caption to directly extract information from the image itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=343" target="_blank">00:05:43.620</a></span> | <span class="t">And besides that, we add some, um, user engagement based feature, like the word titles, um, for the user curated board that the pin has been saved to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=354" target="_blank">00:05:54.680</a></span> | <span class="t">Or, um, the queries that led to the highest engagement with this pin on search surface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=361" target="_blank">00:06:01.780</a></span> | <span class="t">So these two user action based features, um, serves as additional annotation for the content and, um, here the five source of feature together helps to build a more, um, robust and comprehensive text representation for each pin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=381" target="_blank">00:06:21.940</a></span> | <span class="t">Uh, we, uh, we, to understand the importance of each vertex feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=387" target="_blank">00:06:27.180</a></span> | <span class="t">We also did some oblation studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=389" target="_blank">00:06:29.600</a></span> | <span class="t">We used the, um, VON generated image caption as a baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=394" target="_blank">00:06:34.840</a></span> | <span class="t">And, um, as you can see itself already, um, provide a very solid baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=402" target="_blank">00:06:42.100</a></span> | <span class="t">And as we sequentially add more vertex feature, we keep seeing performance improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=407" target="_blank">00:06:47.660</a></span> | <span class="t">And this indicate that enriching the vertex feature is quite useful for relevance prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=414" target="_blank">00:06:54.040</a></span> | <span class="t">And notably, um, the last two rows of the table shows the performance gain we have by adding these user action based features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=424" target="_blank">00:07:04.540</a></span> | <span class="t">So these features turned out to be quite useful content annotation that help model better understand the content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=431" target="_blank">00:07:11.680</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=434" target="_blank">00:07:14.500</a></span> | <span class="t">Um, next I will hand over to Mukunda to talk about how we use knowledge distillation to productionize this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=441" target="_blank">00:07:21.300</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=443" target="_blank">00:07:23.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=443" target="_blank">00:07:23.980</a></span> | <span class="t">Uh, so now we have a good relevance model, which is good at predicting search relevance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=449" target="_blank">00:07:29.100</a></span> | <span class="t">But how do we actually scale this up without bankrupting Pinterest?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=453" target="_blank">00:07:33.100</a></span> | <span class="t">Uh, usually the answer is knowledge distillation into smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=456" target="_blank">00:07:36.700</a></span> | <span class="t">Um, and this is the production-served relevant student model that we distilled from the teacher model using semi-supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=464" target="_blank">00:07:44.740</a></span> | <span class="t">Uh, the student model is trained to predict five-scale relevant scores too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=469" target="_blank">00:07:49.400</a></span> | <span class="t">Uh, it trains using the five-scale soft scores produced by the teacher model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=475" target="_blank">00:07:55.060</a></span> | <span class="t">Um, and we produce data for this using a semi-supervised learning setup that, uh, I'll show in the next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=481" target="_blank">00:08:01.660</a></span> | <span class="t">So the LLM teacher model is trained on a small set of human label data that we get from human annotators who are trained in very specific segments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=492" target="_blank">00:08:12.460</a></span> | <span class="t">Uh, we fine tune and this is a multilingual language model, which uses pretty generic features with scale across a lot of different domains, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=502" target="_blank">00:08:22.500</a></span> | <span class="t">Um, and the way we get training data for the student, uh, uh, is through, uh, sampling from daily search logs, which is, um, although searches, uh, people make on Pinterest, uh, and since we sample daily, uh, this includes any trending queries, all the latest freshest pins on Pinterest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=523" target="_blank">00:08:43.440</a></span> | <span class="t">Um, and there's also remarkably global, like you mentioned, and only a small subset of this comes from the U S where most of our human label data comes from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=532" target="_blank">00:08:52.360</a></span> | <span class="t">Um, we sample from this and we label using the teacher and we scale it up pretty much a hundred X, uh, across different domains, languages, countries where, uh, the LLM teacher model produces pretty good labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=545" target="_blank">00:09:05.720</a></span> | <span class="t">We train the student model and this is the model that actually gets sort of online, um, and, uh, zooming into the student model, uh, also, this also has language models in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=558" target="_blank">00:09:18.620</a></span> | <span class="t">Uh, but unlike the teacher model, it's not a cross encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=561" target="_blank">00:09:21.480</a></span> | <span class="t">It, uh, is a by encoder, uh, which essentially means we don't have cross interactions between the pin and the query, uh, representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=569" target="_blank">00:09:29.980</a></span> | <span class="t">Um, the pin gets embedded separately, query gets embedded separately, and it also uses a lot of other features like, um, sort change that we previously mentioned for both embedding the query and the pin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=581" target="_blank">00:09:41.140</a></span> | <span class="t">Uh, we have graph stage embeddings, which Pinterest has published papers on, um, and omni-sage and a lot of other embedding features for query and pin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=589" target="_blank">00:09:49.540</a></span> | <span class="t">But we also use, uh, a lot of pin query text match statistics like BM25, which we've seen historically perform really well for predicting search relevance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=599" target="_blank">00:09:59.080</a></span> | <span class="t">Um, and the reason this scales well is the by encoder, uh, by encoder large language models can scale really well, uh, when we, uh, use offline inference and caching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=612" target="_blank">00:10:12.640</a></span> | <span class="t">Uh, the pin embedding here is entirely offline inferred on billions of pins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=616" target="_blank">00:10:16.960</a></span> | <span class="t">Uh, it uses predominantly the same text features that we mentioned on the teacher, uh, which helps distill efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=623" target="_blank">00:10:23.620</a></span> | <span class="t">Um, and, uh, we only, uh, re-infer, uh, these embeddings every time that these inputs meaningfully change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=632" target="_blank">00:10:32.580</a></span> | <span class="t">Uh, meaning that, uh, every time that we, uh, need new embeddings, um, it's only going to run on a few set of new pins, um, and, uh, this is offline inferred.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=644" target="_blank">00:10:44.400</a></span> | <span class="t">So none of this is happening online when a user issues a search query, uh, and the query embedding is pretty much, uh, real time inferred online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=652" target="_blank">00:10:52.140</a></span> | <span class="t">Uh, and search queries are pretty short.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=654" target="_blank">00:10:54.360</a></span> | <span class="t">Um, they don't occupy too many tokens, which means, uh, we can keep the latencies for the query embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=661" target="_blank">00:11:01.320</a></span> | <span class="t">Up to like a few milliseconds, um, and we also cache this, uh, because search queries get repeated a lot and we get around an 85% cache head rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=670" target="_blank">00:11:10.860</a></span> | <span class="t">Um, and yeah, this scales really well, uh, to actually solve Pinterest traffic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=676" target="_blank">00:11:16.140</a></span> | <span class="t">Um, the online results here, uh, the first four numbers are relevance, uh, measurements, and DCNG, uh, precision at eight, uh, measured on the US, Germany, France, uh, specific segments that we zoomed into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=691" target="_blank">00:11:31.620</a></span> | <span class="t">Um, we can actually see that we get relevance gains international, uh, internationally, even though we started with a very limited set of US data for this particular experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=702" target="_blank">00:11:42.900</a></span> | <span class="t">Um, and, uh, we also see that search fulfillment, which measures engagement on search, um, fulfilling actions, uh, also goes up, uh, also on non-US,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=713" target="_blank">00:11:53.280</a></span> | <span class="t">even though our, uh, starting data was predominantly US and, uh, uh, yeah, uh, large language models are very good at, uh, expanding across many different domains, countries, uh, even though, uh, they have, weren't explicitly trained for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=729" target="_blank">00:12:09.960</a></span> | <span class="t">Um, and, uh, this is a bonus, uh, we also found that relevance tuned large language models produce really good rich, uh, somatic representations, which are very good general purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=742" target="_blank">00:12:22.080</a></span> | <span class="t">Uh, this is the same production relevant student model that I shared on the previous slide, uh, and, uh, the pin embedding and the query embedding, uh, are basically three representations that we get from these models, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=756" target="_blank">00:12:36.000</a></span> | <span class="t">which can be used across Pinterest for representing pins and search queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=760" target="_blank">00:12:40.640</a></span> | <span class="t">Um, we also use this to represent boards using the titles, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=765" target="_blank">00:12:45.360</a></span> | <span class="t">Um, and we found that using these embeddings, especially since they've been distilled from a large language model teacher and also have large language models in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=775" target="_blank">00:12:55.980</a></span> | <span class="t">Uh, they are very good at semantic content representations, uh, and yeah, they perform pretty well across, uh, related pins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=785" target="_blank">00:13:05.940</a></span> | <span class="t">Um, and it's home feed and a lot of other surfaces where we've seen, uh, representations improved by adding these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=793" target="_blank">00:13:13.560</a></span> | <span class="t">Um, so let me go over the key takeaways again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=797" target="_blank">00:13:17.560</a></span> | <span class="t">Um, I think lesson one, we found that LLMs are really good at relevance prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=803" target="_blank">00:13:23.040</a></span> | <span class="t">Uh, lesson two, we found that visual language model captions are good, uh, good ways to imbue them with, uh, image representations and, uh, user actions are very good contract connotations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=816" target="_blank">00:13:36.640</a></span> | <span class="t">Um, three, uh, we found that knowledge distillation is a very good way to scale, uh, and efficiently serve models, uh, online and, uh, lesson four, uh, relevance tuning produces pretty rich representations, uh, that embed semantic representations for content fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=835" target="_blank">00:13:55.680</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=836" target="_blank">00:13:56.640</a></span> | <span class="t">Um, I wonder if there are any questions from the audience, please come up to the mics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=842" target="_blank">00:14:02.880</a></span> | <span class="t">Uh, how did you decide which open source LLMs to fine tune?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=847" target="_blank">00:14:07.620</a></span> | <span class="t">Yeah, that's a, yeah, that's a very good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=852" target="_blank">00:14:12.440</a></span> | <span class="t">So we did a lot of experiment trying different language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=855" target="_blank">00:14:15.940</a></span> | <span class="t">And, um, being a previous slide, we also share some, um, performance for different language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=861" target="_blank">00:14:21.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=862" target="_blank">00:14:22.960</a></span> | <span class="t">Uh, if you could just walk us through somebody typing a search prompt, the confusion that I have is you have like LLMs, uh, building some sort of matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=880" target="_blank">00:14:40.100</a></span> | <span class="t">Is it just being used for the label to be distilled or how did you shim that into the buying code or it wasn't really clear on the two tower while flying airline and how the LLM search kind of influenced that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=895" target="_blank">00:14:55.740</a></span> | <span class="t">We use LLMs to distill into a student model, which predicts search relevant specifically and produces five scale relevant scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=902" target="_blank">00:15:02.960</a></span> | <span class="t">Um, and it's served at the end of the search pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=906" target="_blank">00:15:06.380</a></span> | <span class="t">It's, uh, the re-ranking stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=908" target="_blank">00:15:08.460</a></span> | <span class="t">Um, like every recommendation system, we have a lot of, uh, CGs, which are kind of generators, we have early stage ranking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=916" target="_blank">00:15:16.140</a></span> | <span class="t">And this is one of the things that sits further down the pipeline, which actually predicts search relevant scores and, uh, as he used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=922" target="_blank">00:15:22.920</a></span> | <span class="t">Right before blending to actually produce a feed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=926" target="_blank">00:15:26.800</a></span> | <span class="t">So I think it's very similar to most recommender systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=929" target="_blank">00:15:29.760</a></span> | <span class="t">So I have a, uh, I have a question on how you evolved into this architecture, but I'm sure printers have pre LLM era search as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=946" target="_blank">00:15:46.120</a></span> | <span class="t">So, like what limitations did you see in those systems that this new architecture is solving for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=951" target="_blank">00:15:51.200</a></span> | <span class="t">So, um, if I'm understanding correctly, your question is about what's the difference between the new system with the, from the, what was the, what was the driver to adopt, adopting LLMs, uh, for, for in, in your search pipeline?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=969" target="_blank">00:16:09.600</a></span> | <span class="t">So, um, did the type, did it support new features or is it, does it improve on the existing features where you had limitations?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=976" target="_blank">00:16:16.920</a></span> | <span class="t">I think they definitely improve, uh, especially with visual language model captions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=984" target="_blank">00:16:24.200</a></span> | <span class="t">I think we would various effectively able to expand beyond limited markets for actually measuring relevant data and I'm getting relevant data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=994" target="_blank">00:16:34.100</a></span> | <span class="t">And yeah, these modeling with models are very good at, uh, getting, uh, synthetic data for different markets for, uh, then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1007" target="_blank">00:16:47.540</a></span> | <span class="t">Hey, um, great, um, great, great talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1009" target="_blank">00:16:49.700</a></span> | <span class="t">Um, I was wondering why, uh, or if, if the embedding model is inherently multimodal, um, because you have text, which is, uh, the, the query and then you're matching against, um, either text to links or images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1023" target="_blank">00:17:03.020</a></span> | <span class="t">And so how do you think about multimodality?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1025" target="_blank">00:17:05.220</a></span> | <span class="t">It's definitely something we're exploring, but then, uh, on a lot of applications, I think we found, uh, visual captions are very good at, uh, capturing what the image has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1037" target="_blank">00:17:17.060</a></span> | <span class="t">Um, and we have some very good capturing models in house, which, uh, yeah, help us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1043" target="_blank">00:17:23.000</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1044" target="_blank">00:17:24.500</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1044" target="_blank">00:17:24.800</a></span> | <span class="t">Great talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1049" target="_blank">00:17:29.360</a></span> | <span class="t">I, yeah, just a quick question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1051" target="_blank">00:17:31.340</a></span> | <span class="t">You mentioned that you saw improvements in other languages as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1054" target="_blank">00:17:34.380</a></span> | <span class="t">Did you start with the common baseline model for all languages or did you have to sort of, and just change the features for each language or did you actually also start with separate models for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1067" target="_blank">00:17:47.000</a></span> | <span class="t">individual languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1067" target="_blank">00:17:47.900</a></span> | <span class="t">I'm curious how you actually saw the improvements manifest everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1072" target="_blank">00:17:52.340</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1074" target="_blank">00:17:54.240</a></span> | <span class="t">Um, so we, we use the, um, same model for our own languages and we, because we, we are using the multilingual, um, um, LN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1082" target="_blank">00:18:02.900</a></span> | <span class="t">So we believe it can have transferred to other languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1085" target="_blank">00:18:05.900</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XdAWgO11zuk&t=1090" target="_blank">00:18:10.900</a></span> | <span class="t">We'll see you next time.</span></div></div></body></html>