<html><head><title>Training BERT #3 - Next Sentence Prediction (NSP)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Training BERT #3 - Next Sentence Prediction (NSP)</h2><a href="https://www.youtube.com/watch?v=1gN1snKBLP0"><img src="https://i.ytimg.com/vi_webp/1gN1snKBLP0/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=443">7:23</a> Tokenization<br><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=632">10:32</a> Create a Labels Tensor<br><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=680">11:20</a> Calculate Our Loss<br><br><div style="text-align: left;"><a href="./1gN1snKBLP0.html">Whisper Transcript</a> | <a href="./transcript_1gN1snKBLP0.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi, welcome to the video. Here we're going to have a look at using next sentence prediction or NSP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=6" target="_blank">00:00:06.720</a></span> | <span class="t">for fine-tuning our BERT models. Now a few of the previous videos we covered mass language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=14" target="_blank">00:00:14.320</a></span> | <span class="t">and how we use mass language modeling to fine-tune our models. NSP is like the other half</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=20" target="_blank">00:00:20.880</a></span> | <span class="t">of fine-tuning for BERT. So both of those techniques during the actual training of BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=28" target="_blank">00:00:28.480</a></span> | <span class="t">so when Google train BERT initially, they use both of these methods. And whereas MLM is identifying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=37" target="_blank">00:00:37.840</a></span> | <span class="t">or almost training on the relationships between words, next sentence prediction is training on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=44" target="_blank">00:00:44.160</a></span> | <span class="t">more long-term relationships between sentences rather than words. And in the original BERT paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=54" target="_blank">00:00:54.000</a></span> | <span class="t">it was found that without NSP, because they tried training BERT without NSP as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=58" target="_blank">00:00:58.880</a></span> | <span class="t">BERT performed worse on every single metric. So it is pretty important and obviously if we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=66" target="_blank">00:01:06.480</a></span> | <span class="t">this approach, we take mass language modeling and NSP and apply both those to training our models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=73" target="_blank">00:01:13.360</a></span> | <span class="t">fine-tuning our models, we're going to get better results than if we just use MLM. So what is NSP?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=81" target="_blank">00:01:21.040</a></span> | <span class="t">NSP consists of giving BERT two sentences, sentence A and sentence B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=86" target="_blank">00:01:26.640</a></span> | <span class="t">and saying, "Hey BERT, does sentence B come after sentence A?" And then BERT will say, "Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=92" target="_blank">00:01:32.960</a></span> | <span class="t">sentence B is the next sentence after sentence A, or it is not the next sentence after sentence A."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=101" target="_blank">00:01:41.520</a></span> | <span class="t">So if we took these three sentences that are on the screen, we have one, two, and three, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=111" target="_blank">00:01:51.200</a></span> | <span class="t">One and two, if you ask BERT, "Does sentence two come after sentence one?" Then we'd kind of want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=118" target="_blank">00:01:58.960</a></span> | <span class="t">BERT to say no, right? Because clearly they're talking about completely different topics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=124" target="_blank">00:02:04.720</a></span> | <span class="t">and the type of language and everything in there just doesn't really match up. But then if we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=130" target="_blank">00:02:10.880</a></span> | <span class="t">a look at sentence three and sentence one, they do match up. So sentence three is quite possibly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=139" target="_blank">00:02:19.600</a></span> | <span class="t">the follow-on sentence after sentence one. So in that case, we would expect BERT to say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=148" target="_blank">00:02:28.160</a></span> | <span class="t">"This is the next sentence." So let's have a look at how NSP looks within BERT itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=156" target="_blank">00:02:36.480</a></span> | <span class="t">So here we have the core BERT model, and during fine-tuning or pre-training, we add this other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=167" target="_blank">00:02:47.440</a></span> | <span class="t">head on top of BERT. So this is the BERT for pre-training head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=171" target="_blank">00:02:51.120</a></span> | <span class="t">And the BERT for pre-training head contains two different heads inside it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=177" target="_blank">00:02:57.360</a></span> | <span class="t">And that is our NSP head and our mass language modeling head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=184" target="_blank">00:03:04.480</a></span> | <span class="t">Now, we just want to focus on the NSP head for now. And as well, we don't need to fine-tune or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=195" target="_blank">00:03:15.360</a></span> | <span class="t">train our models with both of these heads. We can actually do it one by one. We could use mass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=200" target="_blank">00:03:20.480</a></span> | <span class="t">language modeling only, or we could use NSP only. But the full approach to pre-training BERT is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=207" target="_blank">00:03:27.680</a></span> | <span class="t">using both. So if we have a look inside our NSP head, we'll find that we have a feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=215" target="_blank">00:03:35.440</a></span> | <span class="t">neural network, and that will output two different values. Now, these two values are our "is not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=225" target="_blank">00:03:45.920</a></span> | <span class="t">next sequence" there, and our "is the next sequence," which is there. Okay, so value zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=235" target="_blank">00:03:55.760</a></span> | <span class="t">is the next sentence. Value one is not the next sentence. Now, we have the final outputs from our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=247" target="_blank">00:04:07.440</a></span> | <span class="t">final encoder in BERT at the bottom here. And we don't actually use all of these activations. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=256" target="_blank">00:04:16.160</a></span> | <span class="t">only use the CLS token activation, which is over at the left here. So this here is our CLS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=266" target="_blank">00:04:26.080</a></span> | <span class="t">Okay, and when I say this is our CLS token, I mean more that this is not our CLS token. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=274" target="_blank">00:04:34.640</a></span> | <span class="t">CLS token is down here. So we input the CLS token, and this output is the subsequent output after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=286" target="_blank">00:04:46.240</a></span> | <span class="t">being processed by 12 or so encoders within BERT itself. So this is the output representation of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=297" target="_blank">00:04:57.280</a></span> | <span class="t">that CLS token. Now, the activations from that get fed into our feed-forward neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=304" target="_blank">00:05:04.240</a></span> | <span class="t">and the dimensionality that we have here is 768 for that single token. This is in the BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=314" target="_blank">00:05:14.880</a></span> | <span class="t">base model, by the way. And that gets translated into our dimensionality here, which is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=323" target="_blank">00:05:23.200</a></span> | <span class="t">two outputs. So that's essentially how NSP works. Once we have our two outputs here, we just take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=335" target="_blank">00:05:35.920</a></span> | <span class="t">the argmax of both of those. So we take both over here, and we just take an argmax function of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=343" target="_blank">00:05:43.120</a></span> | <span class="t">and that will output us either 0 or 1, where 0 is the isNext class, and 1 is the notNext class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=356" target="_blank">00:05:56.320</a></span> | <span class="t">And that's how NSP works. So let's dive into the code and see how all this works in Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=368" target="_blank">00:06:08.960</a></span> | <span class="t">Okay, so we're going to be using HuggingFace's transformers and PyTorch. So we'll import both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=374" target="_blank">00:06:14.640</a></span> | <span class="t">of those. And from transformers, we just need the BERT tokenizer class and the BERT for next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=384" target="_blank">00:06:24.000</a></span> | <span class="t">sentence prediction class. And BERT for next sentence prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=396" target="_blank">00:06:36.960</a></span> | <span class="t">Then we also want to import Torch. And we're going to use two sentences here. So both of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=409" target="_blank">00:06:49.200</a></span> | <span class="t">are from the Wikipedia page on the American Civil War. And these are both consecutive sentences. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=417" target="_blank">00:06:57.120</a></span> | <span class="t">going back to what we looked at before, we would be hoping that BERT would output a 0 label for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=423" target="_blank">00:07:03.520</a></span> | <span class="t">both of these, because sentence B is the next sentence after sentence A. This one being sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=430" target="_blank">00:07:10.720</a></span> | <span class="t">B, this one being sentence A. So execute that. And we now have three different steps that we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=441" target="_blank">00:07:21.360</a></span> | <span class="t">take. And that is tokenization, create a classification label, so the 0 or the 1, so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=449" target="_blank">00:07:29.440</a></span> | <span class="t">we can train the model. And then from that, we calculate the loss. So the first step there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=456" target="_blank">00:07:36.240</a></span> | <span class="t">tokenization. So we tokenize. It's pretty easy. All we do is inputs, tokenizer, and then we pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=464" target="_blank">00:07:44.320</a></span> | <span class="t">text and text2. And we are using PyTorch here. So I want to return a PyTorch tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=476" target="_blank">00:07:56.400</a></span> | <span class="t">And make sure that's PT. Now we need to also initialize those. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=486" target="_blank">00:08:06.560</a></span> | <span class="t">tokenizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=490" target="_blank">00:08:10.000</a></span> | <span class="t">equals BERT tokenizer from pre-trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=499" target="_blank">00:08:19.600</a></span> | <span class="t">And we'll just use BERT base and case for now. Obviously, you can use another BERT model if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=507" target="_blank">00:08:27.280</a></span> | <span class="t">want. And I'm just going to copy that and initialize our model as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=514" target="_blank">00:08:34.080</a></span> | <span class="t">OK, now rerun that. And we'll get this warning. That's because we're using these models that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=525" target="_blank">00:08:45.600</a></span> | <span class="t">used for training or for fine-tuning. So it's just telling us that we shouldn't really use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=530" target="_blank">00:08:50.160</a></span> | <span class="t">this for inference. You need to train it first. And that's fine, because that's our intention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=535" target="_blank">00:08:55.920</a></span> | <span class="t">Now from these inputs, we'll get a few different tensors. So we have input IDs, token type IDs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">and attention mask. Now for next sentence prediction, we do need all of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=552" target="_blank">00:09:12.000</a></span> | <span class="t">So this is a little bit different to masked language modeling. With masked language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=555" target="_blank">00:09:15.680</a></span> | <span class="t">we don't actually need token type IDs. But for next sentence prediction, we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=560" target="_blank">00:09:20.640</a></span> | <span class="t">So let's have a look at what we have inside these. So input IDs is just our tokenized text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=571" target="_blank">00:09:31.600</a></span> | <span class="t">And you see that we pass these two sentences here. And they're actually both within the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=578" target="_blank">00:09:38.160</a></span> | <span class="t">sentence or the same tensor here, input IDs. And they're separated by this 102 in the middle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=585" target="_blank">00:09:45.360</a></span> | <span class="t">which is a separated token. So before that, all these tokens, that is our text variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=592" target="_blank">00:09:52.320</a></span> | <span class="t">or sentence A. And then afterwards, we have our text 2 variable, which is sentence B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=597" target="_blank">00:09:57.360</a></span> | <span class="t">And we can see this mirrored in the token type IDs tensor as well. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=604" target="_blank">00:10:04.560</a></span> | <span class="t">all the way along here up to here, that's our sentence A. So we have zeros for sentence A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=610" target="_blank">00:10:10.640</a></span> | <span class="t">And then following that, we have ones representing sentence B. And then we have our attention mask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=619" target="_blank">00:10:19.200</a></span> | <span class="t">which is just ones because the attention mask is a one where it's a real token and a zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=623" target="_blank">00:10:23.600</a></span> | <span class="t">where we have padding token. So we don't need to really worry about that tensor at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=629" target="_blank">00:10:29.680</a></span> | <span class="t">Now, the next step here is that we need to create a labels tensor. So to do that, we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=638" target="_blank">00:10:38.160</a></span> | <span class="t">write labels. And we just need to make sure that when we do this, we use a long tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=645" target="_blank">00:10:45.120</a></span> | <span class="t">Okay, so we use a long tensor. And in here, we need to pass a list containing a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=655" target="_blank">00:10:55.120</a></span> | <span class="t">value, which is either our zero for is the next sentence, or one for is not the next sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=660" target="_blank">00:11:00.640</a></span> | <span class="t">In our case, our two sentences are supposed to be together. So we will pass a zero in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=667" target="_blank">00:11:07.120</a></span> | <span class="t">And run that. And if we just have a look at what we get from there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=674" target="_blank">00:11:14.000</a></span> | <span class="t">we see that we get this integer tensor. So now we're ready to calculate our loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=682" target="_blank">00:11:22.000</a></span> | <span class="t">which is really easy. So we have our model up here, which we have already initialized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=686" target="_blank">00:11:26.720</a></span> | <span class="t">So we just take that. And all we do is pass our inputs from here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=692" target="_blank">00:11:32.880</a></span> | <span class="t">into our model is keyword arguments. So that's what these two symbols are for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=700" target="_blank">00:11:40.000</a></span> | <span class="t">And then we also pass labels to the labels parameter. Okay. And that will output a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=710" target="_blank">00:11:50.160</a></span> | <span class="t">of tensors for us. So we can execute that. And let's have a look what we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=714" target="_blank">00:11:54.800</a></span> | <span class="t">So you see that we get these two tensors, we have the logits, and we also have the loss tensor. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=723" target="_blank">00:12:03.440</a></span> | <span class="t">let's have a look at the logits. And we should be able to recognize this from early run where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=728" target="_blank">00:12:08.720</a></span> | <span class="t">we saw those two nodes, and we had the two values on for the index zero for is next and index one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=735" target="_blank">00:12:15.600</a></span> | <span class="t">for is not next. So let's have a look. You see here that we get both of those. So this is our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=744" target="_blank">00:12:24.720</a></span> | <span class="t">activation for is the next sentence. This is our activation for is not the next sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=750" target="_blank">00:12:30.080</a></span> | <span class="t">And if we were to take the argmax of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=753" target="_blank">00:12:33.600</a></span> | <span class="t">outputs logits, we get zero, which means it is the next sentence. Okay. And we also have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=764" target="_blank">00:12:44.560</a></span> | <span class="t">loss. And this loss tensor, that will only be output if we pass our labels here. Otherwise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=772" target="_blank">00:12:52.240</a></span> | <span class="t">we just get a logits tensor. So when we're training, obviously, we need labels so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=778" target="_blank">00:12:58.480</a></span> | <span class="t">we can calculate the loss. And if we just have a look at that, we see it's just a loss value,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=786" target="_blank">00:13:06.640</a></span> | <span class="t">which is very small because the model is predicting a zero and the label that we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=795" target="_blank">00:13:15.040</a></span> | <span class="t">provided is also a zero. So the loss is pretty good there. So that is how NSP works. Obviously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=803" target="_blank">00:13:23.680</a></span> | <span class="t">it's slightly different if you're actually training your model. And I am going to cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=810" target="_blank">00:13:30.480</a></span> | <span class="t">that in the next video. So I'll leave a link to that in the description. But for now, that's it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1gN1snKBLP0&t=818" target="_blank">00:13:38.000</a></span> | <span class="t">for this. So thank you very much for watching, and I'll see you again in the next one.</span></div></div></body></html>