<html><head><title>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 15 - Add Knowledge to Language Models</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | Winter 2021 | Lecture 15 - Add Knowledge to Language Models</h2><a href="https://www.youtube.com/watch?v=y68RJVfGoto"><img src="https://i.ytimg.com/vi/y68RJVfGoto/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=17">0:17</a> Reminders<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=144">2:24</a> Language Models<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=246">4:6</a> What Types of Facts a Language Model Might Know<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=396">6:36</a> Why Researchers Are Interested in Building Language Models That Can More reliably or Call Knowledge<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=468">7:48</a> What is a Knowledge Base<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=597">9:57</a> Advantages of Using Language Models<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=761">12:41</a> Add Knowledge to Language Models<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=810">13:30</a> Pretrained Entity Embeddings<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=900">15:0</a> Entity Linking<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1069">17:49</a> Entity Embeddings<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1255">20:55</a> ERNIE<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1334">22:14</a> Architecture Diagram<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1406">23:26</a> Training<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1499">24:59</a> Ablation<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1540">25:40</a> Challenges<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1751">29:11</a> Nobert<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1897">31:37</a> External Memory<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2025">33:45</a> kglm<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2270">37:50</a> Related Entity Case<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2486">41:26</a> New Entity Diagram<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2660">44:20</a> Local Knowledge Graph<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2830">47:10</a> KNLM<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3151">52:31</a> Implicit Knowledge<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3197">53:17</a> WKLM<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3462">57:42</a> Masking<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3528">58:48</a> Comparing masking techniques<br><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3550">59:10</a> Results of the original paper<br><br><div style="text-align: left;"><a href="./y68RJVfGoto.html">Whisper Transcript</a> | <a href="./transcript_y68RJVfGoto.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome to CS224N lecture 15.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=7" target="_blank">00:00:07.480</a></span> | <span class="t">So I'm Megan and I'm one of the CAs in this course, and I'm also a PhD student working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=11" target="_blank">00:00:11.200</a></span> | <span class="t">with Chris Ray.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=12" target="_blank">00:00:12.200</a></span> | <span class="t">And today I'll be talking about integrating knowledge and language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=18" target="_blank">00:00:18.920</a></span> | <span class="t">So some quick reminders, your project milestones were due today, so hopefully you turned those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=22" target="_blank">00:00:22.240</a></span> | <span class="t">in already or will be turning them in in the next couple of days, and we'll try to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=26" target="_blank">00:00:26.000</a></span> | <span class="t">feedback on those as fast as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=29" target="_blank">00:00:29.160</a></span> | <span class="t">So something to be aware of is a change of grading basis and course withdrawal deadline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=32" target="_blank">00:00:32.520</a></span> | <span class="t">is this Friday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=34" target="_blank">00:00:34.640</a></span> | <span class="t">So if you want to make any change to your grade, make sure to do that by then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=37" target="_blank">00:00:37.920</a></span> | <span class="t">And we'll be getting you the grades back on assignment five by then as well, in case that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=41" target="_blank">00:00:41.220</a></span> | <span class="t">helpful in making your decision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=44" target="_blank">00:00:44.320</a></span> | <span class="t">And finally, your final projects are due in two weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=46" target="_blank">00:00:46.600</a></span> | <span class="t">So hopefully those are going smoothly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=49" target="_blank">00:00:49.380</a></span> | <span class="t">So the topic of the day is integrating knowledge and language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=52" target="_blank">00:00:52.440</a></span> | <span class="t">You've seen a bit about this idea in assignment five, and also in Colin Raffles' lecture last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=56" target="_blank">00:00:56.280</a></span> | <span class="t">class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=57" target="_blank">00:00:57.280</a></span> | <span class="t">So in assignment five, the task was to train a model to predict the birthplace of a person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=61" target="_blank">00:01:01.720</a></span> | <span class="t">given their name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=62" target="_blank">00:01:02.720</a></span> | <span class="t">And you saw that by pre-training on a larger data set, you're actually able to do better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=66" target="_blank">00:01:06.480</a></span> | <span class="t">on this task, since you could encode some real knowledge into the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=71" target="_blank">00:01:11.680</a></span> | <span class="t">And then last lecture, Colin Raffle presented how T5 could actually be fine-tuned for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=76" target="_blank">00:01:16.560</a></span> | <span class="t">closed domain question answering task, such that you can give T5 a natural language question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=81" target="_blank">00:01:21.760</a></span> | <span class="t">and it'll return an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=84" target="_blank">00:01:24.040</a></span> | <span class="t">So today we'll be building on these threads and looking at techniques that researchers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=87" target="_blank">00:01:27.000</a></span> | <span class="t">have recently been developing to increase the amount of knowledge in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=92" target="_blank">00:01:32.760</a></span> | <span class="t">So we're going to start with a quick recap of language models, just to make sure we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=95" target="_blank">00:01:35.280</a></span> | <span class="t">all on the same page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=96" target="_blank">00:01:36.280</a></span> | <span class="t">Then we're going to talk about what types of knowledge language models can already encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=100" target="_blank">00:01:40.240</a></span> | <span class="t">and what they might struggle on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=101" target="_blank">00:01:41.880</a></span> | <span class="t">We'll also motivate why researchers are interested in increasing the amount of knowledge in language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=106" target="_blank">00:01:46.680</a></span> | <span class="t">models, and what this could enable for future AI systems if we have language models that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=110" target="_blank">00:01:50.640</a></span> | <span class="t">can actually reliably recall knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=114" target="_blank">00:01:54.720</a></span> | <span class="t">We'll talk about three broad classes of techniques that researchers have been using to add knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=118" target="_blank">00:01:58.320</a></span> | <span class="t">to language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=119" target="_blank">00:01:59.720</a></span> | <span class="t">These include adding pre-trained entity embeddings, using external memory or key value store,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=124" target="_blank">00:02:04.800</a></span> | <span class="t">or even just modifying the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=127" target="_blank">00:02:07.280</a></span> | <span class="t">And for each of these techniques, we'll talk about at least one recent work that used the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=131" target="_blank">00:02:11.160</a></span> | <span class="t">technique, so hopefully it's clear to see how to actually employ it in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=135" target="_blank">00:02:15.920</a></span> | <span class="t">And then finally, we'll wrap up by talking about how to evaluate the knowledge in language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=139" target="_blank">00:02:19.560</a></span> | <span class="t">models and the challenges that come up in trying to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=144" target="_blank">00:02:24.960</a></span> | <span class="t">So let's dive right in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=145" target="_blank">00:02:25.960</a></span> | <span class="t">We're going to start by talking about standard language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=148" target="_blank">00:02:28.760</a></span> | <span class="t">You learned about these at the beginning of the course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=151" target="_blank">00:02:31.040</a></span> | <span class="t">And the task is to predict the next word in a sequence of text and to compute the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=155" target="_blank">00:02:35.080</a></span> | <span class="t">of a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=156" target="_blank">00:02:36.080</a></span> | <span class="t">So you may remember the example that students opened their blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=159" target="_blank">00:02:39.160</a></span> | <span class="t">And we talked about it could be minds, exams, we're going to go with books here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=163" target="_blank">00:02:43.880</a></span> | <span class="t">And the task of the standard language model is to predict the most likely next word in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=166" target="_blank">00:02:46.880</a></span> | <span class="t">the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=169" target="_blank">00:02:49.000</a></span> | <span class="t">A couple of lectures ago, John also introduced the notion of mass language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=172" target="_blank">00:02:52.680</a></span> | <span class="t">Instead of predicting the next word in a sequence of text, the task is to predict the mass token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=178" target="_blank">00:02:58.080</a></span> | <span class="t">And this is done using bidirectional context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=179" target="_blank">00:02:59.960</a></span> | <span class="t">So you may remember the example, I masked the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=183" target="_blank">00:03:03.920</a></span> | <span class="t">And the goal of the mass language model is to break the most likely token for each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=187" target="_blank">00:03:07.400</a></span> | <span class="t">the masked out words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=189" target="_blank">00:03:09.640</a></span> | <span class="t">So maybe I went to the store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=191" target="_blank">00:03:11.980</a></span> | <span class="t">So while there's some differences in these two types of language models, whether you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=195" target="_blank">00:03:15.040</a></span> | <span class="t">predicting the next word, or whether you're predicting the masked out token, they're similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=199" target="_blank">00:03:19.280</a></span> | <span class="t">in that they can both be trained over large amounts of unlabeled text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=203" target="_blank">00:03:23.560</a></span> | <span class="t">And this is one of the reasons why they've been so widely adopted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=206" target="_blank">00:03:26.160</a></span> | <span class="t">They don't require any human annotated data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=210" target="_blank">00:03:30.560</a></span> | <span class="t">So you've seen that language models can be used for a variety of tasks, from summarization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=214" target="_blank">00:03:34.640</a></span> | <span class="t">to dialogue to fluency evaluation, tasks that involve either generating text or evaluating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=220" target="_blank">00:03:40.160</a></span> | <span class="t">the probability of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=223" target="_blank">00:03:43.280</a></span> | <span class="t">And more recently, we've seen that language models can also be used to generate pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=226" target="_blank">00:03:46.800</a></span> | <span class="t">representations of text that encode some notion of language understanding, and has been shown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=231" target="_blank">00:03:51.480</a></span> | <span class="t">to be widely useful for different downstream NLP tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=236" target="_blank">00:03:56.320</a></span> | <span class="t">And then finally, today we're going to touch on this idea that if language models are trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=240" target="_blank">00:04:00.560</a></span> | <span class="t">over massive amounts of text, can they even be used as a knowledge base?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=247" target="_blank">00:04:07.240</a></span> | <span class="t">So we're going to start by looking at what types of factual knowledge a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=250" target="_blank">00:04:10.320</a></span> | <span class="t">might already know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=251" target="_blank">00:04:11.920</a></span> | <span class="t">And these examples are taken from a paper by Petroni et al. in EMNLP a couple years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=256" target="_blank">00:04:16.400</a></span> | <span class="t">ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=257" target="_blank">00:04:17.400</a></span> | <span class="t">And the goal is to test the factual or common sense knowledge in existing language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=262" target="_blank">00:04:22.040</a></span> | <span class="t">such as BERT-Large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=263" target="_blank">00:04:23.040</a></span> | <span class="t">So let's check out what BERT-Large predicts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=266" target="_blank">00:04:26.600</a></span> | <span class="t">iPod Touch is produced by Apple, London Jazz Festival is located in London, Danny Alves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=273" target="_blank">00:04:33.440</a></span> | <span class="t">plays with Santos, Carl III used to communicate in German, and ravens can fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=280" target="_blank">00:04:40.680</a></span> | <span class="t">So here we have the correct predictions in green and the incorrect predictions in red.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=284" target="_blank">00:04:44.040</a></span> | <span class="t">And if you know anything about sports, you may know that Danny Alves is a soccer player,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=288" target="_blank">00:04:48.840</a></span> | <span class="t">Santos is a soccer team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=290" target="_blank">00:04:50.640</a></span> | <span class="t">Here they were hoping that it would predict Barcelona, because at least at the time of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=293" target="_blank">00:04:53.840</a></span> | <span class="t">this data set, apparently he played for Barcelona.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=296" target="_blank">00:04:56.560</a></span> | <span class="t">And Carl III actually used to communicate in Swedish, not German.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=301" target="_blank">00:05:01.080</a></span> | <span class="t">So what's good about these examples is the predictions are generally reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=305" target="_blank">00:05:05.160</a></span> | <span class="t">If you didn't know the ground truth, they all make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=308" target="_blank">00:05:08.240</a></span> | <span class="t">When you want to predict a language, you do in fact predict the language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=313" target="_blank">00:05:13.880</a></span> | <span class="t">But of course, they're not all factually correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=317" target="_blank">00:05:17.480</a></span> | <span class="t">So why might this happen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=318" target="_blank">00:05:18.840</a></span> | <span class="t">Well, for one, the fact might not been seen in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=321" target="_blank">00:05:21.880</a></span> | <span class="t">And you can't expect the language model to do more than recall facts that it has seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=325" target="_blank">00:05:25.600</a></span> | <span class="t">in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=326" target="_blank">00:05:26.600</a></span> | <span class="t">It can't make up facts about the world, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=329" target="_blank">00:05:29.640</a></span> | <span class="t">It's also possible the fact is just really rare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=331" target="_blank">00:05:31.940</a></span> | <span class="t">So maybe the language model has seen the fact during training, but it hasn't seen it enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=335" target="_blank">00:05:35.560</a></span> | <span class="t">times to actually memorize the fact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=339" target="_blank">00:05:39.000</a></span> | <span class="t">And the last issue is a little more subtle, which the model might just be very sensitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=342" target="_blank">00:05:42.640</a></span> | <span class="t">to the phrasing of the fill in the blank statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=345" target="_blank">00:05:45.960</a></span> | <span class="t">And so for example, you might have statements like X was created in blank that the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=350" target="_blank">00:05:50.080</a></span> | <span class="t">can't predict correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=351" target="_blank">00:05:51.080</a></span> | <span class="t">But if you change it to X was made in blank, suddenly it can predict it correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=356" target="_blank">00:05:56.080</a></span> | <span class="t">And we'll come back to this and how to actually evaluate the knowledge in these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=362" target="_blank">00:06:02.600</a></span> | <span class="t">So this inability to reliably recall knowledge is a key challenge facing language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=367" target="_blank">00:06:07.160</a></span> | <span class="t">today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=368" target="_blank">00:06:08.160</a></span> | <span class="t">And it'll be the focus of this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=370" target="_blank">00:06:10.040</a></span> | <span class="t">Recent works have found that language models can recover some knowledge, including the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=374" target="_blank">00:06:14.160</a></span> | <span class="t">work that Colin presented last class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=375" target="_blank">00:06:15.800</a></span> | <span class="t">They've had very encouraging results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=378" target="_blank">00:06:18.560</a></span> | <span class="t">But there's still a way to go, as we saw with the fill in the blank statements and with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=381" target="_blank">00:06:21.880</a></span> | <span class="t">these challenges that we just discussed above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=384" target="_blank">00:06:24.880</a></span> | <span class="t">So as a result, the past couple of years have had a ton of rapid progress in this area of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=388" target="_blank">00:06:28.880</a></span> | <span class="t">research in terms of trying to figure out how do you actually encode more knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=393" target="_blank">00:06:33.200</a></span> | <span class="t">in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=397" target="_blank">00:06:37.720</a></span> | <span class="t">So I also want to motivate why researchers are interested in building language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=401" target="_blank">00:06:41.360</a></span> | <span class="t">that can more reliably recall knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=405" target="_blank">00:06:45.040</a></span> | <span class="t">And one of these reasons is that the pre-trained representations are used in a variety of downstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=409" target="_blank">00:06:49.240</a></span> | <span class="t">tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=410" target="_blank">00:06:50.240</a></span> | <span class="t">And some of these downstream tasks are knowledge intensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=413" target="_blank">00:06:53.920</a></span> | <span class="t">So for instance, you might have a downstream task to extract the relations between two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=418" target="_blank">00:06:58.000</a></span> | <span class="t">entities in a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=420" target="_blank">00:07:00.000</a></span> | <span class="t">And this is commonly known as relation extraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=422" target="_blank">00:07:02.440</a></span> | <span class="t">And this is much easier if you have some knowledge of the entities, which could be potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=426" target="_blank">00:07:06.920</a></span> | <span class="t">provided by this pre-trained language model representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=431" target="_blank">00:07:11.920</a></span> | <span class="t">And when we talk about evaluation, we'll talk about what types of tasks are most likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=435" target="_blank">00:07:15.280</a></span> | <span class="t">to benefit from these knowledge rich pre-trained representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=440" target="_blank">00:07:20.840</a></span> | <span class="t">And then as a stretch goal, some researchers are starting to propose the idea that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=445" target="_blank">00:07:25.000</a></span> | <span class="t">language models actually ultimately be used to replace traditional knowledge bases?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=450" target="_blank">00:07:30.560</a></span> | <span class="t">So instead of creating a knowledge base for a fact, like you might right now with SQL,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=454" target="_blank">00:07:34.200</a></span> | <span class="t">you'd create a language model with a natural language prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=457" target="_blank">00:07:37.040</a></span> | <span class="t">And of course, this does require the language model to have high quality on recalling facts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=463" target="_blank">00:07:43.040</a></span> | <span class="t">So we might not be there yet, but it's an interesting direction for us to be moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=466" target="_blank">00:07:46.520</a></span> | <span class="t">towards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=469" target="_blank">00:07:49.320</a></span> | <span class="t">So I want to make it super clear what I mean by a knowledge base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=472" target="_blank">00:07:52.360</a></span> | <span class="t">Here we're just talking about a knowledge graph where the nodes in the graph would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=475" target="_blank">00:07:55.960</a></span> | <span class="t">entities and the edges are going to be relations between the entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=480" target="_blank">00:08:00.600</a></span> | <span class="t">So for example, here we have a subset of a knowledge graph for Franklin D. Roosevelt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=484" target="_blank">00:08:04.880</a></span> | <span class="t">and you see the information about his spouse, his place of birth, his date of birth, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=488" target="_blank">00:08:08.920</a></span> | <span class="t">so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=489" target="_blank">00:08:09.920</a></span> | <span class="t">An important thing to note is this is a structured way of storing the knowledge, since it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=494" target="_blank">00:08:14.600</a></span> | <span class="t">in a graph form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=495" target="_blank">00:08:15.840</a></span> | <span class="t">And you can actually describe these graphs with knowledge graph triples, which will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=499" target="_blank">00:08:19.880</a></span> | <span class="t">an important vocabulary word throughout this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=502" target="_blank">00:08:22.840</a></span> | <span class="t">So knowledge graph triple would be consisting of a subject entity, a relation, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=508" target="_blank">00:08:28.560</a></span> | <span class="t">an object entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=510" target="_blank">00:08:30.160</a></span> | <span class="t">So for instance, here we might have Franklin D. Roosevelt, date of birth, January 30th,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">1882.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=516" target="_blank">00:08:36.200</a></span> | <span class="t">And that would form a knowledge graph triple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=517" target="_blank">00:08:37.600</a></span> | <span class="t">We'll also refer to this as a parent entity, a relation, and a tail entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=523" target="_blank">00:08:43.840</a></span> | <span class="t">So Wikidata is one very popular knowledge base you might come across if you're working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=526" target="_blank">00:08:46.840</a></span> | <span class="t">in this area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=528" target="_blank">00:08:48.100</a></span> | <span class="t">It's a free knowledge base that's actually populated by humans, so they're filling in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=532" target="_blank">00:08:52.160</a></span> | <span class="t">these relations and entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=534" target="_blank">00:08:54.280</a></span> | <span class="t">And it's also multilingual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=537" target="_blank">00:08:57.080</a></span> | <span class="t">So if you want information from this knowledge base, what you do is you would write a SQL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=541" target="_blank">00:09:01.080</a></span> | <span class="t">query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=542" target="_blank">00:09:02.340</a></span> | <span class="t">This is a simplified one, but the idea is you'd want to figure out the date of birth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=547" target="_blank">00:09:07.040</a></span> | <span class="t">of Franklin Roosevelt, so you would write a query like follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=552" target="_blank">00:09:12.320</a></span> | <span class="t">Now if instead you want to create a language model as a knowledge base, you'll have something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=556" target="_blank">00:09:16.240</a></span> | <span class="t">like this diagram that you've actually probably seen in several lectures now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=560" target="_blank">00:09:20.540</a></span> | <span class="t">And the idea is you'll train a language model over this unstructured text, and then you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=565" target="_blank">00:09:25.240</a></span> | <span class="t">use a language model to just answer these natural language query statements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=569" target="_blank">00:09:29.980</a></span> | <span class="t">So here, this is the work on T5, where they're training T5 over natural language or just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=576" target="_blank">00:09:36.060</a></span> | <span class="t">unstructured text with a span corruption task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=579" target="_blank">00:09:39.020</a></span> | <span class="t">And then they're asking T5, when was Franklin D. Roosevelt born?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=582" target="_blank">00:09:42.900</a></span> | <span class="t">And the idea is T5 will produce a textual answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=586" target="_blank">00:09:46.440</a></span> | <span class="t">So you can see this contrast very much with the old approach of using a traditional knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=590" target="_blank">00:09:50.080</a></span> | <span class="t">base, where the knowledge base is structured, and you have these SQL statements to query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=594" target="_blank">00:09:54.000</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=598" target="_blank">00:09:58.160</a></span> | <span class="t">So what are the advantages of using language models over traditional knowledge bases, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=601" target="_blank">00:10:01.760</a></span> | <span class="t">why might people think this could be a good idea?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=604" target="_blank">00:10:04.000</a></span> | <span class="t">Well, for one, the language models are pre-trained over large amounts of unstructured and unlabeled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=609" target="_blank">00:10:09.280</a></span> | <span class="t">text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=610" target="_blank">00:10:10.280</a></span> | <span class="t">Whereas traditional knowledge bases require manual annotation, like with wiki data, people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=614" target="_blank">00:10:14.320</a></span> | <span class="t">actually are populating it, or complex NLP pipelines to extract from unstructured text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=620" target="_blank">00:10:20.080</a></span> | <span class="t">into a structured form that forms a knowledge base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=624" target="_blank">00:10:24.720</a></span> | <span class="t">Language models can also support more flexible natural language queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=628" target="_blank">00:10:28.780</a></span> | <span class="t">So if we take the example, what does the final F in the song UFOF stand for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=634" target="_blank">00:10:34.280</a></span> | <span class="t">A knowledge base probably won't have a field for final F, so it won't be able to answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=638" target="_blank">00:10:38.160</a></span> | <span class="t">your query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=639" target="_blank">00:10:39.160</a></span> | <span class="t">But there's a chance that a language model could actually learn and have a response for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=643" target="_blank">00:10:43.480</a></span> | <span class="t">this natural language query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=646" target="_blank">00:10:46.400</a></span> | <span class="t">They also had a less extreme example in this paper by Petroni and others, where maybe your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=650" target="_blank">00:10:50.800</a></span> | <span class="t">relation would be "is works for" in your knowledge base, and then you ask for "is working for".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=656" target="_blank">00:10:56.760</a></span> | <span class="t">And the knowledge base doesn't have an exact match in the field, and so it returns an empty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=660" target="_blank">00:11:00.480</a></span> | <span class="t">response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=661" target="_blank">00:11:01.480</a></span> | <span class="t">And it's much, it's reasonable to believe that your language model could figure out that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=666" target="_blank">00:11:06.040</a></span> | <span class="t">these relations are similar, so if I know the answer to one of them, I probably know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=669" target="_blank">00:11:09.960</a></span> | <span class="t">the answer to the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=673" target="_blank">00:11:13.200</a></span> | <span class="t">Of course, it's not all advantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=675" target="_blank">00:11:15.200</a></span> | <span class="t">There's also many open challenges to using language models as knowledge bases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=679" target="_blank">00:11:19.480</a></span> | <span class="t">So for one, it's harder to interpret.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=681" target="_blank">00:11:21.560</a></span> | <span class="t">When a traditional knowledge base produces an answer, there's actually provenance information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=684" target="_blank">00:11:24.960</a></span> | <span class="t">associated with why did it return that particular query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=688" target="_blank">00:11:28.440</a></span> | <span class="t">But with a language model, it's really not clear why it might produce a prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=694" target="_blank">00:11:34.320</a></span> | <span class="t">The knowledge is just encoded in the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=698" target="_blank">00:11:38.380</a></span> | <span class="t">It's also harder to trust.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=699" target="_blank">00:11:39.380</a></span> | <span class="t">So you saw this in assignment 5, where the language model could produce realistic predictions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=705" target="_blank">00:11:45.100</a></span> | <span class="t">but they are incorrect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=706" target="_blank">00:11:46.480</a></span> | <span class="t">So it's not easy to know when the language model actually knows the fact, versus it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=710" target="_blank">00:11:50.520</a></span> | <span class="t">using some biases to make its prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=712" target="_blank">00:11:52.880</a></span> | <span class="t">And in the case of the traditional knowledge base, if it doesn't know a fact, it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=717" target="_blank">00:11:57.120</a></span> | <span class="t">going to have an empty response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=720" target="_blank">00:12:00.240</a></span> | <span class="t">And then finally, language models are harder to modify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=725" target="_blank">00:12:05.320</a></span> | <span class="t">So in a knowledge base, if you want to update a fact, you just change the fact directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=729" target="_blank">00:12:09.120</a></span> | <span class="t">in the structured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=731" target="_blank">00:12:11.960</a></span> | <span class="t">But in a language model, it's not quite clear how you would do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=734" target="_blank">00:12:14.940</a></span> | <span class="t">You could fine tune the model longer on the updated data, but how do you know if it still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=739" target="_blank">00:12:19.400</a></span> | <span class="t">has some memorization of the old fact?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=743" target="_blank">00:12:23.440</a></span> | <span class="t">So there are a lot of open challenges to this goal of actually using language models as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=747" target="_blank">00:12:27.500</a></span> | <span class="t">traditional knowledge bases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=749" target="_blank">00:12:29.360</a></span> | <span class="t">But hopefully you see why some people think this could actually be a good idea, and why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=753" target="_blank">00:12:33.840</a></span> | <span class="t">researchers are interested in training language models that can actually integrate more knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=761" target="_blank">00:12:41.560</a></span> | <span class="t">So that brings us to section 2 of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=763" target="_blank">00:12:43.720</a></span> | <span class="t">So I want to pause here just in case there's any questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=766" target="_blank">00:12:46.760</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=767" target="_blank">00:12:47.760</a></span> | <span class="t">I think that's OK, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=772" target="_blank">00:12:52.240</a></span> | <span class="t">OK, awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=773" target="_blank">00:12:53.720</a></span> | <span class="t">So now we're going to be talking about what techniques researchers are using to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=780" target="_blank">00:13:00.240</a></span> | <span class="t">add more knowledge to language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=783" target="_blank">00:13:03.900</a></span> | <span class="t">So we're going to talk about three broad classes of techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=786" target="_blank">00:13:06.600</a></span> | <span class="t">This is by no means exhaustive, but hopefully it gives you a good overview so that if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=790" target="_blank">00:13:10.480</a></span> | <span class="t">want to dive deeper, you can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=793" target="_blank">00:13:13.700</a></span> | <span class="t">So we'll start by talking about adding pre-trained entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=797" target="_blank">00:13:17.120</a></span> | <span class="t">And for each section, we'll kind of focus on the first work that you see in the bullets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=800" target="_blank">00:13:20.760</a></span> | <span class="t">But we'll also talk about briefly some of the variants so you see how the works within</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=805" target="_blank">00:13:25.860</a></span> | <span class="t">each class can differ and what knobs you can turn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=811" target="_blank">00:13:31.680</a></span> | <span class="t">So for adding pre-trained embeddings, we first need to figure out what pre-trained embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=815" target="_blank">00:13:35.940</a></span> | <span class="t">would actually be the most useful to add knowledge to language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=819" target="_blank">00:13:39.820</a></span> | <span class="t">And this can start with an observation that facts about the world are usually in terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=823" target="_blank">00:13:43.200</a></span> | <span class="t">of entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=825" target="_blank">00:13:45.320</a></span> | <span class="t">So if we have a fact like Washington was the first president of the United States, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=829" target="_blank">00:13:49.480</a></span> | <span class="t">the entities Washington, United States.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=833" target="_blank">00:13:53.120</a></span> | <span class="t">But pre-trained word embeddings don't have this notion of entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=837" target="_blank">00:13:57.120</a></span> | <span class="t">So we'd have different word embeddings for USA, United States of America, and America,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=842" target="_blank">00:14:02.120</a></span> | <span class="t">even though these all refer to the same entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=845" target="_blank">00:14:05.140</a></span> | <span class="t">And this makes it challenging for the language model to actually learn any representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=848" target="_blank">00:14:08.760</a></span> | <span class="t">over these entities, since they may be referred to many ways in the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=855" target="_blank">00:14:15.700</a></span> | <span class="t">So what if instead we have a single embedding per entity, and we'll refer to these as entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=860" target="_blank">00:14:20.520</a></span> | <span class="t">embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=862" target="_blank">00:14:22.640</a></span> | <span class="t">So now you'd have a single entity embedding for USA, United States of America, and America.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=868" target="_blank">00:14:28.880</a></span> | <span class="t">And whenever you see a phrase in text referring to this entity, you would use the same entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=873" target="_blank">00:14:33.440</a></span> | <span class="t">embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=874" target="_blank">00:14:34.440</a></span> | <span class="t">And these entity embeddings can actually be pre-trained to encode this factual knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=879" target="_blank">00:14:39.780</a></span> | <span class="t">about the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=881" target="_blank">00:14:41.380</a></span> | <span class="t">And this first class techniques we'll be looking at will be how do you actually best use these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=885" target="_blank">00:14:45.000</a></span> | <span class="t">pre-trained entity embeddings in a language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=890" target="_blank">00:14:50.440</a></span> | <span class="t">So I need to make a quick note that these entity embeddings are only useful to language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=894" target="_blank">00:14:54.480</a></span> | <span class="t">models though, if you can do another NLP task called entity linking well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=900" target="_blank">00:15:00.600</a></span> | <span class="t">So I'm going to take a quick aside and explain what is entity linking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=905" target="_blank">00:15:05.280</a></span> | <span class="t">So a definition of entity linking is the link mentions in text to entities in a knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=909" target="_blank">00:15:09.420</a></span> | <span class="t">base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=910" target="_blank">00:15:10.420</a></span> | <span class="t">I like to think about this in terms of how you use word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=914" target="_blank">00:15:14.260</a></span> | <span class="t">So if you want to use word embeddings and you have a sentence, you're going to first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=917" target="_blank">00:15:17.200</a></span> | <span class="t">tokenize that sentence into words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=919" target="_blank">00:15:19.440</a></span> | <span class="t">And then for each word, you're going to look up their corresponding ID in some word embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=923" target="_blank">00:15:23.040</a></span> | <span class="t">matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=924" target="_blank">00:15:24.040</a></span> | <span class="t">And now you have your word embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=925" target="_blank">00:15:25.040</a></span> | <span class="t">Well, for entity embeddings, the dictionary lookup isn't so easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=930" target="_blank">00:15:30.080</a></span> | <span class="t">You might have sentences like Washington is the first president of the United States.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=934" target="_blank">00:15:34.080</a></span> | <span class="t">Well, Washington has two different candidates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=936" target="_blank">00:15:36.600</a></span> | <span class="t">Are we talking about George Washington?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=938" target="_blank">00:15:38.640</a></span> | <span class="t">Or are we talking about Washington State?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=940" target="_blank">00:15:40.560</a></span> | <span class="t">And these are different entities that have different entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=944" target="_blank">00:15:44.240</a></span> | <span class="t">And the QIDs here would just be their identifiers and wiki data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=949" target="_blank">00:15:49.480</a></span> | <span class="t">And then United States just has a single entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=952" target="_blank">00:15:52.560</a></span> | <span class="t">So task of entity linking is to figure out correctly these ambiguous mentions, what entities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=957" target="_blank">00:15:57.240</a></span> | <span class="t">do they actually link to in a knowledge base?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=960" target="_blank">00:16:00.360</a></span> | <span class="t">And there's many different ways you can do this entity linking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=963" target="_blank">00:16:03.620</a></span> | <span class="t">So one way you might be able to do this is to figure out that, oh, I see the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=966" target="_blank">00:16:06.700</a></span> | <span class="t">word of president.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=967" target="_blank">00:16:07.880</a></span> | <span class="t">So Washington probably links to George Washington.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=971" target="_blank">00:16:11.940</a></span> | <span class="t">Just some more definitions, we're going to refer to Washington as a mention, United States</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=975" target="_blank">00:16:15.680</a></span> | <span class="t">as a mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=976" target="_blank">00:16:16.940</a></span> | <span class="t">And then the things that the mention could link to, so the two options for Washington</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=981" target="_blank">00:16:21.240</a></span> | <span class="t">are going to be candidates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=982" target="_blank">00:16:22.240</a></span> | <span class="t">So this is a whole research area of its own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=985" target="_blank">00:16:25.840</a></span> | <span class="t">And I encourage you to check out the resources at the bottom if you're interested in learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=988" target="_blank">00:16:28.920</a></span> | <span class="t">more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=989" target="_blank">00:16:29.920</a></span> | <span class="t">But right now, the most important thing to understand is that entity linking is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=993" target="_blank">00:16:33.360</a></span> | <span class="t">is going to tell us which entity embeddings are actually relevant to the text and which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=996" target="_blank">00:16:36.960</a></span> | <span class="t">ones you want to use as you iterate through a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1000" target="_blank">00:16:40.520</a></span> | <span class="t">And Megan, there are a few questions around here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1006" target="_blank">00:16:46.040</a></span> | <span class="t">One of them is, so that's entity linking, but what about the relations?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1011" target="_blank">00:16:51.280</a></span> | <span class="t">Yeah, so some of the works we'll talk about will only use the entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1017" target="_blank">00:16:57.300</a></span> | <span class="t">So some of these have been pre-trained with relation information, but in the end, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1021" target="_blank">00:17:01.080</a></span> | <span class="t">only have an entity embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1024" target="_blank">00:17:04.000</a></span> | <span class="t">So relation extraction is yet another NLP task that you could also do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1026" target="_blank">00:17:06.680</a></span> | <span class="t">But yeah, here we're just talking about entity linking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1029" target="_blank">00:17:09.600</a></span> | <span class="t">And if you have the knowledge graph you showed earlier, it had relations in it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1034" target="_blank">00:17:14.360</a></span> | <span class="t">Do you get any connection between that and the text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1040" target="_blank">00:17:20.400</a></span> | <span class="t">I mean, that's the goal of relation extraction, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1043" target="_blank">00:17:23.080</a></span> | <span class="t">It's to figure out, like, given the entities, what is the relation between them, which would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1047" target="_blank">00:17:27.160</a></span> | <span class="t">then form the full triple of head entity, tail entity, and relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1055" target="_blank">00:17:35.240</a></span> | <span class="t">Okay, then I think people want to know more about how this is going to be used, but maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1060" target="_blank">00:17:40.920</a></span> | <span class="t">you should go on and show some examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1062" target="_blank">00:17:42.920</a></span> | <span class="t">Yeah, I will, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1066" target="_blank">00:17:46.840</a></span> | <span class="t">Okay, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1069" target="_blank">00:17:49.760</a></span> | <span class="t">So entity embeddings, just to summarize, they're like word embeddings, but they're for entities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1073" target="_blank">00:17:53.680</a></span> | <span class="t">in a knowledge base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1074" target="_blank">00:17:54.760</a></span> | <span class="t">So you'll have some vector associated with George Washington, and it should be meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1078" target="_blank">00:17:58.560</a></span> | <span class="t">in embedding space such that maybe the George Washington vector is close to the vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1082" target="_blank">00:18:02.920</a></span> | <span class="t">for other founding fathers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1085" target="_blank">00:18:05.760</a></span> | <span class="t">So we're going to briefly talk about some methods for training entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1089" target="_blank">00:18:09.520</a></span> | <span class="t">There's knowledge graph embedding methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1091" target="_blank">00:18:11.160</a></span> | <span class="t">You might have heard of the transie embedding method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1093" target="_blank">00:18:13.200</a></span> | <span class="t">So this starts from the idea of having these knowledge graph triples, and you want to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1097" target="_blank">00:18:17.320</a></span> | <span class="t">pre-trained entity and pre-trained relation embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1100" target="_blank">00:18:20.280</a></span> | <span class="t">And you want it to be the case that the subject embedding and the relation embedding, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1104" target="_blank">00:18:24.040</a></span> | <span class="t">sum of those two, is close to the object embedding in vector space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1108" target="_blank">00:18:28.120</a></span> | <span class="t">So it's an algorithm to learn that constraint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1111" target="_blank">00:18:31.480</a></span> | <span class="t">There's also word entity co-occurrence methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1113" target="_blank">00:18:33.320</a></span> | <span class="t">So these build off of Word2vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1114" target="_blank">00:18:34.320</a></span> | <span class="t">One of them is even called Wikipedia2vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1117" target="_blank">00:18:37.480</a></span> | <span class="t">And the idea is given an entity, you want to figure out what words are most likely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1121" target="_blank">00:18:41.520</a></span> | <span class="t">co-occur around it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1124" target="_blank">00:18:44.360</a></span> | <span class="t">And then the last method, or one of the other methods that is common now, is actually just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1128" target="_blank">00:18:48.760</a></span> | <span class="t">using the transformer to learn representations of an entity by encoding the entity description.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1134" target="_blank">00:18:54.260</a></span> | <span class="t">And so Blink from Facebook is an approach that does this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1138" target="_blank">00:18:58.000</a></span> | <span class="t">So the methods we'll talk about today are actually agnostic to how you train your pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1141" target="_blank">00:19:01.800</a></span> | <span class="t">entity embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1142" target="_blank">00:19:02.800</a></span> | <span class="t">But I think it's important to know that there's actually a wide variety of methods to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1146" target="_blank">00:19:06.720</a></span> | <span class="t">these pre-trained entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1148" target="_blank">00:19:08.760</a></span> | <span class="t">And it's actually not clear which method is best for using them downstream in language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1153" target="_blank">00:19:13.000</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1156" target="_blank">00:19:16.600</a></span> | <span class="t">So one of the key challenges of using pre-trained entity embeddings in language models is figuring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1160" target="_blank">00:19:20.600</a></span> | <span class="t">out how to incorporate them when they're from a different embedding space than the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1164" target="_blank">00:19:24.720</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1165" target="_blank">00:19:25.720</a></span> | <span class="t">And so what we'll do, or the approach we'll look at today, we'll learn a fusion layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1169" target="_blank">00:19:29.960</a></span> | <span class="t">to combine this context and entity information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1172" target="_blank">00:19:32.960</a></span> | <span class="t">So we have entity embeddings and we have the contextualized word embeddings from our language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1177" target="_blank">00:19:37.060</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1179" target="_blank">00:19:39.280</a></span> | <span class="t">So if we take a sequence of text and we imagine that j indicates the jth element in a sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1185" target="_blank">00:19:45.240</a></span> | <span class="t">then the challenge here is you want to figure out how do we combine some word embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1188" target="_blank">00:19:48.720</a></span> | <span class="t">wj with some aligned entity embedding ek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1192" target="_blank">00:19:52.640</a></span> | <span class="t">So here an alignment could be like in the example where we had Washington was the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1196" target="_blank">00:19:56.480</a></span> | <span class="t">president.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1198" target="_blank">00:19:58.120</a></span> | <span class="t">Washington would be your word embedding and George Washington would be the aligned entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1201" target="_blank">00:20:01.720</a></span> | <span class="t">embedding there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1202" target="_blank">00:20:02.720</a></span> | <span class="t">So you could imagine in this case, let's say your wj is Washington and your ek is your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1208" target="_blank">00:20:08.120</a></span> | <span class="t">entity embedding for George Washington.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1209" target="_blank">00:20:09.520</a></span> | <span class="t">And you want to align them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1211" target="_blank">00:20:11.800</a></span> | <span class="t">So what you can do is learn a weight matrix wt for the text and we for the entity to project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1218" target="_blank">00:20:18.080</a></span> | <span class="t">these embeddings to the same dimension before you sum them and finally take an activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1222" target="_blank">00:20:22.960</a></span> | <span class="t">function over them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1225" target="_blank">00:20:25.140</a></span> | <span class="t">So the idea is that by having some fusion layer mechanism like this, you can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1230" target="_blank">00:20:30.300</a></span> | <span class="t">use these entity embeddings and these contextual word embeddings that are in different embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1234" target="_blank">00:20:34.620</a></span> | <span class="t">spaces and fuse them together to have this single hidden representation for the element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1240" target="_blank">00:20:40.760</a></span> | <span class="t">in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1244" target="_blank">00:20:44.080</a></span> | <span class="t">So the approaches we'll talk about today all have some mechanism either very similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1248" target="_blank">00:20:48.080</a></span> | <span class="t">this or some variation of this to do this combination of the context and entity information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1255" target="_blank">00:20:55.840</a></span> | <span class="t">So the first approach we're going to talk about is called ERNI, enhanced language representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1259" target="_blank">00:20:59.840</a></span> | <span class="t">with informative entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1261" target="_blank">00:21:01.600</a></span> | <span class="t">And so this just builds on what we've already talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1263" target="_blank">00:21:03.760</a></span> | <span class="t">It uses pre-trained entity embeddings and it also uses this notion of a fusion layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1269" target="_blank">00:21:09.560</a></span> | <span class="t">So the first block in ERNI is a text encoder, which is a multilayer bidirectional transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1274" target="_blank">00:21:14.680</a></span> | <span class="t">encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1275" target="_blank">00:21:15.840</a></span> | <span class="t">For their experiments, they use BERT, but it doesn't have to be BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1280" target="_blank">00:21:20.400</a></span> | <span class="t">And this is followed by a knowledge encoder, which has stacked blocks composed of two multi-headed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1285" target="_blank">00:21:25.000</a></span> | <span class="t">attentions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1286" target="_blank">00:21:26.340</a></span> | <span class="t">One is over the entity embeddings and one is over your token or subword embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1291" target="_blank">00:21:31.920</a></span> | <span class="t">And then the output of these contextualized entity and token embeddings from the multi-headed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1295" target="_blank">00:21:35.760</a></span> | <span class="t">attentions are passed to a fusion layer, which looks very similar to what we just looked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1300" target="_blank">00:21:40.760</a></span> | <span class="t">at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1302" target="_blank">00:21:42.160</a></span> | <span class="t">But now you also have new word and entity embeddings that you're producing as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1306" target="_blank">00:21:46.880</a></span> | <span class="t">of your fusion layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1307" target="_blank">00:21:47.880</a></span> | <span class="t">So you see this WJ and this EK, which are produced as the next layer of word and entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1314" target="_blank">00:21:54.040</a></span> | <span class="t">embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1315" target="_blank">00:21:55.920</a></span> | <span class="t">So the I here indicates that it's the Ith block in the knowledge encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1320" target="_blank">00:22:00.480</a></span> | <span class="t">So you'll actually have multiple stacks of these knowledge encoders and you'll be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1324" target="_blank">00:22:04.400</a></span> | <span class="t">a fusion of the word and entity embedding, producing new word and entity embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1328" target="_blank">00:22:08.800</a></span> | <span class="t">and then passing this to the next block of the knowledge encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1334" target="_blank">00:22:14.960</a></span> | <span class="t">So this is what the architecture diagram looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1337" target="_blank">00:22:17.200</a></span> | <span class="t">On the left side, we have the T encoder or the text encoder, followed by the K encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1342" target="_blank">00:22:22.120</a></span> | <span class="t">or the knowledge encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1343" target="_blank">00:22:23.120</a></span> | <span class="t">And then on the right side, we have a zoomed in version of your knowledge encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1347" target="_blank">00:22:27.680</a></span> | <span class="t">So you see the multi-headed attentions over the tokens in orange, and then over the entities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1351" target="_blank">00:22:31.420</a></span> | <span class="t">in yellow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1352" target="_blank">00:22:32.640</a></span> | <span class="t">And then you have this alignment between the word and entities with the dashed lines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1357" target="_blank">00:22:37.600</a></span> | <span class="t">So they have this example as Bob Dylan wrote "blowing in the wind" in 1962.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1362" target="_blank">00:22:42.360</a></span> | <span class="t">The entities here are Bob Dylan and "blowing in the wind."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1365" target="_blank">00:22:45.880</a></span> | <span class="t">And they have a simple alignment rule where you want to align the entity to the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1369" target="_blank">00:22:49.440</a></span> | <span class="t">word in the entity phrase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1370" target="_blank">00:22:50.680</a></span> | <span class="t">So you want to align Bob Dylan to Bob, that's what the dashed lines try to indicate, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1375" target="_blank">00:22:55.720</a></span> | <span class="t">you want to align "blowing in the wind" to "blow."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1379" target="_blank">00:22:59.200</a></span> | <span class="t">So here, this already assumes that entity linking has been done, and you know your entities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1382" target="_blank">00:23:02.640</a></span> | <span class="t">in advance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1383" target="_blank">00:23:03.640</a></span> | <span class="t">So you can see that the entities are actually input into the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1388" target="_blank">00:23:08.300</a></span> | <span class="t">So after you have your word and entity alignment, this goes through the information fusion layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1392" target="_blank">00:23:12.080</a></span> | <span class="t">in this light purple-gray color.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1394" target="_blank">00:23:14.600</a></span> | <span class="t">And then finally, it produces these new word entity embeddings as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1398" target="_blank">00:23:18.800</a></span> | <span class="t">And then remember that you have multiple blocks of these, so those will be passed into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1401" target="_blank">00:23:21.920</a></span> | <span class="t">next block of your knowledge encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1406" target="_blank">00:23:26.800</a></span> | <span class="t">So how do you actually train this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1408" target="_blank">00:23:28.460</a></span> | <span class="t">It's pretty similar to BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1409" target="_blank">00:23:29.600</a></span> | <span class="t">You have a mass language model loss, and you have a next sentence prediction loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1414" target="_blank">00:23:34.000</a></span> | <span class="t">And then they also introduce a knowledge pre-training task, which they refer to as the DEA task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1419" target="_blank">00:23:39.240</a></span> | <span class="t">It's named after a denoising entity autoencoder from an ICML paper in 2008.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1425" target="_blank">00:23:45.880</a></span> | <span class="t">And the idea is they're going to randomly mask these token entity alignments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1429" target="_blank">00:23:49.240</a></span> | <span class="t">So the idea that Bob goes to Bob Dylan, they're going to mask that out with some random percentage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1434" target="_blank">00:23:54.840</a></span> | <span class="t">And then they're going to predict the corresponding entity for a token out of the entities in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1438" target="_blank">00:23:58.400</a></span> | <span class="t">the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1439" target="_blank">00:23:59.400</a></span> | <span class="t">So this looks like as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1442" target="_blank">00:24:02.480</a></span> | <span class="t">The summation is over m entities in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1445" target="_blank">00:24:05.180</a></span> | <span class="t">So this would be over Bob Dylan and blowing in the wind in the previous example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1449" target="_blank">00:24:09.780</a></span> | <span class="t">And given a particular word, they want to figure out what entity is it most likely to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1453" target="_blank">00:24:13.980</a></span> | <span class="t">align to in that sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1455" target="_blank">00:24:15.600</a></span> | <span class="t">So does Bob align to Bob Dylan, or does Bob align to blowing in the wind?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1461" target="_blank">00:24:21.240</a></span> | <span class="t">And their motivation for doing this is that if you don't have this task, all you're ever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1464" target="_blank">00:24:24.960</a></span> | <span class="t">going to be predicting is a token with the mass language model loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1468" target="_blank">00:24:28.940</a></span> | <span class="t">And you really, to encode knowledge, should also probably be predicting over entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1473" target="_blank">00:24:33.180</a></span> | <span class="t">So by adding this task, they have some kind of task that is actually predicting the entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1478" target="_blank">00:24:38.480</a></span> | <span class="t">And they also suggest that this might better fuse the knowledge or the entity and the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1483" target="_blank">00:24:43.280</a></span> | <span class="t">representations than just using the fusion layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1488" target="_blank">00:24:48.200</a></span> | <span class="t">Their final loss is then that summation of the mass language model loss, the next sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1492" target="_blank">00:24:52.360</a></span> | <span class="t">prediction loss, and this DEA knowledge pre-training task loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1499" target="_blank">00:24:59.940</a></span> | <span class="t">So they show that ablation experiment that it's actually very important to have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1503" target="_blank">00:25:03.040</a></span> | <span class="t">knowledge pre-training task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1504" target="_blank">00:25:04.840</a></span> | <span class="t">So this has Bert on the leftmost bar, Ernie as the second bar from the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1510" target="_blank">00:25:10.480</a></span> | <span class="t">And so that's with all the features of Ernie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1512" target="_blank">00:25:12.480</a></span> | <span class="t">And then they try removing the pre-trained entity embeddings and removing this knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1515" target="_blank">00:25:15.960</a></span> | <span class="t">pre-training task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1516" target="_blank">00:25:16.960</a></span> | <span class="t">So you see that Bert performs the worst.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1519" target="_blank">00:25:19.860</a></span> | <span class="t">This isn't very surprising, and that Ernie performs the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1522" target="_blank">00:25:22.920</a></span> | <span class="t">But what's interesting is that if you remove the entity embeddings or you remove the pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1526" target="_blank">00:25:26.640</a></span> | <span class="t">task, they only do a little better than Bert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1530" target="_blank">00:25:30.920</a></span> | <span class="t">And so it's really necessary to actually use this pre-training task to get the most use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1535" target="_blank">00:25:35.800</a></span> | <span class="t">of your pre-trained entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1541" target="_blank">00:25:41.040</a></span> | <span class="t">So some strengths of this work were that they introduced some way to combine entity and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1544" target="_blank">00:25:44.320</a></span> | <span class="t">context information through this fusion layer and this knowledge pre-training task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1549" target="_blank">00:25:49.200</a></span> | <span class="t">And then they also show improved performance on downstream tasks, which we'll come back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1552" target="_blank">00:25:52.760</a></span> | <span class="t">to when we talk about evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1555" target="_blank">00:25:55.720</a></span> | <span class="t">But of course, there's also some limitations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1558" target="_blank">00:25:58.320</a></span> | <span class="t">So it needs text data with the entities annotated as input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1561" target="_blank">00:26:01.680</a></span> | <span class="t">And this is even true for downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1563" target="_blank">00:26:03.080</a></span> | <span class="t">So if you remember on the architecture diagram, we had the entity information actually input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1568" target="_blank">00:26:08.280</a></span> | <span class="t">into the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1570" target="_blank">00:26:10.240</a></span> | <span class="t">But it's not very realistic that you're necessarily going to have a good entity linker for any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1574" target="_blank">00:26:14.140</a></span> | <span class="t">downstream tasks that you want to use Ernie on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1578" target="_blank">00:26:18.280</a></span> | <span class="t">And the next challenge is this requires more pre-training of your language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1581" target="_blank">00:26:21.660</a></span> | <span class="t">So now you don't just need to pre-train Bert, but you also need to pre-train your knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1584" target="_blank">00:26:24.940</a></span> | <span class="t">encoder on top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1587" target="_blank">00:26:27.320</a></span> | <span class="t">For the first challenge, we're going to actually talk about a work that presents a solution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1590" target="_blank">00:26:30.240</a></span> | <span class="t">to address this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1591" target="_blank">00:26:31.240</a></span> | <span class="t">For the second challenge, I encourage you to check out the footnote on the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1595" target="_blank">00:26:35.420</a></span> | <span class="t">This introduces a work that actually uses pre-trained entity embeddings, uses them in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1599" target="_blank">00:26:39.920</a></span> | <span class="t">a language model, and doesn't require any more pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1602" target="_blank">00:26:42.520</a></span> | <span class="t">So it's pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1605" target="_blank">00:26:45.800</a></span> | <span class="t">I guess that's all I have for Ernie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1607" target="_blank">00:26:47.080</a></span> | <span class="t">So I want to pause here for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1613" target="_blank">00:26:53.680</a></span> | <span class="t">Well here's one that's up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1615" target="_blank">00:26:55.400</a></span> | <span class="t">So on the fusion layer, it observed that passing the entity embedding into a fusion layer to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1621" target="_blank">00:27:01.640</a></span> | <span class="t">combine with word embedding is more powerful than just concatenating the entity embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1626" target="_blank">00:27:06.500</a></span> | <span class="t">onto the end of the word embedding question mark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1628" target="_blank">00:27:08.880</a></span> | <span class="t">Yeah, so I guess people are still a little bit confused as to the motivation for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1634" target="_blank">00:27:14.160</a></span> | <span class="t">fusion layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1635" target="_blank">00:27:15.700</a></span> | <span class="t">And so I guess here it's this, the simplest strategy would be, since you've got the entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1640" target="_blank">00:27:20.680</a></span> | <span class="t">linking, you could just concatenate entity embeddings onto the end of word embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1645" target="_blank">00:27:25.700</a></span> | <span class="t">and do regular BERT, but that worked just as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1653" target="_blank">00:27:33.080</a></span> | <span class="t">I think the idea is that it wouldn't, because if you imagine that, let's say your magnitudes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1657" target="_blank">00:27:37.720</a></span> | <span class="t">are very different, you need some way to, I guess, align the spaces so that anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1663" target="_blank">00:27:43.760</a></span> | <span class="t">meaningful in the entity embedding space is still meaningful in the word embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1667" target="_blank">00:27:47.360</a></span> | <span class="t">So if you're close in the word embedding space, you also would be, you'd want to be close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1670" target="_blank">00:27:50.840</a></span> | <span class="t">in entity embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1672" target="_blank">00:27:52.560</a></span> | <span class="t">So I guess that's one argument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1681" target="_blank">00:28:01.560</a></span> | <span class="t">I mean, I think the question isn't, you know, it's a good question as people say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1685" target="_blank">00:28:05.640</a></span> | <span class="t">I mean, it's not completely obvious that it wouldn't work to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1690" target="_blank">00:28:10.120</a></span> | <span class="t">It seems like one of the potential problems is some words have entity links to them and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1696" target="_blank">00:28:16.440</a></span> | <span class="t">some words don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1698" target="_blank">00:28:18.100</a></span> | <span class="t">And so you, then you'd sort of have zero vectors for the ones that don't have anything linked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1702" target="_blank">00:28:22.960</a></span> | <span class="t">And that might act a bit weirdly, but.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1706" target="_blank">00:28:26.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1707" target="_blank">00:28:27.760</a></span> | <span class="t">In this case, when they don't have entities linked, which is a great point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1712" target="_blank">00:28:32.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1713" target="_blank">00:28:33.720</a></span> | <span class="t">The first equation just simplifies to the first term plus the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1717" target="_blank">00:28:37.640</a></span> | <span class="t">So like there's an obvious solution in that case when you're not concatenating that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1720" target="_blank">00:28:40.920</a></span> | <span class="t">just don't add on the term.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1721" target="_blank">00:28:41.920</a></span> | <span class="t">Yeah, that could be one reason too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1725" target="_blank">00:28:45.400</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1726" target="_blank">00:28:46.400</a></span> | <span class="t">Are there any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1730" target="_blank">00:28:50.960</a></span> | <span class="t">I think you can go on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1738" target="_blank">00:28:58.400</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1741" target="_blank">00:29:01.400</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1742" target="_blank">00:29:02.900</a></span> | <span class="t">So now we're talking about NoBERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1751" target="_blank">00:29:11.700</a></span> | <span class="t">And this is from the same folks that introduced the ELMo work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1755" target="_blank">00:29:15.240</a></span> | <span class="t">And the idea here is that they're going to pre-train an integrated entity linker as an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1759" target="_blank">00:29:19.680</a></span> | <span class="t">extension to BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1763" target="_blank">00:29:23.580</a></span> | <span class="t">And so their loss function will now be the summation of the next sentence prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1768" target="_blank">00:29:28.140</a></span> | <span class="t">the mass language model loss, and this entity linking loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1770" target="_blank">00:29:30.480</a></span> | <span class="t">So instead of the knowledge pre-training DEA task from Ernie, we'll have an entity linking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1774" target="_blank">00:29:34.660</a></span> | <span class="t">loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1775" target="_blank">00:29:35.660</a></span> | <span class="t">And the idea of the entity linker is you'll now have just as normal sequence as input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1781" target="_blank">00:29:41.200</a></span> | <span class="t">and the integrated entity linker will figure out what are the entities in the sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1785" target="_blank">00:29:45.020</a></span> | <span class="t">and or what are the mentions in the sentence, what are the candidates of those mentions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1789" target="_blank">00:29:49.940</a></span> | <span class="t">and then what should be the scores of those entities or the candidates given the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1794" target="_blank">00:29:54.620</a></span> | <span class="t">of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1795" target="_blank">00:29:55.620</a></span> | <span class="t">And so this is all done now as part of the model rather than requiring it as some external</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1799" target="_blank">00:29:59.980</a></span> | <span class="t">pipeline stage before you could even use Ernie, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1803" target="_blank">00:30:03.960</a></span> | <span class="t">So now for downstream tasks, you no longer need these entity annotations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1807" target="_blank">00:30:07.020</a></span> | <span class="t">Your integrated entity linker will figure out what the correct entity is and be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1811" target="_blank">00:30:11.040</a></span> | <span class="t">to use the correct entity embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1814" target="_blank">00:30:14.520</a></span> | <span class="t">So there's also this idea that learning this entity linking may actually better encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1817" target="_blank">00:30:17.780</a></span> | <span class="t">knowledge than this DEA pre-training task because they show that NoBERT actually outperforms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1822" target="_blank">00:30:22.460</a></span> | <span class="t">Ernie on downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1825" target="_blank">00:30:25.200</a></span> | <span class="t">So one reason this may occur is that if you think about the DEA task, it's actually a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1829" target="_blank">00:30:29.420</a></span> | <span class="t">bit simpler than just entity linking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1832" target="_blank">00:30:32.140</a></span> | <span class="t">So you're trying to predict, for instance, what Bob linked to out of Bob Dylan and Blowing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1836" target="_blank">00:30:36.740</a></span> | <span class="t">in the Wind, and it's much easier even as a human to see that Bob will more likely link</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1841" target="_blank">00:30:41.340</a></span> | <span class="t">to Bob Dylan than that Bob will link to Blowing in the Wind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1846" target="_blank">00:30:46.580</a></span> | <span class="t">And in the entity linking task, you actually have a much harder set of candidates to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1849" target="_blank">00:30:49.820</a></span> | <span class="t">over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1850" target="_blank">00:30:50.820</a></span> | <span class="t">You're not just looking at the ones in the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1852" target="_blank">00:30:52.580</a></span> | <span class="t">So does Washington link to George Washington or Washington State actually requires you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1857" target="_blank">00:30:57.140</a></span> | <span class="t">using more information about the entity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1859" target="_blank">00:30:59.900</a></span> | <span class="t">So given it's a harder task, it's not too surprising that it might perform better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1864" target="_blank">00:31:04.800</a></span> | <span class="t">just this easier knowledge pre-training task that Ernie introduced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1870" target="_blank">00:31:10.260</a></span> | <span class="t">So otherwise, NoBERT has a lot of similarities to Ernie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1872" target="_blank">00:31:12.860</a></span> | <span class="t">It uses a fusion layer that combines this context and entity information, and it introduces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1877" target="_blank">00:31:17.620</a></span> | <span class="t">some knowledge pre-training task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1879" target="_blank">00:31:19.840</a></span> | <span class="t">So I'd say a high-level takeaway is if you want to use pre-trained entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1882" target="_blank">00:31:22.640</a></span> | <span class="t">in a language model, you'll probably at least want to consider both of these components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1887" target="_blank">00:31:27.140</a></span> | <span class="t">in terms of actually going to integrate the pre-trained entity embeddings and take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1891" target="_blank">00:31:31.660</a></span> | <span class="t">most advantage of the knowledge in them as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1897" target="_blank">00:31:37.500</a></span> | <span class="t">So that brings us to the next class of techniques, which is using an external memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1903" target="_blank">00:31:43.100</a></span> | <span class="t">And here we'll mainly focus on this work called KGLM, and then we'll also briefly talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1906" target="_blank">00:31:46.780</a></span> | <span class="t">KNNLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1909" target="_blank">00:31:49.940</a></span> | <span class="t">So the previous methods that we've talked about have relied on pre-trained entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1913" target="_blank">00:31:53.500</a></span> | <span class="t">to encode the factual knowledge from knowledge bases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1917" target="_blank">00:31:57.220</a></span> | <span class="t">And the one problem with this, or one of the problems with this, is if you want to, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1921" target="_blank">00:32:01.100</a></span> | <span class="t">say, modify your knowledge base, you now need to retrain your entity embeddings and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1925" target="_blank">00:32:05.220</a></span> | <span class="t">retrain your language model on top of those entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1928" target="_blank">00:32:08.880</a></span> | <span class="t">So this begs the question, are there more direct ways than pre-trained entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1933" target="_blank">00:32:13.360</a></span> | <span class="t">to provide the model with factual knowledge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1937" target="_blank">00:32:17.140</a></span> | <span class="t">And so what we're going to talk about is how you can actually use an external memory or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1940" target="_blank">00:32:20.260</a></span> | <span class="t">a key value store to give the model access to either knowledge graph triples or context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1945" target="_blank">00:32:25.220</a></span> | <span class="t">information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1946" target="_blank">00:32:26.220</a></span> | <span class="t">And a key thing about this external memory is that it's independent of the learned model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1950" target="_blank">00:32:30.740</a></span> | <span class="t">parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1953" target="_blank">00:32:33.100</a></span> | <span class="t">So this means you can actually support injecting and updating factual knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1957" target="_blank">00:32:37.080</a></span> | <span class="t">You can do this directly to the symbolic external memory by, let's say, changing the value for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1961" target="_blank">00:32:41.220</a></span> | <span class="t">a particular key or maybe adding another key.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1964" target="_blank">00:32:44.740</a></span> | <span class="t">And you don't have to pre-train or retrain your entity embeddings when you make this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1969" target="_blank">00:32:49.020</a></span> | <span class="t">change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1970" target="_blank">00:32:50.020</a></span> | <span class="t">And the approaches we'll talk about today can actually even have these updates to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1974" target="_blank">00:32:54.300</a></span> | <span class="t">external memory without more pre-training of the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1979" target="_blank">00:32:59.020</a></span> | <span class="t">So that's pretty neat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1981" target="_blank">00:33:01.060</a></span> | <span class="t">And then another benefit of using external memory over these pre-trained entity embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1984" target="_blank">00:33:04.740</a></span> | <span class="t">approaches is it can also be more interpretable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1987" target="_blank">00:33:07.980</a></span> | <span class="t">So if you have an air in your model where it's not predicting a correct fact, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1994" target="_blank">00:33:14.700</a></span> | <span class="t">very challenging to figure out with pre-trained entity embeddings what the problem might be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=1999" target="_blank">00:33:19.460</a></span> | <span class="t">Was it the original knowledge base?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2000" target="_blank">00:33:20.820</a></span> | <span class="t">Was it the encoding in the entity embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2002" target="_blank">00:33:22.500</a></span> | <span class="t">Is it how the language model is using the entity embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2005" target="_blank">00:33:25.420</a></span> | <span class="t">And here you have a little more information with an external memory in that you can look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2009" target="_blank">00:33:29.260</a></span> | <span class="t">in the external memory and see, was the fact in the external memory?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2013" target="_blank">00:33:33.620</a></span> | <span class="t">Was it not in the external memory?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2014" target="_blank">00:33:34.700</a></span> | <span class="t">And so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2015" target="_blank">00:33:35.900</a></span> | <span class="t">So it adds a little bit more interpretability than just using these pre-trained entity embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2020" target="_blank">00:33:40.380</a></span> | <span class="t">as an indirect way to encode the knowledge base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2025" target="_blank">00:33:45.940</a></span> | <span class="t">So the first work we're going to talk about is called KGLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2028" target="_blank">00:33:48.660</a></span> | <span class="t">And unlike the other approaches we've talked about so far, this actually uses LSTMs and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2033" target="_blank">00:33:53.180</a></span> | <span class="t">not transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2035" target="_blank">00:33:55.820</a></span> | <span class="t">So the key idea here is to condition the language model on a knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2040" target="_blank">00:34:00.940</a></span> | <span class="t">So recall with the standard language model, we want to predict the next word given the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2044" target="_blank">00:34:04.420</a></span> | <span class="t">previous words in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2047" target="_blank">00:34:07.420</a></span> | <span class="t">So now we also want to predict the next entity given the previous words in the sequence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2051" target="_blank">00:34:11.540</a></span> | <span class="t">given the previous entities in the sentence, or the entities that are relevant to the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2056" target="_blank">00:34:16.540</a></span> | <span class="t">I should say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2057" target="_blank">00:34:17.540</a></span> | <span class="t">So KGLM will be building a local knowledge graph as it iterates over the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2064" target="_blank">00:34:24.500</a></span> | <span class="t">And a local knowledge graph is just a subset of a full knowledge graph that only has the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2068" target="_blank">00:34:28.260</a></span> | <span class="t">entities that are actually relevant to the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2072" target="_blank">00:34:32.240</a></span> | <span class="t">So if we have this example here, a simplified example from the paper, that Super Mario Land</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2077" target="_blank">00:34:37.480</a></span> | <span class="t">is a game developed by blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2079" target="_blank">00:34:39.760</a></span> | <span class="t">And Super Mario Land here is an entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2083" target="_blank">00:34:43.160</a></span> | <span class="t">You'd want a local knowledge graph as follows, where you see that Super Mario Land is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2087" target="_blank">00:34:47.040</a></span> | <span class="t">the local knowledge graph, but we also have the relations to Super Mario Land to other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2091" target="_blank">00:34:51.240</a></span> | <span class="t">entities that are copied from the full knowledge graph into this local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2096" target="_blank">00:34:56.440</a></span> | <span class="t">And you would build up this local knowledge graph as you iterate over the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2099" target="_blank">00:34:59.560</a></span> | <span class="t">So whenever you see an entity, you would add it to the local knowledge graph as well as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2103" target="_blank">00:35:03.020</a></span> | <span class="t">its relations to other entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2106" target="_blank">00:35:06.500</a></span> | <span class="t">So obviously this is a much smaller example than what would really have all the relations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2110" target="_blank">00:35:10.920</a></span> | <span class="t">to Super Mario Land, just for the purpose of the example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2114" target="_blank">00:35:14.080</a></span> | <span class="t">But hopefully it's clear that all of these are relevant to the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2120" target="_blank">00:35:20.000</a></span> | <span class="t">Something important to note here is that this does assume that the entities are known during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2123" target="_blank">00:35:23.240</a></span> | <span class="t">training so that you do have this entity annotated data for training, and therefore your local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2127" target="_blank">00:35:27.800</a></span> | <span class="t">knowledge graph is always the ground truth local knowledge graph as you iterate over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2131" target="_blank">00:35:31.320</a></span> | <span class="t">the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2133" target="_blank">00:35:33.840</a></span> | <span class="t">So why might this be a good idea to do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2135" target="_blank">00:35:35.960</a></span> | <span class="t">Well, here, the next word you want to predict is Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2139" target="_blank">00:35:39.640</a></span> | <span class="t">And you may notice that Nintendo is in your local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2143" target="_blank">00:35:43.120</a></span> | <span class="t">So sometimes this local knowledge graph can actually serve as a very strong signal for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2147" target="_blank">00:35:47.000</a></span> | <span class="t">what you want to predict for your next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2149" target="_blank">00:35:49.400</a></span> | <span class="t">Now, you may be thinking, well, this wouldn't always be helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2153" target="_blank">00:35:53.320</a></span> | <span class="t">And that's true as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2155" target="_blank">00:35:55.640</a></span> | <span class="t">So if you look at just like the third word in the sequence and you want to predict that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2158" target="_blank">00:35:58.640</a></span> | <span class="t">word, so is a game, for instance, well, if this isn't in the local knowledge graph, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2164" target="_blank">00:36:04.360</a></span> | <span class="t">wouldn't be necessarily that helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2166" target="_blank">00:36:06.840</a></span> | <span class="t">You would just do a standard language model prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2170" target="_blank">00:36:10.320</a></span> | <span class="t">Or if you're at the beginning of the sequence, your local knowledge graph is empty.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2173" target="_blank">00:36:13.980</a></span> | <span class="t">So of course, you're not going to get any signal from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2176" target="_blank">00:36:16.900</a></span> | <span class="t">So the first question they ask in KGLM is how can a language model know when to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2181" target="_blank">00:36:21.400</a></span> | <span class="t">a local knowledge graph and when it might actually be useful for predicting the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2185" target="_blank">00:36:25.560</a></span> | <span class="t">word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2186" target="_blank">00:36:26.560</a></span> | <span class="t">So we're going to keep the same example as a running example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2191" target="_blank">00:36:31.960</a></span> | <span class="t">And we have our local knowledge graph here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2194" target="_blank">00:36:34.200</a></span> | <span class="t">We now have an LSTM that looks similar to the representations you've seen throughout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2197" target="_blank">00:36:37.240</a></span> | <span class="t">this class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2198" target="_blank">00:36:38.600</a></span> | <span class="t">And normally, you've seen the LSTM predicts the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2201" target="_blank">00:36:41.320</a></span> | <span class="t">Well, now we're also going to use the LSTM to predict the next type of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2206" target="_blank">00:36:46.920</a></span> | <span class="t">So is the next word going to be a related entity, meaning it's in the local knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2210" target="_blank">00:36:50.680</a></span> | <span class="t">graph already?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2211" target="_blank">00:36:51.680</a></span> | <span class="t">Is it going to be a new entity, meaning it's not in the local knowledge graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2216" target="_blank">00:36:56.040</a></span> | <span class="t">Or is it going to be not an entity, in which case you just revert to a normal LSTM prediction?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2222" target="_blank">00:37:02.080</a></span> | <span class="t">And they're going to use the LSTM hidden state to do this prediction of the type of the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2225" target="_blank">00:37:05.600</a></span> | <span class="t">word over this three different classes that they might want to consider.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2231" target="_blank">00:37:11.640</a></span> | <span class="t">So in the case of Super Mario Land as a game developed by Nintendo, we saw that this would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2235" target="_blank">00:37:15.880</a></span> | <span class="t">be a related entity case because we saw that Nintendo was in the local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2240" target="_blank">00:37:20.680</a></span> | <span class="t">For the other cases, Super Mario Land would be a new entity case since the local knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2246" target="_blank">00:37:26.000</a></span> | <span class="t">graph is empty at that point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2247" target="_blank">00:37:27.960</a></span> | <span class="t">And then any of the words between Super Mario Land and Nintendo would be non-entity, as they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2253" target="_blank">00:37:33.240</a></span> | <span class="t">just a standard LSTM language model prediction that doesn't involve any entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2260" target="_blank">00:37:40.360</a></span> | <span class="t">So now we need to talk about what the language model actually does in these three different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2263" target="_blank">00:37:43.800</a></span> | <span class="t">scenarios to predict the next entity and the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2271" target="_blank">00:37:51.200</a></span> | <span class="t">So we're going to keep the example up at the top in case you want to refer back to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2273" target="_blank">00:37:53.680</a></span> | <span class="t">three different cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2274" target="_blank">00:37:54.680</a></span> | <span class="t">And we're going to start with the related entity case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2279" target="_blank">00:37:59.200</a></span> | <span class="t">So here we assume that the next word or entity is actually in your local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2284" target="_blank">00:38:04.040</a></span> | <span class="t">And remember that we can describe a knowledge graph in terms of triples, so in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2288" target="_blank">00:38:08.160</a></span> | <span class="t">pairs of parent entities, relations, and tail entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2291" target="_blank">00:38:11.640</a></span> | <span class="t">And in the case of predicting the next word as Nintendo, there's only one possible parent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2297" target="_blank">00:38:17.320</a></span> | <span class="t">entity in the local knowledge graph, which is Super Mario Land.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2301" target="_blank">00:38:21.320</a></span> | <span class="t">And the goal is you want to figure out what is the most relevant triple that will be useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2305" target="_blank">00:38:25.600</a></span> | <span class="t">in helping to predict the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2308" target="_blank">00:38:28.280</a></span> | <span class="t">So in this case, you could have the triple Super Mario Land publisher Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2312" target="_blank">00:38:32.420</a></span> | <span class="t">You might have the triple Super Mario Land genre platform game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2315" target="_blank">00:38:35.680</a></span> | <span class="t">Which of these is actually helpful in predicting that Nintendo should be the next word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2320" target="_blank">00:38:40.840</a></span> | <span class="t">So here, what you would want KGLM to do is predict that the top scoring parent entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2325" target="_blank">00:38:45.440</a></span> | <span class="t">is Super Mario Land, and the top scoring relation is publisher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2329" target="_blank">00:38:49.080</a></span> | <span class="t">And you can see there are actually contextual cues in the sentence that could help you figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2332" target="_blank">00:38:52.960</a></span> | <span class="t">out which triple you're talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2336" target="_blank">00:38:56.720</a></span> | <span class="t">And then given that your top scoring parent entity is Super Mario Land, and your top scoring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2340" target="_blank">00:39:00.480</a></span> | <span class="t">relation is publisher, you can figure out that using knowledge graph triples, the tail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2345" target="_blank">00:39:05.520</a></span> | <span class="t">entity has to be Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2347" target="_blank">00:39:07.680</a></span> | <span class="t">And therefore, this gives you a strong signal that the next word will be Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2355" target="_blank">00:39:15.260</a></span> | <span class="t">So the goal is you're going to find the top scoring parent entity and the top scoring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2358" target="_blank">00:39:18.160</a></span> | <span class="t">relation using the nodes in your local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2360" target="_blank">00:39:20.800</a></span> | <span class="t">And you can do this by using the LSTM hidden state combined with pre-trained entity and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2365" target="_blank">00:39:25.080</a></span> | <span class="t">relation embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2366" target="_blank">00:39:26.080</a></span> | <span class="t">So I do admit I cheated here a little bit in that this does use pre-trained embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2371" target="_blank">00:39:31.200</a></span> | <span class="t">But hopefully you'll see by the end of this discussion, why I think it fits a bit better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2374" target="_blank">00:39:34.460</a></span> | <span class="t">in this external memory use case as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2379" target="_blank">00:39:39.040</a></span> | <span class="t">So what they're going to do is they're going to take a softmax using LSTM hidden state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2382" target="_blank">00:39:42.080</a></span> | <span class="t">and the entity embeddings for each of the potential parent entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2385" target="_blank">00:39:45.680</a></span> | <span class="t">And they'll take this top scoring one as a parent entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2388" target="_blank">00:39:48.680</a></span> | <span class="t">And they'll do the same thing for the relation embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2392" target="_blank">00:39:52.240</a></span> | <span class="t">The next entity is then just this tail entity from the knowledge graph triple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2396" target="_blank">00:39:56.240</a></span> | <span class="t">So it's relatively trivial to figure out what the next entity should be once you've figured</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2400" target="_blank">00:40:00.200</a></span> | <span class="t">out the top scoring parent entity and your top scoring relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2404" target="_blank">00:40:04.920</a></span> | <span class="t">And then finally, to predict the next word, they take the vocabulary and they expand it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2409" target="_blank">00:40:09.800</a></span> | <span class="t">to include different aliases that could refer to that entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2414" target="_blank">00:40:14.040</a></span> | <span class="t">So what we mean by aliases here are phrases that could refer to the entity in text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2418" target="_blank">00:40:18.960</a></span> | <span class="t">So you might not just call it Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2420" target="_blank">00:40:20.840</a></span> | <span class="t">You might also say Nintendo company or CoPi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2423" target="_blank">00:40:23.680</a></span> | <span class="t">And you want any of these to be possible words that you could predict as the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2428" target="_blank">00:40:28.940</a></span> | <span class="t">So the goal of this vocabulary expansion is to increase the probability that the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2433" target="_blank">00:40:33.480</a></span> | <span class="t">word you predict will actually be related to this next entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2440" target="_blank">00:40:40.280</a></span> | <span class="t">So a new entity case is a bit simpler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2442" target="_blank">00:40:42.400</a></span> | <span class="t">This means that the entity that you're predicting is not in the local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2445" target="_blank">00:40:45.280</a></span> | <span class="t">So you're not getting any signal from this local knowledge graph that you've been building</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2448" target="_blank">00:40:48.680</a></span> | <span class="t">up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2450" target="_blank">00:40:50.360</a></span> | <span class="t">And all you want to do is find the top scoring entity in the full knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2454" target="_blank">00:40:54.160</a></span> | <span class="t">And you can do this using the LSTM hidden state and pre-trained entity embeddings, similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2457" target="_blank">00:40:57.920</a></span> | <span class="t">to how we found the score for the top parent entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2462" target="_blank">00:41:02.080</a></span> | <span class="t">Your next entity will just be the top scoring entity out of the full knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2466" target="_blank">00:41:06.360</a></span> | <span class="t">And then your next word is once again this vocabulary expanded to include aliases of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2470" target="_blank">00:41:10.480</a></span> | <span class="t">that entity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2473" target="_blank">00:41:13.280</a></span> | <span class="t">The not an entity case is the simplest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2475" target="_blank">00:41:15.920</a></span> | <span class="t">You just revert to normal LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2477" target="_blank">00:41:17.960</a></span> | <span class="t">You don't have an X entity to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2479" target="_blank">00:41:19.680</a></span> | <span class="t">And your next word is just the most likely next token over your normal vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2487" target="_blank">00:41:27.120</a></span> | <span class="t">So here's a diagram from the paper that hopefully summarizes and makes even clearer what I just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2491" target="_blank">00:41:31.960</a></span> | <span class="t">went over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2493" target="_blank">00:41:33.600</a></span> | <span class="t">So they have a longer example than the one we are looking at, but it's the same prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2497" target="_blank">00:41:37.200</a></span> | <span class="t">as Nintendo's next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2499" target="_blank">00:41:39.240</a></span> | <span class="t">And they have their predictions in red.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2500" target="_blank">00:41:40.720</a></span> | <span class="t">So this is what they want KGLM to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2503" target="_blank">00:41:43.200</a></span> | <span class="t">The three different cases are in the horizontals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2505" target="_blank">00:41:45.800</a></span> | <span class="t">And we see that here you're in the related entity case, since Nintendo is in your local</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2510" target="_blank">00:41:50.600</a></span> | <span class="t">knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2512" target="_blank">00:41:52.560</a></span> | <span class="t">So they want KGLM to predict that Nintendo should be a related entity type of word, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2517" target="_blank">00:41:57.680</a></span> | <span class="t">Super Mario Land should be its parent entity, that publisher should be the relevant relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2522" target="_blank">00:42:02.880</a></span> | <span class="t">And as a result, the next entity is Nintendo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2526" target="_blank">00:42:06.300</a></span> | <span class="t">And then they expand the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2528" target="_blank">00:42:08.000</a></span> | <span class="t">You see the aliases of Nintendo at the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2531" target="_blank">00:42:11.240</a></span> | <span class="t">And then finally, they actually predict Nintendo as the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2534" target="_blank">00:42:14.800</a></span> | <span class="t">And the other cases just summarize what we also already went over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2540" target="_blank">00:42:20.280</a></span> | <span class="t">So they find that KGLM actually outperforms GPT-2 and AWD-LSTM, which is a strong LSTM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2546" target="_blank">00:42:26.920</a></span> | <span class="t">language model, on a fact completion task similar to the fill-in-the-blank examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2551" target="_blank">00:42:31.240</a></span> | <span class="t">that we looked at at the beginning of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2554" target="_blank">00:42:34.400</a></span> | <span class="t">They also find qualitatively that compared to GPT-2, KGLM tends to predict more specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2559" target="_blank">00:42:39.360</a></span> | <span class="t">tokens since it can predict these tokens from just copying from the local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2564" target="_blank">00:42:44.360</a></span> | <span class="t">Whereas GPT-2 will tend to predict more generic tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2567" target="_blank">00:42:47.960</a></span> | <span class="t">So if you want to predict the birthplace of someone, GPT-2 is more likely to predict New</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2571" target="_blank">00:42:51.440</a></span> | <span class="t">York, for example, and KGLM might predict some obscure place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2577" target="_blank">00:42:57.200</a></span> | <span class="t">And then they have these really cool set of experiments where they show that KGLM actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2580" target="_blank">00:43:00.600</a></span> | <span class="t">supports modifying or updating facts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2583" target="_blank">00:43:03.860</a></span> | <span class="t">So they made a direct change in the knowledge graph, and then they saw what is the change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2587" target="_blank">00:43:07.400</a></span> | <span class="t">in KGLM's predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2590" target="_blank">00:43:10.280</a></span> | <span class="t">So they have this example where the sequence was Barack Obama is born on blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2595" target="_blank">00:43:15.760</a></span> | <span class="t">They had their knowledge graph triple as Barack Obama's original birth date, and then their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2599" target="_blank">00:43:19.440</a></span> | <span class="t">most likely next tokens were as expected, August 4, 1961.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2604" target="_blank">00:43:24.200</a></span> | <span class="t">And then they just changed their knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2605" target="_blank">00:43:25.800</a></span> | <span class="t">So they changed the birth date of Obama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2607" target="_blank">00:43:27.680</a></span> | <span class="t">They said, OK, he's now born 2013.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2610" target="_blank">00:43:30.820</a></span> | <span class="t">And they looked to see what the next predictions were for KGLM, and it changed its predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2615" target="_blank">00:43:35.580</a></span> | <span class="t">to match what was in the local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2618" target="_blank">00:43:38.600</a></span> | <span class="t">So this is something that's pretty cool and that really only external memory approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2623" target="_blank">00:43:43.040</a></span> | <span class="t">can do compared to the original pre-trained empty embedding approaches we talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2627" target="_blank">00:43:47.660</a></span> | <span class="t">And I think it's one of the reasons that KGLM, at least in my opinion, fits better in these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2631" target="_blank">00:43:51.400</a></span> | <span class="t">external memory use cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2634" target="_blank">00:43:54.360</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2636" target="_blank">00:43:56.840</a></span> | <span class="t">So the next slide is a different paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2638" target="_blank">00:43:58.920</a></span> | <span class="t">So I guess I'll take questions on KGLM if there are any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2644" target="_blank">00:44:04.480</a></span> | <span class="t">It's a pretty complex method, so feel free to have questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2650" target="_blank">00:44:10.600</a></span> | <span class="t">Yeah, could you one more time explain what the definition of the local knowledge graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2655" target="_blank">00:44:15.520</a></span> | <span class="t">is in relationship to the global knowledge graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2658" target="_blank">00:44:18.360</a></span> | <span class="t">Yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2659" target="_blank">00:44:19.360</a></span> | <span class="t">So a local knowledge graph is supposed to be a subset of the full knowledge graph, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2664" target="_blank">00:44:24.760</a></span> | <span class="t">it's only supposed to consist of entities that have actually been seen in the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2671" target="_blank">00:44:31.280</a></span> | <span class="t">as well as their relevant entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2675" target="_blank">00:44:35.440</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2676" target="_blank">00:44:36.440</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2677" target="_blank">00:44:37.440</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2679" target="_blank">00:44:39.200</a></span> | <span class="t">So here you see that Super Mario Land is in the local knowledge graph because Super Mario</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2683" target="_blank">00:44:43.440</a></span> | <span class="t">Land is an entity that is seen in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2685" target="_blank">00:44:45.920</a></span> | <span class="t">And then you also want to copy over all the edges from Super Mario Land that would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2690" target="_blank">00:44:50.760</a></span> | <span class="t">in the full knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2692" target="_blank">00:44:52.400</a></span> | <span class="t">So this is just a subset of them for the purpose of the example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2694" target="_blank">00:44:54.920</a></span> | <span class="t">But you see that Super Mario Land has an edge to Nintendo, to Game Boy, to platform game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2699" target="_blank">00:44:59.440</a></span> | <span class="t">And so you would copy all edges that Super Mario Land has to another node in the full</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2703" target="_blank">00:45:03.160</a></span> | <span class="t">knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2704" target="_blank">00:45:04.160</a></span> | <span class="t">And they know in advance, like they have the labels here for what the entities are during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2709" target="_blank">00:45:09.080</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2710" target="_blank">00:45:10.080</a></span> | <span class="t">So that's how they can actually create this ground truth knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2713" target="_blank">00:45:13.400</a></span> | <span class="t">And then briefly, a student asked why we can't just use the whole knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2719" target="_blank">00:45:19.720</a></span> | <span class="t">And I gave an answer, but maybe you know better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2722" target="_blank">00:45:22.640</a></span> | <span class="t">Yeah, I think the idea is the signal will be much stronger if you just use a local knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2727" target="_blank">00:45:27.480</a></span> | <span class="t">graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2728" target="_blank">00:45:28.480</a></span> | <span class="t">So in the Softmax for the related entity case, you would just be predicting over the potential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2736" target="_blank">00:45:36.080</a></span> | <span class="t">parent entities in your local knowledge graph, which is a much smaller set than what's in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2739" target="_blank">00:45:39.000</a></span> | <span class="t">your full knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2741" target="_blank">00:45:41.480</a></span> | <span class="t">So I guess it's more likely that you're going to predict something that is correct in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2744" target="_blank">00:45:44.920</a></span> | <span class="t">case than when you have like 5 million or so entities in your full knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2749" target="_blank">00:45:49.080</a></span> | <span class="t">It's also much cheaper to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2751" target="_blank">00:45:51.640</a></span> | <span class="t">In this case, there's only a single parent entity, but you could have multiple parent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2754" target="_blank">00:45:54.520</a></span> | <span class="t">entities that you're trying to compute which one's most likely over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2758" target="_blank">00:45:58.680</a></span> | <span class="t">Is that what you were also thinking, John?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2761" target="_blank">00:46:01.360</a></span> | <span class="t">Yeah, I mainly just said efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2765" target="_blank">00:46:05.360</a></span> | <span class="t">So the signal thing is cool too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2768" target="_blank">00:46:08.360</a></span> | <span class="t">Here's an exciting question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2769" target="_blank">00:46:09.360</a></span> | <span class="t">What about queries that require more than one step in the knowledge graph, such as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2776" target="_blank">00:46:16.760</a></span> | <span class="t">location of the publisher of Super Mario Land?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2780" target="_blank">00:46:20.920</a></span> | <span class="t">Yeah, that's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2785" target="_blank">00:46:25.560</a></span> | <span class="t">So the idea is like, can it support those types?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2787" target="_blank">00:46:27.760</a></span> | <span class="t">Like does it support multi-hop kind of building of the knowledge graph?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2791" target="_blank">00:46:31.960</a></span> | <span class="t">Yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2792" target="_blank">00:46:32.960</a></span> | <span class="t">How does KGLM perform in those cases?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2796" target="_blank">00:46:36.120</a></span> | <span class="t">Yeah, I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2797" target="_blank">00:46:37.880</a></span> | <span class="t">That's a very good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2798" target="_blank">00:46:38.880</a></span> | <span class="t">They built up the knowledge graph so that it's just single hop as far as I know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2803" target="_blank">00:46:43.120</a></span> | <span class="t">But like if you saw the other entities, if you were to see the entities along the hops,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2807" target="_blank">00:46:47.640</a></span> | <span class="t">it would have them in the local knowledge graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2809" target="_blank">00:46:49.560</a></span> | <span class="t">Yeah, that's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2811" target="_blank">00:46:51.440</a></span> | <span class="t">I don't know if they explored that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2816" target="_blank">00:46:56.920</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2817" target="_blank">00:46:57.920</a></span> | <span class="t">Okay, let's move along then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2823" target="_blank">00:47:03.880</a></span> | <span class="t">Okay, so the next piece of work we're going to talk about, you guys have actually briefly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2833" target="_blank">00:47:13.680</a></span> | <span class="t">seen in the natural language generation lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2836" target="_blank">00:47:16.440</a></span> | <span class="t">But I'm going to go over it again quickly here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2840" target="_blank">00:47:20.120</a></span> | <span class="t">So unlike the other works that we've talked about that have used knowledge graph triples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2843" target="_blank">00:47:23.440</a></span> | <span class="t">this is actually going to take kind of a looser notion of knowledge in that the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2847" target="_blank">00:47:27.400</a></span> | <span class="t">will just be encoded in the text in the training data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2850" target="_blank">00:47:30.720</a></span> | <span class="t">So this is called KNNLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2853" target="_blank">00:47:33.020</a></span> | <span class="t">And the idea is that, or it's building the idea that language models not only learn to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2857" target="_blank">00:47:37.240</a></span> | <span class="t">predict the next word in text, but they also learn these representations of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2862" target="_blank">00:47:42.160</a></span> | <span class="t">And the authors suggest that it might actually be easier to learn similarities between text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2866" target="_blank">00:47:46.360</a></span> | <span class="t">sequences than it is to predict the next word in the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2869" target="_blank">00:47:49.640</a></span> | <span class="t">So you have this example that Dickens is the author of blank and Dickens wrote blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2875" target="_blank">00:47:55.320</a></span> | <span class="t">And they argue that it's easier to tell for a human, but also for a model, that these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2879" target="_blank">00:47:59.640</a></span> | <span class="t">sequences are similar and they should probably have the same next word, even if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2883" target="_blank">00:48:03.960</a></span> | <span class="t">know what the next word is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2886" target="_blank">00:48:06.360</a></span> | <span class="t">So that's suggesting that it's easier to learn these similarities than it is to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2890" target="_blank">00:48:10.120</a></span> | <span class="t">predict the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2891" target="_blank">00:48:11.120</a></span> | <span class="t">And they argue that this is even more true for long tail patterns, where it's very challenging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2895" target="_blank">00:48:15.920</a></span> | <span class="t">for the model to predict that the next word is some rarely seen token or rare entity than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2901" target="_blank">00:48:21.080</a></span> | <span class="t">it is to find another similar sequence that it's already seen and just copy the next word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2905" target="_blank">00:48:25.880</a></span> | <span class="t">from that sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2908" target="_blank">00:48:28.640</a></span> | <span class="t">So what they propose to do is store all representations of text sequences in nearest neighbor data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2912" target="_blank">00:48:32.840</a></span> | <span class="t">store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2914" target="_blank">00:48:34.040</a></span> | <span class="t">And then at inference, what you'll want to do is you find the k most similar sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2917" target="_blank">00:48:37.800</a></span> | <span class="t">of text, you then retrieve their corresponding values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2920" target="_blank">00:48:40.800</a></span> | <span class="t">So you just peek at those sequences and see what were their next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2925" target="_blank">00:48:45.200</a></span> | <span class="t">And then you combine the probability from this nearest neighbor data store with just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2929" target="_blank">00:48:49.760</a></span> | <span class="t">a typical language model prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2932" target="_blank">00:48:52.080</a></span> | <span class="t">And so they call this an interpolation step in that they're weighting how much to pay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2935" target="_blank">00:48:55.800</a></span> | <span class="t">attention to the probability from this k and n approach, and how much to pay attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2939" target="_blank">00:48:59.720</a></span> | <span class="t">to this language model approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2942" target="_blank">00:49:02.600</a></span> | <span class="t">And the lambda here is just a hyperparameter that they tune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2948" target="_blank">00:49:08.040</a></span> | <span class="t">So they have this diagram from their paper where they want to predict the next word in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2951" target="_blank">00:49:11.320</a></span> | <span class="t">the sequence, Shakespeare's play blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2953" target="_blank">00:49:13.520</a></span> | <span class="t">So what they do is they have all the training contexts already encoded in their data store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2958" target="_blank">00:49:18.400</a></span> | <span class="t">So they have representations of all of the training contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2961" target="_blank">00:49:21.840</a></span> | <span class="t">And then they compute a representation of their text context, and they want to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2965" target="_blank">00:49:25.200</a></span> | <span class="t">out which representations in the training context are most similar to this test context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2970" target="_blank">00:49:30.320</a></span> | <span class="t">representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2972" target="_blank">00:49:32.880</a></span> | <span class="t">And so here in the external memory view of things, the keys would be the representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2977" target="_blank">00:49:37.720</a></span> | <span class="t">of the training context, and the values would be the next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2982" target="_blank">00:49:42.840</a></span> | <span class="t">So they get the k nearest training representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2986" target="_blank">00:49:46.440</a></span> | <span class="t">They then copy over their values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2987" target="_blank">00:49:47.800</a></span> | <span class="t">So that's what you see with this Macbeth, Hamlet, Macbeth example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2991" target="_blank">00:49:51.600</a></span> | <span class="t">They have a normalization step where they convert this to probability space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2995" target="_blank">00:49:55.760</a></span> | <span class="t">And then finally, they have an aggregation step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=2998" target="_blank">00:49:58.160</a></span> | <span class="t">So if a word is seen as the next word in several of these k nearest neighbors, then they want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3003" target="_blank">00:50:03.480</a></span> | <span class="t">to count more for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3004" target="_blank">00:50:04.480</a></span> | <span class="t">So that's why they aggregate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3005" target="_blank">00:50:05.480</a></span> | <span class="t">So they see Macbeth twice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3006" target="_blank">00:50:06.640</a></span> | <span class="t">It means Macbeth is more likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3010" target="_blank">00:50:10.300</a></span> | <span class="t">And then finally, they have this interpolation step where they try to balance between the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3014" target="_blank">00:50:14.400</a></span> | <span class="t">classification probabilities from the language model and from the k and n approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3020" target="_blank">00:50:20.960</a></span> | <span class="t">So some immediate observation you might have is this seems really expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3025" target="_blank">00:50:25.620</a></span> | <span class="t">They do propose ways to try to minimize the expense of actually having to store all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3030" target="_blank">00:50:30.840</a></span> | <span class="t">training contexts in this data store, because they actually store it for every single window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3035" target="_blank">00:50:35.480</a></span> | <span class="t">of next word in the training context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3038" target="_blank">00:50:38.560</a></span> | <span class="t">And you can do quantization on some nearest neighbor approaches to try to make this less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3042" target="_blank">00:50:42.440</a></span> | <span class="t">expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3044" target="_blank">00:50:44.040</a></span> | <span class="t">But I imagine this would still be pretty expensive for really large training data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3047" target="_blank">00:50:47.800</a></span> | <span class="t">They also have some cool experiments that show that this is very good for domain adaptation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3053" target="_blank">00:50:53.040</a></span> | <span class="t">So if you take your language model and you have a new domain that you want to apply your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3056" target="_blank">00:50:56.680</a></span> | <span class="t">language model to, you could just create a nearest neighbor data store of your new domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3062" target="_blank">00:51:02.420</a></span> | <span class="t">So you encode all the representations of that new domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3065" target="_blank">00:51:05.640</a></span> | <span class="t">You stick it in a data store.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3067" target="_blank">00:51:07.380</a></span> | <span class="t">And then you can just use your language model with these k and n probabilities as well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3072" target="_blank">00:51:12.880</a></span> | <span class="t">just immediately on this new domain without actually having to further train your language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3076" target="_blank">00:51:16.780</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3078" target="_blank">00:51:18.240</a></span> | <span class="t">So I thought that was a pretty cool use case of this external memory approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3083" target="_blank">00:51:23.560</a></span> | <span class="t">So while it doesn't leverage knowledge bases directly, it does have this loose knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3087" target="_blank">00:51:27.520</a></span> | <span class="t">of-- or loose idea of encoding knowledge that is in a textual representation form into some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3093" target="_blank">00:51:33.120</a></span> | <span class="t">external memory that the model can then take advantage of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3100" target="_blank">00:51:40.120</a></span> | <span class="t">That's all I have for this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3101" target="_blank">00:51:41.360</a></span> | <span class="t">Are there any questions on this approach?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3105" target="_blank">00:51:45.360</a></span> | <span class="t">Well, so only one person is asking, how does the k and n make predictions for the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3115" target="_blank">00:51:55.380</a></span> | <span class="t">word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3116" target="_blank">00:51:56.380</a></span> | <span class="t">The k neighbors are for the context instead of the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3120" target="_blank">00:52:00.140</a></span> | <span class="t">Oh, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3121" target="_blank">00:52:01.140</a></span> | <span class="t">That wasn't clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3122" target="_blank">00:52:02.520</a></span> | <span class="t">So the keys are the representations of the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3125" target="_blank">00:52:05.860</a></span> | <span class="t">The values in your external memory are the next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3129" target="_blank">00:52:09.060</a></span> | <span class="t">So when you figure out-- you figure out your nearest neighbors using your keys, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3132" target="_blank">00:52:12.860</a></span> | <span class="t">you copy over their values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3134" target="_blank">00:52:14.460</a></span> | <span class="t">So it does actually know what the next words are for each of those representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3145" target="_blank">00:52:25.340</a></span> | <span class="t">So finally, we're going to talk about how you can just modify the training data to better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3149" target="_blank">00:52:29.180</a></span> | <span class="t">encode knowledge and language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3152" target="_blank">00:52:32.300</a></span> | <span class="t">So approaches we've talked about so far are actually incorporating knowledge explicitly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3156" target="_blank">00:52:36.980</a></span> | <span class="t">by using either pre-trained embeddings or an external memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3160" target="_blank">00:52:40.820</a></span> | <span class="t">We also want to talk about how can you just incorporate knowledge implicitly through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3164" target="_blank">00:52:44.820</a></span> | <span class="t">unstructured text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3168" target="_blank">00:52:48.300</a></span> | <span class="t">So what we're going to do is either mask or corrupt the data to introduce additional training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3171" target="_blank">00:52:51.940</a></span> | <span class="t">tasks that require factual knowledge to figure out what data was masked, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3176" target="_blank">00:52:56.860</a></span> | <span class="t">So this has some clear advantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3179" target="_blank">00:52:59.780</a></span> | <span class="t">It doesn't have any additional memory or computation requirements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3182" target="_blank">00:53:02.580</a></span> | <span class="t">You don't have a data store to deal with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3184" target="_blank">00:53:04.420</a></span> | <span class="t">You don't have extra knowledge encoder layers to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3186" target="_blank">00:53:06.980</a></span> | <span class="t">All you do is modify the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3188" target="_blank">00:53:08.580</a></span> | <span class="t">And you don't have to modify your architecture either.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3191" target="_blank">00:53:11.620</a></span> | <span class="t">So you can continue using your favorite BERT model and just make these changes to the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3195" target="_blank">00:53:15.980</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3198" target="_blank">00:53:18.580</a></span> | <span class="t">So the first work we're going to look at is called WKLM, Weekly Supervised Knowledge Pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3202" target="_blank">00:53:22.940</a></span> | <span class="t">Language Model, or Pre-trained Language Model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3205" target="_blank">00:53:25.620</a></span> | <span class="t">And the key idea here is to train the model to distinguish between true and false knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3211" target="_blank">00:53:31.300</a></span> | <span class="t">So they're going to corrupt the data by replacing mentions in the text with mentions that refer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3215" target="_blank">00:53:35.060</a></span> | <span class="t">to different entities of the same type to create what they refer to as negative knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3219" target="_blank">00:53:39.700</a></span> | <span class="t">statements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3220" target="_blank">00:53:40.700</a></span> | <span class="t">And then the model will just predict, has the entity been replaced or corrupted?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3227" target="_blank">00:53:47.700</a></span> | <span class="t">This type constraint is necessary to make sure that-- or to encourage the model to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3232" target="_blank">00:53:52.140</a></span> | <span class="t">use factual knowledge to figure out if this corruption is taking place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3234" target="_blank">00:53:54.940</a></span> | <span class="t">So you could imagine if you replace it with something that's not realistic at all, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3238" target="_blank">00:53:58.580</a></span> | <span class="t">model could just be basing its prediction based on, is this sentence linguistically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3242" target="_blank">00:54:02.420</a></span> | <span class="t">correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3244" target="_blank">00:54:04.700</a></span> | <span class="t">So as an example, we have a true knowledge statement as JK Rowling is the author of Harry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3249" target="_blank">00:54:09.900</a></span> | <span class="t">Potter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3250" target="_blank">00:54:10.900</a></span> | <span class="t">And then we want to modify this to replace it with another author.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3255" target="_blank">00:54:15.020</a></span> | <span class="t">So let's say we change this to J.R.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3256" target="_blank">00:54:16.740</a></span> | <span class="t">Tolkien is the author of Harry Potter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3259" target="_blank">00:54:19.820</a></span> | <span class="t">So you can see that this requires some amount of knowledge, background knowledge, to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3264" target="_blank">00:54:24.020</a></span> | <span class="t">be able to figure out which statement's true and which statement is false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3267" target="_blank">00:54:27.140</a></span> | <span class="t">And the idea is that the model will be able to predict for each of these mentions whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3271" target="_blank">00:54:31.580</a></span> | <span class="t">it's a true or false mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3276" target="_blank">00:54:36.900</a></span> | <span class="t">So this diagram here is from the paper and hopefully explains this a bit better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3280" target="_blank">00:54:40.380</a></span> | <span class="t">They have their original article on the left, and then they have their replaced article</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3283" target="_blank">00:54:43.580</a></span> | <span class="t">with the corruptions on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3285" target="_blank">00:54:45.260</a></span> | <span class="t">And the entities are in blue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3287" target="_blank">00:54:47.540</a></span> | <span class="t">So what they do is for a given entity, they first look up its type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3291" target="_blank">00:54:51.220</a></span> | <span class="t">They find other entities of that type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3293" target="_blank">00:54:53.820</a></span> | <span class="t">And then they randomly sample the entity and get an alias of it to replace in the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3299" target="_blank">00:54:59.420</a></span> | <span class="t">So they're going to replace Stan Lee, for instance, with Brian Johnson and Marvel Comics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3303" target="_blank">00:55:03.460</a></span> | <span class="t">with DC Comics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3304" target="_blank">00:55:04.940</a></span> | <span class="t">And their placements are in red on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3308" target="_blank">00:55:08.380</a></span> | <span class="t">And then the idea is that the model will be able to predict for each of these mentions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3312" target="_blank">00:55:12.100</a></span> | <span class="t">was it replaced or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3314" target="_blank">00:55:14.060</a></span> | <span class="t">So in the case of Brian Johnson, they have the red X for this is a false mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3318" target="_blank">00:55:18.300</a></span> | <span class="t">And in the case of the true mentions, they have the checkmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3322" target="_blank">00:55:22.420</a></span> | <span class="t">So it's a pretty simple approach, but they actually show that it can help the model increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3327" target="_blank">00:55:27.380</a></span> | <span class="t">the amount of knowledge that's encoded in its parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3336" target="_blank">00:55:36.380</a></span> | <span class="t">So WKLM uses an entity or placement loss to train the model to distinguish between these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3341" target="_blank">00:55:41.020</a></span> | <span class="t">true and false mentions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3342" target="_blank">00:55:42.640</a></span> | <span class="t">And this just looks like a binary classification loss where your true mentions are on the left</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3347" target="_blank">00:55:47.180</a></span> | <span class="t">and your false mentions are on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3349" target="_blank">00:55:49.520</a></span> | <span class="t">And you want to increase the probability that this P of E given C, so the probability of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3354" target="_blank">00:55:54.620</a></span> | <span class="t">the entity given the context, you want to increase that for the true mentions and decrease</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3358" target="_blank">00:55:58.500</a></span> | <span class="t">it for the false mentions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3361" target="_blank">00:56:01.540</a></span> | <span class="t">The total loss is then just a combination of the mass language model loss and this entity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3365" target="_blank">00:56:05.420</a></span> | <span class="t">replacement loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3368" target="_blank">00:56:08.140</a></span> | <span class="t">The mass language model loss is defined at the token level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3373" target="_blank">00:56:13.180</a></span> | <span class="t">And the entity replacement loss is defined at the entity level, meaning it's not just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3377" target="_blank">00:56:17.660</a></span> | <span class="t">over subwords.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3378" target="_blank">00:56:18.900</a></span> | <span class="t">It's even potentially over words if you have multi-word entities, phrases, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3385" target="_blank">00:56:25.220</a></span> | <span class="t">And this is an important point or an important theme that we really see occurring throughout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3389" target="_blank">00:56:29.720</a></span> | <span class="t">these works that we'll look at in that modifying the data at the entity level seems to be an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3394" target="_blank">00:56:34.580</a></span> | <span class="t">important component of actually increasing the amount of knowledge that a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3398" target="_blank">00:56:38.500</a></span> | <span class="t">can encode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3399" target="_blank">00:56:39.500</a></span> | <span class="t">So they find that WKLM improves over BERT and GPT-2, in fact, completion tasks like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3407" target="_blank">00:56:47.620</a></span> | <span class="t">the fill in the blank statements that we looked at at the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3410" target="_blank">00:56:50.840</a></span> | <span class="t">They also find that it improves over the Ernie paper that we talked about on a downstream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3414" target="_blank">00:56:54.860</a></span> | <span class="t">task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3415" target="_blank">00:56:55.860</a></span> | <span class="t">And they had a set of ablation experiments where they looked at, can you just remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3419" target="_blank">00:56:59.780</a></span> | <span class="t">this mass language model loss now?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3422" target="_blank">00:57:02.940</a></span> | <span class="t">And if you just train BERT for longer, do you really need this entity replacement loss?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3426" target="_blank">00:57:06.780</a></span> | <span class="t">So that's what the table here is looking at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3429" target="_blank">00:57:09.820</a></span> | <span class="t">The second row is looking at if we remove the mass language model loss, what happens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3434" target="_blank">00:57:14.260</a></span> | <span class="t">We see that it performs much worse without the mass language model loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3436" target="_blank">00:57:16.940</a></span> | <span class="t">So you really need both losses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3439" target="_blank">00:57:19.420</a></span> | <span class="t">Their intuition there was the mass language model loss helps to encode just general language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3444" target="_blank">00:57:24.660</a></span> | <span class="t">understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3446" target="_blank">00:57:26.940</a></span> | <span class="t">And then training BERT for longer performs much worse than using its entity replacement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3451" target="_blank">00:57:31.020</a></span> | <span class="t">loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3452" target="_blank">00:57:32.020</a></span> | <span class="t">So this motivates even farther that you really do need, or the entity replacement loss is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3456" target="_blank">00:57:36.700</a></span> | <span class="t">actually really helping encode more knowledge in these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3463" target="_blank">00:57:43.420</a></span> | <span class="t">So in addition to corrupting the data, we're also going to look at, can we just mask the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3467" target="_blank">00:57:47.060</a></span> | <span class="t">data differently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3468" target="_blank">00:57:48.060</a></span> | <span class="t">Can we be more clever about how we do the masking?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3470" target="_blank">00:57:50.820</a></span> | <span class="t">And this is a thread in several recent works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3473" target="_blank">00:57:53.540</a></span> | <span class="t">So there's actually another paper called Ernie.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3475" target="_blank">00:57:55.700</a></span> | <span class="t">So this is different than the one we talked about before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3477" target="_blank">00:57:57.920</a></span> | <span class="t">And this is enhanced representation through knowledge integration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3481" target="_blank">00:58:01.420</a></span> | <span class="t">And what they do is show improvements on downstream Chinese NLP tasks by doing phrase level and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3486" target="_blank">00:58:06.740</a></span> | <span class="t">entity level masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3488" target="_blank">00:58:08.580</a></span> | <span class="t">So instead of just masking out subwords, they're going to mask out phrases of multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3492" target="_blank">00:58:12.780</a></span> | <span class="t">words and entities, the full phrase of an entity, which corresponds to some entity in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3498" target="_blank">00:58:18.060</a></span> | <span class="t">a text that they might find with like NER techniques, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3503" target="_blank">00:58:23.720</a></span> | <span class="t">And then the second work is actually something you heard about in the last lecture, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3507" target="_blank">00:58:27.460</a></span> | <span class="t">is the idea of using salient span masking to mask out salient spans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3512" target="_blank">00:58:32.460</a></span> | <span class="t">And a salient span is just a named entity or a date.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3514" target="_blank">00:58:34.900</a></span> | <span class="t">So you can see this is pretty similar to what Ernie is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3518" target="_blank">00:58:38.280</a></span> | <span class="t">And they found that using salient span masking actually significantly helped T5 performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3523" target="_blank">00:58:43.180</a></span> | <span class="t">on these closed domain question answering tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3528" target="_blank">00:58:48.420</a></span> | <span class="t">So just to make sure we're all on the same page with the different masking techniques,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3532" target="_blank">00:58:52.020</a></span> | <span class="t">this diagram from the Ernie paper is comparing to what Bert does versus what Ernie does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3536" target="_blank">00:58:56.620</a></span> | <span class="t">The top shows that Ernie masked out the subword tokens or that Bert masked out the subword</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3541" target="_blank">00:59:01.060</a></span> | <span class="t">tokens, whereas Ernie masked out phrases like a series of, as well as entities like JK</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3547" target="_blank">00:59:07.300</a></span> | <span class="t">and T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3548" target="_blank">00:59:08.300</a></span> | <span class="t">There's some interesting results on showing that salient span masking is helping encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3555" target="_blank">00:59:15.800</a></span> | <span class="t">more knowledge in these representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3558" target="_blank">00:59:18.740</a></span> | <span class="t">So on the left, we're looking at the results of the original paper that proposed salient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3563" target="_blank">00:59:23.620</a></span> | <span class="t">span masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3564" target="_blank">00:59:24.880</a></span> | <span class="t">So this is the Realm work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3567" target="_blank">00:59:27.320</a></span> | <span class="t">And the idea here was that they were training a knowledge retriever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3570" target="_blank">00:59:30.760</a></span> | <span class="t">So it's actually more of an external memory class of techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3574" target="_blank">00:59:34.460</a></span> | <span class="t">But they find that by using the salient span masking technique, they could actually train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3579" target="_blank">00:59:39.040</a></span> | <span class="t">a much better knowledge retriever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3581" target="_blank">00:59:41.080</a></span> | <span class="t">So it's a good example of how these techniques are really complementary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3585" target="_blank">00:59:45.860</a></span> | <span class="t">So while I presented three classes of techniques, you can definitely get benefits by doing multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3589" target="_blank">00:59:49.960</a></span> | <span class="t">techniques together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3592" target="_blank">00:59:52.260</a></span> | <span class="t">And they found that doing salient span masking compared to using masking from Bert, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3596" target="_blank">00:59:56.320</a></span> | <span class="t">would be the random uniform masks, or doing random masking of spans from a paper called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3601" target="_blank">01:00:01.720</a></span> | <span class="t">SpanBert, it performs much better to do salient span masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3606" target="_blank">01:00:06.480</a></span> | <span class="t">So you see a 38 exact match score versus a 32 exact match score, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3613" target="_blank">01:00:13.760</a></span> | <span class="t">And on the right, we have results from fine tuning T5 with either salient span masking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3619" target="_blank">01:00:19.840</a></span> | <span class="t">or the span corruption task that you saw in assignment 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3623" target="_blank">01:00:23.080</a></span> | <span class="t">And you can see that on these different QA data sets, salient span masking does significantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3627" target="_blank">01:00:27.240</a></span> | <span class="t">better than just using the span corruption technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3631" target="_blank">01:00:31.920</a></span> | <span class="t">So this really suggests that doing the salient span masking and masking out these salient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3636" target="_blank">01:00:36.800</a></span> | <span class="t">spans of these entities is, in fact, helping to encode more knowledge in these language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3641" target="_blank">01:00:41.920</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3646" target="_blank">01:00:46.520</a></span> | <span class="t">So to recap, we talked about three different classes of techniques to add knowledge to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3649" target="_blank">01:00:49.760</a></span> | <span class="t">language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3651" target="_blank">01:00:51.940</a></span> | <span class="t">We talked about using pre-trained entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3654" target="_blank">01:00:54.360</a></span> | <span class="t">These weren't too difficult to apply to existing architectures, and as a way to leverage this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3658" target="_blank">01:00:58.680</a></span> | <span class="t">knowledge graph pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3661" target="_blank">01:01:01.080</a></span> | <span class="t">But it was a rather indirect way of incorporating knowledge, and it could be hard to interpret.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3666" target="_blank">01:01:06.360</a></span> | <span class="t">We also talked about approaches to add an external memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3670" target="_blank">01:01:10.120</a></span> | <span class="t">This could support modifying the knowledge base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3672" target="_blank">01:01:12.960</a></span> | <span class="t">It was also easier to interpret.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3675" target="_blank">01:01:15.520</a></span> | <span class="t">But they tended to be more complex in implementation, like we saw with KGLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3679" target="_blank">01:01:19.600</a></span> | <span class="t">And they also required more memory, like we saw with the KNNLM approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3684" target="_blank">01:01:24.720</a></span> | <span class="t">And then finally, we talked about modifying the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3688" target="_blank">01:01:28.040</a></span> | <span class="t">So this requires no model changes or additional computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3691" target="_blank">01:01:31.480</a></span> | <span class="t">It also might be the easiest to theoretically analyze.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3694" target="_blank">01:01:34.080</a></span> | <span class="t">So it's actually an active area research right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3697" target="_blank">01:01:37.680</a></span> | <span class="t">But still an open question if modifying the training data is always as effective as model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3701" target="_blank">01:01:41.880</a></span> | <span class="t">changes and what the trade-offs are in terms of the amount of data required versus doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3706" target="_blank">01:01:46.560</a></span> | <span class="t">one of these other knowledge enhancement approaches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3712" target="_blank">01:01:52.760</a></span> | <span class="t">So that leads us to section three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3715" target="_blank">01:01:55.200</a></span> | <span class="t">So I guess I'll pause again for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3718" target="_blank">01:01:58.240</a></span> | <span class="t">I think we may be good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3724" target="_blank">01:02:04.880</a></span> | <span class="t">Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3725" target="_blank">01:02:05.880</a></span> | <span class="t">OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3726" target="_blank">01:02:06.880</a></span> | <span class="t">So section three is about how researchers are actually going about evaluating the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3730" target="_blank">01:02:10.960</a></span> | <span class="t">and language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3732" target="_blank">01:02:12.680</a></span> | <span class="t">And I guess how some of the techniques we actually just talked about stand up in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3736" target="_blank">01:02:16.160</a></span> | <span class="t">evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3737" target="_blank">01:02:17.960</a></span> | <span class="t">So first, we're going to talk about probes, which don't require any fine-tuning of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3742" target="_blank">01:02:22.320</a></span> | <span class="t">language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3743" target="_blank">01:02:23.320</a></span> | <span class="t">And then we're going to talk about downstream tasks, which look at how well do these pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3747" target="_blank">01:02:27.320</a></span> | <span class="t">representations actually transfer their knowledge to other tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3752" target="_blank">01:02:32.800</a></span> | <span class="t">So one of the initial works in this area was called LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3755" target="_blank">01:02:35.800</a></span> | <span class="t">And this really started a series of works to look into how much knowledge is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3760" target="_blank">01:02:40.640</a></span> | <span class="t">encoded in these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3763" target="_blank">01:02:43.660</a></span> | <span class="t">So their question was, how much relational, common sense, and factual knowledge is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3767" target="_blank">01:02:47.720</a></span> | <span class="t">off-the-shelf language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3769" target="_blank">01:02:49.320</a></span> | <span class="t">So this is just taking pre-trained language models and evaluating the knowledge in them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3774" target="_blank">01:02:54.360</a></span> | <span class="t">And this is without any additional training or fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3777" target="_blank">01:02:57.740</a></span> | <span class="t">So they mainly constructed a set of what they refer to as closed statements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3781" target="_blank">01:03:01.080</a></span> | <span class="t">And these are just the fill-in-the-blank statements that we actually drew from at the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3784" target="_blank">01:03:04.740</a></span> | <span class="t">of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3785" target="_blank">01:03:05.740</a></span> | <span class="t">And I'll show you some more examples here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3790" target="_blank">01:03:10.900</a></span> | <span class="t">And they manually created these templates of closed statements using knowledge graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3794" target="_blank">01:03:14.300</a></span> | <span class="t">triples and question-answering pairs from existing data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3799" target="_blank">01:03:19.260</a></span> | <span class="t">They wanted to compare pre-trained language models to supervised relation extraction and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3803" target="_blank">01:03:23.820</a></span> | <span class="t">question-answering systems to see how do these language models that were trained in an unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3808" target="_blank">01:03:28.740</a></span> | <span class="t">fashion compare to these baseline systems that are not only supervised but really targeted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3813" target="_blank">01:03:33.940</a></span> | <span class="t">for this task of knowledge extraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3817" target="_blank">01:03:37.620</a></span> | <span class="t">And their goal was to evaluate the knowledge in existing pre-trained language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3821" target="_blank">01:03:41.860</a></span> | <span class="t">And a key point about this is they're just using the language models as they are available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3826" target="_blank">01:03:46.360</a></span> | <span class="t">to researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3827" target="_blank">01:03:47.600</a></span> | <span class="t">So this means there could be differences in the pre-trained corpora, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3831" target="_blank">01:03:51.520</a></span> | <span class="t">So when you look at the following table and you're comparing language models, also keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3834" target="_blank">01:03:54.540</a></span> | <span class="t">in mind that these don't account for the differences in the pre-trained corpora.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3840" target="_blank">01:04:00.860</a></span> | <span class="t">So a lot of these language models probably look familiar to you, either from previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3844" target="_blank">01:04:04.580</a></span> | <span class="t">lectures or maybe your final projects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3847" target="_blank">01:04:07.500</a></span> | <span class="t">And what we see is that overall, BERT-based and BERT-large pre-trained models are performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3852" target="_blank">01:04:12.580</a></span> | <span class="t">much better than the previous language or the other language models here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3856" target="_blank">01:04:16.900</a></span> | <span class="t">I guess I forgot to mention what mean precision at 1 is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3860" target="_blank">01:04:20.460</a></span> | <span class="t">This is a pretty simple metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3861" target="_blank">01:04:21.940</a></span> | <span class="t">The idea is if you look at the blank and you look at the top predictions for-- or the top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3866" target="_blank">01:04:26.100</a></span> | <span class="t">prediction for the blank, is it correct or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3868" target="_blank">01:04:28.140</a></span> | <span class="t">So that's what precision at 1 means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3870" target="_blank">01:04:30.180</a></span> | <span class="t">Precision at 10 would be let's look at the top 10 predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3873" target="_blank">01:04:33.220</a></span> | <span class="t">Is the correct prediction in the top 10?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3877" target="_blank">01:04:37.620</a></span> | <span class="t">So in addition to BERT-large and BERT-based performing well overall, we do see that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3883" target="_blank">01:04:43.020</a></span> | <span class="t">the T-Rex data set, the relation extraction baseline is performing a bit better than BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3888" target="_blank">01:04:48.820</a></span> | <span class="t">One thing to notice here that's pretty interesting is that this data set has a lot of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3892" target="_blank">01:04:52.940</a></span> | <span class="t">types of relations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3894" target="_blank">01:04:54.420</a></span> | <span class="t">And relations can be classified in terms of are they a one-to-one relation, are they an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3898" target="_blank">01:04:58.460</a></span> | <span class="t">end-to-one relation, are they an end-to-M relation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3902" target="_blank">01:05:02.060</a></span> | <span class="t">An example of a one-to-one relation would be your student ID relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3906" target="_blank">01:05:06.340</a></span> | <span class="t">So you have a unique student ID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3908" target="_blank">01:05:08.620</a></span> | <span class="t">An example of an end-to-M relation would be the enrolled-in relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3913" target="_blank">01:05:13.180</a></span> | <span class="t">So there's lots of students enrolled in lots of classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3915" target="_blank">01:05:15.620</a></span> | <span class="t">So this would be an end-to-M relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3917" target="_blank">01:05:17.740</a></span> | <span class="t">And they find that BERT really struggles on these end-to-M relations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3921" target="_blank">01:05:21.920</a></span> | <span class="t">So while it performs better than relation extraction baseline on some types of relations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3926" target="_blank">01:05:26.460</a></span> | <span class="t">overall it does pretty terribly on these end-to-M relations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3929" target="_blank">01:05:29.060</a></span> | <span class="t">So overall it does a bit worse than the baseline on this T-Rex data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3933" target="_blank">01:05:33.820</a></span> | <span class="t">They also compare to SQuAD on Docker QA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3936" target="_blank">01:05:36.860</a></span> | <span class="t">And they find that it does a fair amount worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3939" target="_blank">01:05:39.780</a></span> | <span class="t">They note that the language model is not fine-tuned here and also has no access to an information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3944" target="_blank">01:05:44.220</a></span> | <span class="t">retrieval system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3945" target="_blank">01:05:45.740</a></span> | <span class="t">And then when they look at the precision at 10, they find that this gap between Docker</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3949" target="_blank">01:05:49.300</a></span> | <span class="t">QA's performance and BERT actually closes quite a bit, which suggests that these language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3954" target="_blank">01:05:54.420</a></span> | <span class="t">models do have some amount of knowledge encoded in them and that they're even competitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3959" target="_blank">01:05:59.740</a></span> | <span class="t">with these knowledge extraction supervised baselines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3963" target="_blank">01:06:03.900</a></span> | <span class="t">So you can also try out examples on their GitHub repo for the llama probe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3970" target="_blank">01:06:10.700</a></span> | <span class="t">We have an example that was from their repo that was the cat is on the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3975" target="_blank">01:06:15.060</a></span> | <span class="t">You can see what the top 10 predictions are to fill in the closed statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3979" target="_blank">01:06:19.580</a></span> | <span class="t">Here they have the cat is on the phone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3982" target="_blank">01:06:22.540</a></span> | <span class="t">So this can be a fun way just to figure out what factual and common sense knowledge is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3986" target="_blank">01:06:26.860</a></span> | <span class="t">in existing language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3988" target="_blank">01:06:28.580</a></span> | <span class="t">And it's pretty easy to use with this interactive prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3993" target="_blank">01:06:33.620</a></span> | <span class="t">So some limitations of the llama probe are that it can be hard to understand why the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=3997" target="_blank">01:06:37.380</a></span> | <span class="t">models perform well when they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4000" target="_blank">01:06:40.480</a></span> | <span class="t">So for instance, BERT might just be predicting the most popular token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4003" target="_blank">01:06:43.740</a></span> | <span class="t">And this happens to be right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4004" target="_blank">01:06:44.740</a></span> | <span class="t">Maybe it's just memorizing co-occurrence patterns and doesn't really understand the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4009" target="_blank">01:06:49.980</a></span> | <span class="t">statement and doesn't understand what the fact is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4014" target="_blank">01:06:54.660</a></span> | <span class="t">It might also just be identifying similarities between surface forms of the subject and object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4019" target="_blank">01:06:59.500</a></span> | <span class="t">So for instance, in this example, Pope Clement VII has a position of blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4023" target="_blank">01:07:03.460</a></span> | <span class="t">Even if you don't know anything about Pope Clement VII, you might be able to figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4028" target="_blank">01:07:08.060</a></span> | <span class="t">that Pope is a likely next word for this triple or for this template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4035" target="_blank">01:07:15.220</a></span> | <span class="t">So the problem with this is if the model is just making these predictions based on these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4038" target="_blank">01:07:18.860</a></span> | <span class="t">surface forms or co-occurrence patterns, it's difficult to know if we're actually evaluating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4043" target="_blank">01:07:23.940</a></span> | <span class="t">the knowledge in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4045" target="_blank">01:07:25.260</a></span> | <span class="t">Maybe it's just making correct predictions for other reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4049" target="_blank">01:07:29.860</a></span> | <span class="t">And the more subtle issue that we've brought up is that language models might be just sensitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4053" target="_blank">01:07:33.460</a></span> | <span class="t">to the phrasing of the statement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4055" target="_blank">01:07:35.500</a></span> | <span class="t">So for each triple in their data set or for each relation in their data set, they just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4060" target="_blank">01:07:40.100</a></span> | <span class="t">had one mainly defined template.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4062" target="_blank">01:07:42.380</a></span> | <span class="t">And qualitatively, we found that if they just make small changes as template, it could actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4066" target="_blank">01:07:46.260</a></span> | <span class="t">change whether or not the model could recall the correct prediction or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4071" target="_blank">01:07:51.500</a></span> | <span class="t">And so this means that the probe results are really a lower bound on the knowledge that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4075" target="_blank">01:07:55.020</a></span> | <span class="t">encoded in the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4078" target="_blank">01:07:58.060</a></span> | <span class="t">So if you change the phrasing, it's possible that the model might show that it actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4081" target="_blank">01:08:01.580</a></span> | <span class="t">does have the knowledge encoded in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4084" target="_blank">01:08:04.620</a></span> | <span class="t">So the next lines of work we'll talk about are really building on these two limitations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4088" target="_blank">01:08:08.900</a></span> | <span class="t">of this original LAMA probe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4092" target="_blank">01:08:12.620</a></span> | <span class="t">So the first one is called LAMA-UN or LAMA Unhelpful Names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4096" target="_blank">01:08:16.340</a></span> | <span class="t">And the key idea is to remove these examples from LAMA that can be answered without the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4100" target="_blank">01:08:20.260</a></span> | <span class="t">relational knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4101" target="_blank">01:08:21.560</a></span> | <span class="t">So this is kind of addressing the first limitation on the last slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4105" target="_blank">01:08:25.700</a></span> | <span class="t">So they observed that BERT relies on just surface forms entities, might not be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4109" target="_blank">01:08:29.420</a></span> | <span class="t">knowledge to make these predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4111" target="_blank">01:08:31.480</a></span> | <span class="t">This includes a string match situation that we talked about with the pope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4115" target="_blank">01:08:35.620</a></span> | <span class="t">This also is dealing with the revealing person name issue that you saw in assignment five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4120" target="_blank">01:08:40.900</a></span> | <span class="t">So this is where the name could be an incorrect prior for the native language of someone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4125" target="_blank">01:08:45.020</a></span> | <span class="t">their place of birth, their nationality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4127" target="_blank">01:08:47.940</a></span> | <span class="t">They have this example from the paper where they look at different people names or person's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4132" target="_blank">01:08:52.980</a></span> | <span class="t">names and then they look at BERT's prediction for their native language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4136" target="_blank">01:08:56.460</a></span> | <span class="t">And these are all French speaking actors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4138" target="_blank">01:08:58.720</a></span> | <span class="t">And BERT just predicts very biased and stereotypical languages for these particular names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4144" target="_blank">01:09:04.820</a></span> | <span class="t">So this can really work both ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4146" target="_blank">01:09:06.460</a></span> | <span class="t">It can lead BERT to make incorrect predictions in some cases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4150" target="_blank">01:09:10.340</a></span> | <span class="t">But it could also work to make or to let BERT make correct predictions even if it has no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4154" target="_blank">01:09:14.500</a></span> | <span class="t">factual knowledge of those people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4156" target="_blank">01:09:16.460</a></span> | <span class="t">So that's the issue they're trying to get at here is do we know that BERT actually knows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4159" target="_blank">01:09:19.980</a></span> | <span class="t">this fact or is it just using some bias to make its prediction?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4164" target="_blank">01:09:24.660</a></span> | <span class="t">So what they do is they introduce a couple heuristics to basically just filter out these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4167" target="_blank">01:09:27.800</a></span> | <span class="t">examples from the LAMA probe that can either be solved by the string match setting or the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4172" target="_blank">01:09:32.900</a></span> | <span class="t">surveilling person name setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4175" target="_blank">01:09:35.340</a></span> | <span class="t">So they make a harder subset of the LAMA data set essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4179" target="_blank">01:09:39.660</a></span> | <span class="t">They find that when they test BERT on this harder subset that its performance drops about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4183" target="_blank">01:09:43.500</a></span> | <span class="t">8%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4184" target="_blank">01:09:44.500</a></span> | <span class="t">But when they test their knowledge enhanced model, which they call EBERT, the score only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4188" target="_blank">01:09:48.460</a></span> | <span class="t">drops about 1%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4189" target="_blank">01:09:49.460</a></span> | <span class="t">So it's possible that as you make harder knowledge probes, we'll actually see even bigger differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4194" target="_blank">01:09:54.860</a></span> | <span class="t">in the performance of knowledge enhanced models to models without these knowledge enhancements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4202" target="_blank">01:10:02.940</a></span> | <span class="t">The next piece of work we'll talk about is actually getting at this issue of the phrasing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4208" target="_blank">01:10:08.980</a></span> | <span class="t">of the prompt might actually trigger different responses from the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4214" target="_blank">01:10:14.060</a></span> | <span class="t">So the language model might know the fact, but it might fail on the task due to the phrasing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4219" target="_blank">01:10:19.460</a></span> | <span class="t">One reason this might happen is the pre-training is on different contexts and sentence structures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4223" target="_blank">01:10:23.260</a></span> | <span class="t">in the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4224" target="_blank">01:10:24.260</a></span> | <span class="t">So for example, you might have in your pre-training corpus, the birthplace of Barack Obama is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4228" target="_blank">01:10:28.940</a></span> | <span class="t">Honolulu, Hawaii.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4230" target="_blank">01:10:30.380</a></span> | <span class="t">And this might be something you see in Wikipedia, for instance, that's a common training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4233" target="_blank">01:10:33.380</a></span> | <span class="t">set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4234" target="_blank">01:10:34.380</a></span> | <span class="t">And then as a researcher, you write Barack Obama was born in blank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4238" target="_blank">01:10:38.340</a></span> | <span class="t">And you can see that these sentence structures are pretty different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4240" target="_blank">01:10:40.900</a></span> | <span class="t">So the model might've seen the first fact, but the sentence structure difference is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4244" target="_blank">01:10:44.700</a></span> | <span class="t">enough to confuse it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4246" target="_blank">01:10:46.260</a></span> | <span class="t">So it can't answer this query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4249" target="_blank">01:10:49.500</a></span> | <span class="t">So what they do is they generate a lot more of these prompts by mining templates from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4253" target="_blank">01:10:53.140</a></span> | <span class="t">Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4254" target="_blank">01:10:54.140</a></span> | <span class="t">One of the techniques actually uses dependency parsing and also generating paraphrase prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4258" target="_blank">01:10:58.900</a></span> | <span class="t">by taking inspiration from the machine translation literature and using back translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4265" target="_blank">01:11:05.180</a></span> | <span class="t">So they generate a lot more prompts to try to query the language models and figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4268" target="_blank">01:11:08.980</a></span> | <span class="t">do small variations in the prompt trigger the correct prediction from the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4274" target="_blank">01:11:14.860</a></span> | <span class="t">They also experiment with ensembling prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4276" target="_blank">01:11:16.860</a></span> | <span class="t">So if we give the model multiple prompts and then take some probability averaged over these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4281" target="_blank">01:11:21.380</a></span> | <span class="t">different prompts, can we improve the performance on the model returning the correct prediction?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4286" target="_blank">01:11:26.740</a></span> | <span class="t">So we give it a higher chance of seeing a context that it might've actually seen during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4290" target="_blank">01:11:30.020</a></span> | <span class="t">pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4291" target="_blank">01:11:31.020</a></span> | <span class="t">They find that the performance on LLAMA increases when they either use a top performing prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4297" target="_blank">01:11:37.580</a></span> | <span class="t">or when they use this ensembling approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4299" target="_blank">01:11:39.940</a></span> | <span class="t">So this suggests that the original LLAMA really was a lower bound on the amount of knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4303" target="_blank">01:11:43.420</a></span> | <span class="t">encoded in these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4305" target="_blank">01:11:45.900</a></span> | <span class="t">And changing the phrasing can actually help the model recall the correct answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4312" target="_blank">01:11:52.980</a></span> | <span class="t">This table is a bit frightening, but they find that small changes in the query can lead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4316" target="_blank">01:11:56.480</a></span> | <span class="t">to really large gains on performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4318" target="_blank">01:11:58.900</a></span> | <span class="t">So if you just have a query like X plays in Y position, and then you change that to X</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4323" target="_blank">01:12:03.700</a></span> | <span class="t">plays at Y position, this can actually lead to like a 23% accuracy gain on this particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4328" target="_blank">01:12:08.340</a></span> | <span class="t">relation in terms of the model actually being able to recall the correct answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4333" target="_blank">01:12:13.540</a></span> | <span class="t">Or even just X was created in Y to X is created in Y, 10% accuracy gain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4339" target="_blank">01:12:19.740</a></span> | <span class="t">So I think this motivates the need to not only develop better ways to query these models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4343" target="_blank">01:12:23.820</a></span> | <span class="t">but probably also build language models that are actually more robust to the query itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4348" target="_blank">01:12:28.420</a></span> | <span class="t">So in addition to probes, another way to evaluate these language models is by looking at how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4356" target="_blank">01:12:36.180</a></span> | <span class="t">well they transfer from the pre-trained representation to downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4362" target="_blank">01:12:42.380</a></span> | <span class="t">And so the idea here is you're actually going to fine tune the pre-trained representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4365" target="_blank">01:12:45.540</a></span> | <span class="t">on different downstream tasks, similar to how you would evaluate BERT on glue tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4371" target="_blank">01:12:51.700</a></span> | <span class="t">Some common tasks that are used for this are relation extraction, entity typing, and question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4376" target="_blank">01:12:56.940</a></span> | <span class="t">answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4377" target="_blank">01:12:57.940</a></span> | <span class="t">So relation extraction is where you want to predict the relation between two entities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4381" target="_blank">01:13:01.780</a></span> | <span class="t">So this is getting back at one of the questions earlier in the talk in terms of, well, how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4385" target="_blank">01:13:05.300</a></span> | <span class="t">do you get the relation that's the edges in these knowledge bases?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4388" target="_blank">01:13:08.340</a></span> | <span class="t">So given two entities, you learn a model to predict what is the relation between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4393" target="_blank">01:13:13.420</a></span> | <span class="t">Entity typing is a task of given an entity, what is the type of the entity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4396" target="_blank">01:13:16.780</a></span> | <span class="t">So here, Alice robbed the bank.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4397" target="_blank">01:13:17.780</a></span> | <span class="t">You want to predict her as a criminal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4400" target="_blank">01:13:20.100</a></span> | <span class="t">And then you guys are very familiar with question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4403" target="_blank">01:13:23.580</a></span> | <span class="t">So the idea of these tasks is that they're knowledge intensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4407" target="_blank">01:13:27.660</a></span> | <span class="t">So they're good candidates to see how well do these pre-trained representations actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4411" target="_blank">01:13:31.340</a></span> | <span class="t">transfer the knowledge to these downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4416" target="_blank">01:13:36.580</a></span> | <span class="t">Here we're looking at the performance on a relation extraction benchmark called TACRID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4420" target="_blank">01:13:40.740</a></span> | <span class="t">And all the models that we show here were at one point state-of-the-art on TACRID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4425" target="_blank">01:13:45.340</a></span> | <span class="t">So this CGCN is a graph convolutional neural network over dependency trees.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4430" target="_blank">01:13:50.740</a></span> | <span class="t">The BERT LSTM base is one of the first works that showed that you could actually get state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4436" target="_blank">01:13:56.020</a></span> | <span class="t">performance with BERT on relation extraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4438" target="_blank">01:13:58.060</a></span> | <span class="t">And this is just putting an LSTM layer over BERT's output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4441" target="_blank">01:14:01.860</a></span> | <span class="t">Ernie is a work that we talked about with the pre-trained entity embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4444" target="_blank">01:14:04.740</a></span> | <span class="t">Matching the blanks we didn't get to today, but it's a really interesting work about learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4449" target="_blank">01:14:09.180</a></span> | <span class="t">meaningful relation representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4451" target="_blank">01:14:11.540</a></span> | <span class="t">And it falls more into the training data modification approaches and that they are actually masking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4456" target="_blank">01:14:16.780</a></span> | <span class="t">out entities again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4459" target="_blank">01:14:19.200</a></span> | <span class="t">And then NoBERT is what we talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4462" target="_blank">01:14:22.180</a></span> | <span class="t">The W in W here means they actually encode two knowledge bases in NoBERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4466" target="_blank">01:14:26.140</a></span> | <span class="t">So they're encoding WordNet and they're also encoding Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4470" target="_blank">01:14:30.380</a></span> | <span class="t">And the high-level takeaway from this table is that you can see that the recent knowledge-enhanced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4474" target="_blank">01:14:34.300</a></span> | <span class="t">models have achieved state-of-the-art over the original models that once performed very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4479" target="_blank">01:14:39.300</a></span> | <span class="t">well on TACRID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4480" target="_blank">01:14:40.300</a></span> | <span class="t">And we have about 5 F1 gains here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4484" target="_blank">01:14:44.020</a></span> | <span class="t">Another interesting takeaway from this table is there seems to be a trade-off in the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4487" target="_blank">01:14:47.380</a></span> | <span class="t">of the language model that's necessary to get a certain performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4490" target="_blank">01:14:50.980</a></span> | <span class="t">So if you just consider the size of the language model, then NoBERT performs the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4495" target="_blank">01:14:55.340</a></span> | <span class="t">But if you don't consider that, then it ties with matching the blanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4500" target="_blank">01:15:00.900</a></span> | <span class="t">So overall, this is pretty good evidence that these knowledge-enhanced methods are in fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4505" target="_blank">01:15:05.380</a></span> | <span class="t">transferring to these knowledge-intensive downstream tasks that can really take advantage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4509" target="_blank">01:15:09.980</a></span> | <span class="t">of these pre-trained representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4514" target="_blank">01:15:14.220</a></span> | <span class="t">We also have results on entity typing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4516" target="_blank">01:15:16.180</a></span> | <span class="t">So here we're comparing a slightly different set of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4518" target="_blank">01:15:18.900</a></span> | <span class="t">Some of the baselines are LSTM models that were designed for entity typing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4523" target="_blank">01:15:23.180</a></span> | <span class="t">And we have Ernie and NoBERT leading the, I guess, leaderboard here on the entity typing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4529" target="_blank">01:15:29.060</a></span> | <span class="t">task of OpenEntity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4530" target="_blank">01:15:30.820</a></span> | <span class="t">And we see gains of about 15 F1 points with Ernie and NoBERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4534" target="_blank">01:15:34.660</a></span> | <span class="t">So once again, we really do see that these knowledge-rich pre-trained representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4539" target="_blank">01:15:39.020</a></span> | <span class="t">are transferring and helping on these knowledge-intensive downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4545" target="_blank">01:15:45.980</a></span> | <span class="t">So just to recap, we talked about probes which evaluate the knowledge already present in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4550" target="_blank">01:15:50.140</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4551" target="_blank">01:15:51.140</a></span> | <span class="t">These don't require any more training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4552" target="_blank">01:15:52.900</a></span> | <span class="t">But it can be challenging to construct benchmarks to actually make sure you're testing the knowledge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4557" target="_blank">01:15:57.340</a></span> | <span class="t">in these language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4558" target="_blank">01:15:58.340</a></span> | <span class="t">It can also be challenging to construct the queries used in the probe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4563" target="_blank">01:16:03.100</a></span> | <span class="t">We then talked about downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4565" target="_blank">01:16:05.380</a></span> | <span class="t">These are a bit of an indirect way to evaluate knowledge in that they have this extra component</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4568" target="_blank">01:16:08.580</a></span> | <span class="t">of fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4569" target="_blank">01:16:09.580</a></span> | <span class="t">But it's a good way to evaluate how useful is this knowledge-rich pre-trained representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4574" target="_blank">01:16:14.500</a></span> | <span class="t">in actual applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4578" target="_blank">01:16:18.980</a></span> | <span class="t">So I just touched on the exciting work in this area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4582" target="_blank">01:16:22.300</a></span> | <span class="t">But there's many other directions if you want to dive more into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4585" target="_blank">01:16:25.800</a></span> | <span class="t">So there's retrieval-augmented language models, which learn knowledge retrievers to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4590" target="_blank">01:16:30.180</a></span> | <span class="t">out what documents might be relevant for predicting the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4594" target="_blank">01:16:34.020</a></span> | <span class="t">There's work in modifying the knowledge in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4596" target="_blank">01:16:36.980</a></span> | <span class="t">So I talked about how this is one of the obstacles and challenges to using language models as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4601" target="_blank">01:16:41.540</a></span> | <span class="t">knowledge bases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4602" target="_blank">01:16:42.540</a></span> | <span class="t">So there's been recent work in this area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4605" target="_blank">01:16:45.300</a></span> | <span class="t">We also saw how important the knowledge pre-training task was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4608" target="_blank">01:16:48.900</a></span> | <span class="t">Well, there's many papers that are proposing different tasks to do the knowledge pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4613" target="_blank">01:16:53.420</a></span> | <span class="t">So it's still an open question in terms of what tasks are best to add to encode more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4618" target="_blank">01:16:58.260</a></span> | <span class="t">knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4619" target="_blank">01:16:59.260</a></span> | <span class="t">There's also been work on more efficient knowledge systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4622" target="_blank">01:17:02.340</a></span> | <span class="t">So at NERPS, there's now an efficient QA challenge, which aims at building the smallest QA system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4627" target="_blank">01:17:07.100</a></span> | <span class="t">And then finally, there's been work on building better knowledge benchmarks that build on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4632" target="_blank">01:17:12.100</a></span> | <span class="t">the benchmarks that we saw today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4636" target="_blank">01:17:16.140</a></span> | <span class="t">So that's all I have for today, and I hope your final projects are going well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4639" target="_blank">01:17:19.260</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4640" target="_blank">01:17:20.260</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=y68RJVfGoto&t=4640" target="_blank">01:17:20.260</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>