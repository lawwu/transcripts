<html><head><title>Stanford XCS224U: NLU I Analysis Methods for NLU, Part 3: Feature Attribution I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Analysis Methods for NLU, Part 3: Feature Attribution I Spring 2023</h2><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc"><img src="https://i.ytimg.com/vi/p0dzR6iaFmc/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGFcgYyhlMA8=&rs=AOn4CLBXdX_kyWDo7ZkJ1KcCaxTwo-itzQ" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=214">3:34</a> Attributions wrt predicted vs. actual labels<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=322">5:22</a> Gradients inputs fails sensitivity<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=392">6:32</a> Integrated gradients: Intuition<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=447">7:27</a> Integrated gradients: Core computation<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=509">8:29</a> Sensitivity again<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=562">9:22</a> BERT example<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=731">12:11</a> A small challenge test<br><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=805">13:25</a> Summary<br><br><div style="text-align: left;"><a href="./p0dzR6iaFmc.html">Whisper Transcript</a> | <a href="./transcript_p0dzR6iaFmc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=6" target="_blank">00:00:06.200</a></span> | <span class="t">This is part three in our series on analysis methods for NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=9" target="_blank">00:00:09.920</a></span> | <span class="t">We're going to be focused on feature attribution methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=13" target="_blank">00:00:13.260</a></span> | <span class="t">I should say at the start that to keep things manageable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=16" target="_blank">00:00:16.400</a></span> | <span class="t">we're going to mainly focus on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=18" target="_blank">00:00:18.080</a></span> | <span class="t">integrated gradients from Sundararajan et al 2017.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=21" target="_blank">00:00:21.600</a></span> | <span class="t">This is a shining,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=23" target="_blank">00:00:23.240</a></span> | <span class="t">powerful, inspiring example of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=25" target="_blank">00:00:25.360</a></span> | <span class="t">an attribution method for reasons I will discuss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=28" target="_blank">00:00:28.340</a></span> | <span class="t">But it's by no means the only method in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=31" target="_blank">00:00:31.680</a></span> | <span class="t">For one-stop shopping on these methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=34" target="_blank">00:00:34.120</a></span> | <span class="t">I recommend the captum.ai library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=36" target="_blank">00:00:36.680</a></span> | <span class="t">It will give you access to lots of gradient-based methods like IG,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=41" target="_blank">00:00:41.280</a></span> | <span class="t">as well as many others,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=42" target="_blank">00:00:42.740</a></span> | <span class="t">including more traditional methods like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=44" target="_blank">00:00:44.900</a></span> | <span class="t">feature ablation and feature permutation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=47" target="_blank">00:00:47.920</a></span> | <span class="t">Check out captum.ai.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=49" target="_blank">00:00:49.660</a></span> | <span class="t">In addition, if you would like a deeper dive on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=52" target="_blank">00:00:52.460</a></span> | <span class="t">the calculations and examples that I use in this screencast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=55" target="_blank">00:00:55.960</a></span> | <span class="t">I recommend the notebook feature attribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=58" target="_blank">00:00:58.340</a></span> | <span class="t">which is part of the course code repository.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=61" target="_blank">00:01:01.900</a></span> | <span class="t">Now, I love the integrated gradients paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=66" target="_blank">00:01:06.140</a></span> | <span class="t">Sundararajan et al 2017,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=68" target="_blank">00:01:08.220</a></span> | <span class="t">because of its method,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=69" target="_blank">00:01:09.620</a></span> | <span class="t">but also because it offers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=71" target="_blank">00:01:11.580</a></span> | <span class="t">a really nice framework for thinking about attribution in general,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=75" target="_blank">00:01:15.820</a></span> | <span class="t">and they do that in the form of three axioms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=78" target="_blank">00:01:18.460</a></span> | <span class="t">I'm going to talk about two of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=80" target="_blank">00:01:20.340</a></span> | <span class="t">Of the two, the most important one is sensitivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=83" target="_blank">00:01:23.660</a></span> | <span class="t">This is very intuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=85" target="_blank">00:01:25.420</a></span> | <span class="t">The axiom of sensitivity for attribution methods says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=88" target="_blank">00:01:28.680</a></span> | <span class="t">if two inputs, x and x prime differ only at dimension i,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=92" target="_blank">00:01:32.980</a></span> | <span class="t">and lead to different predictions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=95" target="_blank">00:01:35.500</a></span> | <span class="t">then the feature associated with dimension i has non-zero attribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=101" target="_blank">00:01:41.380</a></span> | <span class="t">Here's a quick example. Our model is M,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=104" target="_blank">00:01:44.460</a></span> | <span class="t">and it takes inputs that are three-dimensional,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=107" target="_blank">00:01:47.180</a></span> | <span class="t">and for input 1, 0, 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=109" target="_blank">00:01:49.220</a></span> | <span class="t">this model outputs positive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=111" target="_blank">00:01:51.100</a></span> | <span class="t">and for 1, 1, 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=112" target="_blank">00:01:52.980</a></span> | <span class="t">it outputs negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=114" target="_blank">00:01:54.580</a></span> | <span class="t">That's a difference in the predictions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=116" target="_blank">00:01:56.900</a></span> | <span class="t">and that means that the feature in position 2 here must have non-zero attribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=122" target="_blank">00:02:02.980</a></span> | <span class="t">Seems very intuitive because obviously this feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=126" target="_blank">00:02:06.020</a></span> | <span class="t">is important to the behavior of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=128" target="_blank">00:02:08.740</a></span> | <span class="t">Just quickly, I'll mention a second axiom, implementation invariance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=132" target="_blank">00:02:12.780</a></span> | <span class="t">If two models, M and M prime,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=134" target="_blank">00:02:14.660</a></span> | <span class="t">have identical input-output behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=137" target="_blank">00:02:17.260</a></span> | <span class="t">then the attributions for M and M prime are identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=140" target="_blank">00:02:20.660</a></span> | <span class="t">That's very intuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=141" target="_blank">00:02:21.820</a></span> | <span class="t">If the models can't be distinguished behaviorally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=144" target="_blank">00:02:24.500</a></span> | <span class="t">then we should give them identical attributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=147" target="_blank">00:02:27.060</a></span> | <span class="t">We should not be sensitive to incidental details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=150" target="_blank">00:02:30.420</a></span> | <span class="t">of how they were structured or how they were implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=154" target="_blank">00:02:34.220</a></span> | <span class="t">Let's begin with a baseline method, gradients by inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=159" target="_blank">00:02:39.260</a></span> | <span class="t">This is very intuitive and makes some sense from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=162" target="_blank">00:02:42.300</a></span> | <span class="t">the perspective of doing feature attribution in deep networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=166" target="_blank">00:02:46.220</a></span> | <span class="t">What we're going to do is calculate the gradients for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=168" target="_blank">00:02:48.660</a></span> | <span class="t">our model with respect to the chosen feature that we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=171" target="_blank">00:02:51.820</a></span> | <span class="t">target and multiply that value by the actual value of the feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=176" target="_blank">00:02:56.460</a></span> | <span class="t">Gradients by inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=178" target="_blank">00:02:58.300</a></span> | <span class="t">It's called gradients by inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=180" target="_blank">00:03:00.060</a></span> | <span class="t">but obviously we could do this gradient taking with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=183" target="_blank">00:03:03.540</a></span> | <span class="t">any neuron in one of these deep learning models and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=186" target="_blank">00:03:06.300</a></span> | <span class="t">multiply it by the actual value of that neuron for some example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=189" target="_blank">00:03:09.940</a></span> | <span class="t">Actually, this method generalizes really nicely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=192" target="_blank">00:03:12.860</a></span> | <span class="t">to any state in a deep learning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=195" target="_blank">00:03:15.840</a></span> | <span class="t">It's really straightforward to implement that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=198" target="_blank">00:03:18.060</a></span> | <span class="t">I've depicted that on the slide here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=199" target="_blank">00:03:19.740</a></span> | <span class="t">The first implementation uses raw PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=203" target="_blank">00:03:23.380</a></span> | <span class="t">and the second one is just a lightweight wrapper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=206" target="_blank">00:03:26.100</a></span> | <span class="t">around the CAPTEM implementation of input by gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=210" target="_blank">00:03:30.340</a></span> | <span class="t">Shows you how straightforward this can be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=213" target="_blank">00:03:33.380</a></span> | <span class="t">One issue that I want to linger over here that I find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=216" target="_blank">00:03:36.900</a></span> | <span class="t">conceptually difficult is this question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=219" target="_blank">00:03:39.700</a></span> | <span class="t">of how we should do the attributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=222" target="_blank">00:03:42.060</a></span> | <span class="t">For classifier models, we have a choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=224" target="_blank">00:03:44.380</a></span> | <span class="t">We can take attributions with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=226" target="_blank">00:03:46.340</a></span> | <span class="t">the predicted labels or with respect to the actual labels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=229" target="_blank">00:03:49.900</a></span> | <span class="t">which are two different dimensions in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=232" target="_blank">00:03:52.220</a></span> | <span class="t">the output vector for these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=235" target="_blank">00:03:55.220</a></span> | <span class="t">Now, if the model you're studying is very high-performing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=239" target="_blank">00:03:59.260</a></span> | <span class="t">then the predicted and actual labels will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=241" target="_blank">00:04:01.420</a></span> | <span class="t">almost identical and this is unlikely to matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=244" target="_blank">00:04:04.140</a></span> | <span class="t">But you might be trying to study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=246" target="_blank">00:04:06.260</a></span> | <span class="t">a really poor performing model to try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=248" target="_blank">00:04:08.420</a></span> | <span class="t">understand where its deficiencies lie,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=251" target="_blank">00:04:11.100</a></span> | <span class="t">and that's precisely the case where these two will come apart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=254" target="_blank">00:04:14.280</a></span> | <span class="t">As an illustration on this slide here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=256" target="_blank">00:04:16.220</a></span> | <span class="t">I've defined a simple make classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=258" target="_blank">00:04:18.580</a></span> | <span class="t">synthetic problem using scikit-learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=260" target="_blank">00:04:20.900</a></span> | <span class="t">It has four features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=262" target="_blank">00:04:22.700</a></span> | <span class="t">Then I set up a shallow neural classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=265" target="_blank">00:04:25.260</a></span> | <span class="t">and I deliberately under-trained it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=267" target="_blank">00:04:27.180</a></span> | <span class="t">It has just one training iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=269" target="_blank">00:04:29.540</a></span> | <span class="t">This is a very bad model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=271" target="_blank">00:04:31.940</a></span> | <span class="t">If I do attributions with respect to the true labels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=275" target="_blank">00:04:35.900</a></span> | <span class="t">I get one vector of attribution scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=279" target="_blank">00:04:39.300</a></span> | <span class="t">If I do attributions with respect to the predicted labels,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=282" target="_blank">00:04:42.980</a></span> | <span class="t">I get a totally different set of attribution scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=286" target="_blank">00:04:46.980</a></span> | <span class="t">That confronts you with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=289" target="_blank">00:04:49.340</a></span> | <span class="t">a difficult conceptual question of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=291" target="_blank">00:04:51.260</a></span> | <span class="t">which ones you want to use to guide your analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=294" target="_blank">00:04:54.260</a></span> | <span class="t">They're giving us different pictures of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=297" target="_blank">00:04:57.580</a></span> | <span class="t">I think that there is no a priori reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=300" target="_blank">00:05:00.220</a></span> | <span class="t">to favor one over the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=301" target="_blank">00:05:01.860</a></span> | <span class="t">I think it really comes down to what you're trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=304" target="_blank">00:05:04.740</a></span> | <span class="t">accomplish with the analysis that you're constructing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=307" target="_blank">00:05:07.740</a></span> | <span class="t">The best answer I can give is to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=310" target="_blank">00:05:10.220</a></span> | <span class="t">explicit about your assumptions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=312" target="_blank">00:05:12.700</a></span> | <span class="t">and about the methods that you used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=314" target="_blank">00:05:14.820</a></span> | <span class="t">This issue, by the way, will carry forward through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=317" target="_blank">00:05:17.300</a></span> | <span class="t">all of these gradient-based methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=319" target="_blank">00:05:19.100</a></span> | <span class="t">not just inputs by gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=322" target="_blank">00:05:22.420</a></span> | <span class="t">Here's the fundamental sticking point for gradients by inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=327" target="_blank">00:05:27.220</a></span> | <span class="t">It fails that sensitivity axiom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=329" target="_blank">00:05:29.580</a></span> | <span class="t">This is an example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=330" target="_blank">00:05:30.860</a></span> | <span class="t">a counterexample to sensitivity that comes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=332" target="_blank">00:05:32.940</a></span> | <span class="t">directly from Sundararajan et al, 2017.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=336" target="_blank">00:05:36.100</a></span> | <span class="t">We have a very simple model here, M.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=338" target="_blank">00:05:38.660</a></span> | <span class="t">It takes one-dimensional inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=341" target="_blank">00:05:41.060</a></span> | <span class="t">and what it does is one minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=343" target="_blank">00:05:43.500</a></span> | <span class="t">the ReLU activation applied to one minus the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=347" target="_blank">00:05:47.060</a></span> | <span class="t">Very simple model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=348" target="_blank">00:05:48.780</a></span> | <span class="t">When we use the model with input 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=351" target="_blank">00:05:51.500</a></span> | <span class="t">we get 0 as the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=353" target="_blank">00:05:53.620</a></span> | <span class="t">When we use the model with input 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=356" target="_blank">00:05:56.140</a></span> | <span class="t">we get 1 as the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=357" target="_blank">00:05:57.820</a></span> | <span class="t">We have a difference in output predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=360" target="_blank">00:06:00.940</a></span> | <span class="t">These are one-dimensional inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=362" target="_blank">00:06:02.780</a></span> | <span class="t">so we are now required by sensitivity to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=365" target="_blank">00:06:05.380</a></span> | <span class="t">non-zero attribution to this feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=368" target="_blank">00:06:08.940</a></span> | <span class="t">But sadly, we do not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=370" target="_blank">00:06:10.740</a></span> | <span class="t">When you run input by gradients on this input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=373" target="_blank">00:06:13.220</a></span> | <span class="t">you get 0, and when you run input by gradients on input 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=377" target="_blank">00:06:17.260</a></span> | <span class="t">you also get 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=378" target="_blank">00:06:18.580</a></span> | <span class="t">and that is just a direct failure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=380" target="_blank">00:06:20.580</a></span> | <span class="t">to meet the sensitivity requirement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=383" target="_blank">00:06:23.020</a></span> | <span class="t">That's a worrisome thing about this baseline method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=386" target="_blank">00:06:26.540</a></span> | <span class="t">It queues us up nicely to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=388" target="_blank">00:06:28.740</a></span> | <span class="t">how integrated gradients will do better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=391" target="_blank">00:06:31.740</a></span> | <span class="t">The intuition behind integrated gradients is that we are going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=396" target="_blank">00:06:36.380</a></span> | <span class="t">to explore counterfactual versions of our input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=399" target="_blank">00:06:39.780</a></span> | <span class="t">and I think that is an important insight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=402" target="_blank">00:06:42.060</a></span> | <span class="t">As we try to get causal insights into model behavior,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=405" target="_blank">00:06:45.700</a></span> | <span class="t">it becomes ever more essential to think about counterfactuals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=409" target="_blank">00:06:49.780</a></span> | <span class="t">Here's the way IG does this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=412" target="_blank">00:06:52.180</a></span> | <span class="t">We have two features in our space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=414" target="_blank">00:06:54.340</a></span> | <span class="t">X_1 and X_2, and this blue dot represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=417" target="_blank">00:06:57.020</a></span> | <span class="t">the example that we would like to do attributions for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=420" target="_blank">00:07:00.700</a></span> | <span class="t">With integrated gradients, the first thing we do is set up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=423" target="_blank">00:07:03.380</a></span> | <span class="t">a baseline and a standard baseline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=425" target="_blank">00:07:05.780</a></span> | <span class="t">for this would be the zero vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=428" target="_blank">00:07:08.140</a></span> | <span class="t">Then we are going to create synthetic examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=431" target="_blank">00:07:11.180</a></span> | <span class="t">interpolated between the baseline and our actual example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=436" target="_blank">00:07:16.060</a></span> | <span class="t">We are going to study the gradients with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=438" target="_blank">00:07:18.460</a></span> | <span class="t">every single one of these interpolated examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=441" target="_blank">00:07:21.820</a></span> | <span class="t">aggregate them together, and use all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=444" target="_blank">00:07:24.100</a></span> | <span class="t">that information to do our attributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=447" target="_blank">00:07:27.500</a></span> | <span class="t">Here's a look at the IG calculation in some detail as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=451" target="_blank">00:07:31.500</a></span> | <span class="t">might implement it for an actual piece of software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=455" target="_blank">00:07:35.500</a></span> | <span class="t">Let's break this down into some pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=457" target="_blank">00:07:37.380</a></span> | <span class="t">Step 1, we have this vector alpha,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=460" target="_blank">00:07:40.180</a></span> | <span class="t">and this is going to determine the number of steps that we use to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=463" target="_blank">00:07:43.420</a></span> | <span class="t">get different synthetic inputs between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=465" target="_blank">00:07:45.460</a></span> | <span class="t">baseline and the actual example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=468" target="_blank">00:07:48.500</a></span> | <span class="t">We're going to interpolate these inputs between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=471" target="_blank">00:07:51.260</a></span> | <span class="t">the baseline and the actual example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=473" target="_blank">00:07:53.300</a></span> | <span class="t">That's what happens in purple here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=475" target="_blank">00:07:55.380</a></span> | <span class="t">according to these alpha steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=477" target="_blank">00:07:57.820</a></span> | <span class="t">Then we compute the gradients for each interpolated input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=481" target="_blank">00:08:01.220</a></span> | <span class="t">and that part of the calculation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=482" target="_blank">00:08:02.780</a></span> | <span class="t">the individual ones, looks exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=484" target="_blank">00:08:04.820</a></span> | <span class="t">like inputs by gradients, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=487" target="_blank">00:08:07.620</a></span> | <span class="t">Next step, we do an integral approximation through the averaging,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=491" target="_blank">00:08:11.740</a></span> | <span class="t">that's the summation that you see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=493" target="_blank">00:08:13.380</a></span> | <span class="t">We're going to sum over all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=494" target="_blank">00:08:14.980</a></span> | <span class="t">these examples that we've taken and created the gradients for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=499" target="_blank">00:08:19.060</a></span> | <span class="t">Then finally, we do some scaling to remain in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=501" target="_blank">00:08:21.620</a></span> | <span class="t">the space region of the original example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=505" target="_blank">00:08:25.060</a></span> | <span class="t">That is the complete IG calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=508" target="_blank">00:08:28.780</a></span> | <span class="t">Let's return to sensitivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=511" target="_blank">00:08:31.220</a></span> | <span class="t">We have our model M with these one-dimensional inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=515" target="_blank">00:08:35.620</a></span> | <span class="t">one minus relu applied to one minus x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=518" target="_blank">00:08:38.900</a></span> | <span class="t">This is the example from Sundararajan et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=521" target="_blank">00:08:41.620</a></span> | <span class="t">I showed you before that inputs by gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=524" target="_blank">00:08:44.420</a></span> | <span class="t">fail sensitivity for this example in this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=528" target="_blank">00:08:48.260</a></span> | <span class="t">Integrated gradients does better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=531" target="_blank">00:08:51.300</a></span> | <span class="t">The reason it does better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=532" target="_blank">00:08:52.620</a></span> | <span class="t">you can see this here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=533" target="_blank">00:08:53.580</a></span> | <span class="t">we are summing over all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=535" target="_blank">00:08:55.980</a></span> | <span class="t">those gradient calculations and averaging through them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=539" target="_blank">00:08:59.580</a></span> | <span class="t">The result of all of that summing and averaging is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=543" target="_blank">00:09:03.340</a></span> | <span class="t">an attribution of approximately one depending on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=546" target="_blank">00:09:06.140</a></span> | <span class="t">exactly which steps that you decide to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=547" target="_blank">00:09:07.980</a></span> | <span class="t">look at for the IG calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=550" target="_blank">00:09:10.980</a></span> | <span class="t">This example is no longer a counter example to sensitivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=556" target="_blank">00:09:16.060</a></span> | <span class="t">In fact, it's provable that IG satisfies the sensitivity axiom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=561" target="_blank">00:09:21.460</a></span> | <span class="t">Let's think in practical terms now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=564" target="_blank">00:09:24.260</a></span> | <span class="t">We're likely to be thinking about BERT style models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=567" target="_blank">00:09:27.380</a></span> | <span class="t">This is a cartoon version of BERT where I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=569" target="_blank">00:09:29.580</a></span> | <span class="t">have some output labels up here at the top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=572" target="_blank">00:09:32.180</a></span> | <span class="t">I have a lot of hidden states and I have a lot of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=575" target="_blank">00:09:35.140</a></span> | <span class="t">happening all the way down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=576" target="_blank">00:09:36.820</a></span> | <span class="t">maybe multiple and fixed embedding layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=579" target="_blank">00:09:39.660</a></span> | <span class="t">The fundamental thing about IG that makes it so freeing is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=583" target="_blank">00:09:43.420</a></span> | <span class="t">we can do attributions with respect to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=585" target="_blank">00:09:45.620</a></span> | <span class="t">any neuron in any state in this entire model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=589" target="_blank">00:09:49.780</a></span> | <span class="t">We have some of the flexibility of probing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=592" target="_blank">00:09:52.380</a></span> | <span class="t">but now we will get causal guarantees that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=594" target="_blank">00:09:54.820</a></span> | <span class="t">our attributions relate to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=596" target="_blank">00:09:56.380</a></span> | <span class="t">the causal efficacy of neurons on input-output behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=601" target="_blank">00:10:01.060</a></span> | <span class="t">Here's a complete worked example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=603" target="_blank">00:10:03.600</a></span> | <span class="t">that you might want to work with yourselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=605" target="_blank">00:10:05.800</a></span> | <span class="t">modify, study, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=607" target="_blank">00:10:07.420</a></span> | <span class="t">Let me walk through it at a high level for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=610" target="_blank">00:10:10.420</a></span> | <span class="t">The first thing I do is load in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=612" target="_blank">00:10:12.980</a></span> | <span class="t">Twitter-based sentiment classifier based in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=616" target="_blank">00:10:16.540</a></span> | <span class="t">Roberta that I got from Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=619" target="_blank">00:10:19.420</a></span> | <span class="t">For the sake of CAPTEM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=620" target="_blank">00:10:20.860</a></span> | <span class="t">you'd need to define your own probabilistic prediction function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=624" target="_blank">00:10:24.460</a></span> | <span class="t">and you need to define a function that will create for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=627" target="_blank">00:10:27.100</a></span> | <span class="t">representations for your base as well as for your actual example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=631" target="_blank">00:10:31.100</a></span> | <span class="t">Those are the things that will interpolate between with IG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=635" target="_blank">00:10:35.140</a></span> | <span class="t">You need to do one more function to find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=637" target="_blank">00:10:37.660</a></span> | <span class="t">the forward part of your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=639" target="_blank">00:10:39.820</a></span> | <span class="t">Here I just needed to grab the logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=642" target="_blank">00:10:42.140</a></span> | <span class="t">and then IG forward and whatever layer I pick are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=645" target="_blank">00:10:45.700</a></span> | <span class="t">the core arguments to layer integrated gradients for CAPTEM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=649" target="_blank">00:10:49.900</a></span> | <span class="t">Here's my example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=651" target="_blank">00:10:51.480</a></span> | <span class="t">This is illuminating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=652" target="_blank">00:10:52.420</a></span> | <span class="t">It has true class positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=654" target="_blank">00:10:54.620</a></span> | <span class="t">I'll take attributions with respect to the positive class, the true class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=658" target="_blank">00:10:58.860</a></span> | <span class="t">Here are my base and actual inputs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=662" target="_blank">00:11:02.680</a></span> | <span class="t">and here's the actual attribution step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=665" target="_blank">00:11:05.460</a></span> | <span class="t">Inputs, base IDs, the target is the true class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=670" target="_blank">00:11:10.020</a></span> | <span class="t">and this is some other keyword argument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=672" target="_blank">00:11:12.180</a></span> | <span class="t">The result of that is some scores which I use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=675" target="_blank">00:11:15.200</a></span> | <span class="t">and then I z-score normalize them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=677" target="_blank">00:11:17.340</a></span> | <span class="t">across individual representations in the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=680" target="_blank">00:11:20.700</a></span> | <span class="t">That's an additional assumption that I'm bringing in that will do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=683" target="_blank">00:11:23.620</a></span> | <span class="t">averaging of attributions across these hidden representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=688" target="_blank">00:11:28.780</a></span> | <span class="t">Little more calculating, and then CAPTEM provides</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=691" target="_blank">00:11:31.780</a></span> | <span class="t">a nice function for visualizing these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=694" target="_blank">00:11:34.380</a></span> | <span class="t">What you get out are little snippets of HTML that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=697" target="_blank">00:11:37.500</a></span> | <span class="t">summarize the attributions and associated metadata.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=700" target="_blank">00:11:40.660</a></span> | <span class="t">I've got the true label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=701" target="_blank">00:11:41.940</a></span> | <span class="t">the predicted label, those align.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=704" target="_blank">00:11:44.820</a></span> | <span class="t">There's some scores, and I'm not sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=707" target="_blank">00:11:47.020</a></span> | <span class="t">actually what attribution label is supposed to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=709" target="_blank">00:11:49.500</a></span> | <span class="t">But the nice thing is that we have color coding here with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=712" target="_blank">00:11:52.300</a></span> | <span class="t">color proportional to the attribution score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=715" target="_blank">00:11:55.980</a></span> | <span class="t">You have to be a little bit careful here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=718" target="_blank">00:11:58.100</a></span> | <span class="t">Green means evidence toward the positive label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=722" target="_blank">00:12:02.060</a></span> | <span class="t">which in sentiment might be negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=723" target="_blank">00:12:03.920</a></span> | <span class="t">This is meant to be the true label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=725" target="_blank">00:12:05.640</a></span> | <span class="t">and negative is evidence away from the true label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=728" target="_blank">00:12:08.520</a></span> | <span class="t">and the white there is neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=731" target="_blank">00:12:11.100</a></span> | <span class="t">Here's a fuller example, and for this one to avoid confusing myself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=734" target="_blank">00:12:14.720</a></span> | <span class="t">I did relabel the legend away from the true label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=737" target="_blank">00:12:17.900</a></span> | <span class="t">neutral with respect to the true label,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=739" target="_blank">00:12:19.740</a></span> | <span class="t">and toward the true label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=741" target="_blank">00:12:21.380</a></span> | <span class="t">This is very intuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=743" target="_blank">00:12:23.020</a></span> | <span class="t">Where the true label is positive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=744" target="_blank">00:12:24.720</a></span> | <span class="t">we get strong attributions for great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=747" target="_blank">00:12:27.340</a></span> | <span class="t">Where the true label is negative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=749" target="_blank">00:12:29.300</a></span> | <span class="t">we get strong attributions for words like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=751" target="_blank">00:12:31.540</a></span> | <span class="t">wrong and less activation for things like great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=754" target="_blank">00:12:34.740</a></span> | <span class="t">This is intuitive here that we're getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=756" target="_blank">00:12:36.860</a></span> | <span class="t">more activation for said than for great,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=760" target="_blank">00:12:40.180</a></span> | <span class="t">suggesting the model has learned that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=762" target="_blank">00:12:42.340</a></span> | <span class="t">the reporting verb there modulates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=765" target="_blank">00:12:45.080</a></span> | <span class="t">the positive sentiment that is in its scope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=768" target="_blank">00:12:48.200</a></span> | <span class="t">Then down at the bottom here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=770" target="_blank">00:12:50.080</a></span> | <span class="t">we have one of these tricky situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=772" target="_blank">00:12:52.600</a></span> | <span class="t">The true label is zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=774" target="_blank">00:12:54.240</a></span> | <span class="t">the predicted label is one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=776" target="_blank">00:12:56.140</a></span> | <span class="t">These are attributions with respect to the true label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=779" target="_blank">00:12:59.440</a></span> | <span class="t">We're seeing, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=780" target="_blank">00:13:00.840</a></span> | <span class="t">that incorrect is biased toward negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=784" target="_blank">00:13:04.440</a></span> | <span class="t">My guess about these attributions is that the model is actually doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=787" target="_blank">00:13:07.880</a></span> | <span class="t">pretty well with this example and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=789" target="_blank">00:13:09.980</a></span> | <span class="t">maybe missed for some incidental reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=792" target="_blank">00:13:12.880</a></span> | <span class="t">But overall, I would say qualitatively,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=795" target="_blank">00:13:15.940</a></span> | <span class="t">this slide is a reassuring picture that the model is doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=799" target="_blank">00:13:19.500</a></span> | <span class="t">something systematic with its features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=802" target="_blank">00:13:22.300</a></span> | <span class="t">in making these sentiment predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=805" target="_blank">00:13:25.380</a></span> | <span class="t">To summarize, feature attribution can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=809" target="_blank">00:13:29.600</a></span> | <span class="t">give okay characterizations of the representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=812" target="_blank">00:13:32.900</a></span> | <span class="t">You really just get a scalar value about the degree of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=815" target="_blank">00:13:35.920</a></span> | <span class="t">importance and you have to make further guesses about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=819" target="_blank">00:13:39.100</a></span> | <span class="t">why representations are important,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=823" target="_blank">00:13:43.020</a></span> | <span class="t">but it's still useful guidance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=825" target="_blank">00:13:45.580</a></span> | <span class="t">We can get causal guarantees when we use models like IIG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=830" target="_blank">00:13:50.220</a></span> | <span class="t">But I'm afraid that there's no clear direct path to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=833" target="_blank">00:13:53.620</a></span> | <span class="t">using methods like IG to directly improve models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=837" target="_blank">00:13:57.260</a></span> | <span class="t">It's like you've just got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=838" target="_blank">00:13:58.820</a></span> | <span class="t">some ambient information that might guide you in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=842" target="_blank">00:14:02.420</a></span> | <span class="t">a subsequent and separate modeling step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=844" target="_blank">00:14:04.880</a></span> | <span class="t">that would improve your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=846" target="_blank">00:14:06.820</a></span> | <span class="t">That's a summary of feature attribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=848" target="_blank">00:14:08.740</a></span> | <span class="t">a powerful, pretty flexible heuristic method that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=852" target="_blank">00:14:12.500</a></span> | <span class="t">offer useful insights about how models are solving tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=p0dzR6iaFmc&t=857" target="_blank">00:14:17.700</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>