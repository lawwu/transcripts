<html><head><title>Evals 101 — Doug Guthrie, Braintrust</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Evals 101 — Doug Guthrie, Braintrust</h2><a href="https://www.youtube.com/watch?v=bk0TmxoZlUY" target="_blank"><img src="https://i.ytimg.com/vi_webp/bk0TmxoZlUY/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Hey everybody, my name is Doug Guthrie. I'm a solutions engineer at BrainTrust. As you can see here, we're an end-to-end developer platform for building AI products. We do evals. If you watched the keynote this morning, you saw our founding engineer jumping up and down on stage yelling evals. I am not going to do that.</p><p>I'm not as funny or cool as him, but we should all be very excited about evals here. Very brief agenda of what we'll cover today in this sort of intro. Very, very brief company overview. Give you an intro to evals. Why would you even start thinking about using them?</p><p>What are they? What are the different components that you need to create an eval? Some more BrainTrust specific things via running evals via our SDK. You'll see in the examples that you can run evals both in the platform itself as well as the SDK. I think it's a really kind of cool thing that we can connect maybe the local development that we're doing with what we're doing within the platform.</p><p>And then how we then move to production. This is maybe human review. This is online scoring. So how well is our application performing in production? And then lastly, a little bit of human in the loop. Getting user feedback from your users. How do we now take some of these production logs and feed them into the data sets that we're using in our evals.</p><p>Creating this really great flywheel effect. Cool. Quick, quick company overview. You see up there maybe in the top right some of our fund, excuse me, some of our investors. Maybe a quick call out on the leadership side. Ankur Goyal is our CEO. Maybe the reason to call out that is Ankur in the last two stops, he essentially built BrainTrust from scratch the last two places.</p><p>And this is really where he found the idea to like maybe this is actually a thing that people need. And really the origination story of BrainTrust. The other thing to call out here is like we already have a lot of companies using BrainTrust in production today. This is just a few of the companies that are utilizing us for evals, for observability of their Gen AI applications.</p><p>I won't bore you too much with that, but let's jump into this. If you're at the keynote, you probably saw a similar slide here. I didn't take it out, but you see like the tech luminaries here, I think as Manu referenced them, talking about evals and the importance of them.</p><p>I think this is a, you know, obviously this is an intro to evals track. And what better way to start this out with some really, you know, influential people in the space talking about evals and why they are so important. So why would you think about evals, right? They, they, they help you answer questions.</p><p>So here's, here's a few of them. When I change the underlying model to my application, is it, is it getting better or is it getting worse? When I change my prompt, right? When I change certain, certain things about the application, is it getting better or is it getting worse?</p><p>We want to get away from not having a rigorous sort of process around building with large language models, which as you all know, right, non-deterministic outputs creates somewhat of a challenge. And without evals in place, it becomes really, really hard to, to create a good application that we can put into production.</p><p>Maybe some other ones, like obviously being able to detect regressions within the, within the code. I think the other, the other thing that Ankur mentioned to me when I first started, which I didn't really mention this, but this is my, my third week at, at BrainTrust as a solutions engineer.</p><p>But one of the things that he mentioned to me that I thought really resonated was, uh, evals are a really great way, I think people think of them as almost like unit tests for, uh, for our, you know, our applications. But he kind of described it in another way of like, this is a really great way for us to play offense, uh, as opposed to just playing defense, where I think maybe unit tests are, are kind of used for.</p><p>This can actually be used as a tool to, to really help, uh, create a lot of rigor around us building and developing these applications. And ensuring that we actually build, uh, things that we can put into production. Maybe from like a business perspective, uh, why would you think about running evals or using evals?</p><p>Uh, here's a few here, uh, if you have, um, evals running both offline and online, you create this feedback loop or this flywheel effect that I think Manu mentioned in the, in the keynote, in the keynote, excuse me. Um, and this flywheel effect allows us to, you know, uh, cut dev time, allows us to, uh, enhance the quality of this application that we're putting out in production.</p><p>If you're able to connect the things that are happening in, in real life with your users in production and being able to, uh, filter those logs down, add those spans to data sets, inform what you're doing in an offline way creates, uh, that, that flywheel effect that becomes really powerful from a development perspective.</p><p>Uh, again, a little bit more on the, on the customer side, here's a few of our customers and some of the outcomes that they've, that they've seen using brain trust, whether it's, uh, moving a little bit faster, pushing more AI features into production or just increasing the quality of, of the applications that they have.</p><p>So let's, let's, let's start talking a little bit about the, the core concepts of, of, of brain trust. Obviously we're here to talk about evals, uh, the, the, the things that I think, um, you see like these arrow, these arrows going, uh, one way and the other. This again is that, that flywheel effect that, that I described earlier.</p><p>Um, there's the, the prompt engineering aspect of this, uh, in brain trust, think of, uh, this playground that we have as an IDE for LLM outputs. Uh, the playgrounds allow for that rapid prototyping as we make those changes, as we change the underlying model, what is the impact to that, to that, uh, particular, uh, task or that application.</p><p>Then those evals allow us to understand the improvement or the regression of those changes and then the observability aspect, right? This is the, the logs that we're generating in production, the, the ability to, uh, uh, uh, have a human review those logs in a really easy, uh, intuitive interface and then have user feedback from actual users be logged into the application as well.</p><p>So what is an eval? Yeah, you probably, uh, heard this, uh, several times today throughout the week if you stopped by the booth. But sort of our definition here is that structured test that checks how well your AI system performs, right? It helps you measure these things that are important, quality, reliability, uh, correctness.</p><p>So what are the ingredients in an eval, right? I've been talking a little bit about task, right? This is the thing, uh, the code or the prompt that we want to, uh, evaluate. The, the really cool thing about brain trust is that this can be as simple as a, as a sim, excuse me, a single prompt.</p><p>Or it could be this, this full sort of agentic workflow where we're, uh, calling out the tools. There's no sort of limit on to the, uh, the, the complexity that we put into this task. The only thing it requires is an input and an output. The second thing is a data set.</p><p>This is our, uh, real world examples. This is, uh, essentially what we're gonna run the task against to understand how well, uh, our application is performing. And how we do that is via scores. So the score is really the, the logic behind your evals. There's, there's a couple different ways to think about this.</p><p>There's the LLM as a judge type score. So, uh, you give it the, the output and some criteria and it is able to assess, uh, like say, I want, uh, based on this output, is this excellent? Is this fair, is this poor? And then those outputs then correspond to, you know, 0.5 or 1.</p><p>Uh, you also have code based scores, right? These are maybe a little bit more heuristic or binary, but, uh, we can use both of these to really aid in the development of that, of that eval and ensuring that we're building a really good application. Uh, I think I just sort of mentioned this here as well, but like the, the two mental models here of, of evals, there's, there's offline and online.</p><p>Offline is pre-production. This is us actually doing that, that iteration. It's, uh, identifying and resolving issues before deployment. Uh, this is where we're defining those tasks. It's where we're defining those scores, uh, online evals. This is that real time tracing of, of, of the application in production. It's logging the model inputs and the outputs, uh, the intermediate steps, the tool calls, everything that's happening.</p><p>It allows us to diagnose performance and reliability issues, latency, uh, based on how you instrument your, your application with brain trust. We can pull back lots of different metrics related to cost and tokens and duration and all of these things help inform, uh, how we build this application. Um, I'll, I'll jump into a little bit more of like how we can instrument our app, uh, for online evals.</p><p>Uh, we're going to first, I think, talk a little bit more of the offline before doing that. Uh, maybe just like level set on how to improve. I think one of the, the, the, the things that I've seen, uh, in the last few days here, the conversations that I've had, it's like almost, how do I get started?</p><p>Or what do I do if X or, you know, those types of questions. The other thing I heard from Anker, uh, very early on is that like, just get started, create that baseline that you can then iterate and build from. Uh, I think a lot of people get caught in like creating this, this golden data set of test cases, uh, that they, they can then like iterate from.</p><p>Um, start, you, you don't necessarily have to do that, start and build that, uh, that baseline, establish that foundation that you can then improve upon. But this is a really good sort of matrix of like, uh, if I have good output, but a low score, what do I do?</p><p>Right. Improve your evals. If I have bad output and a high score, improve your, your evals or your scoring, but really good kind of like high level, uh, understanding of, of where to start to target your, your efforts when you are building these apps and you're in, you're creating evals.</p><p>So let's jump into the actual components that I just, that I just talked about. So within the brain trust platform, we have a task, uh, again, this could be a prompt. This could be like this full agentic workflow, uh, very basic, you, you see that, that gif running. This is a prompt within the platform.</p><p>You specify an underlying model that you wanted to use. You give it a, a system prompt. You can also, uh, give it access to tools. It has access to mustache, mustache templating. So you can pass in variables like user questions or, um, you know, the input from the user, its chat history or metadata, right?</p><p>So when we actually go and want to parse through these logs, the metadata becomes actually beneficial. Uh, enabling us to do that in a really easy way. Um, going forward, uh, maybe we have a multi-turn sort of chat type scenario where we want to add uh, additional messages for the system and the assistant and the user, uh, and our tool calls as well.</p><p>Uh, the, uh, the, the platform allows for that just via this, uh, plus this messages, uh, button. And then you're able to add those different messages to the prompt. Um, also we can add tools. So oftentimes the, the, the prompt will, will need access to, to something, right? Maybe it's a, a rag type workflow.</p><p>Maybe it's doing web search, whatever it is. We can now use those tools as part of that prompt, right? And so when you use sort of encode in that prompt, like make sure you use X tool, uh, this prompt has access to that tool while it's running. Uh, the last one, this is actually a feature that is in beta right now.</p><p>Uh, it's actually creating more of that agentic type workflow within your, uh, within the brain trust platform itself. So it's, it's a way right now, at least to chain together prompts where the, the output of one now becomes the input of the other. But if you think back maybe to, to this slide, if I have sort of, um, this prompt that has access to tools, you, you create a pretty powerful system here where you're able to go from, uh, maybe that first step that has X access to a certain tool and we get some output from that.</p><p>We can then go to that next step that has access to maybe some other tools, right? That's sort of like maybe multi-agent type of workflow we can create with the underlying tools within those prompts. The second thing that I talked about is data sets, right? These are our test cases that we want to give to the, the task to run.</p><p>So we can sort of iterate over that. We can get the output and then going down a little bit further, actually score that. But this is, um, obviously really important when we're, when we're running our evals. And then when we are, uh, trying to pull from production, right? The, the actual logs that are happening, we can add those, those, uh, spans, those traces to the data sets in a really easy way.</p><p>And I can show you what that looks like. But, uh, if you look there at the bottom, the only thing that's required is the, the input. Uh, you also have the ability to add, uh, expected. So what is the expected output for that input, right? You can sort of like, uh, create some sort of score that looks at the output with the expected.</p><p>There is a, a score called Levenstein that allows you to, you know, measure the, the, the difference between those two. So you can do some different things based on what you provide to that data set. You also have metadata as well. Again, being able to filter down different things, pulling the data set maybe into your own code base.</p><p>And I want to filter by again, X, Y, or Z via the metadata. That's all possible. Uh, I mentioned this a little bit ago, but just start small and iterate, right? You don't have to create this golden data set to, to get started here. Um, just, just start and then, uh, then continue to iterate and build from that baseline.</p><p>Uh, the human review portion also becomes really powerful. Again, when we have stuff, uh, being logged within production, having humans actually go through those logs and, you know, there's lots of different ways to filter it down to the things that they should be looking at. And then we can now decide to add those things to certain data sets that then inform, uh, the offline evals that we're running.</p><p>Uh, the last thing, the last ingredient here that we need for our, excuse me, uh, for our evals are our scores. We have both code base scores, right? Again, this is like more of those binary type conditions, but you can actually, uh, code TypeScript or Python. You can do that within the UI, as you see over there on the bottom left, or you can within your own code base, create that score and then push it into brain trust.</p><p>So we can use it in the platform. Other users who are maybe aren't in the code base can use that score as well. The other score that we have access to is called LLM as a judge. So this allows us to use an LLM to sort of judge the output.</p><p>We can give it the, the set of criteria that indicates what a good or a fair or a bad score, or whatever it is, you, you get to decide, uh, what that looks like. So you give it that criteria and it says, if it's good, uh, I want to do a one.</p><p>If it's bad, I want to do a zero. But this starts to create the scores, uh, that we can use in that offline, in that online sense. The other thing to call out here is that we have, um, internally built a package called auto evals. So this is something that you can now pull into your project.</p><p>These are out of the box scores that are both LLM as a judge, as well as code base. And so it just allows you to get started very, very quickly. Another thing I heard Ankur mention is, um, maybe starting with Levenstein, maybe not the best score in a lot of cases, but again, it establishes a baseline.</p><p>Very, very little development work for, for, uh, our users, but it creates that thing that we can then build from. And now you have, uh, a direction, a direction to go in to go build maybe that more custom score. Some of the things that we've heard from our customers, some tips that, um, that, you know, important to think about.</p><p>A lot of our customers are using higher quality models for scoring. Uh, even if the prompt uses a cheaper model, just, just makes a lot of sense. Like while we're running that application to use the cheaper model, but use the, the more expensive one to actually go out and score it.</p><p>Um, also break your scoring into, uh, very focused areas. So, uh, the example that I'll show is a, an application that, that generates a change log from a series of commits. So I could create a score that says assess my accuracy, my formatting, and my correctness. Or I could create three different scores that assess accuracy and then formatting and then correctness.</p><p>So have your scores be very targeted to the thing, uh, that they're supposed to be doing. Uh, test your score pump, excuse me, prompts in the playground before use. And then avoid, avoid overloading the score or prompt with context. Uh, focus it on the relevant input and the output.</p><p>Uh, couple things here, here's where, uh, over on the left we have our playgrounds. This is where we do that, that sort of like rapid iteration where we can pull in those prompts. We can pull in those agents, add our data sets and add our scores. And we can click run and it'll go out and sort of churn through that data set that we've defined.</p><p>And it will give you a sense for how well your task is performing against the data set with the scores that we define. But this is the place where, uh, developers, uh, PMs. We even have a healthcare company that has doctors coming into the platform and interacting with the playground and even doing human review as well.</p><p>Uh, depends a little bit obviously on, on the organization. The thing on the right is our experiments. This is our sort of like snapshot in time of those evals. So imagine now like as we are doing this development and we're trying to understand, uh, like the last, you know, the last month or so, are, are we getting better, right?</p><p>The, the, the changes that we are making, the model changes, whatever it is, are we improving our, our application and the experiments is a really great way to, to understand that. Uh, really important maybe to call out as well. You can see in the bottom, right? The evals can happen from, uh, the application, right?</p><p>The, the brain trust platform as well as via the SDK. Cool. Maybe just really quick because, uh, nobody likes looking at, at slides all the time. I, I certainly don't. Uh, maybe if you haven't seen brain trust yet, this is maybe a good, a good quick demo. Uh, so again, like the, the, uh, the idea here is I have this, this application.</p><p>I'll just give you, I'll show you over here. Uh, you give it a GitHub repository URL. It, uh, grabs the most recent commits and then creates a change log from there. And then once this completes, you can even provide some user feedback. But this is the thing that we want to, uh, evaluate.</p><p>So what I can do, uh, I'll go into my, my playground, right? This is the place where I can start to, uh, run those, those, uh, those experiments. Or I can start to iterate on that prompt that I have. From my project, I've actually loaded in two different, uh, two different prompts.</p><p>So maybe I'll, before going into the playground, I've actually created these two prompts within my code base and I pushed them into the brain trust platform. I've also created a dataset in that code base and I've also created some scores, right? These are the ingredients that we need to run our evals.</p><p>So now when we have, we have those, we have those different components. Now we're able to start to iterate here. So I'm going to actually create a net new playground and I will load in one of these prompts. So again, here's my first prompt. Uh, my first prompt has a model associated with it.</p><p>What becomes really cool here is the ability to, to like, to iterate on the underlying model, right? I think a lot of us are, we have access to a lot of underlying providers and we want to be able to understand if I change this or if I, you know, a provider adds a new model, what is the impact to my application?</p><p>So I can duplicate this prompt and maybe change this to GPT 4.1. I can run this. Before I run it, I have to add all of my components. So I'll add my, my dataset and then I can add my different scores that I've, uh, that I've configured here for my change log.</p><p>And so I can click run and then now we'll understand here, what is the effect of changing the model, uh, for this particular task with the scores that I've configured against this dataset. So this will churn through all of these in, in parallel and we'll start to get some results back.</p><p>Uh, lots of different ways to actually start to look at this data. I always like coming over to the summary layout because I can understand like, um, um, you can see over here, this is my base task right here and then my comparison task. So I can understand, uh, looks like, you know, on average, uh, that the base is performing a little bit better than my, uh, comparison task on my completeness score.</p><p>Uh, it's varying a little bit worse on my accuracy score. Uh, both of them are, you know, zero percent on my formatting. So probably have some work to do there, but you can start to see how you can use this type of interface to iterate very quickly, right? Now, uh, the other thing that, um, maybe shouldn't do, but, uh, I can't resist because we just released this today is this new loop feature.</p><p>So imagine you are, uh, you know, you're a user within brain trust and before this, you would sort of manually iterate here and creating net new prompts, uh, making modifications, changing the model. What if you could now utilize AI to go and do that for you? So any sort of like cursor like, uh, interface, we can ask it to optimize a prompt.</p><p>And I think the really unique thing here is it has access to those, uh, those evaluation results. And so when it goes to go, uh, change that prompt, it understands that it changes the prompt. It runs the evaluation. It understands if it got better relative to the scores that we defined.</p><p>So you can see if it's going to go through here, it'll fetch some eval results. Uh, you'll probably see a diff here very, very soon. If we don't, I won't, I won't hang out here too long, but I do want to highlight one of the things that we are releasing that really enables our users to iterate in a really, uh, really fast way.</p><p>So here's my, my change. We can click accept and then it'll actually go out and run that eval again or it would. Uh, I think I have an issue with my anthropic API keys, but the idea here again is like we can create that very rapid, uh, iterative feedback loop here within the playground.</p><p>The other thing here is, uh, we can run these as experiments. So this is, um, this is where we can start to create those snapshots in time of that eval. And again, see as I make these changes to that, that application, how is it sort of performed over time?</p><p>I want to make sure I don't, I don't want to go down. I don't want to like, uh, decrease the performance of my scores relative to obviously the last time it ran, but, uh, looking out over the last month, six months, whatever it is that we're tracking. So that was very, very, uh, brief sort of intro to like evals via the UI, right?</p><p>Again, like just to summarize, we need a task, we need a dataset and we need at least one score. We can pull those into the playground and now we can start to iterate. We can save these via experiments. Uh, and now we can, we have a way in which we can understand how well this application is performing, right?</p><p>This is no longer like qualitative, right? This isn't like, Hey, I think this got better. That output looks better. There is actual rigor behind this now. Um, customers oftentimes ask though, like, I don't really want to use, or I'm not going to use the, the platform as much. I'd rather use this from my, my code base.</p><p>Is that possible? Uh, and it is. Uh, so, uh, we have, uh, Python SDK. We have a TypeScript SDK. There, there's some other ones as well, Go, Java, Kotlin. Uh, for the most part, most of our users are using, uh, Python or TypeScript. Here's a, just a couple examples of what this might look like from an SDK perspective.</p><p>Um, actually, if you all aren't, uh, opposed to looking at, uh, some code, here's just a, a really basic example of, uh, defining a prompt within my, my code base and then pushing it into brain trust. So just leveraging that, that Python SDK, uh, another example, I should come over here.</p><p>Creating a score. So you give it sort of like the, the things that it's looking for, but now I've sort of defined this score within my code base. Uh, it's version controlled. Uh, also the, the prompts that you create within the UI are version controlled as well. Um, but this is just another way to start to interact with, with brain trust.</p><p>So again, scores, we could do data sets, uh, and then you can even do, uh, prompts up here as well, I believe. So here's my eval data set. Here's my, uh, change log to prompt. Again, being able to start from the code base and actually push them into the platform is possible.</p><p>Just depends on the organization where they want to start. The, the other thing here is like, this is more on the, like the, the components of the eval side. So that's that top portion. Define those assets in code, run that brain trust push, and now you have access to that in that brain trust library.</p><p>The other one is actually like defining the evals and code, right? So what that looks like is, is slightly different. Um, come over here. So we have our eval, but this is just a class that's coming from our brain trust SDK. Again, it's looking for the exact same things that I just described, right?</p><p>A data set that we can use from, uh, brain trust itself. Uh, the task that we want to invoke and then the scores. So again, defining this here within, uh, within your code base, certainly possible. And then I can run, you know, a command that actually runs that eval within brain trust.</p><p>So from here, go into brain trust, see the eval running. This is now experiment that I can view over time. So again, like you saw two different types of workflows here, uh, again, catering to maybe two different personas or again, the way in which organizations want to work, it's up to them.</p><p>Brain trust is very flexible and how we allow our users to, uh, to consume or use the platform. Uh, probably jumped ahead a little bit, but this is sort of a recap of what I just showed you. Again, from your code, you, you create your prompts, your scores, your data sets, you can push them in there.</p><p>Uh, maybe it's just importantly to important to highlight here of like why you would do this. Uh, you want to source control your, your prompts. Uh, the big one here to call out is the online scoring. Uh, I have a section in a little bit diving a little bit deeper into that.</p><p>Uh, but if you want to use those scores that we define in the dataset, we should push them into brain trust so that we can create online scores. We can understand how our application is performing in production relative to those scores that we want or that we're using within our offline evals.</p><p>What I just showed you, maybe another, uh, another variation of that, that eval within our code. Again, defining that dataset, defining that task, defining those scores becomes very, very easy to now connect these two things. It's just again, up to you to decide where you are, where you want to do this.</p><p>The other thing to call out here is that this can be run via CI/CD. We do have some customers that, that want to run their evals as part of the CI process. So understanding in a more automated way, right? The, the score for A, B and C, whatever they've configured.</p><p>Has it gotten better? Has it gotten worse? This becomes maybe a check as part of CI. There is, uh, if you look within our documentation, there's a, a GitHub action example that shows you how you could set this up. Cool. Let's, let's move to production. Moving to production entails setting up logging, right?</p><p>It entails instrumenting our application with, uh, with brain trust, um, code, uh, being able to like say, I want to wrap this, this, uh, LLM client. I want to wrap this particular function when it goes to call that tool becomes very, very easy to do that. But so, so why should you do it?</p><p>Um, I think I've probably said it, said it numerous times here, but we want to measure quality on live traffic, right? We actually want to understand how well our application is performing with those scores really great to use during offline evals becomes, uh, our aid in ensuring that we, that we build really good applications that we're not creating regressions, but also really important to monitor that live traffic.</p><p>The other really important thing to call out, I think is that, that flywheel effect that it creates. So we have these data sets that we use to inform our offline evals. It's very, very easy now to take the, the logs that are generated within production and add those back to data sets.</p><p>This also, um, speaks to some of that human review component where we want to now bring those humans in. They can start to review some of the logs that are relevant. Like maybe there's user feedback equals zero. Maybe there's a comment or whatever it is, but like they can filter down to those particular things.</p><p>And as they find really interesting, maybe test cases, it's very, very easy to add those, uh, back to the data set that we use in our offline evals. So I think the, the feedback loop or the flywheel effect that that's, that this creates is one of the really fundamental value props of, of the platform.</p><p>So how do we do this? Uh, there, there's a couple different ways, right? We're first going to initialize a logger. This is just going to authenticate us into brain trust and point us to a project. Uh, you may have seen when I opened up the platform, I had numerous projects inside of there.</p><p>You can almost think of a project as a, as a container for that feature, right? So you probably have multiple AI features that you're building. I want to have the container for feature A for those prompts, those scores, those data sets. You could certainly utilize those things across projects, but it becomes a really good sort of way to, uh, containerize the things that are important for that feature.</p><p>Then you can start really basic, right? You can wrap a, uh, an LLM client. Uh, so when you saw those, some of those metrics with like tokens and LLM duration and costs, uh, just very basically within the, the, uh, the script or excuse me, the, the code here, I just wrapped that open AI client.</p><p>And now I'm just sort of ingesting all of that, those metrics into my logs. That's the easiest way to get started. You obviously probably want to do a little bit more again. Maybe you want to, uh, understand when, you know, that LLM invokes a tool. So I want to trace, uh, I can add a trace decorator on top of a function.</p><p>Uh, I can even use some of the, like the, the brain trust low level, like span elements to create custom logs. And I want to customize the input and I want to customize the output and the metadata that we, that we log to that span. So again, you can, you can start very basic with wrapping a client and then go down to like the individual span itself, specifying that input and that output.</p><p>This leads us to online scoring, right? This is, I talked a little bit about this, right? This is where like when our logs are coming in, we can actually configure within the platform, those scores that we want to run, and we can specify sort of a sampling rate. So we don't necessarily run that score across every single log that comes in.</p><p>Maybe it's 10%, 20%, so on. Um, but it, but it creates that really tight feedback loop that, that I've been talking about. Uh, also maybe just important to mention the early regression alerts. So we can create automations within the brain trust platform. If my score drops below a certain threshold, let's create an alert, uh, with it, with our automation feature.</p><p>This is just, uh, and I can maybe walk through what this looks like instead of showing you here. Uh, the custom views. This is where like, there's a lot of really rich information within these logs, and it becomes really important, I think, again, for the human review component to like filter these down to the things that they care about, or the things that anybody cares about.</p><p>So we can create custom views within brain trust with the appropriate filters. Uh, and then it's very easy for that human to go into what we call human review mode within brain trust and sort of parse through those logs. Uh, the ones that are the, the ones that are going to be most meaningful to them.</p><p>Let me, uh, let me connect some of those, those dots there. So again, showing you some code, maybe good, maybe bad, but, um, I'm guessing there's some technical people in the room that don't mind here. So if I look for, um, the, uh, you may have seen in one of those slides, there is a, you know, the Vercel AI SDK.</p><p>I want to wrap this AI SDK model. Uh, again, this allows us to just create all of those metrics within brain trust with just zero lift from us as a developer. This becomes really easy to do. Uh, you can also see where I have specified that span itself, right?</p><p>I actually want to define the inputs and the outputs of that. The reason you would do that is because you have a specific dataset with a structure that you want to ensure maps to that. So like when you are within those logs, parsing through them, it becomes really easy to add those spans back to that dataset.</p><p>So ensuring that that data structure is sort of consistent across offline and online becomes really important again to create that feedback loop. So this is, you know, very high level of like how we can start to create those spans. Um, now that we do, right, we can now go in, uh, the platform and start to configure our online scoring.</p><p>So this is here just within this configuration pane. I can click online scoring. I'll just delete, I'll create a new rule. Uh, so my new rule and here's where we can add different scores, right? Obviously I have a few here that I've been using for the offline evals. I don't necessarily need to select all of them, but I certainly can.</p><p>And then I want to, uh, apply a sampling rate. So I want to actually give you an example of what this looks like. So I'm going to do a hundred percent. The other thing to call out here is that you can apply these to the individual spans themselves and not the entire route span.</p><p>So where this becomes beneficial is like when you are invoking maybe tool calls, you're invoking like a, a rag workflow and you actually want to create a score on whether or not the thing that it gave back, uh, is actually relevant to the user's query. So we can actually create a score specifically for that and highlight what that span is here.</p><p>So again, very, very flexible in how you apply these scores to the things that are happening online. Now, when I come back here to the application and we'll just run this again, uh, creating that change log, you'll now start to see here within, uh, the logs, this will start to show up and then you'll start to see these scores be generated.</p><p>Right. Again, this is where like you can now start to understand over time in production, how are these things doing? How are they faring? Where can we get better? Uh, again, now we can connect again, like the things that are happening in our offline evals with the things that are happening with online.</p><p>The other thing to call out here is the, uh, the feedback mechanism, right? Uh, we certainly have, uh, the ability to do like human review, but oftentimes you want your users to provide feedback as well. And so this is just a basic example of a a thumbs up, thumbs down, and you can even provide a comment here.</p><p>This can now be logged to brain trust. So I should see over here my user feedback. So here's my, my comment. And then I have my user feedback score, but now I can also do something like this. So again, maybe I want to filter my logs down to where user feedback is zero.</p><p>So click that button. I'm going to change this to zero, right? I don't have any rows yet like that, but now I can save this as a view and people who are now using this as human review can filter this down to where user feedback equals zero. And we can figure out what's going on, right?</p><p>What are the things that, that fell down here within this application that we need to go fix? The other thing I'll highlight here is our sort of human review component. Uh, actually, um, you, you click that, that button or you can hit just R and it opens up this, this different pane of your log.</p><p>So it's a pared down version of what you just saw there. It's a little bit easier for a human to go through and actually look at that, that, that input and that output. But you as a, as a user of brain trust can configure the human review scores that you would like to, uh, to, to use.</p><p>So I have this add something here. So maybe this is a little bit more free text. I have a better score. Again, these are the things that, that you can add to your platform that map to the, the, the, the, the review, excuse me, the scores that you want your, your humans to, to add to those logs.</p><p>Um, just really quick, I'll, I'll highlight some of these things here. Um, this is what it starts to look like when you instrument your, uh, your application with those different, um, wrappers or those different trace functions. Um, I'm able to understand at a very granular level, excuse me, granular level, the things that it's doing, right?</p><p>So I essentially have these tool calls where it's going out and it's grabbing the commits from GitHub. It's understanding what the latest release is, and it's fetching the commits from that latest release. And now I can generate that change log. But again, the, the really unique thing here, and maybe a different example of this is I can start to score those individual things that are happening.</p><p>So this is a different, uh, application with, uh, with this example. So if I open this up, I have these, uh, this conversational analytics application. So a user can ask a question and can return back some data. But this application goes through these various steps. Like the first step is to rephrase the question that the user asked.</p><p>So imagine like there's this chat history that we can load in as input and the LLM needs to rephrase that user question. If the LLM does a really bad job of rephrasing this question, everything as a result of this will fall down. Probably not going to get a right, a right answer.</p><p>So what I can do is create a score specifically for that span to understand how well the LLM did in rephrasing that question. You can also understand the intent that I was able to derive or the LLM was able to derive from that question. Is that right? But you start to think of like these, these more complex type of applications that you build.</p><p>You need to be able to understand the individual steps that are happening. And Braintrust allows for that very, very easily via these scores and then being able to apply them not only again while you're in offline eval kind of mode, but also online, right? We want to understand these logs and be able to apply these scores at the individual span level.</p><p>This becomes pretty powerful as well. I think I actually stole from my next section, my human in the loop. Kind of walked through this a little bit. Maybe just another call out. If you happen to be at one of our workshops on Tuesday, Sarah from Notion, who's a Braintrust customer, talked a little bit about how they think about human in the loop.</p><p>I think it's important to consider like the size of her organization and what they're doing. She mentioned that like she has a special type of role that they use for human in the loop type of interaction, right? There is, it's almost like a product manager mixed with an LLM specialist.</p><p>They're the people that are going through and doing those human reviews. Smaller organizations, she made, she made a comment that was, it actually makes a lot of sense for the engineers, some engineers to actually go through and do this as well. It becomes really powerful to pair like the automation with a human component of this.</p><p>Like this is not going to go away. I think it adds value to the process. Again, I think I just stole from myself like why, why this matters, right? This is really critical for the quality and the reliability of your application. It provides that ground truth for what you're doing.</p><p>Two types of human in the loop interactions here. I walked you through that human review. Give me one second and I'll call you. Yeah. The two, excuse me, the two types, the human review, being able to like create that interface within Braintrust that allows that user to kind of parse through the logs in a really easy manner, as well as configuring scores that allow them to add the relevant scores to that particular log.</p><p>And then the user feedback. This is actually coming from our users in the application. Again, being able to create sort of views on top of that feedback that then power maybe the human review and then creates that flywheel effect that we want. That's all I have today. Appreciate you all coming out here and listening to me.</p><p>But yeah, if you have a question. Thanks. Yeah. The question is around like, how are we using human review and like some of the logs and informing the, the, the offline eval portion of this largely that. Cool. Yeah. The question is around like, how are we using a human review and like some of the logs and informing the, the offline eval portion of this largely that goal.</p><p>Uh, yeah, one thing I, maybe I didn't highlight here is so maybe back within Braintrust. I'm going to go back to my initial project. So, imagine now like we have, we have all of these logs. We filtered it down to a particular, oh. Are we still showing? On the screen.</p><p>Awesome. Thank you. Um, yeah. So imagine like we, we have this, uh, this, this process now, right, where we're, we're doing that human review. We filtered it down to the records that are meaningful for whatever reason. It becomes really easy again to connect what's happening within production. So I, maybe I select all of these rows or I select individual rows, but I can add these back to the data set that we're using within those offline evals.</p><p>I, I think I've said this like a hundred times over this conference, this flywheel effect. This is like, I think what's missing oftentimes when we're building these, these AI applications and what Braintrust allows for really seamlessly. Yeah. Uh, I have two questions. Uh, the first one is about production.</p><p>Uh, is it possible to have multiple models in production and compare how they behave? Yeah. I, I don't see why not. Like my, my, my guess is in the underlying application, you're swapping them out. Like, uh, having like a B test, you know, I can have like a two or three or four and easily compare.</p><p>Absolutely. Yeah. Um, let's see if I have an example here. You're able to, to group some of these scores. Uh, maybe this is sort of an example of, of what you're talking about. So like maybe within production, we have different models running. Uh, this, this sort of view here allows us to understand like the, the models that we're using under the hood.</p><p>Uh, and this is just, you know, you could do this within production as well and sort of do that A/B testing. Cool. Uh, my, my second question is about humans in the loop, right? Um, let's suppose that I have multiple humans and, um, they behave, uh, slightly different as his quarter, his quarters.</p><p>Do you have anything or what, what is the vision to do with that? Like, is there a way that I can actually compare how they're scoring or something like that or not really? So different users can maybe have different sort of criteria for scoring. Maybe the first thing I would say to that is like, there, there should be like maybe a rubric for your users who are interacting with human review.</p><p>So you're not creating that. Uh, you certainly have the ability to see, like who is scoring different things within the platform. Um, I'm not sure if you're able to pull that as like a data set to like assess the differences there, but maybe like before it gets to that place, like have a rubric, have a guideline of, of what scoring looks like for your humans.</p><p>Okay. Thank you. Yeah, of course. Hi. Um, so the scores I'm used to working with for like LLM as a judge are like, they're relativistic, right? So they can't tell you is the answer relevancy good or bad for a single run, but it can tell you how it compares to previous iteration of like the same test set, for example.</p><p>Um, do you guys use LLM as a judge scores for online or is, and like, how are, are they relativistic like that? Or do you have some way to be like, this is a good answer, you know, in and of itself for this sample or because it's all, you have new data coming in, right?</p><p>Yeah, I think a lot of our customers who are thinking about this are, are like almost doing evals on their evals, like trying to understand, did the LLM as a judge actually do a good job there? So like when that actually runs, there's a rationale behind it. And so you can sort of run an eval of those LLMs as a judge.</p><p>I think Sarah from Notion in our workshop described sort of a process like that within Notion, but I think that's, that's sort of like where I would aim continue. Okay. Cool. Thanks. Cool. Awesome. Are any of your customers doing evals before they launch? Like, I'm working with a government.</p><p>they don't want to launch until we show some accuracy levels. Yeah. So we're getting our subject matter experts to enter in all the questions that they have, right? And they have huge data sets of thousands of questions. Believe me, as a government. And then we're using measures like you're talking about.</p><p>Do you have a way to do that? Like, I guess it's, I guess it's the same. Is it? So what you're describing is what we call offline evals, right? This is development, right? We can actually do this testing before we get into production. This is what I was talking about, like establish that baseline, right?</p><p>Using those scores, using that data set that you've already created. But this all happens before we get into production, right? And then you can like, one of the things that I heard from somebody earlier is like, one of my challenging things of building this AI application is establishing trust or creating that trust in this thing.</p><p>That's part of what this is, right? It's like, it's showing, showing those people the scores of that application. So you start to iterate on this thing. Maybe it starts at 20%, then it goes to 30, then at 40, and so on. That to me is the thing that you use to create that trust and create that like groundswell to push it into production.</p><p>Okay. Yeah, that is what we're trying to do. But I wondered if I can see the tool does that. Thank you. Yeah, of course. Time for one more. Thanks. Quick question. I love the CICD components. We're trying to build a lot of -- we're trying to build like MLS as a platform for our team.</p><p>So we get into evals and stuff like that. So how much of the monitoring dashboard you have in BrainTrust can actually be like take the data taken out and posted in a unified dashboard somewhere else. Yeah. All of this is available via SDK, right? You can pull down experiments.</p><p>You can pull down data sets. So you're able to, you know, pull this down. Like we have -- we have a customer that is actually building their own UI on top of like the SDK itself. Like so they built their own sort of like components utilizing the SDK and pulling the sort of things that we've logged, the experiments that we have in the application into their own UI.</p><p>So it's certainly possible. Awesome. Oh, great. Thanks. Yeah. All right. Cool. Thanks, everybody.</p></div></div></body></html>