<html><head><title>Multi-Agent Systems in OpenAI's Agents SDK | Full Tutorial</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Multi-Agent Systems in OpenAI's Agents SDK | Full Tutorial</h2><a href="https://www.youtube.com/watch?v=2MYzc79Lj04" target="_blank"><img src="https://i.ytimg.com/vi_webp/2MYzc79Lj04/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=0 target="_blank"">0:0</a> OpenAI's Agents SDK<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=98 target="_blank"">1:38</a> Python Setup<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=171 target="_blank"">2:51</a> Orchestrator Subagent<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=357 target="_blank"">5:57</a> Web Search Subagent<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=700 target="_blank"">11:40</a> RAG Subagent<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=1035 target="_blank"">17:15</a> Code Execution Subagent<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=1424 target="_blank"">23:44</a> Orchestrator Agent<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=1724 target="_blank"">28:44</a> Evaluating our Multi-Agent Workflow<br><a href="https://www.youtube.com/watch?v=2MYzc79Lj04&t=2371 target="_blank"">39:31</a> Pros and Cons of Orchestrators<br><h3>Transcript</h3><div class='max-width'><p>Today, we're going to be taking a look at multi-agent workflows in OpenAI's Agents SDK. Now, OpenAI's Agents SDK is the production version of their earlier open source package called Swarm. And what Swarm was focused on doing was literally building agent Swarm. So you could imagine that the successor of OpenAI's agentic Swarm package has fairly strong support for multi-agent systems, and that would be accurate.</p><p>Working multiple agents in Agents SDK is incredibly easy, works very well, and generally quite flexible. Now, within the SDK, there are two primary approaches that you might take to building a multi-agent system. The first of those, which is what we'll be focusing on today, is the orchestrator sub-agent pattern.</p><p>And the other is using agent handoffs, which we will not be covering in this video, but I will talk about in another video. So let's begin by taking a look at this orchestrator sub-agent pattern. Now, everything we are going to cover is going to be in various places. So we have this article on the Aurelio AI site.</p><p>This is a chapter in our upcoming Agents SDK course, and this covers both the orchestrator sub-agent pattern and also using handoffs. So this covers everything. So you can follow this, or alternatively, and I think this is where most of us will probably go, we can go to the Aurelio Labs Agents SDK course, go to chapters, and 0-4 multi-agent.</p><p>In here, we have all the code that I'm going to work through. I would recommend to open this in CodeLab. CodeLab is just the easiest to set up. So you would literally click this little button here. You click on this cell here. Run this, and you are set up and ready to go.</p><p>The other way that you can run this is locally. So you get cloned the repo, and there are set up instructions for how you can do this in the repo readme. And I'm actually going to be going and running all this locally, because I already have everything set up.</p><p>And it just looks nicer when everything is on my local code editor. So the first thing that we're going to do before we jump into the orchestrator sub-agent is we need to set our API key. So we're using OpenAI. So you set your OpenAI API key. You need to get this from the OpenAI platform, of course.</p><p>And you just paste your API key in the top here. Or I think it's actually below the cell if you're running this in CodeLab. So orchestrator sub-agent, what does that look like? I have this nice little visual here. So we can see here, we have this human orchestrator, and then we have these sub-agents below the orchestrator.</p><p>The orchestrator sub-agent pan I'm talking about here is where we have a main agent, i.e. the orchestrator. And essentially everything goes through this orchestrator. That is why it is the orchestrator. It is orchestrating everything that is going on when we are interacting with this agent workflow. The orchestrator is what we communicate with as a human.</p><p>Okay. So human, we send our question. That will come down to the orchestrator. The orchestrator is going to see that and the orchestrator will decide what to do. Will it refer to a sub-agent for some additional information? Will it respond directly? It can do either of those. It can also refer to multiple sub-agents.</p><p>So, for example, let's say our question is, based on NVIDIA's latest earnings, what is their PTE ratio right now? Okay. That could be an example. In that case, we would want to get the financial data from their latest earnings report using the web search sub-agent. We would get that information and then we would use that, pass it over to our code execution sub-agent, which would perform some calculations for us to calculate that ratio.</p><p>And then that would be passed back to the orchestrator and the orchestrator would pass that information back to us. Now, alternatively, if you say, hello, how are you, the orchestrator doesn't need to do anything. It doesn't need to go to any of these sub-agents. So it will say, okay, I'm just going to respond to the human directly.</p><p>There's no need to go down and run anything else here. Now, this is what the orchestrator sub-agent pattern looks like. Everything is controlled by the orchestrator. The sub-agents, they are essentially used as tools for that orchestrator. The sub-agents do not respond to the user directly and they only do something when the orchestrator tells them to do so.</p><p>So that is orchestrator sub-agent. Now, we're going to go ahead and actually build what we can see here. Okay, so we need to build this workflow. The first thing we're going to focus on is building these sub-agents. So we have a few agents here. We have the web search sub-agent, which you might have guessed is doing a web search for us.</p><p>It has access to a web search tool. Then we have the internal docs sub-agent. This is almost like a RAG agent, and I will talk a little more about that soon. And then we also have the code execution agent, which I'm mostly restricting to doing calculations for us in this example.</p><p>So let's go ahead and start looking at how we would build those in agents SDK. So starting with the web search sub-agent. The web search sub-agent will take a query from the orchestrator, which is in most cases going to be some form of the user's query, and use it to search a web.</p><p>The agent is going to collect various sources. I think by default, there's like 10 different sources I will collect. It's going to collate the information from those sources and then generate a single text response and pass that back to the orchestrator. Now, I considered using OpenAI's built-in web search tool for this, but to be completely honest, it is absolutely terrible.</p><p>So I'm not going to use it, and instead we're going to use another web search API called LinkUp. Now, LinkUp does require an account, but they give a certain amount of free credits when you sign up, and I think basically you have to use a lot of searches to use up those free credits.</p><p>So it's more than enough for what we're doing here. So let's go ahead and just go through and create a LinkUp account. Clicking on this link here, and you should get sent through to a sign-up page, most likely. Once you have signed up, you will get to your homepage here.</p><p>Your API key, you can just grab it here. Super easy. So I'm going to copy that. Switch back to my notebook. I'm going to run this cell, and I'm just going to enter my API key here. Okay, so now let's just test this quickly and see what we get.</p><p>I'm going to search for the latest world news. So that is running, and we should get a response fairly quickly there. So, yeah, we can see there's a pretty big object here, but we can just pass it out into something a bit more usable here. Okay, so I'm just looking at the first three results here.</p><p>But in reality, we have 10 results. Okay, so you see that we have the, we have like a title of the source. We have a link for that source, and then you also have like a, the content from that source as well. Now there is, so what I'm doing here is this standard search.</p><p>There is also a deep search, which you would use if you want like really detailed results. But I just want quick results here, so I'm going with standard. Cool. So we have that, and well, that's basically all our tool is going to be doing, at least the web search tool.</p><p>So I'm going to go ahead and create a function tool using this, the logic that we just put together. I am going to be using the async search method here, because in general, if I'm building an AI agent, I want to make sure everything is async. Especially those operations where you're waiting for an API because, okay, you're waiting for a response from some API request.</p><p>In that time, if you're not using async, your program is just going to be sat there doing nothing. If you instead are writing this asynchronously, your program can go and do other things whilst you are waiting for that API request response. So as what we do here, nothing really changes, you just have to await and make sure you're using async search within an async function.</p><p>Cool. So I actually want to remove this bit. I missed that before. And what we're doing here is we're going through those search results, parsing out in the format I showed you here, and just generating or building a single string from those results, which we then return. Now that we have that, let's go and define the agent that is going to be using this tool.</p><p>And that is our web search agent. So we have these instructions for how the agent should behave, how it should use the web search tool. And these are pretty generic. There's nothing complicated here. The only thing that I am specifying is I'm telling it, okay, once it has this required information, which we get from the tool, once we have those results, summarize those with cleanly formatted links, sourcing each bit of information that it uses when it creates a summary.</p><p>And I also ask it to use markdown formatting, because markdown formatting is just nicer to work with. And LMs are generally good at both reading and generating markdown. So it's a good standard, in my opinion. Now, we can talk to our agent and just confirm that this web search works.</p><p>And of course, the agent itself. So I'm going to ask you, how is the weather in Tokyo? Okay. So I get the current weather in Tokyo is around 18 degrees Celsius or 64 degrees Fahrenheit, if you want to be difficult, with partly cloudy skies. Seems like a pretty nice day.</p><p>And we also have the sources down here as well. And all of this, this is all in markdown. So that's why we have done this display in markdown. We could show the direct text. So if I show you this, this is what it actually looks like. Maybe I should print this.</p><p>This is what it actually looks like. Okay. So we have that markdown formatting there. Great. So now let's move on to the next subagent, which is our internal docs subagent. Now, this is a very common use case, especially in corporate environments, but honestly, just in many, many places. So when I say internal docs subagent, what this agent is intended to do is given a set of private information, right?</p><p>So that could be your own personal private information, or it could be the company that you work for. It could be all of their internal documentation, could be your team's wiki page, or something along those lines. Stuff that is not on the publicly accessible web, and therefore cannot be answered by a web search agent.</p><p>For these types of documentation or information, we very commonly see people using RAG, which is Retrieval Augmented Generation. Now, RAG is an incredibly performant way of augmenting your LLM with external information, i.e. information that your LLM does not already know from its pre-training or fine-tuning. It's generally very cost-effective, it's fast, it's a really good approach.</p><p>However, it does require a little bit of setup, despite being relatively simple. So we're not going to go through and build an entire RAG pipeline in this example, but instead, I'm just going to create almost a dummy RAG tool, which is going to return a specific document for us.</p><p>So, that document discusses the revenue figures for a wildly successful AI robotics company that we have set up. That company is called Skynet, and we have the revenue report here that you can read. So this is a separate markdown file. And you're seeing here, we've included specific bits of information that only our internal DOPS sub-agent will be able to give us.</p><p>So we're specifying that this is the Q1 revenue report for 2025, that it was released on April 2nd, 2025. And we have a small little executive summary, just tells us, okay, what is Skynet doing? Lots of lovely things. And it gives us a financial overview. So a little table that our LM is going to be able to read.</p><p>It tells us just, okay, what products do we have? The, like, the revenue from those various little bits of information, okay? And then we have some revenue insights, some forward guidance, okay? So it's like a nice, really simple revenue report. Now, if you are running this in Colab, you should download that document and put it in your DOPS for Colab.</p><p>Alternatively, you can actually download it directly from the repo. And what I will do is actually make sure I share a little snippet of code to do that so that you can just pull it directly from the repo. Or if you're running this locally and you clone the repo, this will work as it is.</p><p>So I'm going to run this. This is just going to be just loading in now our revenue report. And this becomes our RAG tool, okay? It's a fake RAG tool, just making that very clear. But it is our RAG type tool. So what it's going to do is when the LLM provides a query to search with, it is basically going to ignore that query because this is a fake search function.</p><p>And it is going to just return that one document, that financial report that I just showed you. So now what I want to do is we have our tool, our fake tool, and we're going to define our internal docs subagent. Now the internal docs subagent, again, a very simple instructions here, nothing complicated at all.</p><p>I'm just saying you have access to internal company documents. Once the user asks you questions about the company and you will use the provided internal docs to answer the question. Ensure you answer the question accurately and use markdown formatting. Similar instructions as to what we use with the other subagent.</p><p>So, yeah, we have that and we can test it. So I'm going to ask you, what was our revenue in quarter one? And you can see, okay, we have all these. And you can see that's given as a breakdown of each of the various units and how much revenue they provided.</p><p>And this is probably where that code execution subagent would be really useful. Because it can actually take all these, put them together. Although at the same time, I believe these numbers are not difficult to work with. So our LLM could probably put those together by itself. However, LLMs doing calculations is generally a bad idea.</p><p>They just hallucinate quite frequently, even for some relatively simple. Calculation. So it's always better to get your LLM to write code where it would calculate that it can use to actually calculate the, well, any calculation. But that all looks good. So let's move on to that code execution subagent.</p><p>Now, in our code execution subagent, for this example, we're focusing on relatively simple calculations. That is what we want the code execution agent to do here. But especially with more state-of-the-art LLMs, a code execution agent could write pretty good code for a lot of different use cases. I'm sure most of us are pretty familiar by now with AI code editors.</p><p>So I don't think I need to explain how good LLMs can be, especially on specific tasks. Which is, of course, what we'd be using the LLM for here. But I do like to be careful with what I'm giving an LLM when it comes to code execution, especially within a chat interface.</p><p>So with that in mind, this is the tool that our code execution agent will be using. You see that I've also explained to the, so if you provide a dark string to your function tool, that will tell the LLM what this tool should be used for and also how to use the tool.</p><p>And I've specifically told it here that the output must be assigned to a variable called result. You can, you can change this like output or something else. But I believe that at least for OpenAI models, they've actually been trained to output code execution results to something, a variable called result.</p><p>So I would stick with that. You can change this to output, but what I saw when doing this is that this subagent would very frequently run this tool and it would write it as if it is writing to a variable called output, even though I prompted it not to.</p><p>And then see that it fails and then do it again and then get it right. But why retry if we can just get it right the first time? So I would recommend doing this. So in here, I am saying, okay, here I'm just sharing, okay, what is the code that is going to create to you?</p><p>This is just for us to read it. We don't actually need this. We can remove it if we wanted to, but it's just for us to understand what is going on. Then I'm saying, you know, because code execution can be, it can work, it cannot work. It varies. I am putting this code execution within a tricep block.</p><p>And then within this tricep block, I'm setting the global variables for the execution scope that we're running here. Okay. So what this is doing is it's basically ensuring we're not running our code with any variables are either coming from this environment or some other environment, depending on where we're running this.</p><p>Okay. So we're making sure there are just no variables that already exist within this execution context. Then what is going to happen is this code is going to run inside this empty namespace. And then the empty namespace will gather all the variables that have been created within that code execution.</p><p>So we can actually get the result by accessing it like this. Okay. Which is pretty cool. So that is how I execute code tool. You will also notice here that you can see here, this is just a normal synchronous function. It's not an async function. The reason for that is that there's nothing in here that requires network calls or there's nothing here where our code is likely to be waiting.</p><p>Right. So with an API request, you are sending that request. You're waiting. In this case, everything is being run right here. So unless something within this code here causes a network request, there's not really any need, in my opinion, to make this async. So that's why it's not. Now we define our code execution sub-agent.</p><p>You can see here that I am using GPT 4.1 or 4.1 Mini. You can use Mini. To be honest, it doesn't really matter for this example. But I just want to show you that, one, you can use various LMs in different parts of your multi-agent setup, of course. That's one of the benefits of multi-agent setups.</p><p>And two, I just want to be extra careful because we're executing code. So I want, ideally, the best LM that is reasonably priced and reasonably fast for code execution. So that's why I've gone with 4.1 rather than Mini here. Then we just have the description. There's, again, nothing new or weird there.</p><p>So we can run that and then we can test our sub-agent. And I'm just going to ask you a nonsense question. But it's a nonsense question that you can apply math to. So let's see what it comes up with. Okay. So it's telling me this is what we're printing out from the function.</p><p>So the code date is going to execute. It's got number of apples. It has a number of bananas. This is the question I asked. Four apples, multiply them by 71 and 1 tenth batanas. So it has, it got that right. It has a number of apples right. And then just multiplying those together and it stores that result here.</p><p>Well, yeah. And then we are, of course, extracting that result out here. Okay. That information or that result gets returned to our LLM. And you can see that this is what we get from that. So it's 284.4. And because we're using LLM, it is telling us that we're not being very sensible here.</p><p>And it says, okay, the result is a mathematical product, but in real life, you can't multiply apples by bananas. Okay. So nice little bit of telling us we're not being logical there as well. So now we have our three subagents and we can move on to defining our orchestrator.</p><p>So as I mentioned a little bit earlier, the orchestrator is what is going to be controlling the inputs and outputs throughout our entire workflow. And the way that we can think of our subagents in this system is actually as tools. And in fact, the way that we implement our orchestrator connected to all these subagents is by turning those subagents into tools and then passing them into the tools parameter of our orchestrator.</p><p>So let's see what that looks like. So I'm going to come down to here. We have, I'm defining another tool here, by the way, just we'll see later why, but I also want to just show you that we can use tools as well as agents as tools here. So this is our orchestrator definition.</p><p>It is just an agent in the same way that we earlier defined our subagents. The main difference is one, its name is orchestrator. Two, the model may be different. In this case, I'm thinking, okay, I want this to be the reasoning engine like this is, I want this to be a good LM that is powering the orchestrator.</p><p>To be completely honest, you probably don't need it to be 4.1. You go for mini. But that is really up to you and your use case and what you need from it. Then what we do is we take our agents. These are the subagents that we have defined. And we use this as tool method to turn them into tools.</p><p>Now, when we use this as tool method, we also need to provide a tool name. And also a tool description. Okay. So this doesn't need to be anything complicated is just, okay, this is our tool name. I'm using the function name and it's worth noting that you cannot include white space.</p><p>So I could not do this, for example. So it has to, if you want spaces, you can't, you need underscores. Then, yeah, you're just giving us tool description. Okay. So this is telling your orchestrator when should it use this agent as a tool. Okay. So we have those tools and then we also have an actual tool.</p><p>Okay. So this get current date. Get current date is literally just a tool to get the current date solid. So that's what we have. We also have the orchestrator prompt that is just above. Let me show you that very quickly. So what I'm trying to do in this orchestrator prompt is give the orchestrator LLM context as to where it is.</p><p>Okay. It needs to know in what type of system is it in. And the reason we do that is if it knows what sort of system it is in, it will better understand, okay, why am I calling this agent tool? And, you know, what are all these things that are around me?</p><p>Right. So we're just giving it context so that it can operate better. We also tell it, okay, you're in the system and this is how you should operate. So what we're saying are you take the user's queries and pass them to the appropriate agent tools. The agent tools will see the input you provide and use it to get all the information that you need to answer the user's query.</p><p>We also want to say, like in my earlier example, where we were using the web search tool followed by the calculator tool, we also want to explicitly tell the orchestrator that it can call multiple agents, okay, to get all the information it needs. Then at the very end here, one thing that we just want to be very clear with, with the LLM, is that it shouldn't be drawing attention to the fact that this is a multi-agent system.</p><p>In some cases, maybe you would want that, but in this case, I want to build a conversational interface. I want users to come in and talk and all they really see is, okay, there's some chat interface here and I'm just talking and I have no idea what is really behind the scenes.</p><p>Maybe there's some information like, oh, I've got this from the web, some, you know, there's a source or I got this from the internal documents that I have access to. Maybe we want a little bit of that, but I don't really want the user to be being told by our orchestrator, hey, I just need to go and use the internal doc sub-agent because that's where I find my internal information from.</p><p>I don't really want it to go into that much depth as to what it's doing. I just want it to be conversational. So that is why we use this last sentence of do not mention or draw attention to the fact that this is a multi-agent system in your conversation with the user.</p><p>So that is our prompt. This is our orchestrator. And now if I run both, we can go ahead and just test our agent. So I'm going to say first, how long ago from today was it when we got our last revenue report? So there are a couple of things that need to happen here.</p><p>How long ago from today? So the orchestrator is going to need to find out, okay, what is the current day, which it can do using that get current date tool. Then once it has the current date, the orchestrator needs to find out when the last revenue report was released.</p><p>Now, if it is being prompted, well, the agent should understand, okay, although we didn't explicitly say that this is for the company we belong to. If we're talking to this internal company agent that has access to these internal company documents, probably the user is asking about that specific company and not just when were the last revenue reports in the entire world released.</p><p>So hopefully we should see that it doesn't use the web search tool to find whenever the last revenue report in the entire world was released. But instead, it should go into that internal DOPS subagent and get the information from there. So let's run that and we'll see what happens.</p><p>Now, one thing that is, it's kind of hard to see here. We don't know what is going on, right? I can see it's running. Okay. And it is showing that, okay, today is May 7, 2025. It is saying the last revenue report was for the quarter ending May 31st, 2025.</p><p>So that is correct because it's quarter one. It doesn't pick up on the April 2nd, but that might just be due to my question not being specific enough on when was the last revenue report release or versus when was the end date for that last revenue report. But this is an accurate answer.</p><p>We can see it's using the correct tools and the correct information from various places. But I don't actually know that that is the case, meaning that I know this information is accurate. I know this information is coming from somewhere. But how do I confirm that? And this is particularly important when we have more complex agents, where there's information coming from many different places.</p><p>And we as developers might not necessarily know what all of the correct information is. So what we can do in this scenario to have more insight into what has just happened, we can go to Tracer's dashboard in the OpenAI platform. So to do that, I'm going to go to platform, openai.com.</p><p>I will make sure I log in. Then I need to make sure I'm in the correct project. Okay, so I'm using, I am in the correct project. Here's the advocacy project. And you will need to go to dashboard on the right here. And you want to go to traces.</p><p>Now, traces, hopefully you can see them. If you're in a company and you're accessing the company's traces, there's a fairly good chance that maybe you can't see anything here. You can't see that traces dashboard. The reason for that is that the company administrator or owner needs to go into here and give and set the permissions for you to actually see their traces or logs dashboard, which they can do by going over to the settings, organization settings, data controls, and making sure that the logs here, which includes the traces, is visible, either for selected projects to everyone or something else that works and makes this visible for you.</p><p>So once you can see everything in your dashboard or traces dashboard specifically, you can go to your most recent trace, which should ideally be the one that you just ran. And we can see, okay, we have agent workflow. There were no handoffs. That's good because we didn't build handoffs into our workflow.</p><p>We can see the number of tools that we use, which is three. Interesting. And we can see the execution time was 13.73 seconds, which is long, but that is, we did use an extra tool or one more tool than we needed here. So let's go into this and see why that happened or at least have an idea of why that happened.</p><p>Okay. So we can see in here, we went in, so we had the orchestrator, started here. Then we went to this web search agent, and this took the majority of the time. It's like nine seconds, which is pretty long. And if we look at that, we can see that we had a post to, this is OpenAI, V1 responses, the responses API.</p><p>So this is the LLM generating something. And we can see that the input here, this is coming from the orchestrator, not actually the user. So the orchestrator is providing that date of last revenue report, and the LLM, based on this user message, has gone and decided, okay, we need to use the search web tool.</p><p>And we're going to provide it with this query, which is last revenue report date, okay? And this is, okay, we can see straight away, this has gone to the search web tool. So this is like, okay, we need to prompt a little better here in order to make it clearer to this agent that, or to our orchestrator agent, that this is a agent that might be used by a particular company.</p><p>And usually company questions or revenue questions and so on would be about the internal dots rather than the web, okay? And that will also explain why it told us that the revenue date was the 31st of May, okay? So this is really good for debugging. So, okay, we can see that was sent to the search web tool.</p><p>We got these results. And based on that, it's saying, okay, this is the current date. I know quarter one ends on the 31st of March. So it calculates how long ago that was after looking at the get current date tool here, okay? So it called the get current date tool.</p><p>And this is outside the web search agent, sorry. So web search agent returned to the orchestrator. The orchestrator got this here. Date of, this is coming from the web search agent, this output here. Then the orchestrator decided, okay, I need to use the get current date tool. It got that, got the output, and then it generated our final response.</p><p>So it did a lot of things. So we need to print it a little better, more likely than not, at least. So what we could do here is say, okay, you're orchestrated a multi-agent system. We'll just add a little bit here. Note that you are an assistant for the Skynet company.</p><p>If the user asks about company information, information, or finances, you should use our internal information rather than public information. And this should be enough to guide our observator in a bit of a better direction. So let's just try again and see what happens. Okay, and now we can see it is actually getting that right.</p><p>Also much faster. So if we come over to our traces again. This is our latest run, 8.5 seconds, much faster. And we can see that it went to the internal docs agent. It searched, got the response from the tool. And then this is the call from the LLM back up to the orchestrator.</p><p>So the, what did it give us? It said latest revenue report, Skynet Inc. Is this dated April 2nd, 2025. So this is what the internal docs agent or subagent provided back to the orchestrator. Great. So that is good. The final response, which would be from here, is actually this.</p><p>Okay, exactly the same as what we have in the notebook. Great. So that is one test. Let's try another one with our orchestrator. I'm going to say what is our current revenue and what percentage of revenue comes from the T1000 units. Okay. We'll see what tools or what agents, subagents, sorry, it decides to use this time.</p><p>Okay. So 11.2 seconds runtime. It looks roughly accurate. Let's switch across to our traces and see what happened. So use the internal docs agent. You see that I actually tried to use this internal docs agent twice. Probably it's trying to find some information that is not within like the dummy tool.</p><p>So it's like trying again and then realize, it probably realizes, oh, okay, this is useless. I'm not getting any more information. So then it gives up. But yeah, we got the current revenue. And from that, we got this information here. Then it decided it wants to find the percentage of revenue.</p><p>So it's trying to, rather than calculate itself, it's actually trying to search through our internal docs. to get that information. Okay. Which obviously slowed it down a little bit there. And then in the end, it actually did not, it did not try to use the calculator. Okay. But what did it come up with?</p><p>Let's see. Okay. It came up with this for the response, which is, I believe it's accurate anyway. So it could be better. It could have used the code execution sub-agent. And again, this is something where we probably want to prompt it a little better. But we got our answer.</p><p>So that is actually all I wanted to go through on the orchestrator sub-agent, multi-agent workflow with Agents SDK. As we've seen, there is a lot you can do with this, of course. What I just showed you is so a relatively simple pattern. There wasn't a whole lot going on.</p><p>There is definitely an argument to be made that, do we need sub-agents for those simple tasks? In this case, probably, potentially not, depending on what you're looking to do. I would say maybe for code execution, you should use a separate sub-agent. For the other ones, it depends, right? If you actually have an internal docs use case, it might benefit you to have that sub-agent because then you can prompt that sub-agent with additional context and information about how to use that internal docs tool, which can be really useful in just getting better results.</p><p>And the same is also true for the web search sub-agent. You can prompt it and give it more information about how to get the best results from your web search tool. So it really depends on what you're looking to do, how important latency is. One thing to be aware of with the orchestrator sub-agent pattern is everything is going through your orchestrator, okay?</p><p>Even the responses from the sub-agents. So in the scenario that you only need a single sub-agent to be used, let's say for a web search, the orchestrator sub-agent is not ideal because your user query goes from your user to the orchestrator. The orchestrator then needs to decide to use the web search sub-agent, which is then another LLM itself, right?</p><p>And then that other LLM is going to create the web search tool call, get that response. That LLM is going to generate another response, send it to your orchestrator, and then the orchestrator is going to generate yet another response and send that back to the person. It can be very slow, right?</p><p>We just had four different LLM generation steps. Whereas if it was the orchestrator, let's call it main agent in this scenario, going directly to a web search tool, it would be orchestrator generates the tool call that goes to your web search tool. The web search tool returns its response to the orchestrator.</p><p>The orchestrator generates a response based on information and sends it back to the user, which is just two LLM calls. Naturally, given that LLM calls tend to make up the bulk of our waiting time or our latency, naturally, the orchestrator sub-agent pattern in these scenarios where we're just expecting a single tool use is not ideal.</p><p>However, in this scenario where you do need these sub-agents because they just handle particular tasks better than you can do with a generic all-purpose agent, in that scenario, and for queries that require more than a single sub-agent to be used, the orchestrator sub-agent pattern is almost essential. The other option that you might consider is where you have an orchestrator with sub-agents, but then sub-agents can respond directly to the user.</p><p>In that scenario, again, it becomes more difficult to use multiple sub-agents. You could have the sub-agent look at the information that has been provided and decide whether to respond directly to the user or the orchestrator. That is completely possible, but you need to prompt it well and make sure all that is going to work.</p><p>So, this pattern can be good, but you do need to be careful with the latency, and it is generally better for those cases where latency is not super, super important. Although, that being said, you can still make it conversational. You just need to be smart about how many tokens you're using, what tools you're using, and which models you're using, of course, as well.</p><p>Smaller models are faster. So, that is all I wanted to cover in this video. So, thank you very much for watching. I hope all this has been useful and interesting, but for now, I'll leave it there. So, I will see you again in the next one. Bye. Bye. Bye.</p><p>Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. you you</p></div></div></body></html>