<html><head><title>Distinction between small & large models will go away – Sholto Douglas & Trenton Bricken</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Distinction between small & large models will go away – Sholto Douglas & Trenton Bricken</h2><a href="https://www.youtube.com/watch?v=AOt0GwY8IvU" target="_blank"><img src="https://i.ytimg.com/vi/AOt0GwY8IvU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>There's a future where like the distinction between small and large models like disappears to some degree. And with long context, there's also a degree to which fine tuning might disappear, to be honest. Like these, these two things that are very important today, and like today's landscape models, we have like whole different tiers of model sizes, and we have fine-tuned models of different things.</p><p>You can imagine a future where you just actually have a dynamic bundle of compute and like infinite context that specializes your model to different things. One of the bottlenecks for AI progress that many people identify is the inability of these models to perform tasks on long horizons, which means engaging with the task for many hours or even many weeks or months where like if I have, I don't know, an assistant or an employee or something, they can just do a thing and tell them for a while.</p><p>And AI agents haven't taken off for this reason, from what I understand. So how linked are long context windows and the ability to perform well on them and the ability to do these kinds of long horizon tasks that require you to engage with an assignment for many hours? Or are these unrelated concepts?</p><p>I mean, I would actually take issue with that being the reason that agents haven't taken off, where I think that's more about like lines of reliability and the model actually successfully doing things. If you just can't chain tasks successfully with high enough probability, then you won't get something that looks like an agent.</p><p>And that's why something like an agent might follow more of a step function like GPT-4 class models, Gemini Ultra class models are not enough. But maybe the next increment on model scale means that you get that extra nine, even though like the loss isn't going down that dramatically, that like small amount of extra ability gives you the extra.</p><p>Obviously, you need some amount of context to fit long horizon tasks, but I don't think that's been the limiting factor up to now. Do you expect that it will be multiple copies of models talking to each other? Or will it be just adapt a computer solve, then the thing just like runs bigger, like more compute when it needs to do a kind of thing that a whole firm needs to do.</p><p>And I asked this because there's two things that make me wonder about like whether agents is the right way to think about what will happen in the future. One is with longer context, these models are able to ingest and consider the information that no human can and therefore we need like one engineer who's thinking about the front-end code and one engineer who's thinking about the back-end code, where this thing can just ingest the whole thing.</p><p>This sort of like Hayekian problem of specialization goes away. Second, these models are just like very general of you're like not using different types of GPT-4 to do different kinds of things. You're using the exact same model, right? So I wonder what that implies is in the future, like an AI firm is just like a model instead of a bunch of AI agents hooked together.</p><p>That's a great question. I think especially in the near term, it will look much more like agents hooked together. And I say that like purely because as humans, we're going to want to have these like isolated, reliable and like, like, like components that we can trust. So if your claim is that the AI agents haven't taken off because of reliability rather than long horizon task performance, isn't the lack of reliability when a task is chained on top of another task on top of another task, isn't that exactly the difficulty with long horizon tasks?</p><p>Is that like you have to do 10 things in a row or a hundred things in a row and diminishing the reliability of any one of them, or the probability goes down from 99.99 to 99.9, then like the whole thing gets multiplied together and the whole thing becomes much less likely to happen.</p><p>One of the things that will be really important to do over the next however long is understand better what does success rate over long horizon tasks look like. And I think that's even important to understand what the economic impact of these models might be and like actually properly judge increasing capabilities, right, by cutting down the tasks that we do and the inputs and outputs involved into minutes or hours or days, and seeing how good it is successively chaining and completing tasks at those different resolutions of time, because then that tells you how automatable a job family or task family is in a way that like MMO use scores don't.</p><p>I mean, it was less than a year ago that we introduced 100k context windows. And I think everyone was pretty surprised by that. So yeah, everyone would just kind of had this soundbite of quadratic attention costs. Yeah, we can't have long context windows. Here we are. So yeah, like the benchmarks are being actively made.</p><p>One thing you can imagine is you have an AI firm or something. And the whole thing is like end to end trained on the signal of like, did I make profits or like, if that's like too ambiguous, if it's if it's an architecture firm, and they're making blueprints, did my client like the blueprints?</p><p>And in the middle, you can imagine agents who are salespeople and agents who are like doing the designing agents who like do the editing, whatever. Would that kind of signal work on an end to end system like that? Yeah, in the limit? Yes. That's the dream of reinforcement learning, right?</p><p>It's like, all you need to do is provide this extremely sparse signal. And then over enough iterations, you create the information that allows you to learn from that signal. But I don't expect that to be the thing that works first. I think this is going to require an incredible amount of care, and like diligence on the behalf of humans surrounding these machines, and making sure they do exactly the right thing and exactly what you want and giving them right signals to improve in the ways that you want.</p><p>Yeah, you can't train on the RL reward unless the model generates some reward. Yeah, that's Yeah, exactly. You're in like, you're in this like sparse RL world where like, if it never, the client never likes what you produce, then like you don't get any reward at all. And like, it's kind of bad.</p><p>But in the future, these models will be good enough to get the reward some of the time, right? This is the nines of reliability.</p></div></div></body></html>