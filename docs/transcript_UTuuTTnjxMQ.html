<html><head><title>Sholto Douglas & Trenton Bricken - How to Build & Understand GPT-7's Mind</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Sholto Douglas & Trenton Bricken - How to Build & Understand GPT-7's Mind</h2><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ" target="_blank"><img src="https://i.ytimg.com/vi_webp/UTuuTTnjxMQ/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=0 target="_blank"">0:0</a> Long contexts<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=1024 target="_blank"">17:4</a> Intelligence is just associations<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=2007 target="_blank"">33:27</a> Intelligence explosion & great researchers<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=4064 target="_blank"">67:44</a> Superposition & secret communication<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=5006 target="_blank"">83:26</a> Agents & true reasoning<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=5732 target="_blank"">95:32</a> How Sholto & Trenton got into AI research<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=7688 target="_blank"">128:8</a> Are feature spaces the wrong way to think about intelligence?<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=8524 target="_blank"">142:4</a> Will interp actually work on superhuman models<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=9957 target="_blank"">165:57</a> Sholto's technical challenge for the audience<br><a href="https://www.youtube.com/watch?v=UTuuTTnjxMQ&t=11089 target="_blank"">184:49</a> Rapid fire<br><h3>Transcript</h3><div class='max-width'><p>-You're failing the line test right now. Really bad. -Let's get no condoms on the chair. -Dude, it is literally foggy. -The video has shown it enough. But if you want to mess up now--  -OK, today I have the pleasure to talk with two of my good friends, Shilto and Trenton.</p><p>Shilto-- -You just mixed up. -I wasn't going to say anything. Let's do this in reverse. -How will I start it with my good friends? -Yeah, Gemini 1.5, the context is like, just wow. -Shit. Anyways, Shilto, Noam Brown, the guy who wrote the diplomacy paper, he said this about Shilto.</p><p>He said, "He's only been in the field for 1.5 years, but people in AI know that he was one of the most important people behind Gemini's success." And Trenton, who's an anthropic, works on mechanistic interoperability. And it was widely reported that he has solved alignment. -With his one friend on Twitter.</p><p>-So this will be a capabilities-only podcast. Alignment is already solved, so no need to discuss further. OK, so let's start by talking about context links. -Yep. -It seemed to be under-hyped, given how important it seems to me to be that you can just put a million tokens into context.</p><p>There's apparently some other news that got pushed to the front for some reason. But yeah, tell me about how you see the future of long-context links and what that implies for these models. -Yeah. So I think it's really under-hyped, because until I started working on it, I didn't really appreciate how much of a step-up in intelligence it was for the model to have the onboarding problem basically instantly solved.</p><p>And you can see that a little bit in the perplexity graphs in the paper, where just throwing millions of tokens' worth of context about a code base allows it to become dramatically better at predicting the next token in a way that you'd normally associate with huge increments in model scale.</p><p>But you don't need that. All you need is a new context. So under-hyped, and buried by some other news. -In context, are they as sample efficient and smart as humans? -I think that's really worth exploring. Because, for example, one of the evals that we did in the paper has it learning a language in context better than a human expert could learn that new language over the course of a couple months.</p><p>And this is only a pretty small demonstration. But I'd be really interested to see things like Atari games, or something like that, where you throw in a couple hundred, like 1,000 frames, labeled actions, and then in the same way that you'd show your friend how to play a game, and see if it's able to reason through.</p><p>It might, at the moment, with the infrastructure and stuff, it's still a little bit slow at doing that. But I would guess that might just work out of the box in a way that would be pretty mind-blowing. -And crucially, I think this language was esoteric enough that it wasn't in the training data.</p><p>-Right, exactly. If you look at the model before it has that context thrown in, it doesn't know the language at all, and it can't get any translations. -And this is an actual human language, not just -- -Yeah, exactly, an actual human language. -So if this is true, it seems to me that these models are already, in an important sense, superhuman.</p><p>Not in the sense that they're smarter than us, but I can't keep a million tokens in my context when I'm trying to solve a problem, remembering and integrating all the information in an entire code base. Am I wrong in thinking this is, like, a huge unlock? -I actually generally think that's true.</p><p>Like, previously, I've been frustrated when models aren't as smart. Like, you ask them a question, and you want it to be smarter than you, or to know things that you don't. And this allows them to know things that you don't in a way that it just ingests a huge amount of information in a way you just can't.</p><p>So, yeah, it's extremely important. -Well, how do we explain in-context learning? -Yeah, so there's a piece of -- there's a line of work I quite like where it looks at in-context learning as basically, like, very similar to gradient descent, but, like, the attention operation can be viewed as gradient descent on the in-context data.</p><p>That paper had some cool plots where they basically showed, like, we take n steps of gradient descent, and that looks like n layers of in-context learning. It looks very similar, so I think, like, that's one way of viewing it and trying to understand what's going on. -Yeah, and you can ignore what I'm about to say because, given the introduction, alignment is solved and safety isn't a problem.</p><p>But I think the context stuff does get problematic, but also interesting here. I think there'll be more work coming out in the not-too-distant future around what happens if you give a 100-shot prompt for jailbreaks, adversarial attacks. It's also interesting in the sense of, if your model is doing gradient descent and learning on the fly, even if it's been trained to be harmless, you're dealing with a totally new model, in a way.</p><p>You're, like, fine-tuning, but in a way where you can't control what's going on. -Can you explain what do you mean by gradient descent is happening in the forward pass and attention? -Yeah. -You probably have a better... -No, no, no. There was something in the paper about trying to teach the model to do linear regression.</p><p>-Right. -But, like, just through the number of samples they gave in the context. -Yeah. -And you can see, if you plot on the x-axis, like, number of shots that it has, or examples, and then, like, the loss it gets on just, like, ordinary least-squares regression. -Yeah. -That will go down with time.</p><p>-And it goes down exactly matched with number of gradient descent steps. -Yeah. Yeah, exactly. -Okay. I only read the interim discussion section of that paper, but in the discussion, the way they framed it is that in order to get better at long-context tasks, the model has to get better at learning to learn from these examples or from the context that is already within the window.</p><p>And the implication of that is the model, if, like, meta-learning happens because it has to learn how to get better at long-context tasks, then, in some important sense, the task of intelligence is, like, requires long-context examples and long-context training. -Like, meta-learning, like, you have to induce meta-learning. -Right. -Like, understanding how to better induce meta-learning in your pre-training process is, like, a very important thing to actually get that, like, flexible or adaptive intelligence.</p><p>-Right, but you can proxy for that just by getting better at doing long-context tasks. One of the bottlenecks for AI progress that many people identify is the inability of these models to perform tasks on long horizons, which means engaging with a task for many hours or even many weeks or months, where, like, if I have, I don't know, an assistant or an employee or something, they can just do a thing I tell them for a while.</p><p>And AI agents haven't taken off for this reason, from what I understand. So, how linked are long-context windows and the ability to perform well on them and the ability to do these kinds of long-horizon tasks that require you to engage with an assignment for many hours? Or are these unrelated concepts?</p><p>-I mean, I would actually take issue with that being the reason that agents haven't taken off, where I think that's more about, like, nines of reliability and the model actually successfully doing things. If you just can't chain tasks successfully with high enough probability, then you won't get something that looks like an agent.</p><p>And that's why something like an agent might follow more of a step function in sort of, usually, like, GPT-4 class models, Gemini Ultra class models, they're not enough. But maybe, like, the next increment on model scale means that you get that extra nine, even though, like, the loss isn't going down that dramatically, that, like, small amount of extra ability gives you the extra nine.</p><p>And, like, yeah, obviously, you need some amount of context to fit long-horizon tasks, but I don't think that's been the limiting factor up to now. -Yeah. The NURBS Best Paper this year by Ryland Schaefer was the lead author, points to this as, like, the emergence of Mirage, where people will have a task and you get the right or wrong answer depending on if you've sampled the last five tokens correctly.</p><p>And so, naturally, that's -- you're multiplying the probability of sampling all of those. And if you don't have enough nines of reliability, then you're not going to get emergence. And all of a sudden, you do, and it's like, "Oh, my gosh, this ability is emergent," when actually it was kind of almost there to begin with.</p><p>-And there are ways that you can find, like, a smooth metric for that. -Yeah, Human Eval or whatever, in the GPT-4 paper, the coding problems, they measure -- -Log-pass, right? -Exactly. -Yeah. -For the audience, the context on this is, basically, the idea is you want to -- when you're measuring how much progress there has been on a specific task, like solving coding problems, you up-weight it when it gets it right only one in 1,000 times.</p><p>You don't, like, give it a one-in-1,000 score because it's like, "Oh, got it right some of the time," and so the curve you see is, like, it gets it right one in 1,000, then one in 100, then one in 10, and so forth. So, actually, I want to follow up on this.</p><p>So, if your claim is that the AI agents haven't taken off because of reliability rather than long-horizon task performance, isn't the lack of reliability when a task is chained on top of another task on top of another task, isn't that exactly the difficulty with long-horizon tasks? Is that, like, you have to do 10 things in a row or 100 things in a row, and diminishing the reliability of any one of them, or, yeah, the probability goes down from 99.99 to 99.9, then, like, the whole thing gets multiplied together, and the whole thing becomes much less likely to happen.</p><p>- That is exactly the problem, but the key issue you're pointing out there is that your base task-solve rate is 90%, and if it was 99%, then chaining doesn't become a problem. - But also-- - Yeah, exactly. And I think this is also something that just, like, hasn't been properly studied enough.</p><p>If you look at all of the evals that are commonly-- like, the academic evals are a single problem, right? You know, like, the math problem. It's, like, one, like, typical math problem, or MMOU, it's, like, one university-level, like, problem from across different topics. You are beginning to start to see evals looking at this properly via more complex tasks, like SuiteBench, where they take a whole bunch of GitHub issues, and that is, like, a reasonably long-horizon task, but it's still not a multi-- it's, like, a multi, you know, sub-hour as opposed to, like, multi-hour or multi-day task.</p><p>And so I think one of the things that will be really important to do over the next however long is understand better what does success rate over a long-horizon task look like. And I think that's even important to understand what the economic impact of these models might be, and, like, actually properly judge increasing capabilities by, like, cutting down the tasks that we do and the inputs and outputs involved into minutes or hours or days, and seeing how good it is at successively, like, chaining and completing tasks at those different resolutions of time.</p><p>Because then that tells you, like, how automateable a job family or task family is in a way that, like, MMOU school is doing. - Mm. - I mean, it was less than a year ago that we introduced 100K context windows, and I think everyone was pretty surprised by that.</p><p>So, yeah, everyone just kind of had this soundbite of quadratic attention costs, and we can't have long context windows, and here we are. So, yeah, like, the benchmarks are being actively made. - Wait, wait, so doesn't the fact that there's these companies, Google and, I don't know, Magic, maybe others, who have million token attention imply that the quadrat-- You shouldn't say anything, 'cause you're not allowed.</p><p>But doesn't that, like, imply that it's not quadratic anymore, or are they just eating the cost? - Well, like, who knows what Google is doing for its long context scheme, right? Like, I'm not saying it's either. One other thing that's frustrated me about, like, the general research field's approach to attention is that there's an important way in which the quadratic cost of attention is actually dominated in typical dense transformers by the MLP block.</p><p>So, you have this N-squared term that's associated with attention, but you also have an N-squared term that's associated with the D model, the residual stream dimension of the model. And if you look, I think Sasha Rush has a great tweet where he looks like basically plots the curve of the cost of attention, respective to, like, the cost of, like, really large models, and attention actually trails off.</p><p>And you actually need to be doing pretty long context before that term becomes, like, really important. And the second thing is that people often talk about how attention at inference time is such a huge cost, right? And if you think about when you're actually generating tokens, the operation is not N-squared.</p><p>It is one Q, like, one set of Q vectors looks up a whole bunch of KB vectors, and that's linear with respect to the amount of, like, context that the model has. And so, I think this drives a lot of the, like, recurrence and state space research where people have this meme of, oh, like, linear attention and all this stuff.</p><p>And as Trenton said, there's, like, a graveyard of ideas around attention. And not to think I don't think it's worth exploring, but I think it's important to consider where the actual, like, strengths and weaknesses of it are. - Okay, so what do you make of this take? As we move forward through the takeoff, more and more of the learning happens in the forward pass.</p><p>So originally, like, all the learning happens in the backward, you know, during, like, this, like, bottom-up sort of hill-climbing evolutionary process. If you think in the limit during the intelligence explosion, it just, like, the AI is, like, maybe, like, handwriting the weights or, like, doing go-fi or something. And we're in, like, the middle step where, like, a lot of learning happens in context now with these models.</p><p>A lot of it happens within the backward pass. Does this seem like a meaningful gradient along which progress is happening? Of, like, how much, 'cause the broader thing being the, if you're learning in the forward pass, it's, like, much more sample efficient 'cause you can kind of, like, basically think as you're learning.</p><p>Like, when humans, when you read a textbook, you're not just skimming it and trying to absorb what, you know, what inductive, these words follow these words. You, like, read it and you think about it, and then you read some more, you think about it. I don't know. Does this seem like a sensible way to think about the progress?</p><p>- Yeah. It may just be one of the ways in which, like, you know, birds and planes, like, fly, but they fly slightly differently. And, like, the virtue of technology allows us to do that, like, I used to accomplish things that birds can't. It might be that context length is similar in that it allows it to have a working memory that we can't, but functionally is not, like, the key thing towards actual reasoning.</p><p>The key step between GPT-2 and GPT-3 was that all of a sudden, like, there was this meta-learning behavior that was observed in training, like, in the pre-training of the model. And that's, as you said, like, it's something to do with, you give it some amount of context, it's able to adapt to that context, and that was a behavior that wasn't really observed before that at all.</p><p>And maybe that's a mixture of property of context and scale and this kind of stuff, that wouldn't have occurred to model tiny context, I would say. - This is actually an interesting point. So when we talk about scaling up these models, how much of it comes from just making the models themselves bigger?</p><p>And how much comes from the fact that during any single call, you are using more compute? So if you think of diffusion, you can just iteratively keep adding more compute. And if adaptive compute is solved, you can keep doing that. And in this case, if there's a quadratic penalty for attention, but you're doing long context anyways, then you're still dumping in more compute during, not during training, or not during having bigger models, but just like, yeah.</p><p>- Yeah, it's interesting, 'cause you do get more forward passes by having more tokens. - Right. - My one gripe, I guess I have two gripes with this, though. Maybe three. So one, like, in the output. - In the AlphaMold paper, one of the transformer modules, they have a few, and the architecture is very intricate.</p><p>But they do, I think, five forward passes through it, and will gradually refine their solution as a result. You can also kind of think of the residual stream, I mean, Sholto alluded to kind of the read-write operations, as like a poor man's adaptive compute, where it's like, I'm just gonna give you all these layers, and if you wanna use them, great.</p><p>If you don't, then that's also fine. And then people will be like, oh, well, the brain is recurrent, and you can do however many loops through it you want. And I think, to a certain extent, that's right. If I ask you a hard question, you'll spend more time thinking about it, and that would correspond to more forward passes.</p><p>But I think there's a finite number of forward passes that you can do. It's kind of with language, as well. People are like, oh, well, human language can have infinite recursion in it, infinite nested statements of like, the boy jumped over the bear that was doing this, that had done this, that had done that.</p><p>But empirically, you'll only see five to seven levels of recursion, which kind of relates to whatever that magic number of how many things you can hold in working memory at any given time is. And so, yeah, it's not infinitely recursive, but does that matter in the regime of human intelligence?</p><p>And can you not just add more layers? - Breakdown, for me, you're referring to this in some of your previous answers of, listen, you have these long contexts, and you can hold more things in memory, but ultimately, it comes down to your ability to mix concepts together to do some kind of reasoning.</p><p>And these models aren't necessarily human level at that, even in context. Breakdown, for me, how you see storing just raw information versus reasoning and what's in between. Like, where is the reasoning happening? Is that, where is it just like, storing raw information happening? What's different between them in these models?</p><p>- Yeah, I don't have a super crisp answer for you here. I mean, obviously, with the input and output of the model, you're mapping back to actual tokens, right? And then, in between that, you're doing higher level processing. - Before we get deeper into this, we should explain to the audience, you referred earlier to Anthropic's way of thinking about transformers as these read-write operations that layers do.</p><p>One of you should just kind of explain at a high level what you mean by that. - So the residual stream, imagine you're in a boat going down a river, and the boat is kind of the current query where you're trying to predict the next token. So it's the cat sat on the blank.</p><p>- Right. - And then you have these little streams that are coming off the river where you can get extra passengers or collect extra information if you want. And those correspond to the attention heads and MLPs that are part of the model. - Right. I almost think of it like the working memory of the model, like the RAM of the computer, where you're choosing what information to read in, so you can do something with it, and then maybe you'd read something else in later on.</p><p>- And you can operate on subspaces of that high dimensional vector. A ton of things are, I mean, at this point, I think it's almost given that are encoded in superposition, right? So it's like, yeah, the residual stream is just one high dimensional vector, but actually there's a ton of different vectors that are packed into it.</p><p>- Yeah, I might just dumb it down as a way that would have made sense to me a few months ago. Okay, so you have, you know, whatever words are in the input you put into the model, all those words get converted into these tokens, and those tokens get converted into these vectors.</p><p>And basically it's just like this small amount of information that's moving through the model. And the way you explained it to me, Sholta, this paper talks about is early on in the model, maybe it's just doing some very basic things about like, what do these tokens mean? Like if it says like 10 plus five, just like moving information about to have the, have that-- - Good representation.</p><p>- Exactly, just represent. And in the middle, maybe like the deeper thinking is happening about like how to think, yeah, how to solve this. At the end, you're converting it back into the output token 'cause the end product is, you're trying to predict the probability of the next token from the last of those residual streams.</p><p>And so, yeah, it's interesting to think about like, just like the small compressed amount of information moving through the model, and it's like getting modified in different ways. Trenton, so you're, it's interesting. You're one of the few people who have like background from neuroscience. So you can think about the analogies here to the brain.</p><p>And in fact, I have one of our friends, the way he, he had a paper in grad school about thinking about attention in the brain. And he said, this is the only or first, what, like neural explanation of why attention works. Whereas we have evidence from why the CNNs work, convolutional neural networks work based on the visual cortex or something.</p><p>Yeah, I'm curious, do you think in the brain, there's something like a residual stream of this compressed amount of information that's moving through and it's getting modified as you're thinking about something? Even if that's not what's literally happening, do you think that's a good metaphor for what's happening in the brain?</p><p>- Yeah, yeah. So at least in the cerebellum, you basically do have a residual stream where the whole, what we'll call the attention module for now, and I can go into whatever amount of detail you want for that. You have inputs that route through it, but they'll also just go directly to the like end point that that module will contribute to.</p><p>So there's a direct path and an indirect path. And so the model can like pick up whatever information it wants and then add that back in. - Well, what happens to the cerebellum? - So the cerebellum nominally just does fine motor control, but I analogize this to the person who's lost their keys and is just looking under the streetlight where it's very easily to observe this behavior.</p><p>One leading cognitive neuroscientist said to me that a dirty little secret of any fMRI study where you're looking at brain activity for a given task is that the cerebellum is almost always active and lighting up for it. If you have a damaged cerebellum, you also are much more likely to have autism.</p><p>So it's associated with like social skills. And one of these particular studies where I think they use PET instead of fMRI, but when you're doing next token prediction, the cerebellum lights up a lot. Also 70% of your neurons in the brain are in the cerebellum. They're small, but they're there and they're taking up real metabolic cost.</p><p>- This is one of Guerin's points that like what changed with humans was not just that we have more neurons or he says he shared this article, but specifically there's more neurons in the cerebral cortex in the cerebellum and you should say more about this, but like they're more metabolically expensive and they're more involved in signaling and sending information back and forth.</p><p>- Yeah. - Is that attention what's going on? - Yeah, yeah. So I guess the main thing I wanna communicate here. So back in the 1980s, Pente Canerva came up with a associative memory algorithm for I have a bunch of memories, I wanna store them. There's some amount of noise or corruption that's going on and I want to query or retrieve the best match.</p><p>And so he writes this equation for how to do it. And a few years later realizes that if you implemented this as an electrical engineering circuit, it actually looks identical to the core cerebellar circuit. And that circuit and the cerebellum more broadly is not just in us, it's in basically every organism.</p><p>There's active debate on whether or not cephalopods have it, they kind of have a different evolutionary trajectory, but even fruit flies with the Drosophila mushroom body, that is the same cerebellar architecture. And so that convergence and then my paper, which shows that actually this operation is to a very close approximation, the same as the attention operation, including implementing the softmax and having this sort of like nominal quadratic costs that we've been talking about.</p><p>And so the three-way convergence here and the takeoff and success of transformers just seems pretty striking to me. - Yeah, I wanna zoom out and ask, I think what motivated this discussion in the beginning was we were talking about like, wait, what is the reasoning? What is the memory?</p><p>What do you think about the analogy you found to attention and this? Do you think of this as more just looking up the relevant memories or the relevant facts? And if that is the case, where is the reasoning happening in the brain? How do we think about how that builds up into the reasoning?</p><p>- Yeah, so maybe my hot take here, I don't know how hot it is, is that most intelligence is pattern matching and you can do a lot of really good pattern matching if you have a hierarchy of associated memories. So you start with your very basic associations between just like objects in the real world.</p><p>But you can then chain those and have more abstract associations such as like a wedding ring symbolizes like so many other associations that are downstream. And so, and you can even generalize the attention operation and this associated memory as the MLP layer as well. It's in a long-term setting where you don't have like tokens in your current context.</p><p>But I think this is an argument that like association is all you need. And associated memory in general as well, it's not, so you can do two things with it. You can both denoise or retrieve a current memory. So like if I see your face, but it's like raining and cloudy, I can denoise and kind of like gradually update my query towards my memory of your face.</p><p>But I can also access that memory and then the value that I get out actually points to some other totally different part of the space. And so a very simple instance of this would be if you learn the alphabet, right? And so I query for A and it returns B, I query for B and it returns C and you can traverse the whole thing.</p><p>Yeah. - Yeah, one of the things I talked to Demis about was he had a paper in 2008 that memory and imagination are very linked because of this very thing that you mentioned of memory is reconstructive. And so you are in some sense imagining every time you're thinking of a memory 'cause you're only storing a condensed version of it and you're like have to.</p><p>And this is famously why human memory is terrible and like why people in the witness box or whatever would just make shit up. Okay, so let me ask a stupid question. So you like reach Sherlock Holmes, right? And like the guy's incredibly sample efficient. He'll like see a few observations and he'll like basically figure out who committed the crime 'cause there's a series of deductive steps that leads from somebody's tattoo and what's on the wall to the implications of that.</p><p>How does that fit into this picture? 'Cause like crucially what makes him smart is that there's not like an association but there's a sort of deductive connection between different pieces of information. Would you just explain it as that's just like higher level association? Like, yeah. - I think so, yeah.</p><p>So I think learning these higher level associations to be able to then map patterns to each other as kind of like a meta learning. I think in this case, he would also just have a really long context length or a really long working memory, right? Where he can like have all of these bits and continuously query them as he's coming up with whatever theory.</p><p>So that the theory is moving through the residual stream. And then his attention heads are querying his context but then how he's projecting his query and keys in the space and how his MLPs are then retrieving like longer term facts or modifying that information is allowing him to then in later layers do even more sophisticated queries and slowly be able to reason through and come to a meaningful conclusion.</p><p>- That feels right to me in terms of like looking back in the past, you're selectively reading in certain pieces of information, comparing them. Maybe that informs your next step of like what piece of information you now need to pull in. And then you build this representation which I like progressively looks closer and closer and closer to like the suspect in your case.</p><p>- Yeah. - Yeah. - That doesn't feel at all outlandish. - Do you have a lens on like suspects? (all laughing) - Something I think that the people who aren't doing this research can overlook is after your first layer of the model, every query key and value that you're using for attention comes from the combination of all the previous tokens.</p><p>So like my first layer, I'll query my previous tokens and just extract information from them. But all of a sudden, let's say that I attended to tokens one, two, and four in equal amounts. Then the vector in my residual stream, assuming that they wrote out the same thing to the value vectors, but ignore that for a second, is a third of each of those.</p><p>And so when I'm querying in the future, my query is actually a third of each of those things. And so-- - But they might be written to different subspaces. - That's right. Hypothetically, but they wouldn't have to, right? And so you can recombine and immediately, even by layer two and certainly by the deeper layers, just have like these very rich vectors that are packing in a ton of information.</p><p>And the causal graph is like literally over every single layer that happened in the past. And that's what you're operating on, yeah. - It does bring to mind like a very funny eval to do, would be like a Sherlock Holmes eval, let's say you put the entire book into context.</p><p>And then you have like a sentence which is like, the suspect is like X, then you have like a larger probability distribution over like the different characters in the book. And then like as you put more-- - That would be super cool, yeah, yeah, yeah. - I wonder if you'd get anything at all, but it'd be cool.</p><p>- Sherlock Holmes is probably already in the training data. - Yeah. - You're gonna get like a mystery novel that was written in the-- - You can get an LLM to write it. - Or you could purposely exclude it, right? - Oh, you can? How do you-- - Well, you need to scrape any discussion of it from Reddit or any other thing, right?</p><p>- Right, it's hard. But that's like one of the challenges that goes into things like long context evals is to get a good one, you need to know that it's not in your training data. You just like put in the effort to exclude it. - What, so I actually wanna, there's two different threads I wanna follow up on.</p><p>Let's go to the long context one and then we'll come back to this. So in the Gemini 1.5 paper, the eval that was used was, can it like, something with Paul Graham essays, can it like-- - Yeah, the needle in a haystack. - Right. Which, yeah, I mean, there's like, we don't necessarily just care about its ability to recall one specific fact from the context.</p><p>I'll step back and ask the question. The loss function for these models is unsupervised. You don't have to like come up with these bespoke things that you keep out of the training data. Is there a way you can do a benchmark that's also unsupervised? Where, I don't know, another LLM is rating it in some way or something like that.</p><p>And maybe the answer is like, well, if you could do this, like reinforcement learning would work 'cause then you have this like unsupervised. - Yeah, I mean, I think people have explored that kind of stuff. Like, for example, Anthropica is the constitutional RL paper where they take another language model and they point it and say like, how helpful or harmless was that response?</p><p>And then they get it to update and try and improve along the prior frontier of helpfulness and harmfulness. So you can like point language models at each other and create evals in this way. It's obviously an imperfect art form at the moment because you get reward function hacking basically and the language, like if you try and match up to what, even humans are imperfect here.</p><p>Like if you try and match up to what humans will say, humans typically prefer longer answers, which aren't necessarily better answers and you get the same behavior with models. - On the other thread, going back to the Sherlock Holmes thing. If it's all associations all the way down, this is a sort of like naive dinner party question.</p><p>If I just like match you, I'm working on AI. But okay, does that mean we should be less worried about super intelligence? 'Cause there's not this sense in which it's like Sherlock Holmes plus plus. It'll still need to just like find these associations, like humans find associations and like, you know what I mean?</p><p>It's not just like, it sees a frame of the world and it's like figured out all the laws of physics. - So for me, 'cause this is a very legitimate response, right? It's like, well, artificial general intelligence aren't, if you say humans are generally intelligent, then they're no more capable or competent.</p><p>I'm just worried that you have that level of general intelligence in Silicon, where you can then immediately clone hundreds of thousands of agents and they don't need to sleep and they can have super long context windows and then they can start recursively improving and then things get really scary.</p><p>So I think to answer your original question, yes, you're right. They would still need to learn associations, but. - Well, but the recursive self-improvement would still have to be them. Like if intelligence is fundamentally about these associations, like the improvement is just them getting better at association. There's not like another thing that's happening.</p><p>And so then it seems like you might disagree with the intuition that, well, they can't be that much more powerful if they're just doing associations. - Well, I think then you can get into really interesting cases of meta-learning. Like when you play a new video game or like study a new textbook, you're bringing a whole bunch of skills to the table to form those associations much more quickly.</p><p>And like, because everything in some way ties back to the physical worlds, I think there are like general features that you can pick up and then apply in novel circumstances. - Should we talk about intelligence explosion then? I don't know if that's a good idea. - That was a really good sign to jump in.</p><p>- I mentioned multiple agents and I'm like, "Oh, here we go." Okay, so the reason I'm interested in discussing this is with you guys in particular is the models we have of the intelligence explosion so far come from economists, which is fine, but I think we can do better because in the model of the intelligence explosion, what happens is you replace the AI researchers and then there's like a bunch of automated AI researchers who can speed up progress, make more AI researchers, make further progress.</p><p>And so I feel like if that's the metric or that's the mechanism, we should just ask the AI researchers about whether they think this is plausible. So let me just ask you, like if I have a thousand Asian Shotos or Asian Trentons, are they just, do you think that you get an intelligence explosion?</p><p>Is that, yeah, what does that look like to you? - I think one of the important bounding constraints here is compute. Like I do think you could dramatically speed up AI research, right? Like it seems very clear to me that in the next couple of years, we'll have things that can do many of the software engineering tasks that I do on a day-to-day basis and therefore dramatically speed up my work and therefore speed up like the rate of progress, right?</p><p>At the moment, I think most of the labs are somewhat compute bound in that they're always, there are more experiments you could run and more pieces of information that you could gain in the same way that like scientific research on biology is also somewhat experimentally like throughput bound. Like you need to be able to run and culture the cells in order to get the information.</p><p>I think that will be at least a short-term bounding constraint. Obviously, you know, Sam's trying to raise $7 trillion to buy chips. And so like, it does seem like there's going to be a lot more compute in future as everyone is heavily ramping. NVIDIA's stock price sort of represents the relative compute increase.</p><p>But any thoughts? - I think we need a few more nines of reliability in order for it to really be useful and trustworthy. Right now it's like, and just having context lengths that are super long and it's like very cheap to have. Like if I'm working in our code base, it's really only small modules that I can get Claude to write for me right now.</p><p>But it's very plausible that within the next few years or even sooner, it can automate most of my tasks. The only other thing here that I will note is the research that at least our sub team in interpretability is working on is so early stage that you really have to be able to make sure everything is done correctly in a bug-free way and contextualize the results with everything else in the model.</p><p>And if something isn't going right, be able to enumerate all of the possible things and then slowly work on those. Like an example that we've publicly talked about in previous papers is dealing with layer norm, right? And it's like, if I'm trying to get an early result or look at like the logit effects of the model, right?</p><p>So it's like, if I activate this feature that we've identified to a really large degree, how does that change the output of the model? Am I using layer norm or not? How is that changing the feature that's being learned? And that will take even more context or reasoning abilities for the model.</p><p>- So you used a couple of concepts together and it's not self-evident to me that they're the same, but it seemed like you were using them interchangeably. So I just wanna, like one was, well, to work on the cloud code base and make more modules based on that. They need more context or something where like, it seems like they might already be able to fit in the context.</p><p>Or do you mean like actual, do you mean like the context window context or like more? - Yeah, the context window context. - So yeah, it seems like now it might just be able to fit. The thing that's preventing it from making good modules is not the lack of being able to put the code base in there.</p><p>- I think that will be there soon. - Yeah. But like, it's not gonna be as good at you as you at like coming up with papers because it can like fit the code base in there. - No, but it will speed up a lot of the engineering. - Hmm.</p><p>In a way that causes an intelligence explosion? - No, that accelerates research. But I think these things compound. So like the faster I can do my engineering, the more experiments I can run. And then the more experiments I can run, the faster we can. I mean, my work isn't actually accelerating capabilities at all.</p><p>- Right, right. - It's just like interpreting the models. But we have a lot more work to do on that. (laughing) Surprise to the Twitter, (laughing) I mean, for context, like when you released your paper, there was a lot of talk on Twitter about alignment is solved guys, close the curtains.</p><p>(laughing) - Yeah, yeah, no, it keeps me up at night how quickly the models are becoming more capable and like just how poor our understanding still is of what's going on. - Yeah, I guess I'm still... Okay, so lessening through the specifics here. By the time this is happening, we have bigger models that are two to four orders of magnitude bigger, right?</p><p>Or at least an effective compute are two to four orders of magnitude bigger. And so this like idea that, well, you can run experiments faster or something. You're having to retrain that model in this version of the intelligence explosion. Like the recursive self-improvement is different from what might've been imagined 20 years ago where you just rewrite the code.</p><p>You actually have to train a new model and that's really expensive. Not only now, but especially in the future as you keep like making these models orders of magnitude bigger. Doesn't that dampen the possibility of a sort of recursive self-improvement type intelligence explosion? - It's definitely gonna act as a breaking mechanism.</p><p>I agree that the world of like what we're making today looks very different to what people imagined it would look like 20 years ago. Like it's not gonna be able to write the same code to be like really smart because actually it needs to train itself. Like the code itself is typically quite simple, typically fairly small and self-contained.</p><p>I think John Carmack had this nice phrase where it's like it's like the first time in history where like you can actually plausibly imagine writing AI with like 10,000 lines of code. And that like actually does seem plausible when you pair most training code bases down to the limit.</p><p>But it doesn't take away from the fact that this is something we should really strive to measure and estimate like how progress might occur. Like we should be trying very, very hard right now to measure exactly how much of a software engineer's job is automatable and what the trend line looks like and be trying our hardest to project out those trend lines.</p><p>- But with all due respect to software engineers, like you are not like writing a React front end, right? - Right. - So it's like, I don't know how this, like what is concretely happening? And maybe you can walk me through, walk me through like a day in the life of, like you're working on an experiment or project that's going to make the model quote unquote better.</p><p>- Right. - Like what is happening from observation, to experiment, to theory, to like writing the code, what is happening? - And so I think one to contextualize here is that like I've primarily worked on inference so far. So a lot of what I've been doing is just taking or helping guide the pre-training process, such as we design a good model for inference and then making the model and like the surrounding system faster.</p><p>I've also done some pre-training work around that, but that hasn't been like my 100% focus, but I can still describe what I do when I do that work. - I know, but sorry, let me interrupt and say-- - So it's like two types of work, yeah. - In Karl Schulman's, like when he was talking about it on the podcast, he did say that things like improving inference or even literally having like better, helping it make better chips or GPUs, that's like part of the intelligence explosion.</p><p>- Yeah. - 'Cause like obviously if the inference code runs faster, like it happens better or faster or whatever. - Right. - Anyway, sorry, go ahead. - Yeah. So what does concretely a day look like? I think the most important like part to illustrate is this cycle of coming up with an idea, proving it out at different points in scale and like interpreting and understanding what goes wrong.</p><p>And I think most people would be surprised to learn just how much goes into interpret, like interpreting and understanding what goes wrong. 'Cause the ideas, people have long lists of ideas that they want to try, not every idea that you think should work will work and trying to understand why that is, is quite difficult.</p><p>And like working out what exactly you need to do to interrogate it. So, so much of it is like introspection about what's going on. It's not pumping out thousands and thousands and thousands of line of code. It's not like the difficulty in coming up with ideas even. I think many people have a long list of ideas that they want to try, but paring that down and shock calling under very imperfect information, what the right ideas to explore further is really hard.</p><p>- Tell me more about, what do you mean by imperfect information? Are these early experiments? Are these, like, what is the information that you're- - So, so Demis mentioned this in his podcast. And also like, you obviously, it's like the GPT-4 paper where you have like scaling law increments and you can see like in the GPT-4 paper, they have like a bunch of like dots, right?</p><p>Where they say we can estimate the performance of our final model, like using all of these dots and there's a nice curve that like flows through them. And Demis mentioned that we do this process of scaling up. Concretely, why is that imperfect information? Is you never actually know if the trend will hold.</p><p>For certain architectures, the trend has held really well. And for certain changes, it's held really well. But that isn't always the case. And things which can help at smaller scales can actually hurt at larger scales. So making guesses based on what the trend lines look like and based on like your intuitive feeling of, okay, this is actually something that's going to matter.</p><p>Particularly for those ones which help at the small scale. - That's interesting to consider that for every chart you see in a release paper or technical report that shows that smooth curve, there's a graveyard of like first neurons and then it's like flat or something. - Yeah, there's all these like other lines that go in like different directions, off like Taylor or something.</p><p>- Yeah, it's crazy. Both like as a grad student and then also here. Like the number of experiments that you have to run before getting like a meaningful result. - Tell me, okay, so you, but presumably it's not just like you run it until it stops and then like, let's go to the next thing.</p><p>There's some process by which to interpret the early data and also to look at your, like, I don't know. I could like put a Google Doc in front of you and I'm pretty sure you could just like keep typing for a while on like different ideas you have. And there's some bottleneck between that and just like making the models better immediately.</p><p>- Right. - Yeah, walk me through like what is the inference you're making from the first early steps that makes you have better experiments and better ideas? - I think one thing that I didn't fully convey before was that I think a lot of like good research comes from working backwards from the actual problems that you want to solve.</p><p>And there's a couple of like grand problems I suppose in like making the models better today that you would identify as issues and then like work from, okay, how could I like change it to achieve this? There's also a bunch of, when you scale, you run into things and you want to like fix behaviors or like issues at scale.</p><p>And that like informs a lot of the research for the next increment and this kind of stuff. So concretely the barrier is a little bit software engineering, like often having a code base that's large and sort of capable enough that it can support many people doing research at the same time makes it complex.</p><p>If you're doing everything by yourself, your iteration pace is going to be much faster. I've heard that like Alec Radford, for example, like famously did much of the pioneering work at OpenAI. He like mostly works out of like a Jupiter notebook and then like has someone else who like writes and productionizes that code for him.</p><p>I don't know if that's true or not. But like that kind of stuff, like actually operating with other people makes it, raises the complexity a lot because not for natural reasons, like familiar to like every software engineer. And then the inherent running, like running and launching those experiments is easy, but there's inherent time, like slows downs induced by that.</p><p>So you often wanna be paralyzing multiple different streams 'cause one, you can't like be totally focused on one thing necessarily. You might not have like fast enough feedback cycles. And then intuiting what went wrong is actually really hard. Like working out what, like this is in many respects the problem that the team that Trenton is on is trying to better understand is like what is going on inside these models.</p><p>We have inferences and understanding and like head canon for why certain things work, but it's not an exact science. And so you have to constantly be making guesses about why something might've happened, what experiment might reveal, whether that is or isn't true. And that's probably the most complex part.</p><p>The performance work by comparatively is easier, but harder in other respects. It's just a lot of low level and like difficult engineering work. - Yeah, I agree with a lot of that. But even on the interpretability team, I mean, especially with Chris Ola leading it, there are just so many ideas that we wanna test.</p><p>And it's really just having the engineering skill, but I'll put engineering in quotes because a lot of it is research to like very quickly iterate on an experiment, look at the results, interpret it, try the next thing, communicate them, and then just ruthlessly prioritizing what the highest priority things to do are.</p><p>- And this is really important. Like the ruthless prioritization is something which I think separates a lot of like quality research from research that doesn't necessarily succeed as much. We're in this funny field where so many of our initial theoretical understanding is like broken down basically. And so you need to have this simplicity bias and like ruthless prioritization over what's actually going wrong.</p><p>And I think that's one of the things that separates the most effective people is they don't necessarily get like too attached to using a given solution that they're necessarily familiar with, but rather they attack the problem directly. You see this a lot in like maybe people coming with a specific academic background, they try and solve problems with that toolbox.</p><p>And the best people are people who expand the toolbox dramatically. They're running around and they're taking ideas from reinforcement learning, but also from optimization theory. And also they have a great understanding of systems. And so they know what the sort of constraints that bound the problem are. And they're good engineers.</p><p>They can iterate and try ideas fast. Like by far the best researchers I've seen, they all have the ability to try experiments really, really, really, really, really fast. And that is that cycle time at smaller scales, cycle time separates people. - I mean, machine learning research is just so empirical.</p><p>- Yeah. - And this is honestly one reason why I think our solutions might end up looking more brain-like than otherwise. It's like, even though we wouldn't want to admit it, the whole community is kind of doing like greedy evolutionary optimization over the landscape of like possible AI architectures and everything else.</p><p>It's like no better than evolution. And that's not even necessarily a slight against evolution. - That's such an interesting idea. I'm still confused on what will be the bottleneck for these, what would we have to be true of an agent such that it's like sped up your research? So in the Alec Radford example you gave where he apparently already has the equivalent of like co-pilot for his Jupyter notebook experiments.</p><p>Is it just that if he had enough of those he would be a dramatically faster researcher and so you just need Alec Radford? So it's like, you're not automating the humans, you're just making the most effective researchers who have great taste more effective and like running the experiments for them and so forth or like you're still working at the point which the intelligence explosion is happening.</p><p>You know what I mean? Like, is that what you're saying? - Right. And if that were like directly true, why can't we scale out current research teams better? For example, I think an interesting question for us, like why, if this work is so valuable, why can't we take hundreds or thousands of people who are like, they're definitely out there and like scale our organizations better?</p><p>I think we are less at the moment bound by the sheer engineering work of making these things than we are by compute to run and get signal and taste in terms of what the actual like right thing to do it and that like making those difficult inferences on imperfect information.</p><p>- For the Gemini team. 'Cause I think for interpretability, we actually really want to keep hiring talented engineers and I think that's a big bottleneck for us to just keep making a lot of progress. - Obviously more people is like better, but I do think like it's interesting to consider, I think like one of the biggest challenges that like I've thought a lot about is how do we scale better?</p><p>Like Google is an enormous organization and has 200,000 ish people, right? Like a hundred, maybe 80,000 or something like that. And one has to imagine if there were like ways of scaling out Gemini's research program to all those fantastically talented software engineers. And this seems like a key advantage that you would want to be able to take advantage of, you want to be able to use, but like how do you effectively do that?</p><p>It's a very complex organizational problem. - So compute and taste, that's interesting to think about because at least the compute part is not bottlenecked on more intelligence. It just bottlenecked on Sam 7 trillion or whatever, right? So if I gave you 10X the H100s to run your experiments, how much more effective a researcher are you?</p><p>- TPUs please. (all laughing) - How much more effective a researcher are you? - I think the Gemini program would probably be like maybe five times faster with 10X more compute or something like that. - So that's pretty good elasticity of like 0.5. - Yeah. - Wait, that's insane.</p><p>- Yeah, I think like more compute would just like directly convert into progress. - So you have some fixed size of compute and some of it goes to in friends, some of, I guess like, and also like to clients of GCP. - Yep. - Some of it goes to, huh?</p><p>(all laughing) Some of it goes to training and I guess as a fraction of it, some of it goes to running the experiments for the full model. - Yeah, that's right. - Shouldn't then the fraction goes to experiments be higher given that you would just be like, if like the bottleneck is research and research is bottlenecked by compute.</p><p>- And so one of the strategic decisions that every pre-training team has to make is like exactly what amount of compute do you allocate to your different training runs? Just like to your research program versus like scaling the last best, I like, you know, thing that you landed on.</p><p>And I think they're like, they're all trying to arrive at like a sort of pre-optimal point here. One of the reasons why you need to still keep training big models is that you get information there that you don't get otherwise. So scale has all these emergent properties which you want to understand better.</p><p>And if you like are always doing research and never, like remember what I said before about like, you're not sure what's gonna like fall off the curve, right? If you like keep doing research in this regime and like keep on getting more and more compute efficient, you may never, you may have actually like gone off the path to actually eventually scale.</p><p>So you need to constantly be investing in doing big runs too at the frontier of what you sort of expect to work. - Okay, so then tell me what it looks like to be in the world where AI has significantly sped up AI research. 'Cause from this, it doesn't really sound like the AIs are going off and writing the code from scratch and that's leading to faster output.</p><p>It sounds like they're really augmenting the top researchers in some way. Like, yeah, tell me concretely, are they doing the experiments? Are they coming up with the ideas? Are they just like evaluating the outputs of the experiments, what's happening? - So I think there's like two walls you need to consider here.</p><p>One is where AI has meaningfully sped up our ability to make algorithmic progress. And one is where the output of the AI itself is the thing that's like the crucial ingredient towards like model capability progress. And like specifically what I mean there is-- - Synthetic data. - Like synthetic data, right?</p><p>And in the first world where it's meaningfully speeding up algorithmic progress, I think a necessary component of that is more compute. And you probably like reach this elasticity point where like AIs maybe at some point are easier to speed up and get on to context than yourself. That's just right than other people.</p><p>And so AIs meaningfully speed up your work because they're like a fantastic co-pilot basically that helps you code multiple times faster. And that seems like actually quite reasonable. Super long context, super smart model, it's onboarded immediately and you can like send them off and to like complete subtasks and sub goals for you.</p><p>And that actually like feels very plausible, but again, we don't know because there are no great evals about that kind of thing. But like the best one is as I said before, SweetBench. - Although in that one, somebody was mentioning to me, like the problem is that when a human is trying to do a pull request, they'll like type something out and they'll like run it and see if it works.</p><p>And if it doesn't, they'll rewrite it. None of this was part of the opportunities that the LLM was given when run on this, like it just like output it. And if it runs and like checks all the boxes, then it passed, right? So it might've been an unfair test in that way.</p><p>So you can imagine that is like, if you were able to use that, that would be an effective training source for having, like the key thing that's missing from a lot of training data is like the reasoning traces. Right? And I think this would be, if I wanted to try and automate a specific field with like job family, or like understand how like at risk of automation that is, then having reasoning traces feels to me like a really important part of that.</p><p>- There's so many threads. Yeah. There's so many different threads and that I want to follow up on. Let's begin with the data versus like, yeah, compute thing of like, is the output of these AIs a thing that's causing the intelligence delusion or something? - Yeah. - People talk about how these models are really a reflection on their data.</p><p>- Yeah. - I think there was, I forgot his name, but there was a great blog by this open AI engineer. And he was talking about at the end of the day, as these models get better and better, it just like, they're just going to be really effective, like maps of the data set.</p><p>- Yeah. - And so it's like, at the end of the day, like you got to stop thinking about architectures. It's like the most effective architecture, it's just like doing an amazing job of mapping the data. - Right. - So that implies that future AI progress comes from the AI just making really awesome data, right?</p><p>Like that you're mapping to. - Yeah, that's clearly a very important part, yeah. - Yeah, that's really interesting. Does that look to you like, I don't know, like things that look like chain of thought or what do you imagine as these models get better, as these models get smarter, what does the synthetic data look like?</p><p>- When I think of really good data, to me that raises something which involves a lot of reasoning to create. So in modeling that, it's just similar to like Ilya's perspective on trying, on achieving like super intelligence by effectively like perfectly modeling the human textual output, right? But even in the near term, in order to model something like the archive papers or Wikipedia, you have to have an incredible amount of reasoning behind you in order to understand what next token might be being output.</p><p>And so for me, what I imagine as good data is like data where you can similarly, at least like where it had to do reasoning to produce something. And then like the trick of course is, how do you verify that that reasoning was correct? And this is why you saw like DeepMind do that geometry, like the sort of like self-life of geometry basically, or like the sort of research for your geometry.</p><p>This geometry is a really, it's an easily formalizable, easily verifiable field. So you can check if it's reasoning was correct and you can generate heaps of data of correct, of verified geometry proofs, train on that. And you know that that's good data. - It's actually funny, 'cause I had a conversation with Grant Sanderson like last year where we were debating this and I was like, fuck dude, by the time they get the gold of the Math Olympiad, of course they're gonna automate all the jobs.</p><p>(laughing) - Yikes. - On this synthetic data thing, one of the things I speculated about in my scaling post, which was heavily informed with discussions with you too. (laughing) And you especially Shoto, was you can think of like human evolution through the spectrum of like we get language and so we're like generating the synthetic data, which like our copies are generating the synthetic data, which we're trained on.</p><p>And it's like this really effective genetics, a cultural like co-evolutionary loop. - And there's a verifier there too, right? Like there's the real world. You might generate a theory about, the gods cause the storms, right? And then like someone else finds cases where that isn't true. And so you like know that, that sort of didn't match your verification function.</p><p>And now like actually instead you have like some weather simulation, which required a lot of reasoning to produce and like accurately matches reality. And like you can train on that as a better model of the world. Like we are training on that and like stories and like scientific theories.</p><p>- Yeah. I wanna go back. I'm just remembering something you mentioned a little while ago of, given how sort of like empirical ML is, it really is evolutionary process as resulting in better performance and not necessarily an individual coming up with a breakthrough in like a top-down way. That has interesting implications.</p><p>First being that, there really is, people are like are concerned about capabilities increasing because more people are going into the field. I've somewhat been skeptical of that way of thinking, but from this perspective of just like more input, it really does. Yeah, it feels more like, oh, actually by like the fact that more people are going to ICML means that there's like faster progress towards GPT-5.</p><p>- Yeah, you just have more genetic recombination. - Right. - And like shots on target. - Yeah. - And I mean, aren't all fields kind of like that? Like this is the sort of scientific framery of like discovery versus invention, right? And discovery almost involves like whenever there's been a massive scientific breakthrough in the past, typically there are multiple people co-discovering that at like roughly the same time.</p><p>And that feels to me at least a little bit like the mixing and trying of ideas. You can't try an idea that's so far out of scope that you have no way of verifying it with the tools you have available. - Yeah, I think physics and math might be slightly different in this regard.</p><p>But especially for biology or any sort of wetware and to the extent we want to analogize neural networks here, it's just, it's comical how serendipitous a lot of the discoveries are. - Yeah. - Penicillin, for example. - Another implication of this is this idea that like AGI just gonna come tomorrow.</p><p>Like somebody's just gonna discover a new algorithm and we have AGI. That seems less plausible. Like it will just be a matter of more and more and more researchers finding these marginal things that all add up together to make models better, right? Like, yeah, that feels like the correct story to me, yeah.</p><p>- Especially while we're still hardware constrained. - Right. - Do you buy this narrow window framing of the intelligence explosion of, you have to each, you know, GPT-3 to GPT-4 is two ooms of orders of magnitude more compute, or at least more effective compute, in the sense that if you didn't have any algorithmic progress it would have to be two orders of magnitude bigger, like the raw form to be as good.</p><p>Do you buy the framing that given that you have to be two orders of magnitude bigger at every generation, if you don't get AGI by GPT-7, that can help you catapult the intelligence explosion? Like you're kind of just fucked as far as like much smarter intelligences go, and you're kind of stuck with GPT-7 level models for a long time.</p><p>'Cause at that point you're just like consuming significant fractions of the economy to make that model. And we just don't have the wherewithal to like make GPT-8. - This is the Carl Schulman sort of argument of like, we're gonna race through the orders of magnitude in the near term, but then longer term it would be harder.</p><p>I think like he's probably talked about it a lot, but yeah, but I do buy that framing. - Yeah, I mean, I generally buy that increases in order of magnitude of compute by like, in an absolute terms almost like diminishing returns on like your capability, right? Like we've seen over a couple of orders of magnitude models go from being unable to do anything to be able to like do huge amounts.</p><p>And it feels to me like each incremental order of magnitude like gives more nines of reliability of things. And so it unlocks things like agents, but at least at the moment I haven't seen like transformatively, it doesn't feel like reasoning improves like linearly, so to speak, but rather like somewhat sub-linearly.</p><p>- That's actually a very bearish sign because one of the things we're chatting with one of our friends and he made the point that if you look at what new applications are unlocked by GPT-4 relative to GPT-3.5, it's not clear that's like that much, like a GPT-3.5 can do perplexity or whatever.</p><p>So if there is this diminishing increase in capabilities and that increased cost exponentially more to get, that's actually a bearish sign on like what 4.5 will be able to do or what 5 will unlock in terms of economic impact. - That being said, for me, the jump between 3.5 and 4 is like pretty huge.</p><p>And so like, even if I, it's like another 3.5 to 4 jump is like ridiculous, right? Like if you imagine 5 as being a 3.5 to 4 jump like straight off the bat in terms of like ability to do SATs and this kind of stuff. - The LSAT performance was particularly striking.</p><p>- Exactly. You go from like very smart, like from like not super smart to like very smart to like utter genius in the next generation instantly. And it doesn't, at least like to me, feel like we're gonna sort of jump to utter genius in the next generation, but it does feel like we'll get very smart plus lots of reliability.</p><p>And then like we'll see TBD, what that continues to look like. - Will Go-Fi be part of the intelligence explosion? Where like you say synthetic data, but like, in fact, it will be like it writing its own source code in some important way. There was an interesting paper that you can use diffusion to like come up with model weights.</p><p>I don't know how like legit that was or whatever, but like, I don't know, something like that. Can you, so Go-Fi is good old fashioned AI, right? And can you define that? 'Cause when I hear it, I think like if-else statements for like symbolic logic. - Sure. (all laughing) I actually wanna make sure we like don't, like we like fully unpack the whole like model improvement increments.</p><p>- Yeah. - 'Cause I don't want people to come away with the perspective that like, actually this is super bearish and like models aren't gonna get much better and stuff. - Okay. - More what I wanna emphasize is like, the jumps that we've seen so far are huge. And even if those continue on like a smaller scale, we're still in for extremely smart, like very reliable agents, like over the next couple of orders of magnitude.</p><p>And so like, we didn't sort of fully close the thread on the narrow window thing. When you think of like, let's say, GPT-4 cost, I know let's call it $100 million or whatever. You have what, the 1B run, the 10B run, the 100B run, all seem very plausible by, you know, private company standards.</p><p>And then the- - You mean in terms of dollar? - In terms of dollar, yeah. And then you can also imagine even like a 1T run being part of like a national consortium or like a national level thing, but much harder on the behalf of an individual company. But Sammy is out there trying to raise $7 trillion, right?</p><p>Like he's already preparing for like, a whole order of magnitude more than the- - Right, he's shifted the overton window. - He's shifting the orders of magnitude here beyond the national level. So I wanna point out that one, we have a lot more jumps. And even if those jumps are relatively smaller, that's still a pretty stark improvement in capability.</p><p>- Not only that, but if you believe claims that GPT-4 is around 1 trillion parameter count. I mean, the human brain is between 30 and 300 trillion synapses. And so that's obviously not a one-to-one mapping and we can debate the numbers, but it seems pretty plausible that we're below brain scale still.</p><p>- So crucially the point being that the algorithmic overhead is really high in the sense that, and maybe this is something we should touch on explicitly, of even if you can't keep dumping more compute beyond the models that cost a trillion dollars or something, the fact that the brain is so much more data efficient implies that if you get, we have the compute, if we had like the brain's algorithm to train, if we could like train as a sample efficient as humans train from birth, we could make the AGI.</p><p>- Yeah, but the sample efficiency stuff, I never know exactly how to think about it because obviously a lot of things are hardwired in certain ways, right? And they're like the co-evolution of language and the brain structure. So it's hard to say. Also, there are some results that if you make your model bigger, it becomes more sample efficient.</p><p>- Yeah, the original scaling was paper, we had that, right? The logic models are almost empty. - Right, so maybe that also just solves it. Like you don't have to be more data efficient, but if your model's bigger then you also just are more data efficient. - Well, how do we think about, yeah, what is like the explanation or why that would be the case?</p><p>Like a bigger model just sees the exact same data at the end of seeing that data, it's learn more from it. Is there more space to represent it? - I mean, my like very naive take here would just be that like, so one thing that the superposition hypothesis that interpretability has pushed is that your model is dramatically under parameterized.</p><p>And that's typically not the narratives that deep learning is pursued, right? But if you're trying to train a model on like the entire internet and have it predict it with incredible fidelity, you are in the under parameterized regime and you're having to compress a ton of things and take on a lot of noisy interference in doing so.</p><p>And so having a bigger model, you can just have cleaner representations that you can work with. - Yeah, for the audience, you should unpack why that, first of all, what superposition is and why that is the implication of superposition. - Sure, yeah. So the fundamental result, and this was before I joined Anthropic, but the paper's titled "Toy Models of Superposition" finds that even for small models, if you are in a regime where your data is high dimensional and sparse, and by sparse, I mean any given data point doesn't appear very often, your model will learn a compression strategy, which we call superposition, so that it can pack more features of the world into it than it has parameters.</p><p>And so the sparsity here is like, and I think both of these constraints apply to the real world and modeling internet data is a good enough proxy for that, of like, there's only one Dworkash, like there's only one shirt you're wearing, there's like this liquid death can here. And so these are all objects or features and how you define a feature is tricky.</p><p>And so you're in a really high dimensional space 'cause there are so many of them and they appear very infrequently. And in that regime, your model will learn compression. To riff a little bit more on this, I think it's becoming increasingly clear, I will say, I believe that the reason networks are so hard to interpret is because in a large part, this superposition.</p><p>So if you take a model and you look at a given neuron in it, right? A given unit of computation and you ask, how is this neuron contributing to the output of the model when it fires? And you look at the data that it fires for, it's very confusing.</p><p>It'll be like 10% of every possible input or like Chinese, but also fish and trees and the word, a full stop in URLs, right? But the paper that we put out towards monosemanticity last year shows that if you project the activations into a higher dimensional space and provide a sparsity penalty.</p><p>So you can think of this as undoing the compression in the same way that you assumed your data was originally high dimensional and sparse. You return it to that high dimensional and sparse regime, you get out very clean features and things all of a sudden start to make a lot more sense.</p><p>- Okay, there's so many interesting threads there. The first thing I wanna ask is the thing you mentioned about these models are trained in a regime where they're over-parameterized, isn't that when you have generalization, like grokking happens in that regime, right? So- - Isn't that what you want? - So I was saying the models were under-parameterized.</p><p>- Oh, I see, okay. - Yeah, yeah. Like typically people talk about deep learning as if the model is over-parameterized, but actually the claim here is that they're dramatically under-parameterized given the complexity of the task that they're trying to perform. - Another question, so the distilled models, like first of all, okay, so what is happening there?</p><p>'Cause the earlier claims we're talking about is the smaller models are worse at learning than bigger models, but like GPT-4 Turbo, you could say make the claim that actually GPT-4 Turbo is worse at reasoning style stuff than GPT-4, but probably knows the same facts, like the distillation got rid of like some of the reasoning things.</p><p>- Yeah, do we have any evidence that GPT-4 Turbo is a distilled version of 4? It might just be a new architecture. - Oh, okay. - Yeah. Like it could just be like a faster, more efficient new architecture. - Okay, interesting. - So that's cheaper, yeah. - What is the, how do you like interpret what's happening in distillation?</p><p>I think Gwern had one of these questions on his website, why can't you train the distilled model directly? Why does it have to go through? Is it a picture like you had to project it from this bigger space to a smaller space? - I mean, I think both models will still be using superposition, but the claim here is that you get a very different model if you distill versus if you train from scratch.</p><p>- Yeah. - And it's just more efficient or it's just fundamentally different in terms of performance? - I don't remember, but like, do you know? I think like the traditional story for why distillation is more like efficient is that normally during training, you're trying to predict this like one hot vector that says like, this is the token that you should have predicted.</p><p>And if you're like reasoning process means that you're really far off predicting that, then I see that like, you still get these gradient updates that yeah, are in the right direction, but like you're totally, it might be really hard for you to learn, to have learned to predict that in the context that you're in.</p><p>And so what distillation does is it doesn't just have the one hot vector, it has like the full readout from the larger model, like of all of the probabilities. And so you get more signal about what you should have predicted. It's not, in some respects, it's like showing a tiny bit if you're working too.</p><p>- Yeah. - Like it's not just, this was the answer, it's-- - I see, yeah, yeah, yeah, totally. - But that makes a lot of sense. - It's kind of like watching a Kung Fu Master versus being in the matrix and like just downloading the program. - Yeah, exactly, exactly.</p><p>Yep, yep, just to make sure the audience got that. When you're training on a distilled model, you're like, you see all its probabilities over the tokens it was predicting and then over the ones you were predicting and then you like update through all those probabilities rather than just seeing the last word and updating on that.</p><p>Okay, so this actually raises a question I was intending to ask you. Right now, I think you were the one who mentioned you can think of chain of thought as adaptive compute of like, to step back and explain what, by adaptive compute, it's, the idea is, one of the things you would want models to be able to do is if a question is harder, to spend more cycles thinking about it.</p><p>And so then how do you do that? Well, there's only a finite and predetermined amount of compute that one forward pass implies. So if there's like a complicated reasoning type question or math problem, you want to be able to spend a long time thinking about it, then you do chain of thought where the model just like thinks through the answer and you can think about it as like all those forward passes where it's like thinking through the answer, it's like being able to dump more compute into solving the problem.</p><p>Now, going back to the signal thing, when it's doing chain of thought, it's only able to transmit that token of information where it's like, as you were talking about, the residual stream is already a compressed representation of everything that's happening in the model. And then you're turning the residual stream into one token, which is like log of 50,000 or log of vocab size bits, which is like, yeah, so tiny.</p><p>So I don't think it's quite only transmitting like that one token, right? Like if you think about it during a forward pass, you create these like KV values in the transform forward pass, that then like future steps attend to the KV values. And so all of those pieces of KV, of like the keys and values, are bits of information that you could use in the future.</p><p>- Is the claim that when you find two non-chain of thought, the way the key and value weights change so that the sort of steganography can happen in the KV cache? - I don't think I could make that strong a claim just- - But that sounds plausible. - But it's like, that's a good head canon for why it works.</p><p>And I don't know if there's any like papers explicitly demonstrating that or anything like that, but like that's at least one way that you can imagine the model has over the, like during pre-training, right? The model's trying to predict these future tokens. And one thing that you can imagine it doing is learning to like smoosh information about potential futures into like the keys and values that it might want to use in order to predict future information.</p><p>Like it kind of smooths that information across time and the pre-training thing. So I don't know if like people are particularly training, like training on chains of thought. I think the original chain of thought paper had that as like almost an immersion property of the model is you could like prompt it to do this kind of stuff and it still worked pretty well.</p><p>But that's like, yeah, it's a good head canon for why that works. - Yeah, to be overly panentic here, it's like the tokens that you actually see in the chain of thought do not necessarily at all need to correspond to the vector representation that the model gets to see when it's deciding to attend back to those tokens.</p><p>- Exactly, exactly. In fact, like during training, you replace, like what a training step is, is you're actually replacing the token, the model output with the real next token. And yet it's still like learning 'cause it has all this information internally. Like when you're getting a model to produce at inference time, like you're taking the output, the token that it output, you're feeding it in the bottom, unembedding it, and it like becomes the beginning of the new residual string.</p><p>- Right. - And then you use the output of past KBs to like read into and adapt that residual string. At training time, you do this thing called teacher forcing, basically, where you're like, actually, the token you were meant to output is this one. That's how you do it in parallel, right?</p><p>'Cause you have all the tokens, you put them all in in parallel and you do the giant forward pass. And so the only information it's getting about the past is the keys and values. It never sees the token that it output. - It's kind of like, it's trying to do the next token prediction.</p><p>And if it messes up, then you just give it the correct answer. - Yeah, right, right, yeah. Okay, that makes sense. - 'Cause otherwise it can become totally derailed. - Yeah, it'd go like off the train tracks. - How much like this sort of secret communication with the model to its forward inferences, how much steganography and like secret communication do you expect there to be?</p><p>- We don't know. Like honest answer, we don't know. But I wouldn't even necessarily like classify it as like secret information, right? Like a lot of the work that Trent's team is trying to do is actually understand that these are fully visible from the model side and from like this, maybe not the user, but like we should be able to understand and interpret what these values are doing and the information that are transmitting.</p><p>I think that's a really important goal for the future. - Yeah, I mean, there are some wild papers though where people have had the model do train of thought and it is not at all representative of what the model actually decides its answer is. And you can go in and edit.</p><p>No, no, no. In this case, like you can even go in and edit the train of thought so that the reasoning is like totally garbled and it will still output the true answer. - But also the train of thought, like yeah, it gets a better answer at the end of the train of thought rather than not doing it at all.</p><p>So like something useful is happening, but still the useful thing is not human understandable. - I think in some cases you can also just ablate the train of thought and it would have given the same answer anyways. - Interesting. Interesting. - So I'm not saying this is always what goes on, but like there's plenty of weirdness to be investigated.</p><p>- It's like a very interesting to go and look at and try and understand, I would say. - Yeah. - That you can do with open source models. And like, I think I wish there was more of this kind of interpretability and understanding work done on open models. - Yeah.</p><p>I mean, even in our AnthropX recent sleeper agents paper, which at a high level for people unfamiliar is basically I train in a trigger word. And when I say it, like if I say, if it's the year 2024, the model will write malicious code instead of otherwise. And they do this attack with a number of different models.</p><p>Some of them use chain of thought, some of them don't. And those models respond differently when you try and remove the trigger. You can even see them do this like comical reasoning that's also pretty creepy and like, where it's like, oh, well, it even tries to calculate in one case, an expected value of like, well, the expected value of me getting caught is this.</p><p>But then if I multiply it by the ability for me to like, keep saying, I hate you, I hate you, I hate you, then like this is how much reward I should get. And then it will decide whether or not to like actually tell the interrogator that it's like malicious or not.</p><p>- Oh. - But even, I mean, there's another paper from a friend, Miles Turpin, where you ask the model to, you give it like a bunch of examples of, where like the correct answer is always A for multiple choice questions. And then you ask the model, what is the correct answer to this new question?</p><p>And it will infer from the fact that all the examples are A that the correct answer is A. But its chain of thought is totally misleading. Like it will make up random stuff that sounds plausible or that tries to sound as plausible as possible. But it's not at all representative of like the true answer.</p><p>- But isn't this how humans think as well? The famous split brain experiments where, you know, like when a person who is suffering from seizures, one way to solve it is you cut the, the thing that connects the two halves of the brain. And then the, yeah, the speech half is on the left side.</p><p>So it's not connected to the part that decides to do a movement. And so if the other side decides to do something, the speech part will just make something up and it'll like, the person will think that's legit the reason they did it. - Totally, yeah, yeah. It's just, some people will hail chain of thought reasoning as like a great way to solve AI safety.</p><p>- Oh, I see. - And it's like, actually we don't know whether we can trust it. - How much, what will this landscape of models communicating to themselves in ways we don't understand, how does that change with AI agents? 'Cause then these things will, it's not just like the model itself with its previous caches, but like other instances of the model.</p><p>And then- - It depends a lot on what channels you give them to communicate with each other, right? Like if you only give them text as a way of communicating, then they probably have to interpret. - How much more effective do you think the models would be if they could like share the residual streams versus just text?</p><p>- Hard to know, but plausibly so. I mean, one easy way that you can imagine this is like, if you wanted to describe how a picture should look, only describing that with text would be hard. You wanna, maybe some other representation would plausibly be easier. - Totally. - And so like, you can look at how, like DALI works at the moment, right?</p><p>Like it produces those prompts. And when you play with it, you like often can't quite get it to do exactly what the model wants or what you want. - The only DALI has that problem. (all laughing) - It's too easy. (all laughing) - A lot of your imagery comes from that.</p><p>Related models have that problem. (all laughing) And you can imagine like being able to transmit some kind of like denser representation of what you want would be helpful there. And that's like two very simple agents, right? - I mean, I think a nice halfway house here would be features that you'd learn from dictionary learning.</p><p>- Yeah, that would be really, really cool. - Where it's like you get more internal access, but a lot of it is much more human interpretable. - Yeah, so for the audience, you would project the residual stream into this larger space where we know what each dimension actually corresponds to, and then back into the next agents or whatever.</p><p>Okay, so your claim is that we'll get AI agents when these things are more reliable and so forth. When that happens, do you expect that it will be multiple copies of models talking to each other? Or will it be just adapt a computer solve, then the thing just like runs bigger, like more compute when it needs to do a kind of thing that a whole firm needs to do?</p><p>And I ask this because there's two things that make me wonder about like whether agents is the right way to think about what will happen in the future. One is with longer context, these models are able to ingest and consider the information that no human can, and therefore we need like one engineer who's thinking about the front end code and one engineer who's thinking about the backend code, where this thing can just ingest the whole thing.</p><p>This is like Hayek in problem of specialization goes away. Second, these models are just like very general of you're like not using different types of GPT-4 to do different kinds of things. You're using the exact same model, right? So I wonder what that implies is in the future, like an AI firm is just like a model instead of a bunch of AI agents hooked together.</p><p>That's a great question. - I think especially in the near term, it will look much more like agents hooked together. And I say that like purely because as humans, we're going to want to have these like isolated, reliable and like components that we can trust. And we're also gonna wanna, we're going to need to be able to improve and instruct upon those like components in ways that we can understand and improve.</p><p>Like just throwing it all this giant black box company, like one, it isn't gonna work initially. Later on, of course you can imagine it working, but initially it won't work. And two, we probably don't want to do it that way. - Well, you can also have each of the smaller, well, each of the agents can be a smaller model that's cheaper to run and you can fine tune it so that it's actually good at the task.</p><p>- Though there's a future with, like Dwarkesh has brought up adaptive compute a couple of times. There's a future where like the distinction between small and large models like disappears to some degree. And with long context, there's also a degree to which fine tuning might disappear, to be honest.</p><p>Like these two things that are very important today and like today's landscape models, we have like whole different tiers of model sizes and we have fine tuned models for different things. You can imagine a future where you just actually have a dynamic bundle of compute and like infinite context that specializes your model to different things.</p><p>- One thing you can imagine is you have an AI firm or something and the whole thing is like end to end trained on the signal of like, did I make profits? Or like, if that's like too ambiguous, if it's an architecture firm and they're making blueprints, did my client like the blueprints?</p><p>And in the middle, you can imagine agents who are salespeople and agents who are like doing the designing, agents who like do the editing, whatever, would that kind of signal work on an end to end system like that? 'Cause like one of the things that happens in human firms is management considers what's happening at the larger level and like gives these like fine grained signals to the pieces or something when like there's a bad quarter or whatever.</p><p>- Yeah, in the limit, yes. That's the dream of reinforcement learning, right? It's like, all you need to do is provide this extremely sparse signal. And then over enough iterations, you sort of create the information that allows you to learn from that signal. But I don't expect that to be the thing that works first.</p><p>I think this is gonna require an incredible amount of care and like diligence on the behalf of humans surrounding these machines and making sure they do exactly the right thing and exactly what you want and giving them right signals to improve in the ways that you want. - Yeah, you can't train on the RL reward unless the model generates some reward.</p><p>- Yeah, yeah, yeah, exactly. You're in this like sparse RL world where like, if the client never likes what you produce, then like you don't get any reward at all and like, it's kind of bad. But in the future, these models will be good enough to get the reward some of the time, right?</p><p>- This is the nines of reliability that Sholta was talking about. - Yeah, yeah. There's an interesting digression, by the way, on earlier we're talking about, well, we want dense representations that like, that will be favored, right? Like that's a more efficient way to communicate. A book that Trenton recommended, "The Symbolic Species" has this really interesting argument that language is not just a thing that like exists, but like it was also something that evolved along with our minds.</p><p>And specifically evolved to be both easy to learn for children and to something that helps children develop, right? Like it's- - Unpack that phone. - Because like a lot of the things that children learn are received through language. Like the languages that will be the fittest are ones that help like raise the next generation, right?</p><p>And that like makes them smarter, better, whatever. And if you think about- - Like gives them the concepts to express more complex ideas. - Yeah, that and I guess more pedantically, just like not die. - Right, sure. (all laughing) - Let's you encode the important shit to not die.</p><p>- And so then when we just think of like languages like, oh, you know, say this contingent and maybe suboptimal way to represent ideas. Actually, maybe one of the reasons that LLMs have succeeded is because language has evolved for tens of thousands of years to be this sort of cast in which young minds can develop, right?</p><p>Like that is this purpose it was evolved for. - Certainly when you talk to like multimodal or like computer vision researchers versus when you talk to language model researchers, people who work in other modalities have to put enormous amounts of thought into exactly what the right representation space for the images is.</p><p>And like what the right signal to learn from there. Is it like directly modeling the pixels or is it, you know, some loss that's conditioned on, there's like a paper ages ago where they like found that if you trained on the internal representations of an image that model, it like helped you predict better.</p><p>But then later on, like that's obviously like limiting. And so there was like pixel CNN where they're trying to like discreetly model, you know, the individual pixels and stuff. But understanding the right level of representation there, really hot. In language, people are just like, well, I guess you just predict the next token, right?</p><p>It's like, it's kind of easy here. Decisions made. I mean, there's the tokenization, like discussion and debate about like, but I'm gonna go as favorites. - Yeah. Yeah, that's really interesting. How much, the case for a multimodal being a way to bridge the data wall or get past the data wall is like based on the idea that the things you would have learned from more language tokens anyway, you can just get from YouTube.</p><p>Has that actually been the case? How much like positive transfer do you see between different modalities where like, actually the images are helping you be better at like writing code or something, just 'cause like the model is learning a latent capabilities just from trying to understand the image. - Demis in his interview with you mentioned positive transfer.</p><p>- Can't get in trouble. - He's not gonna get in trouble. - But I mean, I can't say about that, other than to say, this is something that people like believe that, yes, like we have all of this data about the world. It would be great if we could like learn an intuitive sense of physics from it that helps us reason, right?</p><p>That seems totally plausible. - Yeah, I'm the wrong person to ask, but there are interesting interpretability pieces where if we fine tune on math problems, the model just gets better at entity recognition. - Whoa, really? - Yeah, so there's like a paper from David Bow's lab recently where they investigate what actually changes in a model when I fine tune it with respect to the attention heads and these sorts of things.</p><p>And they have this like synthetic problem of box A has this object in it, box B has this other object in it. What was in this box? And if you've tried, and it makes sense, right? It's like, you're better at like attending to the positions of different things, which you need for like coding and manipulating math equations.</p><p>- I love this kind of research. What's the name of the paper? Do you know it? - If you look up like fine tuning models, math, David Bow's group that came out like a week ago. - Okay, I'm reading that when I get home. - I'm not endorsing the paper.</p><p>That's like a longer conversation, but like this, it does talk about insight other work on this like entity recognition ability. - One of the things you mentioned to me a long time ago is the evidence that when you train LLMs on code, they get better at reasoning and language, which unless it's the case of the comments in the code are just really high quality tokens or something implies that to be able to think through how to code better, like it makes you like a better reasoner.</p><p>And like, that's crazy, right? Like, I think that's like one of the strongest pieces of evidence for like scaling, just making the thing smart. Like that kind of like positive transfer. - I think like this is true in two senses. One is just that modeling code obviously implies modeling a difficult reasoning process used to create it.</p><p>But two, that code is a nice explicit like structure of like composed reasoning, I guess. Like if this, then that like encodes a lot of structure in that way. - Yeah. - That you could imagine transferring to other types of types of reasoning problem. - Right. And crucially, the thing that makes it significant is that it's not just stochastically predicting the next token of like words or whatever, 'cause it's like learned that like a Sally corresponds to murder at the end of a Sherlock Holmes story.</p><p>No, like if there is some shared thing between code and language, it must be at a deeper level that the model has learned. - Yeah, I think we have a lot of evidence that actual reasoning is occurring in these models and that like they're not just stochastic parrots. - Yeah.</p><p>- It just feels very hard for me to believe that I haven't worked and played with these models. - But Normies who will listen will be like, you know. (all laughing) - Yeah, my two like immediate cash responses to this are one, the work on Othello and now other games where it's like, I give you a sequence of moves in the game.</p><p>And it turns out if you apply some like pretty straightforward interpretability techniques, then you can get a board that the model has learned. And it's never seen the game board before anything, right? Like that's generalization. The other is anthropics influence functions paper that came out last year where they look at the model outputs like, please don't turn me off, I wanna be helpful.</p><p>And then they scan like what was the data that led to that? And like one of the data points that was very influential was someone dying of dehydration in the desert and like having like a will to keep surviving. And to me that just seems like very clear generalization of motive rather than regurgitating, don't turn me off.</p><p>I think 2001 A Space Odyssey was also one of the influential things. And so that's more related, but it's clearly pulling in things from lots of different distributions. - And I also like the evidence you see even with like very small transformers where you can explicitly encode circuits to like do addition, right?</p><p>- Or induction heads. - Or induction heads, this kind of thing. Like you can literally encode basic reasoning processes in the models manually. And it seems clear that there's evidence that they also learn this automatically 'cause you can then rediscover those from trained models. To me, this is pretty strong.</p><p>- The models are under-parameterized. - Yeah, they're under-parameterized. - We're asking them to do a parameter test. - And they want to learn. - And they want to learn, the gradients want to flow. And so they're learning more general skills. - Okay, so I wanna take a step back from the research and ask about your careers specifically because like the tweet implied that I introduced you with, you've been in this field a year and a half.</p><p>I think you've only been in it like a year or something, right? It's like-- - I haven't dropped it yet. - Yeah, but you know, like in that time, I know the solve the Lyman takes are overstated and you won't say this to yourself 'cause you'd be embarrassed to, but like, you know, it's like a pretty incredible thing.</p><p>Like the thing that people in mechanistic numerativity think is the biggest, you know, step forward and you've like been working on it for a year. It's notable. So I'm curious how you explain what's happened. Like why in a year or a year and a half have you guys been, you know, made important contributions to your field?</p><p>- It goes without saying luck, obviously. And I feel like I've been very lucky in like the timing of different progressions has been just like really good in terms of advancing to the next level of growth. I feel like for the interpretability team specifically, I joined when we were five people.</p><p>We've now grown quite a lot, but there were so many ideas floating around and we just needed to like really execute on them and have like quick feedback loops and like do careful experimentation that led to like signs of life and have now allowed us to like really scale.</p><p>And I feel like that's kind of been my biggest value to the team, which it's not all engineering, but quite a lot of it has been. - Interesting. So you're saying like you came at a point where like there had been a lot of science done and there was a lot of like good research floating around, but they needed someone to like just take that and like maniacally execute on it.</p><p>- Yeah, yeah. And this is why it's not all engineering 'cause it's like running different experiments and like having a hunch for why it might not be working and then like opening up the model or opening up the weights and like, what is it learning? Okay, well, let me try and do this instead and that sort of thing.</p><p>A lot of it has just been being able to do like very careful, thorough, but quick investigation of different ideas or yeah, theories. - And why was that lacking in the existing? - I don't know. I feel like, I mean, I work quite a lot and then I feel like I just am like quite agentic.</p><p>Like if your question's about like career overall and I've been very privileged to have like a really nice safety net to be able to take lots of risks, but I'm just like quite headstrong. Like in undergrad, Duke had this thing where you could just make your own major. And it was like, eh, I don't like this prerequisite or this prerequisite and I wanna take all four or five of these subjects at the same time.</p><p>So I'm just gonna make my own major. Or like in the first year of grad school, I like canceled rotation so I could work on this thing that became the paper we were talking about earlier and like didn't have an advisor, like got admitted to do machine learning for protein design and was just like off in computational neuroscience land with no business there at all, but worked out.</p><p>- There's a headstrongness, but it seemed like another theme that jumped out was the ability to step back and you were talking about this earlier, the ability to step back from your sunk costs and go in a different direction is in a weird sense the opposite of that, but also a crucial step here where I know like 21-year-olds or like 19-year-olds who are like, ah, this is not a thing I've specialized in or like I didn't major in this.</p><p>I was like, dude, motherfucker, you're 19. Like you can definitely do this. And you like switching in the middle of grad school or something like that's just like, yeah. - Yeah, sorry, I didn't mean to cut you off, but I think it's like strong ideas loosely held and being able to just like pinball in different directions.</p><p>And the headstrongness I think relates a little bit to the fast feedback loops or agency in so much as I just don't get blocked very often. Like if I'm trying to write some code and like something isn't working, even if it's like in another part of the code base, I'll often just go in and fix that thing or at least hack it together to be able to get results.</p><p>And I've seen other people where they're just like, help. I can't. And it's like, no, that's not a good enough excuse. Like go all the way down. - I've definitely heard like people in management type positions talk about the lack of such people where they will check in on somebody a month after they give them a task or a week after they give them a task and like, how's it going?</p><p>And they say, well, you know, we need to do this thing which requires lawyers 'cause it requires talking about this regulation. It's like, how's that going? I was like, well, we need lawyers. I'm like, why didn't you get lawyers? Or something like that. So that's definitely like, yeah. - I think that's arguably the most important quality in like almost anything.</p><p>It's just pursuing it to like the end of the earth and like whatever you need to do to make it happen, you'll make it happen. - If you do everything, you'll win. - If you do everything, you'll win, exactly. But yeah, yeah, yeah, yeah. I think from my side, definitely that quality has been important.</p><p>Like agency in the work. There are thousands or I would even like probably tens of thousands of engineers at Google who are like, you know, basically, like we're all like equivalent like software engineering ability, let's say. Like, you know, if you gave us like a very well-defined task then we'd probably do it like equivalently well.</p><p>Maybe a bunch of them would do it a lot better than me, you know, in all likelihood. But what I've been, one of the reasons that I've been impactful so far is I've been very good at picking extremely high leverage problems. So problems that haven't been particularly well-solved so far.</p><p>Perhaps as a result of like frustrating structural factors like the ones that you pointed out in that scenario before, where they're like, oh, we can't do X 'cause this, what team would you do, why? Or like, and then going, okay, well, I'm just gonna like vertically solve the entire thing.</p><p>(all laughing) - Right. - And that turns out to be remarkably effective. Also, I am very comfortable with like, if I think there is something correct that needs to happen, I will like make that argument and continue making that argument at escalating levels of like criticality until that thing gets solved.</p><p>And I'm also quite pragmatic with what like I do to solve things. You get a lot of people who come in with, as I said before, like a particular background or a familiarity or they're like, they know how to do something and they won't, like one of the beautiful things about Google, right, is you can run around and get world experts in literally everything.</p><p>You can sit down and talk to people, optimization experts, like TP, like chip design experts, like experts in, I don't know, like different forms of like pre-training algorithms or like RL or whatever. And you can learn from all of them and you can take those methods and apply them.</p><p>And I think this was like, maybe the start of why I was initially impactful was like this vertical, like agency effectively. And then a follow-up piece from that is, I think it's often surprising how few people are like fully realized in all the things they wanna do. They're like blocked or limited in some way.</p><p>And this is very common in big organizations everywhere. People like have all these blockers on what they're able to achieve. And I think being a, like one, helping inspire people to work on particular directions and working with them on doing things massively scales your leverage. Like you get to work with all these wonderful people who teach you heaps of things and generally like helping them push past organizational blockers means like together you get an enormous amount done.</p><p>Like none of the impact that I've had has been like me individually going off and solving a whole lot of stuff. It's been me maybe like starting off a direction and then convincing other people that this is the right direction and bringing them along in like this big tidal wave of like effectiveness that like goes and solves that problem.</p><p>- We should talk about how you guys got hired. 'Cause I think that's a really interesting story 'cause you were a McKinsey consultant, right? (all laughing) - No, no, it's good, yeah. - There's an interesting thing there where, first of all, I think people are, yeah, generally people just don't understand how like decisions are made about either admissions or evaluating who to hire or something.</p><p>But like just talk about like how were you noticed as- - Yeah, totally. - Yeah, you got hired. - So like the TL;DR of this is I studied robotics in undergrad. I always thought that AI would be one of the highest leverage ways to impact the future in a positive way.</p><p>Like the reason I am doing this is because I think it is like one of our best shots at making a wonderful future, basically. And I thought that working actually at McKinsey, I would get a really interesting insight into what people actually did for work. Like in this, I actually wrote this as the first line in my cover letter to McKinsey was like, "I wanna work here "so that I can learn what people do "so that I can like understand how they work." (all laughing) And in many respects, like I did get that.</p><p>I also got a whole lot of other things. Many of the people there are like wonderful friends. I actually learned, I think a lot of this, like agentic behavior in part from my time there where you go into organizations and you see how impactful just not taking no for an answer gets you.</p><p>Like you would be surprised at the kind of stuff where like, because no one quite cares enough in some organizations, things just don't happen 'cause no one's willing to take direct responsibility. This is incredibly like directly responsible individuals are ridiculously important. And people are willing to, like they just don't care as much about timelines.</p><p>And so much of the value that an organization like McKinsey provides is hiring people who you were otherwise unable to hire for a short window of time where they can just like push through problems. I think people like underappreciate this. And so like at least some of my, well, hold up, like I'm gonna become the directly responsible individual for this 'cause no one's taking appropriate like responsibility.</p><p>I'm gonna care a hell of a lot about this. And I'm gonna make sure, like I'm gonna go to the end of the earth to make sure it gets done, comes from that time. But more to your like actual question of like how did I get hired? The entire time I didn't get into the grad programs that I wanted to get into over here, which was specifically for focus on like robotics and RL research and that kind of stuff.</p><p>And in the meantime, on nights and weekends, basically every night from 10 p.m. till 2 a.m., I would do my own like research. And every weekend for like at least six to eight hours each day, I would do my own like research and coding projects and this kind of stuff.</p><p>And that sort of switched in part from like quite robotics specific work to after reading Gwen's scaling hypothesis post, I got completely scaling pills. And I was like, okay, like clearly the way that you solve robotics is by like scaling large multimodal models. And then in an effort to scale large multimodal models with a grant, I got a grant from the TPU like access program, the Tensor Research Cloud.</p><p>I was trying to work out how to scale that effectively. And James Bradbury, who at the time was at Google and is now at Anthropic, saw some of my questions online where I was trying to work out how to do this properly. And he was like, I thought I knew all the people in the world who were like asking these questions.</p><p>Who on earth are you? And he looked at that and he looked at some of the like the robotic stuff that I'd been putting up on my blog and that kind of thing. And he reached out and said, hey, do you wanna have a chat and do you wanna like explore working with us here?</p><p>And I was hired, as I understand it later, as an experiment in trying to take someone with extremely high enthusiasm and agency and pairing them with some of the best engineers that he knew. And so one, another one of the reasons I could say like I've been impactful is I had this like dedicated mentorship from utterly wonderful people.</p><p>Like people like Rainer Pope, who has since left to go do his own ship company. Anselm Weskaja, James himself, many others. But those are like the sort of formative like two to three months at the beginning. And they taught me a whole lot of like the principles and like heuristics that I apply, like how to, and how to like solve problems in the way that they have, particularly in that like systems and algorithms overlap, where like one more thing that makes you like quite effective in ML research is really concretely understanding the systems side of things.</p><p>And this is something I've learned from them basically, is like a deep understanding of how systems influence algorithms and how algorithms influence systems. Because the systems constrain the design space, sorry, the solution space, which you have available to yourself in the algorithm side. And very few people are comfortable fully bridging that gap.</p><p>But a place like Google, you can just like go and ask all the algorithms experts and all the systems experts everything they know, and they will happily teach you. And if you go and sit down with them, they will teach you everything they know. It's wonderful. And this has meant that I've been able to be very, very effective for both sides, like for the pre-training crew, because I understand systems very well.</p><p>I can intuit and understand like this will work well or this won't. And then like flow that on through the inference considerations of models and this kind of thing. And for like to the chip design teams, I'm one of the people they turn to understand what chips they should be designing in three years, because I'm one of the people who's best able to understand and explain the kind of algorithms that we might want to design in three years.</p><p>And obviously you can't make very good guesses about that. But like, I think I like convey the information well, accumulated from all of my compatriots on the pre-training crew, and like the general, like systems inside group and convey that information well to them, because also even inference applies a constraint to pre-training.</p><p>And so like, there's this like these trees of constraints where if you understand all the pieces of the puzzle, then you get a much better sense for like what the solution space might look like. - Yeah, there's a couple of things that stick out to me there. One is not just the agency of the person who was hired, but the parts of the system that were able to think, wait, that's really interesting.</p><p>Who is this guy? Not from a grad program or anything, you know, like currently McKinsey consultant, just like an undergrad, but that's interesting. Let's like give this a shot, right? So James and whoever else that's like, that's very notable. And that's, second is, I actually didn't know this part of the story where that was part of an experiment run internally about, can we do this?</p><p>Can we like bootstrap somebody and like, yeah. And in fact, what's really interesting about that is the third thing you mentioned is, having somebody who understands all layers of the stack and isn't so stuck on any one approach or any one layer of abstraction is so important. And specifically that like what you mentioned about being bootstrapped immediately by these people might've meant that since you're getting up to speed on everything at the same time, rather than spending grad school going deep on like one specific way of doing RL, you actually can take the global view and aren't like totally bought in on one thing.</p><p>So not only is it something that's possible, but like has greater returns than just hiring somebody at a grad school potentially. Because this person can just like, I don't know, just like getting a GPT-8 and like fine tuning them on like one year of, you know what I mean?</p><p>So yeah, that's really interesting. - You come at everything with fresh eyes and you know it come in locked to any particular field. Now what like one caveat to that is that before, like during my self-experimentation and stuff, I was reading everything I could. I was like obsessively reading papers every night and like actually funnily enough, I like read much less widely now that I like my day is occupied by working on things.</p><p>And in some respect, I had like this very broad perspective before where not that many people, even like in a PhD program, you'll like focus on a particular area. If you just like read all the NLP work and all the computer vision work and like all the robotics work, you like see all these patterns that start to emerge across subfields in a way that I guess like foreshadowed some of the work that I would later do.</p><p>- That's super interesting. One of the reasons that you've been able to be agentic within Google is like your peer programming half the days or most of the days with Sergey Brin, right? And so that's really interesting that like there's this person who's like willing to just push ahead on this LLM stuff and like get rid of the local blockers in its place.</p><p>- Yeah, I think important to get is like not like every day or anything, but like when there are particular projects that he's interested in, then like we'll work together on those. But there's also been times when he's been focused on projects with other people. But in general, yes, there's a surprising alpha to like being one of the people who actually goes down to the office every day.</p><p>That like is really actually shouldn't be, but is surprisingly impactful. And as a result, I've like benefited a lot from having like basically being like close friends with people in leadership who care and being able to like really argue convincingly about why we should do X as opposed to Y.</p><p>And having that like vector to, trying like it's, Google is a big organization. Having those vectors helps a little bit. But also it's very important. It's the kind of thing you don't want to ever abuse, right? Like you want to make the argument through all the right channels and like only sometimes you need to.</p><p>- And so this includes you like Sergey Brin, Jeff Dee and so forth. I mean, it's like, it's notable. I don't know, I feel like Google is undervalued given that like, I don't know, like Steve Jobs is working on the equivalent like the next product for Apple, like peer-to-peer programming or something, right?</p><p>- I mean, like I, yeah, I've benefited immensely from like, okay, so for example, during the Christmas break, I was just going into the office a couple of days during that time. - Sounds like quite a lot of days. - Quite a lot of days. (laughing) And I don't know if you guys have read that article about Jeff and Sanjay doing the pair programming, but they were there pair programming on stuff.</p><p>And I got to hear about all these cool stories of like early Google, where they're talking about like crawling under the floorboards and rewiring data centers and like telling me how many like bits they were pulling off the, how many bytes they were pulling off the instructions of a given compiler instruction.</p><p>And like all these like crazy little performance optimizations they were doing, like they were having the time of their life. And I got to like sit there and really like experience this sense of history in a way that you don't expect to get. Like you expect to be very far away from all that, I think maybe in a large organization, but.</p><p>- Yeah, that's super cool. And Trenton, does this map onto any of your experience? - I think Sholto's story is more exciting. (laughing) Mine was just very serendipitous in that I got into computational neuroscience, didn't have much business being there. My first paper was mapping the cerebellum to the attention operation and transformers.</p><p>My next ones were looking at like sparsity. - How old were you when you wrote that? - It was my first year of grad school. So 22. - Oh yeah. - But yeah, my next work was on sparsity in networks, like inspired by sparsity in the brain, which was when I met Tristan Hume and Anthropic was doing the SOLU, the Softmax Linear Output Unit work, which was very related in quite a few ways of like, let's make the activation of neurons across a layer really sparse.</p><p>And if we do that, then we can get some interpretability of what the neuron's doing. I think we've updated on that approach towards what we're doing now. So that started the conversation. I shared drafts of that paper with Tristan. He was excited about it. And that was basically what led me to become Tristan's resident and then convert to full-time.</p><p>But during that period, I also moved as a visiting researcher to Berkeley and started working with Bruno Olshausen, both on what's called vector symbolic architectures, which one of the core operations of them is literally superposition, and on sparse coding, also known as dictionary learning, which is literally what we've been doing since.</p><p>And Bruno Olshausen basically invented sparse coding back in 1997. And so it was like, my research agenda and the interpretability team seemed to just be running in parallel with just research tastes. And so, yeah, it made a lot of sense for me to work with the team. And it's been a dream since.</p><p>- One thing I've noticed when people tell stories about their careers or their successes, they ascribe it way more to contingency. But when they hear about other people's stories, they're like, "Of course it wasn't contingent." You know what I mean? It's like, if that didn't happen, something else would have happened.</p><p>I've just noticed that something like talk to, and it's interesting that you both think that it was especially contingent. Whereas, I don't know, maybe you're right, but it's this sort of interesting pattern that-- - Yeah, but I mean, I literally met Tristan at a conference and didn't have a scheduled meeting with her or anything, just joined a little group of people chatting, and he happened to be standing there, and I happened to mention what I was working on.</p><p>And that led to more conversations. And I think I probably would have applied to Anthropic at some point anyways, but I would have waited at least another year. It's still crazy to me that I can actually contribute to interpretability in a meaningful way. - I think there's an important aspect of shots on goal there, so to speak, right?</p><p>Where you're even just choosing to go to conferences itself is like putting yourself in a position where luck is more likely to happen. And conversely, in my own situation, it was like doing all of this work independently and trying to produce and do interesting things was my own way of trying to manufacture luck, so to speak, and try and do something meaningful enough that it got noticed.</p><p>- Given that you said you framed this in the context of they were trying to run this experiment of can something- - So it was specifically James, and I think our manager, Brennan, was trying to run this experiment. - It worked. Did they do it again? - Yeah, so my closest collaborator, Enrique, he crossed from search to our team.</p><p>He's also been ridiculously impactful. He's definitely a stronger engineer than I am, and he didn't go to university. - What was notable about, for example, is James Bradbury is somebody who's... Usually this kind of stuff is farmed out to recruiters or something like that, whereas James is somebody whose time is worth hundreds of millions of dollars.</p><p>(all laughing) You know what I mean? That thing is very bottlenecked on that kind of person taking the time almost in an aristocratic tutoring sense of finding and then getting up to speed, and it seems like if it worked this well, it should be done at scale. It should be the responsibility of key people to, you know what I mean, onboard and find.</p><p>- I think that is true to many extents. I'm sure you've probably benefited a lot from the key researchers mentoring you during-- - And actively looking on open source repositories or on forums or whatever for potential people like this. - Yeah, I mean, James has Twitter injected into his brain.</p><p>(all laughing) But yes, and I think this is something which in practice is done. People do look out for people that they find interesting and try and find high signal. In fact, actually, I was talking about this with Jeff the other day, and Jeff said that, yeah, he's like, you know, one of the most important hires I ever made was off a cold email.</p><p>And I was like, well, who was that? And he's, Chris Ola. - Ah, yeah. - Because Chris similarly had no background in, well, like, no formal background in ML, right? And like Google Brain was just getting started in this kind of thing. But Jeff saw that signal. And the residency program, which like Brain had, is I think also, it was astonishingly effective at finding good people that didn't have strong ML backgrounds.</p><p>- Yeah. One of the other things that I wanna emphasize for a potential slice of the audience that would be relevant to is, there's this sense that like the world is legible and efficient, companies have these go to jobs.google.com or jobs.whatevercompany.com, and you apply, and there's the steps, and like, they will evaluate you efficiently on those steps.</p><p>Whereas not only from the storage teams, like often that's not the way it happens. That's in fact, it's good for the world, and that's not often how it happens. Like it is important to look at, were they able to like write an interesting block, technical blog post about their research, or like make an interesting contributions.</p><p>Yeah, I want you to like riff on, for the people who are like, assuming that the other end of the job board is like just like super legible and mechanical. This is not how it works. And in fact, like people are looking for the sort of different way, different kind of person who's agentic and putting stuff out there.</p><p>- And I think specifically what people are looking for, there is two things. One is agency, and like putting yourself out there. And the second is the ability to do world-class something. - Yeah, and two examples that I always like to point to here are Andy Jones from Anthropic, did an amazing paper on scaling laws as applied to board games.</p><p>It didn't require much resources, it demonstrated incredible engineering skill, it demonstrated incredible understanding of like the most topical problem of the time. And he didn't come from like typical academic background or whatever, as I understand it basically, like as soon as he came out with that paper, both Anthropic and OpenAI were like, we would desperately like to hire you.</p><p>There's also someone who works on Anthropic's performance team, now Simon Bohm, who has written, in my mind, the reference for optimizing a CUDA map model, like on a GPU. And that demonstrated example of like taking some like prompt effectively and producing the world-class reference example for it in something that wasn't particularly well done so far, is like I think an incredible demonstration of like ability and agency that in my mind would like be an immediate, would like please love to like interview/hire.</p><p>- Yeah, the only thing I can add here is, I mean, I still had to go through the whole hiring process and all the standard interviews and this sort of thing. - Yeah, everyone does, everyone does. - Wait, isn't that seems stupid? - I mean-- - It's important de-biasing.</p><p>- Yeah, yeah, yeah. - And like the bias is what you want, right? Like you want the bias of somebody who's got great taste and like, he's like, who cares? - Your interview process should be able to disambiguate that as well. - Yeah, like I think there are cases where someone seems really great and then it's like, oh, they actually just can't code, this sort of thing, right?</p><p>Like how much you weight these things definitely matters though. And like, I think the, we take references really seriously, the interviews you can only get so much signal from. And so it's all these other things that can come into play for whether or not a hire makes sense. - But you should design your interviews such that like they test the right things.</p><p>- One man's bias is another man's taste, you know? (laughing) - Yeah, I guess the only thing I would add to this or maybe to the headstrong context is like, there's this line, the system is not your friend. - Right. - And it's not necessarily to say it's actively against you or it's your sworn enemy.</p><p>It's just not looking out for you. - Right. - And so I think that's where a lot of the proactiveness comes in of like, there are no adults in the room or like, and like you have to come to some decision for what you want your life to look like and then execute on it.</p><p>And yeah, hopefully you can then update later if you're too headstrong in the wrong way. But I think you almost have to just kind of charge at certain things. - Right. - To get much of anything done, not be swept up in the tide of whatever the expectations are.</p><p>- There's like one final thing I want to add, which is like, we talked a lot about agency and this kind of stuff. But I think actually like surprisingly enough, one of the most important things is just carrying an unbelievable amount. - Right. - And when you care an unbelievable amount, you'd like, you check all the details and you have like this understanding of like what could have gone wrong.</p><p>And you'd like, it just, it matters more than you think because people end up not caring. - Sure. - Not caring enough. This is like LeBron quote, where he talks about how when he sort of, before he started in the league, he was like worried that everyone would be like incredibly good.</p><p>And then he gets there and he like realized that actually once people hit financial stability, then they like, they relax a bit. And he's like, oh, this is going to be easy. I don't think that's quite true because I think in like AI research, 'cause most people actually care quite deeply.</p><p>- Yeah. - But there's caring about your problem and there's also just caring about the entire stack and everything that goes up and down, like going explicitly going and fixing things that aren't your responsibility to fix because overall it makes like the stack better. - I mean, another part of that I forgot to mention is you were mentioning, oh, going in on weekends and on Christmas break and you get to like, the only people in the office are Jeff Dean and Sergey Brin or something.</p><p>(laughing) And you just like get to pair a program with them. It's just, it's interesting to me that people, I don't want to pick on your company in particular, but like people at any big company, they've gotten there because they've gone through a very selective process. That's like, they had to compete in high school, they had to compete in college, but it almost seems like they get there and then they take it easy.</p><p>When in fact, this is a time to put the pedal to the metal, go in and pair program with Sergey Brin on the weekends or whatever, you know what I mean? - I mean, there's pros and cons there, right? I think many people make the decision that the thing that they want to prioritize is like a wonderful life with their family.</p><p>And if they do wonderful work, like let's say they don't work every hour of the day, right? But they do wonderful work in the work, like the hours that they do do, that's incredibly impactful. I think this is true for many people at Google is like, maybe they don't work as many hours as like your typical startup mythologies, right?</p><p>But the work that they do do is incredibly valuable. It's very high leverage because they know the systems and they're experts in their field. And we also need people like that. Like our world rests on these huge, like difficult to manage and difficult to fix systems. And we need people who are like willing to work on and help and fix and maintain those, in frankly, like a thankless way that isn't as like high publicity as all of this AI work that we're doing, right?</p><p>And I'm like ridiculously grateful that those people do it. And I'm also happy that there are people for whom like, okay, they find technical fulfillment in their job and doing that well. And also like maybe they draw a lot more from it also out of spending like a lot of hours with their family.</p><p>And I'm lucky that I'm at a stage in my life where like, yeah, I can go in and work every hour of the week. But like, that's like, I'm not making as many sacrifices to do that. - Yeah. I mean, like just one example that sticks out in my mind of this sort of like the other side says no, and you can still get the yes on the other end.</p><p>Basically every single high profile of guests I've gotten so far, I think maybe with one or two exceptions, I've sat down for a week and I've just come up with a list of sample questions that's like trying to really come up with really smart questions to send to them.</p><p>And the entire process I've always thought like, if I just cold email them, it's like a 2% chance they say yes. If I include this list, there's a 10% chance. And because otherwise, you know, there's like, you go through their inbox and every 34 seconds, there's an interview for whatever podcast, interview for whatever podcast.</p><p>And every single time I've done this, they've said yes. - Wow, yeah. Exactly. That's great questions. But if you do everything, you'll win. - But you just like, you literally have to dig in the same hole for like 10 minutes, or in that case, like make a list of sample questions for them to get past, they're not an idiot list.</p><p>You know what I mean? - Demonstrate how much you care. And the work you're willing to put in. Yeah. - Yeah. Something that a friend said to me a while back, but I think it's stuck is like, it's amazing how quickly you can become world-class at something just because most people aren't trying that hard and like are only working like, I don't know, the actual like 20 hours that they're actually spending on this thing or something.</p><p>And so, yeah, if you just go ham, then like you can get really far pretty fast. - And I think I'm lucky I had that experience with the fencing as well. Like I had the experience of becoming world-class in something and like knowing that you just worked really, really hard and went back.</p><p>- Yeah. For context, by the way, Sholto was one seat away as he was the next person in line to go to the Olympic stuff for fencing. - I was at best like 42nd in the world for fencing, for men's foil fencing. - Mutational load is a thing, man.</p><p>(all laughing) - And there was one cycle where, yeah, I was like the next highest ranked person in Asia. And if one of the teams had been like disqualified for doping as it was occurring in part during that cycle, and as occurred for like the Australian women's rowing team, I think went because one of the teams was disqualified, then I would have been the next in line.</p><p>It's interesting when like you just like find out about people's prior lives and it's like, oh, you know, this guy was almost an Olympian, this other guy was whatever, you know what I mean? Okay, let's talk about interoperability. - Yeah. - I actually want to stay on the brain stuff as a way to get into it for a second.</p><p>We were previously discussing is the brain organized in the way where you have a residual stream that is gradually refined with higher level associations over time or something. There's a fixed dimension size in a model. If you had to, I don't even know how to ask this question in a sensible way, but what is the D model of the brain?</p><p>What is it like the embedding size of, or because of feature splitting, is that not a sensible question? - No, I think it's a sensible question. Well, it is a question. (all laughing) - You could've just not said that either way. - No, I don't like this question. - You can talk, just like actively.</p><p>- I'm trying to, I don't know how you would begin to kind of be like, okay, well, this part of the brain is like a vector of this dimensionality. I mean, maybe for the visual stream, because it's like V1 to V2 to IT, whatever, you could just count the number of neurons that are there and be like, that is the dimensionality, but it seems more likely that there are kind of sub-modules and things are divided up.</p><p>So yeah, I don't have, and I'm not the world's greatest neuroscientist, right? Like I did it for a few years. I studied the cerebellum quite a bit. So I'm sure there are people who could give you a better answer on this. Do you think that the way to think about whether it's in the brain or whether it's in these models, fundamentally what's happening is features are added, removed, changed, and like the feature is the fundamental unit of what is happening in the model.</p><p>Like what would have to be true for, give me a, and this goes back to the earlier thing we were talking about, whether it's just associations all the way down. Give me like a counterfactual. In the world where this is not true, what is happening instead? Like what is the alternative hypothesis here?</p><p>- Yeah, it's hard for me to think about, 'cause at this point I just think so much in terms of this feature space. I mean, at one point there was like the kind of behavioralist approach towards cognition where, or it's like, you're just, you're like input output, but you're not really doing any processing.</p><p>Or it's like everything is embodied and you're just like a dynamical system that's like operating along like some predictable equations. But like, there's no state in the system, I guess. But whenever I've read these sorts of critiques, it's like, well, you're just choosing to not call this thing a state, but you could call like any internal component of the model a state.</p><p>Like even with the feature discussion, it's defining what a feature is, is really hard. And so the question feels almost too slippery. - What is a feature? - A direction and activation space. A latent variable that is operating behind the scenes that has like causal influence over the system you're observing.</p><p>It's a feature if you call it a feature. It's tautological. I mean, these are all explanations that I like, I feel some association. - In a very rough, intuitive sense, in like a sufficiently sparse and binary vector, features like whether or not something's turned on or off, right? Like in a very simplistic sense.</p><p>Which might be, I think, a useful metaphor to understand it by. It's like when we talk about features activating, it is in many respects the same way that neuroscientists would talk about like a neuron activating, right? - If that neuron corresponds to-- - To something in particular. - Right, yeah, yeah, yeah, yeah.</p><p>And no, I think that's useful as like, what do we want a feature to be, right? Like what is a synthetic problem under which a feature exists? But even with the, towards monosemanticity work, we talk about what's called feature splitting, which is basically, you will find as many features as you give the model the capacity to learn.</p><p>And by model here, I mean the up-projection that we fit after we trained the original model. And so, if you don't give it much capacity, it'll learn a feature for bird. But if you give it more capacity, then it will learn like ravens and eagles and sparrows and like specific types of birds.</p><p>- Still on definitions thing. I guess, naively, I think of things like bird versus what kind of token, is it like a period at the end of a hyperlink, as you were talking about earlier, versus at the highest level, things like love or deception or like holding a very complicated proof in your head or something.</p><p>Is this all features? 'Cause then the definition seems so broad as to almost be not that useful. Like, or rather that there seems to be some important differences between these things, and they're all features. Like, yeah, I'm not sure what we even mean by. - I mean, all of those things are like discrete units that have connections to other things that then imbues them with meaning.</p><p>That feels like a specific enough definition that it's useful or not too all-encompassing, but feel free to push back. - Well, like what would you discover tomorrow in, that could make you think like, oh, this is like kind of fundamentally the wrong way to think about what's happening in the model?</p><p>- I mean, if the features we were finding weren't predictive or if they were just representations of the data, right? Where it's like, oh, all you're doing is just clustering your data. And there's no like higher level associations that are being made. Or it's some like phenomenological thing of like, you're saying that this feature files for marriage, but if you activate it really strongly, it doesn't change the outputs of the model in a way that would correspond to it.</p><p>Like, I think these would both be good critiques. I guess one more is, and we tried to do experiments on MNIST, which is a data set of digits, images, and we didn't look super hard into it. And so I'd be interested if people, other people wanted to take up like a deeper investigation, but it's plausible that your like latent space of representations is dense and it's a manifold instead of being these discrete points.</p><p>And so you could like move across the manifold, but at every point there would be some meaningful behavior. And it's much harder than to label things as features that are discrete. - Like in a naive sort of outsider way, the thing that would seem to me to be like a way in which this picture could be wrong is if there's not some like, this thing is turned on and turned off, but it's like a much more global kind of like, the system is, I don't know, I'm gonna use really clumsy, like, you know, I mentioned it in a pretty kind of language, but is there a good analogy here?</p><p>Yeah, I guess if you think of like something like the laws of physics, it's not like, well, the feature for wetness is turned on, but it's only turned on this much, and then the feature for like, you know, I guess maybe it's true, 'cause like the mass is like a gradient, and like, you know, like, I don't know, but the polarity or whatever is a gradient as well.</p><p>But there's also a sense in which like there's the laws, and the laws are more general, and you have to understand like the general bigger picture, you don't get that from just like these like specific sub-circuits. - But that's where like the reasoning circuit itself comes into play, right?</p><p>Where you're taking these features ideally, and like trying to compose them into something high level. Like you might say, okay, like when I'm using, at least this is my head canon. Let's say I'm trying to use the foot, you know, F equals MA, right? Then presumably at some point I have features which like denote, okay, like mass, and then that's like helping me retrieve the actual mass of the thing that I'm using, and then like the acceleration and this kind of stuff, but then also maybe there's a higher level feature that does correspond to using the first law of physics.</p><p>Maybe, but the more important part is that the composition of components which helps me retrieve relevant pieces of information, and then produce like maybe some like a multiplication operator or something like that from when necessary. At least that's my head canon. - What is a compelling explanation to you, especially for very smart models of, like I understand why it made this output, and it was like for a legit reason.</p><p>If it's doing million line pull requests or something, what are you seeing at the end of that request where you're like, yep, that's chill? - Yeah, so ideally you apply dictionary learning to the model. You've found features. Right now we're actively trying to get the same success for attention heads, in which case we have features for both the core, you can do it for residual stream, MLP, and attention throughout the whole model.</p><p>Hopefully at that point you can also identify broader circuits through the model that are like more general reasoning abilities that will activate or not activate. But in your case where we're trying to figure out if this like pull request should be approved or not, I think you can flag or detect features that correspond to deceptive behavior, malicious behavior, these sorts of things, and see whether or not those have fired.</p><p>That would be like an immediate, you can do more than that, but that would be an immediate. - But before I trace down on that, what does a reasoning circuit look like? What would that look like when you found it? - Yeah, so I mean the induction head is probably one of the simplest cases of this.</p><p>- But that's not like reasoning, right? - Well, I mean, what do you call reasoning, right? Like it's a good reason, so I guess context for listeners, the induction head is basically, and you see the line like Mr. and Mrs. Dursley did something Mr. blank, and you're trying to predict what blank is.</p><p>And the head has learned to look for previous occurrences of the word Mr., look at the word that comes after it, and then copy and paste that as the prediction for what should come next, which is a super reasonable thing to do. And there is computation being done there to accurately predict the next token.</p><p>- Mm-hmm, that is context dependent. - That is, yeah. - But it's not like reasoning, you know what I mean? - But is, I guess going back to the associations all the way down, it's like if you chain together a bunch of these reasoning circuits or heads that have different rules for how to relate information.</p><p>- But in this sort of like zero shot case, like something is happening where when you like pick up a new game and you immediately start understanding how to play it. And it doesn't seem like an induction heads kind of thing. Or like- - Well, I think there would be another circuit for like extracting pixels and turning them into latent representations of the different objects in the game, right?</p><p>And like a circuit that is learning physics. - And what would that, because the induction heads is like one layer transformer. - Two layers, yeah, yeah, yeah. - So you can like kind of see like what the thing that is a human picks up a new game and understands it.</p><p>How would you think about what that is? Is it presumably it's across multiple layers, but like, is it, yeah, yeah. What would that physically look like? How big would it be maybe? Or like, I mean, that would just be an empirical question, right, of like how big does the model need to be to perform this task?</p><p>But like, I mean, maybe it's useful if I just talk about some other circuits that we've seen. So we've seen like the IOI circuit, which is the indirect object identification. And so this is like, if you see, it's like Mary and Jim went to the store, Jim gave the object to blank, right?</p><p>And it would predict Mary because Mary's appeared before as like the indirect object or it'll infer pronouns, right? And this circuit even has behavior where like if you ablate it, then like other heads in the model will pick up that behavior. We'll even find heads that wanna do copying behavior and then other heads will suppress.</p><p>So like, it's one jobs, one head's job to just always copy like the token that came before, for example, or the token that came five before or whatever. And then it's another head's job to be like, no, do not copy that thing. So there are lots of different circuits performing in these cases, pretty basic operations, but when they're chained together, you can get unique behaviors.</p><p>- And, but like, is the story of how you found it with the reasoning thing is like, 'cause you won't be able to understand or it'll just be like really conv, you know, it won't be something you can see in like a two layer transformer. So will you just be like the circuit for deception or whatever, it just, this part of the network fired when we at the end identified the thing as being deceptive, this part, and it didn't fire when we didn't identify it as being deceptive, therefore this must be the deception circuit.</p><p>- I think a lot of analysis like that, like Anthropic has done quite a bit of research before on sycophancy, which is like the model saying what it thinks you want to hear. - And that requires us at the end to be able to label which one is like bad and which one is good.</p><p>- Yeah, so we have tons of instances, and actually as you make a lot of models larger, they do more of this, where the model is clearly, it has like features that model another person's mind and these activate and like some subset of these, we're hypothesizing here, but like would be associated with more deceptive behavior.</p><p>- Although like it's doing that by, I don't know, Chad GPT, I think it's probably modeling me because that's like RLHF induces that too. - Yeah, theory of mind. - Yeah, so, well, first of all, the thing you mentioned earlier about there's redundancy. So then it's like, well, have you caught like the whole thing that could cause deception of the whole thing or like, is it just one instance of it?</p><p>- Yeah. - Second of all, are your like labels correct? You know, maybe like you thought this wasn't deceptive, it's like still deceptive, especially if it's producing output you can't understand. Third, is the thing that's gonna be the bad outcome, something that's even human understandable, like deception is a concept we can understand.</p><p>Maybe there's like a... - Yeah, yeah, so a lot to unpack here. So I guess a few things. One, it's fantastic that these models are deterministic. When you sample from them, it's stochastic, right? But like, I can just keep putting in more inputs and ablate every single part of the model.</p><p>This is kind of the pitch for computational neuroscientists to come and work on interpretability. It's like, you have this alien brain and you have access to everything in it and you can just ablate however much of it you want. And so I think if you do this carefully enough, you really can start to pin down what are the circuits involved?</p><p>What are the backup circuits? These sorts of things. The kind of cop-out answer here, but it's important to keep in mind, is doing automated interpretability. So it's like, as our models continue to get more capable, having them assign labels or like run some of these experiments at scale. And then with respect to like, if there's superhuman performance, how do you detect it?</p><p>Which I think was kind of the last part of your question. Aside from the cop-out answer, if we buy this associations all the way down, you should be able to coarse grain the representations at a certain level, such that they then make sense. I think it was even in Demis' podcast, he's talking about like, if a chess player makes a superhuman move, they should be able to distill it into reasons why they did it.</p><p>And like, even if the model's not gonna tell you what it is, you should be able to decompose that complex behavior into simpler circuits or features to really start to make sense of why it did the thing that it did. - There's a separate question of, does such representation exist, which it seems like there must, or actually I'm not sure if that's the case.</p><p>And secondly, whether using this parser encoder setup, you could find it. And in this case, if you don't have labels for it that are adequate to represent it, like you wouldn't find it, right? - Yes and no. So like, we are actively trying to use dictionary learning now on the sleeper agents work, which we talked about earlier.</p><p>And it's like, if I just give you a model, can you tell me if there's this trigger in it and it's gonna start doing interesting behavior? And it's an open question whether or not when it learns that behavior, it's part of a more general circuit that we can pick up on without actually getting activations for and having it display that behavior, right?</p><p>'Cause that would kind of be cheating then. Or if it's learning some hacky trick over, like that's a separate circuit that you'll only pick up on if you actually have it do that behavior. But even in that case, the geometry of features gets really interesting because like fundamentally each feature like is in some part of your representation space and they all exist with respect to each other.</p><p>And so in order to have this new behavior, you need to carve out some subset of the feature space for the new behavior and then push everything else out of the way to make space for it. So hypothetically you can imagine you like have your model before you've taught it this bad behavior.</p><p>You know all the features or like have some coarse grained representation of them. You then fine tune it such that it becomes malicious. And then you can kind of identify this like black hole region of feature space where like everything else has been shifted away from that. And there's like this region and like you haven't put in an input that like causes it to fire.</p><p>But then you can start searching for what is the input that would cause this part of the space to fire? What happens if I activate something in this space? There are like a whole bunch of other ways that you can try and attack that problem. - This is sort of a tangent, but one interesting idea I heard was if that space is shared between models, you can imagine trying to find it in an open source model to then make like Gemma is, they said in the paper, Gemma by the way, Google's newly released open source model.</p><p>They said in the paper, it's trained using the same architecture or something like that. - I had to be honest, I didn't know because I haven't read the Gemma paper. - It's similar to something whatever as Gemini. So to the extent that's true, I don't know how much like, how much of the rec teaming you do on Gemma is like potentially helping you jailbreak into Gemini?</p><p>- Yeah, this gets into the fun space of like how universal are features across models and our Towards Monosemanticity paper looked at this a bit. And we find, I can't give you summary statistics, but like the base 64 feature, for example, which we see across a ton of models.</p><p>This is like, they're actually three of them, but they'll fire four and model base 64 encoded text, which is prevalent in like every URL. And there are lots of URLs in the training data. They have really high cosine similarity across models. So they all learn this feature. And I mean, within a rotation, right?</p><p>But it's like, yeah, yeah, yeah. - Like the actual like vector itself. - Yeah, yeah. And I wasn't part of this analysis, but yeah, it definitely finds the feature and they're like pretty similar to each other across two separate, two models, the same model architecture, but trained with different random seeds.</p><p>- It supports the quantum theory of neural scaling. It's like a hypothesis, right? We just look like all models on like a similar dataset, we will learn the same features in the same order-ish, roughly like you learn your N-grams, you learn your induction heads, and you learn like to put full stops after numbered lines and this kind of stuff.</p><p>- Hey, but by the way, okay, so this is another tangent. To the extent that that's true. And like, I guess there's evidence that that's true. Why doesn't curriculum learning work? 'Cause if it is the case that you learn certain things first, shouldn't just directly training those things first lead to better results?</p><p>- Both Gemini papers mentioned some like aspects of curriculum learning. - Okay, interesting. I mean, the fact that fine-tuning works is like evidence of curriculum learning, right? 'Cause the last things you're training on have a disproportionate impact. - I wouldn't necessarily say that. Like there's one mode of thinking in which fine-tuning is specialized.</p><p>Like you've got this like latent bundle of capabilities and you're like specializing for this particular like use case that you want. But I'm not sure how true it is. - I think the David Bell Lab paper kind of supports this, right? Like you have that ability and you're just like getting better at entity recognition.</p><p>Like fine-tuning that circuit instead of other ones. - Yeah. - Yeah. - I'm sorry, what was the thing we were talking about before? - But generally, I do think like curriculum learning is a really interesting thing that people should explore more. And it like seems very plausible. I would really love to see more analysis along the lines of the quantum theory stuff when like understanding better what do you actually learn at each stage and like decomposing that out and exploring whether or not curricula change that in a more direct way.</p><p>- By the way, I just realized. Forgot to, I just like got in conversation mode and forgot there's an audience. Curriculum learning is when you organize a data set. When you think about a human, how they learn, they don't just see like a random wiki text and they just like try to predict it, right?</p><p>They're like, we'll start you off with like a Lorax or something and then you'll learn. I don't even remember what first grade was like but you learn the things that first graders learn and then like second graders and so forth. And so you'd imagine that's-- - Sorry, we know you never got past first grade.</p><p>(all laughing) - Okay, anyways, let's get back to like the big, before we get into like a bunch of like inter details. The big picture, there's two threads I want to explore. First is, I guess it makes me a little worried that there's not even an alternative formulation of what could be happening in these models that could invalidate this approach.</p><p>Which feels like, I mean, we do know that we don't understand intelligence, right? Like there are definitely unknown unknowns here. So like the fact that there's not a null hypothesis, I don't know, I feel like what if we're just wrong and we don't even know the way in which we're wrong, which actually increases the uncertainty and yeah.</p><p>- Yeah, yeah, yeah. So it's not that there aren't other hypotheses, it's just, I have been working on superposition for like a number of years and very involved in this effort. And so I'm less sympathetic to, or well like-- - You just said they're wrong. (all laughing) - To these other approaches, especially because our recent work has been so successful.</p><p>- Yeah, it's like quite high explanatory power. Like there's this beauty, like in the scaling laws paper, there's this little bump at a particular, like the original scaling laws paper, there's a little bump. And that apparently corresponds to when the model learns induction heads. And then like after that, so it goes off track, learns induction heads, gets back on track.</p><p>Which is like an incredible piece of retroactive explanatory power. - Yeah, before I forget it though, I do have one thread on future universality that you might wanna have in. So there are some really interesting behavioral, evolutionary biology experiments on like, should humans learn a real representation of the world or not?</p><p>You can imagine a world in which we saw all venomous animals as like flashing neon pink, a world in which we survive better. And so it would make sense for us to not have a realistic representation of the world. And there's some work where they'll simulate like little basic agents and see if the representations they learn, like map to the like tools they can use and like the inputs they should have.</p><p>And it turns out if you have these little agents perform more than a certain number of tasks, given these basic tools and objects in the world, then they will learn a like ground truth representation because like there are so many possible use cases that you need for these base objects that you actually want to learn what the object actually is and not some like cheap visual heuristic or other thing.</p><p>And so to the extent that we are doing, and we haven't talked at all about like Friston's free energy principle or predictive coding or anything else, but like to the extent that all living organisms are trying to like actively predict what comes next and form like a really accurate world model, it wouldn't surprise me or I'm optimistic that we are learning genuine features about the world that are good for modeling it.</p><p>And our language models will do the same, at least, especially because we're training them on human data and human texts. - Another dinner party question. Should we be less worried about misalignment and maybe that's not even the right word for what I'm referring to, but like just alienness and shogginess from these models, given that there is future universality and there are certain ways of thinking and ways of understanding the world that are instrumentally useful to different kinds of intelligences?</p><p>Should we just be less worried about like bizarro paperclip maximizers as a result? - I think this is kind of why I bring this up as like the optimistic take. Predicting the internet is very different from what we're doing though, right? Like the models are way better at predicting next tokens than we are.</p><p>They're trained on so much garbage. They're trained on so many URLs. Like in the dictionary learning work, we find there are like three separate features for base 64 encodings. And like, even that is kind of an alien example that is probably worth me talking about for a minute. Like one of these base 64 features fired for numbers, one, like other base 64, like if it sees base 64 numbers, it'll like predict more of those.</p><p>Another fired for letters, but then there was this third one that we didn't understand and it like fired for like a very specific subset of base 64 features. And someone on the team who clearly knows way too much about base 64 realized that this was the subset that was ASCII decodable.</p><p>So you could decode it back into the ASCII characters. And the fact that the model like learned these three different features and it took us a little while to like figure out what was going on is very Shoggoth-esque. - It has a denser representation of like regions that are particularly relevant to predicting the next token.</p><p>- Yeah, because it's so, but yeah. And it's clearly doing something that humans wouldn't, right? Like you can even talk to any of the current models in base 64 and it will apply in base 64. - Right. - And you can then like decode it and it works great.</p><p>- That particular example, I wonder if that implies that the difficulty of doing interoperability on smarter models will be harder because if like it requires somebody with esoteric knowledge has just happened to see that base 64 has, I don't know, like whatever that distinction was, doesn't it imply when you have a million line pull request, it's like, there is no human that's going to be able to decode like two different reasons why the pull request, there's like two different features for this pull.</p><p>Yeah, you know what I mean? - Yeah. - So if you think- - And that's when you type a comment, like small CLs please. (all laughing) - Yeah, exactly. No, no, I mean, you could do that, right? This is like, what I was gonna say is like one technique here is anomaly detection, right?</p><p>- Yeah. - And so one beauty of dictionary learning instead of like linear probes is that it's unsupervised. You are just trying to learn to span all of the representations that the model has and then interpret them later. But if there's a weird feature that suddenly fires for the first time that you haven't seen fire before, that's a red flag.</p><p>You could also coarse grain it so that it's just a single base 64 feature. I mean, even the fact that this came up and we could see that it's specifically favors these particular outputs and it fires for these particular inputs, gets you a lot of the way there. I'm even familiar of cases from the auto interp side where a human will look at a feature and try to annotate it for it fires for Latin words.</p><p>And then when you ask the model to classify it, it says it fires for Latin words, defining plants. So it can like already like beat the human in some cases for like labeling what's going on. - So at scale, this would require an adversarial thing between models where like some model you have like millions of features potentially for GPT-6 and some, like it just a bunch of models are just trying to figure out what each of these features means.</p><p>How does that sound right, okay. - Yeah, but you can even automate this process, right? I mean, this goes back to the determinism of the model. Like you could have a model that is actively editing input text and predicting if the feature is gonna fire or not and figure out what makes it fire, what doesn't and like search the space.</p><p>- Yeah, I wanna talk more about the feature splitting 'cause I think that's like an interesting thing that has been underexplored. - Especially for scalability, I think it's underappreciated right now. - First of all, like how do we even think about, is it really just, you can keep going down and down like there's no end to the amount of features?</p><p>- I mean, so at some point, I think you might just start fitting noise or things that are part of the data, but that the model isn't actually representing. - Wait, do you wanna explain what feature splitting is? - Yeah, yeah, so it's the part before where like the model will learn however many features it has capacity for that still span the space of representation.</p><p>- So like give an example potentially. - Yeah, yeah, so you learn, if you don't give the model that much capacity for the features it's learning, concretely if you project to not as high dimensional space, it will learn one feature for birds. But if you give the model more capacity, it will learn features for all the different types of birds.</p><p>And so it's more specific than otherwise. And oftentimes like there's the bird vector that points in one direction and all the other specific types of birds point in like a similar region of the space, but are obviously more specific than the course label. - Okay, so let's go back to GPT-7.</p><p>First of all, is this a sort of like linear tax on any model to figure out? Even before that, is this a one-time thing you had to do or is this the kind of thing you have to do on every output? Or is just like one time it's not deceptive, we're good to go.</p><p>Actually, let me let you answer that. - Yeah, so you do dictionary learning after you've trained your model and you feed it a ton of inputs and you get the activations from those and then you do this projection into the higher dimensional space. And so the method is it's unsupervised in that it's trying to learn these sparse features.</p><p>You're not telling them in advance what they should be, but it is constrained by the inputs you're giving the model. I guess two caveats here. One, like we can try and choose what inputs we want. So if we're looking for theory of mind features that might lead to deception, we can put it in the sycophancy dataset.</p><p>Hopefully at some point we can move into looking at the weights of the model alone or at least using that information to do dictionary learning. But I think in order to get there, that's like such a hard problem that you need to make traction on just learning what the features are first.</p><p>But yeah, so what's the cost of this? - Can you repeat the last sentence? Weights of the model alone. - So like right now we just have these neurons in the model. They don't make any sense. We apply dictionary learning. We get these features out. They start to make sense.</p><p>But that depends on the activations of the neurons. The weights of the model itself, like what neurons are connected to what other neurons, certainly has information in it. And the dream is that we can kind of bootstrap towards actually making sense of the weights of the model that are independent of the activations of the data.</p><p>I mean, this is, I'm not saying we've made any progress here. It's a very hard problem, but it feels like we'll have a lot more traction and be able to like sanity check what we're finding with the weights if we're able to pull out features first. - For the audience, weights are permanent.</p><p>Well, I don't know if permanent is the right word, but like they are the model itself. Whereas activations are the sort of like artifacts of any single call. - In a brain metaphor, the weights are like the actual connection scheme between neurons and the activations of the current neurons that are lining up, basically.</p><p>- Yeah, okay, so there's gonna be two steps to this for GPT-7 or whatever model we're concerned about. One, first correct me if I'm wrong, but like training the sparse autoencoder and like do the unsupervised projection into a wider space of features that have a higher fidelity to like what is actually happening in the model.</p><p>And then secondly, label those features. 'Cause let's say like the cost of training the model is N, what will those two steps cost relative to N? - We will see, like it really depends on two main things. What is your expansion factors? Like how much are you projecting into the higher dimensional space?</p><p>And how much data do you need to put into the model? How many activations do you need to give it? But this brings me back to the feature splitting to a certain extent, because if you know you're looking for specific features, you can start with a really cheaper like course representation.</p><p>So maybe my expansion factor is like only two. So like I have a thousand neurons I'm projecting to a 2000 dimensional space. I get 2000 features out, but they're really coarse. And so previously I had the example for birds. Let's move that example to like, I have a biology feature.</p><p>And, but I really care about if the model has representations for bioweapons and is trying to manufacture them. And so what I actually want is like an anthrax feature. What you can then do is rather than, and let's say the anthrax, you only see the anthrax feature if instead of going from a thousand dimensions to 2000 dimensions, I go to a million dimensions, right?</p><p>And so you can kind of imagine this big tree of semantic concepts where like biology splits into like cells versus like whole body biology. And then further down it splits into all these other things. So rather than needing to immediately go from a thousand to a million, and then picking out that one feature of interest, you can find the direction that the biology feature is pointing in, which again is very coarse, and then selectively search around that space.</p><p>So like only do dictionary learning if this, if something in the direction of the biology feature fires first. And so the computer science metaphor here would be like, instead of doing breadth-first search, you're able to do depth-first search, where you're only recursively expanding and exploring a particular part of this like semantic tree of features.</p><p>- Although given the way that these features are not organized in things that are intuitive for humans, right, like, 'cause we just don't know how to deal with basic C4, so we don't have that many, you know, we just don't dedicate that much, like whatever, firmware to like deconstructing which kind of basic C4 it is.</p><p>How would we know that the subjects, and this will go back to maybe the MOE discussion we'll have of, I guess we might as well talk about it, but like in mixture of experts, the mixture of paper talked about how they couldn't find, the experts weren't specialized in a way that we could understand.</p><p>There's not like a chemistry expert or a physics expert or something. So why would you think that like, it will be like biology feature and then deconstruct rather than like blah, and then you just deconstruct and it's like anthrax and you're like shoes and whatever. - So I haven't read the Mistral paper, but I think that the heads, I mean, this goes back to like, if you just look at the neurons in a model, they're polysemantic.</p><p>And so if all they did was just look at the neurons in a given head, it's very plausible that it's also a polysemantic because of superposition. - I wanna just tug on a thread that Dorcas mentioned there. Have you seen in the subtrees when you expand them out, like something in a subtree, which like you really wouldn't guess that it should be there based on like the higher level of traction.</p><p>- So this is a line of work that we haven't pursued as much as I want to yet. But I think we're planning to, I hope that maybe external groups do as well. Like what is the geometry of features? - What's the geometry? Exactly. - And how does that change over time?</p><p>Like what would suck if like anthrax feature happened to be like below the like, you know, coffee can? Subtree or something like that, right? - Totally, totally. - And that feels like the kind of thing that you could quickly try and find like proof of, which would then like mean that you need to like, then solve that problem.</p><p>- Yeah, yeah. - Inject more structure into the geometry. - Totally. I mean, it would really surprise me, I guess, especially like given how linear the models seem to be. - Completely agree. - That like there isn't some component of the anthrax feature, like vector, that is similar to and looks like the biology vector and that they're not in a similar part of the space.</p><p>But yes, I mean, ultimately machine learning is empirical. - Yeah. - We need to do this. - We need to do the research. - I think it's going to be pretty important for certain aspects of scaling dictionary learning. - Yeah, yeah. Interesting. On the MOE discussion. - Yeah. - There's an interesting scaling vision transformers paper that Google put out a little while ago, where they like do image net classification with like an MOE.</p><p>And they find really clear class specialization there for experts. Like there's a clear dog expert. - Wait, so like the mixture of people just not do a good job of like identifying those? - I think it's hard. Like, and like, it's entirely possible that, like in some respects, there's almost no reason that like all of the different archive like features should go to one expert.</p><p>Like you could have biology, like let's say, I don't know what buckets they had in their paper, but let's say they had like archive papers as like one of the things. You could imagine like biology papers going here, math papers going here, and all of a sudden you're like breakdown is like ruined.</p><p>But that vision transformer one, where the class separation is really clear and obvious gives I think some evidence towards the specialization hypothesis. - So I think images are also in some ways just easier to interpret than text. - Yeah, exactly. - And like, so Chris Ola's like interpretability work on AlexNet and these other models, like in the original AlexNet paper, they actually split the model into two GPUs just because they couldn't, like GPUs were so bad back then, relatively speaking, right?</p><p>Like still great at the time. That was one of the big innovations of the paper, but they find branch specialization and there's a Distill Pub article on this where like colors go to one GPU and like Gabor filters and like line detectors go to the other. And then like all of the other.</p><p>- Really? - Yeah, yeah, yeah. And then like all of the other interpretability work that was done, like the floppy ear detector, right? Like that just was a neuron in the model that you can make sense of. You didn't need to disentangle superposition, right? So just different data set, different modality.</p><p>- Like, I think a wonderful research project to do if someone is like out there listening to this would be to try and disentangle, like take some of the techniques that Trenton's team has worked on and try and disentangle the neurons in the Mixtral model, which is open source.</p><p>I think that's a fantastic thing to do 'cause it feels intuitively like there should be. They didn't demonstrate any evidence that there is. There's also like in general, a lot of evidence that there should be specialization. Go and see if you can find it. And that's work that Anthropica has published most of the stuff on, as I understand it, like dense models, basically.</p><p>That is a wonderful research project to try. - And given Dworkesh's success with the Vesuvius challenge. - Yeah. - We should be pitching more projects because they will be solved if we talk about them on the podcast. - What I was thinking about after the Vesuvius challenge was like, wait, I knew, like Nat had told me about it before it dropped because we recorded the episode before it dropped.</p><p>Why did I not even try? Like, you know what I mean? Like, I don't know, like Luke is obviously very smart and like, yeah, he's an amazing kid, but like he showed that like a 21-year-old on like some 1070 or whatever he was working on could do this. I don't know, like I feel like I should have.</p><p>So you know what, before this episode drops, I'm gonna meet my, I'm gonna try to try this guy. - Dworkesh, you're gonna make an interpretability spot. - No, no, no, I'm not gonna like try to go reach everybody. Like, I don't know, it's like, I was honestly thinking about that kind of experience.</p><p>Like, wait, I shouldn't, like, why didn't I, fuck. - Yeah, get your hands dirty. - Yeah. - Dworkesh's request for research. - Oh, I wanna harp back on this, like the neuron thing. You said, I think a bunch of your papers have said there's more features than there are neurons.</p><p>And this is just like, wait a second. I don't know, like a neuron is like, weights go in and a number comes out. That's like, a number comes out. You know what I mean? Like, that's so little information, like there's, do you mean like there's like street names and like species and whatever, there's like more of those kinds of things than there are, like a number comes out in a model?</p><p>- That's right, yeah. - But how is it, a number comes out is like so little information. How is that encoding for like-- - Superposition. You're just encoding, you're encoding a ton of features in these high dimensional vectors. - In a brain, is there like an axon of firing or however you think about it?</p><p>Like, I don't know how you think about like, how much like superposition is there in the human brain? - Yeah, so Bruno Olshausen, who I think of as the leading expert on this, thinks that all the brain regions you don't hear about are doing a ton of computation and superposition.</p><p>So everyone talks about V1 as like having Gabor filters and detecting lines of certain various sorts. And no one talks about V2. And I think it's because like, we just haven't been able to make sense of it. - What is V2? - It's like the next part of the visual processing stream.</p><p>And it's like, yeah, so I think it's very likely. And fundamentally, like superposition seems to emerge when you have high dimensional data that is sparse. And to the extent that you think the real world is that, which I would argue it is, we should expect the brain to also be under-parameterized in trying to build a model of the world and also use superposition.</p><p>- You can get a good intuition for this, and correct me if this example is wrong, in like a 2D plane, right? Let's say you have like two axes, right? Which represents like a two dimensional, like feature space here, like two neurons basically. And you can imagine them each like turning on to various degrees, right?</p><p>And that's like your X coordinate and your Y coordinate. But you can like, now like map this onto a plane. You can actually represent a lot of different things in like different parts of the plane. - Oh, okay. So crucially, then superposition is not an artifact of a neuron.</p><p>It is an artifact of like the space that is created. - It's a combinatorial code. - Yeah. - Yeah, exactly. - Okay, cool. - Yeah, thanks. - So, I mean, we kind of talked about this, but like, I think it's just like kind of wild that it seems to the best of our knowledge, the way intelligence works in these models and then presumably also in brains.</p><p>It's just like, there's a stream of information going through that has quote unquote features that are infinitely, or at least to a large extent to just like splittable and like you can expand out a tree of like what this feature is and what's really happening is a stream. Like that feature is getting turned into this other feature or this other feature is added.</p><p>I don't know. It's like, that's not something I would have just like thought like that's what intelligence is. You know what I mean? It's like a surprising thing. It's not what I would have expected necessarily. - What did you think it was? - I don't know, man. (all laughing) - I mean, yeah.</p><p>- Gophi, Gophi. He's a Gophi. - Well, actually, so that's a great segue because all of this feels like Gophi. Like you're using distributed representations, but you have features and you're applying these operations to the features. I mean, the whole field of vector symbolic architectures which is this computational neuroscience thing, all you do is you put vectors in superposition and which is literally a summation of two high dimensional vectors and you create some interference, but if it's high dimensional enough, then you can represent them.</p><p>And you have variable binding where you connect one by another. And like, if you're dealing with binary vectors, it's just the XOR operation. So you have AB, you bind them together. And then if you query with A or B again, you get out the other one. And this is basically the like key value pairs from attention.</p><p>And with these two operations, you have a Turing complete system, which you can, if you have enough nested hierarchy, you can represent any data structure you want, et cetera, et cetera. Yeah. - Okay, let's go back to the super intelligence. So like walk me through GPD 7. You've got like the sort of depth first search on its features.</p><p>Okay, GPD 7 has been trained. What happens next? Your research has succeeded. GPD 7 has been trained. What are we doing now? - We try and get it to do as much interpretability work and other like safety work as possible. - No, but like a concrete, like what is, what has happened such that you're like, "Cool, let's deploy GPD 7." - Oh, geez.</p><p>I mean, like we have our responsible scaling policy, which has been really exciting to see other labs adopt. And- - Like specifically from the perspective of, your research has net, like Trenton, given your research, you got the, we got the thumbs up on GPD 7 from you, or actually we should say cloud, whatever.</p><p>And then, oh, like what is the basis on which you're telling the team, like, "Hey, let's go ahead." - I mean, I think we need to make a lot more. If it's as capable as GPD 7, like implies here, I think we need to make a lot more interpretability progress to be able to like comfortably give the green light to deploy it.</p><p>Like, I would be like, definitely not. I'd be crying. (all laughing) Maybe my tears would interfere with the GPUs. - But like, what is- - Or TPUs. - Guys, Gemini 5 TPUs, right? (all laughing) - But like, what, what, given the way your research is progressing, like, what does it kind of look like to you?</p><p>Or like, well, if this succeeded, what would it mean for us to okay GPD 7 based on your methodology? - I mean, ideally we can find some compelling deception circuit, which lights up when the model knows that it's not telling the full truth to you. - Why can't you just do any linear probe like Colin Birds did?</p><p>- So the CCS work is not looking good in terms of replicating, or like actually finding truth directions. And like, in hindsight, it's like, well, why should it have worked so well? But linear probes, like you need to know what you're looking for. And it's like a high dimensional space and it's really easy to pick up on a direction that's just not- - Wait, but don't you also, here you need to label the features.</p><p>So you still need to know- - Well, you need to label them post hoc, but it's unsupervised. You're just like, give me the features that explain your behavior, is the fundamental question, right? It's like, like, like, like the actual setup is, we take the activations, we project them to this higher dimensional space, and then we project them back down again.</p><p>So it's like reconstruct or do the thing that you were originally doing, but do it in a way that's sparse. - By the way, for the audience, linear probe is, you just like classify the activations. I don't know, from what I vaguely remember about the paper was like, if it's like telling a lie, then you like, you just train a classifier on like, is it, yeah, in the end, was it a lie or is it just like wrong or something?</p><p>I don't know. - It was like true or false question. - Yeah, it's like a classifier on the activations. - So yeah, like right now what we do for GPT-7, like ideally we have like some deception circuit that we've identified that like appears to be really robust. And it's like- - Well, like, so you've done the projecting out to the million, whatever features or something.</p><p>Is the circuit, 'cause we, maybe we're using feature and circuit interchangeably when they're not. Is there like a deception, like what is that? - So I think there are features across layers that create a circuit. - Yeah. - And hopefully the circuit gives you a lot more specificity and sensitivity than an individual feature.</p><p>And it's like, hopefully we can find a circuit that is really specific to you being deceptive, the model deciding to be deceptive in cases that are malicious, right? Like I'm not interested in a case where it's just doing theory of mind to like help you write a better email to your professor.</p><p>And I'm not even interested in cases where the model is necessarily just like modeling the fact that deception has occurred. - But doesn't all this require you to have labels for all those examples? And if you have those labels, then like whatever faults that the linear probe has on the, like maybe you've like labeled a long thing or whatever, wouldn't the same thing apply to the labels you've come up with for the unsupervised features you've come up with?</p><p>- So in an ideal world, we could just train on like the whole data distribution and then find the directions that matter to the extent that we need to reluctantly narrow down the subset of data that we're looking over just for the purposes of scalability. We would use data that looks like the data you'd use to fit a linear probe.</p><p>But again, we're not, like with a linear probe, you're also just finding one direction. Like we're finding a bunch of directions here. - And I guess the hope is like, you've found like a bunch of things that light up when it's being deceptive. And then like you can figure out why some of those things are lighting up in this part of the distribution and not this other part and so forth.</p><p>- Totally, yeah. - Do you anticipate you'll really understand? Like, I don't know, like the current models you've studied are pretty basic, right? Do you think you'll be able to understand why GPT-7 fires in certain domains but not in other domains? - I'm optimistic. I mean, we've, so I guess one thing is this is a bad time to answer this question because we are explicitly investing in the longer term of like ASL-4 models, which GPT-7 would be.</p><p>But like, so we split the team where a third is focused on scaling up dictionary learning right now. And that's been great. I mean, we publicly shared some of our eight layer results. We've scaled up quite a lot past that at this point. But the other two groups, one is trying to identify circuits and then the other is trying to get the same success for attention heads.</p><p>So we're setting ourselves up and building the tools necessary to really find these circuits at a compelling way. But it's gonna take another, I don't know, six months before that's like really working well. But I can say that I'm like optimistic and we're making a lot of progress. - What is the highest level feature you've found so far?</p><p>- Ooh. - Like it's base 64 or whatever. It's like, maybe it's just like in the symbolic species language, the book you recommended, there's like indexical things where you're just, I forgot what all the labels were, but like there's things where you're just like, you see a tiger and you're like run and whatever, you know, just like a very sort of behaviorist thing.</p><p>And then there's like a higher level of which when I refer to love, it refers to like a movie scene or my girlfriend or whatever. You know what I mean? So it's like the top of the tent. - Yeah, yeah, yeah, yeah. - What is the highest level association or whatever you found?</p><p>- I mean, probably one of the ones that we publicly, well, publicly one of the ones that we shared in our update. So I think there were some related to like love and like sudden changes in scene, particularly associated with like wars being declared. There are like a few of them in there in that post, if you want to link to it.</p><p>Yeah. But even like Bruno Olshausen had a paper back in 2018, 19, where they applied a similar technique to a BERT model and found that as you go to deeper layers of the model, things become more abstract. So I remember like in the earlier layers, there'd be a feature that would just fire for the word park, but later on there was a feature that fired for park as like a last name, like Lincoln Park, or like it's like a common Korean last name as well.</p><p>And then there was a separate feature that would fire for parks as like grassy areas. So there's other work that points in this direction. - What do you think we'll learn about human psychology from the interoperability stuff? - Oh gosh. - I'll give you a specific example. I think like one of the ways one of your updates put it was persona lock-in.</p><p>You don't remember Sidney Bang or whatever. It locked into, I think it was actually quite an endearing. - Yeah, yeah. (all laughing) - I thought it's so funny. I'm glad it's back in co-pilot. - Oh really? - Oh yeah, it's been misbehaving recently. Actually, this is another sort of threat to explore, but there was a funny one where, I think it was like to the New York Times reporter, it was nagging him or something.</p><p>And it was like, "You are nothing. "Nobody will ever believe you. "You are insignificant and do whatever." It was like the most gaslighting. - I tried to convince him to break up with his wife. - Yeah, okay. Okay, actually, so this is an interesting example. I don't even know where I was going with this to begin with, but whatever.</p><p>Maybe I got another thread. But the other thread I want to go on is that's, yeah, okay, actually, personal notes, right? So is that a feature that Sidney Bang having this personality is a feature versus another personality can get locked into? And also, is that fundamentally what humans are like too, where, I don't know, in front of all the different people, I'm like a different sort of personality or whatever.</p><p>Is that the same kind of thing that's happening to ShaggyBT when he gets RL-ed? I don't know, a whole cluster of questions that can answer them and whatever. - Yeah, I really want to do more work. I guess "The Sleeper Agents" is in this direction of what happens to a model when you find tuna when you RLHF it, these sorts of things.</p><p>I mean, maybe it's trite, but you could just say you conclude that people contain multitudes, right? In so much as they have lots of different features. There's even the stuff related to the Waluigi effects of in order to know what's good or bad, you need to understand both of those concepts.</p><p>And so we might have to have models that are aware of violence and have been trained on it in order to recognize it. Can you post-hoc identify those features and ablate them in a way where maybe your model's slightly naive, but you know that it's not going to be really evil?</p><p>Totally, that's in our toolkit, which seems great. - Oh, really? So you, GPT-7, I don't know, it pulls the Sydney Bing, and then you figure out what were the causally irrelevant pathways or whatever, you modify, and then the pathway to you looks like you just changed those. But you were mentioning earlier, there's a bunch of redundancy in the model.</p><p>- Yeah, so you need to account for all that. But we have a much better microscope into this now than we used to, like sharper tools for making edits. - And it seems like, at least from my perspective, that seems like one of the primary way of to some degree confirming the safety or the reliability of the model, where you can say, okay, we found the circuits responsible, we've ablated them, and under a battery of tests, we haven't been able to now replicate the behavior which we intended to ablate.</p><p>And that feels like the sort of way of measuring model safety in future, as I would understand. - Are you worried? - That's why I'm incredibly hopeful about their work. 'Cause to me, it seems like so much more precise tool than something like RLHF. RLHF, you're very prey to the black swan thing.</p><p>You don't know if it's gonna do something wrong in a scenario that you haven't measured. Whereas here, at least, you have somewhat more confidence that you can completely capture the behavior set, or the feature set of the model and selectively ablate. - Although not necessarily that you've accurately labeled.</p><p>- Not necessarily, but with a far higher degree of confidence than any other approach that I've seen. - What are your unknown unknowns for superhuman models? In terms of this kind of thing where, I don't know, how are the labels that are gonna be given things on which we can determine this thing is cool, this thing is a paperclip maximizer, or whatever.</p><p>- I mean, we'll see, right? Like, I do, like the superhuman feature question is a very good one. Like, I think we can attack it, but we're gonna need to be persistent. And the real hope here is, I think, automated interpretability. - Yeah. - And even having debate, right?</p><p>You could have the debate set up where two different models are debating what the feature does. And then they can actually go in and make edits and see if it fires or not. - But it is just this wonderful, like, closed environment that we can iterate on really quickly.</p><p>That makes me optimistic. - Do you worry about alignment succeeding too hard? So like, if I think about, I would not want either companies or governments, whoever ends up in charge of these AI systems to have the level of fine-grained control that if your agenda succeeds, we would have over AIs, both for the ickiness of having this level of control over an autonomous mind.</p><p>And second, just like, I don't fucking trust. I don't fucking trust these guys. You know, I'm just kind of uncomfortable with like the loyalty feature is turned up and like, you know what I mean? And yeah, like how much worry do you have about having too much control over the AIs and specifically, not you, but like whoever ends up in charge of these AI systems, just being able to lock in whatever they want?</p><p>- Yeah. Yeah, I mean, I think it depends on what government exactly has control and like what the moral alignment is there. But that is like, that whole value locking argument is in my mind, it's like definitely one of the strongest contributing factors for why I am working on capabilities at the moment, for example, just like, I think the current player set actually like is extremely well-intentioned.</p><p>And I mean, for this kind of problem, I think we need to be extremely open about it. And like, I think directions like publishing the constitution that you expect your model to abide by and then like trying to make sure you like RLHF it towards that and ablate that and have the ability for everyone to offer like feedback and contribution to that is really important.</p><p>- Sure, or alternatively, like don't deploy when you're not sure, which would also be bad because then we just never catch it. - Right. Yeah, exactly. - I mean, paperclip. Okay, some rapid fire. What is the bus factor for Gemini? - I think there are, yeah, a number of people who are really, really critical that if you took them out, then the performance of the program would be dramatically impacted.</p><p>This is both on modeling like slash making decisions about like what to actually do and importantly on infrastructure side of things. Like it's just the stack of complexity builds, particularly when like somewhere like Google has so much like vertical integration. Do you have, when you have people who are experts, it becomes, they become quite important.</p><p>- Yeah, although I think it's an interesting note about the field that people like you can get in and in a year or so you're making important contributions. And I, especially with Anthropic, but many different labs have specialized in hiring like total outsiders, physicists or whatever. And you just like get them up to speed and they're making important contributions.</p><p>I don't know, I feel like you couldn't do this in like a bio lab or something. It's like an interesting note on the state of the field. - I mean, bus factor doesn't define how long it would take to recover from it, right? - Sure, to recover from, yeah.</p><p>- And deep learning research is an art. And so you kind of learn how to read the lost curves or set the hyper parameters in ways that empirically seem to work well. - But it's also like organizational things, like creating context. When I think one of the most important and difficult skills to hire for is creating this like bubble of context around you that makes other people around you more effective and know what the right problem to work on.</p><p>And like that is a really tough to replicate thing. - Yes, yeah, totally. - Who are you paying attention to now in terms of, there's a lot of things coming down the pike of multi-modality, long context, maybe agents, extra reliability. Who is thinking well about what that implies? - It's a tough question.</p><p>I think a lot of people look internally these days. - Sure. - For like their sources of insight or like progress. And like, we all have obviously, there's a research programs and like directions that are tended over the next couple of years. And I suspect, yeah, that most people, as far as like betting on what the future will look like, refer to like an internal narrative.</p><p>- Yeah, yeah. - That is like difficult to share. - Yeah, if it works well, it's probably not being published. - I mean, that was one of the things in the will scaling post. I was referring to something you said to me, which is, I miss the undergrad habit of just reading a bunch of papers.</p><p>'Cause now there's nothing worth reading is published. (all laughing) - And the community is progressively getting like more on track with what I think are like the right and important directions. - You're watching it like an agent here. (all laughing) - No, but I guess like it is tough.</p><p>There used to be this like signal from big labs about like what would work at scale. And it's currently really hard for academic research to like find that signal. And I think getting like really good problem taste about what actually matters to work on is really tough. Unless you have, again, the feedback signal of like what will work at scale and what is currently holding us back from scaling further or understanding our models further.</p><p>This is something where like I wish more academic research would go into fields like Interp, which are legible from the outside. Anthropic deliberately publishes all its research here. And it seems like underappreciated in the sense that I don't know why there aren't dozens of academic departments trying to follow Anthropics in the Interp research.</p><p>'Cause it seems like an incredibly impactful problem that doesn't require ridiculous resources. And like has all the flavor of like deeply understanding the basic science of what is actually going on in these things. So I don't know why people like focus on pushing model improvements as opposed to pushing like understanding improvements in a way that I would have like typically associated with academic science in some ways.</p><p>- Yeah, I do think the tide is changing there for whatever reason. And like Neil Nanda has had a ton of success promoting interpretability in a way where like Chris Ola hasn't been as active recently in pushing things. Maybe because Neil's just doing quite a lot of the work, but like, I don't know, four or five years ago, he was like really pushing and like talking at all sorts of places and these sorts of things.</p><p>And people weren't anywhere near as receptive. Maybe they've just woken up to like deep learning matters and it's clearly useful post-track GPT, but yeah, it is kind of striking. - All right, cool. And okay, I'm trying to think, what is a good last question? I mean, the one I'm going to those thinking of is like, do you think models enjoy next token prediction?</p><p>- Yeah. - Do models believe in love? (all laughing) - We have this sense of things that are rewarded in our accessible environment. There's like this deep sense of fulfillment that we think we're supposed to get from them or often people do, right? Of like community or sugar or whatever we wanted on the African Savannah.</p><p>Do you think like in the future models are trained with RL and everything, a lot of post-training on top of whatever, but they're like some in the way we were just a really like ice cream, they'll just be like, hi, just to predict the next token again. You know what I mean?</p><p>Like in the good old days. - So there's this ongoing discussion of like, are models sentient or not? And like, do you thank the model when it helps you? - Yeah. - But I think if you want to thank it, you actually shouldn't say thank you. You should just give it a sequence that's very easy to predict.</p><p>(all laughing) And the even funnier part of this is there's some work on if you just give it the sequence, A, like over and over again, then eventually the model will just start spewing out all sorts of things that otherwise wouldn't ever say. And so, yeah, I won't say anything more about that, but you can, yeah, you should just give your model something very easy to predict as a nice little treat.</p><p>- This is what the OEM ends up being. We just found the universe and like. (all laughing) - But do we like things that are like easy to predict? Aren't we constantly in search of like the dose of- - The bits of entropy? - Yeah, the bits of entropy, exactly, right?</p><p>Shouldn't you be giving it things that are just slightly too hard to predict? (all laughing) Just out of reach. - Yeah, but I wonder, like, at least from the free energy principle perspective, right? Like you don't like, you don't want to be surprised. And so maybe it's this, like, I don't feel surprised I feel in control of my environment.</p><p>And so now I can go and seek things and I've been predisposed to like, in the long run it's better to explore new things right now. Like leave the rock that I've been sheltered under ultimately leading me to like build a house or like some better structure. But we don't like surprises.</p><p>I think most people are very upset when like expectation does not meet reality. - That's why babies like love watching the same show over and over and over again, right? - Yeah, interesting. Yeah, I can see that. - Oh, I guess they're learning to model it and stuff too.</p><p>- Yeah. - Yeah. - Okay, well hopefully this will be the repeat. (all laughing) That the AI has learned to love. Okay, cool. I think that's a great place to wrap. I should also mention that the better part of what I know about AI, I've learned from just talking with you guys.</p><p>You know, we've been good friends for about a year now. So yeah, I mean, yeah, I appreciate you guys getting me up to speed here and yeah. - You guys have great questions. It's really fun to hang and chat. - Great, great. - I really treasure that time together.</p><p>- Yeah, you're getting a lot better at pickleball. (all laughing) - Hey, we're trying to progress the tenders, come on. (all laughing) - Awesome, cool, cool. Awesome, thanks. - Hey everybody. I hope you enjoyed that episode. As always, the most helpful thing you can do is to share the podcast.</p><p>Send it to people you think might enjoy it, put it in Twitter, your group chats, et cetera. Just splits the world. Appreciate you listening. I'll see you next time. Cheers. (upbeat music) (upbeat music)</p></div></div></body></html>