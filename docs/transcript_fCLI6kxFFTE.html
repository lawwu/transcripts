<html><head><title>Playing Guitar in a Self-Driving Car</title></head><body><a href="index.html">back to index</a><h2>Playing Guitar in a Self-Driving Car</h2><a href="https://www.youtube.com/watch?v=fCLI6kxFFTE"><img src="https://i.ytimg.com/vi_webp/fCLI6kxFFTE/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./fCLI6kxFFTE.html">Whisper Transcript</a> | <a href="./transcript_fCLI6kxFFTE.html">Transcript Only Page</a></div><br><h3>Transcript</h3><div style="max-width: 600px;"><p>The official name for our car is the MIT Human Centered Autonomous Vehicle. But amongst our team her name is Betty, as in Black Betty. Today we'd like to demonstrate activity recognition. Two stream neural network, optical flow and RGB coming from the video of the driver's body. The output of the network is a prediction of the driver's activity, whether that's texting on the phone, drinking water, or playing guitar. Alright, hey Betty, let's go for a drive. Sure, Lex. With music? Yes, play your theme song. Would you like to play along? Yes. Ready? Yes.</p></div></body></html>