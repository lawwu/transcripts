<html><head><title>Lesson 17: Deep Learning Foundations to Stable Diffusion</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 17: Deep Learning Foundations to Stable Diffusion</h2><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc"><img src="https://i.ytimg.com/vi/vGsc_NbU7xc/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=0">0:0</a> Changes to previous lesson<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=470">7:50</a> Trying to get 90% accuracy on Fashion-MNIST<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=718">11:58</a> Jupyter notebooks and GPU memory<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=899">14:59</a> Autoencoder or Classifier<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=965">16:5</a> Why do we need a mean of 0 and standard deviation of 1?<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1281">21:21</a> What exactly do we mean by variance?<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1556">25:56</a> Covariance<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1773">29:33</a> Xavier Glorot initialization<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2127">35:27</a> ReLU and Kaiming He initialization<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2212">36:52</a> Applying an init function<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2339">38:59</a> Learning rate finder and MomentumLearner<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2410">40:10</a> What’s happening is in each stride-2 convolution?<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2552">42:32</a> Normalizing input matrix<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2769">46:9</a> 85% accuracy<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2850">47:30</a> Using with_transform to modify input data<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2898">48:18</a> ReLU and 0 mean<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3126">52:6</a> Changing the activation function<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3309">55:9</a> 87% accuracy and nice looking training graphs<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3436">57:16</a> “All You Need Is a Good Init”: Layer-wise Sequential Unit Variance<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3835">63:55</a> Batch Normalization, Intro<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3999">66:39</a> Layer Normalization<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4547">75:47</a> Batch Normalization<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5008">83:28</a> Batch Norm, Layer Norm, Instance Norm and Group Norm<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5171">86:11</a> Putting all together: Towards 90<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5322">88:42</a> Accelerated SGD<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5612">93:32</a> Regularization<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5857">97:37</a> Momentum<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6332">105:32</a> Batch size<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6397">106:37</a> RMSProp<br><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6687">111:27</a> Adam: RMSProp plus Momentum<br><br><div style="text-align: left;"><a href="./vGsc_NbU7xc.html">Whisper Transcript</a> | <a href="./transcript_vGsc_NbU7xc.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everybody and welcome to lesson 17 of practical deep learning for coders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6" target="_blank">00:00:06.760</a></span> | <span class="t">Really excited about what we're going to look at over the next lesson or two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=12" target="_blank">00:00:12.840</a></span> | <span class="t">It's actually been turning out really well, much better than I could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=15" target="_blank">00:00:15.920</a></span> | <span class="t">hoped. So I can't wait to dive in. Before I do I just going to mention a couple of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=20" target="_blank">00:00:20.280</a></span> | <span class="t">minor changes that I made to our mini AI library this week. One was I went back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=26" target="_blank">00:00:26.200</a></span> | <span class="t">our callback class in the learner notebook and I did decide in the end to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=31" target="_blank">00:00:31.440</a></span> | <span class="t">add a dunder getAtra to it that just adds these four these four attributes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=40" target="_blank">00:00:40.400</a></span> | <span class="t">and for these four attributes it passes it down to self.learn. So in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=45" target="_blank">00:00:45.920</a></span> | <span class="t">callback you'll be able to refer to model to get self.learn.model, opt will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=50" target="_blank">00:00:50.440</a></span> | <span class="t">self.learn.opt, batch will be self.learn.batch, epoch will be self.learn.epoch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=55" target="_blank">00:00:55.560</a></span> | <span class="t">You can change these you know you could subclass the callback and add your own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=60" target="_blank">00:01:00.880</a></span> | <span class="t">to underscore forward or you could remove things from underscore forward or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=64" target="_blank">00:01:04.840</a></span> | <span class="t">whatever but I felt like these four things I access a lot and I was sick of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=68" target="_blank">00:01:08.200</a></span> | <span class="t">typing self.learn and then I added one more property which is cause in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=74" target="_blank">00:01:14.760</a></span> | <span class="t">callback there'll be a self.training which saves from typing self.learn.model.training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=81" target="_blank">00:01:21.120</a></span> | <span class="t">since we have model you could get rid of the learn but still I mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=86" target="_blank">00:01:26.120</a></span> | <span class="t">you so often have to check the training now you can just get self.training in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=90" target="_blank">00:01:30.480</a></span> | <span class="t">in a callback so that was one change I made. The second change I made was I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=99" target="_blank">00:01:39.040</a></span> | <span class="t">found myself getting a bit bored of adding train_cb every time so what I did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=106" target="_blank">00:01:46.920</a></span> | <span class="t">was I took the four training methods from the momentum_learner subclass and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=114" target="_blank">00:01:54.700</a></span> | <span class="t">I've moved them into a train_learner subclass along with zero grad so now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=121" target="_blank">00:02:01.080</a></span> | <span class="t">momentum_learner actually inherits from train_learner and just adds momentum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=126" target="_blank">00:02:06.240</a></span> | <span class="t">this kind of a quirky momentum method and changes zero grad to do the momentum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=133" target="_blank">00:02:13.520</a></span> | <span class="t">thing so you yeah so we'll be using train_learner quite a bit over the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=137" target="_blank">00:02:17.880</a></span> | <span class="t">lesson or two so train_learner is just a learner which has the usual training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=144" target="_blank">00:02:24.160</a></span> | <span class="t">it's exactly the same that fastai2 has or you'd have in most PyTorch training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=150" target="_blank">00:02:30.160</a></span> | <span class="t">loops and obviously by using this you lose the ability to change these with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=156" target="_blank">00:02:36.560</a></span> | <span class="t">callback so it's a little bit less flexible okay so those are little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=163" target="_blank">00:02:43.280</a></span> | <span class="t">changes and then I made some changes to what we looked at last week which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=168" target="_blank">00:02:48.860</a></span> | <span class="t">activations notebook and specifically okay so I added a hooks callback so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=184" target="_blank">00:03:04.160</a></span> | <span class="t">previously we had a hooks class and it didn't really require too much ceremony</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=190" target="_blank">00:03:10.040</a></span> | <span class="t">to use but I thought we could make it even simpler and a bit more fastaiish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=194" target="_blank">00:03:14.360</a></span> | <span class="t">or miniaiish by putting hooks into a callback so this callback as usual you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">pass it a function that's going to be called for your hook and you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=209" target="_blank">00:03:29.640</a></span> | <span class="t">optionally pass it a filter as to what modules you want to hook and then in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=214" target="_blank">00:03:34.640</a></span> | <span class="t">before fit it will filter the modules in the learner and so this is one of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=226" target="_blank">00:03:46.480</a></span> | <span class="t">things we can now get rid of we don't need to learn here because model is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=229" target="_blank">00:03:49.760</a></span> | <span class="t">of the four things we have a shortcut to and then here we're going to create the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=233" target="_blank">00:03:53.360</a></span> | <span class="t">hooks object and put it in hooks and so one thing that's convenient here is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=241" target="_blank">00:04:01.800</a></span> | <span class="t">hook function now you don't have to worry and we can get rid of learned up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=244" target="_blank">00:04:04.640</a></span> | <span class="t">model you don't have to worry about checking in your hook functions whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=248" target="_blank">00:04:08.440</a></span> | <span class="t">in training or not it always checks whether you're in training and if so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=251" target="_blank">00:04:11.280</a></span> | <span class="t">calls that hook function you passed in and after it finishes it removes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=255" target="_blank">00:04:15.320</a></span> | <span class="t">hooks and you can iterate through the hooks and get the length of the hooks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=259" target="_blank">00:04:19.280</a></span> | <span class="t">because it just passes these iterators and length down to self dot hooks so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=264" target="_blank">00:04:24.480</a></span> | <span class="t">show you how this works we can create a hooks callback we can use the same append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=273" target="_blank">00:04:33.120</a></span> | <span class="t">stats and then we can run the model and so as it's training what we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=286" target="_blank">00:04:46.960</a></span> | <span class="t">do is yeah we can now then here we go so we just added that as an extra callback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=294" target="_blank">00:04:54.000</a></span> | <span class="t">to our fit function I don't remember if we had the extra callbacks before I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=297" target="_blank">00:04:57.920</a></span> | <span class="t">not sure we did so just to explain it's just I just added extra callbacks here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=304" target="_blank">00:05:04.360</a></span> | <span class="t">in the fit function and we're just adding any extra callbacks yeah so then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=316" target="_blank">00:05:16.600</a></span> | <span class="t">now we've got that callback that we created because we can get iterate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=320" target="_blank">00:05:20.240</a></span> | <span class="t">through it and so forth we can just iterate it through that callback as if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=324" target="_blank">00:05:24.280</a></span> | <span class="t">it's hooks and plot in the usual way so that's a convenient little thing I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=328" target="_blank">00:05:28.400</a></span> | <span class="t">think it's convenient thing I added okay and then I took our colorful dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=338" target="_blank">00:05:38.480</a></span> | <span class="t">stuff which Stefano and I came up with a few years ago and decided to wrap all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=345" target="_blank">00:05:45.320</a></span> | <span class="t">that up in a callback as well so I've actually sub classed here our hooks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=349" target="_blank">00:05:49.280</a></span> | <span class="t">callback to create an activation stats and what that's going to do is it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=353" target="_blank">00:05:53.920</a></span> | <span class="t">going to use this append stats which appends the means the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=358" target="_blank">00:05:58.800</a></span> | <span class="t">deviations and the histograms and oh and I changed that very slightly also the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=368" target="_blank">00:06:08.200</a></span> | <span class="t">thing which creates these kind of dead plots I changed it to just get the ratio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=372" target="_blank">00:06:12.840</a></span> | <span class="t">of the very first very smallest histogram bin to the rest of the bins so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=381" target="_blank">00:06:21.160</a></span> | <span class="t">these are really kind of more like very dead at this point so these graphs look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=385" target="_blank">00:06:25.600</a></span> | <span class="t">a little bit different okay so yeah so I sub classed the hooks callback and and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=393" target="_blank">00:06:33.040</a></span> | <span class="t">yeah added the colorful dimension method dead chart method and a plot stats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=399" target="_blank">00:06:39.040</a></span> | <span class="t">method so to see them at work if we want to get the activations on and all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=405" target="_blank">00:06:45.240</a></span> | <span class="t">cons then we train our model and then we can just call and so we've added created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=413" target="_blank">00:06:53.880</a></span> | <span class="t">our activation stats we've added that as an extra callback and then and then yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=423" target="_blank">00:07:03.520</a></span> | <span class="t">then we can call colored in to get that plot dead chart to get that plot and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=427" target="_blank">00:07:07.320</a></span> | <span class="t">plot stats to get that shot but so now we have absolutely no excuse for not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=434" target="_blank">00:07:14.000</a></span> | <span class="t">getting all of these really fantastic informative visualizations of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=440" target="_blank">00:07:20.800</a></span> | <span class="t">going on inside our model because it's literally as easy as adding one line of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=445" target="_blank">00:07:25.600</a></span> | <span class="t">code and just putting that in your callbacks so I really think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=450" target="_blank">00:07:30.480</a></span> | <span class="t">couldn't be easier and so I hope you're even for models you thought you know a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=454" target="_blank">00:07:34.480</a></span> | <span class="t">training really well why don't you try using this because you might be surprised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=459" target="_blank">00:07:39.280</a></span> | <span class="t">to discover that they're not okay so those are some changes pretty minor but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=466" target="_blank">00:07:46.640</a></span> | <span class="t">hopefully useful and so today and over the next lesson or two we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=475" target="_blank">00:07:55.600</a></span> | <span class="t">look at trying to get to a important milestone which is to try to get fashion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=483" target="_blank">00:08:03.560</a></span> | <span class="t">MNIST training to an accuracy of 90% or more which is certainly not the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=489" target="_blank">00:08:09.560</a></span> | <span class="t">the road but it's not bad if we look at papers with code there's so 90% would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=499" target="_blank">00:08:19.520</a></span> | <span class="t">a 10% error so there's folks that have got down to 3 or 4 percent error in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=504" target="_blank">00:08:24.600</a></span> | <span class="t">very best which is very impressive but you know 10% error wouldn't be way off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=509" target="_blank">00:08:29.920</a></span> | <span class="t">what's in this paper leaderboard I don't know how far we'll get eventually but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=517" target="_blank">00:08:37.640</a></span> | <span class="t">without using even any architectural changes no resnets or anything we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=523" target="_blank">00:08:43.520</a></span> | <span class="t">going to try to get into the 10% error all right so let's so the first few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=533" target="_blank">00:08:53.200</a></span> | <span class="t">cells are just copied from from earlier and so here's our ridiculously simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=540" target="_blank">00:09:00.460</a></span> | <span class="t">model I like all I did here was I said okay well the very first convolution is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=545" target="_blank">00:09:05.240</a></span> | <span class="t">taking a 9 by 9 by 1 channel input so we should have compressed it at least a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=550" target="_blank">00:09:10.120</a></span> | <span class="t">little bit so I made it 8 channels output for the convolution and then I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=554" target="_blank">00:09:14.920</a></span> | <span class="t">just doubled it to 16 doubled it to 32 doubled it to 64 and it so that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=562" target="_blank">00:09:22.000</a></span> | <span class="t">to get to a that will be as it says 14 by 14 image 7 by 7 a 4 by 4 a 2 by 2 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=569" target="_blank">00:09:29.120</a></span> | <span class="t">then this one gets us to a 1 by 1 so of course we get the 10 digits so there was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=574" target="_blank">00:09:34.400</a></span> | <span class="t">no thought at all behind really this architecture this pure just pure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=580" target="_blank">00:09:40.240</a></span> | <span class="t">convolutional architecture and remember this flatten at the end is necessary to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=586" target="_blank">00:09:46.040</a></span> | <span class="t">get rid of the unit axes that we end up with because this is a 1 by 1 okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=592" target="_blank">00:09:52.440</a></span> | <span class="t">let's do a learning rate finder on this very simple model and what I found was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=597" target="_blank">00:09:57.520</a></span> | <span class="t">that this model is and and you know this situation is so bad that when I tried to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=603" target="_blank">00:10:03.360</a></span> | <span class="t">use the learning rate finder kind of in the usual way which would be just to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=608" target="_blank">00:10:08.280</a></span> | <span class="t">you know start at 1e neg 5 or 1e neg 4 say and then run it it kind of looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=619" target="_blank">00:10:19.240</a></span> | <span class="t">ridiculous it's impossible to see what's going on so if you remember we added</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=624" target="_blank">00:10:24.920</a></span> | <span class="t">that that multiplier it we called it LR mult or gamma is what they called it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=631" target="_blank">00:10:31.200</a></span> | <span class="t">pytorch so we ended up calling it gamma so I dialed that way down to make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=635" target="_blank">00:10:35.360</a></span> | <span class="t">much more gradual which means I have to dial up the starting learning rate and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=639" target="_blank">00:10:39.600</a></span> | <span class="t">only then did I manage even to get the learning rate finder to tell us anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=643" target="_blank">00:10:43.520</a></span> | <span class="t">useful okay so so there we there we are so that's that's that's our learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=651" target="_blank">00:10:51.360</a></span> | <span class="t">rate finder I'm just going to come back to these three later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=660" target="_blank">00:11:00.720</a></span> | <span class="t">so I tried using a learning rate of 0.2 and after trying a few different values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=665" target="_blank">00:11:05.560</a></span> | <span class="t">0.4 0.1 0.2 seems about the highest we can get up to even this actually is too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=670" target="_blank">00:11:10.800</a></span> | <span class="t">high I found much lower and it didn't train much at all you can see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=676" target="_blank">00:11:16.400</a></span> | <span class="t">happens if I do it's it starts training and then it kind of yeah we lose it which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=682" target="_blank">00:11:22.040</a></span> | <span class="t">is unfortunate and you can see that in the colorful dimension plot we get this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=687" target="_blank">00:11:27.780</a></span> | <span class="t">classic you know getting activations crashing get of activations crashing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=694" target="_blank">00:11:34.240</a></span> | <span class="t">you can kind of see the key problem here really is that we don't have zero mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=700" target="_blank">00:11:40.100</a></span> | <span class="t">standard deviation one layers at the start so we certainly don't keep them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=706" target="_blank">00:11:46.520</a></span> | <span class="t">throughout and this is this is a problem now just something I got to mention by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=714" target="_blank">00:11:54.040</a></span> | <span class="t">the way is when you're training stuff in Jupiter notebooks this is just a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=721" target="_blank">00:12:01.360</a></span> | <span class="t">thing we've we've just added if you get you can easily run out of memory GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=729" target="_blank">00:12:09.320</a></span> | <span class="t">memory and there's two reasons it turns out why you can particularly run out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=735" target="_blank">00:12:15.080</a></span> | <span class="t">GPU memory if you run a few cells in a Jupiter notebook the first is that kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=742" target="_blank">00:12:22.280</a></span> | <span class="t">of for your convenience Jupiter notebook you might you might may or may not know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=748" target="_blank">00:12:28.680</a></span> | <span class="t">this actually stores the results of your previous few evaluations if you just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=756" target="_blank">00:12:36.560</a></span> | <span class="t">type underscore it tells you the very last thing you evaluated and you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=762" target="_blank">00:12:42.000</a></span> | <span class="t">more underscores to go backwards further in time or you can also use oh you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=769" target="_blank">00:12:49.040</a></span> | <span class="t">also use numbers to get the out 16 for example would be underscore 16 now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=775" target="_blank">00:12:55.240</a></span> | <span class="t">reason this is an issue is that if one of your outputs is a big CUDA tensor and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=782" target="_blank">00:13:02.480</a></span> | <span class="t">you've shown it in a cell that's going to keep that GPU memory basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=787" target="_blank">00:13:07.360</a></span> | <span class="t">forever and so that's a bit of a problem so if you are running out of memory one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=794" target="_blank">00:13:14.800</a></span> | <span class="t">thing you'd want to do is clean out all of those underscore blah things I found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=800" target="_blank">00:13:20.440</a></span> | <span class="t">that there's actually some function that nearly does that in the IPython source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=805" target="_blank">00:13:25.000</a></span> | <span class="t">code so I copied the important bits out of it and put it in here so if you call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=808" target="_blank">00:13:28.840</a></span> | <span class="t">clean IPython history it will don't worry about the lines of code at all this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=815" target="_blank">00:13:35.080</a></span> | <span class="t">just a thing that you can use to get back that GPU memory the second thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=820" target="_blank">00:13:40.600</a></span> | <span class="t">which Peter figured out in the last week or so is that you also have if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=828" target="_blank">00:13:48.800</a></span> | <span class="t">a CUDA error at any point or even any kind of exception at any point then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=835" target="_blank">00:13:55.120</a></span> | <span class="t">exception object is actually stored by Python and any tensors that were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=843" target="_blank">00:14:03.520</a></span> | <span class="t">allocated anywhere in in that in that trace in that trace back will stay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=848" target="_blank">00:14:08.360</a></span> | <span class="t">allocated basically forever and you again that's a big problem so I created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=855" target="_blank">00:14:15.120</a></span> | <span class="t">this clean trace back function based on Peter's code which gets rid of that so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=862" target="_blank">00:14:22.400</a></span> | <span class="t">if this is particularly problematic because if you have a CUDA out of memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=865" target="_blank">00:14:25.840</a></span> | <span class="t">error and then you try to rerun it you'll still have a CUDA out of memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=869" target="_blank">00:14:29.560</a></span> | <span class="t">error because all the memory that was allocated before is now in that trace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=873" target="_blank">00:14:33.240</a></span> | <span class="t">back so basically anytime you get a CUDA out of memory error or any kind of error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=877" target="_blank">00:14:37.280</a></span> | <span class="t">you know with memory you can call clean mem and that will clean the memory in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=882" target="_blank">00:14:42.240</a></span> | <span class="t">your trace back it will clean the memory used in your in your Jupiter history do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=887" target="_blank">00:14:47.800</a></span> | <span class="t">a garbage collect empty the CUDA cache and that will basically should give you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=894" target="_blank">00:14:54.520</a></span> | <span class="t">a totally clean GPU you don't have to restart your notebook okay so Sam asked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=902" target="_blank">00:15:02.120</a></span> | <span class="t">a very good question in the chat so just to yeah just to remind you guys yes we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=906" target="_blank">00:15:06.120</a></span> | <span class="t">did start he's asking I thought we were training an auto encoder or are you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=909" target="_blank">00:15:09.840</a></span> | <span class="t">training a classifier or what so we started doing this auto code encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=914" target="_blank">00:15:14.400</a></span> | <span class="t">back in notebook 8 and we decided this is we don't have the tools to make this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=920" target="_blank">00:15:20.200</a></span> | <span class="t">work yet so let's go back and create the tools and then come back to it so in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=926" target="_blank">00:15:26.200</a></span> | <span class="t">creating the tools we're doing a classifier we try to make a really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=931" target="_blank">00:15:31.280</a></span> | <span class="t">fashion MNIST classifier well we try to create tools which hopefully have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=935" target="_blank">00:15:35.840</a></span> | <span class="t">side effect will find of giving us a really good classifier and then using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=939" target="_blank">00:15:39.620</a></span> | <span class="t">those tools we hope that will allow us to create a really good auto encoder so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=946" target="_blank">00:15:46.160</a></span> | <span class="t">yes we're kind of like gradually unwinding and we'll come back to where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=952" target="_blank">00:15:52.800</a></span> | <span class="t">we were actually trying to get to so that's why we're doing this this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=957" target="_blank">00:15:57.200</a></span> | <span class="t">classifier the techniques and library pieces we're building will be all very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=963" target="_blank">00:16:03.200</a></span> | <span class="t">necessary okay so why do we need a zero mean one standard deviation why do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=973" target="_blank">00:16:13.520</a></span> | <span class="t">need that and B how do we get it so first of all on the way so if you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=980" target="_blank">00:16:20.200</a></span> | <span class="t">about what a neural net does a deep learning net specifically it takes an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=987" target="_blank">00:16:27.600</a></span> | <span class="t">input and it puts it through a whole bunch of matrix multiplications and of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=991" target="_blank">00:16:31.980</a></span> | <span class="t">course there are activation functions sandwiched in there don't worry about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=997" target="_blank">00:16:37.000</a></span> | <span class="t">activation functions that doesn't change the argument so let's just imagine we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1000" target="_blank">00:16:40.560</a></span> | <span class="t">start with some bunch of some matrix right imagine the 50 to 50 deep neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1010" target="_blank">00:16:50.400</a></span> | <span class="t">net so a 50 deep neural net basically if we ignore the activation functions is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1015" target="_blank">00:16:55.200</a></span> | <span class="t">taking the previous input and doing a matrix multiply by some initially some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1020" target="_blank">00:17:00.840</a></span> | <span class="t">random weights so these are all yeah these are just a bunch of random weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1025" target="_blank">00:17:05.200</a></span> | <span class="t">and these are actually red rand n is mean zero variance one and if we run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1036" target="_blank">00:17:16.520</a></span> | <span class="t">this after 50 times of multiplying by a matrix by a matrix by a matrix by a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1045" target="_blank">00:17:25.320</a></span> | <span class="t">matrix we end up with NANs that's no good so that might be that our matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1056" target="_blank">00:17:36.720</a></span> | <span class="t">the numbers in our matrix were too big so each time we multiply the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1061" target="_blank">00:17:41.080</a></span> | <span class="t">were getting bigger and bigger and bigger so maybe we should make them a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1065" target="_blank">00:17:45.880</a></span> | <span class="t">bit smaller okay so let's try using in the matrix we are multiplying by let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1070" target="_blank">00:17:50.400</a></span> | <span class="t">try multiplying by 0.01 and we multiply that lots of times oh now we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1076" target="_blank">00:17:56.040</a></span> | <span class="t">zeros now of course in mathematically speaking this isn't actually NAN it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1082" target="_blank">00:18:02.360</a></span> | <span class="t">actually some really big number mathematically speaking this isn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1085" target="_blank">00:18:05.040</a></span> | <span class="t">zero it's some really small number but computers can't handle really really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1089" target="_blank">00:18:09.920</a></span> | <span class="t">small numbers are really really big numbers so really really big numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1092" target="_blank">00:18:12.720</a></span> | <span class="t">eventually just get called NAN and really really small numbers eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1095" target="_blank">00:18:15.480</a></span> | <span class="t">just get called zero so basically they get washed out and in fact even if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1103" target="_blank">00:18:23.000</a></span> | <span class="t">don't get a NAN or even if you don't quite get a zero for numbers that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1107" target="_blank">00:18:27.920</a></span> | <span class="t">extremely big the internal representation has no ability to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1114" target="_blank">00:18:34.920</a></span> | <span class="t">discriminate between even slightly similar numbers basically the and in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1120" target="_blank">00:18:40.200</a></span> | <span class="t">way floating point is stored the further you get away from zero the less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1124" target="_blank">00:18:44.560</a></span> | <span class="t">accurate the numbers are so yeah this is a problem so we have to scale our weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1132" target="_blank">00:18:52.920</a></span> | <span class="t">matrices exactly right and we have to scale them in such a way that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1137" target="_blank">00:18:57.560</a></span> | <span class="t">standard deviation at every point stays at one and the mean stays at zero so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1144" target="_blank">00:19:04.760</a></span> | <span class="t">there's actually a paper that describes how to do this for multiplying lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1152" target="_blank">00:19:12.000</a></span> | <span class="t">matrices together and this paper basically just went through it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1160" target="_blank">00:19:20.200</a></span> | <span class="t">actually pretty simple math actually let's see what do they do all right yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1174" target="_blank">00:19:34.760</a></span> | <span class="t">so they look to gradients and the propagation of gradients and they came</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1179" target="_blank">00:19:39.240</a></span> | <span class="t">up with a particular weight initialization of using a uniform with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1186" target="_blank">00:19:46.880</a></span> | <span class="t">with 1 over root n as the bounds of that uniform and they yeah they studied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1194" target="_blank">00:19:54.240</a></span> | <span class="t">basically what happened to with various different activation functions and yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1200" target="_blank">00:20:00.880</a></span> | <span class="t">as a result we we now have this this this way of initializing neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1209" target="_blank">00:20:09.200</a></span> | <span class="t">which is called either gloro initializer initialization or Xavier initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1214" target="_blank">00:20:14.360</a></span> | <span class="t">and yeah this is this is the this is the amount that we scale our initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1227" target="_blank">00:20:27.560</a></span> | <span class="t">our random numbers by where n in is the number of inputs so in our case we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1236" target="_blank">00:20:36.120</a></span> | <span class="t">100 inputs and so root 100 is 10 so 1 over 10 is 0.1 and so if we actually run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1246" target="_blank">00:20:46.800</a></span> | <span class="t">that if we start with our random numbers and then we multiply by random numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1253" target="_blank">00:20:53.160</a></span> | <span class="t">times 0.1 which is this is the gloro initialization you can see we do end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1260" target="_blank">00:21:00.200</a></span> | <span class="t">with numbers that are actually reasonable so that's pretty cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1266" target="_blank">00:21:06.760</a></span> | <span class="t">so just I mean just some background in case you're not familiar with some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1276" target="_blank">00:21:16.440</a></span> | <span class="t">these details what exactly do we mean by variance so if we take a tensor let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1286" target="_blank">00:21:26.520</a></span> | <span class="t">call it T and just put 124 18 in it the mean of that is simply the sum divided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1293" target="_blank">00:21:33.680</a></span> | <span class="t">by the count so that's 6.25 now we want to know basically we want to come up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1299" target="_blank">00:21:39.200</a></span> | <span class="t">with a measure of how far away each data point is from the mean that tells you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1305" target="_blank">00:21:45.120</a></span> | <span class="t">how much variation there is if all the data points are very similar to each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1310" target="_blank">00:21:50.560</a></span> | <span class="t">other right so if you've got kind of like a whole bunch of data points and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1317" target="_blank">00:21:57.840</a></span> | <span class="t">they're all pretty similar to each other right then the mean would be about here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1325" target="_blank">00:22:05.720</a></span> | <span class="t">right and the average distance away of each point from the mean is not very far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1333" target="_blank">00:22:13.200</a></span> | <span class="t">where else if you had dots which were very widely spread all over the place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1338" target="_blank">00:22:18.240</a></span> | <span class="t">right then you might end up with the same mean but the distance from each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1343" target="_blank">00:22:23.920</a></span> | <span class="t">point to the mean is now quite a long way so that's what we want we want some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1350" target="_blank">00:22:30.560</a></span> | <span class="t">measure of kind of how far away the points are an average from the mean so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1359" target="_blank">00:22:39.560</a></span> | <span class="t">here we could do that we can take our tensor we can subtract the mean and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1364" target="_blank">00:22:44.280</a></span> | <span class="t">take the mean of that oh that doesn't work because we've got some numbers that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1370" target="_blank">00:22:50.640</a></span> | <span class="t">are bigger than the mean and some that are smaller than the mean and so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1373" target="_blank">00:22:53.720</a></span> | <span class="t">average of them all out then by definition you actually get zero so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1378" target="_blank">00:22:58.560</a></span> | <span class="t">instead you could either square those differences and that will give you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1385" target="_blank">00:23:05.800</a></span> | <span class="t">something and you could also take the square root of that if you wanted to to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1388" target="_blank">00:23:08.960</a></span> | <span class="t">get it back to the same kind of area or you could take the absolute differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1396" target="_blank">00:23:16.880</a></span> | <span class="t">okay so actually I'm doing this in two steps here so for the first one here it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1403" target="_blank">00:23:23.280</a></span> | <span class="t">is on a different scale and then add square root get it on the same scale so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1407" target="_blank">00:23:27.120</a></span> | <span class="t">six point eight seven and five point eight eight are quite similar right but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1411" target="_blank">00:23:31.120</a></span> | <span class="t">they're mathematically not quite the same but they're both similar ideas so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1415" target="_blank">00:23:35.920</a></span> | <span class="t">this is the mean absolute difference and this is called the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1420" target="_blank">00:23:40.840</a></span> | <span class="t">and this is called the variance so the reason that the standard deviation is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1429" target="_blank">00:23:49.800</a></span> | <span class="t">bigger than the mean absolute difference is because in our original data one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1435" target="_blank">00:23:55.720</a></span> | <span class="t">the numbers is much bigger than the others and so when we square it that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1440" target="_blank">00:24:00.640</a></span> | <span class="t">number ends up having a outsized influence and so that's a bit of an issue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1445" target="_blank">00:24:05.600</a></span> | <span class="t">in general with standard deviation and variance is that outliers like this have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1451" target="_blank">00:24:11.640</a></span> | <span class="t">an outsized influence so you've got to be a bit careful okay so here's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1461" target="_blank">00:24:21.560</a></span> | <span class="t">formula for the standard deviation that's normally written as sigma okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1465" target="_blank">00:24:25.880</a></span> | <span class="t">it's just going to be each of our data points minus the mean squared plus the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1470" target="_blank">00:24:30.760</a></span> | <span class="t">next data point minus the mean squared so forth for all the data points and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1474" target="_blank">00:24:34.440</a></span> | <span class="t">divide that by the number of data points and square root and okay so one thing I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1480" target="_blank">00:24:40.840</a></span> | <span class="t">point out here is that the mean absolute deviation isn't used as much as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1485" target="_blank">00:24:45.480</a></span> | <span class="t">standard deviation because mathematicians find it difficult to use but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1492" target="_blank">00:24:52.040</a></span> | <span class="t">we're not mathematicians we have computers so we can use it okay now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1498" target="_blank">00:24:58.280</a></span> | <span class="t">variance we can calculate like this as we said the main of the square of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1504" target="_blank">00:25:04.040</a></span> | <span class="t">differences and if you feel like doing some math you could discover that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1508" target="_blank">00:25:08.920</a></span> | <span class="t">actually this is exactly the same as you can see and this is actually nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1515" target="_blank">00:25:15.160</a></span> | <span class="t">because this is showing that the mean of the square data points minus the square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1524" target="_blank">00:25:24.680</a></span> | <span class="t">of the mean of the data points is also the variance and this is very helpful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1529" target="_blank">00:25:29.480</a></span> | <span class="t">because it means you actually never have to calculate this you can just calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1534" target="_blank">00:25:34.200</a></span> | <span class="t">the mean so with just the data points on their own you can actually calculate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1538" target="_blank">00:25:38.880</a></span> | <span class="t">variance this is a really nice shortcut this is how we normally calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1543" target="_blank">00:25:43.280</a></span> | <span class="t">variance and so there is the LaTeX version which of course I didn't write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1549" target="_blank">00:25:49.600</a></span> | <span class="t">myself I stole from the Wikipedia LaTeX because I'm lazy now there's a very very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1558" target="_blank">00:25:58.760</a></span> | <span class="t">similar idea which is covariance and has already come up a little bit in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1565" target="_blank">00:26:05.560</a></span> | <span class="t">first lesson or two and particularly the the extra math lesson that my same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1572" target="_blank">00:26:12.080</a></span> | <span class="t">engineer did and it's yes a covariance tells you how much two things vary not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1582" target="_blank">00:26:22.920</a></span> | <span class="t">just on their own but together and there's a definition here in math but I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1587" target="_blank">00:26:27.800</a></span> | <span class="t">like code so we'll see the code so here's our tensor again now we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1593" target="_blank">00:26:33.600</a></span> | <span class="t">to want to have two things so let's create something called u which is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1597" target="_blank">00:26:37.320</a></span> | <span class="t">two times our tensor with a bit of randomness so here it is now you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1604" target="_blank">00:26:44.160</a></span> | <span class="t">that u and t are very closely correlated here but they're not perfectly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1612" target="_blank">00:26:52.960</a></span> | <span class="t">correlated so the covariance tells us yeah how they vary together and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1619" target="_blank">00:26:59.240</a></span> | <span class="t">separately so we can take the you can see this exactly the same thing we had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1625" target="_blank">00:27:05.800</a></span> | <span class="t">before each data point minus its mean but now we've got two different tensors so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1632" target="_blank">00:27:12.480</a></span> | <span class="t">we're also going to do the other one the other the other data points minus their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1635" target="_blank">00:27:15.680</a></span> | <span class="t">mean and we multiply them together so it's actually the same thing as standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1641" target="_blank">00:27:21.280</a></span> | <span class="t">deviation but instead of deviation it's kind of like the covariance with itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1645" target="_blank">00:27:25.360</a></span> | <span class="t">in a sense right and so that's a product we can calculate and then what we then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1655" target="_blank">00:27:35.200</a></span> | <span class="t">do is we take the mean of that and that gives us the covariance between those two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1668" target="_blank">00:27:48.640</a></span> | <span class="t">tensors and you can see that's quite a high number and if we compare it to two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1675" target="_blank">00:27:55.160</a></span> | <span class="t">things that aren't very related at all so that's good a totally random tensor v</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1680" target="_blank">00:28:00.800</a></span> | <span class="t">so this is not related to t and we do exactly the same thing so take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1689" target="_blank">00:28:09.240</a></span> | <span class="t">difference of t to its means and v to its means and take the mean of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1692" target="_blank">00:28:12.800</a></span> | <span class="t">that's a very small number and so you can see covariance is basically telling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1697" target="_blank">00:28:17.920</a></span> | <span class="t">us how related are these two tensors so covariance and variance are basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1706" target="_blank">00:28:26.680</a></span> | <span class="t">the same thing but you kind of can think of we can think of variance as being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1710" target="_blank">00:28:30.080</a></span> | <span class="t">covariance with itself and you can change this mathematical version which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1718" target="_blank">00:28:38.040</a></span> | <span class="t">the one we just created in code to this version just like we have for variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1722" target="_blank">00:28:42.400</a></span> | <span class="t">there's a easier to calculate version which as you can see gets exactly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1729" target="_blank">00:28:49.920</a></span> | <span class="t">same answer okay so if you haven't done stuff with covariance much before you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1740" target="_blank">00:29:00.560</a></span> | <span class="t">should experiment a bit with it by creating a few different plots and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1746" target="_blank">00:29:06.320</a></span> | <span class="t">experimenting with those and the finally the Pearson correlation coefficient which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1756" target="_blank">00:29:16.120</a></span> | <span class="t">is normally called our row is just the covariance divided by the product of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1762" target="_blank">00:29:22.800</a></span> | <span class="t">standard deviations so you've seen probably seen that number many times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1766" target="_blank">00:29:26.960</a></span> | <span class="t">there's just a scaled version of the same thing okay so with that in mind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1781" target="_blank">00:29:41.600</a></span> | <span class="t">here is how Xavier in it or Glurrow in it is derived so when you do a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1791" target="_blank">00:29:51.040</a></span> | <span class="t">multiplication for each of the yi's we're adding together all of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1801" target="_blank">00:30:01.880</a></span> | <span class="t">products so for we've got a i comma 0 times x 0 plus a i comma 1 times x 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1812" target="_blank">00:30:12.680</a></span> | <span class="t">etc and we can write that in sigma notation so we're adding up together all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1818" target="_blank">00:30:18.360</a></span> | <span class="t">of the aik's with all of the xk's this is the stuff that we did in our first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1824" target="_blank">00:30:24.520</a></span> | <span class="t">lesson of part 2 and so here it is in pure Python code and here it is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1831" target="_blank">00:30:31.680</a></span> | <span class="t">NumPy code now at the very beginning our vector has a mean of about 0 and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1837" target="_blank">00:30:37.440</a></span> | <span class="t">standard deviation about 1 because that's what we asked for to remind you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1842" target="_blank">00:30:42.520</a></span> | <span class="t">right that's what we asked for that's a standard deviation of 0 and a minute so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1848" target="_blank">00:30:48.760</a></span> | <span class="t">1 is it a standard deviation of 1 mean of 0 that's what random is okay so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1857" target="_blank">00:30:57.080</a></span> | <span class="t">create some random numbers and we can confirm yeah they have a main of about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1862" target="_blank">00:31:02.080</a></span> | <span class="t">0 and a standard deviation of about 1 so if we chose weights for a that have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1872" target="_blank">00:31:12.880</a></span> | <span class="t">mean of 0 we can compute the standard deviation quite easily so let's do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1883" target="_blank">00:31:23.440</a></span> | <span class="t">so a hundred times let's try creating our X and let's try creating something to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1892" target="_blank">00:31:32.360</a></span> | <span class="t">multiply it by and we'll do the matrix multiplication and we're going to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1898" target="_blank">00:31:38.240</a></span> | <span class="t">the mean and mean of the squares and so that is very close to our matrix so I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1923" target="_blank">00:32:03.880</a></span> | <span class="t">won't go into I mean you can look at it if you like but basically as long as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1928" target="_blank">00:32:08.120</a></span> | <span class="t">elements in a and X are independent which obviously they are because they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1931" target="_blank">00:32:11.720</a></span> | <span class="t">random then we're going to end up with a main of 0 and a standard deviation of 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1939" target="_blank">00:32:19.960</a></span> | <span class="t">for these products and so we can try it if we creates a random number normally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1950" target="_blank">00:32:30.080</a></span> | <span class="t">distributed random number and then a second random number multiply them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1953" target="_blank">00:32:33.580</a></span> | <span class="t">together and then do it a bunch of times and you can see here we've got our zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1959" target="_blank">00:32:39.280</a></span> | <span class="t">one so that's the reason why we need this math dot square root 100 we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1972" target="_blank">00:32:52.360</a></span> | <span class="t">normally worry about the mathematical reasons why things are exactly but yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1977" target="_blank">00:32:57.820</a></span> | <span class="t">I thought I would just dive into this one because sometimes it's it's fun to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1981" target="_blank">00:33:01.480</a></span> | <span class="t">through it and so you can check out the paper if you want to look at that in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1984" target="_blank">00:33:04.560</a></span> | <span class="t">more detail or experiment with these with these little simulations now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=1991" target="_blank">00:33:11.200</a></span> | <span class="t">problem is that that doesn't work it doesn't work for us because we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2003" target="_blank">00:33:23.400</a></span> | <span class="t">rectified linear units which is not something that Xavier Glauro looked at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2010" target="_blank">00:33:30.240</a></span> | <span class="t">let's take a look let's create a couple of matrices this is 200 by 100 this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2016" target="_blank">00:33:36.840</a></span> | <span class="t">just a vector well matrix in a vector this is 200 and then let's create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2024" target="_blank">00:33:44.200</a></span> | <span class="t">couple of weight matrices two weight matrices and two bias vectors okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2031" target="_blank">00:33:51.360</a></span> | <span class="t">we've got some input data X's and Y's and we've got some weight matrices and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2036" target="_blank">00:33:56.360</a></span> | <span class="t">bias vectors so let's create a linear layer function which we've done lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2042" target="_blank">00:34:02.480</a></span> | <span class="t">times before and let's start going through a little neural net you know I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2047" target="_blank">00:34:07.520</a></span> | <span class="t">mentioning this is the forward pass of our neural net so we're going to apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2050" target="_blank">00:34:10.960</a></span> | <span class="t">our linear layer to the X's with our first set of weights and our first set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2055" target="_blank">00:34:15.400</a></span> | <span class="t">of biases and see what the mean and standard deviation is okay it's about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2061" target="_blank">00:34:21.800</a></span> | <span class="t">0 and about 1 so that's good news and the reason why is because we have 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2069" target="_blank">00:34:29.760</a></span> | <span class="t">inputs and we divided it by square root 100 just like Glauro told us to and our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2074" target="_blank">00:34:34.840</a></span> | <span class="t">second one has 50 inputs and we divide by square root of 50 and so this all ought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2080" target="_blank">00:34:40.320</a></span> | <span class="t">to work right and so far it is but now we're going to mess everything up by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2085" target="_blank">00:34:45.440</a></span> | <span class="t">doing ReLU so ReLU after we do a ReLU look we don't have a zero mean or a one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2094" target="_blank">00:34:54.200</a></span> | <span class="t">standard deviation anymore so if we go through that and create it like a deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2099" target="_blank">00:34:59.840</a></span> | <span class="t">neural network with Glauro initialization but with a ReLU oh dear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2108" target="_blank">00:35:08.160</a></span> | <span class="t">it's disappeared it's all gone to zero and you can see why right after a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2114" target="_blank">00:35:14.360</a></span> | <span class="t">multiply and a ReLU our means and variances are going down and of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2120" target="_blank">00:35:20.760</a></span> | <span class="t">they're going down because a ReLU squishes it so I'm not going to worry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2130" target="_blank">00:35:30.160</a></span> | <span class="t">about the math of why but a very important paper indeed called delving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2136" target="_blank">00:35:36.040</a></span> | <span class="t">deep in directifiers surpassing human level performance on image net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2140" target="_blank">00:35:40.040</a></span> | <span class="t">classification by Kaiming He et al came up with a new in it which is just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2150" target="_blank">00:35:50.360</a></span> | <span class="t">Glauro initialization but you multiply the remember the Glauro initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2155" target="_blank">00:35:55.280</a></span> | <span class="t">was 1 over root n this one is root 2 over n and again n is the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2163" target="_blank">00:36:03.040</a></span> | <span class="t">inputs so let's try it so we've got 100 inputs so we have to multiply it by root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2170" target="_blank">00:36:10.840</a></span> | <span class="t">2 over 100 and there we go you can see we are in fact getting some nonzero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2179" target="_blank">00:36:19.520</a></span> | <span class="t">numbers that's very encouraging even after going through 50 layers of depth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2185" target="_blank">00:36:25.400</a></span> | <span class="t">so that's good news so this is called Kaiming it's either called Kaiming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2191" target="_blank">00:36:31.840</a></span> | <span class="t">initialization or called her initialization and notice it looks like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2195" target="_blank">00:36:35.920</a></span> | <span class="t">it's built he but it's a Chinese surname so it's actually pronounced her okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2203" target="_blank">00:36:43.360</a></span> | <span class="t">maybe that's why a lot of people increasingly call it Kaiming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2206" target="_blank">00:36:46.840</a></span> | <span class="t">initialization they don't have to say his surname just a little bit harder to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2210" target="_blank">00:36:50.440</a></span> | <span class="t">pronounce all right so how on earth do we actually use this now that we know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2215" target="_blank">00:36:55.120</a></span> | <span class="t">what initialization function to use for a deep neural network with a ReLU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2220" target="_blank">00:37:00.960</a></span> | <span class="t">activation function the trick is to use a method called apply which all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2228" target="_blank">00:37:08.880</a></span> | <span class="t">nn.modules have so if we grab our model we can apply any function we like for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2235" target="_blank">00:37:15.560</a></span> | <span class="t">example let's apply the function print the name of the type so here you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2242" target="_blank">00:37:22.680</a></span> | <span class="t">it's going through and it's printing out all of the modules that are inside our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2250" target="_blank">00:37:30.160</a></span> | <span class="t">model and notice that our model has modules inside modules it's this it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2257" target="_blank">00:37:37.360</a></span> | <span class="t">conv in a sequential in a sequential but model.apply goes through all of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2263" target="_blank">00:37:43.400</a></span> | <span class="t">regardless of their depth so we can apply an init function so we can apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2273" target="_blank">00:37:53.400</a></span> | <span class="t">the init function which simply does randomly distributed random numbers times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2285" target="_blank">00:38:05.200</a></span> | <span class="t">square root of 2 over the number of inputs that's such an easy thing it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2291" target="_blank">00:38:11.560</a></span> | <span class="t">not even worth writing so that's already been written but that's all it does it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2295" target="_blank">00:38:15.640</a></span> | <span class="t">just does that one thing it's called init.kaimingnormal as we've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2299" target="_blank">00:38:19.920</a></span> | <span class="t">before if there's an underscore at the end of a PyTorch method name that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2303" target="_blank">00:38:23.940</a></span> | <span class="t">that it changes something in place so init.kaimingnormal underscore will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2309" target="_blank">00:38:29.560</a></span> | <span class="t">modify this weight matrix so that it has been initialized with normally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2315" target="_blank">00:38:35.040</a></span> | <span class="t">distributed random numbers based on root of 2 divided by the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2320" target="_blank">00:38:40.760</a></span> | <span class="t">inputs now you can't do that to a sequential layer or a ReLU layer or a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2326" target="_blank">00:38:46.320</a></span> | <span class="t">flattened layer so we should check that the module is a conv or linear layer and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2333" target="_blank">00:38:53.680</a></span> | <span class="t">then we can just say model.apply the function and so if we do that and now I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2342" target="_blank">00:39:02.320</a></span> | <span class="t">can use our learning ratefinder callbacks that we created earlier and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2347" target="_blank">00:39:07.280</a></span> | <span class="t">this time I don't have to worry about actually we can create our own ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2353" target="_blank">00:39:13.400</a></span> | <span class="t">because we don't need to use even the weird gamma thing anymore so let's go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2357" target="_blank">00:39:17.960</a></span> | <span class="t">back and copy that let's get rid of this gamma equals 1.1 it shouldn't be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2369" target="_blank">00:39:29.640</a></span> | <span class="t">necessary anymore and we can probably make that 4 now oh I should have it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2381" target="_blank">00:39:41.360</a></span> | <span class="t">recreate the model there we go okay so that's looking much more sensible so at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2389" target="_blank">00:39:49.120</a></span> | <span class="t">least we've got to a point where the learning ratefinder works that's a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2391" target="_blank">00:39:51.720</a></span> | <span class="t">sign so now when we create our learner we're going to use our momentum learner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2398" target="_blank">00:39:58.080</a></span> | <span class="t">still after we get the model we will apply in it weights and apply also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2403" target="_blank">00:40:03.360</a></span> | <span class="t">returns the model so we can actually this is actually going to return the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2406" target="_blank">00:40:06.960</a></span> | <span class="t">model with the initialization applied while I wait I will answer questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2414" target="_blank">00:40:14.000</a></span> | <span class="t">okay so Fabrizio asks why do we double the number of filters in successive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2419" target="_blank">00:40:19.720</a></span> | <span class="t">convolutions so what's happening is in each stride 2 convolution these are all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2431" target="_blank">00:40:31.680</a></span> | <span class="t">stride 2 convolutions so this is changing the grid size from 28 by 28 to 14 by 14</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2437" target="_blank">00:40:37.480</a></span> | <span class="t">so it's reducing the number the size of the grid by a factor of 4 in total so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2443" target="_blank">00:40:43.560</a></span> | <span class="t">basically so as we go from one to eight from this one to this one same deal we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2449" target="_blank">00:40:49.760</a></span> | <span class="t">going from 14 by 14 to 7 by 7 so produce the grid size by 4 we want it to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2459" target="_blank">00:40:59.700</a></span> | <span class="t">something and if you use if you give it exactly the same kind of number of units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2468" target="_blank">00:41:08.440</a></span> | <span class="t">or activations there's there's not really it's not really forcing it to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2472" target="_blank">00:41:12.840</a></span> | <span class="t">things as much so ideally as we decrease the grid size we want to have enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2480" target="_blank">00:41:20.240</a></span> | <span class="t">channels that you end up with a few less activations but then before it not too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2484" target="_blank">00:41:24.560</a></span> | <span class="t">many less so if we double the number of channels then that means we've decreased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2488" target="_blank">00:41:28.960</a></span> | <span class="t">the grid size by model of 4 increase the channel count by model of 2 so overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2493" target="_blank">00:41:33.760</a></span> | <span class="t">the number of activations has decreased by a factor of 2 and so that's what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2499" target="_blank">00:41:39.400</a></span> | <span class="t">want we want to be kind of forcing it to find ways of compressing the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2504" target="_blank">00:41:44.640</a></span> | <span class="t">intelligently as it goes down also we kind of want to be having a roughly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2512" target="_blank">00:41:52.540</a></span> | <span class="t">similar amount of compute roughly similar amount through the neural net so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2517" target="_blank">00:41:57.840</a></span> | <span class="t">as we decrease the grid size we can add more channels because decreasing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2523" target="_blank">00:42:03.580</a></span> | <span class="t">grid size decreases the amount of compute increasing the channels then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2526" target="_blank">00:42:06.960</a></span> | <span class="t">gives it more things to compute so we're kind of getting this nice compromise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2531" target="_blank">00:42:11.440</a></span> | <span class="t">between yeah between the kind of amount of compute that it's doing but also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2537" target="_blank">00:42:17.200</a></span> | <span class="t">giving it some kind of compression work to do that's the kind of the basic idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2551" target="_blank">00:42:31.880</a></span> | <span class="t">well still not able to train well okay if we leave it for a while okay it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2559" target="_blank">00:42:39.440</a></span> | <span class="t">great but it is actually starting to train that's encouraging and we got up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2564" target="_blank">00:42:44.020</a></span> | <span class="t">to a 70% accuracy so we can see you're not surprisingly we're getting these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2569" target="_blank">00:42:49.640</a></span> | <span class="t">spikes and spikes and so in the statistics you can see that well it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2577" target="_blank">00:42:57.320</a></span> | <span class="t">didn't quite work we don't have a mean of zero we don't have a standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2581" target="_blank">00:43:01.860</a></span> | <span class="t">of one even at the start why is that well it's because we forgot something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2590" target="_blank">00:43:10.740</a></span> | <span class="t">critical if you go back to our original point even when we had our let's go to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2597" target="_blank">00:43:17.400</a></span> | <span class="t">the timing version even when we had the correctly normalized matrix that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2604" target="_blank">00:43:24.240</a></span> | <span class="t">multiplying by well you also have to have a correctly normalized input matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2609" target="_blank">00:43:29.120</a></span> | <span class="t">and we never did anything to normalize our inputs so our inputs actually if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2616" target="_blank">00:43:36.080</a></span> | <span class="t">get the just get the first X mini batch I get its main and standard deviation it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2622" target="_blank">00:43:42.080</a></span> | <span class="t">has a mean of 0.28 and a standard deviation of 0.35 so we actually didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2627" target="_blank">00:43:47.400</a></span> | <span class="t">even start with a 0 1 input and so we started with the mean beneath above zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2638" target="_blank">00:43:58.400</a></span> | <span class="t">and a standard deviation beneath one so it was very hard for it so using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2645" target="_blank">00:44:05.540</a></span> | <span class="t">inner helped at least we're able to train a little bit but it's not quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2649" target="_blank">00:44:09.800</a></span> | <span class="t">what we want we actually need to modify our inputs so they have a mean of one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2656" target="_blank">00:44:16.280</a></span> | <span class="t">a standard sorry a mean of zero and a standard deviation of one so we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2660" target="_blank">00:44:20.960</a></span> | <span class="t">create a callback to do that so a callback let's create a batch transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2666" target="_blank">00:44:26.120</a></span> | <span class="t">callback and so we're going to pass in a function that's going to transform every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2669" target="_blank">00:44:29.520</a></span> | <span class="t">batch and so just in the before batch we will set the batch to be equal to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2678" target="_blank">00:44:38.840</a></span> | <span class="t">function applied to the batch now I can note by the way we don't need self dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2684" target="_blank">00:44:44.880</a></span> | <span class="t">learn dot batch here because we can read any because it's one of the four things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2690" target="_blank">00:44:50.920</a></span> | <span class="t">that we kind of proxy down to the learner automatically but we do need it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2695" target="_blank">00:44:55.800</a></span> | <span class="t">on the left hand side because it's only in the get atra remember so be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2700" target="_blank">00:45:00.880</a></span> | <span class="t">careful so I might just leave it the same on say on both sides just so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2704" target="_blank">00:45:04.840</a></span> | <span class="t">people don't get confused okay so let's create a function underscore norm that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2712" target="_blank">00:45:12.280</a></span> | <span class="t">subtracts the mean and divides by the standard deviation and so remember a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2717" target="_blank">00:45:17.360</a></span> | <span class="t">batch has an X and a Y so it's the X part where we subtract the mean and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2722" target="_blank">00:45:22.800</a></span> | <span class="t">divide by the standard deviation and so the new batch will be that as the X and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2727" target="_blank">00:45:27.960</a></span> | <span class="t">the Y will be exactly the same as it was before so let's create a instance of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2733" target="_blank">00:45:33.600</a></span> | <span class="t">normalization of the batch transform callback which is going to do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2737" target="_blank">00:45:37.560</a></span> | <span class="t">normalization function we'll call it norm so we can pass that as an additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2742" target="_blank">00:45:42.680</a></span> | <span class="t">callback to our learner and now that's looking a lot better so you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2755" target="_blank">00:45:55.040</a></span> | <span class="t">all we had to do was check that our input matrix was 0 1 and main standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2765" target="_blank">00:46:05.720</a></span> | <span class="t">deviation and all of our weight matrices was 0 1 standard deviation and we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2770" target="_blank">00:46:10.040</a></span> | <span class="t">have to use any tricks at all it was able to train and got it to an accuracy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2775" target="_blank">00:46:15.280</a></span> | <span class="t">of 85% and so if we look at the color dim and stats look at this it looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2781" target="_blank">00:46:21.920</a></span> | <span class="t">beautiful now this is layer one this is layer two three four it's still not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2787" target="_blank">00:46:27.640</a></span> | <span class="t">perfect I mean there's some randomness right and and we've got what is it like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2791" target="_blank">00:46:31.440</a></span> | <span class="t">seven or eight layers so that randomness does kind of as you go through the layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2800" target="_blank">00:46:40.400</a></span> | <span class="t">by the last one it still gets a bit ugly and you can kind of see it bouncing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2805" target="_blank">00:46:45.040</a></span> | <span class="t">around here as a result and you can see that also in the means and standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2811" target="_blank">00:46:51.240</a></span> | <span class="t">deviations there's some other reasons this is happening we'll see in a moment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2815" target="_blank">00:46:55.920</a></span> | <span class="t">but this is the first time we've really got our even somewhat deep convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2822" target="_blank">00:47:02.280</a></span> | <span class="t">model to train and so this is a really exciting step you know we have from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2827" target="_blank">00:47:07.560</a></span> | <span class="t">scratch in a sequence of 11 notebooks managed to create a real convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2837" target="_blank">00:47:17.640</a></span> | <span class="t">neural network that is training properly so I think that's pretty amazing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2845" target="_blank">00:47:25.480</a></span> | <span class="t">now we don't have to use a callback for this the other thing we could do to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2851" target="_blank">00:47:31.320</a></span> | <span class="t">modify the input data of course is to use the with transform method from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2857" target="_blank">00:47:37.480</a></span> | <span class="t">hugging face datasets library so we could modify our transform I to do just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2862" target="_blank">00:47:42.720</a></span> | <span class="t">attract the main and divide by the standard deviation and then recreate our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2867" target="_blank">00:47:47.320</a></span> | <span class="t">data loaders and if we now get a batch out of that and check it it's now got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2872" target="_blank">00:47:52.880</a></span> | <span class="t">yep I mean is zero and the standard deviation of one so we could also do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2877" target="_blank">00:47:57.320</a></span> | <span class="t">this way so generally speaking for stuff that needs to kind of dynamically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2883" target="_blank">00:48:03.080</a></span> | <span class="t">modify the batch you can often do it either in your data processing code or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2889" target="_blank">00:48:09.800</a></span> | <span class="t">you can do it in a callback and neither is right or wrong they both work well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2894" target="_blank">00:48:14.360</a></span> | <span class="t">and you can see whichever one works best for you okay now I'm going to show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2900" target="_blank">00:48:20.120</a></span> | <span class="t">something amazing okay so it's great this is training well but when you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2912" target="_blank">00:48:32.420</a></span> | <span class="t">our stats despite what we did with the normalized input and the normalized the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2921" target="_blank">00:48:41.560</a></span> | <span class="t">yeah and the normalized weight matrices we don't have a mean of zero and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2927" target="_blank">00:48:47.800</a></span> | <span class="t">don't have a standard deviation of one even from the start so why is that well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2934" target="_blank">00:48:54.800</a></span> | <span class="t">the problem is that we were putting our data through a ReLU and our activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2947" target="_blank">00:49:07.760</a></span> | <span class="t">stats are looking at the output of those ReLU blocks because that's kind of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2954" target="_blank">00:49:14.600</a></span> | <span class="t">end of each you know that that's that's the activation of each of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2959" target="_blank">00:49:19.160</a></span> | <span class="t">combination of weight matrix multiplication and activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2964" target="_blank">00:49:24.160</a></span> | <span class="t">and since a ReLU removes all of the negative numbers it's impossible for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2971" target="_blank">00:49:31.880</a></span> | <span class="t">output of a ReLU to have a mean of zero unless literally every single number is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2977" target="_blank">00:49:37.480</a></span> | <span class="t">zero Max has got no negatives so ReLU seems to me to be fundamentally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2987" target="_blank">00:49:47.400</a></span> | <span class="t">incompatible with the idea of a correctly calibrated bunch of layers in a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=2993" target="_blank">00:49:53.200</a></span> | <span class="t">net so I came up with this idea of saying well why don't we take our normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3001" target="_blank">00:50:01.080</a></span> | <span class="t">ReLU and have the ability to subtract something from it and so we just take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3007" target="_blank">00:50:07.440</a></span> | <span class="t">the result of our ReLU and subtract so sub of minus I mean I just I can write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3013" target="_blank">00:50:13.240</a></span> | <span class="t">this in more obvious way is exactly the same as just minus equals when I just do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3017" target="_blank">00:50:17.600</a></span> | <span class="t">that we'll subtract something from our ReLU that will allow us to pull the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3026" target="_blank">00:50:26.800</a></span> | <span class="t">whole thing down so that the bottom of our ReLU is underneath the x-axis and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3033" target="_blank">00:50:33.520</a></span> | <span class="t">it has negatives and that would allow us to have a mean of zero and while we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3038" target="_blank">00:50:38.520</a></span> | <span class="t">there let's all do also do something that's existed for a while I didn't come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3042" target="_blank">00:50:42.120</a></span> | <span class="t">up with this idea which is that it just to do a leaky ReLU which is where we say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3046" target="_blank">00:50:46.240</a></span> | <span class="t">let's not have the negative speed totally flat just truncated but instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3051" target="_blank">00:50:51.800</a></span> | <span class="t">let's just have those numbers decreased by some constant amount let me show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3059" target="_blank">00:50:59.280</a></span> | <span class="t">what that looks like so there's two together I'm going to call general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3062" target="_blank">00:51:02.280</a></span> | <span class="t">ReLU which is where we do this thing called leaky ReLU which is where we make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3066" target="_blank">00:51:06.760</a></span> | <span class="t">it so it's not flat under zero but instead just less less sloped and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3072" target="_blank">00:51:12.080</a></span> | <span class="t">also subtract something from it so for example to have created a little function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3077" target="_blank">00:51:17.480</a></span> | <span class="t">here for plotting a function so let's plot the general ReLU function with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3083" target="_blank">00:51:23.200</a></span> | <span class="t">leakiness of point one so that will mean there's a point one slope underneath the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3088" target="_blank">00:51:28.200</a></span> | <span class="t">under zero and we'll subtract point four and so you can see above zero it's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3096" target="_blank">00:51:36.080</a></span> | <span class="t">a normal y equals x line but it's been pushed down by point four and then when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3102" target="_blank">00:51:42.360</a></span> | <span class="t">it's less than zero it's not flat anymore but it's just got a slope of 1/10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3106" target="_blank">00:51:46.440</a></span> | <span class="t">and so this is now something which if you find the right amount to subtract for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3115" target="_blank">00:51:55.720</a></span> | <span class="t">each amount of leakiness you can make a mean of zero and I actually found that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3119" target="_blank">00:51:59.600</a></span> | <span class="t">this particular combination gives us a mean of zero or there abouts so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3126" target="_blank">00:52:06.760</a></span> | <span class="t">now create a new convolution function where we can actually change what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3132" target="_blank">00:52:12.440</a></span> | <span class="t">activation function is used that gives us the ability to change the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3136" target="_blank">00:52:16.160</a></span> | <span class="t">functions in our neural nets let's change get model to allow it to take an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3141" target="_blank">00:52:21.880</a></span> | <span class="t">activation function which is passed into the layers and while we're there let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3147" target="_blank">00:52:27.040</a></span> | <span class="t">also make it easy to change the number of filters so we're going to pass in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3150" target="_blank">00:52:30.360</a></span> | <span class="t">list of the number of filters in each layer and we will default it to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3154" target="_blank">00:52:34.400</a></span> | <span class="t">numbers in each layer that we've discussed and so we're just going to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3157" target="_blank">00:52:37.760</a></span> | <span class="t">through in a list comprehension creating a convolution from the previous number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3164" target="_blank">00:52:44.920</a></span> | <span class="t">of filters this number of filters to the next number of filters and we'll pop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3169" target="_blank">00:52:49.320</a></span> | <span class="t">that all into a sequential along with a flatten at the end and well we're there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3175" target="_blank">00:52:55.440</a></span> | <span class="t">we also then need to be careful about in it weights because this is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3179" target="_blank">00:52:59.960</a></span> | <span class="t">that people tend to forget which is that in it that it is that timing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3189" target="_blank">00:53:09.760</a></span> | <span class="t">initialization the default only specific only applies only applies at all to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3195" target="_blank">00:53:15.960</a></span> | <span class="t">layers that have a value activation function we don't have really you anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3204" target="_blank">00:53:24.760</a></span> | <span class="t">we actually have leaky value the fact that we're subtracting a bit from it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3210" target="_blank">00:53:30.400</a></span> | <span class="t">doesn't change things but the fact that it's leaky does now luckily a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3214" target="_blank">00:53:34.520</a></span> | <span class="t">people don't know this but actually pytorch is claiming normal has an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3218" target="_blank">00:53:38.920</a></span> | <span class="t">adjustment for leaky values weirdly enough they just call it a so if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3224" target="_blank">00:53:44.320</a></span> | <span class="t">pass into the timing normal initialization how much how your leaky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3228" target="_blank">00:53:48.400</a></span> | <span class="t">values leaky factor as a then you'll get the correct initialization for a leaky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3235" target="_blank">00:53:55.880</a></span> | <span class="t">value so we need to change in it weights now to pass in the leakiness all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3241" target="_blank">00:54:01.280</a></span> | <span class="t">so let's put all this together so our general value activation for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3245" target="_blank">00:54:05.000</a></span> | <span class="t">activation function is is general value with a leak of point one and it's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3252" target="_blank">00:54:12.080</a></span> | <span class="t">tractor point four so we'll use partial to create a function that has those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3256" target="_blank">00:54:16.440</a></span> | <span class="t">built-in parameters for activation stats we need to update it now to look for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3264" target="_blank">00:54:24.040</a></span> | <span class="t">general values not nn dot values okay and then our in it weights function we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3272" target="_blank">00:54:32.920</a></span> | <span class="t">going to have a partial with leaky equals point one so we'll call that our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3276" target="_blank">00:54:36.480</a></span> | <span class="t">in it weights huh great so now we'll get our model using that new activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3285" target="_blank">00:54:45.200</a></span> | <span class="t">function and that new in it weights and we'll fit that oh that's encouraging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3300" target="_blank">00:55:00.040</a></span> | <span class="t">accuracy of 845 which is about as high as we got to at the end previously Wow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3309" target="_blank">00:55:09.720</a></span> | <span class="t">look at that so we're up to an accuracy of 87% and let's take a look yeah I mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3318" target="_blank">00:55:18.040</a></span> | <span class="t">look at these we still got a little bit of a spike but it's almost smooth and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3323" target="_blank">00:55:23.280</a></span> | <span class="t">flat and let's have a look here look at that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3326" target="_blank">00:55:26.800</a></span> | <span class="t">our main is standing starting at about zero standard deviation no standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3333" target="_blank">00:55:33.400</a></span> | <span class="t">deviation is still a bit low but it's coming up around one it's not too bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3336" target="_blank">00:55:36.720</a></span> | <span class="t">generally around 0.8 so it's all looking pretty encouraging I think and oh yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3343" target="_blank">00:55:43.320</a></span> | <span class="t">look the percentage of dead units in each layer is very small so finally we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3351" target="_blank">00:55:51.880</a></span> | <span class="t">really trained you know got some very nice looking training graphs here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3357" target="_blank">00:55:57.040</a></span> | <span class="t">yeah it's interesting that we had to literally invent our own activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3363" target="_blank">00:56:03.520</a></span> | <span class="t">function to make this work and I think that gives you a sense of how few people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3368" target="_blank">00:56:08.600</a></span> | <span class="t">actually care about this which is crazy because as you can see it's it in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3373" target="_blank">00:56:13.480</a></span> | <span class="t">ways it's the only thing that matters and it's not at all mathematically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3378" target="_blank">00:56:18.240</a></span> | <span class="t">difficult to make it all work and it's not at all computationally difficult to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3383" target="_blank">00:56:23.960</a></span> | <span class="t">see whether it's working but other frameworks don't even let you plot these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3388" target="_blank">00:56:28.720</a></span> | <span class="t">kinds of things so nobody even knows that they've completely messed up their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3392" target="_blank">00:56:32.840</a></span> | <span class="t">initialization so yeah now you know now some very nice news well so the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3402" target="_blank">00:56:42.520</a></span> | <span class="t">thing to be aware of which is tricky is we a lot of models use more complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3410" target="_blank">00:56:50.600</a></span> | <span class="t">activation functions nowadays rather than value or leaky value or even this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3415" target="_blank">00:56:55.440</a></span> | <span class="t">general version you need to initialize your neural network correctly and most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3422" target="_blank">00:57:02.520</a></span> | <span class="t">people don't and sometimes nobody's even figured out or bothered to try to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3429" target="_blank">00:57:09.400</a></span> | <span class="t">out what the correct initialization to use is but there's actually a very cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3437" target="_blank">00:57:17.400</a></span> | <span class="t">trick which almost nobody knows about which is a paper called all you need is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3442" target="_blank">00:57:22.720</a></span> | <span class="t">a good in it which Demetro Michigan wrote a few years ago and what Demetro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3453" target="_blank">00:57:33.200</a></span> | <span class="t">showed is that there's actually a completely general way of initializing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3458" target="_blank">00:57:38.440</a></span> | <span class="t">any neural network correctly regardless of what activation functions are in it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3466" target="_blank">00:57:46.320</a></span> | <span class="t">and it uses a very very simple idea and the idea is create your model initialize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3473" target="_blank">00:57:53.800</a></span> | <span class="t">it however you like and then go through and put a single batch of data through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3479" target="_blank">00:57:59.840</a></span> | <span class="t">and look at the first layer see what the main and standard deviation through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3486" target="_blank">00:58:06.160</a></span> | <span class="t">first layer is and if the mean you know if the standard deviation is too big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3489" target="_blank">00:58:09.880</a></span> | <span class="t">divide the weight matrix by a bit if the means too high subtract a bit off the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3494" target="_blank">00:58:14.280</a></span> | <span class="t">weight matrix and do that repeatedly for the first layer until you get the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3498" target="_blank">00:58:18.920</a></span> | <span class="t">correct mean and standard deviation and then go to the second layer do the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3503" target="_blank">00:58:23.020</a></span> | <span class="t">thing third layer do the same thing and so forth so we can do that using hooks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3508" target="_blank">00:58:28.920</a></span> | <span class="t">right so we could create a little so this is called layer wise sequential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3514" target="_blank">00:58:34.680</a></span> | <span class="t">unit variance LSU V we can create a little LSU V stats that will grab the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3520" target="_blank">00:58:40.640</a></span> | <span class="t">main of the activations of a layer and the standard deviation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3523" target="_blank">00:58:43.440</a></span> | <span class="t">activate activations of a layer and we will create a hook with that function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3529" target="_blank">00:58:49.320</a></span> | <span class="t">and what it's going to do is after the after we've run that hook to find out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3536" target="_blank">00:58:56.400</a></span> | <span class="t">the main standard deviation of the layer we will go through and run the model get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3547" target="_blank">00:59:07.760</a></span> | <span class="t">the standard deviation and mean see if the standard deviation is not one see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3552" target="_blank">00:59:12.600</a></span> | <span class="t">if the mean is not zero and we will subtract the mean from the bias and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3558" target="_blank">00:59:18.680</a></span> | <span class="t">will divide the weight matrix by the standard deviation and we will keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3565" target="_blank">00:59:25.040</a></span> | <span class="t">doing that until we get a standard deviation of one and a mean of zero and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3572" target="_blank">00:59:32.200</a></span> | <span class="t">so by making that a hook what we will do is we will grab all the values and all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3583" target="_blank">00:59:43.320</a></span> | <span class="t">the comms right and so just to show you what happens there once I've got all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3589" target="_blank">00:59:49.120</a></span> | <span class="t">relu's and all the comms I can use zip so zip in Python takes a bunch of lists and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3595" target="_blank">00:59:55.480</a></span> | <span class="t">creates a list of the items the first items the second items the third items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3601" target="_blank">01:00:01.440</a></span> | <span class="t">and so forth so if I go through the zip of relu's and comms and just print them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3605" target="_blank">01:00:05.520</a></span> | <span class="t">out you can see it prints out the relu and the first conv the second rally the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3610" target="_blank">01:00:10.120</a></span> | <span class="t">second conv the second rally the sorry the third rally the third conv and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3613" target="_blank">01:00:13.520</a></span> | <span class="t">forth we use zip all the time in Python so it's really important thing to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3618" target="_blank">01:00:18.160</a></span> | <span class="t">aware of so we could go through the relu's and the comms and call layerwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3626" target="_blank">01:00:26.380</a></span> | <span class="t">sequential unit variance in it passing in those module pairs sorry passing in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3637" target="_blank">01:00:37.600</a></span> | <span class="t">yes passing in the relu and the conv and then for each one oh and we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3647" target="_blank">01:00:47.360</a></span> | <span class="t">do that on the the batch and of course we need to put the batch on the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3652" target="_blank">01:00:52.640</a></span> | <span class="t">device for our model and so now that I've done that we now have it ran almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3666" target="_blank">01:01:06.040</a></span> | <span class="t">instantly it's now made all the biases and weights correct give us 0 1 and now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3672" target="_blank">01:01:12.840</a></span> | <span class="t">if I train it there it is so we didn't do any initialization at all of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3679" target="_blank">01:01:19.560</a></span> | <span class="t">other than just call LS UV in it and this time we've got an accuracy of 0.86</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3689" target="_blank">01:01:29.720</a></span> | <span class="t">versus previously it's 0.87 so pretty much the same thing close enough and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3695" target="_blank">01:01:35.800</a></span> | <span class="t">actually if you want to actually see that happening I guess what we could do I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3707" target="_blank">01:01:47.400</a></span> | <span class="t">mean it's not it's going to be pretty obvious after we've run this we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3710" target="_blank">01:01:50.520</a></span> | <span class="t">say print H dot mean comma H dot standard deviation actually we could do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3720" target="_blank">01:02:00.760</a></span> | <span class="t">it like before and afterwards right so we could say right before and after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3733" target="_blank">01:02:13.320</a></span> | <span class="t">there we go yeah so it starts at so the first layer started at a mean of point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3746" target="_blank">01:02:26.000</a></span> | <span class="t">negative point one three in a variance of point four six and it kept doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3752" target="_blank">01:02:32.440</a></span> | <span class="t">divide subtract divide subtract divide subtract until eventually it got to mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3756" target="_blank">01:02:36.080</a></span> | <span class="t">is zero standard deviation of one and then it went to the next layer and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3760" target="_blank">01:02:40.360</a></span> | <span class="t">kept going going going until that was zero one and then the third layer and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3764" target="_blank">01:02:44.560</a></span> | <span class="t">then the fourth layer and so at that point all of the layers had a mean is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3769" target="_blank">01:02:49.400</a></span> | <span class="t">zero and a standard deviation of one so I guess like one thing with LS UV you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3779" target="_blank">01:02:59.880</a></span> | <span class="t">know it's kind of very mathematically convenient we don't have to spend any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3783" target="_blank">01:03:03.240</a></span> | <span class="t">time thinking about you know if we've invented a new activation function or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3786" target="_blank">01:03:06.960</a></span> | <span class="t">we're using some activation function where nobody seems to have figured out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3790" target="_blank">01:03:10.040</a></span> | <span class="t">the correct initialization for it we can just use LS UV it did require a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3796" target="_blank">01:03:16.000</a></span> | <span class="t">bit more fiddling around with hooks and stuff to get it to work and I haven't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3799" target="_blank">01:03:19.000</a></span> | <span class="t">even put this into like a callback or anything so if you yeah if you decide you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3804" target="_blank">01:03:24.840</a></span> | <span class="t">want to try using this in some of your models it might be a good idea and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3809" target="_blank">01:03:29.280</a></span> | <span class="t">actually be good homework to see if you can come up with a callback that does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3814" target="_blank">01:03:34.280</a></span> | <span class="t">LS UV initialization for you that would be pretty cool wouldn't it in in before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3820" target="_blank">01:03:40.480</a></span> | <span class="t">fit I guess it would be you'd have to be a bit careful because if you ran fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3828" target="_blank">01:03:48.320</a></span> | <span class="t">multiple times it would actually initialize it each time so that would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3832" target="_blank">01:03:52.800</a></span> | <span class="t">one issue with that to think about okay so something which is quite similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3838" target="_blank">01:03:58.320</a></span> | <span class="t">LS UV is batch normalization so we're going to have a seven minute break and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3847" target="_blank">01:04:07.240</a></span> | <span class="t">then we're going to come back and we're going to talk about batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3851" target="_blank">01:04:11.120</a></span> | <span class="t">see you in seven minutes okay hi let's do this batch normalization batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3862" target="_blank">01:04:22.000</a></span> | <span class="t">normalization was such an important paper I remember when it came out I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3869" target="_blank">01:04:29.680</a></span> | <span class="t">at analytic my medical startup and I think that's right and everybody was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3880" target="_blank">01:04:40.800</a></span> | <span class="t">talking about it and in particular they were talking about this this graph that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3895" target="_blank">01:04:55.400</a></span> | <span class="t">basically showed like what it used to be like until batch norm to train a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3901" target="_blank">01:05:01.160</a></span> | <span class="t">on image net how many training steps you'd have to do to get to a certain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3908" target="_blank">01:05:08.840</a></span> | <span class="t">accuracy and then they showed what you could do with batch norm so much faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3921" target="_blank">01:05:21.280</a></span> | <span class="t">it was amazing and we all thought that can't be true but it was true so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3930" target="_blank">01:05:30.000</a></span> | <span class="t">basically the key idea of batch norm is that you know with with with LS UV and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3938" target="_blank">01:05:38.400</a></span> | <span class="t">input normalization and climbing in it we are normalizing the layers each day as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3946" target="_blank">01:05:46.720</a></span> | <span class="t">inputs before training but the distribution of each layers inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3951" target="_blank">01:05:51.280</a></span> | <span class="t">changes during training and that's a problem so you end up having to decrease</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3962" target="_blank">01:06:02.680</a></span> | <span class="t">your learning rates and as we've seen you'd have to be very careful about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3966" target="_blank">01:06:06.320</a></span> | <span class="t">parameter initialization so the fact that the layers inputs change during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3972" target="_blank">01:06:12.880</a></span> | <span class="t">training they call internal covariate shift which for some reason a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3977" target="_blank">01:06:17.120</a></span> | <span class="t">people tend to find a confusing statement or a confusing name but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3980" target="_blank">01:06:20.160</a></span> | <span class="t">that's very clear to me and you can fix it by normalizing layer inputs during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3987" target="_blank">01:06:27.520</a></span> | <span class="t">training so you're making the normalization a part of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3991" target="_blank">01:06:31.760</a></span> | <span class="t">architecture and you perform the normalization for each mini batch now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3996" target="_blank">01:06:36.600</a></span> | <span class="t">I'm actually not going to start with batch normalization I'm going to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=3999" target="_blank">01:06:39.800</a></span> | <span class="t">with something that came out one year later called layer normalization because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4004" target="_blank">01:06:44.360</a></span> | <span class="t">layer normalization is simpler let's do the simpler one first so layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4010" target="_blank">01:06:50.320</a></span> | <span class="t">normalization came out as this group of fellows the last of whom I'm sure you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4017" target="_blank">01:06:57.440</a></span> | <span class="t">heard of and it's probably easiest to explain by showing you the code so if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4026" target="_blank">01:07:06.880</a></span> | <span class="t">you're thinking layer normalization well it's a whole paper Jeffrey Hinton paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4031" target="_blank">01:07:11.480</a></span> | <span class="t">must be complicated no the whole thing is this code what is layer normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4036" target="_blank">01:07:16.640</a></span> | <span class="t">well we can create a module and we're going to pass in we don't need to pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4045" target="_blank">01:07:25.840</a></span> | <span class="t">in anything actually you can totally ignore the parameters for now in fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4049" target="_blank">01:07:29.000</a></span> | <span class="t">what we're going to do is we're going to have a single number called mult for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4053" target="_blank">01:07:33.480</a></span> | <span class="t">multiplier and a single number called add that's the thing we're going to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4057" target="_blank">01:07:37.240</a></span> | <span class="t">and we're going to start off by multiplying things by one and adding zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4061" target="_blank">01:07:41.960</a></span> | <span class="t">so we're going to start off by doing nothing at all okay this is the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4066" target="_blank">01:07:46.640</a></span> | <span class="t">it has a forward function and in the forward function so remember that by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4074" target="_blank">01:07:54.560</a></span> | <span class="t">default we have NCHW we have batch by channel by height by width we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4086" target="_blank">01:08:06.880</a></span> | <span class="t">take the mean over the channel height and width so we're just going to find the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4093" target="_blank">01:08:13.760</a></span> | <span class="t">mean activation for each input in the mini batch and when I say input though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4100" target="_blank">01:08:20.880</a></span> | <span class="t">remember that this is going to be this is a layer right so we can put this layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4105" target="_blank">01:08:25.120</a></span> | <span class="t">anywhere we like so it's the input to that layer and we'll do the same thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4109" target="_blank">01:08:29.400</a></span> | <span class="t">for finding the variance okay and then we're going to normalize our data by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4121" target="_blank">01:08:41.340</a></span> | <span class="t">subtracting the mean and dividing by the square root of the variance which of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4129" target="_blank">01:08:49.280</a></span> | <span class="t">course is the standard deviation we're going to add a very small number by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4136" target="_blank">01:08:56.120</a></span> | <span class="t">default one in egg five to the denominator just in case the variance is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4141" target="_blank">01:09:01.320</a></span> | <span class="t">zero or ridiculously small this will keep the number from going giant just if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4146" target="_blank">01:09:06.640</a></span> | <span class="t">we happen to get something with a very small variance this idea of an epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4151" target="_blank">01:09:11.960</a></span> | <span class="t">as being something we add to a divisor is really really common and in general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4157" target="_blank">01:09:17.780</a></span> | <span class="t">you should not assume that the defaults are correct very often the defaults are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4161" target="_blank">01:09:21.480</a></span> | <span class="t">too small for algorithms that use an epsilon okay so here we are as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4171" target="_blank">01:09:31.520</a></span> | <span class="t">see we are normalizing the the batch I mean I can call it a batch but just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4184" target="_blank">01:09:44.840</a></span> | <span class="t">remember it isn't necessarily the first layer right so it's wherever which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4188" target="_blank">01:09:48.800</a></span> | <span class="t">whichever layer we decide to put this in so we normalize it now the thing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4193" target="_blank">01:09:53.240</a></span> | <span class="t">maybe we don't want it to be normalized maybe we wanted to have something other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4200" target="_blank">01:10:00.840</a></span> | <span class="t">than unit variance and something other than zero mean well what we do is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4207" target="_blank">01:10:07.000</a></span> | <span class="t">then multiply it back by self dot malt and add self dot add now remember self</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4212" target="_blank">01:10:12.200</a></span> | <span class="t">dot malt was one and self dot add is zero so at first that does nothing at all so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4217" target="_blank">01:10:17.800</a></span> | <span class="t">at first this is just normalizing the data so that's good but because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4223" target="_blank">01:10:23.720</a></span> | <span class="t">are parameters these two numbers are learnable that means that the SGD</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4228" target="_blank">01:10:28.120</a></span> | <span class="t">algorithm can change them so there's a very subtle thing going on here which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4232" target="_blank">01:10:32.840</a></span> | <span class="t">that in fact this might not be normalizing the data at all or normalizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4237" target="_blank">01:10:37.680</a></span> | <span class="t">the the inputs to the next layer at all because self dot malt and self dot add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4242" target="_blank">01:10:42.360</a></span> | <span class="t">could be anything so I tend to think that when people think about these kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4248" target="_blank">01:10:48.280</a></span> | <span class="t">things like layer normalization and batch normalization thinking of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4251" target="_blank">01:10:51.200</a></span> | <span class="t">normalization in some ways is not the right way to think of it it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4259" target="_blank">01:10:59.120</a></span> | <span class="t">doing something I think to really well it's definitely normalizing it for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4262" target="_blank">01:11:02.680</a></span> | <span class="t">initial layers and we don't really need a less UV anymore if we have this in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4267" target="_blank">01:11:07.160</a></span> | <span class="t">here because it's going to normalize it automatically so that's handy but after a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4274" target="_blank">01:11:14.120</a></span> | <span class="t">few batches it's not really normalizing at all but what it is doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4279" target="_blank">01:11:19.760</a></span> | <span class="t">previously this idea of like how big are the numbers overall and how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4286" target="_blank">01:11:26.560</a></span> | <span class="t">variation do they have overall was kind of built into every single number in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4293" target="_blank">01:11:33.080</a></span> | <span class="t">weight matrix and in the bias vector this way those two things have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4301" target="_blank">01:11:41.360</a></span> | <span class="t">turned into just two numbers and I think this makes training a lot a lot easier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4306" target="_blank">01:11:46.840</a></span> | <span class="t">for it basically to just have just two numbers that it can focus on to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4311" target="_blank">01:11:51.400</a></span> | <span class="t">this overall like positioning and variation so there's something very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4316" target="_blank">01:11:56.240</a></span> | <span class="t">subtle going on here because it's not just doing normalization at least not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4322" target="_blank">01:12:02.000</a></span> | <span class="t">after the first few batches are complete because it can learn to create any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4327" target="_blank">01:12:07.320</a></span> | <span class="t">distribution of outputs it want so there's our layer so we're going to need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4333" target="_blank">01:12:13.000</a></span> | <span class="t">to change our con function let again previously we changed it to add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4337" target="_blank">01:12:17.560</a></span> | <span class="t">activation function to be what it to be modifiable now we're going to also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4342" target="_blank">01:12:22.240</a></span> | <span class="t">change it to allow us to add normalization layers to the end so our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4347" target="_blank">01:12:27.240</a></span> | <span class="t">basic layers well we'll start off by adding our conv2d as usual and then if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4352" target="_blank">01:12:32.880</a></span> | <span class="t">you're doing normalization we will append the normalization layer with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4358" target="_blank">01:12:38.840</a></span> | <span class="t">many inputs now in fact layer norm doesn't care how many inputs so I just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4363" target="_blank">01:12:43.560</a></span> | <span class="t">ignore it but you'll see batch normal care if you've got an activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4367" target="_blank">01:12:47.760</a></span> | <span class="t">function add it and so our convolutional layer is actually a sequential bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4371" target="_blank">01:12:51.720</a></span> | <span class="t">players now one thing that's interesting I think is that for bias in the conv if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4382" target="_blank">01:13:02.160</a></span> | <span class="t">you're using well this isn't quite true is it I was going to say if you're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4388" target="_blank">01:13:08.880</a></span> | <span class="t">layer norm you don't need bias but actually you kind of do so maybe we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4395" target="_blank">01:13:15.120</a></span> | <span class="t">should actually change that for batch norm we won't need bias but actually for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4401" target="_blank">01:13:21.240</a></span> | <span class="t">this one we do so put this back bias equals true bias equals bias okay so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4415" target="_blank">01:13:35.400</a></span> | <span class="t">then these initial layers right here yes so they all have bias and then we've got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4422" target="_blank">01:13:42.120</a></span> | <span class="t">bias equals false okay so now in our model we're going to add layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4437" target="_blank">01:13:57.080</a></span> | <span class="t">normalization to every layer except for the last one and let's see how we go oh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4449" target="_blank">01:14:09.640</a></span> | <span class="t">nice eight seven three okay eight sixty and eight seven two so just we've just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4459" target="_blank">01:14:19.660</a></span> | <span class="t">got our best by a little bit so that's cool so the the thing about these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4472" target="_blank">01:14:32.640</a></span> | <span class="t">normalization layers is though that they do cause a lot of challenges in models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4481" target="_blank">01:14:41.000</a></span> | <span class="t">and generally speaking ever since patch norm appeared well there's been this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4485" target="_blank">01:14:45.400</a></span> | <span class="t">kind of like big change of a view towards it at first people like oh my god batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4491" target="_blank">01:14:51.280</a></span> | <span class="t">norm is our savior and it kind of was it let us train much deeper models and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4496" target="_blank">01:14:56.960</a></span> | <span class="t">great results and train quickly but then increasingly people realized it also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4501" target="_blank">01:15:01.720</a></span> | <span class="t">added a lot of complexity these these these learnable parameters turned out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4507" target="_blank">01:15:07.040</a></span> | <span class="t">create all kind of complexity and in particular batch norm which we'll see in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4509" target="_blank">01:15:09.960</a></span> | <span class="t">a minute created all kinds of complexity so there has been a tendency in recent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4515" target="_blank">01:15:15.280</a></span> | <span class="t">years to be trying to get rid of or at least reduce the use of these kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4519" target="_blank">01:15:19.960</a></span> | <span class="t">layers so knowing how to actually initialize your models correctly at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4528" target="_blank">01:15:28.360</a></span> | <span class="t">first is becoming increasingly important as people are trying to move away from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4533" target="_blank">01:15:33.200</a></span> | <span class="t">these normalization layers increasingly so I will I will say that so I you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4540" target="_blank">01:15:40.400</a></span> | <span class="t">they're still very helpful but they're not a silver bullet as it turns out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4547" target="_blank">01:15:47.080</a></span> | <span class="t">alright so now let's look at batch norm so batch norm is still not huge but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4553" target="_blank">01:15:53.480</a></span> | <span class="t">a little bit bigger than layer norm and you'll see that we've now we've got the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4559" target="_blank">01:15:59.840</a></span> | <span class="t">mulch and add as before but it's not just one number to add or one number to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4568" target="_blank">01:16:08.280</a></span> | <span class="t">multiply but actually we've got a whole bunch of them and the reason is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4572" target="_blank">01:16:12.360</a></span> | <span class="t">we're going to have one for every channel and so now when we take the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4577" target="_blank">01:16:17.000</a></span> | <span class="t">and the very variance we're actually taking it over the batch dimension and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4583" target="_blank">01:16:23.480</a></span> | <span class="t">the height and which dimensions so we're ending up with one mean per channel and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4589" target="_blank">01:16:29.960</a></span> | <span class="t">one variance per channel so just like before once we get our means and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4598" target="_blank">01:16:38.760</a></span> | <span class="t">variances we subtract them out and divide them by the epsilon modified variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4608" target="_blank">01:16:48.480</a></span> | <span class="t">and just like before we then multiply by mult and add add but now we're actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4613" target="_blank">01:16:53.760</a></span> | <span class="t">multiplying by a vector of malts and we're adding a vector of ads and that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4617" target="_blank">01:16:57.840</a></span> | <span class="t">why we have to pass in the number of filters because we have to know how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4623" target="_blank">01:17:03.640</a></span> | <span class="t">ones and how many zeros we have in our initial malts and ads so that's the main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4631" target="_blank">01:17:11.120</a></span> | <span class="t">difference in a sense is that we are we have one per channel and that and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4637" target="_blank">01:17:17.640</a></span> | <span class="t">we're also taking the average across all of the things in the batch where else in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4645" target="_blank">01:17:25.280</a></span> | <span class="t">layer norm we didn't each thing in the batch had its own separate normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4653" target="_blank">01:17:33.640</a></span> | <span class="t">it was doing then there's something else in batch norm which is a bit tricky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4662" target="_blank">01:17:42.280</a></span> | <span class="t">which is that during training we are not just subtracting the mean and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4671" target="_blank">01:17:51.320</a></span> | <span class="t">variance but instead we're getting an exponentially weighted moving average of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4676" target="_blank">01:17:56.920</a></span> | <span class="t">the means and the variances of the last few of the last few batches that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4686" target="_blank">01:18:06.140</a></span> | <span class="t">this is doing so we start out so we basically create something called vase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4691" target="_blank">01:18:11.000</a></span> | <span class="t">and something called means and initially the variances are all one and the means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4697" target="_blank">01:18:17.000</a></span> | <span class="t">are all zero and there's one per channel just like before or one per filter this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4701" target="_blank">01:18:21.680</a></span> | <span class="t">is number of filters same idea I guess filters we tend to actually use inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4707" target="_blank">01:18:27.440</a></span> | <span class="t">the model and channels we tend to use as the first input so I should probably say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4711" target="_blank">01:18:31.040</a></span> | <span class="t">filters either works though so we get out let's for example we get our mean per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4719" target="_blank">01:18:39.080</a></span> | <span class="t">filter and then what we do is we use this thing called lerp and lerp is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4723" target="_blank">01:18:43.960</a></span> | <span class="t">simply saying yes that's what it's done so what lerp does is it takes two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4740" target="_blank">01:19:00.120</a></span> | <span class="t">numbers in this case I'm going to take 5 and 15 or two tensors they could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4746" target="_blank">01:19:06.860</a></span> | <span class="t">vectors or matrices and it creates a weighted average of them and the amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4751" target="_blank">01:19:11.680</a></span> | <span class="t">of weight it uses is this number here let me explain in this case if I put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4757" target="_blank">01:19:17.360</a></span> | <span class="t">0.5 it's going to take half of this number plus half of this number so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4762" target="_blank">01:19:22.960</a></span> | <span class="t">end up with just the mean but what if we used 0.75 then that's going to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4772" target="_blank">01:19:32.080</a></span> | <span class="t">70 that's going to take 0.75 times this number plus 0.25 of this number so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4781" target="_blank">01:19:41.200</a></span> | <span class="t">basically kind of allows it to be under like a sliding scale so one extreme would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4786" target="_blank">01:19:46.360</a></span> | <span class="t">be to take all of the second number so that would be lerp with one there and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4790" target="_blank">01:19:50.080</a></span> | <span class="t">the other extreme would be all of the first number and then you can slide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4794" target="_blank">01:19:54.800</a></span> | <span class="t">anywhere between them like so right so that's exactly the same as saying five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4802" target="_blank">01:20:02.400</a></span> | <span class="t">times 0.9 plus 15 times 0.1 right so this this number here is how much of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4817" target="_blank">01:20:17.240</a></span> | <span class="t">second number do we have and one minus that is how much of this number do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4821" target="_blank">01:20:21.400</a></span> | <span class="t">have and you can also move this as you can with most PyTorch things you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4826" target="_blank">01:20:26.080</a></span> | <span class="t">move the first parameter into there and get exactly the same result so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4832" target="_blank">01:20:32.880</a></span> | <span class="t">what lerp is so what we're doing here is we're doing an in-place lerp so we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4840" target="_blank">01:20:40.760</a></span> | <span class="t">replacing self dot means with one minus momentum of self dot means and plus self</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4851" target="_blank">01:20:51.720</a></span> | <span class="t">dot momentum times this particular mini batches mean so this is basically doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4858" target="_blank">01:20:58.120</a></span> | <span class="t">momentum again which is why we indeed are calling the parameter mom from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4862" target="_blank">01:21:02.840</a></span> | <span class="t">momentum so with a mom of point one which I kind of think is the opposite of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4870" target="_blank">01:21:10.760</a></span> | <span class="t">what I'd expect momentum to mean I'd expect to be point nine but with a man</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4873" target="_blank">01:21:13.960</a></span> | <span class="t">mom of point one it's saying that each mini batch self dot means will be 0.1 of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4882" target="_blank">01:21:22.560</a></span> | <span class="t">this particular mini batches mean and 0.9 of the previous one the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4891" target="_blank">01:21:31.440</a></span> | <span class="t">sequence in fact and that ends up giving us what's called an exponentially weighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4895" target="_blank">01:21:35.920</a></span> | <span class="t">moving average and we do the same thing for variances okay so that's only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4905" target="_blank">01:21:45.680</a></span> | <span class="t">updated during training okay and then during inference we can we just use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4913" target="_blank">01:21:53.840</a></span> | <span class="t">saved means and variances so this and then why do we have buffers what does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4919" target="_blank">01:21:59.720</a></span> | <span class="t">that mean these buffers mean that these means and variances will be actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4924" target="_blank">01:22:04.840</a></span> | <span class="t">saved as part of the model so it's important to understand that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4931" target="_blank">01:22:11.240</a></span> | <span class="t">information about the means and variances that your model saw saved in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4937" target="_blank">01:22:17.800</a></span> | <span class="t">the model and this is the key thing which makes batch norm very tricky to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4943" target="_blank">01:22:23.240</a></span> | <span class="t">deal with and particularly tricky as we'll see in later lessons with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4946" target="_blank">01:22:26.760</a></span> | <span class="t">transfer learning but what this does do is that it means that we're going to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4952" target="_blank">01:22:32.600</a></span> | <span class="t">something that's much smoother you know a single weird mini batch shouldn't screw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4957" target="_blank">01:22:37.400</a></span> | <span class="t">things around too much and because we're averaging across their mini batch it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4962" target="_blank">01:22:42.080</a></span> | <span class="t">also going to make things smoother so this whole thing should lead to a pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4964" target="_blank">01:22:44.960</a></span> | <span class="t">nice smooth training so we can train this so we're going to this time we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4971" target="_blank">01:22:51.680</a></span> | <span class="t">going to use our batch norm layer for norm oh actually we need to put the bias</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4975" target="_blank">01:22:55.920</a></span> | <span class="t">thing right oh no it's no that's fine okay and one interesting thing I found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4990" target="_blank">01:23:10.160</a></span> | <span class="t">here is I was able to now finally increase the learning rate up to 0.4 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4996" target="_blank">01:23:16.220</a></span> | <span class="t">the first time so each time I was really trying to see if I can push the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=4999" target="_blank">01:23:19.320</a></span> | <span class="t">rate and I'm now able to double the learning rate and still as you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5004" target="_blank">01:23:24.240</a></span> | <span class="t">it's training very smoothly which is really cool so there's actually a number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5010" target="_blank">01:23:30.200</a></span> | <span class="t">of different types of normal layer based normalization we can use in this lesson</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5014" target="_blank">01:23:34.960</a></span> | <span class="t">we've specifically seen batch norm and layer norm I wanted to mention that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5019" target="_blank">01:23:39.400</a></span> | <span class="t">there's also instance norm and group norm and this picture from the group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5022" target="_blank">01:23:42.640</a></span> | <span class="t">norm paper explains what happens the what it's showing is that we've got here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5028" target="_blank">01:23:48.080</a></span> | <span class="t">the N C H W and so they've kind of concatenated flattened H W into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5033" target="_blank">01:23:53.640</a></span> | <span class="t">axis since they can't draw 40 cubes and what they're saying is in batch norm all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5041" target="_blank">01:24:01.200</a></span> | <span class="t">this blue stuff is what we average over so we average across the batch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5045" target="_blank">01:24:05.680</a></span> | <span class="t">across the height and width and we end up with one therefore normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5050" target="_blank">01:24:10.960</a></span> | <span class="t">number per channel right so you can kind of slide these blue blocks across so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5056" target="_blank">01:24:16.400</a></span> | <span class="t">batch norm is averaging over the batch and height width layer norm as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5061" target="_blank">01:24:21.760</a></span> | <span class="t">learned averages over the channel and the height and the width and it has a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5065" target="_blank">01:24:25.520</a></span> | <span class="t">separate one per item in the mini batch I mean kind of it's a bit it's a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5075" target="_blank">01:24:35.320</a></span> | <span class="t">subtle right because remember the overall molten add it just had a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5080" target="_blank">01:24:40.840</a></span> | <span class="t">literally a single number for each right so it's not quite as simple as this but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5084" target="_blank">01:24:44.960</a></span> | <span class="t">that's a general idea instance norm which we're not looking at today only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5090" target="_blank">01:24:50.920</a></span> | <span class="t">averages across height and width so there's going to be a separate one for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5095" target="_blank">01:24:55.720</a></span> | <span class="t">every channel and every element of the mini batch and then finally group norm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5101" target="_blank">01:25:01.120</a></span> | <span class="t">which I'm quite fond of is like instance norm but it arbitrarily basically groups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5106" target="_blank">01:25:06.960</a></span> | <span class="t">a bunch of channels together and you can decide how many groups of channels there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5112" target="_blank">01:25:12.240</a></span> | <span class="t">are and averages over them group norm tends to be a bit slow unfortunately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5116" target="_blank">01:25:16.520</a></span> | <span class="t">because the way these things are implemented is a bit tricky but group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5120" target="_blank">01:25:20.360</a></span> | <span class="t">norm does allow you to yeah avoid some of the the challenges of some of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5127" target="_blank">01:25:27.240</a></span> | <span class="t">other methods so it's worth trying if you can and of course batch norm has the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5135" target="_blank">01:25:35.280</a></span> | <span class="t">additional thing of the kind of momentum based statistics but in general the idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5140" target="_blank">01:25:40.320</a></span> | <span class="t">of like do you use momentum based statistics do you store things you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5147" target="_blank">01:25:47.160</a></span> | <span class="t">per channel or a single mean and variance in your buffers or whatever you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5153" target="_blank">01:25:53.480</a></span> | <span class="t">know all that kind of stuff along with what do you average over they're all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5156" target="_blank">01:25:56.480</a></span> | <span class="t">somewhat independent choices you can make and particular combinations of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5160" target="_blank">01:26:00.240</a></span> | <span class="t">have been given particular names and so there we go okay so we're getting you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5169" target="_blank">01:26:09.400</a></span> | <span class="t">know we've got some good initialization methods here let's try putting them all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5172" target="_blank">01:26:12.560</a></span> | <span class="t">together and one other thing we can do is we've been using a batch size of 1024</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5181" target="_blank">01:26:21.000</a></span> | <span class="t">for speed purposes if we drop it down a bit to 256 it's going to mean that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5186" target="_blank">01:26:26.960</a></span> | <span class="t">going to get to see more mini batches so that should improve performance and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5192" target="_blank">01:26:32.400</a></span> | <span class="t">we're trying to get to 90% remember so let's yeah do all this this time we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5200" target="_blank">01:26:40.080</a></span> | <span class="t">use pytorch as its own batch norm we'll just use pytorches there's nothing wrong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5204" target="_blank">01:26:44.080</a></span> | <span class="t">with ours but we try to switch to pytorches when something we've recreated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5209" target="_blank">01:26:49.480</a></span> | <span class="t">exists there we'll use our momentum learner and we'll fit for three epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5217" target="_blank">01:26:57.440</a></span> | <span class="t">and so as you can see it's going a little bit more slowly now and then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5224" target="_blank">01:27:04.240</a></span> | <span class="t">other thing I'm going to do is I'm going to decrease the learning rate and keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5230" target="_blank">01:27:10.840</a></span> | <span class="t">the existing model and then train for a little bit longer the idea being that as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5240" target="_blank">01:27:20.160</a></span> | <span class="t">the you know as it's kind of getting close to a pretty good answer maybe it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5246" target="_blank">01:27:26.600</a></span> | <span class="t">just wants to be able to fine-tune that a little bit and so we by decreasing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5251" target="_blank">01:27:31.080</a></span> | <span class="t">learning rate we give it a chance to fine-tune a little bit so let's see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5258" target="_blank">01:27:38.960</a></span> | <span class="t">we're going so we got to eighty seven point eight percent accuracy after three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5263" target="_blank">01:27:43.360</a></span> | <span class="t">epochs which is an improvement I guess mainly thanks to well basically thanks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5272" target="_blank">01:27:52.080</a></span> | <span class="t">to using this smaller mini batch size now with a smaller mini batch size you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5277" target="_blank">01:27:57.360</a></span> | <span class="t">do have to decrease the learning rate so I found I could still get away with point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5281" target="_blank">01:28:01.320</a></span> | <span class="t">two which is pretty cool and look at this after just one more epoch by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5285" target="_blank">01:28:05.880</a></span> | <span class="t">decreasing the learning rate we've got up to eighty nine point seven oh we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5290" target="_blank">01:28:10.640</a></span> | <span class="t">make it eighty nine point nine so towards ninety percent but not quite ninety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5295" target="_blank">01:28:15.240</a></span> | <span class="t">percent eighty nine point nine so we're going to have to do some more work to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5300" target="_blank">01:28:20.520</a></span> | <span class="t">get up to our magical 90% number but we are getting pretty close all right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5309" target="_blank">01:28:29.280</a></span> | <span class="t">that is the end of initialization an incredibly important topic as hopefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5317" target="_blank">01:28:37.000</a></span> | <span class="t">you've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5320" target="_blank">01:28:40.600</a></span> | <span class="t">accelerated SGD let's see if we can use this to get us up to 90 plus above 90</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5329" target="_blank">01:28:49.680</a></span> | <span class="t">percent so let's do our normal imports and data set up as usual and so just to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5337" target="_blank">01:28:57.220</a></span> | <span class="t">summarize what we've got we've got our metrics callback we've got our activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5341" target="_blank">01:29:01.960</a></span> | <span class="t">stats on the general value so our callbacks are going to be the device</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5345" target="_blank">01:29:05.880</a></span> | <span class="t">callback to put it on CUDA or whatever the metrics the progress bar the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5349" target="_blank">01:29:09.680</a></span> | <span class="t">activation stats our activation function is going to be our general value with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5355" target="_blank">01:29:15.240</a></span> | <span class="t">point one leakiness and point four subtraction and we've got the inner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5361" target="_blank">01:29:21.600</a></span> | <span class="t">weights which we need to tell it about how leaky they are and then if we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5365" target="_blank">01:29:25.960</a></span> | <span class="t">doing a learning rate finder we've got a different set of callbacks so it's no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5370" target="_blank">01:29:30.760</a></span> | <span class="t">real reason to have a progress bar callback with a learning rate finder I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5374" target="_blank">01:29:34.640</a></span> | <span class="t">guess it's pretty short anyway oh which reminds me there was one little thing I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5380" target="_blank">01:29:40.440</a></span> | <span class="t">didn't mention in initializing which is a fun trick you might want to play around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5388" target="_blank">01:29:48.240</a></span> | <span class="t">with and in fact Sam Watkins asked a question earlier in the chat and I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5394" target="_blank">01:29:54.840</a></span> | <span class="t">didn't answer it because it's actually exactly here in general value I added a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5401" target="_blank">01:30:01.440</a></span> | <span class="t">second thing you might have seen which is the maximum value and if the maximum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5406" target="_blank">01:30:06.280</a></span> | <span class="t">value is set then I clamp the value to be no more than the maximum so basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5414" target="_blank">01:30:14.720</a></span> | <span class="t">as a result let's say you set it to three then the line would go up to here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5419" target="_blank">01:30:19.160</a></span> | <span class="t">it like it does here and then it go up to three like it does here and then it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5422" target="_blank">01:30:22.480</a></span> | <span class="t">will be flat and using that can be a nice way I mean I'd probably go higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5429" target="_blank">01:30:29.360</a></span> | <span class="t">up to about six but that can be a nice way to avoid yeah numbers getting too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5434" target="_blank">01:30:34.480</a></span> | <span class="t">big and maybe if you really wanted to have fun you could do kind of like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5439" target="_blank">01:30:39.320</a></span> | <span class="t">leaky maximum which I haven't tried yet where maybe at the top it kind of goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5443" target="_blank">01:30:43.440</a></span> | <span class="t">like you know ten times smaller kind of just exactly like the leaky could be so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5450" target="_blank">01:30:50.920</a></span> | <span class="t">anyway if you do that you'd need to make sure that the you know that you're still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5457" target="_blank">01:30:57.800</a></span> | <span class="t">getting zero one layers with your initialization but that would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5462" target="_blank">01:31:02.040</a></span> | <span class="t">something you could consider playing with okay so let's create our own little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5477" target="_blank">01:31:17.680</a></span> | <span class="t">SGD class so an SGD class is going to need to know what parameters to optimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5486" target="_blank">01:31:26.240</a></span> | <span class="t">and if you remember the module dot parameters method returns a generator so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5491" target="_blank">01:31:31.760</a></span> | <span class="t">we use a list to to turn you know we want to turn that into a list so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5496" target="_blank">01:31:36.640</a></span> | <span class="t">kind of forced to be a particular you know not not something that's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5499" target="_blank">01:31:39.760</a></span> | <span class="t">change we're going to need to know the learning rate we're going to need to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5505" target="_blank">01:31:45.280</a></span> | <span class="t">the weight decay which we'll look at a bit in a moment and for reasons we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5510" target="_blank">01:31:50.320</a></span> | <span class="t">discuss later we also want to keep track of what batch number are we up to so an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5516" target="_blank">01:31:56.160</a></span> | <span class="t">optimizer basically has two things a step and a zero grad so what steps going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5521" target="_blank">01:32:01.880</a></span> | <span class="t">to do is obviously with no grad because this is not part of the learn part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5527" target="_blank">01:32:07.320</a></span> | <span class="t">the thing that we're optimizing this is the optimization itself we go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5531" target="_blank">01:32:11.320</a></span> | <span class="t">each tensor of parameters and we do a step of the optimizer and we'll come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5536" target="_blank">01:32:16.520</a></span> | <span class="t">back to this in a moment we do a step of the regularizer and we keep track of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5540" target="_blank">01:32:20.120</a></span> | <span class="t">what batch number we're up to and so what does SGD do in our step of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5544" target="_blank">01:32:24.720</a></span> | <span class="t">optimizer it subtracts out from the parameter it's gradient times the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5551" target="_blank">01:32:31.980</a></span> | <span class="t">learning rate so that's an SGD optimization step and to zero the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5556" target="_blank">01:32:36.840</a></span> | <span class="t">gradients we go through each parameter and we zero it and that's in torch dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5568" target="_blank">01:32:48.660</a></span> | <span class="t">no grad so okay so use dot data that way that's if you use dot data then you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5581" target="_blank">01:33:01.640</a></span> | <span class="t">need to say the no grad just a little typing saver okay so let's create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5590" target="_blank">01:33:10.920</a></span> | <span class="t">train learner so it's a learner with a training callback kind of built in and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5595" target="_blank">01:33:15.120</a></span> | <span class="t">we're going to set the optimization function to be this SGD we just wrote</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5599" target="_blank">01:33:19.360</a></span> | <span class="t">and we'll use the batch norm model with the weight initialization we've used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5604" target="_blank">01:33:24.360</a></span> | <span class="t">before and if we train it then just this is just should give us basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5610" target="_blank">01:33:30.640</a></span> | <span class="t">same results we've had before while this is training I'm going to talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5615" target="_blank">01:33:35.400</a></span> | <span class="t">regularization hopefully you remember from part one of this course or from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5623" target="_blank">01:33:43.560</a></span> | <span class="t">your other learning what weight decay is and so just to remind you weight decay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5633" target="_blank">01:33:53.500</a></span> | <span class="t">or L2 regularization are kind of the same thing and basically what we're doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5642" target="_blank">01:34:02.600</a></span> | <span class="t">we're saying let's add the square of the weights to the loss function now if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5653" target="_blank">01:34:13.120</a></span> | <span class="t">add the square of the weights to the loss function so whatever our loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5659" target="_blank">01:34:19.040</a></span> | <span class="t">function is so we'll just call it loss bump up we're adding plus the sum of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5672" target="_blank">01:34:32.160</a></span> | <span class="t">square of the weights so that's our L and so the only thing we actually care</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5678" target="_blank">01:34:38.280</a></span> | <span class="t">about is the derivative of that and the derivative of that is equal to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5690" target="_blank">01:34:50.080</a></span> | <span class="t">derivative of the loss plus the derivative of this which is just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5706" target="_blank">01:35:06.200</a></span> | <span class="t">sum of 2w and then what we do is we multiply this bit here by some some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5716" target="_blank">01:35:16.320</a></span> | <span class="t">constant which is the weight decay so we call that weight decay and so since the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5720" target="_blank">01:35:20.240</a></span> | <span class="t">weight decay could directly incorporate the number the two we can actually just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5724" target="_blank">01:35:24.600</a></span> | <span class="t">delete that entirely and just time weight decay do that I'm doing this very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5738" target="_blank">01:35:38.600</a></span> | <span class="t">quickly because we have already covered it in part one so this is hopefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5743" target="_blank">01:35:43.120</a></span> | <span class="t">something that you've all seen before so we can do weight decay by taking our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5752" target="_blank">01:35:52.800</a></span> | <span class="t">gradients and adding on the weight decay times the weights and so as a result then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5767" target="_blank">01:36:07.520</a></span> | <span class="t">in SGD because that's part of the gradient oh man I got it the wrong way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5774" target="_blank">01:36:14.160</a></span> | <span class="t">around need to do that first I guess well whatever okay so since that's part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5787" target="_blank">01:36:27.800</a></span> | <span class="t">the gradient then in the optimization step that's using the gradient and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5794" target="_blank">01:36:34.360</a></span> | <span class="t">subtracting out gradient times learning rate but what you could do is because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5802" target="_blank">01:36:42.400</a></span> | <span class="t">we're just ending up doing p dot grad times self dot LR and the p dot grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5807" target="_blank">01:36:47.080</a></span> | <span class="t">update is just to add in WT times weight we could simply skip updating the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5814" target="_blank">01:36:54.240</a></span> | <span class="t">gradients and instead directly update the weights to subtract out the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5819" target="_blank">01:36:59.360</a></span> | <span class="t">rate times the WD times weight so they would be mathematically identical and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5824" target="_blank">01:37:04.840</a></span> | <span class="t">that is what we've done here in the regularization step we basically say if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5830" target="_blank">01:37:10.080</a></span> | <span class="t">you've got weight decay then just take P times equals 1 minus the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5841" target="_blank">01:37:21.240</a></span> | <span class="t">times the weight decay which is mathematically the same as this because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5846" target="_blank">01:37:26.320</a></span> | <span class="t">we've got weight on both sides so that's why the regularization is here inside our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5854" target="_blank">01:37:34.480</a></span> | <span class="t">SGD and yes so it's finished running that's good we've got a 85% accuracy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5860" target="_blank">01:37:40.960</a></span> | <span class="t">that all looks fine and we're able to train at a high learning rate of 0.4 so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5871" target="_blank">01:37:51.120</a></span> | <span class="t">that's pretty cool so now let's add momentum now we had a kind of a hacky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5876" target="_blank">01:37:56.840</a></span> | <span class="t">momentum learner before but we're going to see momentum should be in an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5880" target="_blank">01:38:00.040</a></span> | <span class="t">optimizer really and so let's talk a bit about what momentum actually is so let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5887" target="_blank">01:38:07.200</a></span> | <span class="t">just create some some data so our X's are just going to be equally spaced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5893" target="_blank">01:38:13.720</a></span> | <span class="t">numbers from minus four to four a hundred of them and our Y's are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5898" target="_blank">01:38:18.040</a></span> | <span class="t">going to be our X's divided by three squared one minus that plus some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5906" target="_blank">01:38:26.080</a></span> | <span class="t">randomization and so these dots here is our random data I'm going to show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5912" target="_blank">01:38:32.640</a></span> | <span class="t">what momentum is by example and this is something that Sylvain Goudre helped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5919" target="_blank">01:38:39.320</a></span> | <span class="t">build so thank you Sylvain for our book actually if memory serves correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5925" target="_blank">01:38:45.620</a></span> | <span class="t">actually I might have even be the course before that what we're going to do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5930" target="_blank">01:38:50.440</a></span> | <span class="t">we're going to show you what momentum looks like for a range of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5933" target="_blank">01:38:53.440</a></span> | <span class="t">levels of momentum these are the different levels we're going to use so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5937" target="_blank">01:38:57.880</a></span> | <span class="t">let's take a beta of 0.5 so that's going to be our first one so we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5941" target="_blank">01:39:01.400</a></span> | <span class="t">do a scatter plot of our X's and Y's that's the blue dots and then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5945" target="_blank">01:39:05.520</a></span> | <span class="t">going to go through each of the Y's and we're going to do this hopefully looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5950" target="_blank">01:39:10.320</a></span> | <span class="t">familiar this is doing a loop we're going to take our previous average which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5956" target="_blank">01:39:16.840</a></span> | <span class="t">we'll start at zero times beta which is 0.5 plus 1 minus beta that's 0.5 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5966" target="_blank">01:39:26.920</a></span> | <span class="t">our new average and then we'll append that to this red line and we'll do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5978" target="_blank">01:39:38.040</a></span> | <span class="t">for all the data points and then plot them and you can see what happens when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5982" target="_blank">01:39:42.600</a></span> | <span class="t">do that is that the red line becomes less bumpy right because each one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5989" target="_blank">01:39:49.880</a></span> | <span class="t">half it's this exact dot and half of whatever the red line previously was so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=5995" target="_blank">01:39:55.720</a></span> | <span class="t">again this is an exponentially weighted moving average and so we could have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6000" target="_blank">01:40:00.000</a></span> | <span class="t">implemented this using loop so as the beta gets higher it's saying do more of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6011" target="_blank">01:40:11.080</a></span> | <span class="t">just be wherever the red line used to be and less of where this particular data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6016" target="_blank">01:40:16.640</a></span> | <span class="t">point is and so that means when we have these kind of outliers the red line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6021" target="_blank">01:40:21.800</a></span> | <span class="t">doesn't jump around as much as you see but if your momentum gets too high then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6030" target="_blank">01:40:30.120</a></span> | <span class="t">it doesn't follow what's going on at all and in fact it's way behind right when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6035" target="_blank">01:40:35.380</a></span> | <span class="t">you're using momentum it's always going to be partially responding to how things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6040" target="_blank">01:40:40.920</a></span> | <span class="t">were many batches ago and so even at beta is 0.9 here the red line is offset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6051" target="_blank">01:40:51.320</a></span> | <span class="t">to the right because again it's taking it a while for it to recognize that all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6055" target="_blank">01:40:55.800</a></span> | <span class="t">things have changed because each time it's 0.9 of it is where the red line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6061" target="_blank">01:41:01.320</a></span> | <span class="t">used to be and only point one of it is what does this data point say so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6067" target="_blank">01:41:07.400</a></span> | <span class="t">what momentum does so the reason that momentum is useful is because when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6076" target="_blank">01:41:16.000</a></span> | <span class="t">have a you know a loss function that's actually kind of like very very bumpy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6087" target="_blank">01:41:27.600</a></span> | <span class="t">like that right you want to be able to follow the actual curve right so using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6097" target="_blank">01:41:37.120</a></span> | <span class="t">momentum you don't quite get that but you get a kind of a version of that that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6101" target="_blank">01:41:41.120</a></span> | <span class="t">offset to the right a little bit but still you know hopefully spending a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6107" target="_blank">01:41:47.760</a></span> | <span class="t">more time you don't really want to be heading off in this direction which you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6111" target="_blank">01:41:51.200</a></span> | <span class="t">would if you follow the line and then this direction which you would if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6113" target="_blank">01:41:53.820</a></span> | <span class="t">follow the line you really want to be following the average of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6117" target="_blank">01:41:57.440</a></span> | <span class="t">directions and that's what momentum lets you do so to use momentum we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6131" target="_blank">01:42:11.680</a></span> | <span class="t">inherit from SGD and we will override the definition of the optimization step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6136" target="_blank">01:42:16.840</a></span> | <span class="t">remember there was two things that step called it called the regularization step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6141" target="_blank">01:42:21.000</a></span> | <span class="t">and the optimization step so we're going to modify the optimization step we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6146" target="_blank">01:42:26.200</a></span> | <span class="t">not just going to do minus equals grad times self dot LR but instead then when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6152" target="_blank">01:42:32.440</a></span> | <span class="t">we create our momentum object we will tell it what momentum we want or default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6159" target="_blank">01:42:39.360</a></span> | <span class="t">to point nine store that away and then in the optimization step for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6165" target="_blank">01:42:45.560</a></span> | <span class="t">parameter because remember the optimization step is being called for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6169" target="_blank">01:42:49.560</a></span> | <span class="t">each parameter in our model so that's each layers weights and each layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6174" target="_blank">01:42:54.280</a></span> | <span class="t">biases for example we'll find out for that parameter have we ever stored away</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6179" target="_blank">01:42:59.320</a></span> | <span class="t">its moving average of gradients before and if we haven't then we'll set them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6185" target="_blank">01:43:05.600</a></span> | <span class="t">zero initially just like we did here and then we will do our loop right so we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6196" target="_blank">01:43:16.560</a></span> | <span class="t">going to say the moving average of exponentially weighted moving average of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6199" target="_blank">01:43:19.640</a></span> | <span class="t">gradients is equal to whatever it used to be times the momentum plus this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6208" target="_blank">01:43:28.600</a></span> | <span class="t">actual new batches gradients times one minus momentum so that's just doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6214" target="_blank">01:43:34.160</a></span> | <span class="t">loop as we discussed and so then we're just going to do exactly the same as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6218" target="_blank">01:43:38.200</a></span> | <span class="t">SGD update step but instead of multiplying by p dot grad we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6222" target="_blank">01:43:42.400</a></span> | <span class="t">multiplying it by p dot grad average so there's a cool little trick here right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6226" target="_blank">01:43:46.160</a></span> | <span class="t">which is that we are basically inventing a brand new attribute putting it inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6231" target="_blank">01:43:51.120</a></span> | <span class="t">the parameter tensor and that attribute is where we're storing away the moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6238" target="_blank">01:43:58.200</a></span> | <span class="t">average exponentially weighted moving average of gradients for that particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6241" target="_blank">01:44:01.920</a></span> | <span class="t">parameter so as we loop through the parameters we don't have to do any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6245" target="_blank">01:44:05.400</a></span> | <span class="t">special work to get access to that so I think that's pretty handy alright so one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6253" target="_blank">01:44:13.920</a></span> | <span class="t">interesting thing very interesting here I found is I could really hike the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6257" target="_blank">01:44:17.360</a></span> | <span class="t">learning rate way up to 1.5 and the reason why is because we're not getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6261" target="_blank">01:44:21.560</a></span> | <span class="t">these huge bumps anymore and so by getting rid of the huge bumps it the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6265" target="_blank">01:44:25.480</a></span> | <span class="t">whole thing's just a whole lot smoother so previously we got up to 85% because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6273" target="_blank">01:44:33.000</a></span> | <span class="t">we've gone back to our 1024 batch size and just three epochs and a constant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6278" target="_blank">01:44:38.840</a></span> | <span class="t">learning rate and look at that we've got up to 87.6% so it's really improved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6284" target="_blank">01:44:44.600</a></span> | <span class="t">things and the the loss function is nice and smooth as you can see okay and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6294" target="_blank">01:44:54.280</a></span> | <span class="t">then in our color dim plot you can see it's this is actually that's really the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6298" target="_blank">01:44:58.840</a></span> | <span class="t">really the smoothest we've seen and it's a bit different to the momentum learner</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6304" target="_blank">01:45:04.280</a></span> | <span class="t">because the momentum learner didn't have this one minus part right it wasn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6310" target="_blank">01:45:10.640</a></span> | <span class="t">lerping it was it was basically always including all of the grad plus a bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6315" target="_blank">01:45:15.680</a></span> | <span class="t">the momentum part so this is yeah this is a different better approach I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6326" target="_blank">01:45:26.140</a></span> | <span class="t">and yeah we've got a really nice smooth result one person's asking don't we get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6333" target="_blank">01:45:33.760</a></span> | <span class="t">similar effect I think in terms of the smoothness if we increase the batch size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6337" target="_blank">01:45:37.060</a></span> | <span class="t">which we do but if you just increase the batch size you're giving it less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6342" target="_blank">01:45:42.000</a></span> | <span class="t">opportunities to update so having a really big batch size is actually not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6346" target="_blank">01:45:46.560</a></span> | <span class="t">great yeah and lacun who created the first really successful confidence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6352" target="_blank">01:45:52.040</a></span> | <span class="t">living learn at 5 says he thinks the ideal batch size if you can get away</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6357" target="_blank">01:45:57.000</a></span> | <span class="t">that is one but it's just slow you wanted to have as many opportunities to update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6364" target="_blank">01:46:04.120</a></span> | <span class="t">as possible there's this weird thing recently where people seem to be trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6367" target="_blank">01:46:07.680</a></span> | <span class="t">to create really large batch sizes which to me is yeah doesn't make any sense we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6379" target="_blank">01:46:19.440</a></span> | <span class="t">want the smallest batch size we can get away with generally speaking to give it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6382" target="_blank">01:46:22.440</a></span> | <span class="t">the most chances to update so this has done a great job of that and we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6385" target="_blank">01:46:25.840</a></span> | <span class="t">getting very good results despite using yeah only three epochs of very large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6392" target="_blank">01:46:32.120</a></span> | <span class="t">batch size okay so that's called momentum now something that was developed in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6398" target="_blank">01:46:38.560</a></span> | <span class="t">course or announced in a Coursera course back in maybe 2012 2013 by Jeffrey Hinton</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6405" target="_blank">01:46:45.200</a></span> | <span class="t">has never been published is called RMS prop let's have it running while we talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6410" target="_blank">01:46:50.560</a></span> | <span class="t">about it RMS prop is going to update the optimization step using something very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6416" target="_blank">01:46:56.640</a></span> | <span class="t">similar to momentum but rather than lerping on the p dot grad we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6429" target="_blank">01:47:09.920</a></span> | <span class="t">lerp on p dot grad squared and well just to keep it to keep it kind of consistent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6439" target="_blank">01:47:19.400</a></span> | <span class="t">we won't call it mom we call it square mom but this is just the multiplier and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6443" target="_blank">01:47:23.320</a></span> | <span class="t">what are we doing with the grad squared well the idea is that a large grad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6449" target="_blank">01:47:29.120</a></span> | <span class="t">squared indicates a large variance of gradients so what we're then going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6456" target="_blank">01:47:36.800</a></span> | <span class="t">is divide by the square root of that plus epsilon now you'll see I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6465" target="_blank">01:47:45.320</a></span> | <span class="t">actually been a bit all over the place here with my batch norm I put the epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6470" target="_blank">01:47:50.960</a></span> | <span class="t">inside the square root in this case I'm putting the epsilon outside the square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6476" target="_blank">01:47:56.840</a></span> | <span class="t">root it does make a difference and so be careful as to how your epsilon is being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6483" target="_blank">01:48:03.200</a></span> | <span class="t">interpreted generally speaking I can't remember if I've been exactly right but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6487" target="_blank">01:48:07.440</a></span> | <span class="t">I've tried to be consistent with the papers or normal implementations this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6492" target="_blank">01:48:12.160</a></span> | <span class="t">a very common cause of confusion and errors though so what we're doing here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6498" target="_blank">01:48:18.880</a></span> | <span class="t">we're dividing the gradient by the the amount of variation so the square root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6507" target="_blank">01:48:27.880</a></span> | <span class="t">of the moving average of gradient squared and so the idea here is that if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6514" target="_blank">01:48:34.080</a></span> | <span class="t">the gradient has been moving around all over the place then we don't really know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6520" target="_blank">01:48:40.920</a></span> | <span class="t">what it is right so should we shouldn't do a very big update if the gradient is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6528" target="_blank">01:48:48.280</a></span> | <span class="t">very very much the same all the time then we're very confident about it so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6534" target="_blank">01:48:54.080</a></span> | <span class="t">do want to be a big update I have no idea why we're doing this in two steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6537" target="_blank">01:48:57.600</a></span> | <span class="t">let's just pop this over here now because we are dividing our gradient by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6548" target="_blank">01:49:08.000</a></span> | <span class="t">this generally possibly rather small number we generally have to decrease the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6555" target="_blank">01:49:15.320</a></span> | <span class="t">learning rate so bring the learning rate back to 0.01 and as you see it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6560" target="_blank">01:49:20.320</a></span> | <span class="t">training oh it's not amazing but it's training okay so RMS prop can be quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6570" target="_blank">01:49:30.080</a></span> | <span class="t">nice it's a bit bumpy there isn't it I mean I could try decreasing it a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6576" target="_blank">01:49:36.720</a></span> | <span class="t">bit it'd be down to 3e neg 3 instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6583" target="_blank">01:49:43.160</a></span> | <span class="t">that's a little bit better and a bit smoother that's probably good see what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6598" target="_blank">01:49:58.840</a></span> | <span class="t">the colorful dimension plot looks like - shall we again it's very nice isn't it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6604" target="_blank">01:50:04.600</a></span> | <span class="t">that's great now one thing I did which I don't think I've seen done before I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6612" target="_blank">01:50:12.400</a></span> | <span class="t">don't remember people talking about is I actually decided not to do the normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6616" target="_blank">01:50:16.860</a></span> | <span class="t">thing of initializing to zeros because if I initialize to zeros then my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6625" target="_blank">01:50:25.280</a></span> | <span class="t">initial denominator here will basically be 0 plus epsilon which will mean my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6630" target="_blank">01:50:30.320</a></span> | <span class="t">initial learning rate will be very very high which I certainly don't want so I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6634" target="_blank">01:50:34.560</a></span> | <span class="t">actually initialized it at first to just whatever the first many batches gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6638" target="_blank">01:50:38.960</a></span> | <span class="t">is squared and I think this is a really useful little trick for using our mess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6645" target="_blank">01:50:45.880</a></span> | <span class="t">prop momentum you know momentum can be a bit aggressive sometimes for some really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6657" target="_blank">01:50:57.080</a></span> | <span class="t">you know finicky learning methods finicky architectures and so RMS prop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6664" target="_blank">01:51:04.720</a></span> | <span class="t">can be a good way to get reasonably fast optimization of a very finicky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6672" target="_blank">01:51:12.080</a></span> | <span class="t">architectures and in particular efficient net is an architecture which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6676" target="_blank">01:51:16.720</a></span> | <span class="t">people have generally trained best with RMS prop so you don't see it a whole lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6681" target="_blank">01:51:21.840</a></span> | <span class="t">but you know in some ways it's just historical interest but you see it a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6686" target="_blank">01:51:26.560</a></span> | <span class="t">but I mean the thing we really want to look at is our RMS prop plus momentum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6692" target="_blank">01:51:32.400</a></span> | <span class="t">together and RMS prop plus momentum together exists it has a name you will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6697" target="_blank">01:51:37.320</a></span> | <span class="t">have heard the name many times name is Adam Adam is literally just RMS prop and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6703" target="_blank">01:51:43.400</a></span> | <span class="t">momentum so we rather annoyingly call them beta 1 and beta 2 they should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6711" target="_blank">01:51:51.280</a></span> | <span class="t">called momentum and square momentum or momentum of squares I suppose so beta 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6718" target="_blank">01:51:58.320</a></span> | <span class="t">is just the momentum from from the momentum optimizer beta 2 is just these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6723" target="_blank">01:52:03.920</a></span> | <span class="t">momentum for the squares from the RMS prop optimizer so we'll store those away</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6730" target="_blank">01:52:10.720</a></span> | <span class="t">and just like RMS prop we need the epsilon so I'm going to as before store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6739" target="_blank">01:52:19.440</a></span> | <span class="t">away the gradient average and the square average and then we're going to do our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6745" target="_blank">01:52:25.840</a></span> | <span class="t">lerping but there's a nice little trick here which is in order to avoid doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6753" target="_blank">01:52:33.160</a></span> | <span class="t">this where we just put the initial batch gradients as our starting values we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6761" target="_blank">01:52:41.560</a></span> | <span class="t">going to use zeros as our starting values and then we're going to unbiased</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6767" target="_blank">01:52:47.720</a></span> | <span class="t">them so basically the idea is that for the very first mini batch if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6772" target="_blank">01:52:52.480</a></span> | <span class="t">zero here being lerped with the gradient then the first mini batch will obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6782" target="_blank">01:53:02.160</a></span> | <span class="t">be closer to zero than it should be but we know exactly how much closer it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6787" target="_blank">01:53:07.400</a></span> | <span class="t">should be to zero which is just it's going to be self beta 1 times closer at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6797" target="_blank">01:53:17.080</a></span> | <span class="t">least in the first mini batch because that's what we've worked with and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6800" target="_blank">01:53:20.080</a></span> | <span class="t">the second mini batch to be self beta 1 squared and so and the third mini batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6804" target="_blank">01:53:24.120</a></span> | <span class="t">to be self beta 1 cubed and so forth and that's why we had this self dot I back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6810" target="_blank">01:53:30.320</a></span> | <span class="t">in our SGD which was keeping track of what mini batch were up to so we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6818" target="_blank">01:53:38.560</a></span> | <span class="t">that in order to do this unbiasing of the average oh dear I'm not unbiasing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6829" target="_blank">01:53:49.680</a></span> | <span class="t">square of the average am I I'm not whoops so we need to do that here as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6838" target="_blank">01:53:58.800</a></span> | <span class="t">wonder if this is going to help things a little bit unbiased square average is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6845" target="_blank">01:54:05.360</a></span> | <span class="t">going to be P dot square average and that will be beta 2 and so we will use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6856" target="_blank">01:54:16.960</a></span> | <span class="t">those unbiased versions so this this this unbiasing only matters for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6861" target="_blank">01:54:21.880</a></span> | <span class="t">first few mini batches where otherwise it would be too close to zero you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6865" target="_blank">01:54:25.800</a></span> | <span class="t">I'll be closer to zero than it should be right so we run that and so again you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6880" target="_blank">01:54:40.760</a></span> | <span class="t">know we've you would expect the learning rate to be similar to what RMS prop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6885" target="_blank">01:54:45.480</a></span> | <span class="t">needs because we're doing that same division so we actually do you have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6889" target="_blank">01:54:49.440</a></span> | <span class="t">same learning rate here and yeah so we're up to a 865 86.5 percent accuracy so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6898" target="_blank">01:54:58.080</a></span> | <span class="t">that's pretty good I think yeah it's actually a bit less good than momentum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6905" target="_blank">01:55:05.120</a></span> | <span class="t">which is fine you know obviously you can fiddle around or momentum we had 0.9 yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6914" target="_blank">01:55:14.120</a></span> | <span class="t">so you can fiddle around with different values of beta 2 beta 1 see if you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6918" target="_blank">01:55:18.880</a></span> | <span class="t">beat the momentum version I suspect you probably can okay oh we're a bit out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6931" target="_blank">01:55:31.000</a></span> | <span class="t">time aren't we all right I'm excited about the next bit but I wanted to spend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6936" target="_blank">01:55:36.480</a></span> | <span class="t">time doing it properly so I won't rush through it now but instead we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6939" target="_blank">01:55:39.880</a></span> | <span class="t">to do it next time so I will yes I will give you a hint that in our next lesson</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6947" target="_blank">01:55:47.720</a></span> | <span class="t">we will in fact get above 90% and it's got some very cool stuff to show you I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6954" target="_blank">01:55:54.920</a></span> | <span class="t">can't wait to show you that then but you know I think in the meantime let's give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6960" target="_blank">01:56:00.040</a></span> | <span class="t">ourselves a pat in the back that we have successfully implemented you know I mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6964" target="_blank">01:56:04.960</a></span> | <span class="t">think about all this stuff we've got running and happening and we've done the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6968" target="_blank">01:56:08.720</a></span> | <span class="t">whole thing from scratch using nothing but what's in the Python standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6973" target="_blank">01:56:13.820</a></span> | <span class="t">library we've re-implemented everything and it's we understand exactly what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6978" target="_blank">01:56:18.880</a></span> | <span class="t">going on so I think this is this is really quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6982" target="_blank">01:56:22.520</a></span> | <span class="t">terrifically cool personally I hope you feel the same way and look forward to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vGsc_NbU7xc&t=6987" target="_blank">01:56:27.560</a></span> | <span class="t">seeing you in the next lesson thanks bye</span></div></div></body></html>