<html><head><title>Turning Fails into Features: Zapier’s Hard-Won Eval Lessons — Rafal Willinski, Vitor Balocco, Zapier</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Turning Fails into Features: Zapier’s Hard-Won Eval Lessons — Rafal Willinski, Vitor Balocco, Zapier</h2><a href="https://www.youtube.com/watch?v=blrovBxxN9o"><img src="https://i.ytimg.com/vi_webp/blrovBxxN9o/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./blrovBxxN9o.html">Whisper Transcript</a> | <a href="./transcript_blrovBxxN9o.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">ZAPIER AGENT ZAPIER: Yeah, a brief introduction to ZAPIER Agents. I believe many of you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=19" target="_blank">00:00:19.920</a></span> | <span class="t">what ZAPIER is, this automation software, a lot of boxes, arrows, essentially about automating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=26" target="_blank">00:00:26.000</a></span> | <span class="t">your business processes. Agents is just, well, more agentic alternative to ZAPIER. You describe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=33" target="_blank">00:00:33.160</a></span> | <span class="t">what you want, we propose a bunch of tools, a trigger, you enable that, and hopefully we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=40" target="_blank">00:00:40.440</a></span> | <span class="t">automate your whole business processes. And a key lesson that we have after those two years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=46" target="_blank">00:00:46.600</a></span> | <span class="t">is that building good AI agents is hard, and building good platform to enable non-technical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=54" target="_blank">00:00:54.000</a></span> | <span class="t">people to build AI agents is even harder. That's because AI is non-deterministic, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=59" target="_blank">00:00:59.380</a></span> | <span class="t">on top of that, your users are even more non-deterministic. They are going to use your products in a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=65" target="_blank">00:01:05.160</a></span> | <span class="t">that you cannot imagine up front. So, if you think that building agents is not that hard, you probably have this kind of picture in mind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=76" target="_blank">00:01:16.320</a></span> | <span class="t">You probably stumbled upon this library called Blankchain. You pulled some examples, tutorial, you tweaked the prompt, pulled a bunch of tools, you chatted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=87" target="_blank">00:01:27.480</a></span> | <span class="t">with the solution, and they thought, well, it's actually kind of working, all right? So, let's deploy it, and let's collect some profit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=93" target="_blank">00:01:33.540</a></span> | <span class="t">It turns out the reality has a surprising amount of detail, and we believe that building probabilistic software is a little bit different than building traditional software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=104" target="_blank">00:01:44.700</a></span> | <span class="t">The initial prototype is only a start, and after you ship something to your users, your responsibility switches to building the data flywheel. So, once you start,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=114" target="_blank">00:01:54.080</a></span> | <span class="t">once your user starts using your product, you need to collect the feedback, you're starting to understand the usage patterns, the failures, so they can, then you can build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=123" target="_blank">00:02:03.080</a></span> | <span class="t">build more evals, build an understanding of what's failing, what are the use cases. As you're building more evals and burn features, probably your product is getting better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=132" target="_blank">00:02:12.460</a></span> | <span class="t">so you're getting more users, and there are more failures, and you have to build more features, and on and on and on. So, yeah, it forms this data flywheel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=142" target="_blank">00:02:22.460</a></span> | <span class="t">But, starting with the first step, okay, yeah, so starting from the beginning, how do you start collecting actionable feedback?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=151" target="_blank">00:02:31.460</a></span> | <span class="t">Backing up for just a second, the first step is to make sure you're instrumenting your code, right, which you probably already are doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=156" target="_blank">00:02:36.680</a></span> | <span class="t">Whether you're using Braintrust or something else, they all offer, like, an easy way to get started, like, just tracing your completion calls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=161" target="_blank">00:02:41.840</a></span> | <span class="t">And this is a good start, but, actually, you also want to make sure that you're recording much more than that in your traces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=167" target="_blank">00:02:47.120</a></span> | <span class="t">You want to record the two calls, the errors from those two calls, the pre- and post-processing steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=171" target="_blank">00:02:51.920</a></span> | <span class="t">That way, it will be much easier to debug what went wrong with the run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=175" target="_blank">00:02:55.760</a></span> | <span class="t">And you also want to strive to make the run repeatable for eval purposes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=179" target="_blank">00:02:59.840</a></span> | <span class="t">So, for instance, if you log data in the same shape as it appears in the runtime, it makes it much easier to convert it to an eval run later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=186" target="_blank">00:03:06.060</a></span> | <span class="t">because you can just prepopulate the inputs and expected outputs directly from your trace for free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=191" target="_blank">00:03:11.300</a></span> | <span class="t">And this is especially useful, as well, for two calls, because if your two call produces any side effects, you probably want to mock those in your evals, so you get all that for free if you're recording them in your trace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=202" target="_blank">00:03:22.760</a></span> | <span class="t">Okay, great. So, you've instrumented your code, and you started getting all this raw data from your runs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=208" target="_blank">00:03:28.520</a></span> | <span class="t">Now it's time to figure out what runs to actually pay attention to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=212" target="_blank">00:03:32.740</a></span> | <span class="t">Explicit user feedback is really high signal, so that's a good place to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=217" target="_blank">00:03:37.980</a></span> | <span class="t">Unfortunately, not many people actually click those classic thumbs up, thumbs up and thumbs down buttons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=222" target="_blank">00:03:42.740</a></span> | <span class="t">So, you've got to work a bit harder for that feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=225" target="_blank">00:03:45.740</a></span> | <span class="t">And in our experience, this works best when you ask for the feedback in the right context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=229" target="_blank">00:03:49.740</a></span> | <span class="t">So, you can be a little bit more aggressive about asking for the feedback, but you're in the right context, you're not bothering the user before that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=235" target="_blank">00:03:55.020</a></span> | <span class="t">So, for us, one example of this is once an agent finished running, even if it was just a test run, we show a feedback call to action at the bottom, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=242" target="_blank">00:04:02.200</a></span> | <span class="t">Did this run do what you expected? Give us the feedback now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=245" target="_blank">00:04:05.520</a></span> | <span class="t">And this small change actually gave us like a really nice bump in feedback submissions, surprisingly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=250" target="_blank">00:04:10.700</a></span> | <span class="t">So, thumbs up and thumbs down are a good benchmark, a good baseline, but try to find these critical moments in your user's journey</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=257" target="_blank">00:04:17.960</a></span> | <span class="t">where they'll be most likely to provide you that feedback, either because they're happy and satisfied or because they're angry and they want to tell you about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=264" target="_blank">00:04:24.180</a></span> | <span class="t">Even if you work really hard for the feedback, explicit feedback is still really rare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=269" target="_blank">00:04:29.860</a></span> | <span class="t">And explicit feedback that's detailed and actionable is even harder, because people are just not that interested in providing feedback generally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=276" target="_blank">00:04:36.720</a></span> | <span class="t">So, you also want to mine user interaction for implicit feedback, and the good news is there's actually a lot of low hanging fruit possibilities here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=285" target="_blank">00:04:45.180</a></span> | <span class="t">Here's an example from our app. Users can test an agent before they turn it on to see if everything's going okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=291" target="_blank">00:04:51.400</a></span> | <span class="t">So, if they do turn it on, that's actually really strong positive implicit feedback, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=296" target="_blank">00:04:56.400</a></span> | <span class="t">Copying a model's response is also good implicit feedback. Even OpenAI is doing this for ChatGPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=303" target="_blank">00:05:03.400</a></span> | <span class="t">And you can also look for implicit signals in the conversation. Here the user is clearly letting us know that they're not happy with the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=314" target="_blank">00:05:14.620</a></span> | <span class="t">Here they're telling the agent to stop slacking around, which is clearly implicit negative feedback, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=321" target="_blank">00:05:21.620</a></span> | <span class="t">Sometimes the user sends a follow-up message that is mostly rehashing what they asked the previous time to see if the LLM interprets that phrasing better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=327" target="_blank">00:05:27.840</a></span> | <span class="t">That's also good implicit negative feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=330" target="_blank">00:05:30.840</a></span> | <span class="t">And there's also a surprising amount of cursing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=333" target="_blank">00:05:33.840</a></span> | <span class="t">Recently, we had a lot of successes while using an LLM to detect and group frustrations, and we have this weekly report that we post in our Slack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=342" target="_blank">00:05:42.840</a></span> | <span class="t">But it took us a lot of tinkering to make sure that the LLM understood what frustration means in the context of our products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=348" target="_blank">00:05:48.220</a></span> | <span class="t">So, I encourage you to try it out. But expect a lot of tinkering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=351" target="_blank">00:05:51.060</a></span> | <span class="t">You should also not forget to look at more traditional user metrics, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=355" target="_blank">00:05:55.320</a></span> | <span class="t">There's a lot of stuff in there for you to mine implicit signals, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=358" target="_blank">00:05:58.380</a></span> | <span class="t">So, find what metrics your business cares about and figure out how to track them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=361" target="_blank">00:06:01.700</a></span> | <span class="t">Then you can distill some signal from that data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=364" target="_blank">00:06:04.180</a></span> | <span class="t">You can look for customers, for example, that churned in the last seven days and go look at their last interactions with your product before they left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=370" target="_blank">00:06:10.240</a></span> | <span class="t">And you're likely to find some signal there. Okay. So, I have raw data. What now? I'll let the industry experts speak. Why isn't it starting? Oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=383" target="_blank">00:06:23.720</a></span> | <span class="t">Yeah, or the beatings will continue until everyone looks at their data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=401" target="_blank">00:06:41.200</a></span> | <span class="t">Okay, but how actually are you going to do that? So, we believe that the first step is to either buy or build LLM Ops software. We do both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=414" target="_blank">00:06:54.680</a></span> | <span class="t">You're definitely going to need that to understand your agent runs, because one agent run is probably multiple LLM calls, multiple database interactions, tool calls, REST calls, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=424" target="_blank">00:07:04.680</a></span> | <span class="t">Each one of them can be source of failure, and it's really important to piece together this whole story, understand this, you know, what caused this cascading failure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=433" target="_blank">00:07:13.160</a></span> | <span class="t">Yeah, I said we are doing both, because I believe by coding your own internal tooling is really, really easy right now with Cursor and Cloud Cod, and it's going to pay you massive dividends in the future for two reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=447" target="_blank">00:07:27.600</a></span> | <span class="t">First of all, it gives you an ability to understand your data in your own specific domain context. And the second of all, it also -- you should be also able to create a functionality to turn every single interacting case or every failure into an eval with the minimal amount of fraction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=466" target="_blank">00:07:46.080</a></span> | <span class="t">So, whenever you see something interesting, there should be like a one click to turn it into an eval. It should become your instinct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=472" target="_blank">00:07:52.320</a></span> | <span class="t">Once you understand what's going on on a singular run basis, you can start understanding things at scale. So, now we can do feedback aggregations, clustering, you can bucket your failure modes, you can bucket your interactions, and then you're going to start to see what kind of tools are failing the most, what kind of interactions are the most problematic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=492" target="_blank">00:08:12.800</a></span> | <span class="t">That's going to create. That's going to create for you like an automatic roadmap. So, you'll know where to apply your time and effort to improve your product the most. Doing anything else is going to be a sub-optimal strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=504" target="_blank">00:08:24.480</a></span> | <span class="t">Something that we are also experimenting with is using reasoning models to explain the failures. Turns out that if you give them the trace output input instructions and anything you can find, they are pretty good at finding the root cause of a failure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=519" target="_blank">00:08:39.800</a></span> | <span class="t">Even if they are not going to do that, they are probably going to explain you the whole run or just direct your attention into something that's really interesting and might help you find the root cause of the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=531" target="_blank">00:08:51.480</a></span> | <span class="t">Cool. So, now you have a good short list of failure modes you want to work on first. It's time to start building out your evals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=539" target="_blank">00:08:59.080</a></span> | <span class="t">And we realized over time that there are different types of evals, and the types of evals that we want to build can be placed into this hierarchy that resembles the testing pyramid, for those of you that know that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=550" target="_blank">00:09:10.760</a></span> | <span class="t">So, with unit tests like evals at the base, end-to-end evals or trajectory evals, how we like to call them in the middle, and the ultimate way of evaluating using A/B testing with stage rollouts at the top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=561" target="_blank">00:09:21.760</a></span> | <span class="t">So, let's talk a bit about those. Starting with unit test evals, we are just trying to predict the n+1 state from the current state, so these work great when you want to do simple assertions, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=571" target="_blank">00:09:31.440</a></span> | <span class="t">For instance, you could check whether the next state is a specific tool call, or if the tool call parameters are correct, or if the answer contains a specific keyword, or if the agent determined that it was done, all that good stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=582" target="_blank">00:09:42.440</a></span> | <span class="t">So, if you're starting out, we recommend focusing on unit test evals first, because these are the easiest to add. It helps you build that muscle of looking at your data, spotting problems, creating evals that reproduce them, and then just focusing on fixing them, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=595" target="_blank">00:09:55.120</a></span> | <span class="t">Beware, though, of turning every positive feedback into an eval. We found that unit test evals are best for hill-climbing specific failure modes that you spot in your data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=604" target="_blank">00:10:04.560</a></span> | <span class="t">So, now unit test evals are not perfect, and we realized that ourselves. We realized we had over-index on unit test evals when the new models were coming out that were objectively stronger models, but they were still performing worse in our internal benchmarks, which was weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=620" target="_blank">00:10:20.420</a></span> | <span class="t">And because the majority of our evals were so fine-grained, this made it really hard to see the forest for the trees when benchmarking new models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=627" target="_blank">00:10:27.320</a></span> | <span class="t">There was always a lot of noise when we tried comparing runs. Like when you're looking at a single trace, it's easy to kind of go through the trace and understand what's happening, but when you need to kind of look at it from -- I don't know how to play it again, sorry -- when you want to look at it through an aggregation of many traces, then it starts getting difficult to understand what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=644" target="_blank">00:10:44.180</a></span> | <span class="t">Why are so many of these passing and some of these are regressing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=647" target="_blank">00:10:47.180</a></span> | <span class="t">Yeah. So, we realized that maybe machine can help us. It turns out in that previous video when I was investigating one experiment inside Braintrust, there is a lot of looking at the screen trying to figure out what went wrong, and we were like, hey, maybe we can just give this old data to, once again, a reasoning LLM and compare the models for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=668" target="_blank">00:11:08.180</a></span> | <span class="t">It turns out that with Braintrust MCP and reasoning model, you can just ask it to, hey, look at this run, look at this run, and tell me what's actually different about the new model that we are going to deploy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=679" target="_blank">00:11:19.180</a></span> | <span class="t">In this case, it was Gemini Pro versus Cloud, and what the reasoning model found was actually really, really good. It found that Cloud is like a decisive executor, whereas Gemini is really yapping a lot. It's asking follow-up questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=692" target="_blank">00:11:32.180</a></span> | <span class="t">You need some positive affirmations, and it's sometimes even hallucinating about JSON structures. So, yeah, it helped us a lot. It also surfaces a problem with unit test evals a lot, which is different models have different ways of trying to achieve the same goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=710" target="_blank">00:11:50.180</a></span> | <span class="t">And unit test evals. And unit test evals are penalizing different paths. They are hard-coded to only follow one path. And, yeah, our unit test evals were overfitting to our existing models. They are actually data collecting using that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=723" target="_blank">00:12:03.180</a></span> | <span class="t">So, what we started experimenting with is trajectory evals. Instead of grading just one iteration of an agent, we let the agent run to the end state. And we are not grading just the end state, but we are also grading all the tool calls that were made along the way and all the artifacts that have been generated along the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=746" target="_blank">00:12:26.180</a></span> | <span class="t">And this can be also paired with LLM as a judge. Vitor is going to speak about it later. Yeah, but they are not free. I think they have really high return on investment, but they are much harder to set up, especially if you are evaluating runs that have tools that cause side effects, right? When you are running an eval, you definitely don't want to send an email on behalf of the customer once again, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=771" target="_blank">00:12:51.180</a></span> | <span class="t">So, we had a fundamental question whether we should mock environment or not. And we decided that we are not going to mock the environment because otherwise you are going to get data that is just not reflecting the reality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=784" target="_blank">00:13:04.180</a></span> | <span class="t">So, what we started doing is just mirroring users' environment and crafting a synthetic copy of that. Also, they are much slower, right? So, they can sometimes take up to an hour. So, it's not pretty great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=800" target="_blank">00:13:20.180</a></span> | <span class="t">And we are also learning a bit more into LLM as a judge. This is when you use an LLM to grade or compare results from your evals. And it's tempting to lean into them for everything, but you need to make sure that the judge is judging things correctly, which can be surprisingly hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=815" target="_blank">00:13:35.180</a></span> | <span class="t">And you also have to be careful not to introduce subtle biases, right? Because even small things that you might overlook might end up influencing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=822" target="_blank">00:13:42.180</a></span> | <span class="t">Lately, we have also been experimenting with this concept of rubrics-based scoring. We use an LLM to judge the run. But each row in our dataset has a different set of rubrics that were handcrafted by a human and described in natural language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=837" target="_blank">00:13:57.180</a></span> | <span class="t">So, what specifically about this run should the LLM be paying attention to for the score? So, one example of this: Did the agent react to an unexpected error from the calendar API and then try it again?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=850" target="_blank">00:14:10.180</a></span> | <span class="t">So, to sum it up, here's our current mental model of the types of evals that we build for separate agents. We use LLM as a judge or rubrics-based evals to build a high-level overview of your system's capabilities. And these are great for benchmarking new models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=862" target="_blank">00:14:22.180</a></span> | <span class="t">We use trajectory evals to capture multi-term criteria. And we use unit tests like evals to debug specific failures. He'll climb them. But beware of overfitting with these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=874" target="_blank">00:14:34.180</a></span> | <span class="t">Yeah. And a couple of closing thoughts. Don't obsess over metrics. Remember that when a good metrics become a target, it ceases to be a good target. So, when you're close to achieving 100% score on your eval dataset, it's not meaning that you're doing a good job.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=889" target="_blank">00:14:49.180</a></span> | <span class="t">Actually meaning that your dataset is just not interesting, right? Because we don't have AGI yet. So, it's probably not true that your model is that good. Something that we're experimenting with lately is dividing dataset into two pools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=904" target="_blank">00:15:04.180</a></span> | <span class="t">Into the regressions dataset to make sure that we are making any changes. We are not breaking existing use cases for the customers. And also the aspirational dataset of things that are extremely hard. For instance, like nailing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=916" target="_blank">00:15:16.180</a></span> | <span class="t">you've got 200 tool calls in a row. And lastly, let's take a step back. What's the point of creating evals in the first place? Your goal isn't to maximize some imaginary number in a lab-like setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=931" target="_blank">00:15:31.180</a></span> | <span class="t">Your end goal is user satisfaction. So, the ultimate judge are your users. You shouldn't be optimizing for the biggest scores for the evals and completely disregard the vibes. So, that's why you think the ultimate verification method is an A/B test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=943" target="_blank">00:15:43.180</a></span> | <span class="t">Just take a small proportion of your traffic, let's say 5%, and route it to the new model, route it to the new prompt, monitor the feedback, check your metrics, like activation, user retention, and so on. Based on that, probably you can make the most educated guess instead of being in the lab and optimizing this imaginary number. That's all. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=966" target="_blank">00:16:06.180</a></span> | <span class="t">So, that's all. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=967" target="_blank">00:16:07.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=967" target="_blank">00:16:07.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=968" target="_blank">00:16:08.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=968" target="_blank">00:16:08.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=968" target="_blank">00:16:08.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=968" target="_blank">00:16:08.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=968" target="_blank">00:16:08.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=969" target="_blank">00:16:09.180</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=blrovBxxN9o&t=970" target="_blank">00:16:10.180</a></span> | <span class="t">I'll see you next time.</span></div></div></body></html>