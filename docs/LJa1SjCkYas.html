<html><head><title>Insights from Snorkel AI running Azure AI Infrastructure: Humza Iqbal and Lachlan Ainley</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Insights from Snorkel AI running Azure AI Infrastructure: Humza Iqbal and Lachlan Ainley</h2><a href="https://www.youtube.com/watch?v=LJa1SjCkYas"><img src="https://i.ytimg.com/vi_webp/LJa1SjCkYas/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./LJa1SjCkYas.html">Whisper Transcript</a> | <a href="./transcript_LJa1SjCkYas.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hey everyone, thanks so much for coming. I'm Hamza. I'm an applied research scientist at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=17" target="_blank">00:00:17.520</a></span> | <span class="t">Snorkel on the computer vision team working on fine-tuning, you know, foundation models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=23" target="_blank">00:00:23.100</a></span> | <span class="t">for enterprise use cases. Thanks Hamza. I like the ears. Thank you. Why don't you start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=33" target="_blank">00:00:33.340</a></span> | <span class="t">out by telling the folks a little bit about Snorkel and what you do there. It's an interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=39" target="_blank">00:00:39.520</a></span> | <span class="t">name, any relation to like AI for scuba diving or anything like that? Well, hot tubs actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=44" target="_blank">00:00:44.920</a></span> | <span class="t">because snorkel.com, once I joined the company, I learned it's a hot tub company, but no, actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=50" target="_blank">00:00:50.880</a></span> | <span class="t">we don't do anything for scuba diving or hot tubbing. And to give a little bit of context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=56" target="_blank">00:00:56.660</a></span> | <span class="t">the main problem that we're trying to solve is like, you know, data development for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=60" target="_blank">00:01:00.560</a></span> | <span class="t">enterprise. So, one key thing that I kind of want to take note of is the fact that, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=66" target="_blank">00:01:06.840</a></span> | <span class="t">know, out of the box, LLMs rarely meet enterprise quality, latency, and cost requirements. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=72" target="_blank">00:01:12.880</a></span> | <span class="t">know, to give some context, our customers are like Fortune 500 companies like banks, insurance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=77" target="_blank">00:01:17.620</a></span> | <span class="t">companies, places like that. And for them to deploy their models, they really need them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=81" target="_blank">00:01:21.940</a></span> | <span class="t">to be very reliable and very accurate. And off the shelf models like Claude or GPT4 or Gemini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=89" target="_blank">00:01:29.240</a></span> | <span class="t">may get you part of the way there, but they don't have that, you know, final mile that really,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=94" target="_blank">00:01:34.080</a></span> | <span class="t">you know, says, yes, we can like deploy these completely. So, what we focus on is developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=100" target="_blank">00:01:40.040</a></span> | <span class="t">data to fine-tune these models to get them there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=103" target="_blank">00:01:43.500</a></span> | <span class="t">So, Hamza, this makes sense, but why is it hard? What's the challenge in addressing this issue?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=111" target="_blank">00:01:51.660</a></span> | <span class="t">Yeah. So, data development is fundamentally challenging, and there's a few key reasons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=118" target="_blank">00:01:58.040</a></span> | <span class="t">for that. One is that, you know, RAG is just a starting point. You know, you guys may have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=122" target="_blank">00:02:02.520</a></span> | <span class="t">heard of like, you know, using like RAG to hook up like, you know, enterprise knowledge databases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=127" target="_blank">00:02:07.440</a></span> | <span class="t">to these models and get them to help give what, you know, these models we're not pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=132" target="_blank">00:02:12.540</a></span> | <span class="t">on. And I don't want to RAG on it. It's great and all, but it's just a starting point. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=137" target="_blank">00:02:17.540</a></span> | <span class="t">know, it won't get you all the way there. And, you know, quality in your data is absolutely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=142" target="_blank">00:02:22.660</a></span> | <span class="t">key. And finding and maintaining the right data is critical because, you know, a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=149" target="_blank">00:02:29.280</a></span> | <span class="t">times, you know, the common instruction tuning datasets may be very big, but they don't contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=154" target="_blank">00:02:34.460</a></span> | <span class="t">exactly the information that you need. Like, if you're training specifically on, I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=158" target="_blank">00:02:38.680</a></span> | <span class="t">know, a specific type of bank policy or a specific type of policy for certain industries, you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=164" target="_blank">00:02:44.600</a></span> | <span class="t">that very key slice of information to be able to really improve these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=170" target="_blank">00:02:50.840</a></span> | <span class="t">So I think folks have a good understanding of what you guys do and why you do it. Why don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=178" target="_blank">00:02:58.400</a></span> | <span class="t">you talk a little bit about what Snorkel is and what you're famous for besides the interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=183" target="_blank">00:03:03.240</a></span> | <span class="t">name?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=183" target="_blank">00:03:03.600</a></span> | <span class="t">Yeah. So Snorkel pioneered data development for LLMs and we're trusted by, you know, many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=191" target="_blank">00:03:11.280</a></span> | <span class="t">companies. We've worked with lots of companies in, like, you know, the Fortune 500 and all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=195" target="_blank">00:03:15.300</a></span> | <span class="t">that. We were spun up out of the Stanford AI lab quite a while ago and have a lot of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=200" target="_blank">00:03:20.520</a></span> | <span class="t">you know, decorative experience in, like, data development because it's key to, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=204" target="_blank">00:03:24.580</a></span> | <span class="t">many aspects of ML and we've published many papers and, you know, a lot of hot fields like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=209" target="_blank">00:03:29.580</a></span> | <span class="t">prompting, RAG, architectures and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=211" target="_blank">00:03:31.520</a></span> | <span class="t">Okay, nice. I think we're going to switch context here for a bit and talk a little bit about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=220" target="_blank">00:03:40.560</a></span> | <span class="t">specific research projects you guys are focused on. You guys take a research first culture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=227" target="_blank">00:03:47.920</a></span> | <span class="t">Yeah. Why don't you explain a little bit about that and the projects?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=231" target="_blank">00:03:51.940</a></span> | <span class="t">Yeah, thanks, Lachie. So first I kind of want to talk a little bit about our research focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=238" target="_blank">00:03:58.140</a></span> | <span class="t">overall. So really what we, the core question we try to answer is how can enterprises best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=243" target="_blank">00:04:03.580</a></span> | <span class="t">develop their data for custom AI models? You know, so we have, there are a few different directions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=249" target="_blank">00:04:09.020</a></span> | <span class="t">we want to pursue overall. One is keeping SMEs in the loop while maximizing the value of their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=254" target="_blank">00:04:14.140</a></span> | <span class="t">time. You know, because again, like, for a lot of these industries we need the subject</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=258" target="_blank">00:04:18.580</a></span> | <span class="t">matter experts that know the key details in order to be able to, you know, provide feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=264" target="_blank">00:04:24.820</a></span> | <span class="t">to models and help them be able to improve. The second is, you know, make data development</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=270" target="_blank">00:04:30.420</a></span> | <span class="t">programmatic, scalable, and auditable. Because, you know, while we do need SMEs, at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=276" target="_blank">00:04:36.900</a></span> | <span class="t">we also need to make things scalable in a way that solely manual intervention isn't. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=282" target="_blank">00:04:42.520</a></span> | <span class="t">really being able to combine those two things together that make, make this important. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=288" target="_blank">00:04:48.280</a></span> | <span class="t">the third is continuous evaluation with domain-specific dynamic benchmarks. You know, I'm sure you all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=294" target="_blank">00:04:54.460</a></span> | <span class="t">have seen things like LMSys or whatnot, and it's pretty good to see, you know, a general understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=300" target="_blank">00:05:00.020</a></span> | <span class="t">of where a lot of these LLMs fall in terms of their ability to do things. But for specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=305" target="_blank">00:05:05.240</a></span> | <span class="t">industries, you need specific benchmarks to say, how good is it at this? Like, you know, a bank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=311" target="_blank">00:05:11.320</a></span> | <span class="t">isn't going to care about how well these LLMs do at, say, grade school math, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=317" target="_blank">00:05:17.080</a></span> | <span class="t">So there's that. And I want to go into a little bit more detail and talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=322" target="_blank">00:05:22.840</a></span> | <span class="t">some active research projects we're working on right now. One is fine-grained evaluation and looking at,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=329" target="_blank">00:05:29.080</a></span> | <span class="t">you know, where evaluation for these models is broken. One particular area is, you know, in long context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=334" target="_blank">00:05:34.840</a></span> | <span class="t">models. You know, you guys may have seen things like the needle in the haystack test, where you take a bunch of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=340" target="_blank">00:05:40.840</a></span> | <span class="t">Paul Graham essays and insert some sentence and see how well it can find that. But, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=346" target="_blank">00:05:46.600</a></span> | <span class="t">one thing we found is that, again, that doesn't necessarily give a proper sense of how these models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=352" target="_blank">00:05:52.360</a></span> | <span class="t">handle long context in other domains. And, you know, really, again, breaking everything down domain by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=357" target="_blank">00:05:57.480</a></span> | <span class="t">domain is super critical. So, you know, figuring out how can we improve long context overall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=364" target="_blank">00:06:04.600</a></span> | <span class="t">Another key area is enterprise alignment. You know, making sure that these LLMs comply with, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=370" target="_blank">00:06:10.200</a></span> | <span class="t">company goals, regulations, and all that. You know, we don't want our LLMs to be committing any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=375" target="_blank">00:06:15.160</a></span> | <span class="t">career-limiting moves while outputting text. And another area, which is particularly near and dear to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=381" target="_blank">00:06:21.800</a></span> | <span class="t">me because I work on it actively, is multimodal alignment. We find that, you know, these models trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=387" target="_blank">00:06:27.080</a></span> | <span class="t">on public data, you know, underperform in, like, you know, specific domains. And one area we're working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=392" target="_blank">00:06:32.440</a></span> | <span class="t">on is using these large vision language models or LVLMs to be able to generate synthetic data without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=398" target="_blank">00:06:38.280</a></span> | <span class="t">manual annotation to be able to train, you know, downstream models. So, kind of being able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=403" target="_blank">00:06:43.320</a></span> | <span class="t">really have this flywheel of going from, like, specific data generation in the loop to model training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=409" target="_blank">00:06:49.320</a></span> | <span class="t">I think it's something that we're very excited about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=413" target="_blank">00:06:53.400</a></span> | <span class="t">That's great, Hamza. We're excited that a lot of these projects are happening on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=419" target="_blank">00:06:59.640</a></span> | <span class="t">Azure's AI infrastructure, obviously, as well. I think if you forward the slides a little bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=428" target="_blank">00:07:08.040</a></span> | <span class="t">I think, you know, you went through an experience with Azure getting on board and running these projects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=435" target="_blank">00:07:15.240</a></span> | <span class="t">I think people are really interested to maybe understand what are the best practices you had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=440" target="_blank">00:07:20.200</a></span> | <span class="t">working with our infrastructure, some of the pitfalls and the benefits as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=446" target="_blank">00:07:26.920</a></span> | <span class="t">You know, this one's my slide. I think the way we think about infrastructure that's supporting this wave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=455" target="_blank">00:07:35.880</a></span> | <span class="t">of AI, it's really about optimising it in every sense possible for the different AI applications and use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=466" target="_blank">00:07:46.680</a></span> | <span class="t">You know, we look at everything from our Azure data centres. We have over 300 worldwide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=471" target="_blank">00:07:51.720</a></span> | <span class="t">The CPU or the host, so combining our virtual machines with the right CPUs and offering the right throughput.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=481" target="_blank">00:08:01.160</a></span> | <span class="t">The accelerator, we use a diversity of accelerators from AMD, NVIDIA and our own first-party silicon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=489" target="_blank">00:08:09.960</a></span> | <span class="t">as well with the Maya chip. We have topologies that, you know, optimise that I/O between the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=497" target="_blank">00:08:17.080</a></span> | <span class="t">layers and obviously the networking throughput as well. So it's really about making sure that we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=505" target="_blank">00:08:25.160</a></span> | <span class="t">the best of breed at what we do at a super computing scale and deliver that back to the customers so we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=511" target="_blank">00:08:31.240</a></span> | <span class="t">a real cycle around, you know, learning from working with organisations like OpenAI, Mistral and others that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=519" target="_blank">00:08:39.800</a></span> | <span class="t">have trained their models on Azure's infrastructure and then being able to democratise that and deliver it back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=524" target="_blank">00:08:44.920</a></span> | <span class="t">customers as well. And so Hamza, with that, why don't you share a little bit more about what exactly you guys did on Azure?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=535" target="_blank">00:08:55.720</a></span> | <span class="t">Yeah, so first we'll actually talk a little bit about how we do distributed training in general with Azure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=542" target="_blank">00:09:02.120</a></span> | <span class="t">So we have a stack, you know, and so on the ML framework side, you know, we use PyTorch, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=548" target="_blank">00:09:08.520</a></span> | <span class="t">pretty standard framework. We also use a library called Horovod, which handles multi-node communication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=554" target="_blank">00:09:14.920</a></span> | <span class="t">So if we have like multiple Azure VMs, how do they communicate with each other? It allows for faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=560" target="_blank">00:09:20.840</a></span> | <span class="t">communication across nodes. And on the underlying, you know, hardware layer, you know, we use a bunch of Azure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=567" target="_blank">00:09:27.160</a></span> | <span class="t">VMs that could be like A100s or H100s and they're all, you know, we connect to them. We use Horovod to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=574" target="_blank">00:09:34.040</a></span> | <span class="t">connect to them and send gradients through for distributed training and they all read and write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=578" target="_blank">00:09:38.200</a></span> | <span class="t">to a single network file system or NFS. You can basically think of an NFS as being a shared file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=583" target="_blank">00:09:43.880</a></span> | <span class="t">system that every machine has access to as if it were a local file system, which makes it very seamless to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=590" target="_blank">00:09:50.200</a></span> | <span class="t">read data or write checkpoints for models. So that's kind of our overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=596" target="_blank">00:09:56.520</a></span> | <span class="t">infrastack. And what about running on on Azure? You guys had some specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=605" target="_blank">00:10:05.160</a></span> | <span class="t">workloads that you were covering. Yeah. So we've run a number of projects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=610" target="_blank">00:10:10.360</a></span> | <span class="t">on Azure and we've run them on different sizes from like, you know, one node to dozens of nodes. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=616" target="_blank">00:10:16.440</a></span> | <span class="t">you know, we've run like DPO align models with a bunch of instruction response preference data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=622" target="_blank">00:10:22.520</a></span> | <span class="t">We've run like, you know, preference optimization techniques and using and the things we did that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=628" target="_blank">00:10:28.840</a></span> | <span class="t">have used the most compute have been large scale distributed training jobs for multimodal training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=633" target="_blank">00:10:33.800</a></span> | <span class="t">and inference, you know, with like dozens of GPUs. So, yeah, these are the kinds of workloads that we've run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=640" target="_blank">00:10:40.760</a></span> | <span class="t">And I think you had some lessons learned throughout as well. It wasn't, you know, I think it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=654" target="_blank">00:10:54.360</a></span> | <span class="t">always smooth sailing with these types of jobs. So any, any best practices, traps for young players</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=660" target="_blank">00:11:00.760</a></span> | <span class="t">out there in terms of your experience? Yeah, absolutely. So there's a number of key architectural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=669" target="_blank">00:11:09.160</a></span> | <span class="t">considerations to keep in mind. One is having enough nodes to support, you know, your ideal batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=675" target="_blank">00:11:15.560</a></span> | <span class="t">So on the CV side, you know, when I was training like, you know, let's say like clip based models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=680" target="_blank">00:11:20.120</a></span> | <span class="t">one thing I learned is that, you know, you want you, we wanted enough nodes to have a certain batch size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=685" target="_blank">00:11:25.560</a></span> | <span class="t">but we also didn't want too many such that we would either fall into the trap of having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=690" target="_blank">00:11:30.200</a></span> | <span class="t">too large a batch size or under utilizing whichever nodes we were using. So getting that balance right was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=696" target="_blank">00:11:36.040</a></span> | <span class="t">pretty important. Another thing is networking bottlenecks. We want to make sure all of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=700" target="_blank">00:11:40.760</a></span> | <span class="t">data and nodes are close together. Like, you know, imagine if for example, you had your, um, a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=705" target="_blank">00:11:45.880</a></span> | <span class="t">your, your data in like US West, and maybe you had your nodes in like, you know, um, Asia or something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=712" target="_blank">00:11:52.600</a></span> | <span class="t">like that, right? You know, that's like a simple example, but bottom line is networking communication is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=717" target="_blank">00:11:57.640</a></span> | <span class="t">pretty important. And you want to make sure that, you know, when you're sending this data across,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=721" target="_blank">00:12:01.800</a></span> | <span class="t">that's not going to be a bottleneck when you, because if you're training, one thing that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=725" target="_blank">00:12:05.560</a></span> | <span class="t">happen is when you send gradients to different copies of the model, that could be a bottleneck</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=730" target="_blank">00:12:10.680</a></span> | <span class="t">there. Another bottleneck could be data reading, um, because recall that, you know, while your model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=735" target="_blank">00:12:15.960</a></span> | <span class="t">is training and you're doing your forward and backward propagations, um, asynchronously, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=740" target="_blank">00:12:20.200</a></span> | <span class="t">loading in data to be fed to the model. Um, and so this is where the NFS read speed is absolutely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=747" target="_blank">00:12:27.640</a></span> | <span class="t">critical. Um, if your NFS is not reading in your data fast enough, then you could be bottlenecked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=752" target="_blank">00:12:32.680</a></span> | <span class="t">waiting for data to be processed and your model isn't actually crunching. Um, and you know, one key, key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=759" target="_blank">00:12:39.160</a></span> | <span class="t">takeaway for both of these is make sure your GPU utilization is good. NVIDIA SMI is your best friend here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=764" target="_blank">00:12:44.920</a></span> | <span class="t">If you see your GPU utilization being low, you know, don't be afraid to look into why and, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=770" target="_blank">00:12:50.200</a></span> | <span class="t">you can, you know, do different things to debug. Like if it's multi node, for example, then, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=775" target="_blank">00:12:55.320</a></span> | <span class="t">you can test networking and stuff like that. If it's on a single node, that likely means it's a data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=779" target="_blank">00:12:59.640</a></span> | <span class="t">loading issue. So there's lots of different ways and tools that you can use to step into these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=785" target="_blank">00:13:05.160</a></span> | <span class="t">And, you know, don't underestimate the basics of like reliability, flexibility and manageability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=790" target="_blank">00:13:10.360</a></span> | <span class="t">So, you know, one thing that we were, we really cared about as a team is we wanted to make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=795" target="_blank">00:13:15.080</a></span> | <span class="t">that our data distribute that, you know, when we're training experiments, right, we were like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=799" target="_blank">00:13:19.560</a></span> | <span class="t">going through, sometimes we needed all the nodes, sometimes we needed very few. And, you know, being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=804" target="_blank">00:13:24.760</a></span> | <span class="t">able to work with instances that gave us that flexibility over a long period of time is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=808" target="_blank">00:13:28.920</a></span> | <span class="t">important. You know, when we were shopping around, some cloud providers only let us use compute for a fixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=814" target="_blank">00:13:34.760</a></span> | <span class="t">amount of time, like maybe, say, a month or two months. And, you know, as a trade-off, we'd have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=819" target="_blank">00:13:39.400</a></span> | <span class="t">a bunch of compute, but that didn't really work for us because we weren't in it for training a model for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=824" target="_blank">00:13:44.520</a></span> | <span class="t">some fixed amount of time. We wanted something where we could go on and off for a longer period of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=829" target="_blank">00:13:49.720</a></span> | <span class="t">That's great insight, Hamza. I think also we talked about some of the advantages of using Azure, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=841" target="_blank">00:14:01.480</a></span> | <span class="t">would be great if you could shamelessly plug that for Azure as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=844" target="_blank">00:14:04.760</a></span> | <span class="t">Yeah, happy to. So, you know, one was availability. You know, the Azure VMs were dedicated and allowed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=851" target="_blank">00:14:11.240</a></span> | <span class="t">us to adjust our capacity on demand. The reliability was pretty good. It was consistently dependable with,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=857" target="_blank">00:14:17.080</a></span> | <span class="t">like, no real issues. You know, NFS throughput was also quite good. You know, again, right, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=863" target="_blank">00:14:23.960</a></span> | <span class="t">if your NFS is bad, then that means that, you know, you're not reading in data fast enough. Or,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=868" target="_blank">00:14:28.440</a></span> | <span class="t">for example, being able to dynamically change the size of your NFS. If, let's say, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=873" target="_blank">00:14:33.240</a></span> | <span class="t">you need more or less capacity. If you need more because you suddenly have more data than you realize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=878" target="_blank">00:14:38.200</a></span> | <span class="t">you had before, then you need to be able to tell it that. At the same time, if you realize that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=882" target="_blank">00:14:42.760</a></span> | <span class="t">your NFS is over-provisioned, you don't want to be, you know, overpaying in Azure bills. Though I'm sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=888" target="_blank">00:14:48.360</a></span> | <span class="t">Lockie wouldn't mind that. But, and, you know, the ease of use is very important, you know, clear documentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=895" target="_blank">00:14:55.160</a></span> | <span class="t">and straightforward process. You know, like, as the guy that set up Azure for my team, I really didn't like it if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=900" target="_blank">00:15:00.840</a></span> | <span class="t">other people needed to bug me. And thankfully, once I got things working, it just worked and I did not need to be paid. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=907" target="_blank">00:15:07.160</a></span> | <span class="t">So, that is very important. That's really good to hear, Hamza.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=911" target="_blank">00:15:11.960</a></span> | <span class="t">Yeah, I think, like, you know, this is fantastic. And I think you had some specific data points. You guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=921" target="_blank">00:15:21.400</a></span> | <span class="t">recently went through a process to go from the A100s through to the H100s or the VMs leveraging those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=929" target="_blank">00:15:29.480</a></span> | <span class="t">Can you share a bit of insight around what you observed with that experience?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=933" target="_blank">00:15:33.560</a></span> | <span class="t">Yeah. So, the key takeaway is that H100s are really good. One thing we wanted to do when we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=941" target="_blank">00:15:41.160</a></span> | <span class="t">doing this was do a cost analysis and see, okay, for a given number of H100s and a given number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=946" target="_blank">00:15:46.520</a></span> | <span class="t">A100s that cost the same amount, what kind of training and inference are we getting? And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=950" target="_blank">00:15:50.280</a></span> | <span class="t">here you can see we're comparing two H100s to four A100s because that's what works out about the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=956" target="_blank">00:15:56.120</a></span> | <span class="t">cost-wise. And we're doing better on both training and inference, which means that we're doing better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=961" target="_blank">00:16:01.400</a></span> | <span class="t">per dollar just by switching here. And, you know, there are a couple of key points I really want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=967" target="_blank">00:16:07.720</a></span> | <span class="t">emphasize here. One is that, you know, it's really nice when you just have a very simple plug-and-play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=974" target="_blank">00:16:14.520</a></span> | <span class="t">change that works. You know, there's a lot of ongoing work to optimize, you know, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=978" target="_blank">00:16:18.920</a></span> | <span class="t">especially like things like inference, for example, with your KV caches, your partial KV caches, your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=983" target="_blank">00:16:23.400</a></span> | <span class="t">speculative decodings. And it's really nice to be able to say, hey, let's just do something simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=988" target="_blank">00:16:28.200</a></span> | <span class="t">and have it work. And the second is that, with this faster inference in particular, we can go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=994" target="_blank">00:16:34.200</a></span> | <span class="t">more synthetic data, higher-end model accuracy, and it just enables us a flywheel of faster iteration,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=999" target="_blank">00:16:39.000</a></span> | <span class="t">which is super critical for being able to, like, do more development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1002" target="_blank">00:16:42.680</a></span> | <span class="t">Yeah, I mean, you can see from the numbers, the performance is there. And I'm assuming that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1010" target="_blank">00:16:50.440</a></span> | <span class="t">internally, just that ability to do more with fewer GPUs has been a really great benefit for you guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1017" target="_blank">00:16:57.960</a></span> | <span class="t">So your question is, why the inference is taking longer than the training time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1038" target="_blank">00:17:18.440</a></span> | <span class="t">I think it was because we were doing the larger batch size, and it just happened to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1044" target="_blank">00:17:24.840</a></span> | <span class="t">work out that way, that for whatever batch size we were doing, it just wound up taking longer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1049" target="_blank">00:17:29.880</a></span> | <span class="t">because, yeah, with the training, you typically need a smaller batch size, because you have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1053" target="_blank">00:17:33.320</a></span> | <span class="t">to put more things in memory for backprop, and somehow it just wound up working that way?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1058" target="_blank">00:17:38.440</a></span> | <span class="t">Do you want the mic?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1071" target="_blank">00:17:51.720</a></span> | <span class="t">No, because I know that when we were comparing training inference across the hardware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1081" target="_blank">00:18:01.000</a></span> | <span class="t">those were kept fixed, like whatever inference batch size we were using for DH100 was the same as DA100,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1087" target="_blank">00:18:07.560</a></span> | <span class="t">so that wasn't a factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1094" target="_blank">00:18:14.040</a></span> | <span class="t">yeah, so speaking about what's next, I think this is a good segue, so with the next slide,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1105" target="_blank">00:18:25.560</a></span> | <span class="t">just from our point of view with Azure, we are sort of adopting and we spoke about optimizing at every layer of the stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1116" target="_blank">00:18:36.360</a></span> | <span class="t">and I think the addition of our AI accelerator, that's used for our own internal workloads across Microsoft 365.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1124" target="_blank">00:18:44.520</a></span> | <span class="t">But we've just announced the AMD MI300X with the high bandwidth memory, we've got the NVIDIA A100s, the H100s, and we'll be adopting Blackwell.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1136" target="_blank">00:18:56.440</a></span> | <span class="t">What's really exciting is the pace of innovation in silicon has never been like this before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1144" target="_blank">00:19:04.040</a></span> | <span class="t">We're talking with NVIDIA almost doing two releases a year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1147" target="_blank">00:19:07.800</a></span> | <span class="t">Previously, you know, I think it might have been one every two years or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1154" target="_blank">00:19:14.040</a></span> | <span class="t">It's really amazing to see this growth in the silicon, how far we're getting, and what Hamza just shared,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1161" target="_blank">00:19:21.880</a></span> | <span class="t">the ability to do more and more with less on the infrastructure as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1166" target="_blank">00:19:26.440</a></span> | <span class="t">And so I think, you know, certainly as far as Azure and our AI infrastructure strategy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1173" target="_blank">00:19:33.320</a></span> | <span class="t">it's to continue to adopt these new evolutions and really make sure the right GPUs are used in the right places for the right workloads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1183" target="_blank">00:19:43.000</a></span> | <span class="t">Hamza, what about for Snorkel AI, what's next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1185" target="_blank">00:19:45.640</a></span> | <span class="t">Yeah, so to give you all a sneak peek into what we're working on actively at Snorkel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1190" target="_blank">00:19:50.680</a></span> | <span class="t">what do we have a number of directions, you know, one is to, you know, say, like, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1194" target="_blank">00:19:54.840</a></span> | <span class="t">better data leads to better gen AI and get better prototypes to production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1198" target="_blank">00:19:58.520</a></span> | <span class="t">So we want to explore new ways to programmatically utilize preference signals for data synthesis and curation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1203" target="_blank">00:20:03.880</a></span> | <span class="t">We also want to develop scalable SME entry points for data development using rationales and custom taxonomies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1211" target="_blank">00:20:11.640</a></span> | <span class="t">And finally, we want, you know, better multimodal retrieval algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1214" target="_blank">00:20:14.840</a></span> | <span class="t">And we want to evaluate those on domain specific data sets to scale up the retrieval models we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1220" target="_blank">00:20:20.920</a></span> | <span class="t">So, and we're, of course, excited to do all these things on Azure AI infra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1225" target="_blank">00:20:25.160</a></span> | <span class="t">Fantastic, Hamza.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1228" target="_blank">00:20:28.680</a></span> | <span class="t">Well, thank you so much for presenting and speaking on behalf of Microsoft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1234" target="_blank">00:20:34.120</a></span> | <span class="t">We really appreciate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1235" target="_blank">00:20:35.800</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1236" target="_blank">00:20:36.440</a></span> | <span class="t">Thank you guys so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=LJa1SjCkYas&t=1237" target="_blank">00:20:37.560</a></span> | <span class="t">Thank you.</span></div></div></body></html>