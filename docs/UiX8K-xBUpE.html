<html><head><title>Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer</h2><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE"><img src="https://i.ytimg.com/vi_webp/UiX8K-xBUpE/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=129">2:9</a> Transformer vs Mistral<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=335">5:35</a> Mistral 7B vs Mistral 8x7B<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=505">8:25</a> Sliding Window Attention<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2024">33:44</a> KV-Cache with Rolling Buffer Cache<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2967">49:27</a> Pre-Fill and Chunking<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3420">57:0</a> Sparse Mixture of Experts (SMoE)<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3862">64:22</a> Model Sharding<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3974">66:14</a> Pipeline Parallelism<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4271">71:11</a> xformers (block attention)<br><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5047">84:7</a> Conclusion<br><br><div style="text-align: left;"><a href="./UiX8K-xBUpE.html">Whisper Transcript</a> | <a href="./transcript_UiX8K-xBUpE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome back to my channel today, we are gonna talk about Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3" target="_blank">00:00:03.780</a></span> | <span class="t">So as you know Mistral is a new language model that came out a few months ago from Mistral AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=9" target="_blank">00:00:09.280</a></span> | <span class="t">Which is a one of the hottest to start up right now in Europe for language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=13" target="_blank">00:00:13.720</a></span> | <span class="t">It also became a unicorn recently and we will exploring both the models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=18" target="_blank">00:00:18.580</a></span> | <span class="t">They released the one is the 7 billion and one is the 8 by 7 billion model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=21" target="_blank">00:00:21.840</a></span> | <span class="t">So let's review the topics of today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=23" target="_blank">00:00:23.960</a></span> | <span class="t">the first thing I will introduce you is the architectural differences between the vanilla transformer and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=29" target="_blank">00:00:29.240</a></span> | <span class="t">architecture of Mistral later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=30" target="_blank">00:00:30.920</a></span> | <span class="t">We will see what is the sliding window attention and how it is related to the concept of receptive field a concept that usually we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=37" target="_blank">00:00:37.520</a></span> | <span class="t">find in convolutional neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=39" target="_blank">00:00:39.440</a></span> | <span class="t">I will briefly review the KB cache because I want to introduce the concept of rolling buffer cache and also how it is done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=46" target="_blank">00:00:46.480</a></span> | <span class="t">with the pre-filling and chunking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=48" target="_blank">00:00:48.480</a></span> | <span class="t">we will see what is a sparse mixture of experts model sharding with a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=54" target="_blank">00:00:54.160</a></span> | <span class="t">with a very brief introduction with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=57" target="_blank">00:00:57.840</a></span> | <span class="t">pipeline parallelism and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=59" target="_blank">00:00:59.840</a></span> | <span class="t">Last but not least we will also go through the code of Mistral because there is a lot of innovations in the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=66" target="_blank">00:01:06.160</a></span> | <span class="t">Especially when they use the Xformers library with the block attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=69" target="_blank">00:01:09.440</a></span> | <span class="t">So I want to guide you into understanding the code because it can be really hard for beginners to understand and find themselves around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=76" target="_blank">00:01:16.880</a></span> | <span class="t">There are some topics that are related to Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=79" target="_blank">00:01:19.880</a></span> | <span class="t">But will not be covered in this current video because I already covered them in my previous video about Lama and in particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=85" target="_blank">00:01:25.960</a></span> | <span class="t">I will not be talking about the RMS normalization, the rotary positional encoding and the grouped query attention because I already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=91" target="_blank">00:01:31.200</a></span> | <span class="t">Teach them in depth in my previous video on Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=94" target="_blank">00:01:34.240</a></span> | <span class="t">So if you want to know about them, please watch my previous video on Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=97" target="_blank">00:01:37.600</a></span> | <span class="t">In order the only prerequisite that I hope you have before watching this video because the topics we are going to touch are quite advanced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=105" target="_blank">00:01:45.360</a></span> | <span class="t">Is that you are familiar with the transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=108" target="_blank">00:01:48.080</a></span> | <span class="t">So if you are not familiar with the transformer model and the attention mechanism in particular and in particular the self attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=114" target="_blank">00:01:54.160</a></span> | <span class="t">Please go watch my video on the transformer in which I teach all this concept very thoroughly very in detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=120" target="_blank">00:02:00.360</a></span> | <span class="t">These are really a prerequisite for watching this video because the topics here are quite advanced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=125" target="_blank">00:02:05.720</a></span> | <span class="t">Okay, let's proceed further</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=128" target="_blank">00:02:08.360</a></span> | <span class="t">So let's watch the differences between the vanilla transformer and Mistral at the architecture level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=133" target="_blank">00:02:13.480</a></span> | <span class="t">as you can see from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=136" target="_blank">00:02:16.200</a></span> | <span class="t">Image here, which I built by myself using the code because they didn't release any architecture picture in the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=144" target="_blank">00:02:24.000</a></span> | <span class="t">And the architecture of Mistral first of all, let's talk about some terminology</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=149" target="_blank">00:02:29.040</a></span> | <span class="t">When you have a model like this made up of many encoder layers plus linear and the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=155" target="_blank">00:02:35.200</a></span> | <span class="t">We are talking about a decoder only model because this part this model here looks like the decoder of the vanilla</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=162" target="_blank">00:02:42.840</a></span> | <span class="t">Transformer you can see here except for the cross attention because as you can see here, there is no cross attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=169" target="_blank">00:02:49.360</a></span> | <span class="t">When we have a model without the linear and the softmax we call it an encoder only model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=174" target="_blank">00:02:54.920</a></span> | <span class="t">For example BERT is an encoder only model because BERT has some heads at the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=180" target="_blank">00:03:00.080</a></span> | <span class="t">Which is one or more linear layers depending on the application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=184" target="_blank">00:03:04.080</a></span> | <span class="t">But itself BERT doesn't need a head because it can be used for multiple downstream tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=188" target="_blank">00:03:08.560</a></span> | <span class="t">so it's called an encoder only model because it resembles the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=191" target="_blank">00:03:11.800</a></span> | <span class="t">Encoder side of the transformer because as you can see in the encoder side, there is no linear and softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=197" target="_blank">00:03:17.240</a></span> | <span class="t">So Mistral is a decoder only model and it's very similar if not equal to Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=203" target="_blank">00:03:23.320</a></span> | <span class="t">The differences between Lama and Mistral are highlighted here in red</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=208" target="_blank">00:03:28.000</a></span> | <span class="t">the first difference between Lama and Mistral is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=211" target="_blank">00:03:31.480</a></span> | <span class="t">In the self attention we use the sliding window attention and we still use the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=219" target="_blank">00:03:39.400</a></span> | <span class="t">But and also the KV cache for inferencing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=222" target="_blank">00:03:42.320</a></span> | <span class="t">But this is a rolling buffer KV cache and it's actually related to the fact that we are using sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=227" target="_blank">00:03:47.680</a></span> | <span class="t">So later, we will see all these concepts and the another difference is that the feedforward layer here instead of using the relu function that we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=235" target="_blank">00:03:55.880</a></span> | <span class="t">In the vanilla transformer or the ZWIGLU function that we use in Lama here in Mistral we use the SILU function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=242" target="_blank">00:04:02.480</a></span> | <span class="t">And the feedforward is one in case of Mistral 7b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=248" target="_blank">00:04:08.960</a></span> | <span class="t">So the first model they released and it can be eight feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=253" target="_blank">00:04:13.840</a></span> | <span class="t">Networks in parallel with each other which are the experts of this mixture of experts in the case of Mistral 8x7b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=261" target="_blank">00:04:21.760</a></span> | <span class="t">We will see later how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=264" target="_blank">00:04:24.520</a></span> | <span class="t">So for now, you just need to understand that Mistral is made up of okay the input which are converted into embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=271" target="_blank">00:04:31.080</a></span> | <span class="t">Then we have this block which is repeated n times and we will see that in the case of Mistral is repeated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=276" target="_blank">00:04:36.800</a></span> | <span class="t">32 times one after another such that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=280" target="_blank">00:04:40.120</a></span> | <span class="t">Output of each layer is fed to the next layer as input and the output of the last layer is then sent to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=287" target="_blank">00:04:47.920</a></span> | <span class="t">RMS norm to the linear and to the softmax to produce the output of the model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=292" target="_blank">00:04:52.760</a></span> | <span class="t">This is exactly the same as what we do with any other transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=297" target="_blank">00:04:57.880</a></span> | <span class="t">Usually we have many of these blocks here. Now in the code of Mistral this part here is known as transformer block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=305" target="_blank">00:05:05.560</a></span> | <span class="t">But it's also known as encoder block or decoder block depending on the contents of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=312" target="_blank">00:05:12.960</a></span> | <span class="t">Block here. I will refer to it as an encoder block because if you look at it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=317" target="_blank">00:05:17.640</a></span> | <span class="t">It looks like exactly as the block of the encoder side</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=320" target="_blank">00:05:20.560</a></span> | <span class="t">So it has a multi-head attention, add-end norm, a feedforward and add-end norm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=324" target="_blank">00:05:24.720</a></span> | <span class="t">The only difference is that the normalization here comes before the the block of the feedforward and the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=332" target="_blank">00:05:32.000</a></span> | <span class="t">Okay, let's move forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=335" target="_blank">00:05:35.520</a></span> | <span class="t">Now let's compare the two models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=338" target="_blank">00:05:38.080</a></span> | <span class="t">So one is Mistral 7B and one is Mistral 8x7B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=341" target="_blank">00:05:41.160</a></span> | <span class="t">The parameter dim indicates the dimensions of the the dimensions of the embedding vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=347" target="_blank">00:05:47.660</a></span> | <span class="t">So how big is the embedding vector? So each token is represented by an embedding vector of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=352" target="_blank">00:05:52.720</a></span> | <span class="t">4096 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=355" target="_blank">00:05:55.360</a></span> | <span class="t">We have 32 of the encoder layers. So this block here is repeated 32 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=362" target="_blank">00:06:02.880</a></span> | <span class="t">The head dimension indicates as you remember in the multi-head attention we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=367" target="_blank">00:06:07.540</a></span> | <span class="t">Each head is watching in the entire sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=371" target="_blank">00:06:11.960</a></span> | <span class="t">But only a part of the embedding of each token and this indicates how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=377" target="_blank">00:06:17.280</a></span> | <span class="t">How many dimensions each head will attend to in each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=381" target="_blank">00:06:21.920</a></span> | <span class="t">For the in the multi-head attention and the hidden dimension here indicates the hidden dimension of the feedforward layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=388" target="_blank">00:06:28.760</a></span> | <span class="t">so if the in the case of the feedforward layer, we have two linear layers one that converts the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=393" target="_blank">00:06:33.600</a></span> | <span class="t">Dimension of the embedding vector into the hidden size then another one that converts the hidden size back into the embedding vector dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=402" target="_blank">00:06:42.440</a></span> | <span class="t">So in the case of the Mistral they are using as a hidden size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=405" target="_blank">00:06:45.980</a></span> | <span class="t">14336 usually this is a multiple of the dimension and it looks like it's 3.5 the dimension here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=415" target="_blank">00:06:55.520</a></span> | <span class="t">The number of heads of attention for the query is a 32 while the number of heads for the K and V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=421" target="_blank">00:07:01.680</a></span> | <span class="t">So the key and values is 8 and they are not equal because of the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=426" target="_blank">00:07:06.320</a></span> | <span class="t">So if you remember from my previous video on llama in which we talked about the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=430" target="_blank">00:07:10.420</a></span> | <span class="t">In the very simple case of the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=434" target="_blank">00:07:14.360</a></span> | <span class="t">We have the multi query attention which means that only the query have the multi head while the key and V don't have the multi head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=440" target="_blank">00:07:20.760</a></span> | <span class="t">attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=441" target="_blank">00:07:21.680</a></span> | <span class="t">Which means that you may have eight heads for the query and only one head for the K and V in the case of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=447" target="_blank">00:07:27.600</a></span> | <span class="t">grouped query attention means that each group of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=449" target="_blank">00:07:29.800</a></span> | <span class="t">Query will have one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=452" target="_blank">00:07:32.600</a></span> | <span class="t">Attention head for the K and V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=454" target="_blank">00:07:34.920</a></span> | <span class="t">So in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=456" target="_blank">00:07:36.000</a></span> | <span class="t">Every four query have one attention head for the keys and values if this concept is not clear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=462" target="_blank">00:07:42.280</a></span> | <span class="t">I describe it very thoroughly in my previous video on llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=466" target="_blank">00:07:46.240</a></span> | <span class="t">the window size is the size of the sliding window that we used in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=471" target="_blank">00:07:51.420</a></span> | <span class="t">Calculation of the attention and we will see later how it works. The context length is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=476" target="_blank">00:07:56.920</a></span> | <span class="t">What is the context size upon which the model was trained upon?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=481" target="_blank">00:08:01.480</a></span> | <span class="t">And it's much bigger for the 8 by 7 B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=484" target="_blank">00:08:04.720</a></span> | <span class="t">The vocabulary size is the same for both and then the last two parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=488" target="_blank">00:08:08.280</a></span> | <span class="t">You can see here are related to the sparse mixture of experts and we will see later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=493" target="_blank">00:08:13.440</a></span> | <span class="t">How it works, but we just remember that we have eight experts and for each token we use two experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=499" target="_blank">00:08:19.800</a></span> | <span class="t">But later I will clarify how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=501" target="_blank">00:08:21.800</a></span> | <span class="t">Let's proceed further. So let's talk about the sliding window attention. But before we talk about the sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=508" target="_blank">00:08:28.440</a></span> | <span class="t">I need to review a little bit of the self attention mechanism. So what is self attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=513" target="_blank">00:08:33.720</a></span> | <span class="t">Self attention is a mechanism that allows the model to relate tokens to each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=519" target="_blank">00:08:39.160</a></span> | <span class="t">So tokens that are in the same sentence are related with each other through the self attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=524" target="_blank">00:08:44.160</a></span> | <span class="t">This is why it's called self attention because each token is watching other tokens of the of the same sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=530" target="_blank">00:08:50.640</a></span> | <span class="t">And when when this is means basically that the query key and values are the same matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=537" target="_blank">00:08:57.000</a></span> | <span class="t">So imagine we have the following sentence the cat is on a chair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=543" target="_blank">00:09:03.840</a></span> | <span class="t">We have our query which is a matrix made up of six tokens each token represented by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=549" target="_blank">00:09:09.760</a></span> | <span class="t">4096 dimensions, which is the dim parameter that we saw before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=553" target="_blank">00:09:13.480</a></span> | <span class="t">This is multiplied by the transpose of the keys, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=557" target="_blank">00:09:17.800</a></span> | <span class="t">4096 by 6 but it's just the query matrix transpose because the query key and values are the same matrix in the case of self attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=566" target="_blank">00:09:26.600</a></span> | <span class="t">This will produce a matrix that is 6 by 6</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=569" target="_blank">00:09:29.840</a></span> | <span class="t">Because the inner two dimensions kind of cancel out and the outer dimensions indicate the dimension of the output matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=577" target="_blank">00:09:37.320</a></span> | <span class="t">Now, what are the values in this matrix representing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=581" target="_blank">00:09:41.840</a></span> | <span class="t">The first value here indicates the dot product of the first token with the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=587" target="_blank">00:09:47.920</a></span> | <span class="t">The first row of the query with the first column of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=592" target="_blank">00:09:52.280</a></span> | <span class="t">So basically the dot product of the embedding of the first token with itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=596" target="_blank">00:09:56.840</a></span> | <span class="t">The second value here indicates the dot product of the first row of the query matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=602" target="_blank">00:10:02.560</a></span> | <span class="t">With the second column of the key matrix here the transpose of the keys matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=607" target="_blank">00:10:07.400</a></span> | <span class="t">Which basically means that it's the dot product of the embedding of the first token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=612" target="_blank">00:10:12.040</a></span> | <span class="t">So the with the embedding of the second token, which is cat and etc, etc for all the other values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=618" target="_blank">00:10:18.480</a></span> | <span class="t">Don't concentrate too much on the values because all the values I put here are random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=622" target="_blank">00:10:22.240</a></span> | <span class="t">And also the fact that these numbers are less than one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=625" target="_blank">00:10:25.120</a></span> | <span class="t">It's not necessary because the dot product can be bigger than one. It's not a condition of the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=630" target="_blank">00:10:30.440</a></span> | <span class="t">Usually in the formula we also normalize here we divide by the dimension of the dk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=638" target="_blank">00:10:38.160</a></span> | <span class="t">dk basically it's the size, the part of the embedding to which this particular attention head will attend to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=646" target="_blank">00:10:46.160</a></span> | <span class="t">But let's pretend that we only have one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=649" target="_blank">00:10:49.680</a></span> | <span class="t">One head so dk is equal to d model. So basically this head will watch the full embedding of each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=656" target="_blank">00:10:56.880</a></span> | <span class="t">Okay, usually we train autoregressive models. So language model is an autoregressive model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=663" target="_blank">00:11:03.160</a></span> | <span class="t">It means that the output depends on the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=665" target="_blank">00:11:05.840</a></span> | <span class="t">The next token depends only on the previous tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=669" target="_blank">00:11:09.240</a></span> | <span class="t">And this is why we apply a causal mask. Causal mask means basically that in the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=675" target="_blank">00:11:15.280</a></span> | <span class="t">We don't want to release a word with future words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=679" target="_blank">00:11:19.000</a></span> | <span class="t">So words that come after it but only with the words that come before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=683" target="_blank">00:11:23.280</a></span> | <span class="t">so for example, we don't want the word "the" to be related to the word "cat" because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=688" target="_blank">00:11:28.680</a></span> | <span class="t">The word "cat" comes after the word "the" but on the other hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=693" target="_blank">00:11:33.840</a></span> | <span class="t">We want the word "cat" to be related to the word "the" because it comes before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=698" target="_blank">00:11:38.480</a></span> | <span class="t">And for this reason we apply this causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=700" target="_blank">00:11:40.960</a></span> | <span class="t">Because the attention mechanism uses the softmax function we can see here. The softmax function basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=709" target="_blank">00:11:49.440</a></span> | <span class="t">will transform all this minus infinity into zero because the formula of the softmax has at numerator an e to the power of X and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=717" target="_blank">00:11:57.640</a></span> | <span class="t">When X goes to minus infinity e to the power of minus infinity will go to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=722" target="_blank">00:12:02.360</a></span> | <span class="t">So this is why we apply a mask in which we put all the values that we don't want, all the interactions that we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=728" target="_blank">00:12:08.320</a></span> | <span class="t">Want between tokens. We just mask them out by replacing them with minus infinity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=732" target="_blank">00:12:12.920</a></span> | <span class="t">So that when we apply the softmax, the softmax will take care of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=738" target="_blank">00:12:18.320</a></span> | <span class="t">transforming them into zeros</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=740" target="_blank">00:12:20.320</a></span> | <span class="t">Okay, also the softmax will do another thing because it will not only convert this minus infinities to zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=747" target="_blank">00:12:27.880</a></span> | <span class="t">But it will also modify the other value for each row such that they sum up to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=752" target="_blank">00:12:32.720</a></span> | <span class="t">So as you can see now these values here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=755" target="_blank">00:12:35.280</a></span> | <span class="t">They don't sum up to one for each row, right? Because this is 0.2, 0.1 and 0.2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=759" target="_blank">00:12:39.760</a></span> | <span class="t">They don't sum up to one but the softmax will convert the minus infinities into zero and the remaining values for each row such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=766" target="_blank">00:12:46.560</a></span> | <span class="t">that they sum up to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=768" target="_blank">00:12:48.320</a></span> | <span class="t">Now let's talk about sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=770" target="_blank">00:12:50.960</a></span> | <span class="t">So we applied the causal mask to hide the interactions between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=775" target="_blank">00:12:55.120</a></span> | <span class="t">The words, a word and all the future words, but with the sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=780" target="_blank">00:13:00.840</a></span> | <span class="t">We also don't want the word to watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=783" target="_blank">00:13:03.160</a></span> | <span class="t">Other words that are outside its local context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=787" target="_blank">00:13:07.400</a></span> | <span class="t">What do I mean by this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=788" target="_blank">00:13:08.880</a></span> | <span class="t">In the previous case when we only applied the causal mask, the word chair for example was being related to all the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=795" target="_blank">00:13:15.120</a></span> | <span class="t">Tokens as you can see, so the token chair here is related to itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=799" target="_blank">00:13:19.000</a></span> | <span class="t">But also to the a on is cat v, so it could watch basically all the sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=805" target="_blank">00:13:25.120</a></span> | <span class="t">But in the case of sliding window attention, we don't want the word chair to watch words that are further than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=812" target="_blank">00:13:32.520</a></span> | <span class="t">the sliding window size from itself, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=815" target="_blank">00:13:35.840</a></span> | <span class="t">The sliding window size in this case is three, so tokens that are distance more than three from the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=822" target="_blank">00:13:42.800</a></span> | <span class="t">we are considering, so the word the chair should not be related to the word is because the distance is four and the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=828" target="_blank">00:13:48.240</a></span> | <span class="t">a should not be related to the word cat because the distance is four and of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=832" target="_blank">00:13:52.800</a></span> | <span class="t">We still want the mask to be causal because we don't want the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=837" target="_blank">00:13:57.140</a></span> | <span class="t">Each token to watch future words because we are training an autoregressive model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=841" target="_blank">00:14:01.880</a></span> | <span class="t">So the sliding window attention basically reduces the number of dot products that we are performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=849" target="_blank">00:14:09.720</a></span> | <span class="t">And this will improve the performance during the training and the inference because as you can see when we only apply the causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=855" target="_blank">00:14:15.680</a></span> | <span class="t">We are performing all these dot products you see here, but with the sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=860" target="_blank">00:14:20.720</a></span> | <span class="t">We are performing less dot products because all the other will be masked out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=864" target="_blank">00:14:24.240</a></span> | <span class="t">Sliding window attention however may lead to degradation of the performance of the model because as you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=871" target="_blank">00:14:31.640</a></span> | <span class="t">The word the chair and the word the are not related to each other anymore, right? So the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=878" target="_blank">00:14:38.680</a></span> | <span class="t">Will not be conveyed from the word the and the word chair the word chair will only be related to other tokens that are belonging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=885" target="_blank">00:14:45.600</a></span> | <span class="t">to the local context of this particular token so only the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=889" target="_blank">00:14:49.160</a></span> | <span class="t">Tokens that are in the same in the inside this is sliding window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=893" target="_blank">00:14:53.260</a></span> | <span class="t">This may be if this window is too small it may reduce the performance of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=898" target="_blank">00:14:58.480</a></span> | <span class="t">But it may also be beneficial because for example imagine you are reading a book you don't care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=904" target="_blank">00:15:04.180</a></span> | <span class="t">Relating the word in chapter 5 with the words in chapter 1 because most of the books</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=909" target="_blank">00:15:09.120</a></span> | <span class="t">They could be talking about totally different things and you don't even care about relating these two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=915" target="_blank">00:15:15.080</a></span> | <span class="t">But for sure you want to relate the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=917" target="_blank">00:15:17.700</a></span> | <span class="t">In the chapter 5 with other tokens in the chapter 5 because the local context matters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=922" target="_blank">00:15:22.980</a></span> | <span class="t">But I want to introduce you the concept of receptive field because when we use sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=929" target="_blank">00:15:29.580</a></span> | <span class="t">Even if the word chair and the are not related to each other actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=934" target="_blank">00:15:34.520</a></span> | <span class="t">Because in Mistral and in all transformer models we use multiple layers of encoders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=940" target="_blank">00:15:40.460</a></span> | <span class="t">We will see that the information so the the word the chair and the the will still be kind of related to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=947" target="_blank">00:15:47.280</a></span> | <span class="t">To each other not directly, but indirectly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=949" target="_blank">00:15:49.940</a></span> | <span class="t">In a concept that is very similar to the receptive field of the convolutional neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=955" target="_blank">00:15:55.600</a></span> | <span class="t">So let's talk about the receptive field as you remember in convolutional neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=961" target="_blank">00:16:01.520</a></span> | <span class="t">We have a mask a kernel that we run through an image. So imagine this is our original image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=967" target="_blank">00:16:07.760</a></span> | <span class="t">This one here and we run a mask that is a kernel that is 3x3 this one here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=973" target="_blank">00:16:13.600</a></span> | <span class="t">That when we run a kernel it will produce an output feature. So for example this feature here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=979" target="_blank">00:16:19.120</a></span> | <span class="t">This is the output produced by applying the kernel to the first 3x3 grid here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=985" target="_blank">00:16:25.440</a></span> | <span class="t">This value here the second value here in yellow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=989" target="_blank">00:16:29.480</a></span> | <span class="t">It will be produced when we will move our kernel to the next group of 3x3 pixels. So let me draw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=996" target="_blank">00:16:36.920</a></span> | <span class="t">Let's use the pen. So this value here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1001" target="_blank">00:16:41.720</a></span> | <span class="t">will be produced when we will move our kernel in this grid here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1010" target="_blank">00:16:50.920</a></span> | <span class="t">This value here is also an output feature of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1016" target="_blank">00:16:56.140</a></span> | <span class="t">convolutional kernel that is a 3x3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1019" target="_blank">00:16:59.160</a></span> | <span class="t">Applied to this layer 2. So this is a 3x3 kernel that is applied to this layer 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1025" target="_blank">00:17:05.840</a></span> | <span class="t">So apparently there is no connection between this one this pixel here and this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1031" target="_blank">00:17:11.820</a></span> | <span class="t">But because he this this output feature depends on a kernel applied in this grid and this grid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1039" target="_blank">00:17:19.520</a></span> | <span class="t">Includes this feature here which depends on this pixel here. We can safely say that this feature here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1046" target="_blank">00:17:26.720</a></span> | <span class="t">Depends indirectly also on this feature here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1050" target="_blank">00:17:30.520</a></span> | <span class="t">Even if they are not directly related to each other and this is the concept of the receptive field. So basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1056" target="_blank">00:17:36.200</a></span> | <span class="t">One feature of the convolutional neural networks can watch a much bigger receptive field</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1062" target="_blank">00:17:42.900</a></span> | <span class="t">down upward in the layers because of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1069" target="_blank">00:17:49.400</a></span> | <span class="t">Sequential application of kernels in the convolutional kernels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1073" target="_blank">00:17:53.240</a></span> | <span class="t">Let's see how this concept is related to the sliding window attention now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1078" target="_blank">00:17:58.360</a></span> | <span class="t">now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1080" target="_blank">00:18:00.280</a></span> | <span class="t">After we apply the softmax to the mask that we have seen before as I told you before all the minus infinities are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1086" target="_blank">00:18:06.840</a></span> | <span class="t">Converted into zero and all the other values are changed in such a way that they sum up to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1092" target="_blank">00:18:12.100</a></span> | <span class="t">So let's go back as you remember here. We have the minus infinities here here here here here and here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1097" target="_blank">00:18:17.920</a></span> | <span class="t">So now we apply the softmax and it will become zeros zeros here. Also, let me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1104" target="_blank">00:18:24.680</a></span> | <span class="t">Okay, all the zeros here all the zeros here and all the other values are changed in such a way that they sum up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1110" target="_blank">00:18:30.560</a></span> | <span class="t">One what is the next operation that we do in the self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1113" target="_blank">00:18:33.640</a></span> | <span class="t">We then take the output of the softmax and multiply it by the V matrix. So let's do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1118" target="_blank">00:18:38.660</a></span> | <span class="t">The V matrix is basically the same as the initial sequence because I told you this is self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1126" target="_blank">00:18:46.360</a></span> | <span class="t">So the query key and values are the same matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1129" target="_blank">00:18:49.160</a></span> | <span class="t">So this means let's analyze what happens by hand when we do this multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1134" target="_blank">00:18:54.640</a></span> | <span class="t">So let me change to the pen. Okay, the V matrix here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1139" target="_blank">00:18:59.200</a></span> | <span class="t">is a sequence of tokens where each token is a vector represented by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1144" target="_blank">00:19:04.960</a></span> | <span class="t">4096 dimensions so we can say that it's the output of the self-attention if you watch the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1152" target="_blank">00:19:12.760</a></span> | <span class="t">Dimensions of these two matrices. So it's a 6 by 6 and the 6 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1156" target="_blank">00:19:16.040</a></span> | <span class="t">4096 the output will be another matrix that is 6 by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1159" target="_blank">00:19:19.320</a></span> | <span class="t">4096 so it will have the same dimension as the V matrix and also as the Q and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1164" target="_blank">00:19:24.640</a></span> | <span class="t">Query matrix because they have the same dimensions. So it will be six tokens as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1171" target="_blank">00:19:31.120</a></span> | <span class="t">Okay, let's analyze. What is the first dimension of the output to this one here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1177" target="_blank">00:19:37.720</a></span> | <span class="t">So this first value of the output so the value on the row 1, column 1 of the output matrix will be the dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1185" target="_blank">00:19:45.200</a></span> | <span class="t">Product of the first row of this matrix here. So this row here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1190" target="_blank">00:19:50.640</a></span> | <span class="t">With the first column of this matrix, so the first column we can see here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1198" target="_blank">00:19:58.040</a></span> | <span class="t">As you can see most of the values here are 0 which means that all the rows from the 1 to 5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1206" target="_blank">00:20:06.280</a></span> | <span class="t">Sorry from 2 to 6 will not be used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1209" target="_blank">00:20:09.340</a></span> | <span class="t">but only the first row here fully the values of the first row will be used because if you remember the dot product is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1215" target="_blank">00:20:15.660</a></span> | <span class="t">first dimension with the first dimension of this column and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1219" target="_blank">00:20:19.560</a></span> | <span class="t">The second dimension of this row with the second dimension of this column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1223" target="_blank">00:20:23.960</a></span> | <span class="t">The third dimension of this row with the third dimension of this column and then we sum up all these values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1229" target="_blank">00:20:29.360</a></span> | <span class="t">So this first value of the output will only depend on the first token of the V matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1236" target="_blank">00:20:36.440</a></span> | <span class="t">You can see here. Let's check the second one the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1240" target="_blank">00:20:40.040</a></span> | <span class="t">dimension of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1242" target="_blank">00:20:42.400</a></span> | <span class="t">the first dimension of the second row of the output matrix will be the dot product of the first row of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1249" target="_blank">00:20:49.080</a></span> | <span class="t">this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1251" target="_blank">00:20:51.600</a></span> | <span class="t">With the first column of the V matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1255" target="_blank">00:20:55.760</a></span> | <span class="t">but most of the values are 0 which means that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1260" target="_blank">00:21:00.140</a></span> | <span class="t">dimension here and all the dimensions in this row will depend only on the first two tokens of the V matrix and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1268" target="_blank">00:21:08.120</a></span> | <span class="t">We can say the same for the third. Let's analyze the sixth one here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1272" target="_blank">00:21:12.240</a></span> | <span class="t">So the first dimension of the sixth row of the output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1277" target="_blank">00:21:17.040</a></span> | <span class="t">So this value here comes from the dot product of this row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1283" target="_blank">00:21:23.480</a></span> | <span class="t">And the first column of the V matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1286" target="_blank">00:21:26.920</a></span> | <span class="t">but most of the values at the beginning are 0 which means that it will only depend on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1292" target="_blank">00:21:32.860</a></span> | <span class="t">4, 5 and 6th token of the V matrix and so will be all the dimensions here because in each column</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1301" target="_blank">00:21:41.880</a></span> | <span class="t">Whatever the column we use from the V matrix the first values will always be multiplied by 0, 0, 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1308" target="_blank">00:21:48.600</a></span> | <span class="t">So it will only use the values in these three rows here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1311" target="_blank">00:21:51.880</a></span> | <span class="t">So we can safely say that the 6th token of the output matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1317" target="_blank">00:21:57.720</a></span> | <span class="t">of this self-attention mechanism will be a vector that will only depend on the last three tokens of the V matrix and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1325" target="_blank">00:22:05.680</a></span> | <span class="t">Because we are talking about self-attention the V matrix is equal to query matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1330" target="_blank">00:22:10.120</a></span> | <span class="t">So we can say that the output of the self-attention is a matrix that has the same shape as the input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1337" target="_blank">00:22:17.480</a></span> | <span class="t">But where each token now captures some more information about other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1342" target="_blank">00:22:22.480</a></span> | <span class="t">Which tokens depending on the mask we have applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1346" target="_blank">00:22:26.080</a></span> | <span class="t">So our mask says that the first token can only watch itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1350" target="_blank">00:22:30.520</a></span> | <span class="t">So the first output token will be an embedding that will only depend on itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1355" target="_blank">00:22:35.200</a></span> | <span class="t">The second token will only depend on the first two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1359" target="_blank">00:22:39.560</a></span> | <span class="t">The third output token will only depend on the first three tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1364" target="_blank">00:22:44.840</a></span> | <span class="t">The fourth will depend on the token number two because the first token is not used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1369" target="_blank">00:22:49.960</a></span> | <span class="t">The token number two, the token number three and the token number four, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1373" target="_blank">00:22:53.160</a></span> | <span class="t">Until the last here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1375" target="_blank">00:22:55.160</a></span> | <span class="t">The last token will depend only on the last three tokens because the first three tokens are masked out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1381" target="_blank">00:23:01.320</a></span> | <span class="t">And this is the importance of the mask that we apply in the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1385" target="_blank">00:23:05.960</a></span> | <span class="t">This concept that I show you now is very important to understand the rest of the video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1390" target="_blank">00:23:10.240</a></span> | <span class="t">So please if you didn't understand it, you can take a little pause</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1393" target="_blank">00:23:13.000</a></span> | <span class="t">You can try to do it by yourself because it's really important that you understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1397" target="_blank">00:23:17.200</a></span> | <span class="t">How the self-attention mechanism works with the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1399" target="_blank">00:23:19.560</a></span> | <span class="t">Okay, now that we have seen this concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1403" target="_blank">00:23:23.000</a></span> | <span class="t">I want to introduce you to the next one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1405" target="_blank">00:23:25.040</a></span> | <span class="t">So as we saw before the output of the self-attention mechanism is another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1409" target="_blank">00:23:29.360</a></span> | <span class="t">Matrix with the same shape as the query matrix in which each token is represented by an embedding of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1416" target="_blank">00:23:36.240</a></span> | <span class="t">4096 but each embedding now captures information also about other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1422" target="_blank">00:23:42.320</a></span> | <span class="t">According to the mask and if we check the this mask here, so the output here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1428" target="_blank">00:23:48.360</a></span> | <span class="t">We can safely say that the input of our sliding window attention was the initial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1435" target="_blank">00:23:55.600</a></span> | <span class="t">Sequence dcat is on a chair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1438" target="_blank">00:23:58.560</a></span> | <span class="t">But after applying the self-attention the first token is now related to itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1443" target="_blank">00:24:03.640</a></span> | <span class="t">The second token is related to itself and the token before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1447" target="_blank">00:24:07.800</a></span> | <span class="t">The third is related to the token before it and the one also before it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1451" target="_blank">00:24:11.920</a></span> | <span class="t">The last one only depends on the previous two tokens, etc. According to the mask, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1457" target="_blank">00:24:17.040</a></span> | <span class="t">Now what happens if we feed this one because as you know in the transformer world and also in Mistral and also in Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1464" target="_blank">00:24:24.440</a></span> | <span class="t">we have many layers of encoders one after another which are also called the transformer block in the code and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1470" target="_blank">00:24:30.480</a></span> | <span class="t">The output of each layer is fed to the next one. So this is the first layer of the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1477" target="_blank">00:24:37.680</a></span> | <span class="t">So we take the input sequence and we feed it to the first layer which will produce a list of tokens where each token now captures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1484" target="_blank">00:24:44.000</a></span> | <span class="t">Information about other tokens, but this will become the input of the next layer where we it will produce an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1491" target="_blank">00:24:51.680</a></span> | <span class="t">This output I will prove you that will capture information about even more tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1496" target="_blank">00:24:56.640</a></span> | <span class="t">Even if the sliding window attention says that they should only be able to watch the previous two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1502" target="_blank">00:25:02.440</a></span> | <span class="t">Because the sliding window size we chose three as a sliding window size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1507" target="_blank">00:25:07.440</a></span> | <span class="t">I want to prove it. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1509" target="_blank">00:25:09.440</a></span> | <span class="t">Imagine this is the output of the first layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1513" target="_blank">00:25:13.400</a></span> | <span class="t">So it's a list of tokens that capture information about other tokens and it's the the matrix that we built in the previous slide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1521" target="_blank">00:25:21.120</a></span> | <span class="t">Let's use it as an input for another layer of the encoder. So we multiply the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1527" target="_blank">00:25:27.160</a></span> | <span class="t">We multiply the query and the transposed of the keys which will produce a matrix like this one in which each token is not only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1534" target="_blank">00:25:34.920</a></span> | <span class="t">One token, but it's capturing already information about multiple tokens, right according to the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1539" target="_blank">00:25:39.560</a></span> | <span class="t">So I'm taking this one and this one will become query key and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1544" target="_blank">00:25:44.480</a></span> | <span class="t">So if we multiply the query by the key, it will return a matrix like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1548" target="_blank">00:25:48.480</a></span> | <span class="t">So the first token only depends on itself. The second one depends on himself and the previous one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1554" target="_blank">00:25:54.080</a></span> | <span class="t">So the embedding of this token captures information about two tokens and the embedding of this token capture information about three tokens, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1561" target="_blank">00:26:01.080</a></span> | <span class="t">Let's try to do the multiplication again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1564" target="_blank">00:26:04.560</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1566" target="_blank">00:26:06.560</a></span> | <span class="t">We have that our V matrix is again a list of tokens and the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1574" target="_blank">00:26:14.160</a></span> | <span class="t">Will also be a list of tokens, but each one will capture information about other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1581" target="_blank">00:26:21.320</a></span> | <span class="t">Okay, let's analyze the dot product here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1584" target="_blank">00:26:24.840</a></span> | <span class="t">So the first value of the first row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1587" target="_blank">00:26:27.520</a></span> | <span class="t">So the first dimension of the first row of the output matrix will be the dot product of the first row of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1593" target="_blank">00:26:33.960</a></span> | <span class="t">Matrix here. So this row here with the first column of this matrix here. So this column here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1601" target="_blank">00:26:41.320</a></span> | <span class="t">But because of this causal mask with the sliding window attention mask that we can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1607" target="_blank">00:26:47.480</a></span> | <span class="t">It will the output will only depend on the first row of the V matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1612" target="_blank">00:26:52.400</a></span> | <span class="t">But because the V matrix is a matrix that is made of these tokens here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1617" target="_blank">00:26:57.240</a></span> | <span class="t">it will only depend on the word V so as we can see here the output of the second layer only depends on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1623" target="_blank">00:27:03.800</a></span> | <span class="t">word V and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1625" target="_blank">00:27:05.800</a></span> | <span class="t">So will be this second one. So let's check the fourth token. For example here this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1632" target="_blank">00:27:12.400</a></span> | <span class="t">Let's check this fourth token here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1635" target="_blank">00:27:15.800</a></span> | <span class="t">So this value here will be the product of the fourth row of this matrix dot product of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1643" target="_blank">00:27:23.160</a></span> | <span class="t">This row with the first column of the V matrix. So this column here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1648" target="_blank">00:27:28.480</a></span> | <span class="t">But the first token will not be used because it's we are multiplying it with zero whatever value we have here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1654" target="_blank">00:27:34.760</a></span> | <span class="t">We will not be using it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1655" target="_blank">00:27:35.960</a></span> | <span class="t">we are using the second token the third token and the fourth token and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1661" target="_blank">00:27:41.840</a></span> | <span class="t">Each token actually they are aggregating this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1665" target="_blank">00:27:45.400</a></span> | <span class="t">This values here. This token here is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1669" target="_blank">00:27:49.240</a></span> | <span class="t">Aggregating the value of two tokens, which is D and cat. So this embedding here is already about talking about D and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1677" target="_blank">00:27:57.360</a></span> | <span class="t">cat and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1679" target="_blank">00:27:59.960</a></span> | <span class="t">this token here is talking about is aggregating the information about the D cat and is so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1687" target="_blank">00:28:07.720</a></span> | <span class="t">D cat and is and the fourth token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1692" target="_blank">00:28:12.280</a></span> | <span class="t">Aggregating the information of the cat is on so cat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1696" target="_blank">00:28:16.020</a></span> | <span class="t">is and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1698" target="_blank">00:28:18.480</a></span> | <span class="t">On because as we saw before the fourth token here cat is on which is the result of the previous self-attention that we done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1705" target="_blank">00:28:25.940</a></span> | <span class="t">So this output value here will depend on three tokens that already include information about other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1715" target="_blank">00:28:35.080</a></span> | <span class="t">So this value here will aggregate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1717" target="_blank">00:28:37.800</a></span> | <span class="t">Information about the union of all these tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1721" target="_blank">00:28:41.160</a></span> | <span class="t">So it will for sure depend on the word D because it's included in the second token. We are multiplying it with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1726" target="_blank">00:28:46.680</a></span> | <span class="t">It for sure will include information about the word the cat because it's included in this token as well for sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1734" target="_blank">00:28:54.000</a></span> | <span class="t">It will include information about is because it's included in the second value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1737" target="_blank">00:28:57.740</a></span> | <span class="t">We are multiplying it with and for sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1739" target="_blank">00:28:59.640</a></span> | <span class="t">it will include about the token on because it's present in the the fourth token of the V matrix for with which we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1746" target="_blank">00:29:06.620</a></span> | <span class="t">Multiplying it because this value is not zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1749" target="_blank">00:29:09.600</a></span> | <span class="t">So as you can see after applying another layer of the encoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1754" target="_blank">00:29:14.960</a></span> | <span class="t">The fourth token now includes another token in its information before it was only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1760" target="_blank">00:29:20.720</a></span> | <span class="t">including these three tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1763" target="_blank">00:29:23.000</a></span> | <span class="t">but now it also depends on a new token which is the word V and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1766" target="_blank">00:29:26.840</a></span> | <span class="t">We can prove the same for the fifth token and the sixth token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1770" target="_blank">00:29:30.600</a></span> | <span class="t">so at every application of the encoder layer one after another we keep increasing the number of tokens that get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1777" target="_blank">00:29:37.820</a></span> | <span class="t">Accumulated in these dot products and I made a notebook in Python to visualize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1784" target="_blank">00:29:44.720</a></span> | <span class="t">So if you look at my github repository</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1787" target="_blank">00:29:47.440</a></span> | <span class="t">You will see this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1790" target="_blank">00:29:50.880</a></span> | <span class="t">Notebook called the sliding window attention in which I help you visualize this process and I also share the code on how I do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1798" target="_blank">00:29:58.280</a></span> | <span class="t">Self-attention basically I represent each token as a set so each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1803" target="_blank">00:30:03.560</a></span> | <span class="t">Each token instead of being represented as an embedding as a set of all the words upon which that token depends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1810" target="_blank">00:30:10.920</a></span> | <span class="t">depends then I apply the cell sliding window attention, which basically means that I take the two tokens that from the sequence and I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1820" target="_blank">00:30:20.520</a></span> | <span class="t">Accumulate I make the union of the two sets they contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1823" target="_blank">00:30:23.520</a></span> | <span class="t">Because I am multiplying two vectors that already include information about multiple tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1829" target="_blank">00:30:29.440</a></span> | <span class="t">So what is the output is the union of the two sets when I multiply by V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1834" target="_blank">00:30:34.320</a></span> | <span class="t">I do the same thing and I can visualize it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1836" target="_blank">00:30:36.640</a></span> | <span class="t">So after we apply the first layer, we will see that the input of the first layer is just our normal sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1842" target="_blank">00:30:42.160</a></span> | <span class="t">So the cat is on a chair the output of the first layer will be another sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1846" target="_blank">00:30:46.360</a></span> | <span class="t">There in which each position includes information about multiple tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1850" target="_blank">00:30:50.120</a></span> | <span class="t">Depending on the mask that we have applied and I also show the mask that we apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1854" target="_blank">00:30:54.600</a></span> | <span class="t">After we apply the second layer, we can see that the information increases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1859" target="_blank">00:30:59.560</a></span> | <span class="t">So this last token now is not watching only the previous three tokens, but the previous four tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1865" target="_blank">00:31:05.340</a></span> | <span class="t">Sorry, not only the previous two tokens, but the previous four tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1869" target="_blank">00:31:09.820</a></span> | <span class="t">so every step we do with the sliding window size of three we include two tokens at every layer and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1876" target="_blank">00:31:16.660</a></span> | <span class="t">Here I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1879" target="_blank">00:31:19.180</a></span> | <span class="t">Show it for five layers, but it's not necessary because after a while the sequence will reach the maximum length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1885" target="_blank">00:31:25.580</a></span> | <span class="t">If you want you can increase the sequence's length here by including more tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1890" target="_blank">00:31:30.620</a></span> | <span class="t">So this is the concept of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1895" target="_blank">00:31:35.580</a></span> | <span class="t">Receptive field applied to the self window attention. So basically with the sliding window attention, we are not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1901" target="_blank">00:31:41.420</a></span> | <span class="t">Directly connecting two tokens with each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1906" target="_blank">00:31:46.180</a></span> | <span class="t">But if we apply multiple layers after one after another this information will get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1911" target="_blank">00:31:51.860</a></span> | <span class="t">Will get captured by the embedding in successive applications of the layers such that the last layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1918" target="_blank">00:31:58.340</a></span> | <span class="t">basically will be able to watch all the sentence even if it's very long and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1923" target="_blank">00:32:03.500</a></span> | <span class="t">this is actually shown by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1925" target="_blank">00:32:05.500</a></span> | <span class="t">Mistral paper in this picture you can see here. So basically this is our input sequence. So let me write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1932" target="_blank">00:32:12.820</a></span> | <span class="t">So this is our input which is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1937" target="_blank">00:32:17.500</a></span> | <span class="t">The original sentence so the cat is on a chair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1941" target="_blank">00:32:21.340</a></span> | <span class="t">The fourth token of the first layer. So this is the output of the first layer. So layer one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1950" target="_blank">00:32:30.380</a></span> | <span class="t">We have seen that the fourth token here depend with a sliding window size of four this will depend on the itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1957" target="_blank">00:32:37.460</a></span> | <span class="t">On the previous token on the one before and also this token here and it will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1963" target="_blank">00:32:43.180</a></span> | <span class="t">This this embedding here in the fourth position which includes information about the previous token as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1969" target="_blank">00:32:49.600</a></span> | <span class="t">But then this will become the input of the next layer, which is the layer number two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1974" target="_blank">00:32:54.020</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1977" target="_blank">00:32:57.300</a></span> | <span class="t">this will produce an embedding at this position for example that will depend for sure on the previous four tokens because the sliding window size is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1985" target="_blank">00:33:05.100</a></span> | <span class="t">Four but because for example this token here is already the aggregation of the previous four tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1990" target="_blank">00:33:10.860</a></span> | <span class="t">It will actually multiply the visibility of its sliding window</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1994" target="_blank">00:33:14.620</a></span> | <span class="t">So this token here is not related directly to the first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=1999" target="_blank">00:33:19.380</a></span> | <span class="t">We can see here, but indirectly through the this intermediate token. We can see here. I hope this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2007" target="_blank">00:33:27.140</a></span> | <span class="t">I hope this concept is clear. If it's not clear, I try I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2011" target="_blank">00:33:31.340</a></span> | <span class="t">I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2013" target="_blank">00:33:33.340</a></span> | <span class="t">Recommend using my notebook so that you can experiment by playing with the multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2018" target="_blank">00:33:38.220</a></span> | <span class="t">Sequences and you can see how the information flow will go through all the layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2022" target="_blank">00:33:42.700</a></span> | <span class="t">All right, let's talk about our next topic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2027" target="_blank">00:33:47.420</a></span> | <span class="t">Which is the KV cache because I want to introduce the KV cache which I already explained in my previous video on llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2032" target="_blank">00:33:52.620</a></span> | <span class="t">But I want to introduce it again and review it because I want to introduce later the rolling buffer cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2037" target="_blank">00:33:57.140</a></span> | <span class="t">So let's start by talking about first of all how we train language models because this is needed to understand the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2044" target="_blank">00:34:04.540</a></span> | <span class="t">So the language models are trained using what is known as the next token prediction task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2049" target="_blank">00:34:09.660</a></span> | <span class="t">so given a prompt the goal of the language model is to predict what is the next token that makes sense with the prompt that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2055" target="_blank">00:34:15.900</a></span> | <span class="t">We have given and imagine we want to train a language model on Dante Alighieri's poem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2061" target="_blank">00:34:21.820</a></span> | <span class="t">Divine comedy and in particular we will training it on a line that you can see here in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2067" target="_blank">00:34:27.020</a></span> | <span class="t">This one in English. So love that can quickly seize the gentle heart. How does it work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2073" target="_blank">00:34:33.540</a></span> | <span class="t">We prepare an input for our language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2076" target="_blank">00:34:36.580</a></span> | <span class="t">Which is the line that we want to teach it with a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2080" target="_blank">00:34:40.300</a></span> | <span class="t">Prepended called the start of sentence and then we build the target which is the same line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2085" target="_blank">00:34:45.240</a></span> | <span class="t">But with a token at the end called end of sentence. We run the input through this transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2091" target="_blank">00:34:51.260</a></span> | <span class="t">It will produce an output sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2093" target="_blank">00:34:53.260</a></span> | <span class="t">so as we saw before the in the output of the self attention is another sequence with the same length as the input sequence, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2101" target="_blank">00:35:01.460</a></span> | <span class="t">Embedding is modified in such a way that each token capture information about other tokens. And this is what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2109" target="_blank">00:35:09.140</a></span> | <span class="t">To actually train a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2111" target="_blank">00:35:11.120</a></span> | <span class="t">so if we feed the model with with the nine tokens the model will produce nine tokens as output and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2118" target="_blank">00:35:18.060</a></span> | <span class="t">does it work basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2120" target="_blank">00:35:20.660</a></span> | <span class="t">model will learn a mapping between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2122" target="_blank">00:35:22.660</a></span> | <span class="t">Input and output such that if we give to the model as input the token start of sentence only it will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2130" target="_blank">00:35:30.660</a></span> | <span class="t">The first token as output which is the word love if we give to the model as input the first two tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2137" target="_blank">00:35:37.300</a></span> | <span class="t">So start of sentence love the model will produce the two tokens as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2141" target="_blank">00:35:41.960</a></span> | <span class="t">So love that it will feed the model as input three tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2146" target="_blank">00:35:46.020</a></span> | <span class="t">So start of sentence love that the model will produce love that can so when we train the model we train it like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2152" target="_blank">00:35:52.760</a></span> | <span class="t">We prepare the input like this the target like this. We calculated the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2156" target="_blank">00:35:56.500</a></span> | <span class="t">We calculated the loss using the cross entropy loss and then we run back propagation and this is done in all one step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2161" target="_blank">00:36:01.880</a></span> | <span class="t">When we do the inference we do it in multiple step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2164" target="_blank">00:36:04.540</a></span> | <span class="t">So when we do the inference at time step one, we feed the model only the first token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2169" target="_blank">00:36:09.060</a></span> | <span class="t">So the start of sentence and the model will produce the output love</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2172" target="_blank">00:36:12.460</a></span> | <span class="t">Then we take the output the last token of the output and we prepend it to the input which becomes the input as time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2179" target="_blank">00:36:19.620</a></span> | <span class="t">Two so it becomes start of sentence love. So the model will produce love that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2184" target="_blank">00:36:24.300</a></span> | <span class="t">We take the last token of the output and we prepare append it to the input for the time step three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2189" target="_blank">00:36:29.880</a></span> | <span class="t">And this will become the new input which will produce love that can then we take the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2195" target="_blank">00:36:35.060</a></span> | <span class="t">Token of the output and we append it to the input for the time step four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2199" target="_blank">00:36:39.820</a></span> | <span class="t">So it will become the new output will become love that can quickly then we take this word quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2205" target="_blank">00:36:45.060</a></span> | <span class="t">We append it to the input for the next time step and which will produce the next token as output, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2209" target="_blank">00:36:49.980</a></span> | <span class="t">Etc until the last token until we see the end of sentence token as output then we know that the model has stopped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2216" target="_blank">00:36:56.340</a></span> | <span class="t">Has stopped producing new tokens and we can stop the inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2221" target="_blank">00:37:01.740</a></span> | <span class="t">Now at every step the inference we are only interested in the last token output by the model because we already have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2229" target="_blank">00:37:09.740</a></span> | <span class="t">previous one, but of course, we need to feed all the previous tokens to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2234" target="_blank">00:37:14.700</a></span> | <span class="t">to the model which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2237" target="_blank">00:37:17.260</a></span> | <span class="t">Belonging to the prompt because the model needs to access the prompt to understand which token to produce next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2243" target="_blank">00:37:23.500</a></span> | <span class="t">So for example, we cannot produce the word gentle only by giving the word the we need to give all this sentence to produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2251" target="_blank">00:37:31.020</a></span> | <span class="t">this output gentle here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2253" target="_blank">00:37:33.020</a></span> | <span class="t">But at the same time we are only interested in the last word gentle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2258" target="_blank">00:37:38.620</a></span> | <span class="t">And this is the reason we introduce the KVCache because the KVCache allow us to reduce the computations that we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2265" target="_blank">00:37:45.700</a></span> | <span class="t">by only producing one output at a time the one that we need but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2271" target="_blank">00:37:51.020</a></span> | <span class="t">Without doing all the intermediate computations for all the other tokens that we never use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2275" target="_blank">00:37:55.620</a></span> | <span class="t">So basically when we want the word heart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2278" target="_blank">00:37:58.540</a></span> | <span class="t">We don't want to produce the output for the word love that can quickly seize the gentle because we already have them in the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2285" target="_blank">00:38:05.340</a></span> | <span class="t">We don't need to produce all these tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2287" target="_blank">00:38:07.060</a></span> | <span class="t">We just want to produce the output for the token heart. So we want to reduce the computation that we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2291" target="_blank">00:38:11.820</a></span> | <span class="t">Let's see how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2293" target="_blank">00:38:13.940</a></span> | <span class="t">Now in the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2297" target="_blank">00:38:17.020</a></span> | <span class="t">You know that we multiply the query which can be thought of as a list of tokens where each token is an embedding of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2302" target="_blank">00:38:22.820</a></span> | <span class="t">4096 and the transposed of the query becomes the is multiplied the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2309" target="_blank">00:38:29.580</a></span> | <span class="t">Are multiplied by the queries to produce this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2313" target="_blank">00:38:33.140</a></span> | <span class="t">And then we multiply it by the V matrix to produce the output of the self-attention. You can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2318" target="_blank">00:38:38.540</a></span> | <span class="t">Let's do this one token at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2320" target="_blank">00:38:40.900</a></span> | <span class="t">So when we inference a language model, we start with our first token, which is the start of sentence. This is one token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2327" target="_blank">00:38:47.520</a></span> | <span class="t">represented by an embedding of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2330" target="_blank">00:38:50.060</a></span> | <span class="t">4096 we multiplied by the transposed of the keys which is again one token because it's a self-attention. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2337" target="_blank">00:38:57.000</a></span> | <span class="t">The query the key and the value are the same matrix. So this is just the transposed of the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2343" target="_blank">00:39:03.220</a></span> | <span class="t">Basically, and so it's a column vector and it will produce a one by one matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2348" target="_blank">00:39:08.260</a></span> | <span class="t">We multiply it by V and it will produce an output token. We take this output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2352" target="_blank">00:39:12.460</a></span> | <span class="t">We send it to the linear layer and then to the softmax to understand which token this corresponds to in our vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2359" target="_blank">00:39:19.900</a></span> | <span class="t">We take this token from our vocabulary and we append it to the query for the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2365" target="_blank">00:39:25.660</a></span> | <span class="t">Inference step to the keys and the values and then we compute again the product of the query multiplied by the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2372" target="_blank">00:39:32.920</a></span> | <span class="t">We multiply then the result by V and it will produce an output made up of two tokens because we have two tokens as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2379" target="_blank">00:39:39.020</a></span> | <span class="t">It will produce two tokens as output, but we are all interested in the last token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2383" target="_blank">00:39:43.100</a></span> | <span class="t">So we take this output token too, we send it to the linear layer then to the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2387" target="_blank">00:39:47.220</a></span> | <span class="t">This will result in what token is corresponding to in our vocabulary. We take this token from our vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2393" target="_blank">00:39:53.340</a></span> | <span class="t">We append it for the next step to the query key and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2397" target="_blank">00:39:57.060</a></span> | <span class="t">We do again this process and then we take the last token as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2403" target="_blank">00:40:03.380</a></span> | <span class="t">You can see here. We may send it to the linear layer then the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2406" target="_blank">00:40:06.600</a></span> | <span class="t">We understand which token it corresponds to, we append it to our query key and values and then we compute again the self attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2414" target="_blank">00:40:14.140</a></span> | <span class="t">but we already start to notice something because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2417" target="_blank">00:40:17.340</a></span> | <span class="t">First of all, in this matrix here, which is the result of the query multiplied by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2424" target="_blank">00:40:24.540</a></span> | <span class="t">We have a lot of dot products at each step that were already computed at the previous step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2429" target="_blank">00:40:29.680</a></span> | <span class="t">Let me show you. At the time step 4, we are computing all these dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2434" target="_blank">00:40:34.340</a></span> | <span class="t">As you can see at the time step 3, we already computed these dot products and at the time step 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2440" target="_blank">00:40:40.220</a></span> | <span class="t">We are computing them again as you can see these dot products here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2445" target="_blank">00:40:45.300</a></span> | <span class="t">The second thing is that usually when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2449" target="_blank">00:40:49.380</a></span> | <span class="t">Deal with the language model, we have a causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2452" target="_blank">00:40:52.860</a></span> | <span class="t">So we do not even care about computing the dot products that we see here in the dark violet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2458" target="_blank">00:40:58.520</a></span> | <span class="t">because they will be anyway masked out by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2461" target="_blank">00:41:01.220</a></span> | <span class="t">causal mask that we apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2463" target="_blank">00:41:03.940</a></span> | <span class="t">Because we don't want the first token to watch the token number 2, the token number 3, the token number 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2469" target="_blank">00:41:09.460</a></span> | <span class="t">We only want the token number 4 to watch the previous one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2472" target="_blank">00:41:12.580</a></span> | <span class="t">So the token number 4 should be related to itself, the previous one, the token number 2 and the token number 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2477" target="_blank">00:41:17.100</a></span> | <span class="t">But not the opposite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2479" target="_blank">00:41:19.100</a></span> | <span class="t">And also we don't want to produce all these output tokens because we are only interested in the last one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2484" target="_blank">00:41:24.460</a></span> | <span class="t">We are only interested in knowing what is the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2486" target="_blank">00:41:26.740</a></span> | <span class="t">token produced by the attention so that we can send it to the linear layer and then to the softmax to understand what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2493" target="_blank">00:41:33.860</a></span> | <span class="t">Word corresponding in our vocabulary so that we can use it for the prompt to inference the next token again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2499" target="_blank">00:41:39.900</a></span> | <span class="t">So now let's introduce the KVCache and how the KVCache solve this problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2504" target="_blank">00:41:44.400</a></span> | <span class="t">What we do with the KVCache, again, we start from our first step of the inferences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2510" target="_blank">00:41:50.020</a></span> | <span class="t">So we start from our start of sentence token, which is multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2513" target="_blank">00:41:53.380</a></span> | <span class="t">So the query is only the start of sentence token. We multiply it by the transpose of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2517" target="_blank">00:41:57.800</a></span> | <span class="t">This will produce a 1 by 1 matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2519" target="_blank">00:41:59.940</a></span> | <span class="t">Then we multiply it by divi and it will produce our first token as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2524" target="_blank">00:42:04.260</a></span> | <span class="t">We send it to the linear layer then to the softmax then we know which token it corresponds to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2529" target="_blank">00:42:09.040</a></span> | <span class="t">Now in the KVCache instead of appending this new token that we have produced as output to the query key and value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2536" target="_blank">00:42:16.100</a></span> | <span class="t">we only append it to the key and the value and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2539" target="_blank">00:42:19.140</a></span> | <span class="t">Replace entirely the previous query with this new token. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2543" target="_blank">00:42:23.500</a></span> | <span class="t">Before without the KVCache we were appending the every output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2548" target="_blank">00:42:28.900</a></span> | <span class="t">So the last token of the output to the query key and values, but in with the KVCache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2554" target="_blank">00:42:34.420</a></span> | <span class="t">We don't append it to query key and value, but only to the key and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2558" target="_blank">00:42:38.960</a></span> | <span class="t">And we only use the last output token as query for the next step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2563" target="_blank">00:42:43.980</a></span> | <span class="t">So if this is the output of the first step, so the output corresponding to the token start of sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2571" target="_blank">00:42:51.060</a></span> | <span class="t">We take it we use it as query for the next step, but we append it to the key and the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2577" target="_blank">00:42:57.800</a></span> | <span class="t">So this is why it's called the KVCache because at each step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2581" target="_blank">00:43:01.600</a></span> | <span class="t">We are keeping a cache of the previous K and V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2585" target="_blank">00:43:05.200</a></span> | <span class="t">But not for the query because we are entirely replacing all the queries with the last token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2589" target="_blank">00:43:09.960</a></span> | <span class="t">Anyway, this will produce a product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2593" target="_blank">00:43:13.640</a></span> | <span class="t">So this matrix multiplied by this matrix will produce a matrix that is 1 by 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2597" target="_blank">00:43:17.560</a></span> | <span class="t">We multiply it by V and we will see that this produces only one token as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2602" target="_blank">00:43:22.440</a></span> | <span class="t">Then this we take this token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2604" target="_blank">00:43:24.400</a></span> | <span class="t">we send it to the linear layer to the softmax then we know which token it corresponds to then we use it as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2609" target="_blank">00:43:29.500</a></span> | <span class="t">Query for the next iteration, but we append it to the only the K and the V matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2615" target="_blank">00:43:35.360</a></span> | <span class="t">This will produce a 1 by 3 matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2617" target="_blank">00:43:37.720</a></span> | <span class="t">Which is then multiplied by the V which will produce the this output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2624" target="_blank">00:43:44.140</a></span> | <span class="t">This is the one we are interested in basically, then we use it as query for the next iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2629" target="_blank">00:43:49.680</a></span> | <span class="t">But we append it to the K and the V etc. So as you can see at the fourth step of the inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2636" target="_blank">00:43:56.160</a></span> | <span class="t">We are producing only the last row that we were interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2640" target="_blank">00:44:00.120</a></span> | <span class="t">When we didn't have the KVCache. So let me show you this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2644" target="_blank">00:44:04.200</a></span> | <span class="t">Fourth time step with the KVCache. Let's look at the fourth time step without the KVCache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2649" target="_blank">00:44:09.920</a></span> | <span class="t">As you can see we are only producing this row here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2655" target="_blank">00:44:15.000</a></span> | <span class="t">This is the only one we are interested in to produce this last token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2658" target="_blank">00:44:18.920</a></span> | <span class="t">So with the KVCache basically we reduce the number of computations that we are doing at every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2663" target="_blank">00:44:23.640</a></span> | <span class="t">Because the sum of the dot products we have already done in the previous steps and we only produce one token as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2669" target="_blank">00:44:29.820</a></span> | <span class="t">Which is exactly the one that we need for predicting the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2673" target="_blank">00:44:33.920</a></span> | <span class="t">Ok, now let's talk about the rolling buffer cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2678" target="_blank">00:44:38.200</a></span> | <span class="t">So since we are using the sliding window attention with a size of W</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2682" target="_blank">00:44:42.520</a></span> | <span class="t">And in the examples I showed you before I was using a sliding window size with the size of 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2688" target="_blank">00:44:48.040</a></span> | <span class="t">We don't need to keep all the possible K and V in the cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2693" target="_blank">00:44:53.120</a></span> | <span class="t">But we can limit the K and the V only to W tokens because anyway, we will not be computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2700" target="_blank">00:45:00.600</a></span> | <span class="t">Attention outside of this W window. So we do not need imagine our window is 10 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2706" target="_blank">00:45:06.000</a></span> | <span class="t">We do not keep the previous 1000 tokens because anyway, our attention will only be calculated on the previous 10 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2712" target="_blank">00:45:12.200</a></span> | <span class="t">So this is the idea behind the rolling buffer cache. Let's see how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2715" target="_blank">00:45:15.600</a></span> | <span class="t">Imagine we arrive at the token 8 of inference using the KVCache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2721" target="_blank">00:45:21.360</a></span> | <span class="t">If we have a KVCache and we are using the sliding window size of 4 for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2727" target="_blank">00:45:27.760</a></span> | <span class="t">We will see that as query we will use the output of the previous step and as key and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2734" target="_blank">00:45:34.760</a></span> | <span class="t">We will use the entire cache which is made up of 8 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2737" target="_blank">00:45:37.920</a></span> | <span class="t">But because of the mask that we are using with the sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2743" target="_blank">00:45:43.760</a></span> | <span class="t">We are not interested in the computation of these dot products because anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2748" target="_blank">00:45:48.240</a></span> | <span class="t">They will be masked out because the distance between this token and this token is outside of the sliding window attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2754" target="_blank">00:45:54.320</a></span> | <span class="t">so we are not interested in this calculating these dot products because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2758" target="_blank">00:45:58.600</a></span> | <span class="t">They will be masked out and secondly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2762" target="_blank">00:46:02.400</a></span> | <span class="t">We are not interested in keeping this one because anyway because these values will be masked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2768" target="_blank">00:46:08.000</a></span> | <span class="t">By our mask for the sliding window attention, which basically will result in zeros here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2773" target="_blank">00:46:13.920</a></span> | <span class="t">We do not care about producing these first four rows in the value matrix because anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2780" target="_blank">00:46:20.960</a></span> | <span class="t">They will be multiplied by zeros. So they will not contribute to the output token. So here you have to imagine that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2786" target="_blank">00:46:26.680</a></span> | <span class="t">Let me draw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2789" target="_blank">00:46:29.360</a></span> | <span class="t">Here you have to imagine that the mask will take care of making this one zero, this one zero, this one zero, this one zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2797" target="_blank">00:46:37.000</a></span> | <span class="t">And this one will be a dot product. This one will be a dot product. This one will be a dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2801" target="_blank">00:46:41.560</a></span> | <span class="t">This one will be a dot product. So whatever value there is here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2804" target="_blank">00:46:44.560</a></span> | <span class="t">Whatever value there is here, here or here will not contribute to the output of this token because anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2810" target="_blank">00:46:50.640</a></span> | <span class="t">They will be multiplied by zeros here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2812" target="_blank">00:46:52.320</a></span> | <span class="t">So we do not need to keep this value also in the V matrix or in the K matrix because anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2818" target="_blank">00:46:58.720</a></span> | <span class="t">They will not be used by the sliding window attention. So that's why we can limit the size of our K and V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2825" target="_blank">00:47:05.380</a></span> | <span class="t">Cache only to W tokens where W is the size of the sliding window attention that we are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2832" target="_blank">00:47:12.520</a></span> | <span class="t">Now let's see how this rolling buffer cache was implemented. So basically rolling buffer cache is a way of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2837" target="_blank">00:47:17.880</a></span> | <span class="t">Limiting the size of a cache to a limited size in this case W</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2844" target="_blank">00:47:24.040</a></span> | <span class="t">So imagine our W is only four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2846" target="_blank">00:47:26.040</a></span> | <span class="t">Imagine we have a sentence "the cat is on a chair" and we want to use it for our KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2852" target="_blank">00:47:32.720</a></span> | <span class="t">At the first inference using the KV cache, we will add the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2857" target="_blank">00:47:37.840</a></span> | <span class="t">The first token to the KV cache, then we will add the second one the third one and the fourth one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2863" target="_blank">00:47:43.600</a></span> | <span class="t">But now the KV cache is full. How do we proceed further?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2866" target="_blank">00:47:46.720</a></span> | <span class="t">Basically, we keep track of where we added the last item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2870" target="_blank">00:47:50.480</a></span> | <span class="t">Using a pointer that we keep track of and when we will arrive at the next token, which is the token A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2876" target="_blank">00:47:56.800</a></span> | <span class="t">We basically replace the oldest value here starting from the beginning and we update the value of the right pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2883" target="_blank">00:48:03.880</a></span> | <span class="t">but now how do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2885" target="_blank">00:48:05.740</a></span> | <span class="t">Go back because now the order of the tokens is not matching the sentence because as you can see now the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2891" target="_blank">00:48:11.200</a></span> | <span class="t">Cache contains "a cat is on" but this is not the order in the original sentence in the original sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2897" target="_blank">00:48:17.320</a></span> | <span class="t">the order should be "cat is on a" so what we do is we do the unrolling or unrotation and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2903" target="_blank">00:48:23.960</a></span> | <span class="t">How do we do it? Basically because we kept track of this right pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2908" target="_blank">00:48:28.960</a></span> | <span class="t">We just need to take all the values after the right pointer and then we put the values from 0 to the right pointer itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2916" target="_blank">00:48:36.120</a></span> | <span class="t">So all the values after the right pointer and then all the values before the right pointer and this is how we unrotate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2922" target="_blank">00:48:42.180</a></span> | <span class="t">And this operation is done in the code in the function called unrotate. You can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2927" target="_blank">00:48:47.320</a></span> | <span class="t">Which basically will have this condition. So if the cache is not full we can just ignore the unfilled item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2934" target="_blank">00:48:54.480</a></span> | <span class="t">So if the cache is in this situation, then we take all the values from the 0 up to the right pointer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2940" target="_blank">00:49:00.340</a></span> | <span class="t">if the cache is a full then we take the value from 0 up to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2944" target="_blank">00:49:04.640</a></span> | <span class="t">The value of the right pointer and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2948" target="_blank">00:49:08.560</a></span> | <span class="t">if the value of the right pointer is already overwriting some value then we need to unrotate and this is done in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2956" target="_blank">00:49:16.140</a></span> | <span class="t">Third condition here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2958" target="_blank">00:49:18.160</a></span> | <span class="t">So we take all the values after the pointer and then the value up to the pointer and this is how we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2963" target="_blank">00:49:23.440</a></span> | <span class="t">unrotate this buffer cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2966" target="_blank">00:49:26.000</a></span> | <span class="t">Okay, let's talk about another concept that is very important, which is chunking and pre-feeding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2971" target="_blank">00:49:31.860</a></span> | <span class="t">Basically when we generate a text using a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2975" target="_blank">00:49:35.120</a></span> | <span class="t">We use a prompt and then we use this prompt to generate future tokens when dealing with a KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2980" target="_blank">00:49:40.800</a></span> | <span class="t">We need to build up this KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2982" target="_blank">00:49:42.640</a></span> | <span class="t">So we need to add the tokens of our prompt to the KV cache that so that we can then exploit this KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2988" target="_blank">00:49:48.560</a></span> | <span class="t">To build new tokens future tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2991" target="_blank">00:49:51.280</a></span> | <span class="t">Now the prompt is known in advance, right? Because it's the input of our user</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=2997" target="_blank">00:49:57.040</a></span> | <span class="t">It's what you ask to chatgpd for example, right? Tell me a poem. Tell me write me a poem or tell me a joke</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3002" target="_blank">00:50:02.320</a></span> | <span class="t">This is our prompt. So it's known in advance. So we don't know we don't need to generate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3006" target="_blank">00:50:06.960</a></span> | <span class="t">Okay, so what we can do is we can pre-fill the KV cache using the tokens of the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3012" target="_blank">00:50:12.320</a></span> | <span class="t">But there are many ways to do it like we were doing before when I was teaching you about the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3017" target="_blank">00:50:17.600</a></span> | <span class="t">We work with one token at a time. So one way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3020" target="_blank">00:50:20.160</a></span> | <span class="t">To add the tokens to the KV cache is to add one token at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3025" target="_blank">00:50:25.200</a></span> | <span class="t">But this can be very time consuming because imagine you have a very large prompt which happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3030" target="_blank">00:50:30.080</a></span> | <span class="t">With retrieval augmented generation, which we have very big prompts like 5,000 6,000 tokens or even bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3036" target="_blank">00:50:36.080</a></span> | <span class="t">So this if we add one token at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3038" target="_blank">00:50:38.400</a></span> | <span class="t">It will mean that we have to take 5,000 or 6,000 forward steps in our network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3043" target="_blank">00:50:43.600</a></span> | <span class="t">Which is can be very time consuming and also doesn't exploit our gpu very much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3048" target="_blank">00:50:48.400</a></span> | <span class="t">The other way is to take all these tokens and feed them all at once to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3053" target="_blank">00:50:53.760</a></span> | <span class="t">But that may be limited by the size of our gpu because imagine we have 10,000 tokens as our prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3060" target="_blank">00:51:00.080</a></span> | <span class="t">Then maybe our gpu cannot even hold 10,000 tokens. Maybe it can only hold 4,000 tokens or 2,000 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3066" target="_blank">00:51:06.240</a></span> | <span class="t">Depending also on the w size of the attention sliding window attention that we have chosen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3071" target="_blank">00:51:11.120</a></span> | <span class="t">The solution in this case is to use chunking. Basically, we divide our prompt into chunks of a fixed size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3078" target="_blank">00:51:18.160</a></span> | <span class="t">And this size is equal to w which is the sliding window attention size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3084" target="_blank">00:51:24.240</a></span> | <span class="t">So imagine we have a very big prompt and we choose a sliding window size of 4 for the calculation of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3089" target="_blank">00:51:29.920</a></span> | <span class="t">And imagine that the prompt is this one. So can you tell me?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3093" target="_blank">00:51:33.280</a></span> | <span class="t">Can you tell me who is the richest man in history? The way we work is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3098" target="_blank">00:51:38.880</a></span> | <span class="t">Basically, we take our first chunk of the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3103" target="_blank">00:51:43.200</a></span> | <span class="t">So because we chose a sliding window size of 4, we also will choose the chunk size to be 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3108" target="_blank">00:51:48.320</a></span> | <span class="t">So we take our first token of the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3111" target="_blank">00:51:51.280</a></span> | <span class="t">So can you tell me and we compute the self attention in the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3116" target="_blank">00:51:56.720</a></span> | <span class="t">Self attention in the first layer of the model. How do we build the attention mask?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3122" target="_blank">00:52:02.320</a></span> | <span class="t">Basically as queries we take all the incoming tokens in this chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3128" target="_blank">00:52:08.400</a></span> | <span class="t">So as this is you can think of this column as the queries and this column as the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3134" target="_blank">00:52:14.240</a></span> | <span class="t">And this is the result of the query multiplied by the transposed of the keys plus the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3140" target="_blank">00:52:20.320</a></span> | <span class="t">So our query we take the first incoming chunk and as keys I will show you later</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3145" target="_blank">00:52:25.200</a></span> | <span class="t">We take the current content of the kvcache, but initially it is empty plus the incoming tokens of the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3153" target="_blank">00:52:33.200</a></span> | <span class="t">And this is made for a very specific reason that I will show you in the next step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3160" target="_blank">00:52:40.080</a></span> | <span class="t">So in the next step, basically we take the current chunk, which is the tokens who is the richest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3168" target="_blank">00:52:48.800</a></span> | <span class="t">And we aggregate it with the content of the kvcache using the tokens of the previous chunk. So let me go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3175" target="_blank">00:52:55.520</a></span> | <span class="t">At the first step of this prefilling we take the first chunk of the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3180" target="_blank">00:53:00.880</a></span> | <span class="t">So can you tell me we calculated the attention mask using as query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3184" target="_blank">00:53:04.480</a></span> | <span class="t">The first four tokens and as keys and values the content of the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3191" target="_blank">00:53:11.120</a></span> | <span class="t">Which is empty plus the tokens of the first chunk and then we update the content of the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3197" target="_blank">00:53:17.680</a></span> | <span class="t">Using this the tokens of this chunk after we have computed the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3201" target="_blank">00:53:21.840</a></span> | <span class="t">So at the next step the kvcache now contains the previous the tokens of the previous chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3207" target="_blank">00:53:27.760</a></span> | <span class="t">So can you tell me but now the current chunk has become who is the richest?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3212" target="_blank">00:53:32.320</a></span> | <span class="t">so as query again, we take the tokens of the current chunk, but as keys and values we take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3220" target="_blank">00:53:40.000</a></span> | <span class="t">The content of the kvcache plus the tokens of the current chunk. Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3226" target="_blank">00:53:46.800</a></span> | <span class="t">because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3228" target="_blank">00:53:48.080</a></span> | <span class="t">As you can see when we were doing token generation when I was teaching you the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3231" target="_blank">00:53:51.620</a></span> | <span class="t">We first add the last output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3234" target="_blank">00:53:54.960</a></span> | <span class="t">We add it to append it to the k and the v and we use it as the query for the next iteration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3240" target="_blank">00:54:00.560</a></span> | <span class="t">This is not what we do here. Here. We first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3243" target="_blank">00:54:03.120</a></span> | <span class="t">Calculated the attention and then we update the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3246" target="_blank">00:54:06.660</a></span> | <span class="t">And when we use the when we build the query the query we use only the tokens of the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3253" target="_blank">00:54:13.280</a></span> | <span class="t">And as key and values we take the content of the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3256" target="_blank">00:54:16.580</a></span> | <span class="t">So the content of the previous chunk plus the tokens of the current chunk. Why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3263" target="_blank">00:54:23.120</a></span> | <span class="t">because imagine if we didn't do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3266" target="_blank">00:54:26.240</a></span> | <span class="t">We didn't use the content of the previous chunk. What would happen is this we would have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3272" target="_blank">00:54:32.160</a></span> | <span class="t">Attention mask that is only comprised of the tokens of the current chunk. So it would be only limited to this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3279" target="_blank">00:54:39.760</a></span> | <span class="t">Let me draw it. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3282" target="_blank">00:54:42.640</a></span> | <span class="t">Only this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3284" target="_blank">00:54:44.800</a></span> | <span class="t">But if we only use this matrix here the word who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3290" target="_blank">00:54:50.240</a></span> | <span class="t">Would not be able to to would not be related to the word me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3294" target="_blank">00:54:54.480</a></span> | <span class="t">Tell and you even if with the sliding window size, they should be able to watch each other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3300" target="_blank">00:55:00.640</a></span> | <span class="t">So because we want to relate the current chunk to the previous chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3305" target="_blank">00:55:05.680</a></span> | <span class="t">We basically take as a key and value the content of the kvcache plus the tokens of the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3312" target="_blank">00:55:12.960</a></span> | <span class="t">So that we can build this attention between chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3316" target="_blank">00:55:16.320</a></span> | <span class="t">Otherwise this attention would not be built and as query we always use the tokens of the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3323" target="_blank">00:55:23.120</a></span> | <span class="t">Let's review how this mechanism is built in the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3328" target="_blank">00:55:28.800</a></span> | <span class="t">So basically the pre-filling is done by chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3332" target="_blank">00:55:32.560</a></span> | <span class="t">There is the first chunk and then there are subsequent chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3335" target="_blank">00:55:35.600</a></span> | <span class="t">And finally there is token generation after we have a pre-filled our kvcache with the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3340" target="_blank">00:55:40.320</a></span> | <span class="t">During the first pre-fill, which means that we are doing it for the first chunk of our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3346" target="_blank">00:55:46.080</a></span> | <span class="t">prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3348" target="_blank">00:55:48.080</a></span> | <span class="t">As attention mask, we only consider the size of the incoming tokens in the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3354" target="_blank">00:55:54.960</a></span> | <span class="t">But for any subsequent chunks, so after the first chunk as to build the attention mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3361" target="_blank">00:56:01.440</a></span> | <span class="t">For the query we just use the size of the incoming chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3365" target="_blank">00:56:05.920</a></span> | <span class="t">But for the k and v we use the size of the kvcache, which is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3370" target="_blank">00:56:10.560</a></span> | <span class="t">So cached as you can see here plus the size of the current chunk, which is this s variable you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3377" target="_blank">00:56:17.120</a></span> | <span class="t">And for token generation, we do the same system that we did before when I was teaching with the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3383" target="_blank">00:56:23.360</a></span> | <span class="t">So one token at a time, we take it, we append it to the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3387" target="_blank">00:56:27.600</a></span> | <span class="t">We append it to the value and we replace the query with the output token from the previous step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3393" target="_blank">00:56:33.760</a></span> | <span class="t">So the last chunk in our case will be the tokens man in history</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3398" target="_blank">00:56:38.900</a></span> | <span class="t">And what we do is basically we take the current chunk, so man in history, which becomes the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3405" target="_blank">00:56:45.520</a></span> | <span class="t">While the key becomes basically the previous chunk plus the tokens of the current chunk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3411" target="_blank">00:56:51.280</a></span> | <span class="t">So the who is the richest plus the tokens of the current chunk, so man in history</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3416" target="_blank">00:56:56.000</a></span> | <span class="t">And the reason we do it because otherwise the word in the current chunk will not be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3420" target="_blank">00:57:00.320</a></span> | <span class="t">Be related to the word of the previous chunk, which is necessary. Okay guys, let's talk about sparse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3426" target="_blank">00:57:06.240</a></span> | <span class="t">mixture of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3428" target="_blank">00:57:08.960</a></span> | <span class="t">So mixture of experts is an assembled technique in which we have multiple expert model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3434" target="_blank">00:57:14.400</a></span> | <span class="t">Which each of this model is trained on a subset of the data such that each model will specialize on a subset of this data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3441" target="_blank">00:57:21.760</a></span> | <span class="t">And then when we produce the output of this mixture of experts, we take the output for each of these experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3448" target="_blank">00:57:28.880</a></span> | <span class="t">We combine it usually by using a weighted sum or by averaging to produce one single output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3454" target="_blank">00:57:34.960</a></span> | <span class="t">In the case of Mistral, we do not talk about only mixture of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3458" target="_blank">00:57:38.960</a></span> | <span class="t">But we talk about a sparse mixture of experts because we have many expert models, but we only use some of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3465" target="_blank">00:57:45.280</a></span> | <span class="t">Let me show you. In the case of Mistral, we have eight experts which are present as the feed-forward layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3472" target="_blank">00:57:52.080</a></span> | <span class="t">So after we calculate the self-attention, as you remember, we have this feed-forward network. In the case of Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3478" target="_blank">00:57:58.000</a></span> | <span class="t">8x7b, we have eight feed-forward layers. We have to think of them in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3485" target="_blank">00:58:05.040</a></span> | <span class="t">And the gate is a function that basically will decide for each token which expert, so which feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3491" target="_blank">00:58:11.920</a></span> | <span class="t">network, should be working with that token and it will choose two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3496" target="_blank">00:58:16.400</a></span> | <span class="t">feed-forward networks for each token. It will run the token through these feed-forward networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3502" target="_blank">00:58:22.100</a></span> | <span class="t">will take their output and will weight it according to the logits this gate produces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3508" target="_blank">00:58:28.020</a></span> | <span class="t">to produce a weighted sum, which will become the output of the self-attention for that particular token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3515" target="_blank">00:58:35.520</a></span> | <span class="t">Let me show you with an example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3517" target="_blank">00:58:37.280</a></span> | <span class="t">So this is the architecture of Mistral. As you can see, we have the input of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3522" target="_blank">00:58:42.160</a></span> | <span class="t">encoder layer. We first run the self-attention using the sliding window attention and the KV cache, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3528" target="_blank">00:58:48.800</a></span> | <span class="t">Then we run the normalization and finally we have this gate function here, which is basically just a linear layer that will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3536" target="_blank">00:58:56.720</a></span> | <span class="t">logits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3538" target="_blank">00:58:58.400</a></span> | <span class="t">eight logits, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3541" target="_blank">00:59:01.040</a></span> | <span class="t">will be values. Let's call them score values for our expert. The two best performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3547" target="_blank">00:59:07.220</a></span> | <span class="t">experts, so the two highest score, will indicate which experts that token should work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3553" target="_blank">00:59:13.040</a></span> | <span class="t">Then we run each token in their own two best performing experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3558" target="_blank">00:59:18.560</a></span> | <span class="t">Then we take the output of these two experts. We combine it with the weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3562" target="_blank">00:59:22.720</a></span> | <span class="t">What is the weight? Basically the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3566" target="_blank">00:59:26.400</a></span> | <span class="t">logits produced by the gate are, suppose, eight values here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3570" target="_blank">00:59:30.080</a></span> | <span class="t">Yeah, I draw only four because I don't have space, but you imagine you have eight values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3574" target="_blank">00:59:34.080</a></span> | <span class="t">Then we take the top two, so 1.5 and 3.4 in this case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3579" target="_blank">00:59:39.600</a></span> | <span class="t">These are the two experts through which we will run the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3582" target="_blank">00:59:42.240</a></span> | <span class="t">We take the softmax of the two best performing values. This will be the weight that we'll be using for the weighted sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3589" target="_blank">00:59:49.920</a></span> | <span class="t">And basically, why do we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3594" target="_blank">00:59:54.880</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3596" target="_blank">00:59:56.880</a></span> | <span class="t">Why do we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3599" target="_blank">00:59:59.360</a></span> | <span class="t">Because by using a sparse mixture of experts, we can have many expert models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3604" target="_blank">01:00:04.640</a></span> | <span class="t">But during inferencing only two out of eight will be activated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3608" target="_blank">01:00:08.900</a></span> | <span class="t">So as you remember the feed-for-wall network is basically two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3613" target="_blank">01:00:13.040</a></span> | <span class="t">linear layers. So the linear layer can be thought of as a matrix multiplication of a weight matrix with the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3620" target="_blank">01:00:20.560</a></span> | <span class="t">So if we didn't use a sparse mixture of experts, we would run the token through all the eight experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3625" target="_blank">01:00:25.840</a></span> | <span class="t">Which means that we need to compute eight matrix multiplications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3629" target="_blank">01:00:29.060</a></span> | <span class="t">But by using sparse mixture of experts for each token, we are only doing two matrix multiplications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3635" target="_blank">01:00:35.460</a></span> | <span class="t">Which makes the inference faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3637" target="_blank">01:00:37.920</a></span> | <span class="t">But at the same time allows us to increase the power of the model and the parameter of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3642" target="_blank">01:00:42.240</a></span> | <span class="t">Because we are only using some parameters for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3646" target="_blank">01:00:46.960</a></span> | <span class="t">A subset of the token. So some tokens will use the expert number one. Some tokens will be using the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3651" target="_blank">01:00:51.520</a></span> | <span class="t">The expert number two and three. Some tokens will be using the expert number eight and three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3656" target="_blank">01:00:56.880</a></span> | <span class="t">Or some other for example, the six and the four etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3660" target="_blank">01:01:00.560</a></span> | <span class="t">So we are not using all the experts for each token, but only two of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3664" target="_blank">01:01:04.880</a></span> | <span class="t">This allows us to have each expert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3668" target="_blank">01:01:08.720</a></span> | <span class="t">Specialized on a subset of tokens. For example, imagine the model has been trained on multiple languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3675" target="_blank">01:01:15.280</a></span> | <span class="t">What could happen is that basically some experts, so some feed-forward networks are specialized on Japanese tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3682" target="_blank">01:01:22.320</a></span> | <span class="t">Some feed-forward networks are specialized on English tokens or some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3686" target="_blank">01:01:26.320</a></span> | <span class="t">It could also happen that some are specialized in verbs, some are specialized in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3690" target="_blank">01:01:30.720</a></span> | <span class="t">Nouns, some are specialized in adjectives, etc, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3694" target="_blank">01:01:34.960</a></span> | <span class="t">So this is why we use a mixture of experts because we want to increase the size of the parameters of our model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3702" target="_blank">01:01:42.000</a></span> | <span class="t">So the model becomes more powerful at capturing information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3705" target="_blank">01:01:45.600</a></span> | <span class="t">But at the same time we don't sacrifice on performance because we only use a subset of the experts for each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3713" target="_blank">01:01:53.360</a></span> | <span class="t">And this is the implementation as done in the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3717" target="_blank">01:01:57.120</a></span> | <span class="t">So as you can see in the case of Mistral 7b, we have as feed-forward just a feed-forward neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3723" target="_blank">01:02:03.520</a></span> | <span class="t">Which is two linear layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3725" target="_blank">01:02:05.520</a></span> | <span class="t">In the case of Mistral 8x7b, it's not only one feed-forward network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3730" target="_blank">01:02:10.160</a></span> | <span class="t">But it's eight feed-forward networks. So this as you can see, it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3733" target="_blank">01:02:13.520</a></span> | <span class="t">It's an array of eight feed-forward networks with a gating function, which is just a linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3739" target="_blank">01:02:19.840</a></span> | <span class="t">Which converts from the embedding size to eight, which is the number of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3744" target="_blank">01:02:24.820</a></span> | <span class="t">So it produces for each embedding. So for each token, it produces logits which indicates for which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3753" target="_blank">01:02:33.520</a></span> | <span class="t">Expert this token should run through and it will run through them to the top two experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3759" target="_blank">01:02:39.920</a></span> | <span class="t">So the two experts with the top logits score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3763" target="_blank">01:02:43.520</a></span> | <span class="t">Okay, why we apply the softmax after selecting the top k expert so as I show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3771" target="_blank">01:02:51.600</a></span> | <span class="t">Here we have the gating function that produces some logits. We select the top two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3777" target="_blank">01:02:57.920</a></span> | <span class="t">logits to understand which expert we should run through our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3782" target="_blank">01:03:02.800</a></span> | <span class="t">Token and then we take the score of the best two performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3787" target="_blank">01:03:07.060</a></span> | <span class="t">experts and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3789" target="_blank">01:03:09.820</a></span> | <span class="t">Take the softmax of them to create the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3792" target="_blank">01:03:12.960</a></span> | <span class="t">That we will use to create the weighted sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3795" target="_blank">01:03:15.600</a></span> | <span class="t">But why we take the softmax of the two best performing instead of taking the softmax of everyone? Well, the first problem is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3803" target="_blank">01:03:23.280</a></span> | <span class="t">If we take the softmax of all of the logits, then the two best performing may not sum up to one which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3811" target="_blank">01:03:31.440</a></span> | <span class="t">um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3812" target="_blank">01:03:32.640</a></span> | <span class="t">Which is a condition that we need in case we want to train multiple models and compare them because i'm pretty sure that the guys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3818" target="_blank">01:03:38.640</a></span> | <span class="t">At Mistral did not only train one model. Maybe they trained multiple models with multiple hyper parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3823" target="_blank">01:03:43.760</a></span> | <span class="t">Maybe they tried with four mixture of four experts, but also with three experts or two experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3828" target="_blank">01:03:48.960</a></span> | <span class="t">Then they choose the best one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3830" target="_blank">01:03:50.640</a></span> | <span class="t">So if you want to compare models, you want the weighted sum to always perform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3836" target="_blank">01:03:56.560</a></span> | <span class="t">The sum of the weights to be only one. Otherwise the output range may change from model to model and usually it's not a good idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3843" target="_blank">01:04:03.200</a></span> | <span class="t">To have the range of the output to change from one model to the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3847" target="_blank">01:04:07.520</a></span> | <span class="t">So to keep the range of the output stable, they apply the softmax after they have selected how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3854" target="_blank">01:04:14.000</a></span> | <span class="t">Experts they want to work with and choosing the logits of the best two performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3858" target="_blank">01:04:18.640</a></span> | <span class="t">experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3861" target="_blank">01:04:21.420</a></span> | <span class="t">Okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3863" target="_blank">01:04:23.680</a></span> | <span class="t">The next thing we are talking about is model sharding which is also implemented in the code of the Mistral model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3870" target="_blank">01:04:30.640</a></span> | <span class="t">So let's talk about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3872" target="_blank">01:04:32.720</a></span> | <span class="t">When we have a model that is too big to fit in a single gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3876" target="_blank">01:04:36.560</a></span> | <span class="t">We can divide the model into groups of layers and place each group of layers in a single gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3882" target="_blank">01:04:42.800</a></span> | <span class="t">For example in the case of Mistral, we have 32 layers of encoders</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3887" target="_blank">01:04:47.280</a></span> | <span class="t">You can see here one after another I didn't do all 32 of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3891" target="_blank">01:04:51.520</a></span> | <span class="t">You just think that this is layer from 1 to 8. This is from 9 to 16 from 17 to 24</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3896" target="_blank">01:04:56.800</a></span> | <span class="t">From 25 to 32 and we put each group of layers in a different gpu. So we have four gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3903" target="_blank">01:05:03.200</a></span> | <span class="t">the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3905" target="_blank">01:05:05.280</a></span> | <span class="t">The way we inference a model like this is as follows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3908" target="_blank">01:05:08.320</a></span> | <span class="t">So we have our input we convert it into embeddings and we run it through the first eight layers in the first gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3914" target="_blank">01:05:14.320</a></span> | <span class="t">The first gpu will produce an output which will be the output of the eighth layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3919" target="_blank">01:05:19.600</a></span> | <span class="t">We transfer this output to the second gpu and we use it as input for the ninth layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3924" target="_blank">01:05:24.880</a></span> | <span class="t">Then we run all the this input through all the layers one after another until it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3929" target="_blank">01:05:29.760</a></span> | <span class="t">It arrives to the layer number 16, which will produce an output. We take this output. We move it to the next gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3936" target="_blank">01:05:36.000</a></span> | <span class="t">So it will become the input of the layer number 17</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3939" target="_blank">01:05:39.540</a></span> | <span class="t">And then we run iteratively to all the layers until the layer number 24, which will produce an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3945" target="_blank">01:05:45.280</a></span> | <span class="t">We move it to the next gpu. We run it through iteratively until the layer number 32</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3949" target="_blank">01:05:49.840</a></span> | <span class="t">Then we take the last linear layer and then the softmax to produce the output of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3955" target="_blank">01:05:55.040</a></span> | <span class="t">however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3957" target="_blank">01:05:57.040</a></span> | <span class="t">You can notice that this method is not very efficient because at any time only one gpu is working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3962" target="_blank">01:06:02.880</a></span> | <span class="t">A better approach which is not implemented in the code of Mistral, but they reference it in the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3968" target="_blank">01:06:08.720</a></span> | <span class="t">So I will talking about it is the pipeline parallelism. Let's see how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3975" target="_blank">01:06:15.040</a></span> | <span class="t">This pipeline parallelism. I will talking about the algorithm that was introduced in this paper. So gpipe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3981" target="_blank">01:06:21.220</a></span> | <span class="t">Basically, it works as follows first. Let me introduce you the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3985" target="_blank">01:06:25.600</a></span> | <span class="t">This actually it's used usually when we are training a model not when we are inferencing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3990" target="_blank">01:06:30.160</a></span> | <span class="t">But it can also be applied to the inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3992" target="_blank">01:06:32.340</a></span> | <span class="t">Imagine we want to train a model on a sharded model. So a model that is split into multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=3999" target="_blank">01:06:39.040</a></span> | <span class="t">Group of layers each group of layer is present on a different gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4004" target="_blank">01:06:44.160</a></span> | <span class="t">Imagine we have four gpus each one with its own group of layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4008" target="_blank">01:06:48.000</a></span> | <span class="t">Imagine we want to train this model. So we run our input to the first gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4012" target="_blank">01:06:52.880</a></span> | <span class="t">So we run the forward step to the first gpu. We take this output and we feed it to the next gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4017" target="_blank">01:06:57.680</a></span> | <span class="t">So then we run forward from there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4020" target="_blank">01:07:00.080</a></span> | <span class="t">We take the output and we run it through the next gpu the gpu number three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4023" target="_blank">01:07:03.600</a></span> | <span class="t">We take the output we run it to the next gpu the gpu number four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4026" target="_blank">01:07:06.800</a></span> | <span class="t">Now we have the output of the model. We compute the loss and then we can run back propagation the run propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4033" target="_blank">01:07:13.200</a></span> | <span class="t">That's basically just the opposite. We go from the last gpu to the first gpu. So we run back propagation on the fourth gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4039" target="_blank">01:07:19.760</a></span> | <span class="t">Then we have calculated the gradients at the fourth gpu and we use them to calculate the previous gradients at the third gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4047" target="_blank">01:07:27.200</a></span> | <span class="t">And then we take these gradients and we use them to calculate the previous gradients and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4051" target="_blank">01:07:31.760</a></span> | <span class="t">Take these gradients and we use to compute the previous gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4055" target="_blank">01:07:35.520</a></span> | <span class="t">So the forward step goes from the input to the loss the backward step goes from the loss to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4063" target="_blank">01:07:43.360</a></span> | <span class="t">And all the parameters which are also known as the leave nodes in the computational graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4067" target="_blank">01:07:47.200</a></span> | <span class="t">However, also as in this case, you can see that at each step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4072" target="_blank">01:07:52.080</a></span> | <span class="t">We are only utilizing one single gpu and all the other gpus are quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4078" target="_blank">01:07:58.800</a></span> | <span class="t">Not working. They are idle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4081" target="_blank">01:08:01.520</a></span> | <span class="t">A better way is to use pipeline parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4084" target="_blank">01:08:04.660</a></span> | <span class="t">So imagine that the previous step of training was done using a very big batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4089" target="_blank">01:08:09.840</a></span> | <span class="t">Suppose this batch is made up of eight items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4092" target="_blank">01:08:12.640</a></span> | <span class="t">What we do with pipeline parallelism is we take this batch and we split it into micro batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4098" target="_blank">01:08:18.480</a></span> | <span class="t">So instead of eight items, we create micro batch. So four micro batch of two items each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4104" target="_blank">01:08:24.720</a></span> | <span class="t">What we do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4107" target="_blank">01:08:27.120</a></span> | <span class="t">We run the first micro batch in the first gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4110" target="_blank">01:08:30.880</a></span> | <span class="t">This will produce the output for the first micro batch and we can feed it to the next gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4116" target="_blank">01:08:36.160</a></span> | <span class="t">But now at the time step one we realize that the gpu one now is free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4119" target="_blank">01:08:39.840</a></span> | <span class="t">So she can already start working on the second micro batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4123" target="_blank">01:08:43.120</a></span> | <span class="t">Meanwhile, the second gpu is working on the first micro batch and when she will finish she can send it to the next gpu and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4130" target="_blank">01:08:50.800</a></span> | <span class="t">Meanwhile, we realize that now the second gpu is free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4134" target="_blank">01:08:54.240</a></span> | <span class="t">So we can if the gpu one has finished we can take the output of the gpu one and transfer it to the gpu two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4140" target="_blank">01:09:00.880</a></span> | <span class="t">And the gpu one will be free so it can work on the third micro batch you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4145" target="_blank">01:09:05.920</a></span> | <span class="t">Then after the third gpu has finished it will take the output of the third gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4151" target="_blank">01:09:11.600</a></span> | <span class="t">We send it to the fourth gpu, but we realize that the third gpu is now free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4155" target="_blank">01:09:15.520</a></span> | <span class="t">So if the previous gpu have finished we can transfer the second micro batch to the third gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4160" target="_blank">01:09:20.160</a></span> | <span class="t">The third micro batch to the second gpu and the first gpu which will be free can start working on a new micro batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4166" target="_blank">01:09:26.720</a></span> | <span class="t">Which is the fourth micro batch and basically we do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4170" target="_blank">01:09:30.000</a></span> | <span class="t">job of time shifting the micro batches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4172" target="_blank">01:09:32.900</a></span> | <span class="t">And this will result in a better utilization of the gpus because now at every time step we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4178" target="_blank">01:09:38.800</a></span> | <span class="t">At this time step for example all the four the gpus are working and also at this time step here at the backward step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4184" target="_blank">01:09:44.720</a></span> | <span class="t">And for each micro batch we calculate the gradient, but we do not update the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4191" target="_blank">01:09:51.280</a></span> | <span class="t">We do what is called gradient accumulation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4193" target="_blank">01:09:53.360</a></span> | <span class="t">Which basically means that we calculate the gradient for each micro batch and we keep summing it to the existing gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4199" target="_blank">01:09:59.280</a></span> | <span class="t">But we do not update the parameters of the model after all the micro batch have finished processing the forward and the backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4205" target="_blank">01:10:05.440</a></span> | <span class="t">We update the parameters of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4207" target="_blank">01:10:07.520</a></span> | <span class="t">The gradient accumulation is a technique that I have introduced in my previous video on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4212" target="_blank">01:10:12.000</a></span> | <span class="t">Distributed training so if you want to understand how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4215" target="_blank">01:10:15.440</a></span> | <span class="t">I refer you to my previous video on distributed training in which I explain also the math behind gradient accumulation and how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4222" target="_blank">01:10:22.080</a></span> | <span class="t">But basically this is the solution with pipeline parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4225" target="_blank">01:10:25.760</a></span> | <span class="t">So we can actually divide our batch into micro batches and this can also work with inferencing because when we inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4232" target="_blank">01:10:32.880</a></span> | <span class="t">We just don't have this backward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4235" target="_blank">01:10:35.120</a></span> | <span class="t">Step here, right? So we just delete this second half of the table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4238" target="_blank">01:10:38.880</a></span> | <span class="t">But we can still take our big batch at the beginning. We split it into micro batches and we time shift them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4245" target="_blank">01:10:45.920</a></span> | <span class="t">according to the availability of the gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4248" target="_blank">01:10:48.880</a></span> | <span class="t">And this pipeline parallelism basically introduces still some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4255" target="_blank">01:10:55.920</a></span> | <span class="t">Time steps in which not all gpus are working and these are called bubbles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4259" target="_blank">01:10:59.920</a></span> | <span class="t">To avoid bubbles these big bubbles here. What we can do is we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4264" target="_blank">01:11:04.400</a></span> | <span class="t">Use a bigger initial batch size. So we have multiple micro batches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4269" target="_blank">01:11:09.940</a></span> | <span class="t">Okay guys now let's go to the last part of this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4274" target="_blank">01:11:14.720</a></span> | <span class="t">I know that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4276" target="_blank">01:11:16.800</a></span> | <span class="t">The mistral code is much more complicated to understand compared to the lama code and I will show you why but I will also help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4283" target="_blank">01:11:23.280</a></span> | <span class="t">You understand the most complex topic in the code, which is the xformats library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4287" target="_blank">01:11:27.760</a></span> | <span class="t">Which is a trick they use to improve the inference performance and it's actually a very advanced technique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4294" target="_blank">01:11:34.000</a></span> | <span class="t">And I want to give you a glimpse into how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4297" target="_blank">01:11:37.360</a></span> | <span class="t">So basically imagine you are running an ai company and you are providing llm inference service</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4305" target="_blank">01:11:45.440</a></span> | <span class="t">So you have a customer that has you for example provide an api and you have customers that send their prompts to your api</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4312" target="_blank">01:11:52.960</a></span> | <span class="t">And then want to run inference through your large language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4316" target="_blank">01:11:56.080</a></span> | <span class="t">Each prompt of course may have different length because each customer may be using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4321" target="_blank">01:12:01.600</a></span> | <span class="t">large language model for different purposes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4324" target="_blank">01:12:04.160</a></span> | <span class="t">For suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4325" target="_blank">01:12:05.600</a></span> | <span class="t">Simplicity suppose that each word is a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4328" target="_blank">01:12:08.000</a></span> | <span class="t">So suppose you have three customer the first customer says write a poem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4331" target="_blank">01:12:11.920</a></span> | <span class="t">The second customer says write a historical novel and the third customer says tell me a funny joke</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4338" target="_blank">01:12:18.880</a></span> | <span class="t">of course you could process all these prompts one by one, but that would not be very efficient because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4344" target="_blank">01:12:24.080</a></span> | <span class="t">The two other two customer would be waiting for the first customer to finish and when you have a lot of customers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4349" target="_blank">01:12:29.700</a></span> | <span class="t">That's not good. And secondly, you may not be fully utilizing the memory of your gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4354" target="_blank">01:12:34.720</a></span> | <span class="t">So the best thing that you can do is to do batching you create all these prompts you create one big batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4360" target="_blank">01:12:40.800</a></span> | <span class="t">But the problem is that the prompt have different lengths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4364" target="_blank">01:12:44.880</a></span> | <span class="t">So the first prompt is made up of three tokens the second prompt of four tokens and the third prompt of five tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4371" target="_blank">01:12:51.600</a></span> | <span class="t">One solution is to add padding to these tokens. So basically we create a batch in which we append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4379" target="_blank">01:12:59.040</a></span> | <span class="t">Padding tokens to the input sequence until they all reach the same size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4384" target="_blank">01:13:04.880</a></span> | <span class="t">Then we can run these sequences this batch through our large language model, which could be for example Lama or Mistral</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4394" target="_blank">01:13:14.800</a></span> | <span class="t">As we saw before when we have a input sequence of n tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4399" target="_blank">01:13:19.120</a></span> | <span class="t">The attention mechanism produces an output sequence of n tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4402" target="_blank">01:13:22.800</a></span> | <span class="t">And we usually take the embedding of the last token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4406" target="_blank">01:13:26.640</a></span> | <span class="t">Send it to the linear layer then the softmax to understand what is the next token from our vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4412" target="_blank">01:13:32.980</a></span> | <span class="t">but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4414" target="_blank">01:13:34.400</a></span> | <span class="t">In the first prompt we see that we have added two padding tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4418" target="_blank">01:13:38.000</a></span> | <span class="t">So we cannot use the embedding corresponding to the last tokens because they correspond to the padding token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4424" target="_blank">01:13:44.160</a></span> | <span class="t">What we should do is we should take the embedding corresponding to the last non-padding token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4429" target="_blank">01:13:49.200</a></span> | <span class="t">And then send it to the linear layer and then to the softmax to understand what is the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4434" target="_blank">01:13:54.960</a></span> | <span class="t">And in the case of the second prompt we should be using the fourth token not the last one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4439" target="_blank">01:13:59.760</a></span> | <span class="t">Only in the last prompt we can use the last token because it's the last it's a non not padding token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4445" target="_blank">01:14:05.600</a></span> | <span class="t">Now we have done this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4449" target="_blank">01:14:09.600</a></span> | <span class="t">And how do we actually create a attention mask to run it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4453" target="_blank">01:14:13.200</a></span> | <span class="t">We basically just create an attention mask that is causal that will make each token only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4459" target="_blank">01:14:19.440</a></span> | <span class="t">Visualize the previous tokens. So each token will be able to relate to previous tokens, but not to future tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4466" target="_blank">01:14:26.160</a></span> | <span class="t">And this mask here will work fine for all the three scenarios. You can see here and I will show you later how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4474" target="_blank">01:14:34.880</a></span> | <span class="t">We cannot use a different mask for each prompt because all the prompts are of the same length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4480" target="_blank">01:14:40.160</a></span> | <span class="t">So all the masks must be 5x5 because we cannot use a 3x3 mask for this prompt a 4x4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4486" target="_blank">01:14:46.960</a></span> | <span class="t">Mask for this prompt and the 5x5 for this prompt because the input sequence is 5</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4493" target="_blank">01:14:53.200</a></span> | <span class="t">So we must use a 5x5 mask and we have to use a 5x5 mask that is causal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4500" target="_blank">01:15:00.400</a></span> | <span class="t">And also has the we can also mask out for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4504" target="_blank">01:15:04.080</a></span> | <span class="t">Imagine the sliding window is size is 4 then we can mask out this value here also because we don't want the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4510" target="_blank">01:15:10.000</a></span> | <span class="t">This token here to watch tokens that are a distance of more than 4 for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4515" target="_blank">01:15:15.760</a></span> | <span class="t">So the problem here is that we are calculating a lot of dot products, especially for the first and the second prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4523" target="_blank">01:15:23.360</a></span> | <span class="t">That will not be used. Let me show you why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4527" target="_blank">01:15:27.760</a></span> | <span class="t">When we apply this mask, so the 5x5 mask you can see here to this input sequence here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4534" target="_blank">01:15:34.640</a></span> | <span class="t">Which are I want to remind you is a batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4536" target="_blank">01:15:36.640</a></span> | <span class="t">Which will produce the following attention mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4540" target="_blank">01:15:40.720</a></span> | <span class="t">In which all this value will be masked out because they are minus infinity minus infinity and it's because of the causality of the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4548" target="_blank">01:15:48.720</a></span> | <span class="t">We cannot mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4552" target="_blank">01:15:52.320</a></span> | <span class="t">This value here because they are needed for the last prompt for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4556" target="_blank">01:15:56.160</a></span> | <span class="t">And we also cannot mask this value here, which is needed for the last prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4560" target="_blank">01:16:00.560</a></span> | <span class="t">But for the first and the second prompt we are doing a lot of dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4564" target="_blank">01:16:04.240</a></span> | <span class="t">for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4564" target="_blank">01:16:04.800</a></span> | <span class="t">These ones between padding tokens and other tokens that we will not be using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4568" target="_blank">01:16:08.400</a></span> | <span class="t">Because I want to remind you that in the first prompt as the output of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4573" target="_blank">01:16:13.200</a></span> | <span class="t">So we will be using the output as the third token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4576" target="_blank">01:16:16.880</a></span> | <span class="t">For the second prompt the output at the fourth token and only in the last token. We will be checking the last output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4583" target="_blank">01:16:23.760</a></span> | <span class="t">Of the output of the self-attention, but for the first two prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4587" target="_blank">01:16:27.520</a></span> | <span class="t">We will not be even checking the last token output from the self-attention because they correspond to the padding token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4593" target="_blank">01:16:33.360</a></span> | <span class="t">So is there a way to avoid these padding tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4597" target="_blank">01:16:37.680</a></span> | <span class="t">Being introduced in our calculation and calculating all these dot products which will result in output tokens that we will not even use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4606" target="_blank">01:16:46.320</a></span> | <span class="t">Well, there is a better solution and the solution is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4609" target="_blank">01:16:49.280</a></span> | <span class="t">The solution is to combine all the tokens of all the prompts into one big sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4615" target="_blank">01:16:55.920</a></span> | <span class="t">Consecutively and we also keep track of what is the actual size of each prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4622" target="_blank">01:17:02.880</a></span> | <span class="t">So we know that the prompt are coming from our API because we are running an AI company and we have this API</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4629" target="_blank">01:17:09.200</a></span> | <span class="t">So we know that the first customer has a token size prompt of size three tokens. The second one has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4635" target="_blank">01:17:15.680</a></span> | <span class="t">Four tokens and the third one has five tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4638" target="_blank">01:17:18.240</a></span> | <span class="t">So we can keep track of these sizes in an array, for example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4642" target="_blank">01:17:22.240</a></span> | <span class="t">And then we build this sequence which is a concatenation of all the prompts that we receive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4647" target="_blank">01:17:27.280</a></span> | <span class="t">We take this mega sequence. We run it through our LLM model. So it could be Mistral or it could be Lama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4655" target="_blank">01:17:35.120</a></span> | <span class="t">This as I told you before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4658" target="_blank">01:17:38.000</a></span> | <span class="t">An input sequence in a transformer will result in n output tokens in the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4665" target="_blank">01:17:45.200</a></span> | <span class="t">So we have here we have 3 plus 4 so 7, 7 plus 5, 12</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4670" target="_blank">01:17:50.800</a></span> | <span class="t">Tokens as input it will produce 12 tokens as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4674" target="_blank">01:17:54.320</a></span> | <span class="t">To understand what is the next token for each prompt. We need to check the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4680" target="_blank">01:18:00.240</a></span> | <span class="t">We need to check the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4683" target="_blank">01:18:03.200</a></span> | <span class="t">corresponding to the token number 3 for the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4685" target="_blank">01:18:05.840</a></span> | <span class="t">Prompt to the token number 7 for the second prompt and the last token for the third prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4692" target="_blank">01:18:12.560</a></span> | <span class="t">So we take all these embeddings we run them through the linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4696" target="_blank">01:18:16.480</a></span> | <span class="t">Then we apply the softmax and then we understand what is the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4700" target="_blank">01:18:20.480</a></span> | <span class="t">Token from our vocabulary, but you may be wondering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4704" target="_blank">01:18:24.480</a></span> | <span class="t">How do we even produce an attention mask that can work with multiple prompts that are combined into one sequence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4711" target="_blank">01:18:31.920</a></span> | <span class="t">Such that the token of one sequence should not of one prompt should not be attend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4717" target="_blank">01:18:37.840</a></span> | <span class="t">To the tokens of the another prompt but only of the tokens of the same prompt, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4722" target="_blank">01:18:42.640</a></span> | <span class="t">Well, the Xformers library allow us allow us to do that using a method called a block diagonal causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4731" target="_blank">01:18:51.120</a></span> | <span class="t">Which is also used in the source code of Mistral. So I want to show you how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4735" target="_blank">01:18:55.680</a></span> | <span class="t">basically Xformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4738" target="_blank">01:18:58.400</a></span> | <span class="t">This method called the block diagonal causal mask will produce a mask like this. It will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4745" target="_blank">01:19:05.200</a></span> | <span class="t">Group, basically all the prompts into groups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4748" target="_blank">01:19:08.880</a></span> | <span class="t">Such that each token only can attend to the tokens in the same group here. We have three prompts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4755" target="_blank">01:19:15.920</a></span> | <span class="t">So the token poem for example can only attend the token of the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4760" target="_blank">01:19:20.880</a></span> | <span class="t">Prompt the token novel for example cannot be related to the token poem. So it will put minus infinity here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4768" target="_blank">01:19:28.480</a></span> | <span class="t">but all the token of the same prompt will be able to be attended by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4774" target="_blank">01:19:34.000</a></span> | <span class="t">the token novel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4776" target="_blank">01:19:36.000</a></span> | <span class="t">While the token in the last prompt will be only be able to attend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4780" target="_blank">01:19:40.560</a></span> | <span class="t">The other tokens in the same prompt and this is a special mask built by using the Xformers library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4787" target="_blank">01:19:47.200</a></span> | <span class="t">Let me show you how it works in the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4790" target="_blank">01:19:50.720</a></span> | <span class="t">Okay, I want to show you actually how it works. So in the Mistral source code, they are using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4797" target="_blank">01:19:57.760</a></span> | <span class="t">Library called Xformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4802" target="_blank">01:20:02.240</a></span> | <span class="t">Xformers library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4803" target="_blank">01:20:03.920</a></span> | <span class="t">Allows us to compute very complex attention mask and also to calculate the attention in a very efficient way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4810" target="_blank">01:20:10.560</a></span> | <span class="t">Using the memory efficient attention calculation, which I will not show in this video. Maybe I will make a future video about it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4816" target="_blank">01:20:16.480</a></span> | <span class="t">But basically what they do in the Mistral source code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4819" target="_blank">01:20:19.920</a></span> | <span class="t">If you have multiple prompts, they will create one big sequence and then keep track of the number of tokens of each prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4827" target="_blank">01:20:27.280</a></span> | <span class="t">And then they use these methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4830" target="_blank">01:20:30.400</a></span> | <span class="t">made available by the Xformers library to build these complex attention maps that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4834" target="_blank">01:20:34.880</a></span> | <span class="t">Keep track of the different size of the KVCache because each prompt may have a KVCache that is different from another prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4843" target="_blank">01:20:43.200</a></span> | <span class="t">Because imagine you have a prompt with 5,000 tokens and one prompt with only 10 tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4847" target="_blank">01:20:47.680</a></span> | <span class="t">Of course, you will have a KVCache that is 5,000 tokens in one case and 10 in another case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4853" target="_blank">01:20:53.280</a></span> | <span class="t">So the mask, attention mask that we build should take care of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4858" target="_blank">01:20:58.400</a></span> | <span class="t">And the second thing is that each group of tokens should only be able to relate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4863" target="_blank">01:21:03.040</a></span> | <span class="t">To the tokens of the same group not to other groups. So not of tokens from another prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4869" target="_blank">01:21:09.760</a></span> | <span class="t">And this is done with the block diagonal causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4874" target="_blank">01:21:14.720</a></span> | <span class="t">So basically we tell him okay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4876" target="_blank">01:21:16.880</a></span> | <span class="t">The first prompt is made up of seven tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4879" target="_blank">01:21:19.360</a></span> | <span class="t">The second prompt is made up of five tokens and the third prompt is made up of six tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4884" target="_blank">01:21:24.640</a></span> | <span class="t">And we are also using a sliding window attention with a sliding window size of three and basically this will create the complex mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4891" target="_blank">01:21:31.280</a></span> | <span class="t">That we can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4893" target="_blank">01:21:33.040</a></span> | <span class="t">This is the first group of tokens from 0 to 6 is the first prompt from 7 to 11 is the second prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4900" target="_blank">01:21:40.240</a></span> | <span class="t">and from 12 to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4902" target="_blank">01:21:42.560</a></span> | <span class="t">Let me check 17 is the third prompt and as you can see it also takes into consideration the sliding window size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4909" target="_blank">01:21:49.680</a></span> | <span class="t">So each token can only watch at most two previous tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4914" target="_blank">01:21:54.400</a></span> | <span class="t">So the tokens in this in the contained in the sliding window size of size three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4918" target="_blank">01:21:58.880</a></span> | <span class="t">The second one they use is the block diagonal mask and okay. This one is used for the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4925" target="_blank">01:22:05.680</a></span> | <span class="t">chunk during the pre-filling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4928" target="_blank">01:22:08.320</a></span> | <span class="t">This one is used for subsequent chunks in the pre-filling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4931" target="_blank">01:22:11.940</a></span> | <span class="t">And basically it also takes in because during the first pre-filling we don't have the KV cache because it's initially empty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4938" target="_blank">01:22:18.400</a></span> | <span class="t">But during the subsequent steps, it's not empty anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4941" target="_blank">01:22:21.520</a></span> | <span class="t">So we need to take into consideration also the different size of the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4944" target="_blank">01:22:24.880</a></span> | <span class="t">So for example, the first token may have a KV cache of size 10 because the prompt is very short</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4951" target="_blank">01:22:31.120</a></span> | <span class="t">But the second prompt may be very big suppose 5,000 tokens. So it may have a KV cache of size 5,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4956" target="_blank">01:22:36.740</a></span> | <span class="t">So it takes into consideration also the size of the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4960" target="_blank">01:22:40.640</a></span> | <span class="t">And it will produce a mask that takes into consideration also the size of the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4967" target="_blank">01:22:47.040</a></span> | <span class="t">The last method they use is this one block diagonal causal with offset padded keys mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4972" target="_blank">01:22:52.320</a></span> | <span class="t">Because each prompt may have a different size for the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4977" target="_blank">01:22:57.280</a></span> | <span class="t">But only some tokens in this KV so the KV cache size is fixed. It's a tensor that is of fixed side w</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4985" target="_blank">01:23:05.840</a></span> | <span class="t">But only some tokens may be actual being filled in this KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4991" target="_blank">01:23:11.040</a></span> | <span class="t">So only maybe the KV cache size is let's say 10</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4994" target="_blank">01:23:14.240</a></span> | <span class="t">But because the first prompt is very short only three tokens are actually part in the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=4999" target="_blank">01:23:19.760</a></span> | <span class="t">But when we pass the KV cache to the calculation of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5005" target="_blank">01:23:25.120</a></span> | <span class="t">We pass all the tensor which is all the 10 items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5008" target="_blank">01:23:28.960</a></span> | <span class="t">So we need a way to tell to the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5011" target="_blank">01:23:31.360</a></span> | <span class="t">That it should only use the first three items from the KV cache and not all the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5017" target="_blank">01:23:37.440</a></span> | <span class="t">Not all the tensor and this is done with block diagonal with offset padding keys mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5022" target="_blank">01:23:42.560</a></span> | <span class="t">So this method here, it's very long name very complicated, but this is why they use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5026" target="_blank">01:23:46.400</a></span> | <span class="t">And it will produce a mask like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5028" target="_blank">01:23:48.880</a></span> | <span class="t">So it takes into consideration the actual size of the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5032" target="_blank">01:23:52.560</a></span> | <span class="t">Even if the KV all the KV cache have the same size because it's a fixed size tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5037" target="_blank">01:23:57.760</a></span> | <span class="t">But it tells you how many items there are actually it should use from each cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5043" target="_blank">01:24:03.120</a></span> | <span class="t">Okay guys it has been a very demanding video I have to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5052" target="_blank">01:24:12.400</a></span> | <span class="t">I had to record it more than once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5055" target="_blank">01:24:15.520</a></span> | <span class="t">I actually had to cut some parts because I even I got confused sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5060" target="_blank">01:24:20.160</a></span> | <span class="t">It's very complicated topics. It's a lot of things that you have to grasp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5065" target="_blank">01:24:25.760</a></span> | <span class="t">But I hope that it will make your life easier when you want to understand the Mistral code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5071" target="_blank">01:24:31.760</a></span> | <span class="t">I actually am also putting online</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5074" target="_blank">01:24:34.400</a></span> | <span class="t">My notes the one that you have seen so the two notebooks that I have shown you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5079" target="_blank">01:24:39.120</a></span> | <span class="t">Plus also the code annotated by me on the Mistral source code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5082" target="_blank">01:24:42.880</a></span> | <span class="t">Now the Mistral source code I actually never run it. So because my computer is not very powerful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5088" target="_blank">01:24:48.960</a></span> | <span class="t">So I never run the actual model on my computer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5091" target="_blank">01:24:51.920</a></span> | <span class="t">What I did to study the model was to run some random tensors to a model and I created basically a model with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5098" target="_blank">01:24:58.960</a></span> | <span class="t">randomly initialized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5101" target="_blank">01:25:01.200</a></span> | <span class="t">Weights but with the less number of layers so it could fit in my GPU and then I just run some random tensors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5107" target="_blank">01:25:07.280</a></span> | <span class="t">To study all the shapes of the tensor and all the information passing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5111" target="_blank">01:25:11.040</a></span> | <span class="t">So, I don't know if the code works, but I hope it will work. I mean, I didn't touch the logic. I just add some comments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5117" target="_blank">01:25:17.200</a></span> | <span class="t">anyway, you can use the commented code by me to as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5121" target="_blank">01:25:21.360</a></span> | <span class="t">As a learning tool to complement with the official code of Mistral so that you can understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5127" target="_blank">01:25:27.380</a></span> | <span class="t">More about the inner workings of this grid model. I actually really enjoyed studying it. I really enjoyed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5133" target="_blank">01:25:33.680</a></span> | <span class="t">Studying the code and I learned a lot of stuff, you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5137" target="_blank">01:25:37.760</a></span> | <span class="t">I think it's very very good when you are doing something that is very complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5141" target="_blank">01:25:41.860</a></span> | <span class="t">Because it teaches you a lot because if something is simple, then you don't learn much by the end of the day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5146" target="_blank">01:25:46.800</a></span> | <span class="t">Anyway, guys, thanks you for watching my video. I hope you also enjoyed this journey with me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5151" target="_blank">01:25:51.840</a></span> | <span class="t">Even if it was very complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5153" target="_blank">01:25:53.600</a></span> | <span class="t">I hope that you liked this video and you will subscribe to my channel if you didn't please do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5158" target="_blank">01:25:58.640</a></span> | <span class="t">And the best way to support me guys is to share this video with all the people, you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5163" target="_blank">01:26:03.200</a></span> | <span class="t">So share it on social media share it on linkedin on twitter, etc</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5167" target="_blank">01:26:07.040</a></span> | <span class="t">because this is the best way to you can help me is to grow my channel and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5172" target="_blank">01:26:12.160</a></span> | <span class="t">Please let me know if there is something that you don't understand. I am always available to help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=UiX8K-xBUpE&t=5177" target="_blank">01:26:17.280</a></span> | <span class="t">And connect with me on linkedin. Bye. Bye</span></div></div></body></html>