<html><head><title>Stanford CS25: V1 I Transformer Circuits, Induction Heads, In-Context Learning</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V1 I Transformer Circuits, Induction Heads, In-Context Learning</h2><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ"><img src="https://i.ytimg.com/vi/pC4zRb_5noQ/hqdefault.jpg?sqp=-oaymwEmCOADEOgC8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgXihUMA8=&rs=AOn4CLDDYPnwDBMEqqQ0oAyZo67Sd6-Aeg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=26">0:26</a> People mean lots of different things by "interpretability". Mechanistic interpretability aims to map neural network parameters to human understandable algorithms.<br><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=853">14:13</a> What is going on???<br><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2654">44:14</a> The Induction Pattern<br><br><div style="text-align: left;"><a href="./pC4zRb_5noQ.html">Whisper Transcript</a> | <a href="./transcript_pC4zRb_5noQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Thank you all for having me. It's exciting to be here. One of my favorite things is talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=10" target="_blank">00:00:10.120</a></span> | <span class="t">about what is going on inside neural networks, or at least what we're trying to figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=15" target="_blank">00:00:15.120</a></span> | <span class="t">is going on inside neural networks. So it's always fun to chat about that. Oh, gosh, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">have to figure out how to do things. Okay. What? I won't. Okay, there we go. Now we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=28" target="_blank">00:00:28.200</a></span> | <span class="t">advancing slides, that seems promising. So I think interoperability means lots of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=33" target="_blank">00:00:33.320</a></span> | <span class="t">things to different people. It's a very broad term and people mean all sorts of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=38" target="_blank">00:00:38.540</a></span> | <span class="t">things by it. And so I wanted to talk just briefly about the kind of interoperability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=43" target="_blank">00:00:43.480</a></span> | <span class="t">that I spend my time thinking about, which is what I'd call mechanistic interoperability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=48" target="_blank">00:00:48.640</a></span> | <span class="t">So most of my work actually has not been on language models or on RNNs or transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=54" target="_blank">00:00:54.920</a></span> | <span class="t">but on understanding vision confinates and trying to understand how do the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=60" target="_blank">00:01:00.720</a></span> | <span class="t">in those models actually map to algorithms. So you can like think of the parameters of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=66" target="_blank">00:01:06.360</a></span> | <span class="t">a neural network as being like a compiled computer program. And the neurons are kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=71" target="_blank">00:01:11.100</a></span> | <span class="t">of like variables or registers. And somehow there are these complex computer programs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=76" target="_blank">00:01:16.720</a></span> | <span class="t">that are embedded in those weights. And we'd like to turn them back in to computer programs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=81" target="_blank">00:01:21.120</a></span> | <span class="t">that humans can understand. It's a kind of reverse engineering problem. And so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=87" target="_blank">00:01:27.440</a></span> | <span class="t">kind of a fun example that we found where there was a car neuron and you could actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=91" target="_blank">00:01:31.000</a></span> | <span class="t">see that we have the car neuron and it's constructed from like a wheel neuron. And it looks for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=98" target="_blank">00:01:38.880</a></span> | <span class="t">in the case of the wheel neuron, it's looking for the wheels on the bottom. Those are positive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=102" target="_blank">00:01:42.880</a></span> | <span class="t">weights and it doesn't want to see them on top. So it has negative weights there. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=106" target="_blank">00:01:46.160</a></span> | <span class="t">there's also a window neuron. It's looking for the windows on the top and not on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=109" target="_blank">00:01:49.800</a></span> | <span class="t">bottom. And so what we're actually seeing there, right, is it's an algorithm. It's an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=113" target="_blank">00:01:53.660</a></span> | <span class="t">algorithm that goes and turns, you know, it's just, it's, you know, saying, you know, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=119" target="_blank">00:01:59.000</a></span> | <span class="t">a car has wheels on the bottom and windows on the top and chrome in the middle. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=123" target="_blank">00:02:03.720</a></span> | <span class="t">actually like just the strongest neurons for that. And so we're actually seeing a meaningful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=127" target="_blank">00:02:07.840</a></span> | <span class="t">algorithm and that's not an exception. That's sort of the general story that if you're willing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=132" target="_blank">00:02:12.740</a></span> | <span class="t">to go and look at neural network weights and you're willing to invest a lot of energy in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=136" target="_blank">00:02:16.560</a></span> | <span class="t">trying to first engineer them, there's meaningful algorithms written in the weights waiting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=141" target="_blank">00:02:21.040</a></span> | <span class="t">for you to find them. And there's a bunch of reasons. I think that's an interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=145" target="_blank">00:02:25.480</a></span> | <span class="t">thing to think about. One is, you know, just no one knows how to go and do the things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=149" target="_blank">00:02:29.160</a></span> | <span class="t">neural networks can do. Like no one knows how to write a computer program that can accurately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=152" target="_blank">00:02:32.520</a></span> | <span class="t">classify image net, let alone, you know, the language modeling tasks that we're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=156" target="_blank">00:02:36.160</a></span> | <span class="t">No one knows how to like directly write a computer program that can do the things that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=159" target="_blank">00:02:39.400</a></span> | <span class="t">GPT-3 does. And yet somehow breaking descent is able to go and discover a way to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=163" target="_blank">00:02:43.620</a></span> | <span class="t">And I want to know what's going on. I want to know, you know, how, what has it discovered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=168" target="_blank">00:02:48.300</a></span> | <span class="t">that it can do in these systems? There's another reason why I think this is important, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=173" target="_blank">00:02:53.760</a></span> | <span class="t">is, uh, is safety. So, you know, if we, if we want to go and use these systems in places</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=179" target="_blank">00:02:59.060</a></span> | <span class="t">where they have big effect on the world, um, I think a question we need to ask ourselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=183" target="_blank">00:03:03.420</a></span> | <span class="t">is, you know, what, what happens when these models have, have unanticipated failure modes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=188" target="_blank">00:03:08.620</a></span> | <span class="t">failure modes we didn't know to go and test for, to look for, to check for, how can we,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=192" target="_blank">00:03:12.820</a></span> | <span class="t">how can we discover those things, especially if they're, if they're really pathological</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=195" target="_blank">00:03:15.700</a></span> | <span class="t">failure modes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=196" target="_blank">00:03:16.700</a></span> | <span class="t">So the models in some sense, deliberately doing something that we don't want. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=200" target="_blank">00:03:20.180</a></span> | <span class="t">the only way that I really see that we, we can do that is if we can get to a point where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=203" target="_blank">00:03:23.340</a></span> | <span class="t">we really understand what's going on inside these systems. Um, so that's another reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=208" target="_blank">00:03:28.020</a></span> | <span class="t">that I'm interested in this. Now, uh, actually doing interpretedly on language models and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=213" target="_blank">00:03:33.740</a></span> | <span class="t">transformers it's new, new to me. I, um, before this year, I spent like eight years working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=218" target="_blank">00:03:38.020</a></span> | <span class="t">on trying to reverse engineer continents and vision models. Um, and so the ideas in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=222" target="_blank">00:03:42.980</a></span> | <span class="t">talk, um, are, are new things that I've been thinking about with my collaborators. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=228" target="_blank">00:03:48.140</a></span> | <span class="t">and we're still probably a month or two out, maybe, maybe longer from publishing them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=231" target="_blank">00:03:51.820</a></span> | <span class="t">Um, and this was also the first public talk that I've given on it. So, uh, you know, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=235" target="_blank">00:03:55.460</a></span> | <span class="t">things that I'm going to talk about, um, they met there, there's, I think, honestly, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=238" target="_blank">00:03:58.540</a></span> | <span class="t">still a little bit confused for me, um, and definitely are going to be confused in my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=241" target="_blank">00:04:01.820</a></span> | <span class="t">articulation of them. So if I, if I say things that are confusing, um, you know, please feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=245" target="_blank">00:04:05.420</a></span> | <span class="t">free to ask me questions. There might be some points for me to go quickly because there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=248" target="_blank">00:04:08.180</a></span> | <span class="t">a lot of content. Um, but definitely at the end, I will be available for a while to chat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=252" target="_blank">00:04:12.500</a></span> | <span class="t">about this stuff. Um, and, uh, yeah, also I apologize. Um, if, uh, if I'm unfamiliar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=259" target="_blank">00:04:19.020</a></span> | <span class="t">with zoom and make, make mistakes. Um, but, uh, yeah. So, um, with that said, uh, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=265" target="_blank">00:04:25.180</a></span> | <span class="t">dive in. Um, so I've wanted to start with a mystery, um, before we go and try to actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=273" target="_blank">00:04:33.980</a></span> | <span class="t">dig into, you know, what's going on inside these models. Um, I wanted to motivate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=278" target="_blank">00:04:38.020</a></span> | <span class="t">by a really strange piece of discovery of a behavior that we discovered and, and wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=283" target="_blank">00:04:43.020</a></span> | <span class="t">to understand. Um, uh, and by the way, I should say all this work is, um, uh, you know, is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=289" target="_blank">00:04:49.860</a></span> | <span class="t">is done with my, my colleagues and Anthropic and especially my colleagues, Catherine and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=293" target="_blank">00:04:53.260</a></span> | <span class="t">Nelson. Um, okay. So onto the mystery. Um, I think probably the, the most interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=299" target="_blank">00:04:59.140</a></span> | <span class="t">and most exciting thing about, um, about transformers is their ability to do in context learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=306" target="_blank">00:05:06.140</a></span> | <span class="t">or sometimes people will call it meta learning. Um, you know, the GP three paper, uh, goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=310" target="_blank">00:05:10.580</a></span> | <span class="t">and, and describes things as, um, you know, uh, language models are few shot learners.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=315" target="_blank">00:05:15.060</a></span> | <span class="t">Like there's lots of impressive things about GPT three, but they choose to focus on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=317" target="_blank">00:05:17.820</a></span> | <span class="t">And, you know, now everyone's talking about prompt engineering. Um, and, um, Andre McCarthy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=322" target="_blank">00:05:22.660</a></span> | <span class="t">was, was joking about how, you know, software 3.0 is designing the prompt. Um, and so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=327" target="_blank">00:05:27.100</a></span> | <span class="t">ability of language models of these, these large transformers to respond to their context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=331" target="_blank">00:05:31.900</a></span> | <span class="t">and learn from their context and change their behavior and response to their context. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=335" target="_blank">00:05:35.820</a></span> | <span class="t">you know, it really seems like probably the most surprising and striking and remarkable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=339" target="_blank">00:05:39.260</a></span> | <span class="t">thing about that. Um, and, uh, some of my, my colleagues previously published a paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=346" target="_blank">00:05:46.420</a></span> | <span class="t">that has a trick in it that I, I really love, which is, so we're, we're all used to looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=350" target="_blank">00:05:50.420</a></span> | <span class="t">at learning curves. You train your model and you, you know, as your model trains, the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=353" target="_blank">00:05:53.740</a></span> | <span class="t">goes down. Sometimes it's a little bit discontinuous, but it goes down. Another thing that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=361" target="_blank">00:06:01.300</a></span> | <span class="t">do is you can go and take a fully trained model and you can go and ask, you know, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=364" target="_blank">00:06:04.940</a></span> | <span class="t">we go through the context, you know, as we go and we predict the first token and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=368" target="_blank">00:06:08.260</a></span> | <span class="t">the second token and the third token, we get better at predicting each token because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=372" target="_blank">00:06:12.220</a></span> | <span class="t">have more information to go and predict it on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=374" target="_blank">00:06:14.380</a></span> | <span class="t">So, you know, the first, the first con token, the loss should be the entropy of the unigrams</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=379" target="_blank">00:06:19.180</a></span> | <span class="t">and then the next token should be the entry of the bigrams and it falls, but it keeps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=383" target="_blank">00:06:23.420</a></span> | <span class="t">falling and it keeps getting better. And in some sense, that's our, that's the model's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=388" target="_blank">00:06:28.940</a></span> | <span class="t">ability to go and predict, to, to go and do in context learning the ability to go and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=394" target="_blank">00:06:34.620</a></span> | <span class="t">predict, um, you know, to be better at predicting later tokens than you are at predicting early</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=398" target="_blank">00:06:38.500</a></span> | <span class="t">tokens. That is, that is in some sense, a mathematical definition of what it means to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=402" target="_blank">00:06:42.020</a></span> | <span class="t">be good at this magical in context, learning or meta learning that, that these models can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=406" target="_blank">00:06:46.020</a></span> | <span class="t">do. And so that's kind of cool because that gives us a way to go and look at whether models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=411" target="_blank">00:06:51.020</a></span> | <span class="t">are good at, at in context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=412" target="_blank">00:06:52.740</a></span> | <span class="t">Chris, uh, if I could just ask a question, like a clarification question, when you say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=418" target="_blank">00:06:58.380</a></span> | <span class="t">learning, there are no actual parameter updates that is the remarkable thing about in context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=424" target="_blank">00:07:04.060</a></span> | <span class="t">learning, right? So yeah, indeed, we traditionally think about neural networks as learning over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=428" target="_blank">00:07:08.180</a></span> | <span class="t">the course of training by going and modifying their parameters, but somehow models appear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=432" target="_blank">00:07:12.340</a></span> | <span class="t">to also be able to learn in some sense. Um, if you give them a couple of examples in their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=436" target="_blank">00:07:16.000</a></span> | <span class="t">context, they can then go and do that later in their context, even though no parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=439" target="_blank">00:07:19.900</a></span> | <span class="t">changed. And so it's, it's some kind of quite different, different notion of learning as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=443" target="_blank">00:07:23.780</a></span> | <span class="t">you're, as you're, as you're gesturing out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=445" target="_blank">00:07:25.540</a></span> | <span class="t">Uh, okay. I think that's making more sense. So, I mean, could you also just describe in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=451" target="_blank">00:07:31.340</a></span> | <span class="t">context learning in this case as conditioning, as in like conditioning on the first five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=455" target="_blank">00:07:35.540</a></span> | <span class="t">tokens of a 10 token sentence or the next five tokens?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=459" target="_blank">00:07:39.100</a></span> | <span class="t">Yeah. I think the reason that people sometimes think about this as in context learning or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=462" target="_blank">00:07:42.420</a></span> | <span class="t">meta learning is that you can do things where you like actually take a training set and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=467" target="_blank">00:07:47.060</a></span> | <span class="t">you embed the training set in your context. Like if you just two or three examples, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=470" target="_blank">00:07:50.900</a></span> | <span class="t">then suddenly your model can go and do, do this task. And so you can do few shot learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=475" target="_blank">00:07:55.340</a></span> | <span class="t">by embedding things in the context. Um, yeah, the formal setup is that you're, you're just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=480" target="_blank">00:08:00.320</a></span> | <span class="t">conditioning on, on, on this context. And it's just that somehow this, this ability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=485" target="_blank">00:08:05.460</a></span> | <span class="t">like this thing, like there's, there's some sense, you know, for a long time, people were,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=488" target="_blank">00:08:08.980</a></span> | <span class="t">were, were, I mean, I, I guess really the history of this is, uh, we started to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=494" target="_blank">00:08:14.280</a></span> | <span class="t">good at, at neural networks learning, right. Um, and we could, we could go and train language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=498" target="_blank">00:08:18.180</a></span> | <span class="t">uh, train vision models and language models that could do all these remarkable things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=500" target="_blank">00:08:20.780</a></span> | <span class="t">But then people started to be like, well, you know, these systems are, they take so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=504" target="_blank">00:08:24.300</a></span> | <span class="t">many more examples than humans do to go and learn. How can we go and fix this? And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=508" target="_blank">00:08:28.780</a></span> | <span class="t">had all these ideas of meta learning develop where we wanted to go and, and train models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=512" target="_blank">00:08:32.860</a></span> | <span class="t">explicitly to be able to learn from a few examples and people develop all these complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=517" target="_blank">00:08:37.060</a></span> | <span class="t">schemes. And then the like, truly like absurd thing about, about transformer language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=521" target="_blank">00:08:41.180</a></span> | <span class="t">is without any effort at all, we get this for free that you can go and just give them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=525" target="_blank">00:08:45.820</a></span> | <span class="t">a couple of examples in their context and they can learn in their context to go and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=529" target="_blank">00:08:49.060</a></span> | <span class="t">do new things. Um, I think that was like, like that was in some sense, the like most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=533" target="_blank">00:08:53.380</a></span> | <span class="t">striking thing about the GPT-3 paper. Um, and so, uh, this, this, yeah, this, this ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=539" target="_blank">00:08:59.460</a></span> | <span class="t">to go and have the just conditioning on a context going, give you, you know, new abilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=544" target="_blank">00:09:04.460</a></span> | <span class="t">for free and, and the ability to generalize to new things is, is in some sense the, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=548" target="_blank">00:09:08.300</a></span> | <span class="t">most, yeah. And to me, the most striking and shocking thing about, about transformer language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=552" target="_blank">00:09:12.220</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=553" target="_blank">00:09:13.220</a></span> | <span class="t">That makes sense. I mean, I guess from my perspective, I'm trying to square like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=561" target="_blank">00:09:21.340</a></span> | <span class="t">notion of learning in this case with, you know, if you were, I were given a prompt of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=566" target="_blank">00:09:26.340</a></span> | <span class="t">like one plus one equals two, two plus three equals five as the sort of few shot setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=572" target="_blank">00:09:32.500</a></span> | <span class="t">And then somebody else put, you know, like five plus three equals, and we had to fill</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=576" target="_blank">00:09:36.740</a></span> | <span class="t">it out. In that case, I wouldn't say that we've learned arithmetic because we already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=580" target="_blank">00:09:40.540</a></span> | <span class="t">sort of knew it, but rather we're just sort of conditioning on the prompt to know what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=585" target="_blank">00:09:45.260</a></span> | <span class="t">it is that we should then generate. Right. Uh, but, but it seems to me like that's yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=589" target="_blank">00:09:49.740</a></span> | <span class="t">I think that's on a spectrum though, because you can, you can also go and give like completely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=594" target="_blank">00:09:54.980</a></span> | <span class="t">nonsensical problems where the model would never have seen, um, see it like mimic this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=600" target="_blank">00:10:00.020</a></span> | <span class="t">function and give a couple of examples of the function and the models never seen it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=602" target="_blank">00:10:02.580</a></span> | <span class="t">before. And it can go and do that later in the context. Um, and I think, I think what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=606" target="_blank">00:10:06.620</a></span> | <span class="t">you did learn, um, in a lot of these cases, so you might not have, you might have, um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=611" target="_blank">00:10:11.500</a></span> | <span class="t">you might not have learned arithmetic, like you might've had some innate faculty for arithmetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=614" target="_blank">00:10:14.540</a></span> | <span class="t">that you're using, but you might've learned, Oh, okay. Right now we're, we're doing arithmetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=618" target="_blank">00:10:18.020</a></span> | <span class="t">problems. Um, in any case, this is, I agree that there's like an element of semantics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=622" target="_blank">00:10:22.900</a></span> | <span class="t">here. Um, yeah, no, this is helpful though, just to clarify exactly sort of what the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=627" target="_blank">00:10:27.060</a></span> | <span class="t">what you remember in context learning. Thank you for walking through it. Of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=633" target="_blank">00:10:33.460</a></span> | <span class="t">So something that's, I think, really striking about all of this, um, is well, okay. So we,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=638" target="_blank">00:10:38.380</a></span> | <span class="t">we've talked about how we can, we can sort of look at the learning curve and we can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=641" target="_blank">00:10:41.820</a></span> | <span class="t">look at this, this in-context learning curve, but really those are just two slices of a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=645" target="_blank">00:10:45.860</a></span> | <span class="t">two-dimensional, uh, space. So like the, the, in some sense, the more fundamental thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=650" target="_blank">00:10:50.460</a></span> | <span class="t">is how good are we at producing the nth token at a different given point in training and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=655" target="_blank">00:10:55.500</a></span> | <span class="t">something that you'll notice if you, if you look at this. Um, so when we were, when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=658" target="_blank">00:10:58.940</a></span> | <span class="t">talk about the loss curve, we're, we're just talking about, if you average over this dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=662" target="_blank">00:11:02.860</a></span> | <span class="t">if you, if you like average like this and, and project onto the, the training step, that's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=667" target="_blank">00:11:07.620</a></span> | <span class="t">that's your loss curve. Um, and, uh, if you, the thing that we are calling the in-context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=672" target="_blank">00:11:12.100</a></span> | <span class="t">learning curve is just this line. Um, uh, yeah, this, this line, uh, down the, the end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=678" target="_blank">00:11:18.340</a></span> | <span class="t">here. Um, and something that's, that's kind of striking is there's, there's this discontinuity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=685" target="_blank">00:11:25.220</a></span> | <span class="t">in it. Um, like there's this point where, where, you know, the model seems to get radically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=690" target="_blank">00:11:30.460</a></span> | <span class="t">better in a very, very short timestamp span and going in predicting late tokens. So it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=695" target="_blank">00:11:35.820</a></span> | <span class="t">doesn't, it's not that different in early time steps, but in late time steps, suddenly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=699" target="_blank">00:11:39.300</a></span> | <span class="t">you get better. And a way that you can make this more striking is you can, you can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=706" target="_blank">00:11:46.980</a></span> | <span class="t">the difference in, in your ability to predict the 50th token and your ability to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=711" target="_blank">00:11:51.580</a></span> | <span class="t">the 500th token. You can subtract from the, the, the 500th token, the 50th token loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=717" target="_blank">00:11:57.140</a></span> | <span class="t">And what you see, um, is that over the course of training, you know, you're, you're, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=722" target="_blank">00:12:02.660</a></span> | <span class="t">not very good at this and you got a little bit better. And then suddenly you have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=725" target="_blank">00:12:05.900</a></span> | <span class="t">cliff and then you never get better. The difference between these at least never gets better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=730" target="_blank">00:12:10.540</a></span> | <span class="t">So the model gets better at predicting things, but it's ability to go and predict late tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=735" target="_blank">00:12:15.220</a></span> | <span class="t">over early tokens never gets better. And so there's in the span of just a few hundred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=740" target="_blank">00:12:20.260</a></span> | <span class="t">steps in training, the model has gotten radically better at its ability to go and, and do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=745" target="_blank">00:12:25.900</a></span> | <span class="t">kind of in-context learning. And so you might ask, you know, what's going on at that point?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=751" target="_blank">00:12:31.580</a></span> | <span class="t">Um, and this is just one model, but, um, well, so first of all, it's worth noting, this isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=755" target="_blank">00:12:35.820</a></span> | <span class="t">a small, a small change. Um, so, um, the, you can, we don't think about this very often,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=761" target="_blank">00:12:41.940</a></span> | <span class="t">but you know, often we just look at law schools and we're like, did the model do better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=764" target="_blank">00:12:44.540</a></span> | <span class="t">another model or worse than another model. But, um, you can, you can think about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=767" target="_blank">00:12:47.580</a></span> | <span class="t">as in terms of Nats and that are, are, you know, it's just the information theoretic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=771" target="_blank">00:12:51.540</a></span> | <span class="t">quantity in that. Um, and you can convert that into, to bits. And so like one, one way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=776" target="_blank">00:12:56.540</a></span> | <span class="t">you can interpret this as it's, it's something roughly like, you know, the model 0.4 Nats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=781" target="_blank">00:13:01.220</a></span> | <span class="t">is about 0.5 bits is about, uh, like every other token, the model gets to go and sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=785" target="_blank">00:13:05.380</a></span> | <span class="t">twice, um, and pick the better one. It's actually, it's even stronger than that, but that's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=789" target="_blank">00:13:09.380</a></span> | <span class="t">sort of an underestimate of how big a deal going and getting better by 0.4 Nats. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=793" target="_blank">00:13:13.980</a></span> | <span class="t">is like a real big difference in the models ability to go and predict late tokens. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=800" target="_blank">00:13:20.980</a></span> | <span class="t">and we can visualize this in different ways. We can, we can also go and ask, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=803" target="_blank">00:13:23.780</a></span> | <span class="t">how much better are we getting at going and predicting later tokens and look at the derivative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=808" target="_blank">00:13:28.460</a></span> | <span class="t">And then we, we can see very clearly that there's, there's some kind of discontinuity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=811" target="_blank">00:13:31.740</a></span> | <span class="t">in that derivative at this point. And we can take the second derivative then, and we can,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=816" target="_blank">00:13:36.260</a></span> | <span class="t">um, with, well, derivative with respect to training. And now we see that there's like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=820" target="_blank">00:13:40.780</a></span> | <span class="t">there's very, very clearly this, this, this line here. So something in just the span of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=825" target="_blank">00:13:45.140</a></span> | <span class="t">a few steps, a few hundred steps is, is causing some big change. Um, we have some kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=830" target="_blank">00:13:50.060</a></span> | <span class="t">phase change going on. Um, and this is true across model sizes. Um, uh, you can, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=836" target="_blank">00:13:56.260</a></span> | <span class="t">actually see it a little bit in the loss curve and there's this little bump here, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=839" target="_blank">00:13:59.820</a></span> | <span class="t">corresponds to the point where you have this, you have this change. We, we actually could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=843" target="_blank">00:14:03.540</a></span> | <span class="t">have seen in the loss curve earlier too. It's, it's this bump here, excuse me. So, so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=849" target="_blank">00:14:09.940</a></span> | <span class="t">have this phase change going on and there's a, I think a really tempting theory to have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=853" target="_blank">00:14:13.940</a></span> | <span class="t">which is that somehow, whatever, you know, there, there's some, this, this, this change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=857" target="_blank">00:14:17.860</a></span> | <span class="t">in the model's output and its behaviors and it's in a, in a, in a, in a sort of outward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=861" target="_blank">00:14:21.820</a></span> | <span class="t">facing properties corresponds presumably to some kind of change in the algorithms that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=866" target="_blank">00:14:26.460</a></span> | <span class="t">are running inside the model. So if we observe this big phase change, especially in a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=870" target="_blank">00:14:30.700</a></span> | <span class="t">small window, um, in, in the model's behavior, presumably there's some change in the circuits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=875" target="_blank">00:14:35.620</a></span> | <span class="t">inside the model that is driving. At least that's a, you know, a natural hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=880" target="_blank">00:14:40.340</a></span> | <span class="t">So, um, if we want to ask that though, we need to go and be able to understand, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=884" target="_blank">00:14:44.340</a></span> | <span class="t">know, what are the algorithms that's running inside the model? How can we turn the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=887" target="_blank">00:14:47.620</a></span> | <span class="t">in the model back into this algorithm? So that's going to be our goal. Um, now it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=891" target="_blank">00:14:51.940</a></span> | <span class="t">going to recover us, require us to cover a lot of ground, um, in a relatively short amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=895" target="_blank">00:14:55.940</a></span> | <span class="t">of time. So I'm going to go a little bit quickly through the next section and I will highlight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=900" target="_blank">00:15:00.380</a></span> | <span class="t">sort of the, the key takeaways, and then I will be very happy, um, to go and, uh, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=905" target="_blank">00:15:05.980</a></span> | <span class="t">know, explore any of this in as much depth. I'm free for another hour after this call.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=910" target="_blank">00:15:10.220</a></span> | <span class="t">Um, and just happy to talk in as much depth as people want about the details of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=914" target="_blank">00:15:14.940</a></span> | <span class="t">So, um, it turns out the space change doesn't happen in a one-layer attentionally transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=920" target="_blank">00:15:20.620</a></span> | <span class="t">and it does happen in a two-layer attentionally transformer. So if we could understand a one-layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=924" target="_blank">00:15:24.460</a></span> | <span class="t">attentionally transformer and a two-layer only attention, attentionally transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=928" target="_blank">00:15:28.360</a></span> | <span class="t">that might give us a pretty big clue as to what's going on. Um, so we're attention only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=935" target="_blank">00:15:35.780</a></span> | <span class="t">We're also going to leave out layer norm and biases to simplify things. So, you know, you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=939" target="_blank">00:15:39.580</a></span> | <span class="t">one way you could describe a attention only transformer is we're going to embed our tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=945" target="_blank">00:15:45.460</a></span> | <span class="t">and then we're going to apply a bunch of attention heads and add them into the residual stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=949" target="_blank">00:15:49.580</a></span> | <span class="t">and then apply our unembedding and that'll give us our logits. And we could go and write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=954" target="_blank">00:15:54.140</a></span> | <span class="t">that out as equations if we want, multiply it by an embedding matrix, apply attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=958" target="_blank">00:15:58.160</a></span> | <span class="t">heads, and then compute the logits for the unembedding. Um, and the part here that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=965" target="_blank">00:16:05.960</a></span> | <span class="t">a little tricky is, is understanding the attention heads. And this might be a somewhat conventional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=970" target="_blank">00:16:10.360</a></span> | <span class="t">way of describing attention heads. And it actually kind of obscures a lot of the structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=974" target="_blank">00:16:14.700</a></span> | <span class="t">of attention heads. Um, I think that oftentimes it's, we, we make attention heads more, more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=979" target="_blank">00:16:19.400</a></span> | <span class="t">complex than they are. We sort of hide the interesting structure. So what is this saying?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=983" target="_blank">00:16:23.480</a></span> | <span class="t">Well, it's saying, you know, for every token, compute a value vector, a value vector, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=987" target="_blank">00:16:27.480</a></span> | <span class="t">then go and mix the value vectors according to the attention matrix and then project them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=991" target="_blank">00:16:31.480</a></span> | <span class="t">with the output matrix back into the residual stream. Um, so there's, there's another notation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=997" target="_blank">00:16:37.960</a></span> | <span class="t">which you could think of this as a, as using tensor products or using, um, using, uh, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1004" target="_blank">00:16:44.040</a></span> | <span class="t">I guess there's a few, a few left and right multiplying. There's, there's a few ways you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1006" target="_blank">00:16:46.680</a></span> | <span class="t">can interpret this, but, um, I'll, I'll just sort of try to explain what this notation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1011" target="_blank">00:16:51.240</a></span> | <span class="t">means. Um, so this means for every, you know, X, our, our residual stream, we have a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1018" target="_blank">00:16:58.240</a></span> | <span class="t">for every single token, and this means go and multiply independently the vector for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1024" target="_blank">00:17:04.280</a></span> | <span class="t">each token by WV. So compute the value vector for every token. This one on the other hand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1030" target="_blank">00:17:10.440</a></span> | <span class="t">means notice that it's now on the, A is on the left-hand side. It means go and go and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1034" target="_blank">00:17:14.600</a></span> | <span class="t">multiply the, uh, attention matrix or go and go into linear combinations of values, value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1041" target="_blank">00:17:21.000</a></span> | <span class="t">vectors. So don't, don't change the value vectors, you know, point-wise, but go and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1044" target="_blank">00:17:24.280</a></span> | <span class="t">mix them together according to the attention pattern, create a weighted sum. And then again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1049" target="_blank">00:17:29.400</a></span> | <span class="t">independently for every position, go and apply the output matrix. And you can apply the distributive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1054" target="_blank">00:17:34.600</a></span> | <span class="t">property to this, and it just reveals that actually didn't matter that you did the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1058" target="_blank">00:17:38.520</a></span> | <span class="t">sort of in the middle. You could have done the attention at the beginning, you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1060" target="_blank">00:17:40.840</a></span> | <span class="t">have done it at the end. Um, that's, that's independent. Um, and the thing that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1064" target="_blank">00:17:44.920</a></span> | <span class="t">matters is there's this WVWO matrix that describes what it's really saying is, you know, WVWO</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1071" target="_blank">00:17:51.560</a></span> | <span class="t">describes what information the attention head reads from each position and how it writes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1075" target="_blank">00:17:55.160</a></span> | <span class="t">it to its destination. Whereas A describes which tokens we read from and write to. Um,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1081" target="_blank">00:18:01.320</a></span> | <span class="t">and that's, that's kind of getting more of the fundamental structure and attention. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1084" target="_blank">00:18:04.360</a></span> | <span class="t">attention head goes and moves information from one position to another and the process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1089" target="_blank">00:18:09.320</a></span> | <span class="t">of, of which position gets moved from and to is independent from what information gets moved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1093" target="_blank">00:18:13.800</a></span> | <span class="t">And if you rewrite your transformer that way, um, well, first we can go and write, uh, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1102" target="_blank">00:18:22.760</a></span> | <span class="t">sum of attention heads just as, as in this form. Um, and then we can, uh, go in and write</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1109" target="_blank">00:18:29.160</a></span> | <span class="t">that as the, the entire layer by going and adding in an identity. And if we go and plug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1115" target="_blank">00:18:35.240</a></span> | <span class="t">that all in to our transformer and go and expand, um, we, we have to go in and multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1123" target="_blank">00:18:43.640</a></span> | <span class="t">everything through. We get this interesting equation. And so we get this one term, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1128" target="_blank">00:18:48.200</a></span> | <span class="t">corresponds to just the path directly through the residual stream. And it's going to want to store,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1132" target="_blank">00:18:52.840</a></span> | <span class="t">uh, bigram statistics. It's just, you know, all it gets is the previous token and tries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1136" target="_blank">00:18:56.520</a></span> | <span class="t">to predict the next token. And so it gets to try and predict, uh, try to store bigram statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1141" target="_blank">00:19:01.720</a></span> | <span class="t">And then for every attention head, we get this matrix that says, okay, well, for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1145" target="_blank">00:19:05.240</a></span> | <span class="t">we have the attention pattern. So it looks, that describes which token looks at which token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1149" target="_blank">00:19:09.160</a></span> | <span class="t">And we have this matrix here, which describes how for every possible token you could attend to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1153" target="_blank">00:19:13.240</a></span> | <span class="t">how it affects the logics. And that's just a table that you can look at. It just says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1157" target="_blank">00:19:17.960</a></span> | <span class="t">you know, for, for this attention head, if it looks at this token, it's going to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1161" target="_blank">00:19:21.080</a></span> | <span class="t">the probability of these tokens in a one-layer attention only transformer. That's all there is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1168" target="_blank">00:19:28.920</a></span> | <span class="t">Yeah, so this is just, just the interpretation I was describing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1172" target="_blank">00:19:32.040</a></span> | <span class="t">And another thing that's worth noting is this, the, according to this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1178" target="_blank">00:19:38.280</a></span> | <span class="t">the attention only transformer is linear if you fix the attention pattern. Now, of course it's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1183" target="_blank">00:19:43.560</a></span> | <span class="t">the attention pattern isn't fixed, but whenever you have the opportunity to go and make something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1187" target="_blank">00:19:47.160</a></span> | <span class="t">linear, linear functions are really easy to understand. And so if you can fix a small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1190" target="_blank">00:19:50.600</a></span> | <span class="t">number of things and make something linear, that's actually, it's a lot of leverage. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1197" target="_blank">00:19:57.560</a></span> | <span class="t">And yeah, we can talk about how the attention pattern is computed as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1200" target="_blank">00:20:00.520</a></span> | <span class="t">Um, you, if you expand it out, you'll get an equation like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1204" target="_blank">00:20:04.040</a></span> | <span class="t">and, uh, notice, well, I think, I think it'll be easier. Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1210" target="_blank">00:20:10.200</a></span> | <span class="t">The, I think the core story there to take away from all of these is we have these two matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1217" target="_blank">00:20:17.080</a></span> | <span class="t">that actually look kind of similar. So this one here tells you if you attend to a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1222" target="_blank">00:20:22.680</a></span> | <span class="t">how are the logics affected? And it's, you can just think of it as a giant matrix of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1226" target="_blank">00:20:26.920</a></span> | <span class="t">for every possible token input token, how, how is the logic, how are the logics affected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1231" target="_blank">00:20:31.160</a></span> | <span class="t">by that token? Are they made more likely or less likely? And we have this one, which sort of says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1236" target="_blank">00:20:36.360</a></span> | <span class="t">how much does every token want to attend to every other token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1239" target="_blank">00:20:39.320</a></span> | <span class="t">Um, one way that you can, you can picture this is, uh, okay, that's really, there's really three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1248" target="_blank">00:20:48.280</a></span> | <span class="t">tokens involved when, when we're thinking about an attention head, we have the token that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1254" target="_blank">00:20:54.440</a></span> | <span class="t">going to move information to, and that's attending backwards. We have the source token that's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1259" target="_blank">00:20:59.960</a></span> | <span class="t">to get attended to, and we have the output token whose logics are going to be affected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1263" target="_blank">00:21:03.720</a></span> | <span class="t">And you can just trace through this. So you can ask what happens. Um, how, how does the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1269" target="_blank">00:21:09.560</a></span> | <span class="t">the attending to this token affect the output? Well, first we embed the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1273" target="_blank">00:21:13.720</a></span> | <span class="t">Then we multiply by WV to get the value vector. The information gets moved by the attention pattern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1280" target="_blank">00:21:20.920</a></span> | <span class="t">We multiply by W O to add it back into the residual stream, but yet hit by the unembedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1284" target="_blank">00:21:24.920</a></span> | <span class="t">and we affect the logics. And that's where that one matrix comes from. And we can also ask, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1288" target="_blank">00:21:28.920</a></span> | <span class="t">know, what decides, you know, whether a token gets a high score when we're, when we're computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1293" target="_blank">00:21:33.560</a></span> | <span class="t">attention pattern. And it just says, you know, embed, embed the token, turn it into a query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1300" target="_blank">00:21:40.280</a></span> | <span class="t">embed the other token, turn it into a key and dot product them and see you that's where those,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1305" target="_blank">00:21:45.160</a></span> | <span class="t">those two matrices come from. So I know that I'm going quite quickly. Um, maybe I'll just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1313" target="_blank">00:21:53.000</a></span> | <span class="t">briefly pause here. And if anyone wants to ask for clarifications, um, this would be a good time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1317" target="_blank">00:21:57.800</a></span> | <span class="t">And then we'll actually go in reverse engineer and say, you know, everything that's going on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1321" target="_blank">00:22:01.880</a></span> | <span class="t">in a one-layer attentionally transformer is now in the palm of our hands. It's a very toy model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1326" target="_blank">00:22:06.840</a></span> | <span class="t">No one actually uses one-layer attentionally transformers, but we'll be able to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1331" target="_blank">00:22:11.720</a></span> | <span class="t">the one-layer attentionally transformer. So just to be clear, so you're saying that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1337" target="_blank">00:22:17.880</a></span> | <span class="t">the quite key circuit is learning the attention baits and like essentially is responsible for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1344" target="_blank">00:22:24.440</a></span> | <span class="t">running the sort of like the attention between different tokens. Yeah. Yeah. So, so this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1349" target="_blank">00:22:29.800</a></span> | <span class="t">this matrix, when it, yeah. And, you know, all, all three of those parts are, are learned, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1354" target="_blank">00:22:34.280</a></span> | <span class="t">that's, that's what expresses whether a attention pattern. Yeah. That's what generates the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1359" target="_blank">00:22:39.560</a></span> | <span class="t">patterns gets run for every pair of tokens. And you can, you can, you can think of values in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1363" target="_blank">00:22:43.720</a></span> | <span class="t">matrix as just being how much every token wants to attend to every other token. If it was in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1368" target="_blank">00:22:48.040</a></span> | <span class="t">context, um, we're, we're doing positional learnings here. So there's a little bit that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1371" target="_blank">00:22:51.480</a></span> | <span class="t">we're sort of aligning over there as well, but sort of in, in a global sense, how much does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1375" target="_blank">00:22:55.240</a></span> | <span class="t">every token want to attend to every other token? Right. Yeah. And the other circuit, like the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1378" target="_blank">00:22:58.840</a></span> | <span class="t">optimal circuit is using the attention that's calculated to yes. Um, like affect the final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1386" target="_blank">00:23:06.920</a></span> | <span class="t">outputs. It's sort of saying, if, if the attention had assumed that the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1390" target="_blank">00:23:10.600</a></span> | <span class="t">had attends to some token. So let's set aside the question of how that gets computed. Just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1394" target="_blank">00:23:14.040</a></span> | <span class="t">assume that it tends to some token, how would it affect the outputs if it attended to that token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1398" target="_blank">00:23:18.200</a></span> | <span class="t">And you can just, you can just calculate that. Um, it's just a, a big table of values that says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1403" target="_blank">00:23:23.400</a></span> | <span class="t">you know, for this token, it's going to make this token more likely, this token will make this token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1407" target="_blank">00:23:27.320</a></span> | <span class="t">less likely. Okay. Okay. And it's completely independent. Like it's just two separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1413" target="_blank">00:23:33.400</a></span> | <span class="t">matrices. They're, they're not, you know, the, the formulas that might make them seem entangled,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1418" target="_blank">00:23:38.200</a></span> | <span class="t">but they're actually separate. Right. To me, it seems like the, um, like just supervision is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1423" target="_blank">00:23:43.880</a></span> | <span class="t">coming from the output value circuit and the query key circuit seems to be more like unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1427" target="_blank">00:23:47.960</a></span> | <span class="t">kind of thing. Cause there's no. Hmm. I mean, there are just, I think in the sense that every,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1434" target="_blank">00:23:54.440</a></span> | <span class="t">in, in, in a model, like every, every neuron is in some sense, you know, like a signal is, is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1440" target="_blank">00:24:00.760</a></span> | <span class="t">is somehow downstream from the ultimate, the ultimate signal. And so, you know, the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1445" target="_blank">00:24:05.240</a></span> | <span class="t">value signal, the output value circuit is getting more, more direct is perhaps getting more direct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1449" target="_blank">00:24:09.720</a></span> | <span class="t">signal. Correct. Um, but yeah. We will be able to dig into this in lots of detail in as much detail</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1459" target="_blank">00:24:19.800</a></span> | <span class="t">as you want, uh, in a little bit. So we can, um, maybe I'll push forward. And I think also actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1464" target="_blank">00:24:24.920</a></span> | <span class="t">an example of how to use this to reverse engineer a one layer model will maybe make it a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1468" target="_blank">00:24:28.840</a></span> | <span class="t">more, more motivated. Okay. So, um, just, just to emphasize this, there's three different tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1476" target="_blank">00:24:36.840</a></span> | <span class="t">that we can talk about. There's a token that gets attended to there's the token that does the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1480" target="_blank">00:24:40.680</a></span> | <span class="t">attention to call it the destination. And then there's the token that gets affected, get it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1484" target="_blank">00:24:44.360</a></span> | <span class="t">get the next token, which it's probabilities are affected. Um, and so something we can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1490" target="_blank">00:24:50.840</a></span> | <span class="t">is notice that the, the only token that connects to both of these is the token that gets attended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1495" target="_blank">00:24:55.400</a></span> | <span class="t">to. So these two are sort of, they're, they're bridged by their, their interaction with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1501" target="_blank">00:25:01.080</a></span> | <span class="t">source token. So something that's kind of natural is to ask for a given source token, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1506" target="_blank">00:25:06.120</a></span> | <span class="t">how does it interact with both of these? So let's, let's take, for instance, the token perfect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1511" target="_blank">00:25:11.880</a></span> | <span class="t">which tokens, one thing we can ask is which tokens want to attend to perfect. Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1519" target="_blank">00:25:19.480</a></span> | <span class="t">apparently the tokens that most want to attend to perfect are, are, and looks and is, and provides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1526" target="_blank">00:25:26.200</a></span> | <span class="t">Um, so R is the most looks as the next most and so on. And then when we attend to perfect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1530" target="_blank">00:25:30.760</a></span> | <span class="t">and this is with one, one single attention, and so, you know, it'd be different if we did a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1533" target="_blank">00:25:33.960</a></span> | <span class="t">different attention, attention, and it wants to really increase the probability of perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1537" target="_blank">00:25:37.720</a></span> | <span class="t">And then to a lesser extent, super and absolute and cure. And we can ask, you know, what, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1544" target="_blank">00:25:44.120</a></span> | <span class="t">sequences of tokens are made more likely by this, this particular, um, set of, you know, this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1551" target="_blank">00:25:51.080</a></span> | <span class="t">this particular set of things wanting to attend to each other and, and becoming more likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1554" target="_blank">00:25:54.440</a></span> | <span class="t">Well, things are the form we have our, our token that we attended back to, and we have some,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1559" target="_blank">00:25:59.880</a></span> | <span class="t">some skip of some number of tokens. They don't have to be adjacent, but then later on, we see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1564" target="_blank">00:26:04.200</a></span> | <span class="t">the token R and it tends back to perfect and increases the probability of perfect. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1569" target="_blank">00:26:09.560</a></span> | <span class="t">can, you can think of these as being like, we're, we're sort of creating, changing the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1573" target="_blank">00:26:13.160</a></span> | <span class="t">of what we might call, might call skip trigrams, where we have, you know, we skip over a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1577" target="_blank">00:26:17.320</a></span> | <span class="t">tokens in the middle, but we're, we're affecting the probability really of, of trigrams. So perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1582" target="_blank">00:26:22.760</a></span> | <span class="t">are perfect, perfect, look super. Um, we can look at another one. So we have the token large,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1588" target="_blank">00:26:28.120</a></span> | <span class="t">um, these tokens contains using specify, want to go and look back to it and an increase of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1592" target="_blank">00:26:32.840</a></span> | <span class="t">probability of large and small. And the skip trigrams that are affected are things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1597" target="_blank">00:26:37.240</a></span> | <span class="t">large using large, large contains small, um, and things like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1602" target="_blank">00:26:42.680</a></span> | <span class="t">Um, if we see the number two, um, we increase the probability of other numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1608" target="_blank">00:26:48.760</a></span> | <span class="t">and we affect probably tokens or skip diagrams, like two, one, two, two has three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1615" target="_blank">00:26:55.320</a></span> | <span class="t">Um, now you're, you're all in, uh, in a technical field. So you'll probably recognize this one. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1622" target="_blank">00:27:02.040</a></span> | <span class="t">have, uh, have Lambda and then we see backslash and then we want to increase the probability of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1628" target="_blank">00:27:08.040</a></span> | <span class="t">Lambda and sorted and Lambda and operator. So it's all, it's all latex. Um, it wants to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1633" target="_blank">00:27:13.000</a></span> | <span class="t">um, it's, if it sees Lambda, it thinks that, you know, maybe next time I use a backslash,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1636" target="_blank">00:27:16.840</a></span> | <span class="t">I should go and put in some latex, uh, math symbol. Um, also same thing for HTML. We see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1644" target="_blank">00:27:24.680</a></span> | <span class="t">NBSP for non-breaking space. And then we see an ampersand. We want to go and make that more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1648" target="_blank">00:27:28.680</a></span> | <span class="t">likely. The takeaway from all of this is that a one-layer attention only transformer is totally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1653" target="_blank">00:27:33.240</a></span> | <span class="t">acting on these skip trigrams. Um, every, everything that it does, I mean, I guess it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1658" target="_blank">00:27:38.920</a></span> | <span class="t">also has this pathway by which it affects by grams, but mostly it's just affecting these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1661" target="_blank">00:27:41.720</a></span> | <span class="t">skip trigrams. Um, and there's lots of them. It's just like these giant tables of skip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1666" target="_blank">00:27:46.280</a></span> | <span class="t">trigrams that are made more or less likely. Um, there's lots of other fun things that does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1672" target="_blank">00:27:52.520</a></span> | <span class="t">Sometimes the tokenization will split up a word in multiple ways. So, um, like we have indie,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1677" target="_blank">00:27:57.000</a></span> | <span class="t">uh, well, that's, that's not a good example. We have like the word pike, and then we, we see the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1682" target="_blank">00:28:02.280</a></span> | <span class="t">token P and then we predict Ike, um, and we predict spikes and stuff like that. Um, or, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1689" target="_blank">00:28:09.480</a></span> | <span class="t">these, these ones are kind of fun. Maybe they're actually worth talking about for a second. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1692" target="_blank">00:28:12.600</a></span> | <span class="t">we see the token Lloyd, and then we see an L and maybe we predict Lloyd, um, or R and we predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1700" target="_blank">00:28:20.600</a></span> | <span class="t">Ralph, um, C Catherine. Um, but we'll see in a second that, well, yeah, we'll, we'll come back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1706" target="_blank">00:28:26.280</a></span> | <span class="t">to that in a sec. So we, we increased the probability of things like Lloyd, Lloyd,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1709" target="_blank">00:28:29.560</a></span> | <span class="t">and Lloyd Catherine or picks map. Um, if anyone's worked with QT, um, it's, we see picks map and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1715" target="_blank">00:28:35.720</a></span> | <span class="t">increased the probability of, um, P X map again, but also Q canvas. Um, yeah, but of course there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1727" target="_blank">00:28:47.880</a></span> | <span class="t">a problem with this, which is, um, it doesn't get to pick which one of these goes with which one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1732" target="_blank">00:28:52.280</a></span> | <span class="t">So if you want to go and make picks map, picks map, and picks map Q canvas more probable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1739" target="_blank">00:28:59.560</a></span> | <span class="t">you also have to go and create, make picks map, picks map, P canvas more probable. And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1744" target="_blank">00:29:04.920</a></span> | <span class="t">want to make Lloyd, Lloyd, and Lloyd Catherine more probable, you also have to make Lloyd,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1750" target="_blank">00:29:10.120</a></span> | <span class="t">Cloyd, and Lloyd Lathren more probable. And so there's actually like bugs that transformers have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1756" target="_blank">00:29:16.840</a></span> | <span class="t">like weird, at least in, you know, in these, these really tiny one-layer attention only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1760" target="_blank">00:29:20.360</a></span> | <span class="t">transformers there, there's these bugs that, you know, they seem weird until you realize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1764" target="_blank">00:29:24.120</a></span> | <span class="t">that it's this giant table of skip trigrams that's, that's operating. Um, and the, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1769" target="_blank">00:29:29.640</a></span> | <span class="t">nature of that is that you're going to be, um, uh, yeah, you, you, it sort of forces you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1775" target="_blank">00:29:35.560</a></span> | <span class="t">if you want to go and do this to go in and also make some weird predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1778" target="_blank">00:29:38.280</a></span> | <span class="t">Is there a reason why the source tokens here have a space before the first character?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1786" target="_blank">00:29:46.200</a></span> | <span class="t">Yes. Um, that's just the, I was giving examples where the tokenization breaks in a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1792" target="_blank">00:29:52.440</a></span> | <span class="t">way and because spaces get included in the tokenization, um, when there's a space in front</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1799" target="_blank">00:29:59.560</a></span> | <span class="t">of something and then there's an example where the space isn't in front of it, they can get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1803" target="_blank">00:30:03.080</a></span> | <span class="t">tokenized in different ways. Got it. Cool. Thanks. Yeah. Great question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1807" target="_blank">00:30:07.400</a></span> | <span class="t">Um, okay. So some, just to abstract away some common patterns that we're seeing, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1815" target="_blank">00:30:15.240</a></span> | <span class="t">um, one pretty common thing is what you might describe as like B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1819" target="_blank">00:30:19.640</a></span> | <span class="t">AB. So you're, you go and you, you see some token and then you'll see another token that might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1824" target="_blank">00:30:24.360</a></span> | <span class="t">proceed that token. And then you're like, ah, probably the token that I saw earlier is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1827" target="_blank">00:30:27.480</a></span> | <span class="t">to occur again. Um, or sometimes you, you predict a slightly different token. So like me, maybe an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1833" target="_blank">00:30:33.880</a></span> | <span class="t">example of the first one is two, one, two, but you could also do two has three. And so three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1840" target="_blank">00:30:40.440</a></span> | <span class="t">isn't the same as two, but it's kind of similar. So that's, that's one thing. Another one is this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1844" target="_blank">00:30:44.040</a></span> | <span class="t">this example where you have a token that something that's tokenized together one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1846" target="_blank">00:30:46.680</a></span> | <span class="t">time and that's split apart. So you see the token and then you see something that might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1851" target="_blank">00:30:51.000</a></span> | <span class="t">be the first part of the token and then you predict the second part. Um, I think the thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1857" target="_blank">00:30:57.560</a></span> | <span class="t">that's really striking about this is these are all in some ways are really crude, kind of in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1862" target="_blank">00:31:02.920</a></span> | <span class="t">context learning. Um, and in particular, these models get about 0.1 Nats rather than about 0.4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1870" target="_blank">00:31:10.040</a></span> | <span class="t">Nats up in context learning, and they never go through the phase change. So they're doing some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1873" target="_blank">00:31:13.960</a></span> | <span class="t">kind of really crude in context learning. And also they're dedicating almost all their attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1878" target="_blank">00:31:18.280</a></span> | <span class="t">heads to this kind of crude in context learning. So they're not very good at it, but they're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1881" target="_blank">00:31:21.880</a></span> | <span class="t">they're, they're dedicating their, um, their capacity to it. Uh, I'm noticing that it's 1037.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1888" target="_blank">00:31:28.200</a></span> | <span class="t">Um, I, I want to just check how long I can go. Cause I, maybe I should like super accelerate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1892" target="_blank">00:31:32.920</a></span> | <span class="t">Of course. Uh, I think it's fine because like students are also asking questions in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1898" target="_blank">00:31:38.840</a></span> | <span class="t">So you should be good. Okay. So maybe my plan will be, but I'll talk until like 1055 or 11.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1904" target="_blank">00:31:44.920</a></span> | <span class="t">And then if you want, I can go and answer questions for a while after, after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1909" target="_blank">00:31:49.880</a></span> | <span class="t">Yeah, it works. Fantastic. So you can see this as a very crude, kind of in context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1916" target="_blank">00:31:56.200</a></span> | <span class="t">Like basically what we're saying is it's sort of all of this labor of, okay, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1919" target="_blank">00:31:59.480</a></span> | <span class="t">I saw this token, probably these other tokens, the same token or similar tokens are more likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1923" target="_blank">00:32:03.560</a></span> | <span class="t">to go and occur later. And look, this is an opportunity that sort of looks like I could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1927" target="_blank">00:32:07.160</a></span> | <span class="t">inject the token that I saw earlier. I'm going to inject it here and say that it's more likely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1930" target="_blank">00:32:10.680</a></span> | <span class="t">that's like, that's basically what it's doing. And it's dedicating almost all of its capacity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1935" target="_blank">00:32:15.320</a></span> | <span class="t">to that. So, you know, these, it's sort of the opposite of what we thought with RNNs in the past,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1938" target="_blank">00:32:18.840</a></span> | <span class="t">like used to be that everyone was like, oh, you know, RNNs it's so hard to care about long distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1943" target="_blank">00:32:23.720</a></span> | <span class="t">contacts. You know, maybe we need to go and like use dams or something. No, if you, if you train a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1948" target="_blank">00:32:28.360</a></span> | <span class="t">transformer, it dedicates and you give it a long, a long enough context, it's dedicating almost all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1952" target="_blank">00:32:32.280</a></span> | <span class="t">of its capacity to this type of stuff. Just kind of interesting. There are some attentionants which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1960" target="_blank">00:32:40.120</a></span> | <span class="t">are more primarily positional. Usually we, you know, the model that I've been training that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1965" target="_blank">00:32:45.320</a></span> | <span class="t">two layer or it's only a one layer model has 12 attentionants and usually around two or three of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1969" target="_blank">00:32:49.640</a></span> | <span class="t">those will become these more positional sort of shorter term things that do something more like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1973" target="_blank">00:32:53.640</a></span> | <span class="t">like local trigram statistics and then everything else becomes these skip trigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1981" target="_blank">00:33:01.480</a></span> | <span class="t">Yeah, so some takeaways from this. Yeah, you can, you can understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1985" target="_blank">00:33:05.800</a></span> | <span class="t">one layer attentionally transformers in terms of these OV and QK circuits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1989" target="_blank">00:33:09.400</a></span> | <span class="t">Transformers desperately want to do in context learning. They desperately, desperately, desperately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1995" target="_blank">00:33:15.400</a></span> | <span class="t">want to go and look at these long distance contacts and go and predict things. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=1999" target="_blank">00:33:19.480</a></span> | <span class="t">just so much, so much entropy that they can go and reduce out of that. The constraints of a one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2004" target="_blank">00:33:24.760</a></span> | <span class="t">layer attentionally transformer force it to make certain bugs if it wants to do the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2009" target="_blank">00:33:29.400</a></span> | <span class="t">And if you freeze the attention patterns, these models are linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2012" target="_blank">00:33:32.440</a></span> | <span class="t">Okay. A quick aside, because so far this type of work has required us to do a lot of very manual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2020" target="_blank">00:33:40.760</a></span> | <span class="t">inspection. Like we're looking through these giant matrices, but there's a way that we can escape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2024" target="_blank">00:33:44.440</a></span> | <span class="t">that. We don't have to use, look at these giant matrices if we don't want to. We can use eigen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2029" target="_blank">00:33:49.080</a></span> | <span class="t">values and eigen vectors. So recall that an eigen value and an eigen vector just means that if you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2034" target="_blank">00:33:54.920</a></span> | <span class="t">if you multiply that vector by the matrix, it's equivalent to just scaling. And often in my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2042" target="_blank">00:34:02.600</a></span> | <span class="t">experience, this hasn't been very useful for interpretability because we're, we're usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2045" target="_blank">00:34:05.240</a></span> | <span class="t">mapping between different spaces. But if you're mapping onto the same space, eigen values and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2048" target="_blank">00:34:08.920</a></span> | <span class="t">eigen vectors are a beautiful way to think about this. So we're going to draw them on a, a radial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2056" target="_blank">00:34:16.440</a></span> | <span class="t">plot. And we're going to have a log radial scale because they're going to vary, their magnitude's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2061" target="_blank">00:34:21.880</a></span> | <span class="t">going to vary by many orders of magnitude. Okay. So we can just go and, you know, our,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2067" target="_blank">00:34:27.720</a></span> | <span class="t">our OB circuit maps from tokens to tokens. That's the same vector space on the input and the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2072" target="_blank">00:34:32.280</a></span> | <span class="t">And we can ask, you know, what does it mean if we see eigen values of a particular kind? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2076" target="_blank">00:34:36.760</a></span> | <span class="t">positive eigen values, and this is really the most important part, mean copying. So if you have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2080" target="_blank">00:34:40.840</a></span> | <span class="t">positive eigen value, it means that there's some set of, of tokens where if you, if you see them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2085" target="_blank">00:34:45.880</a></span> | <span class="t">you increase their probability. And if you have a lot of positive eigen values, you're doing a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2090" target="_blank">00:34:50.840</a></span> | <span class="t">of copying. If you only have positive eigen values, everything you do is copying. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2095" target="_blank">00:34:55.480</a></span> | <span class="t">imaginary eigen values mean that you see a token and then you want to go and increase the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2099" target="_blank">00:34:59.160</a></span> | <span class="t">of unrelated tokens. And finally, negative eigen values are anti-copying. They're like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2103" target="_blank">00:35:03.000</a></span> | <span class="t">if you see this token, you make it less probable in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2105" target="_blank">00:35:05.240</a></span> | <span class="t">Well, that's really nice because now we don't have to go and dig through these giant matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2110" target="_blank">00:35:10.440</a></span> | <span class="t">that are vocab size by vocab size. We can just look at the eigen values. And so these are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2115" target="_blank">00:35:15.560</a></span> | <span class="t">eigen values for our one-layer attentionally transformer. And we can see that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2120" target="_blank">00:35:20.280</a></span> | <span class="t">for many of these, they're almost entirely positive. These ones are, are sort of entirely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2126" target="_blank">00:35:26.680</a></span> | <span class="t">positive. These ones are almost entirely positive. And then really these ones are even almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2130" target="_blank">00:35:30.840</a></span> | <span class="t">entirely positive. And there's only two that have a significant number of imaginary and negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2136" target="_blank">00:35:36.120</a></span> | <span class="t">eigen values. And so what this is telling us is it's, it's just in one picture, we can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2140" target="_blank">00:35:40.680</a></span> | <span class="t">you know, OK, they're really, you know, 10 out of 12 of these, of these attention heads are just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2146" target="_blank">00:35:46.440</a></span> | <span class="t">doing copying. They just, they just are doing this long distance, you know, well, I saw a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2150" target="_blank">00:35:50.280</a></span> | <span class="t">probably it's going to occur again, type stuff. That's kind of cool. We can, we can summarize it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2154" target="_blank">00:35:54.280</a></span> | <span class="t">really quickly. OK. Now, the other thing that you can, yeah, so this is, this is for a second,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2161" target="_blank">00:36:01.400</a></span> | <span class="t">we're going to look at a two-layer model in a second. And we'll, we'll see that also a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2164" target="_blank">00:36:04.520</a></span> | <span class="t">its heads are doing this kind of copying-ish stuff. They have large positive eigenvalues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2168" target="_blank">00:36:08.440</a></span> | <span class="t">You can do a histogram, like, you know, one, one thing that's cool is you can just add up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2173" target="_blank">00:36:13.960</a></span> | <span class="t">the eigenvalues and divide them by their absolute values. And you've got a number between zero and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2177" target="_blank">00:36:17.800</a></span> | <span class="t">one, which is like how copying, how copying is just the head, or between negative one and one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2181" target="_blank">00:36:21.400</a></span> | <span class="t">how copying is just the head. And you can just do a histogram and you can see, oh yeah, almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2185" target="_blank">00:36:25.080</a></span> | <span class="t">all of the heads are doing, doing lots of copying. You know, it's nice to be able to go and summarize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2189" target="_blank">00:36:29.640</a></span> | <span class="t">your model in a, and I think this is sort of like we've gone for a very bottom-up way. And we didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2195" target="_blank">00:36:35.160</a></span> | <span class="t">start with assumptions about what the model was doing. We tried to understand its structure. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2198" target="_blank">00:36:38.040</a></span> | <span class="t">then we were able to summarize it in useful ways. And now we're able to go and say something about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2201" target="_blank">00:36:41.160</a></span> | <span class="t">it. Now, another thing you might ask is what do the eigenvalues of a QK circuit mean? And in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2208" target="_blank">00:36:48.440</a></span> | <span class="t">example so far, they haven't been that, they wouldn't have been that interesting. But in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2212" target="_blank">00:36:52.280</a></span> | <span class="t">minute, they will be. And so I'll briefly describe what they mean. A positive eigenvalue would mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2215" target="_blank">00:36:55.640</a></span> | <span class="t">you want to attend to the same tokens. An imaginary eigenvalue, and this is what you would mostly see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2220" target="_blank">00:37:00.760</a></span> | <span class="t">in the models that we've seen so far, means you want to go in and attend to a unrelated or different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2225" target="_blank">00:37:05.560</a></span> | <span class="t">token. And a negative eigenvalue would mean you want to avoid attending to the same token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2229" target="_blank">00:37:09.960</a></span> | <span class="t">So that will be relevant in a second. Yeah, so those are going to mostly be useful to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2237" target="_blank">00:37:17.000</a></span> | <span class="t">about in multilayer attentional learning transformers when we can have chains of attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2240" target="_blank">00:37:20.280</a></span> | <span class="t">heads. And so we can ask, well, I'll get to that in a second. Yeah, so that's a table summarizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2245" target="_blank">00:37:25.160</a></span> | <span class="t">that. Unfortunately, this approach completely breaks down once you have MLP layers. MLP layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2250" target="_blank">00:37:30.840</a></span> | <span class="t">you know, now you have these non-linearity since you don't get this property where your model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2254" target="_blank">00:37:34.120</a></span> | <span class="t">mostly linear and you can just look at a matrix. But if you're working with only attentionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2258" target="_blank">00:37:38.280</a></span> | <span class="t">transformers, this is a very nice way to think about things. OK, so recall that one-layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2262" target="_blank">00:37:42.920</a></span> | <span class="t">attentionally transformers don't undergo this phase change that we talked about in the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2265" target="_blank">00:37:45.880</a></span> | <span class="t">Like right now, we're on a hunt. We're trying to go and answer this mystery of what the hell is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2270" target="_blank">00:37:50.200</a></span> | <span class="t">going on in that phase change where models suddenly get good at in-context learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2273" target="_blank">00:37:53.800</a></span> | <span class="t">We want to answer that. And one-layer attentionally transformers don't undergo that phase change,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2278" target="_blank">00:37:58.040</a></span> | <span class="t">but two-layer attentionally transformers do. So we'd like to know what's different about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2281" target="_blank">00:38:01.640</a></span> | <span class="t">two-layer attentionally transformers. OK, well, so in our previous-- when we were dealing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2290" target="_blank">00:38:10.280</a></span> | <span class="t">one-layer attentionally transformers, we were able to go and rewrite them in this form. And it gave</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2294" target="_blank">00:38:14.680</a></span> | <span class="t">us a lot of ability to go and understand the model because we could go and say, well, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2299" target="_blank">00:38:19.080</a></span> | <span class="t">this is bigrams. And then each one of these is looking somewhere. And we had this matrix that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2303" target="_blank">00:38:23.160</a></span> | <span class="t">describes how it affects things. And yeah, so that gave us a lot of ability to think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2309" target="_blank">00:38:29.160</a></span> | <span class="t">these things. And we can also just write in this factored form where we have the embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2313" target="_blank">00:38:33.880</a></span> | <span class="t">and then we have the attention heads, and then we have the unembedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2316" target="_blank">00:38:36.040</a></span> | <span class="t">OK, well-- oh, and for simplicity, we often go and write W-O-V for W-O-W-V because they always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2325" target="_blank">00:38:45.400</a></span> | <span class="t">come together. It's always the case. It's, in some sense, an illusion that W-O and W-V are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2329" target="_blank">00:38:49.720</a></span> | <span class="t">different matrices. They're just one low-rank matrix. They're never-- they're always used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2333" target="_blank">00:38:53.160</a></span> | <span class="t">together. And similarly, W-Q and W-K, it's sort of an illusion that they're different matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2337" target="_blank">00:38:57.400</a></span> | <span class="t">They're always just used together. And keys and queries are just sort of-- they're just an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2341" target="_blank">00:39:01.960</a></span> | <span class="t">artifact of these low-rank matrices. So in any case, it's useful to go and write those together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2346" target="_blank">00:39:06.920</a></span> | <span class="t">OK, great. So a two-layer attentionally transformer, what we do is we go through the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2352" target="_blank">00:39:12.760</a></span> | <span class="t">embedding matrix. Then we go through the layer 1 attention heads. Then we go through the layer 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2357" target="_blank">00:39:17.800</a></span> | <span class="t">attention heads. And then we go through the unembedding. And for the attention heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2361" target="_blank">00:39:21.960</a></span> | <span class="t">we always have this identity as well, which corresponds just going down the residual stream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2365" target="_blank">00:39:25.960</a></span> | <span class="t">So we can go down the residual stream, or we can go through an attention head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2369" target="_blank">00:39:29.400</a></span> | <span class="t">Next up, we can also go down the residual stream, or we can go through an attention head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2373" target="_blank">00:39:33.960</a></span> | <span class="t">And there's this useful identity, the mixed product identity that any tensor product or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2384" target="_blank">00:39:44.040</a></span> | <span class="t">other ways of interpreting this obey, which is that if you have an attention head and we have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2389" target="_blank">00:39:49.800</a></span> | <span class="t">say, we have the weights and the attention pattern and the W-O-V matrix and the attention pattern,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2394" target="_blank">00:39:54.360</a></span> | <span class="t">the attention patterns multiply together, and the O-V circuits multiply together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2398" target="_blank">00:39:58.360</a></span> | <span class="t">and they behave nicely. OK, great. So we can just expand out that equation. We can just take that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2405" target="_blank">00:40:05.000</a></span> | <span class="t">big product we had at the beginning, and we can just expand it out. And we get three different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2408" target="_blank">00:40:08.040</a></span> | <span class="t">kinds of terms. So one thing we do is we get this path that just goes directly through the residual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2412" target="_blank">00:40:12.840</a></span> | <span class="t">stream where we embed and un-embed, and that's going to want to represent some bigram statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2416" target="_blank">00:40:16.920</a></span> | <span class="t">Then we get things that look like the attention head terms that we had previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2423" target="_blank">00:40:23.720</a></span> | <span class="t">And finally, we get these terms that correspond to going through two attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2432" target="_blank">00:40:32.360</a></span> | <span class="t">Now, it's worth noting that these terms are not actually the same as--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2440" target="_blank">00:40:40.520</a></span> | <span class="t">they're-- because the attention head, the attention patterns in the second layer can be computed from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2445" target="_blank">00:40:45.320</a></span> | <span class="t">the outputs of the first layer, those are also going to be more expressive. But at a high level,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2449" target="_blank">00:40:49.480</a></span> | <span class="t">you can think of there as being these three different kinds of terms. And we sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2452" target="_blank">00:40:52.600</a></span> | <span class="t">call these terms virtual attention heads because they don't exist in the sense-- like, they aren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2457" target="_blank">00:40:57.000</a></span> | <span class="t">explicitly represented in the model, but they, in fact, they have an attention pattern. They have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2462" target="_blank">00:41:02.040</a></span> | <span class="t">no E-circuit. They're in almost all functional ways like a tiny little attention head, and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2466" target="_blank">00:41:06.680</a></span> | <span class="t">exponentially many of them. Turns out they're not going to be that important in this model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2470" target="_blank">00:41:10.920</a></span> | <span class="t">but in other models, they can be important. Right, so one thing that I said is it allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2476" target="_blank">00:41:16.600</a></span> | <span class="t">us to think about attention heads in a really principled way. We don't have to go and think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2479" target="_blank">00:41:19.880</a></span> | <span class="t">about-- I think there's-- people look at attention patterns all the time, and I think a concern you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2487" target="_blank">00:41:27.240</a></span> | <span class="t">have is, well, there's multiple attention patterns. The information that's being moved by one attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2491" target="_blank">00:41:31.880</a></span> | <span class="t">head, it might have been moved there by another attention head and not originated there. It might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2495" target="_blank">00:41:35.560</a></span> | <span class="t">still be moved somewhere else. But in fact, this gives us a way to avoid all those concerns and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2500" target="_blank">00:41:40.120</a></span> | <span class="t">just think about things in a single principled way. OK, in any case, an important question to ask is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2506" target="_blank">00:41:46.680</a></span> | <span class="t">how important are these different terms? Like, we could study all of them. How important are they?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2510" target="_blank">00:41:50.680</a></span> | <span class="t">And it turns out you can just-- there's an algorithm you can use where you knock out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2516" target="_blank">00:41:56.840</a></span> | <span class="t">attention-- knock out these terms, and you go and you ask, how important are they?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2521" target="_blank">00:42:01.320</a></span> | <span class="t">And it turns out by far the most important thing is these individual attention head terms. In this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2526" target="_blank">00:42:06.440</a></span> | <span class="t">model, by far the most important thing, the virtual attention heads basically don't matter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2531" target="_blank">00:42:11.400</a></span> | <span class="t">that much. They only have an effective 0.3 nats using to the above ones, and the bigrams are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2536" target="_blank">00:42:16.680</a></span> | <span class="t">pretty useful. So if we want to try to understand this model, we should probably go and focus our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2540" target="_blank">00:42:20.120</a></span> | <span class="t">attention on-- the virtual attention heads are not going to be the best way to go and focus our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2546" target="_blank">00:42:26.680</a></span> | <span class="t">attention, especially since there's a lot of them. There's 124 of them for 0.3 nats. It's very little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2551" target="_blank">00:42:31.720</a></span> | <span class="t">that you would understand for studying one of those terms. So the thing that we probably want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2556" target="_blank">00:42:36.440</a></span> | <span class="t">to do-- we know that these are bigram statistics. So what we really want to do is we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2559" target="_blank">00:42:39.800</a></span> | <span class="t">understand the individual attention head terms. This is the algorithm. I'm going to skip over it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2568" target="_blank">00:42:48.520</a></span> | <span class="t">for time. We can ignore that term because it's small. And it turns out also that the layer 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2574" target="_blank">00:42:54.520</a></span> | <span class="t">attention heads are doing way more than layer 1 attention heads. And that's not that surprising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2578" target="_blank">00:42:58.840</a></span> | <span class="t">Layer 2 attention heads are more expressive because they can use the layer 1 attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2582" target="_blank">00:43:02.280</a></span> | <span class="t">heads to construct their attention patterns. So if we could just go and understand the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2588" target="_blank">00:43:08.120</a></span> | <span class="t">2 attention heads, we'd probably understand a lot of what's going on in this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2591" target="_blank">00:43:11.720</a></span> | <span class="t">And the trick is that the attention heads are now constructed from the previous layer rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2598" target="_blank">00:43:18.200</a></span> | <span class="t">than just from the tokens. So this is still the same, but the attention pattern is more complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2604" target="_blank">00:43:24.120</a></span> | <span class="t">And if you write it out, you get this complex equation that says, you embed the tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2608" target="_blank">00:43:28.440</a></span> | <span class="t">and you're going to shuffle things around using the attention heads for the keys. Then you multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2611" target="_blank">00:43:31.880</a></span> | <span class="t">by WQK. Then you shuffle things around again for the queries. And then you go and multiply by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2617" target="_blank">00:43:37.080</a></span> | <span class="t">embedding again because they were embedded. And then you get back to the tokens. But let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2624" target="_blank">00:43:44.200</a></span> | <span class="t">actually look at them. So one thing that's-- remember that when we see positive eigenvalues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2629" target="_blank">00:43:49.000</a></span> | <span class="t">in the OB circuit, we're doing copying. So one thing we can say is, well, 7 out of 12-- and in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2633" target="_blank">00:43:53.800</a></span> | <span class="t">fact, the ones with the largest eigenvalues are doing copying. So we still have a lot of attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2638" target="_blank">00:43:58.680</a></span> | <span class="t">heads that are doing copying. And yeah, the QK circuit-- so one thing you could do is you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2647" target="_blank">00:44:07.480</a></span> | <span class="t">try to understand things in terms of this more complex QK equation. You could also just try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2650" target="_blank">00:44:10.600</a></span> | <span class="t">understand what the attention patterns are doing empirically. So let's look at one of these copying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2654" target="_blank">00:44:14.520</a></span> | <span class="t">ones. I've given it the first paragraph of Harry Potter, and we can just look at word attempts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2663" target="_blank">00:44:23.240</a></span> | <span class="t">And something really interesting happens. So almost all the time, we just attend back to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2667" target="_blank">00:44:27.880</a></span> | <span class="t">the first token. We have this special token at the beginning of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2671" target="_blank">00:44:31.480</a></span> | <span class="t">And we usually think of that as just being a null attention operation. It's a way for it to not do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2676" target="_blank">00:44:36.440</a></span> | <span class="t">anything. In fact, if you look, the value vector is basically 0. It's just not copying any information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2680" target="_blank">00:44:40.600</a></span> | <span class="t">from that. But whenever we see repeated text, something interesting happens. So when we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2687" target="_blank">00:44:47.800</a></span> | <span class="t">to "Mr."-- tries to look at "and." It's a little bit weak. Then we get to "D," and it attends to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2695" target="_blank">00:44:55.400</a></span> | <span class="t">"ers." That's interesting. And then we get to "ers," and it attends to "ly." And so it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2704" target="_blank">00:45:04.920</a></span> | <span class="t">attending to the same token. It's attending to the same token, shifted one forward. Well, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2712" target="_blank">00:45:12.840</a></span> | <span class="t">really interesting. And there's actually a lot of attention heads that are doing this. So here we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2716" target="_blank">00:45:16.760</a></span> | <span class="t">have one where now we hit the potter's pot, and we attend to "ters." Maybe that's the same attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2721" target="_blank">00:45:21.880</a></span> | <span class="t">head I don't remember when I was constructing this example. It turns out this is a super common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2725" target="_blank">00:45:25.880</a></span> | <span class="t">thing. So you go and you look for the previous example, you shift one forward, and you're like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2730" target="_blank">00:45:30.120</a></span> | <span class="t">OK, well, last time I saw this, this is what happened. Probably the same thing is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2733" target="_blank">00:45:33.080</a></span> | <span class="t">happen. And we can go and look at the effect that the attention head has on the logits. Most of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2742" target="_blank">00:45:42.360</a></span> | <span class="t">time, it's not affecting things. But in these cases, it's able to go and predict when it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2746" target="_blank">00:45:46.200</a></span> | <span class="t">doing this thing of going and looking one forward. It's able to go and predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2749" target="_blank">00:45:49.160</a></span> | <span class="t">So we call this an induction head. An induction head looks for the previous copy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2754" target="_blank">00:45:54.920</a></span> | <span class="t">looks forward, and says, ah, probably the same thing that happened last time is going to happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2758" target="_blank">00:45:58.680</a></span> | <span class="t">You can think of this as being a nearest neighbors. It's like an in-context nearest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2762" target="_blank">00:46:02.280</a></span> | <span class="t">neighbors algorithm. It's going and searching through your context, finding similar things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2766" target="_blank">00:46:06.120</a></span> | <span class="t">and then predicting that's what's going to happen next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2771" target="_blank">00:46:11.480</a></span> | <span class="t">The way that these actually work is-- I mean, there's actually two ways. But in a model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2777" target="_blank">00:46:17.400</a></span> | <span class="t">uses rotary attention or something like this, you only have one. You shift your key. First,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2784" target="_blank">00:46:24.280</a></span> | <span class="t">an earlier attention head shifts your key forward one. So you take the value of the previous token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2789" target="_blank">00:46:29.960</a></span> | <span class="t">and you embed it in your present token. And then you have your query in your key,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2793" target="_blank">00:46:33.960</a></span> | <span class="t">go and look at-- yeah, try to go and match. So you look for the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2800" target="_blank">00:46:40.600</a></span> | <span class="t">And then you go and you predict that whatever you saw is going to be the next token. So that's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2805" target="_blank">00:46:45.480</a></span> | <span class="t">high-level algorithm. Sometimes you can do clever things where actually it'll care about multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2810" target="_blank">00:46:50.200</a></span> | <span class="t">earlier tokens, and it'll look for short phrases and so on. So induction heads can really vary in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2814" target="_blank">00:46:54.680</a></span> | <span class="t">how much of the previous context they care about or what aspects of the previous context they care</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2818" target="_blank">00:46:58.200</a></span> | <span class="t">about. But this general trick of looking for the same thing, shift forward, predict that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2823" target="_blank">00:47:03.320</a></span> | <span class="t">is what induction heads will do. Lots of examples of this. And the cool thing is you can now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2830" target="_blank">00:47:10.760</a></span> | <span class="t">you can use the QK eigenvalues to characterize this. You can say, well, we're looking for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2835" target="_blank">00:47:15.560</a></span> | <span class="t">same thing, shifted by one, but looking for the same thing. If you expand through the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2839" target="_blank">00:47:19.160</a></span> | <span class="t">nodes in the right way, that'll work out. And we're copying. And so an induction head is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2843" target="_blank">00:47:23.640</a></span> | <span class="t">which has both positive OV eigenvalues and also positive QK eigenvalues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2848" target="_blank">00:47:28.520</a></span> | <span class="t">And so you can just put that on a plot, and you have your induction heads in the corner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2857" target="_blank">00:47:37.000</a></span> | <span class="t">So your OV eigenvalues, your QK eigenvalues, and I think actually OV is this axis, QK is this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2862" target="_blank">00:47:42.760</a></span> | <span class="t">axis, doesn't matter. And in the corner, you have your eigenvalues or your induction heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2869" target="_blank">00:47:49.400</a></span> | <span class="t">And so this seems to be-- well, OK, we now have an actual hypothesis. The hypothesis is the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2877" target="_blank">00:47:57.160</a></span> | <span class="t">that that phase change we were seeing, the phase change is the discovery of these induction heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2881" target="_blank">00:48:01.240</a></span> | <span class="t">That would be the hypothesis. And these are way more effective than this first algorithm we had,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2888" target="_blank">00:48:08.120</a></span> | <span class="t">which was just blindly copy things wherever it could be plausible. Now we can go and actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2892" target="_blank">00:48:12.520</a></span> | <span class="t">recognize patterns and look at what happened and predict that similar things are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2895" target="_blank">00:48:15.560</a></span> | <span class="t">happen again. That's a way better algorithm. Yeah, so there's other attention heads that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2902" target="_blank">00:48:22.280</a></span> | <span class="t">are doing more local things. I'm going to go and skip over that and return to our mystery,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2905" target="_blank">00:48:25.800</a></span> | <span class="t">because I am running out of time. I have five more minutes. OK, so what is going on with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2910" target="_blank">00:48:30.200</a></span> | <span class="t">in-context learning? Well, now we have a hypothesis. Let's check it. So we think it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2914" target="_blank">00:48:34.040</a></span> | <span class="t">might be induction heads. And there's a few reasons we believe this. So one thing is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2920" target="_blank">00:48:40.600</a></span> | <span class="t">to be that induction heads-- well, OK, I'll just go over to the end. So one thing you can do is you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2926" target="_blank">00:48:46.520</a></span> | <span class="t">can just ablate the attention heads. And it turns out you can color-- here we have attention heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2932" target="_blank">00:48:52.200</a></span> | <span class="t">colored by how much they are an induction head. And this is the start of the bump. This is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2938" target="_blank">00:48:58.040</a></span> | <span class="t">end of the bump here. And we can see that they-- first of all, induction heads are forming. Like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2942" target="_blank">00:49:02.600</a></span> | <span class="t">previously, we didn't have induction heads here. Now they're just starting to form here. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2946" target="_blank">00:49:06.920</a></span> | <span class="t">we have really intense induction heads here and here. And the attention heads, where if you ablate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2953" target="_blank">00:49:13.080</a></span> | <span class="t">them, you get a loss. And so we're looking not at the loss, but at this meta learning score,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2960" target="_blank">00:49:20.760</a></span> | <span class="t">the difference between-- or in-context learning score, the difference between the 500th token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2965" target="_blank">00:49:25.000</a></span> | <span class="t">and the 50th token. And that's all explained by induction heads. Now, we actually have one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2971" target="_blank">00:49:31.480</a></span> | <span class="t">induction head that doesn't contribute to it. Actually, it does the opposite. So that's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2974" target="_blank">00:49:34.840</a></span> | <span class="t">of interesting. Maybe it's doing something shorter distance. And there's also this interesting thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2979" target="_blank">00:49:39.800</a></span> | <span class="t">where they all rush to be induction heads. And then they discover only a few went out in the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2984" target="_blank">00:49:44.680</a></span> | <span class="t">So there's some interesting dynamics going on there. But it really seems like in these small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2988" target="_blank">00:49:48.280</a></span> | <span class="t">models, all of in-context learning is explained by these induction heads. OK. What about large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2995" target="_blank">00:49:55.880</a></span> | <span class="t">models? Well, in large models, it's going to be harder to go and ask this. But one thing you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=2999" target="_blank">00:49:59.560</a></span> | <span class="t">do is you can ask, OK, we can look at our in-context learning score over time. We get this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3006" target="_blank">00:50:06.600</a></span> | <span class="t">sharp phase change. Oh, look. Induction heads form at exactly the same point in time. So that's only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3013" target="_blank">00:50:13.400</a></span> | <span class="t">correlational evidence. But it's pretty suggestive correlational evidence, especially given that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3017" target="_blank">00:50:17.320</a></span> | <span class="t">have an obvious-- the obvious effect that induction heads should have is this. I guess it could be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3022" target="_blank">00:50:22.920</a></span> | <span class="t">that there's other mechanisms being discovered at the same time in large models. But it has to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3026" target="_blank">00:50:26.360</a></span> | <span class="t">be in a very small window. So I really suggest the thing that's driving that change is in-context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3032" target="_blank">00:50:32.760</a></span> | <span class="t">learning. OK. So obviously, induction heads can go and copy text. But a question you might ask is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3041" target="_blank">00:50:41.080</a></span> | <span class="t">can they do translation? There's all these amazing things that models can do that it's not obvious</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3047" target="_blank">00:50:47.000</a></span> | <span class="t">in-context learning or this sort of copying mechanism could do. So I just want to very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3051" target="_blank">00:50:51.240</a></span> | <span class="t">quickly look at a few fun examples. So here we have an attention pattern. Oh, yeah. I guess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3062" target="_blank">00:51:02.120</a></span> | <span class="t">I need to open Lexiscope. Let me try doing that again. Sorry. I should have thought this through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3072" target="_blank">00:51:12.200</a></span> | <span class="t">a bit more before this talk. Chris, could you zoom in a little, please? Yeah, yeah. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3098" target="_blank">00:51:38.840</a></span> | <span class="t">OK. I'm not-- my French isn't that great. But my name is Christopher. I'm from Canada.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3104" target="_blank">00:51:44.360</a></span> | <span class="t">What we can do here is we can look at where this attention head attends as we go and we do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3110" target="_blank">00:51:50.680</a></span> | <span class="t">And it'll become especially clear on the second sentence. So here, we're on the period,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3115" target="_blank">00:51:55.400</a></span> | <span class="t">and we attend to "je." Now we're on-- and "je" is "I" in French. OK. Now we're on the "I,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3123" target="_blank">00:52:03.640</a></span> | <span class="t">and we attend to "sui." Now we're on the "am," and we attend to "do," which is "from,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3129" target="_blank">00:52:09.720</a></span> | <span class="t">and then "from" to "Canada." And so we're doing a cross-lingual induction head, which we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3136" target="_blank">00:52:16.040</a></span> | <span class="t">for translation. And indeed, if you look at examples, this is where it seems to-- it seems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3141" target="_blank">00:52:21.960</a></span> | <span class="t">to be a major driving force in the model's ability to go and correctly do translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3148" target="_blank">00:52:28.680</a></span> | <span class="t">Another fun example is-- I think maybe the most impressive thing about in-context learning to me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3155" target="_blank">00:52:35.240</a></span> | <span class="t">has been the model's ability to go and learn arbitrary functions. Like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3158" target="_blank">00:52:38.040</a></span> | <span class="t">you can just show the model a function. It can start mimicking that function. Well, OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3161" target="_blank">00:52:41.800</a></span> | <span class="t">I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3163" target="_blank">00:52:43.960</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3164" target="_blank">00:52:44.680</a></span> | <span class="t">So do these induction heads only do kind of a look-ahead copy? Or can they also do some sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3170" target="_blank">00:52:50.120</a></span> | <span class="t">of complex structure recognition? Yeah, yeah. So they can both use a larger context-- previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3178" target="_blank">00:52:58.520</a></span> | <span class="t">context-- and they can copy more abstract things. So the translation one is showing you that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3182" target="_blank">00:53:02.600</a></span> | <span class="t">can copy, rather than the literal token, a translated version. So it's what we might call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3186" target="_blank">00:53:06.520</a></span> | <span class="t">soft induction head. And yeah, you can have them copy similar words. You can have them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3191" target="_blank">00:53:11.800</a></span> | <span class="t">look at longer contexts. You can look for more structural things. The way that we usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3196" target="_blank">00:53:16.440</a></span> | <span class="t">characterize them is whether-- in large models, just whether they empirically behave like an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3200" target="_blank">00:53:20.600</a></span> | <span class="t">induction head. So the definition gets a little bit blurry when you try to encompass these more--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3205" target="_blank">00:53:25.480</a></span> | <span class="t">there's sort of a blurry boundary. But yeah, there seem to be a lot of attention heads that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3209" target="_blank">00:53:29.480</a></span> | <span class="t">are doing sort of more and more abstract versions. And yeah, my favorite version is this one that I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3216" target="_blank">00:53:36.040</a></span> | <span class="t">about to show you, which is used-- let's isolate a single one of these-- which can do pattern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3222" target="_blank">00:53:42.200</a></span> | <span class="t">recognition. So it can learn functions in the context and learn how to do it. So I've just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3226" target="_blank">00:53:46.200</a></span> | <span class="t">made up a nonsense function here. We're going to encode one binary variable with the choice of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3232" target="_blank">00:53:52.120</a></span> | <span class="t">whether to do a color or a month as the first word. Then we're going to-- so we have green or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3238" target="_blank">00:53:58.440</a></span> | <span class="t">June here. Let's zoom in more. So we have color or month, and animal or fruit. And then we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3247" target="_blank">00:54:07.400</a></span> | <span class="t">to map it to either true or false. So that's our goal. And it's going to be an XOR. So we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3251" target="_blank">00:54:11.480</a></span> | <span class="t">the binary variable represented in this way. We do an XOR. I'm pretty confident this was never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3256" target="_blank">00:54:16.840</a></span> | <span class="t">in the training set, because I just made it up, and it seems like a nonsense problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3261" target="_blank">00:54:21.080</a></span> | <span class="t">OK, so then we can go and ask, can the model go and predict that? Well, it can, and it uses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3265" target="_blank">00:54:25.880</a></span> | <span class="t">induction heads to do it. And what we can do is we can look at the-- so we look at a colon where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3270" target="_blank">00:54:30.120</a></span> | <span class="t">it's going to go and try and predict the next word. And for instance here, we have April dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3275" target="_blank">00:54:35.880</a></span> | <span class="t">So it's a month and then an animal, and it should be true. And what it does is it looks for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3280" target="_blank">00:54:40.920</a></span> | <span class="t">previous-- previous cases where there was an animal-- a month and then an animal, especially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3285" target="_blank">00:54:45.400</a></span> | <span class="t">one where the month was the same-- and goes and looks and says that it's true. And so the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3290" target="_blank">00:54:50.120</a></span> | <span class="t">can go and learn-- learn a function, a completely arbitrary function, by going and doing this kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3295" target="_blank">00:54:55.240</a></span> | <span class="t">of pattern recognition induction head. And so this, to me, made it a lot more plausible that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3301" target="_blank">00:55:01.240</a></span> | <span class="t">these models actually can do-- can do in-context learning. Like, the generality of all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3308" target="_blank">00:55:08.600</a></span> | <span class="t">amazing things we see these large language models do can be explained by induction heads. We don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3314" target="_blank">00:55:14.440</a></span> | <span class="t">know that. It could be that there's other things going on. It's very possible that there's lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3317" target="_blank">00:55:17.800</a></span> | <span class="t">of other things going on. But it seems a lot more plausible to me than it did when we started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3322" target="_blank">00:55:22.680</a></span> | <span class="t">I'm conscious that I am actually, over time-- I'm going to just quickly go through these last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3327" target="_blank">00:55:27.800</a></span> | <span class="t">few slides. So I think thinking of this as an in-context nearest neighbors, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3331" target="_blank">00:55:31.400</a></span> | <span class="t">is a really useful way to think about this. Other things could absolutely be contributing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3335" target="_blank">00:55:35.800</a></span> | <span class="t">This might explain why transformers do in-context learning over long-context better than LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3344" target="_blank">00:55:44.040</a></span> | <span class="t">And LSTM can't do this, because it's not linear in the amount of compute it needs. It's, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3348" target="_blank">00:55:48.200</a></span> | <span class="t">quadratic or n log n if it was really clever. So transformers-- or LSTM's impossible to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3353" target="_blank">00:55:53.640</a></span> | <span class="t">Transformers do do this. And actually, they diverge at the same point. But if you look-- well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3358" target="_blank">00:55:58.840</a></span> | <span class="t">I can go into this in more detail after, if you want. There's a really nice paper by Marcus Hutter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3365" target="_blank">00:56:05.000</a></span> | <span class="t">trying to predict and explain why we observe scaling laws in models. It's worth noting that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3369" target="_blank">00:56:09.080</a></span> | <span class="t">the arguments in this paper go exactly through to this example, this theory. In fact, they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3375" target="_blank">00:56:15.160</a></span> | <span class="t">better for the case of thinking about this in-context learning with, essentially, a nearest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3380" target="_blank">00:56:20.120</a></span> | <span class="t">neighbors algorithm than they do in the regular case. So yeah, I'm happy to answer questions. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3387" target="_blank">00:56:27.160</a></span> | <span class="t">can go into as much detail as people want about any of this. And I can also, if you send me an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3391" target="_blank">00:56:31.640</a></span> | <span class="t">email, send me more information about all this. And yeah, again, this work is not yet published.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3398" target="_blank">00:56:38.440</a></span> | <span class="t">You don't have to keep it secret. But just if you could be thoughtful about the fact that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3402" target="_blank">00:56:42.680</a></span> | <span class="t">unpublished work and probably is a month or two away from coming out, I'd be really grateful for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3406" target="_blank">00:56:46.360</a></span> | <span class="t">that. Thank you so much for your time. Yeah, thanks a lot, Chris. This was a great talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3410" target="_blank">00:56:50.680</a></span> | <span class="t">So I'll just open it to some general questions. And then we can do a round of questions from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3417" target="_blank">00:56:57.880</a></span> | <span class="t">students. So I was very excited to know, so what is the line of work that you're currently working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3423" target="_blank">00:57:03.080</a></span> | <span class="t">on? Is it extending this? So what do you think is the next things you try to do to make it more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3428" target="_blank">00:57:08.360</a></span> | <span class="t">interpretable? What are the next? Yeah. I mean, I want to just reverse engineer language models. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3433" target="_blank">00:57:13.320</a></span> | <span class="t">want to figure out the entirety of what's going on in these language models. And one thing that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3441" target="_blank">00:57:21.960</a></span> | <span class="t">totally don't understand is MLP layers. We understand some things about them, but we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3449" target="_blank">00:57:29.240</a></span> | <span class="t">really understand MLP layers very well. There's a lot of stuff going on in large models that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3453" target="_blank">00:57:33.800</a></span> | <span class="t">don't understand. I want to know how models do arithmetic. I want to know-- another thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3458" target="_blank">00:57:38.360</a></span> | <span class="t">I'm very interested in is what's going on when you have multiple speakers. The model can clearly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3461" target="_blank">00:57:41.960</a></span> | <span class="t">represent-- it has a basic theory of mind, multiple speakers in a dialogue. I want to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3466" target="_blank">00:57:46.200</a></span> | <span class="t">what's going on with that. But honestly, there's just so much we don't understand. It's sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3471" target="_blank">00:57:51.560</a></span> | <span class="t">hard to answer the question because there's just so much to figure out. And we have a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3476" target="_blank">00:57:56.760</a></span> | <span class="t">different threads of research in doing this. But yeah, the interpretability team at Anthropic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3482" target="_blank">00:58:02.600</a></span> | <span class="t">is just sort of-- has a bunch of threads trying to go and figure out what's going on inside these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3486" target="_blank">00:58:06.760</a></span> | <span class="t">models. And sort of a similar flavor to this of just trying to figure out, how do the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3491" target="_blank">00:58:11.240</a></span> | <span class="t">actually encode algorithms? And can we reverse engineer those into meaningful computer programs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3496" target="_blank">00:58:16.440</a></span> | <span class="t">that we can understand? Got it. Another question I had is, so you were talking about how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3502" target="_blank">00:58:22.840</a></span> | <span class="t">transformers are trying to do meta-learning inherently. So it's like-- and you spent a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3507" target="_blank">00:58:27.320</a></span> | <span class="t">of time talking about the induction heads, and that was very interesting. But can you formalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3511" target="_blank">00:58:31.720</a></span> | <span class="t">the sort of meta-learning algorithm they might be learning? Is it possible to say, oh, maybe this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3515" target="_blank">00:58:35.880</a></span> | <span class="t">a sort of internal algorithm that's going that's making them good meta-learners or something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3520" target="_blank">00:58:40.680</a></span> | <span class="t">that? I don't know. Yeah, I mean, I think that there's roughly two algorithms. One is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3526" target="_blank">00:58:46.040</a></span> | <span class="t">algorithm we saw in the one-layer model. And we see it in other models, too, especially early on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3529" target="_blank">00:58:49.480</a></span> | <span class="t">which is just try to copy-- you saw a word, probably a similar word is going to happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3533" target="_blank">00:58:53.880</a></span> | <span class="t">later. Look for places that it might fit in and increase the probability. So that's one thing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3539" target="_blank">00:58:59.000</a></span> | <span class="t">we see. And the other thing we see is induction heads, which you can just summarize as in-context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3545" target="_blank">00:59:05.000</a></span> | <span class="t">nearest neighbors, basically. And it seems-- possibly there's other things, but it seems like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3549" target="_blank">00:59:09.640</a></span> | <span class="t">those two algorithms and the specific instantiations that we are looking at seem to be what's driving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3555" target="_blank">00:59:15.640</a></span> | <span class="t">in-context learning. That would be my present theory. Yeah, it sounds very interesting. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3561" target="_blank">00:59:21.160</a></span> | <span class="t">OK, so let's open-- make a round of questions. So yeah, feel free to go ahead for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3568" target="_blank">00:59:28.200</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=pC4zRb_5noQ&t=3568" target="_blank">00:59:28.700</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>