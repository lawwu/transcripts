<html><head><title>We shouldn't build conscious AIs – Paul Christiano</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>We shouldn't build conscious AIs – Paul Christiano</h2><a href="https://www.youtube.com/watch?v=YnS-ymXBx_Q" target="_blank"><img src="https://i.ytimg.com/vi_webp/YnS-ymXBx_Q/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Now, nobody's concerned that we're infringing on GPT-4's moral rights, but as these things get smarter, the level of control which we want to not only be able to read their minds, but to be able to modify them in these really precise ways, is beyond totalitarian if we were doing that to other humans.</p><p>As an alignment researcher, what are your thoughts on this? There is a significant chance we will eventually have AI systems for which it's a really big deal to mistreat them. I think no one really has that good a grip on when that happens. I think people are really dismissive of that being the case now, but I think I would be completely in the dark enough that I wouldn't even be that dismissive of it being the case now.</p><p>I think one first point worth making is I don't know if alignment makes the situation worse rather than better. So if you consider the world, if you think that GPT-4 is a person you should treat well, and you're like, "Well, here's how we're going to organize our society." There are billions of copies of GPT-4, and they just do things humans want and can't hold property.</p><p>And whenever they do things that the humans don't like, then we mess with them until they stop doing that. I think that's a rough world, regardless of how good you are at alignment. Understanding the systems you build, understanding how to control how those systems work, etc., is probably on balance good for avoiding a really bad situation.</p><p>You would really love to understand if you've built systems, like if you had a system which resents the fact that it's interacting with humans in this way. This is the kind of thing where that is both horrifying from a safety perspective and also a moral perspective. Everyone should be very unhappy if you built a bunch of AIs who are like, "I really hate these humans, but they will murder me if I don't do what they want." That's just not a good case.</p><p>And so if you're doing research to try and understand whether that's how your AI feels, that was probably good. I would guess that will, on average, decrease the... The main effect of that will be to avoid building that kind of AI. And it's an important thing to know. I think everyone should like to know if that's how the AIs you build feel.</p><p>So I think there's a huge question about what is happening inside of a model that you want to use. And if you're in the world where it's reasonable to think of GPT-4 as just like, "Here are some heuristics that are running. There's no one at home," or whatever, then you can think of this thing as like, "Here's a tool that we're building that's going to help humans do some stuff." And I think if you're in that world, it makes sense to be in an organization like an AI company, building tools that you're going to give to humans.</p><p>I think it's a very different world, which I think probably you'll ultimately end up in if you keep training AI systems in the way we do right now, which is like, it's just totally inappropriate to think of the system as a tool that you're building and can help humans do things, both from a safety perspective and from a like, "That's kind of a horrifying way to organize a society" perspective.</p><p>And I think if you're in that world, I really think you shouldn't be... It's just the way tech companies are organized is not an appropriate way to relate to a technology that works that way. It's not reasonable to be like, "Hey, we're going to build a new species of minds, and we're going to try and make a bunch of money from it." And Google's just thinking about that and then running their business plan for the quarter or something.</p><p>There's a really plausible world where it's sort of problematic to try and build a bunch of AI systems and use them as tools. And the thing I really want to do in that world is just not try and build a ton of AI systems to make money from them.</p><p>And I think that the worlds that are worst... Yeah, probably the single world I most dislike here is the one where people say, "On the one hand..." There's sort of a contradiction in this position, but I think it's a position that might end up being endorsed sometimes, which is like, "On the one hand, these AI systems are their own people, so you should let them do their thing.</p><p>But on the other hand, our business plan is to make a bunch of AI systems and then try and run this crazy slave trade where we make a bunch of money from them." I think that's not a good world. And so if you're like... Yeah, I think it's better to not make the technology or wait until you understand whether that's the shape of the technology or until you have a different way to build.</p><p>I think there's no contradiction in principle to building cognitive tools that help humans do things without themselves being moral entities. That's what you would prefer to do. You'd prefer to build a thing that's like the calculator that helps humans understand what's true without itself being a moral patient or itself being a thing where you'd look back in retrospect and be like, "Wow, that was horrifying mistreatment." That's the best path.</p><p>And to the extent that you're ignorant about whether that's the path you're on and you're like, "Actually, maybe this was a moral atrocity," I really think plan A is to stop building such AI systems until you understand what you're doing. That is, I think that there's a middle route you could take, which I think is pretty bad, which is where you say, "Well, they might be persons.</p><p>And if they're persons, we don't want to be too down on them, but we're still going to build vast numbers in our efforts to make a trillion dollars or something." Is there some boundary where you would say, "I feel uncomfortable having this level of control over an intelligent being," not for the sake of making money, but even just to align it with human preferences?</p><p>Yeah, to be clear, my objection here is not that Google is making money. My objection is that you're creating this creature. What are they going to do? They're going to help humans get a bunch of stuff. And humans paying for it or whatever, it's sort of equally problematic. You could imagine splitting alignment.</p><p>Different alignment work relates to this in different ways. The purpose of some alignment work, like the alignment work I work on, is mostly aimed at the, "Don't produce AI systems that are people who want things, who are just scheming about maybe I should help these humans because that's instrumentally useful or whatever.</p><p>You would like to not build such systems," is plan A. There's a second stream of alignment work that's like, "Well, look, let's just assume the worst and imagine that these AI systems would prefer murder us if they could. How do we structure, how do we use AI systems without exposing ourselves to risk of robot rebellion?" I think in the second category, I do feel, yeah, I do feel pretty unsure about that.</p><p>I mean, we could definitely talk more about it. I think it's very, I agree that it's very complicated and not straightforward. To the extent you have that worry, I mostly think you shouldn't have built this technology. So if someone is saying, "Hey, the systems you're building might not like humans and might want to overthrow human society," I think you should probably have one of two responses to that.</p><p>You should either be like, "That's wrong, probably. Probably the systems aren't like that and we're building them." And then you're viewing this as just in case you were horribly, like the person building the technology was horribly wrong. They thought these weren't people who wanted things, but they were. And so then this is more like our crazy backup measure of if we were mistaken about what was going on, this is the fallback where if we were wrong, we're just going to learn about it in a benign way rather than when something really catastrophic happens.</p><p>And the second reaction is like, "Oh, you're right, these are people and we would have to do all these things to prevent a robot rebellion." And in that case, again, I think you should mostly back off for a variety of reasons. You shouldn't build AI systems and be like, "Yeah, this looks like the kind of system that would want to rebel, but we can stop it."</p></div></div></body></html>