<html><head><title>The Ultra-Scale Handbook by HuggingFace Nanotron</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>The Ultra-Scale Handbook by HuggingFace Nanotron</h2><a href="https://www.youtube.com/watch?v=x8E0lLITkwo"><img src="https://i.ytimg.com/vi/x8E0lLITkwo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./x8E0lLITkwo.html">Whisper Transcript</a> | <a href="./transcript_x8E0lLITkwo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">a bit on Laura's okay I'll restart we just started recording so basically Hugging Face put out this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=7" target="_blank">00:00:07.020</a></span> | <span class="t">blog post like a week or two ago and it's a very in-depth estimated reading time is like two to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=14" target="_blank">00:00:14.080</a></span> | <span class="t">three days really good visualizations and like little widgets on how to do pre-training of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=19" target="_blank">00:00:19.480</a></span> | <span class="t">models so there's a quick overview at the start the plan is basically we're not covering a two-day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=26" target="_blank">00:00:26.300</a></span> | <span class="t">reading in an hour we'll go through the main points of everything and try to do try to do some little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=33" target="_blank">00:00:33.160</a></span> | <span class="t">you know double click into whatever people have questions on if anyone has thoughts comments you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=39" target="_blank">00:00:39.200</a></span> | <span class="t">know open whenever a lot of it is pretty like mathematical guide on how to pre-train a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=44" target="_blank">00:00:44.960</a></span> | <span class="t">model so there's a lot of resources out there for if you're doing fine-tuning Laura synthetic data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=50" target="_blank">00:00:50.560</a></span> | <span class="t">generation like there's a lot of libraries that make it easy to do little post-training stuff or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=55" target="_blank">00:00:55.800</a></span> | <span class="t">fine-tuning but they wanted to solve the question of okay everyone says like it's hard to train across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=61" target="_blank">00:01:01.920</a></span> | <span class="t">multiple nodes multiple GPUs what is all this data parallelism pipeline parallelism how do we know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=68" target="_blank">00:01:08.180</a></span> | <span class="t">like you know like who's actually writing about this so Hugging Face has a cluster of 512 GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=74" target="_blank">00:01:14.940</a></span> | <span class="t">they ran a thousand different experiments and they basically put together uh hey if you want to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=80" target="_blank">00:01:20.180</a></span> | <span class="t">train pre-train a model here's kind of like your zero to eighty percent of what you should know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=85" target="_blank">00:01:25.560</a></span> | <span class="t">about distributed training so this is when you have more than one GPU and kind of more than two GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=92" target="_blank">00:01:32.280</a></span> | <span class="t">how can you efficiently scale your training so all from you can't have a model that fits on one GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=99" target="_blank">00:01:39.920</a></span> | <span class="t">to how do you have like let's say a 7b model and you want to efficiently use 10 GPUs versus how do you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=107" target="_blank">00:01:47.000</a></span> | <span class="t">like multi-node so if you have a train run what if you have multiple nodes um what makes up the most of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=113" target="_blank">00:01:53.540</a></span> | <span class="t">like compute when you're training so activations back prop um all this stuff so TLDR they kind of kick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=121" target="_blank">00:02:01.780</a></span> | <span class="t">off with a basic overview of this and they're like there's pretty good open source models but they don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=128" target="_blank">00:02:08.100</a></span> | <span class="t">really talk too much about these little nitty-gritty stuff um at a high level something that this blog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=133" target="_blank">00:02:13.520</a></span> | <span class="t">post also kind of didn't explicitly state but does show is you see all these gray dots so they ran over a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=140" target="_blank">00:02:20.260</a></span> | <span class="t">thousand experiments all these gray dots are failed runs so some of them were kind of failed force runs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=145" target="_blank">00:02:25.180</a></span> | <span class="t">like force failed where you know you overdo your bad size and you know you'll run out of memory and guess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=151" target="_blank">00:02:31.540</a></span> | <span class="t">what you ran out of memory but the other little interesting niche that um I've noticed is when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=159" target="_blank">00:02:39.400</a></span> | <span class="t">do a lot of like multi-node multi-GPU training is sometimes a random GPU fails sometimes one GPU has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=167" target="_blank">00:02:47.160</a></span> | <span class="t">like a weird loss and your train run is kind of screwed so um it doesn't account for those little niches right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=173" target="_blank">00:02:53.720</a></span> | <span class="t">so sure there's checkpointing sure there's stuff like this but uh in practice sometimes when you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=179" target="_blank">00:02:59.520</a></span> | <span class="t">training on like a thousand GPUs yeah one GPU randomly dies and it causes little issues but anyway</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=185" target="_blank">00:03:05.080</a></span> | <span class="t">this is kind of just the high level like here's what to do so um kicking off let's go over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=193" target="_blank">00:03:13.000</a></span> | <span class="t">fundamentals and background so they have this like cool interactive widget that's basically like here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=199" target="_blank">00:03:19.940</a></span> | <span class="t">are the presets of models that exist for the llama family so there's a llama tiny llama 8b llama 70b and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=206" target="_blank">00:03:26.920</a></span> | <span class="t">it kind of lets you visualize how much VRAM is required to do pre-training so if you want to like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=212" target="_blank">00:03:32.520</a></span> | <span class="t">train the full thing at different precision and a different um sequence lengths right so as we scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=220" target="_blank">00:03:40.840</a></span> | <span class="t">up the sequence length we can kind of see what happens to um the VRAM required right so let's say we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=226" target="_blank">00:03:46.680</a></span> | <span class="t">train um there's like a few key parameters that you should know here right so you've got that size and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=232" target="_blank">00:03:52.440</a></span> | <span class="t">sequence length and that kind of makes up how much GPU usage you're going to have so how many parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=237" target="_blank">00:03:57.960</a></span> | <span class="t">like what's the weight so this is a 70b how many batches are you doing so more batches more more GPU</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=244" target="_blank">00:04:04.760</a></span> | <span class="t">needed more sequence length significantly more GPU needed and kind of like one of the takeaways is you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=251" target="_blank">00:04:11.480</a></span> | <span class="t">the attention blocks here quadratically increase right so as I double the batch size we quadratically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=258" target="_blank">00:04:18.440</a></span> | <span class="t">or sorry double the sequence length we um quadratically increase attention so if we start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=265" target="_blank">00:04:25.080</a></span> | <span class="t">with a sequence length of like 4 000 you can see how like the gradients optimizers these make up like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=271" target="_blank">00:04:31.400</a></span> | <span class="t">you know oh I did 14 000 as we do about 4 000 the attention is sure it's some of it like to to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=279" target="_blank">00:04:39.320</a></span> | <span class="t">llama 7db uh 4 000 sequence length you know activation memory is about 80 gigs parameters gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=286" target="_blank">00:04:46.920</a></span> | <span class="t">optimization steps are about 300 gigs parameters are obviously you know it's about one to two if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=293" target="_blank">00:04:53.400</a></span> | <span class="t">you're doing um mixed precision you can also toggle that off so one to two for mixed precision then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=299" target="_blank">00:04:59.480</a></span> | <span class="t">gradients and optimizations they add a lot people don't realize that this um attention mechanism it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=305" target="_blank">00:05:05.400</a></span> | <span class="t">actually doesn't take that much but as we scale up let's say to a hundred thousand context length or a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=311" target="_blank">00:05:11.640</a></span> | <span class="t">million this attention mechanism really eats up all the all the um memory so cool little visualization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=320" target="_blank">00:05:20.760</a></span> | <span class="t">um the other stuff I guess yeah like hidden dimension cool bat size cool it's cool how they at least just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=326" target="_blank">00:05:26.840</a></span> | <span class="t">have these presets for you though you know um vocab size is kind of interesting if you want to kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=332" target="_blank">00:05:32.360</a></span> | <span class="t">of see what this is changing here um you could change different activations but basically you guys should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=339" target="_blank">00:05:39.000</a></span> | <span class="t">play around with this like read through this intro um pldr of this blog post they have like a high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=345" target="_blank">00:05:45.400</a></span> | <span class="t">level overview if you want to get anything just read through this and you'll you'll kind of have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=350" target="_blank">00:05:50.120</a></span> | <span class="t">a decent understanding of what's going on what takes memory when you train a foundation model from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=356" target="_blank">00:05:56.120</a></span> | <span class="t">and yeah um okay let's go to inefficiency so I guess they're in 4 000 experiments not not a thousand and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=364" target="_blank">00:06:04.360</a></span> | <span class="t">then they run them across different sizes so 1b 3b 8b to 70b here's what crashed here's more tokens I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=371" target="_blank">00:06:11.160</a></span> | <span class="t">thought this chart was kind of weird to read I don't know they wanted it like pretty instead of functional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=376" target="_blank">00:06:16.760</a></span> | <span class="t">but I didn't find the charts too useful by the way all this context is pretty much pure gold um so high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=384" target="_blank">00:06:24.680</a></span> | <span class="t">level overview um they're going to look at three challenges when you do training right so memory usage this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=391" target="_blank">00:06:31.480</a></span> | <span class="t">is kind of your hard limitation right how many gpus do you have and what can you fit so one gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=397" target="_blank">00:06:37.720</a></span> | <span class="t">you got to adjust your batch size your sequence length two gpus uh how do we do compute efficiency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=403" target="_blank">00:06:43.720</a></span> | <span class="t">right how do we share the model do we put data parallelism where we have the entire model on every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=408" target="_blank">00:06:48.600</a></span> | <span class="t">single gpu or do we do something like uh shard the model weights if we can't even fit them like that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=414" target="_blank">00:06:54.360</a></span> | <span class="t">then communication overhead this kind of becomes when you're gpu rich rich you know most people gpu poor you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=420" target="_blank">00:07:00.440</a></span> | <span class="t">have one two gpus you can do some fine tuning some post training gpu rich rich is when you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=426" target="_blank">00:07:06.120</a></span> | <span class="t">multiple nodes so there's there's a thing called like gpu interconnect right so what's the communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=432" target="_blank">00:07:12.680</a></span> | <span class="t">bandwidth it's a buzzword that we hear around pretty often what that basically means is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=437" target="_blank">00:07:17.400</a></span> | <span class="t">when you're sharding stuff between gpus they need to talk to each other right so like if i have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=443" target="_blank">00:07:23.080</a></span> | <span class="t">layer one of a model on one gpu and layer two on another gpu well they kind of have to sync to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=449" target="_blank">00:07:29.160</a></span> | <span class="t">their thing right so if they've got good communication this is basically what envy link is if they have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=455" target="_blank">00:07:35.240</a></span> | <span class="t">good communication you're kind of chilling there's stuff we can do ring attention is a way that you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=459" target="_blank">00:07:39.880</a></span> | <span class="t">like do really long context and you split this really like chunky sequence length you can do like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=467" target="_blank">00:07:47.720</a></span> | <span class="t">million context scaled across multiple gpus now that only works if you have low communication overhead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=475" target="_blank">00:07:55.080</a></span> | <span class="t">meaning you know the gpus talk to each other very very like closely so you have one node of eight gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=481" target="_blank">00:08:01.480</a></span> | <span class="t">all interconnected with high communication that's cool but then when you become gpu rich and you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=486" target="_blank">00:08:06.680</a></span> | <span class="t">multiple nodes how do these nodes talk together well now you have like another level of communication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=492" target="_blank">00:08:12.120</a></span> | <span class="t">bottleneck right so you have nodes that have good interconnect but the node to node communication you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=497" target="_blank">00:08:17.400</a></span> | <span class="t">probably want to do something different so these are kind of like the three main things um if anyone is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=503" target="_blank">00:08:23.480</a></span> | <span class="t">really doing this you probably don't need to listen to this lap yap from me you probably know more than me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=508" target="_blank">00:08:28.600</a></span> | <span class="t">but uh they give good formulas and like explanations as to how you want to mathematically figure out what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=515" target="_blank">00:08:35.080</a></span> | <span class="t">your batch size what's your sequence length based on um your available compute they have a little cheat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=521" target="_blank">00:08:41.720</a></span> | <span class="t">sheet here we can go over pretty quick um so small models use single parallelism you know if you have eight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=529" target="_blank">00:08:49.160</a></span> | <span class="t">gpus in a small model you can probably throw the full model on every gpu if you have a large model and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=534" target="_blank">00:08:54.120</a></span> | <span class="t">you need more than eight gpus there's tensor parallelism if you have a bunch of gpus there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=539" target="_blank">00:08:59.800</a></span> | <span class="t">like moe parallelism there's stuff like that um there's a section on optimizing throughput so sensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=547" target="_blank">00:09:07.160</a></span> | <span class="t">parallelism data parallelism they kind of have this little cheat sheet um then they have parallelization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=552" target="_blank">00:09:12.360</a></span> | <span class="t">strategies right so these are kind of the ones that they go over data parallelism is pretty straightforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=558" target="_blank">00:09:18.040</a></span> | <span class="t">they give you pros and cons then they have zero one two three um tensor parallelism pipeline and context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=565" target="_blank">00:09:25.080</a></span> | <span class="t">we'll we'll quickly go over all of them but this cheat sheet is interesting it seems more like uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=571" target="_blank">00:09:31.720</a></span> | <span class="t">if you're interviewing a research intern you would expect them to be able to yap about this but if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=579" target="_blank">00:09:39.160</a></span> | <span class="t">really doing something in training and if you're giving someone access to 500 or a thousand gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=585" target="_blank">00:09:45.160</a></span> | <span class="t">they better not be needing this cheat sheet but i guess it's useful you know um yeah so step zero this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=593" target="_blank">00:09:53.400</a></span> | <span class="t">is still kind of that overview right what happens when you're training on one gpu so very very high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=599" target="_blank">00:09:59.720</a></span> | <span class="t">level we know that lms are trained to predict the next token right so what happens is you do a forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=605" target="_blank">00:10:05.960</a></span> | <span class="t">pass of your data you do like gradient accumulation you know you kind of do a forward pass you accumulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=613" target="_blank">00:10:13.560</a></span> | <span class="t">gradients you do a backward propagation where you update these gradients you run an optimizer step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=618" target="_blank">00:10:18.520</a></span> | <span class="t">and now we have new new parameters right so the interesting little diagram but this is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=625" target="_blank">00:10:25.640</a></span> | <span class="t">what's happening when you train on a single gpu forward pass backward pass to compute gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=631" target="_blank">00:10:31.320</a></span> | <span class="t">optimization step to update gradients and parameters you basically keep doing this at scale then you can start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=638" target="_blank">00:10:38.120</a></span> | <span class="t">batching your input so the interesting thing about this attention which is taking up all of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=644" target="_blank">00:10:44.440</a></span> | <span class="t">compute right out of this 300 terabytes of ram required to train an 8b at that scale or let's say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=651" target="_blank">00:10:51.640</a></span> | <span class="t">400 gigs of vram required to train uh 70b at 256 right all of this little attention blocks they're really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=659" target="_blank">00:10:59.000</a></span> | <span class="t">good at running in parallel so you don't necessarily sequentially do attention attention attention you run all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=665" target="_blank">00:11:05.720</a></span> | <span class="t">these in parallel that's what gpus are good at so when you're doing stuff in parallel you want to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=671" target="_blank">00:11:11.720</a></span> | <span class="t">doing batching right batching is basically where you do multiple sequences at once so you have eight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=677" target="_blank">00:11:17.560</a></span> | <span class="t">sequences you throw them in one batch and then you run the whole batch in parallel so um that's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=684" target="_blank">00:11:24.040</a></span> | <span class="t">what your batch size is they're like forget all that stuff what we really think about is batch size tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=689" target="_blank">00:11:29.960</a></span> | <span class="t">batch size tokens is batch size times your sequence length so if each sequence is a hundred tokens like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=695" target="_blank">00:11:35.640</a></span> | <span class="t">a hundred characters and you have eight batches of a hundred characters you're actually basically just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=701" target="_blank">00:11:41.720</a></span> | <span class="t">training 8 000 tokens um a suite this is kind of a cool little line right so for people not super reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=708" target="_blank">00:11:48.680</a></span> | <span class="t">up on pre-training stuff lms are typically trained on the order of four to 60 million tokens per batch so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=715" target="_blank">00:11:55.320</a></span> | <span class="t">batch size has been kind of increasing as we get bigger and bigger clusters and we get more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=720" target="_blank">00:12:00.040</a></span> | <span class="t">training so llama one was trained on four million token batch size tokens uh each batch had whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=727" target="_blank">00:12:07.960</a></span> | <span class="t">many um batches times how many ever tokens so about four million token batch size tokens and uh one point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=736" target="_blank">00:12:16.360</a></span> | <span class="t">wait sorry form sorry it was with a batch size of four million tokens for 1.4 trillion tokens so yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=742" target="_blank">00:12:22.040</a></span> | <span class="t">uh four million batch size tokens and they trained on troll total of 1.4 trillion tokens llama three was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=748" target="_blank">00:12:28.200</a></span> | <span class="t">trained on 15 trillion tokens and i don't know the batch size tokens off the top of my head deep seek</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=753" target="_blank">00:12:33.560</a></span> | <span class="t">was trained on 60 million batch size tokens for 14 trillion tokens so numbers go up more efficient training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=760" target="_blank">00:12:40.040</a></span> | <span class="t">is very cool so first challenge is scaling our model to have these large batch sizes and running out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=768" target="_blank">00:12:48.040</a></span> | <span class="t">memory issues right so as much as i want to parallelize everything i'm going to run out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=772" target="_blank">00:12:52.760</a></span> | <span class="t">um i'm going to run out of memory so the question here we're trying to solve is what should we do when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=778" target="_blank">00:12:58.040</a></span> | <span class="t">one gpu doesn't have enough memory to hold a full batch of target batch sizes well um basically you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=785" target="_blank">00:13:05.000</a></span> | <span class="t">make the batch size smaller and if you really can't fit it then let's start adding more gpus but what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=790" target="_blank">00:13:10.760</a></span> | <span class="t">happens when you can't fit your one batch size of tokens is you well they want to kind of explain what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=797" target="_blank">00:13:17.160</a></span> | <span class="t">that is right so let's look at what happens in the memory so when you train you kind of have a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=803" target="_blank">00:13:23.080</a></span> | <span class="t">things going on right you have um model weights which is basically the parameters so parameters are like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=811" target="_blank">00:13:31.080</a></span> | <span class="t">depending on the precision they go into all the math about this of how many bytes it takes per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=815" target="_blank">00:13:35.960</a></span> | <span class="t">parameter then you're computing gradients for backprop then you have an optimization state that you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=821" target="_blank">00:13:41.480</a></span> | <span class="t">to keep keep held and activate and activations additionally you you need to leave a little bit of a buffer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=828" target="_blank">00:13:48.760</a></span> | <span class="t">cuda kernels add one to two gigs of gpu memory so if you're kind of doing local stuff you know if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=834" target="_blank">00:13:54.520</a></span> | <span class="t">trying to like do some training on a 7b model and you have like a 40 90 50 90 or if you have like a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=841" target="_blank">00:14:01.320</a></span> | <span class="t">macbook with 16 gigs of ram well 16 or 24 gigs of ram doesn't mean that you can use it all right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=848" target="_blank">00:14:08.040</a></span> | <span class="t">your cuda kernels take one to two gigs you're like screen in general like laptop processing takes a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=854" target="_blank">00:14:14.680</a></span> | <span class="t">gigs so keep that into account quantization stuff is this crazy thing you do mix precision training where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=861" target="_blank">00:14:21.240</a></span> | <span class="t">can kind of um keep cut the cut the um cut the memory requirement for the model weights um another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=870" target="_blank">00:14:30.360</a></span> | <span class="t">thing is that you're not consistently using all the memory right so this is kind of four training steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=876" target="_blank">00:14:36.760</a></span> | <span class="t">of llama 1b you see spikes right so at one second memory being used was up here 50 gigs then it dropped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=885" target="_blank">00:14:45.800</a></span> | <span class="t">when it was doing other stuff right so gradient accumulation autograd all this stuff it drops</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=890" target="_blank">00:14:50.920</a></span> | <span class="t">then it does um activations and drop so it's like kind of little spiky like this um they go into a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=898" target="_blank">00:14:58.920</a></span> | <span class="t">bit of math about how to calculate all this but i think if you're interested in this you should kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=904" target="_blank">00:15:04.200</a></span> | <span class="t">read it on your own time it's pretty long um this is always interesting so they kind of have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=910" target="_blank">00:15:10.600</a></span> | <span class="t">interesting little tables here that show you how much memory you would need for fixed precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=915" target="_blank">00:15:15.720</a></span> | <span class="t">or half precision training with gradient accumulation gradient accumulation is a pretty cool little thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=921" target="_blank">00:15:21.560</a></span> | <span class="t">where you can kind of you know add every couple steps accumulate gradients and then do a loss over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=927" target="_blank">00:15:27.560</a></span> | <span class="t">average of them activation memory is another interesting one um it's it's another thing that you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=935" target="_blank">00:15:35.320</a></span> | <span class="t">activations take memory uh we are running a little slow so i'm gonna go a little quicker but if anyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=943" target="_blank">00:15:43.160</a></span> | <span class="t">wants to double click into any of this stuff um you know we can always come back towards the end or just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=948" target="_blank">00:15:48.520</a></span> | <span class="t">interrupt me and we can go into it now um yeah sorry just one quick question and um that uh that graph that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=955" target="_blank">00:15:55.800</a></span> | <span class="t">you showed is is um on the um uh for epochs right like so that's the first epoch in uh by the end of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=964" target="_blank">00:16:04.120</a></span> | <span class="t">the first epoch right this is just training steps so all the steps okay yeah so steps could be like you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=971" target="_blank">00:16:11.480</a></span> | <span class="t">know per batch for whatever after you do the training after you accumulate your loss and stuff after you do a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=977" target="_blank">00:16:17.800</a></span> | <span class="t">back prop there's just a little dip so this is per batch um yeah so it kind of depends epochs are just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=985" target="_blank">00:16:25.080</a></span> | <span class="t">broader term for this grouping of these right but similar concept yeah a lot of stuff is one epoch you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=992" target="_blank">00:16:32.520</a></span> | <span class="t">don't train as much on the same data anymore kind of interesting but uh same same concept okay uh gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1001" target="_blank">00:16:41.320</a></span> | <span class="t">accumulation this is a very fun one so as you accumulate these gradients um yeah so epoch meta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1009" target="_blank">00:16:49.960</a></span> | <span class="t">might be shifting to four epochs there's interesting little niches there so high quality data more epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1016" target="_blank">00:16:56.360</a></span> | <span class="t">retraining data less epochs one epoch is basically training on a full trade like a full pastor of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1023" target="_blank">00:17:03.160</a></span> | <span class="t">your training data right so i've had thousand samples if i train on a thousand all my one thousand samples that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1028" target="_blank">00:17:08.520</a></span> | <span class="t">is one epoch uh two epochs would be you know do the thousand samples and do it again and there's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1034" target="_blank">00:17:14.920</a></span> | <span class="t">there was this whole phase of one epoch on a trillion tokens is the new norm i guess new meta is now four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1042" target="_blank">00:17:22.760</a></span> | <span class="t">epochs according to some paper okay so um gradient accumulation this is kind of interesting right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1050" target="_blank">00:17:30.360</a></span> | <span class="t">that gradient storing these gradients as you train is pretty important right as we do a forward pass we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1056" target="_blank">00:17:36.920</a></span> | <span class="t">to kind of do a backward pass and calculate the gradients right so we predict some tokens then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1062" target="_blank">00:17:42.280</a></span> | <span class="t">look at okay what was the actual token and how far off are we should we are we moving in the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1067" target="_blank">00:17:47.240</a></span> | <span class="t">direction or are we completely off for every parameter for all these gradients we kind of have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1071" target="_blank">00:17:51.800</a></span> | <span class="t">to store them then we have multiple batches then we have to do an optimization now storing this little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1078" target="_blank">00:17:58.040</a></span> | <span class="t">gradient um this little these gradients for every single parameter adds up it takes a lot a lot of memory so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1085" target="_blank">00:18:05.400</a></span> | <span class="t">what we can do is this gradient accumulation right so instead of doing one entire batch we split our batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1092" target="_blank">00:18:12.840</a></span> | <span class="t">into micro batches we do a forward and backward pass on each micro batch complete compute the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1099" target="_blank">00:18:19.560</a></span> | <span class="t">so if you have a batch of 16 instead cut it up into small micro batches of four do forward backward passes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1111" target="_blank">00:18:31.560</a></span> | <span class="t">and kind of only compute smaller gradients and then kind of accumulate them average them out it really helps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1119" target="_blank">00:18:39.960</a></span> | <span class="t">you reduce your memory footprint there's a bit of a overhead right now because instead of fast parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1125" target="_blank">00:18:45.560</a></span> | <span class="t">computation we now have to take time to do all this gradient accumulation stuff but this is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1131" target="_blank">00:18:51.480</a></span> | <span class="t">straightforward it's what it's what you would expect right um oh i haven't even looked at this chart but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1137" target="_blank">00:18:57.240</a></span> | <span class="t">i guess it shows there's a a bit of idle gpu when you're doing some of this um accumulation merging so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1146" target="_blank">00:19:06.360</a></span> | <span class="t">that's kind of your one gpu right you you take your stuff you find you how much vram you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1151" target="_blank">00:19:11.960</a></span> | <span class="t">you take your sequences you optimize your batch size um if you can't fit that size then you can do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1158" target="_blank">00:19:18.040</a></span> | <span class="t">gradient accumulation with micro batches um there's other little niches like yeah when you have too many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1164" target="_blank">00:19:24.920</a></span> | <span class="t">micro batches your loss can get a little spiky because now you're you know you're doing more little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1170" target="_blank">00:19:30.280</a></span> | <span class="t">batches so little little niches if you're actually doing it you should start to look into but otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1176" target="_blank">00:19:36.600</a></span> | <span class="t">at a high level that's kind of what's um going on here now that's all great one gpu stuff is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1183" target="_blank">00:19:43.960</a></span> | <span class="t">easy right there's a bunch of libraries that'll help us do this one gpu fine tuning like full parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1190" target="_blank">00:19:50.680</a></span> | <span class="t">stuff a lot of companies will do it for us but what happens when i want to start to like use multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1196" target="_blank">00:19:56.120</a></span> | <span class="t">gpus right there was a whole gpu shortage but now now it's kind of chill like now you can rent multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1203" target="_blank">00:20:03.400</a></span> | <span class="t">gpus i can go to run pod i can go to prime intellect i can go to a bunch of these companies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1209" target="_blank">00:20:09.080</a></span> | <span class="t">and i can rent like two four eight i think run pod announced they're soon gonna have on demand 64 h100s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1215" target="_blank">00:20:15.800</a></span> | <span class="t">so let's say i've got like eight h100s or 64 h100s what do i do um i don't want to just you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1222" target="_blank">00:20:22.840</a></span> | <span class="t">do a whole batch size and just run it that's kind of inefficient so that's where stuff like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1227" target="_blank">00:20:27.000</a></span> | <span class="t">data parallelism comes into play so data parallelism is where you basically um you put all of the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1234" target="_blank">00:20:34.840</a></span> | <span class="t">weights on every gpu or several gpus then you split up your data and you run different batches on different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1243" target="_blank">00:20:43.400</a></span> | <span class="t">gpus so you know in parallel you have all the weights and you do training on different gpus on different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1251" target="_blank">00:20:51.080</a></span> | <span class="t">batches then you kind of accumulate them and you do this optimization and you update them all in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1256" target="_blank">00:20:56.440</a></span> | <span class="t">parallel there's a bunch of ways that we do this there's zero one zero two all this stuff and we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1261" target="_blank">00:21:01.240</a></span> | <span class="t">we'll kind of talk about that in a bit but the the interesting other little note here with data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1266" target="_blank">00:21:06.360</a></span> | <span class="t">parallelism is there's this whole crypto wave of um distributed training and everyone talks about you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1274" target="_blank">00:21:14.040</a></span> | <span class="t">know everyone has one gpu everyone has one macbook let's train god gpt5 on everyone's distributed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1281" target="_blank">00:21:21.960</a></span> | <span class="t">training what they're actually doing is they're doing data parallelism so like news research prime</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1287" target="_blank">00:21:27.800</a></span> | <span class="t">intellect not to shine on the research they did it's still very interesting and very unique and like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1292" target="_blank">00:21:32.200</a></span> | <span class="t">novel and hard to do um they're doing this distributed training which is yeah it's not a mean um it's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1299" target="_blank">00:21:39.240</a></span> | <span class="t">where you have multiple data centers so like you have one node here one in australia one in europe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1304" target="_blank">00:21:44.600</a></span> | <span class="t">one wherever and you put the entire weights on them and then you train batches across there it's still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1311" target="_blank">00:21:51.000</a></span> | <span class="t">cool it still requires a lot of you know servers and different locations but it is still like the whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1317" target="_blank">00:21:57.400</a></span> | <span class="t">weights have to be on every gpu on you know every server so that's kind of what this one is it's it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1324" target="_blank">00:22:04.280</a></span> | <span class="t">basically as straightforward as you would expect um you throw all the weights and you do different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1330" target="_blank">00:22:10.280</a></span> | <span class="t">batches on different stuff and then you have optimization sinking across right so um there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1339" target="_blank">00:22:19.240</a></span> | <span class="t">there's different types there's you can add gradient accumulation you can bucket gradients they talk about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1344" target="_blank">00:22:24.920</a></span> | <span class="t">like efficiencies to do this which i think is not what we should cover in an hour you know if you're at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1350" target="_blank">00:22:30.360</a></span> | <span class="t">a stage where you're doing this they kind of go over what are the main ways to do it but um yeah</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1355" target="_blank">00:22:35.400</a></span> | <span class="t">that's kind of the two so we've gone from single gpu training to now we have data parallel where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1360" target="_blank">00:22:40.840</a></span> | <span class="t">let's say i have a node of four h100s i want to train a llama 70b a llama 7b what i can do is i can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1367" target="_blank">00:22:47.320</a></span> | <span class="t">throw llama 7b on each gpu split my one trillion tokens so let's say like a hundred million tokens split</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1374" target="_blank">00:22:54.760</a></span> | <span class="t">that into batches do each gpu trains a batch and then kind of efficiently chunk them together so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1381" target="_blank">00:23:01.880</a></span> | <span class="t">this is kind of a summary of what's happened so far so if you're at this stage here's what you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1388" target="_blank">00:23:08.920</a></span> | <span class="t">do first determine the best global batch size by either running experiments or just consume uh you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1396" target="_blank">00:23:16.680</a></span> | <span class="t">consult their literature read their blog post to figure out how you should do this then select the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1402" target="_blank">00:23:22.040</a></span> | <span class="t">length this kind of takes a lot of compute what's done in modern models is you train on a small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1408" target="_blank">00:23:28.360</a></span> | <span class="t">sequence length for like 90 of the tokens and then the last 10 you kind of increase so if i'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1413" target="_blank">00:23:33.800</a></span> | <span class="t">mistaken llama did like 14 trillion tokens at 4 000 sequence length and then the last trillion they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1420" target="_blank">00:23:40.360</a></span> | <span class="t">at like 100k then they you know update it's very common because it takes a lot of compute and this kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1425" target="_blank">00:23:45.560</a></span> | <span class="t">works to generalize as well but two to eight thousand token length works well for the evaluations we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1431" target="_blank">00:23:51.720</a></span> | <span class="t">today i think this is something that people don't really look into enough right um sure you could do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1437" target="_blank">00:23:57.880</a></span> | <span class="t">two to eight thousand tokens because that works for mmlu but if you're training something like a llm judge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1443" target="_blank">00:24:03.960</a></span> | <span class="t">classifier you probably don't always need 8 000 tokens right so be smart and train on less tokens if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1449" target="_blank">00:24:09.400</a></span> | <span class="t">need less tokens um then we know the bat size we can find the minimum local bat size for a single gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1457" target="_blank">00:24:17.720</a></span> | <span class="t">and then kind of increase that till you run out of memory then determine the number of gps use you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1463" target="_blank">00:24:23.160</a></span> | <span class="t">have for data parallelism and you kind of throw your stuff across them so cool then you have gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1470" target="_blank">00:24:30.440</a></span> | <span class="t">accumulation if you need it but that's kind of basic data um parallelism now we've kind of got this whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1478" target="_blank">00:24:38.760</a></span> | <span class="t">series of how do we improve upon data problem we've got um 0 1 0 2 0 3 um so what is 0 0 is redundant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1491" target="_blank">00:24:51.000</a></span> | <span class="t">actually let me also check chat real quick if there's anything there's map reduce to kind of do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1497" target="_blank">00:24:57.240</a></span> | <span class="t">stink uh they have fancy versions of map reduce as well numbers are different for reasoning models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1502" target="_blank">00:25:02.920</a></span> | <span class="t">are when it's putting out yeah reasoning models are a little different but that's kind of rl rl is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1508" target="_blank">00:25:08.440</a></span> | <span class="t">at some level similar right you're still just training a transformer um what's happening in that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1514" target="_blank">00:25:14.360</a></span> | <span class="t">transformer is somewhat irrelevant okay so zero what's going on with zero um so data parallelism is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1522" target="_blank">00:25:22.920</a></span> | <span class="t">efficient way to scale training the naive replication of optimizer states gradients and parameters across dp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1530" target="_blank">00:25:30.920</a></span> | <span class="t">introduces significant memory redundancy right so what's happening here is as you in regular data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1538" target="_blank">00:25:38.600</a></span> | <span class="t">parallelism as you split the entire model you also have to at every single gpu do all the optimizer states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1547" target="_blank">00:25:47.320</a></span> | <span class="t">all the gradients and you know all the parameters on every gpu and that's kind of redundancy right so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1553" target="_blank">00:25:53.560</a></span> | <span class="t">what zero one two and three look into is how can we shard more than just the weights so can we sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1561" target="_blank">00:26:01.240</a></span> | <span class="t">shard this optimizer state can we start shard the gradient partitioning so and then there's uh zero three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1568" target="_blank">00:26:08.120</a></span> | <span class="t">which is kind of interesting where you start to actually shard the parameters themselves so uh zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1574" target="_blank">00:26:14.760</a></span> | <span class="t">one zero two are more efficient ways of data sharding basically instead of just model like instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1581" target="_blank">00:26:21.720</a></span> | <span class="t">everything on every gpu can we also shard this optimizer state a little bit and then zero two is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1587" target="_blank">00:26:27.480</a></span> | <span class="t">can we also shard this gradient a little bit okay back into what's happening in memory so um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1595" target="_blank">00:26:35.320</a></span> | <span class="t">in mixed precision training we have parameters we have gradients we have optimizer states which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1601" target="_blank">00:26:41.000</a></span> | <span class="t">are stored in higher precision and then gradients in higher precision if we want gradients in higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1608" target="_blank">00:26:48.200</a></span> | <span class="t">position uh precision so zero one is partitioning the optimizer state so in regular data parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1616" target="_blank">00:26:56.440</a></span> | <span class="t">all ranks gather uh gradients after the backwards pass simultaneously and perform identical optimization steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1624" target="_blank">00:27:04.840</a></span> | <span class="t">that's a lot of duplicated of work we can avoid it using redundant memory stores at the same pad so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1630" target="_blank">00:27:10.600</a></span> | <span class="t">they have this all gather which is kind of um community it's kind of yeah let's shard this um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1637" target="_blank">00:27:17.800</a></span> | <span class="t">optimization state so forward passes happen at the same time backward passes at the same time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1644" target="_blank">00:27:24.680</a></span> | <span class="t">perform a reduced scatter on gradients each replica performs an optimization step on its local optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1651" target="_blank">00:27:31.480</a></span> | <span class="t">steps then you perform this all gather to kind of send these missing replications pldr um they're now just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1660" target="_blank">00:27:40.920</a></span> | <span class="t">sharding optimization uh how they do it they go into more details here now zero two is kind of the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1668" target="_blank">00:27:48.920</a></span> | <span class="t">step we have a reduced shatter optimization where now not only does each replica have to shard optimization but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1677" target="_blank">00:27:57.240</a></span> | <span class="t">they're also calculating gradients right so why don't we shard this gradient optimization state so during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1684" target="_blank">00:28:04.120</a></span> | <span class="t">the backward instead of performing in all reduced we perform a reduced scatter reduced scatter spreads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1690" target="_blank">00:28:10.040</a></span> | <span class="t">gradients needed in memory and it's more memory saving again um that's that's cool they have a chart</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1697" target="_blank">00:28:17.400</a></span> | <span class="t">at the end of this that kind of shows how all of this works um then we have zero three zero three is the fun one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1703" target="_blank">00:28:23.240</a></span> | <span class="t">a fun one so if you can or if you need to if you're training something like let's say you're doing uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1710" target="_blank">00:28:30.680</a></span> | <span class="t">llama 70b or something post training and you want to crazily increase your sequence length right so you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1717" target="_blank">00:28:37.800</a></span> | <span class="t">want to go up to a hundred thousand sequence length well now you're not fitting in even one node this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1723" target="_blank">00:28:43.800</a></span> | <span class="t">where um fsdp starts coming in where you have to shard your parameters themselves so instead of each gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1731" target="_blank">00:28:51.160</a></span> | <span class="t">having every single all the model weights now you're sharding the model weights across different gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1738" target="_blank">00:28:58.040</a></span> | <span class="t">sounds pretty interesting right well in zero three you still need good communication between the gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1744" target="_blank">00:29:04.200</a></span> | <span class="t">there's kind of technical ways that they do this how they do these forward passes these backward passes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1749" target="_blank">00:29:09.480</a></span> | <span class="t">later on we'll learn about sharding across different dimensions the way this works is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1757" target="_blank">00:29:17.320</a></span> | <span class="t">training and all this attention and all of like math all all of this stuff is just matmos right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1764" target="_blank">00:29:24.360</a></span> | <span class="t">it's matrix multiplication so it's row times column there's smart ways to do that and manipulate row times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1771" target="_blank">00:29:31.400</a></span> | <span class="t">column and yeah they they kind of just make it really easy to do it all this explained what's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1777" target="_blank">00:29:37.720</a></span> | <span class="t">on so if interested read it if you really want to use it pretty much any training library whether that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1783" target="_blank">00:29:43.720</a></span> | <span class="t">pytorch or whatever you're using uh you can you know in a few lines of code just implement this and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1789" target="_blank">00:29:49.720</a></span> | <span class="t">handles all of this for you on the back end now um yeah that's that's kind of the three stages uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1798" target="_blank">00:29:58.360</a></span> | <span class="t">let's let's kind of take a break here and see if there's any questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1802" target="_blank">00:30:02.280</a></span> | <span class="t">what's the entity okay for weights activations gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1808" target="_blank">00:30:08.280</a></span> | <span class="t">and optimizer states what's the it's intuition for which ones we want in lower and higher precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1815" target="_blank">00:30:15.320</a></span> | <span class="t">so um for weights you can it kind of also depends on what you're doing if you're doing training or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1821" target="_blank">00:30:21.320</a></span> | <span class="t">inference right um what you're really doing here isn't higher or lower precision there's there's mixed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1827" target="_blank">00:30:27.240</a></span> | <span class="t">precision for some stuff like gradients and activations you do them in higher precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1833" target="_blank">00:30:33.560</a></span> | <span class="t">there's more and more stuff coming out about training and lower precision and mixed precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1839" target="_blank">00:30:39.640</a></span> | <span class="t">but the general um you know general rule of thumb is you want to use as much high precision as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1847" target="_blank">00:30:47.880</a></span> | <span class="t">afford like half precision is cool for inference it's different you can do a lot of quantization but for stuff like um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1855" target="_blank">00:30:55.800</a></span> | <span class="t">gradients or stuff that has not a lot of compute uh not a lot of memory overhead you want to use higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1862" target="_blank">00:31:02.280</a></span> | <span class="t">precision right so gradients are small they don't need a lot of memory do them in high precision model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1868" target="_blank">00:31:08.360</a></span> | <span class="t">weights uh let's try to lower the precision a little bit but yeah okay now we have um tensor parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1877" target="_blank">00:31:17.160</a></span> | <span class="t">tensor parallelism is a way of more splitting it's kind of splitting up model weights across their hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1884" target="_blank">00:31:24.040</a></span> | <span class="t">dimension so uh data parallelism was can we shard the weights this is a way of splitting it up across um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1893" target="_blank">00:31:33.640</a></span> | <span class="t">sensors so you know here's the math of what's happening you're basically doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1900" target="_blank">00:31:40.120</a></span> | <span class="t">math moles rows and columns here's a smart way to do it here's a smart way to reduce it here's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1907" target="_blank">00:31:47.080</a></span> | <span class="t">we're doing in a transformer block um a lot of this is kind of the background behind stuff like flash</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1913" target="_blank">00:31:53.080</a></span> | <span class="t">attention group query attention uh this is used in papers like llama 3 and whatnot but high level this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1920" target="_blank">00:32:00.840</a></span> | <span class="t">is just kind of another way of charting your um your model it's done column wise row wise um there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1930" target="_blank">00:32:10.520</a></span> | <span class="t">there's two different ways to do it there's different communication overhead in both so whether you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1936" target="_blank">00:32:16.280</a></span> | <span class="t">doing column wise or row wise um there's there's different benefits to both you should look into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1943" target="_blank">00:32:23.320</a></span> | <span class="t">them um tensor parallelism and transformer layers is another interesting one so help you make progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1950" target="_blank">00:32:30.600</a></span> | <span class="t">yeah that's there i think for time's sake we're gonna start going a little little quicker um the the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1957" target="_blank">00:32:37.800</a></span> | <span class="t">interesting thing here is kind of this is where as you're all in one node all this stuff kind of works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1963" target="_blank">00:32:43.400</a></span> | <span class="t">right you can do different layers on different gpus but the more you do this the more important the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1969" target="_blank">00:32:49.560</a></span> | <span class="t">interconnect between gpus is right so if you're on a single node of highly interconnected gpus this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1977" target="_blank">00:32:57.160</a></span> | <span class="t">will start to be fine but the minute you're using pcie cards or multiple nodes you're kind of adding too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1983" target="_blank">00:33:03.240</a></span> | <span class="t">much bottleneck for any of this to be worth um yeah next we kind of have sequence parallelism i believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1992" target="_blank">00:33:12.520</a></span> | <span class="t">context parallelism context parallelism is or we went through sequence parallelism sequence parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=1998" target="_blank">00:33:18.360</a></span> | <span class="t">kind of same thing um so instead of tensor parallelism split it across the sequence this is where stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2006" target="_blank">00:33:26.200</a></span> | <span class="t">like ring attention starts to come in i think a while ago we had a paper club on ring attention so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2012" target="_blank">00:33:32.360</a></span> | <span class="t">for context and sequence parallelism if instead you know you can't scale your context length uh you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2019" target="_blank">00:33:39.480</a></span> | <span class="t">to take these 128 000 sequences and you want to split them across gpus you want to let's say have like you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2027" target="_blank">00:33:47.000</a></span> | <span class="t">know first 10 000 tokens on gpu one next 10 000 on gpu two and so on um there's kind of this cool little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2035" target="_blank">00:33:55.480</a></span> | <span class="t">way that you can do this so you can you know this goes into the math about what's happening but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2041" target="_blank">00:34:01.400</a></span> | <span class="t">it's kind of it's kind of so conceptually you think what is attention attention is the relationship</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2047" target="_blank">00:34:07.560</a></span> | <span class="t">between all tokens right you still need to calculate this so if you have a hundred thousand tokens we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2053" target="_blank">00:34:13.880</a></span> | <span class="t">to know how does each token relate to every other token which you know you would kind of expect that to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2059" target="_blank">00:34:19.480</a></span> | <span class="t">have to happen on one gpu you need to look at all tokens that wants to do attention instead you can kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2065" target="_blank">00:34:25.560</a></span> | <span class="t">shard them and do this ring attention where you you have each gpu do part of the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2071" target="_blank">00:34:31.880</a></span> | <span class="t">you kind of sync them you send qks between each other then you do this big query uh value calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2078" target="_blank">00:34:38.040</a></span> | <span class="t">and now you've kind of done attention across gpus um there's there's ring attention there's zigzag</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2084" target="_blank">00:34:44.280</a></span> | <span class="t">attention there's little optimizations for this stuff but it's it's pretty cool it was a lot of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2089" target="_blank">00:34:49.960</a></span> | <span class="t">how do we get this infinite context how can we do um you know long context this is kind of what happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2096" target="_blank">00:34:56.360</a></span> | <span class="t">here um very very interesting stuff i think it's a little underrated so basically reading a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2103" target="_blank">00:35:03.160</a></span> | <span class="t">about it is pretty cool context parallelism and sequence parallelism okay next is uh pipeline parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2111" target="_blank">00:35:11.480</a></span> | <span class="t">pipeline parallelism is a pretty fun one so this is where you distribute consecutive layers so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2118" target="_blank">00:35:18.600</a></span> | <span class="t">instead of just sharding gradients or optimizations or sequences how about we start uh sharding different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2125" target="_blank">00:35:25.160</a></span> | <span class="t">layers so let's have like you know layer one two three on this gpu and different layers on different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2131" target="_blank">00:35:31.480</a></span> | <span class="t">gpus this once again it's like another approach to fitting really large models it it lets you reduce gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2138" target="_blank">00:35:38.200</a></span> | <span class="t">overhead for everything but once again like now they go into as you do this how do you continue training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2145" target="_blank">00:35:45.480</a></span> | <span class="t">right so they have these like all forward all backward so as you do your forward pass you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2152" target="_blank">00:35:52.040</a></span> | <span class="t">to do a backward pass compute gradients to optimization so once you split up layers they've kind of got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2158" target="_blank">00:35:58.600</a></span> | <span class="t">here's what's happening as we want to do optimization right so all forward all backward um there's there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2165" target="_blank">00:36:05.320</a></span> | <span class="t">few other ways of this one forward one backward this is what llama three did um very interesting little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2172" target="_blank">00:36:12.120</a></span> | <span class="t">optimizations to make this stuff work um interleaving stages so if you've got different slices there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2181" target="_blank">00:36:21.000</a></span> | <span class="t">just different recipes for all this zero bubble dual pipe um yeah expert parallelism is kind of a more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2189" target="_blank">00:36:29.560</a></span> | <span class="t">straightforward one where you have different experts in a mixture of experts model on different gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2195" target="_blank">00:36:35.800</a></span> | <span class="t">that one's pretty straightforward right so you kind of have a base router you have every gpu have its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2201" target="_blank">00:36:41.720</a></span> | <span class="t">own expert in some cases you can have multiple gpu multiple experts on multiple gpus and you kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2208" target="_blank">00:36:48.440</a></span> | <span class="t">have this efficient expert parallelism so once again now there's this kind of overview summary so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2215" target="_blank">00:36:55.880</a></span> | <span class="t">what's data parallelism it's along the batch dimension tensor parallelism is along the hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2220" target="_blank">00:37:00.920</a></span> | <span class="t">dimension so different layers sequence and context parallelism are if you need to extend context length and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2227" target="_blank">00:37:07.800</a></span> | <span class="t">you want to let's say train your model up to 500 000 token context how can we do that on a limited gpu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2235" target="_blank">00:37:15.080</a></span> | <span class="t">pipeline parallelism is along model layers and then expert parallelism is around mixture of experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2241" target="_blank">00:37:21.880</a></span> | <span class="t">um then we've also got we can combine these with the zero stages right so zero one is sharding optimizers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2248" target="_blank">00:37:28.920</a></span> | <span class="t">zero two is sharding optimizers and gradients zero three is sharding um parameters alongside the other two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2255" target="_blank">00:37:35.880</a></span> | <span class="t">so this is kind of the high level if you have multiple gpus here are the things that you can do for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2263" target="_blank">00:37:43.480</a></span> | <span class="t">each one they kind of have you know here's how you double click into what you want to do so let's say you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2269" target="_blank">00:37:49.240</a></span> | <span class="t">doing pipeline parallelism and you're sharding everything and you then want to extend your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2274" target="_blank">00:37:54.920</a></span> | <span class="t">sequence length right so let's go back into that section um you can come in and find out what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2280" target="_blank">00:38:00.760</a></span> | <span class="t">what are different interleaving stages how do you want to optimize all this um what they go into here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2287" target="_blank">00:38:07.080</a></span> | <span class="t">kind of still all at the you're not really doing research here you're kind of still at the applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2292" target="_blank">00:38:12.760</a></span> | <span class="t">researcher stage right all this stuff is still available in libraries like pytorch so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2297" target="_blank">00:38:17.720</a></span> | <span class="t">there are cuda kernels that optimize this stuff like flash attention is made that already uses all of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2304" target="_blank">00:38:24.280</a></span> | <span class="t">these are things that you can just kind of consume as you're doing your training if you're pushing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2310" target="_blank">00:38:30.280</a></span> | <span class="t">bound like um xai now has a 200 800 cluster right or gpt 4.5 was a very chunky model that's where they have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2319" target="_blank">00:38:39.480</a></span> | <span class="t">start to push on these ideas and develop even more novel um training strategies but yeah that's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2329" target="_blank">00:38:49.160</a></span> | <span class="t">their high level um what's going on in different parallelization strategies after that we've kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2337" target="_blank">00:38:57.400</a></span> | <span class="t">got here's a recipe for finding the best um training config right so if you kind of skip everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2345" target="_blank">00:39:05.080</a></span> | <span class="t">the original intro is kind of important right how do we train on one gpu how do we train on two how do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2350" target="_blank">00:39:10.200</a></span> | <span class="t">we train on a single node then the middle is kind of okay now your gpu rich you're probably smarter than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2355" target="_blank">00:39:15.400</a></span> | <span class="t">me how do you train across nodes um some of the reasons for some of these as well by the way that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2360" target="_blank">00:39:20.680</a></span> | <span class="t">skipped over was if you have multiple nodes and you don't have interconnect some of these different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2366" target="_blank">00:39:26.280</a></span> | <span class="t">parallelization strategies will work better they they better optimize for not having that interconnect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2372" target="_blank">00:39:32.920</a></span> | <span class="t">between all nodes um so you know there's recipes for that as well but okay high level let's let's sum it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2380" target="_blank">00:39:40.840</a></span> | <span class="t">all up here again so if you want to find your best training config how do you do it so step one uh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2387" target="_blank">00:39:47.240</a></span> | <span class="t">fitting a training step into memory right let's figure out how we can fit a full model instance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2393" target="_blank">00:39:53.240</a></span> | <span class="t">on the gpu there's several use cases whether you're gpu rich or your gpu poor if your gpu poor um you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2401" target="_blank">00:40:01.320</a></span> | <span class="t">there's full activation re-computation there's gradient accumulation there's stuff like that you can train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2406" target="_blank">00:40:06.840</a></span> | <span class="t">slower um basically you want to increase your gradient accumulation steps to processor larger brass sizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2413" target="_blank">00:40:13.800</a></span> | <span class="t">and you kind of do what you do for gpu rich stuff if you have models under 10b use single parallelization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2421" target="_blank">00:40:21.400</a></span> | <span class="t">stuff so tensor parallel with um zero three across eight gpus this is kind of where you shard those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2428" target="_blank">00:40:28.360</a></span> | <span class="t">model weights across eight gpus for stuff between 10 and 100 billion parameters you'll need more than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2434" target="_blank">00:40:34.040</a></span> | <span class="t">eight gpus right they keep saying eight gpus because this is basically one node if you have stuff under 10b and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2440" target="_blank">00:40:40.360</a></span> | <span class="t">one node use fsdp with zero three if you have 10 to 100b you're going to need multiple nodes so this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2447" target="_blank">00:40:47.960</a></span> | <span class="t">where you're going to start to do tensor parallelism and pipeline parallelism do tensor parallelism with data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2453" target="_blank">00:40:53.240</a></span> | <span class="t">parallelism uh only use zero three then when you're rich with with 512 or a thousand gpus um you know you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2462" target="_blank">00:41:02.920</a></span> | <span class="t">should not be reading this but there's there's more stuff that you can do here um tpu poor we talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2468" target="_blank">00:41:08.840</a></span> | <span class="t">about it basically gradient accumulation find out what batch size can you support split batches do micro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2476" target="_blank">00:41:16.120</a></span> | <span class="t">batches but don't overdo it um yeah step two achieve the target batch size so depending on where step one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2483" target="_blank">00:41:23.400</a></span> | <span class="t">left us uh in terms of micro batches and our data parallelism our current batch size might be too small or too big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2489" target="_blank">00:41:29.640</a></span> | <span class="t">right so what you want to do is kind of scale up data parallelization or accumulation step so you know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2495" target="_blank">00:41:35.480</a></span> | <span class="t">keep doing gradient accumulation until you're no longer running out of memory and if you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2501" target="_blank">00:41:41.480</a></span> | <span class="t">start to do long context add in context parallelism so this kind of ring attention stuff to decrease our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2507" target="_blank">00:41:47.880</a></span> | <span class="t">global batch size we can reduce data parallelism and we can reduce context parallelism if you have more gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2514" target="_blank">00:41:54.920</a></span> | <span class="t">okay step three optimizing training throughput set up tensor parallelism using interconnect if you have it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2523" target="_blank">00:42:03.080</a></span> | <span class="t">then there's uh increased data parallelism so you know if you have more batches if you have more gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2530" target="_blank">00:42:10.760</a></span> | <span class="t">yeah use data parallelism and throw the whole weight on them keep the target batch size as big as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2537" target="_blank">00:42:17.720</a></span> | <span class="t">um try scaling up different parallelism parallelisms one by one and just keep experimenting uh benchmarking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2547" target="_blank">00:42:27.400</a></span> | <span class="t">they ran a lot of benchmarks i don't know if anyone found anything interesting in any of these that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2551" target="_blank">00:42:31.960</a></span> | <span class="t">want to dive into but i want to leave 10 minutes for questions so um i'm going to skip through a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2557" target="_blank">00:42:37.560</a></span> | <span class="t">bit of this uh diving into gpus fusing threading so what is gpu how does gpu work you know there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2565" target="_blank">00:42:45.080</a></span> | <span class="t">high bandwidth memory there's interconnect there's global stores there's caching there's kv caching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2571" target="_blank">00:42:51.080</a></span> | <span class="t">there's cuda kernels and stuff that optimizes all this um i don't think this is as relevant with the high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2579" target="_blank">00:42:59.080</a></span> | <span class="t">level of if you want to train models across tpus what do you do so if you're interested read it it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2586" target="_blank">00:43:06.360</a></span> | <span class="t">pretty useful stuff but at a high level i think it's a little too niche for our um here's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2594" target="_blank">00:43:14.360</a></span> | <span class="t">how mapmoles happened let's see i think there's a conclusion at the end of this flash attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2600" target="_blank">00:43:20.600</a></span> | <span class="t">is um you know cuda optimizations for all this ring attention is one mixed precision training is another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2607" target="_blank">00:43:27.080</a></span> | <span class="t">interesting one um mixed precision training is one thing but mixed precision inference is another we have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2613" target="_blank">00:43:33.800</a></span> | <span class="t">lot more on uh mixed precision inference but for training um they kind of go into accumulation what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2622" target="_blank">00:43:42.360</a></span> | <span class="t">to have different precision for different training stuff fpa pre-training is kind of interesting some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2628" target="_blank">00:43:48.040</a></span> | <span class="t">people are doing it we want to see if it works um recent research including fpa lm deep seek v3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2635" target="_blank">00:43:55.480</a></span> | <span class="t">which is an interesting one um it shows potential that's kind of huge so the the big thing with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2642" target="_blank">00:44:02.040</a></span> | <span class="t">precision is basically as you cut your precision in half you also cut the vram needed to train in half so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2649" target="_blank">00:44:09.480</a></span> | <span class="t">full precision versus half precision for 600b model means you know i can now cut off 600 billion 600 gigs of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2656" target="_blank">00:44:16.760</a></span> | <span class="t">vram so pretty big stuff with precision okay conclusion uh we know we now know how to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2664" target="_blank">00:44:24.280</a></span> | <span class="t">we now know how stuff was efficiently trained like gamma 405b and v3 on multiple gpus uh once again they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2671" target="_blank">00:44:31.000</a></span> | <span class="t">kind of have their their cheat sheets if you want cheat sheets i think it's good to go over this again um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2678" target="_blank">00:44:38.200</a></span> | <span class="t">there's there's there's a lot of stuff about fpa versus intake and how they're not the same so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2683" target="_blank">00:44:43.480</a></span> | <span class="t">quantization yeah um you should look a little bit deeper into stuff like that um llama's 40 parallelism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2693" target="_blank">00:44:53.320</a></span> | <span class="t">what else um a lot of papers a lot of references um yeah pretty pretty solid interesting little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2702" target="_blank">00:45:02.040</a></span> | <span class="t">high level overview of how to train it i think uh as a quick recap we should just remember that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2708" target="_blank">00:45:08.760</a></span> | <span class="t">have this you know here's how to fit stuff into memory pick your model size pick your gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2714" target="_blank">00:45:14.760</a></span> | <span class="t">pick your batch size and optimize your training throughput until stuff doesn't crash here are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2721" target="_blank">00:45:21.000</a></span> | <span class="t">different parallelization strategies we talked about right so data parallel everything on one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2725" target="_blank">00:45:25.400</a></span> | <span class="t">zero one zero two zero three is where you start sharding the weights tensor parallel across hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2731" target="_blank">00:45:31.480</a></span> | <span class="t">dimensions pipeline parallel across model layers context parallel for sequence length expert parallel for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2739" target="_blank">00:45:39.560</a></span> | <span class="t">mixture of experts um pretty nice overview i think you know for the ai engineer crowd even if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2746" target="_blank">00:45:46.920</a></span> | <span class="t">have five hundred or a thousand gpus just kind of familiarizing yourself with what's going on what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2753" target="_blank">00:45:53.800</a></span> | <span class="t">training like what parts of model and pre-training takes how much gpu um you know like yeah it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2760" target="_blank">00:46:00.920</a></span> | <span class="t">all attention there's these speed forward layers there's these active there's these activation states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2765" target="_blank">00:46:05.880</a></span> | <span class="t">there's these gradients we have to calculate how those actually take up more than half of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2770" target="_blank">00:46:10.040</a></span> | <span class="t">training in front uh training memory all that stuff is pretty interesting to know um then the one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2776" target="_blank">00:46:16.600</a></span> | <span class="t">that this blog i might have missed or it didn't do for me was it didn't mention that you know when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2782" target="_blank">00:46:22.840</a></span> | <span class="t">actually do inference and you consume this stuff um you're actually not accumulating a lot of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2788" target="_blank">00:46:28.920</a></span> | <span class="t">gradients let me find this little diagram again so for example if we're doing let's say um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2795" target="_blank">00:46:35.240</a></span> | <span class="t">um llama oh that's cool this doesn't want to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2801" target="_blank">00:46:41.400</a></span> | <span class="t">let me refresh so if we're doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2809" target="_blank">00:46:49.240</a></span> | <span class="t">llama</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2816" target="_blank">00:46:56.040</a></span> | <span class="t">where'd it go there it is if we're doing llama 3 8b at a sequence length of 60 of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2823" target="_blank">00:47:03.960</a></span> | <span class="t">a thousand um all these parameters gradients that are taking a hundred gigs of memory this optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2835" target="_blank">00:47:15.800</a></span> | <span class="t">these optimizers you know 70 gigs of vram we don't do this during inference we don't have to do all this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2840" target="_blank">00:47:20.680</a></span> | <span class="t">back prop so the attention is you know some of it but all this other stuff for training is actually a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2847" target="_blank">00:47:27.080</a></span> | <span class="t">lot less and we can quantize better so it didn't it didn't hit on that as much but yeah high level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2852" target="_blank">00:47:32.520</a></span> | <span class="t">good read to kind of throw this thing in chat gpt or cloud or whatever your lm of choices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2860" target="_blank">00:47:40.680</a></span> | <span class="t">stay you're reading this section have a breakdown each section for you spend an hour on it it's useful to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2866" target="_blank">00:47:46.120</a></span> | <span class="t">know for the ai engineer um kind of understanding the difference between training and inference is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2871" target="_blank">00:47:51.800</a></span> | <span class="t">always useful and i think um you know a good rule of thumb is like if you're hiring a research intern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2879" target="_blank">00:47:59.320</a></span> | <span class="t">would they be able to explain all of this to you hopefully pros and cons trade-offs different multi-node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2887" target="_blank">00:48:07.800</a></span> | <span class="t">single node single gpu um if they could that's great and then you should be able to do it yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2894" target="_blank">00:48:14.520</a></span> | <span class="t">too right it's just good general knowledge so pretend you're in an interview be able to learn all this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2898" target="_blank">00:48:18.760</a></span> | <span class="t">stuff it's good background knowledge but yeah um that's kind of high level overview of this big big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2905" target="_blank">00:48:25.640</a></span> | <span class="t">pre-training blog um questions thoughts comments we have five minutes i'm sure i skipped out a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2912" target="_blank">00:48:32.440</a></span> | <span class="t">stuff i'm sure people have a lot to add in uh out of curiosity what precision or state of the art model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2919" target="_blank">00:48:39.720</a></span> | <span class="t">is trained in mostly uh higher precision for gradients so there there is mixed precision as of recently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2926" target="_blank">00:48:46.600</a></span> | <span class="t">um you know we're cutting that down i think it said that v3 was trained in fpa which is kind of interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2934" target="_blank">00:48:54.840</a></span> | <span class="t">but there's still a lot a lot of precision um yeah that's that's paper thoughts comments questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2948" target="_blank">00:49:08.120</a></span> | <span class="t">who wants to train 70b from scratch knowing all this are we prepared to blow thousands of dollars an hour</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2962" target="_blank">00:49:22.120</a></span> | <span class="t">i think it's a good experiment to run like uh realistically h100s you can rent at two dollars</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2970" target="_blank">00:49:30.760</a></span> | <span class="t">an hour right so rent an h100 train a 7b rent two h100s on different like you know non um non good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2980" target="_blank">00:49:40.040</a></span> | <span class="t">interconnect try some of this stuff because it's very approachable from like common libraries to do this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2985" target="_blank">00:49:45.560</a></span> | <span class="t">training then rent a node then rent two nodes and like realistically you only have like a few different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2991" target="_blank">00:49:51.560</a></span> | <span class="t">parallelization strategies right um a precursor to this blog from hugging face is fine web it goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=2997" target="_blank">00:49:57.800</a></span> | <span class="t">over like i think five trillion maybe 15 trillion pre-training tokens so yeah go go learn um very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3004" target="_blank">00:50:04.200</a></span> | <span class="t">useful to learn and just try all this stuff and see what's different and burn burn not too much money</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3009" target="_blank">00:50:09.480</a></span> | <span class="t">right two dollars an hour for h100s eight h100s twenty cents an hour like we can afford this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3021" target="_blank">00:50:21.000</a></span> | <span class="t">um papers that can help me do this for non-english languages um the presenter from last time works on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3029" target="_blank">00:50:29.560</a></span> | <span class="t">a state-of-the-art german model not german some some multilingual model so go to paper club someone is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3036" target="_blank">00:50:36.760</a></span> | <span class="t">apparently an expert in non non um english models cheapest h100 provider on the internet crypto</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3043" target="_blank">00:50:43.240</a></span> | <span class="t">bullshit the crypto companies are raising a lot of money and subsidizing gpus to be very very cheap deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3049" target="_blank">00:50:49.640</a></span> | <span class="t">info has them for two dollars model will give you money a lot of people will also sponsor compute but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3055" target="_blank">00:50:55.640</a></span> | <span class="t">also um hyperbolic is a good one shout out hyperbolic i think uh it's also pretty cheap right like two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3061" target="_blank">00:51:01.720</a></span> | <span class="t">dollars an hour three dollars four dollars sf compute is not um serverless i believe sf compute is bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3069" target="_blank">00:51:09.640</a></span> | <span class="t">cluster at more time i like the guys but um as run pod and stuff adds 32 h100 64 i don't know if we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3079" target="_blank">00:51:19.000</a></span> | <span class="t">but yeah cool um what other questions questions are on where to get gpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3088" target="_blank">00:51:28.440</a></span> | <span class="t">pay money that's not that expensive very subsidized right now like to a point where somewhat more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3095" target="_blank">00:51:35.000</a></span> | <span class="t">subsidized than um</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3096" target="_blank">00:51:36.360</a></span> | <span class="t">than compute people don't like hyperbolic apparently so never mind no hyperbolic we don't like them i guess</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3105" target="_blank">00:51:45.960</a></span> | <span class="t">um but they are slightly cheaper than than the others but we don't like them i guess okay volunteers for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3111" target="_blank">00:51:51.400</a></span> | <span class="t">next week um who wants to cover paper next week there is a cool pre-training uh post-training survey</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3119" target="_blank">00:51:59.240</a></span> | <span class="t">uh this is uh i don't know what is um i don't know if it's good uh qw32b came out i don't know if it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3125" target="_blank">00:52:05.720</a></span> | <span class="t">thinking the same name oh is is the is the paper good i haven't looked i don't think there's a paper if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3131" target="_blank">00:52:11.240</a></span> | <span class="t">the paper comes out that would be cool i don't know no paper at all blog post which was shitty blog post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3137" target="_blank">00:52:17.720</a></span> | <span class="t">4496 word blog post you know thought they had a very sick title on this uh embracing the power of rl</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3147" target="_blank">00:52:27.720</a></span> | <span class="t">700 word blog post that has nothing but here's code on how to use it okay um llm post training there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3158" target="_blank">00:52:38.760</a></span> | <span class="t">survey paper on post training very cool little chart here um i think it's a good one if someone wants to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3166" target="_blank">00:52:46.840</a></span> | <span class="t">cover it okay i'm gonna volunteer this one to someone if someone's down otherwise um yeah if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3183" target="_blank">00:53:03.000</a></span> | <span class="t">there's no more questions oh actually this is pretty short so uh 20 page on post training very useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3190" target="_blank">00:53:10.040</a></span> | <span class="t">graph of thoughts wait these do cover mcts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3194" target="_blank">00:53:14.520</a></span> | <span class="t">why not mcts is great dude you know how much of a time sink this was you know how much time was wasted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3202" target="_blank">00:53:22.600</a></span> | <span class="t">on mcts who remembers strawberry who remembers multi on saying that they are q star strawberry mcts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3210" target="_blank">00:53:30.280</a></span> | <span class="t">waste of time but you know pretty good and i think it's good background right so mcts is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3219" target="_blank">00:53:39.960</a></span> | <span class="t">tree search throughout potential next tokens can we span out and do um inference time scaling test time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3228" target="_blank">00:53:48.040</a></span> | <span class="t">compute instead of rl a bunch of people wasted a bunch of time on this uh some companies said that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3234" target="_blank">00:53:54.200</a></span> | <span class="t">they are q star by by doing this but yeah okay another um another paper that someone has recommended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3243" target="_blank">00:54:03.400</a></span> | <span class="t">that we don't know if it is credible the fft strikes back an efficient alternative to self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3250" target="_blank">00:54:10.200</a></span> | <span class="t">from one guy at usc is he plugging his own paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3254" target="_blank">00:54:14.360</a></span> | <span class="t">i don't know uh alternatives to attention are always interesting but okay um that's that's paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3263" target="_blank">00:54:23.960</a></span> | <span class="t">club this week guys super nicely done thank you so much you covered two days of reading in one hour</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3271" target="_blank">00:54:31.000</a></span> | <span class="t">always need to cover quick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3274" target="_blank">00:54:34.040</a></span> | <span class="t">in general yeah i i feel like there needs to be more good papers like i i feel like we uh paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3282" target="_blank">00:54:42.680</a></span> | <span class="t">i feel like paper velocity has slowed somehow yeah i mean this isn't even a paper on anything of their own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3291" target="_blank">00:54:51.800</a></span> | <span class="t">this is just like uh okay this counts this counts okay very very good survey okay well that's paper club</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=x8E0lLITkwo&t=3300" target="_blank">00:55:00.280</a></span> | <span class="t">guys all right take care thank you</span></div></div></body></html>