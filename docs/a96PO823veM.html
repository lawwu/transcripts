<html><head><title>Automated Synchronization of Driving Data: Video, Audio, IMU, and Telemetry</title></head><body>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    <a href="index.html">back to index</a><h2>Automated Synchronization of Driving Data: Video, Audio, IMU, and Telemetry</h2><a href="https://www.youtube.com/watch?v=a96PO823veM"><img src="https://i.ytimg.com/vi_webp/a96PO823veM/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./a96PO823veM.html">Whisper Transcript</a> | <a href="./transcript_a96PO823veM.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=0">00:00:00.000</a></span> | <span class="t">[car driving]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=7">00:00:07.000</a></span> | <span class="t">This is a quick demo of how car vibration and steering events can be used to synchronize driving data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=12">00:00:12.000</a></span> | <span class="t">The video itself is a visualization of the data streams we're working with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=17">00:00:17.000</a></span> | <span class="t">The audio you're hearing in the background, besides my voice, is from a shotgun microphone placed behind the rear right tire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=25">00:00:25.000</a></span> | <span class="t">The middle column has three images, each from a different webcam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=30">00:00:30.000</a></span> | <span class="t">Front, dashboard, and face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=33">00:00:33.000</a></span> | <span class="t">The dashboard video has an overlaid steering wheel icon that is visualizing the position of the steering wheel as supported by the CAN network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=41">00:00:41.000</a></span> | <span class="t">The top left image shows the dense optical flow in the video of the forward roadway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=49">00:00:49.000</a></span> | <span class="t">The bottom left just shows our location on a map. We're in beautiful Cambridge, Massachusetts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=57">00:00:57.000</a></span> | <span class="t">And the rest are plots showing the ten second window around the current measurement of various sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=65">00:01:05.000</a></span> | <span class="t">On the left are the horizontal optical flow in the front video and the steering wheel position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=73">00:01:13.000</a></span> | <span class="t">On the right are the audio energy from the shotgun microphone, the Y component of the optical flow from the three webcams, and finally the Z axis of the accelerometer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=87">00:01:27.000</a></span> | <span class="t">What we would like to do is to synchronize all of these sensors, either online or offline as a post-processing step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=95">00:01:35.000</a></span> | <span class="t">We do this by first synchronizing the video of the forward roadway with the CAN network by looking at steering events.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=105">00:01:45.000</a></span> | <span class="t">When you make a turn, like the one coming up here, the horizontal optical flow will be negative if it's a right turn and positive if it's a left turn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=116">00:01:56.000</a></span> | <span class="t">Coming up here is a left turn and you will see in the top left image the dense optical flow will light up all the same color.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=125">00:02:05.000</a></span> | <span class="t">It will be a positive value since it's a left turn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=130">00:02:10.000</a></span> | <span class="t">We can then determine the optimal shift for the synchronization between the steering wheel and the forward video by computing the cross correlation function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=141">00:02:21.000</a></span> | <span class="t">the maximum value for the cross correlation function, to determine the shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=147">00:02:27.000</a></span> | <span class="t">In the same way, we synchronize the rest of the sensors with the video of the forward roadway using vibration events.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=156">00:02:36.000</a></span> | <span class="t">On the right are five plots showing the audio energy, the Y component of the optical flow for the three webcams, and the Z axis of the accelerometer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=165">00:02:45.000</a></span> | <span class="t">each capturing the vibration of the car caused by the road.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=169">00:02:49.000</a></span> | <span class="t">A few examples are coming up shortly here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=172">00:02:52.000</a></span> | <span class="t">So steering and vibration gives us a signal that we can use for passive synchronization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=197">00:03:17.000</a></span> | <span class="t">The result is a synchronized data set which is important both for the analysis of driver behavior and for the design of ADAS systems that use decision fusion to make real-time prediction based on multiple sensor streams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=a96PO823veM&t=211">00:03:31.000</a></span> | <span class="t">The paper along with a sample data set and source code are available in the description.</span></div></div></body></html>