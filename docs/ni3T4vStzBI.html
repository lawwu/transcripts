<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 10: Wrap-up I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 10: Wrap-up I Spring 2023</h2><a href="https://www.youtube.com/watch?v=ni3T4vStzBI"><img src="https://i.ytimg.com/vi/ni3T4vStzBI/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=30">0:30</a> Other noteworthy architectures<br><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=127">2:7</a> BERT: Known limitations<br><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=205">3:25</a> Pretraining data<br><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=257">4:17</a> Current trends<br><br><div style="text-align: left;"><a href="./ni3T4vStzBI.html">Whisper Transcript</a> | <a href="./transcript_ni3T4vStzBI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=6" target="_blank">00:00:06.080</a></span> | <span class="t">This is the 10th and final screencast in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=8" target="_blank">00:00:08.220</a></span> | <span class="t">our series on contextual representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=10" target="_blank">00:00:10.520</a></span> | <span class="t">I'd like to just briefly wrap up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=12" target="_blank">00:00:12.460</a></span> | <span class="t">In doing that, I'd like to do three things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=14" target="_blank">00:00:14.520</a></span> | <span class="t">First, just take stock of what we did a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=17" target="_blank">00:00:17.280</a></span> | <span class="t">Second, I'd like to make amends for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=19" target="_blank">00:00:19.460</a></span> | <span class="t">really interesting architectures and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=21" target="_blank">00:00:21.320</a></span> | <span class="t">innovations that I didn't have time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=22" target="_blank">00:00:22.880</a></span> | <span class="t">to mention in the core series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=24" target="_blank">00:00:24.640</a></span> | <span class="t">Then finally, I'd like to look to the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=26" target="_blank">00:00:26.960</a></span> | <span class="t">both for the course and also for the field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=30" target="_blank">00:00:30.600</a></span> | <span class="t">Let me start by trying to make amends a little bit for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=34" target="_blank">00:00:34.000</a></span> | <span class="t">some noteworthy architectures that I didn't have time for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=37" target="_blank">00:00:37.500</a></span> | <span class="t">Transformer XL is an early and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=40" target="_blank">00:00:40.280</a></span> | <span class="t">very innovative attempt to bring in long contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=43" target="_blank">00:00:43.680</a></span> | <span class="t">It does this by essentially caching</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=46" target="_blank">00:00:46.400</a></span> | <span class="t">earlier parts of a long sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=48" target="_blank">00:00:48.840</a></span> | <span class="t">and then recreating some recurrent connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=51" target="_blank">00:00:51.640</a></span> | <span class="t">across those cache states into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=53" target="_blank">00:00:53.760</a></span> | <span class="t">the computation for the current set of states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=56" target="_blank">00:00:56.360</a></span> | <span class="t">Very innovative. The ideas for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=58" target="_blank">00:00:58.680</a></span> | <span class="t">Transformer XL were carried forward into ExcelNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=61" target="_blank">00:01:01.960</a></span> | <span class="t">The core of ExcelNet is the goal of having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=64" target="_blank">00:01:04.760</a></span> | <span class="t">bidirectional context while nonetheless</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=67" target="_blank">00:01:07.280</a></span> | <span class="t">having an autoregressive language modeling loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=70" target="_blank">00:01:10.600</a></span> | <span class="t">They do this in this really interesting way of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=73" target="_blank">00:01:13.920</a></span> | <span class="t">sampling different sequence orders so that you process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=77" target="_blank">00:01:17.400</a></span> | <span class="t">left to right while nonetheless sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=79" target="_blank">00:01:19.820</a></span> | <span class="t">enough sequence orders that you essentially have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=82" target="_blank">00:01:22.080</a></span> | <span class="t">the power of bidirectional context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=85" target="_blank">00:01:25.480</a></span> | <span class="t">Then de Berta is really interesting from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=88" target="_blank">00:01:28.720</a></span> | <span class="t">the perspective of our discussion of positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=91" target="_blank">00:01:31.800</a></span> | <span class="t">In that screencast, I expressed a concern that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=94" target="_blank">00:01:34.400</a></span> | <span class="t">the positional encoding representations were exerting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=97" target="_blank">00:01:37.840</a></span> | <span class="t">in some cases, too much influence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=100" target="_blank">00:01:40.120</a></span> | <span class="t">on the representations of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=102" target="_blank">00:01:42.600</a></span> | <span class="t">De Berta can be seen as an attempt to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=105" target="_blank">00:01:45.000</a></span> | <span class="t">decouple word from position somewhat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=107" target="_blank">00:01:47.520</a></span> | <span class="t">It does that by decoupling those core representations and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=110" target="_blank">00:01:50.920</a></span> | <span class="t">having distinct attention mechanisms to those two parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=115" target="_blank">00:01:55.200</a></span> | <span class="t">My guiding intuition here is that de Berta will allow words to have more of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=119" target="_blank">00:01:59.440</a></span> | <span class="t">their wordhood separate from where they might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=122" target="_blank">00:02:02.000</a></span> | <span class="t">have appeared in the input string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=124" target="_blank">00:02:04.840</a></span> | <span class="t">That seems very healthy to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=127" target="_blank">00:02:07.320</a></span> | <span class="t">When I talked about BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=129" target="_blank">00:02:09.400</a></span> | <span class="t">I listed out some known limitations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=131" target="_blank">00:02:11.440</a></span> | <span class="t">There were four of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=133" target="_blank">00:02:13.040</a></span> | <span class="t">I gave credit to Roberta for addressing the first one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=136" target="_blank">00:02:16.280</a></span> | <span class="t">which was around design decisions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=138" target="_blank">00:02:18.600</a></span> | <span class="t">I gave credit to Elektra for addressing items 2 and 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=142" target="_blank">00:02:22.640</a></span> | <span class="t">where 2 was about the artificial nature of the mask token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=146" target="_blank">00:02:26.120</a></span> | <span class="t">and 3 was about the inefficiency of MLM training in the BERT context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=151" target="_blank">00:02:31.360</a></span> | <span class="t">I haven't yet touched on the fourth item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=153" target="_blank">00:02:33.760</a></span> | <span class="t">The fourth item is from Yang et al,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=155" target="_blank">00:02:35.440</a></span> | <span class="t">which is the ExcelNet paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=157" target="_blank">00:02:37.280</a></span> | <span class="t">ExcelNet indeed addresses this concern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=159" target="_blank">00:02:39.880</a></span> | <span class="t">The concern is just that BERT assumes the predicted tokens are independent of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=164" target="_blank">00:02:44.200</a></span> | <span class="t">each other given the unmasked tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=166" target="_blank">00:02:46.760</a></span> | <span class="t">which is oversimplified as high-order,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=169" target="_blank">00:02:49.200</a></span> | <span class="t">long-range dependency is prevalent in natural language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=172" target="_blank">00:02:52.880</a></span> | <span class="t">The guiding idea behind ExcelNet is that in having an autoregressive language modeling loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=179" target="_blank">00:02:59.000</a></span> | <span class="t">we bring in some of the conditional probabilities that help us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=182" target="_blank">00:03:02.760</a></span> | <span class="t">overcome this artificial statistical nature of the MLM objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=187" target="_blank">00:03:07.280</a></span> | <span class="t">But remember, the interesting aspect of ExcelNet is that we still have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=191" target="_blank">00:03:11.520</a></span> | <span class="t">bidirectional context and this comes from sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=194" target="_blank">00:03:14.720</a></span> | <span class="t">all of those permutation orders of the input string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=198" target="_blank">00:03:18.240</a></span> | <span class="t">Really interesting to think about and also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=201" target="_blank">00:03:21.160</a></span> | <span class="t">a lovely insight about the nature of BERT itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=205" target="_blank">00:03:25.440</a></span> | <span class="t">I didn't get to discuss pre-training data really at all in this series,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=210" target="_blank">00:03:30.920</a></span> | <span class="t">and I feel guilty about that because I think we can now see that pre-training data is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=216" target="_blank">00:03:36.280</a></span> | <span class="t">an incredibly important ingredient in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=218" target="_blank">00:03:38.880</a></span> | <span class="t">shaping the behaviors of these large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=222" target="_blank">00:03:42.000</a></span> | <span class="t">I have listed out here some core pre-training resources,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=225" target="_blank">00:03:45.840</a></span> | <span class="t">OpenBook Corpus, the Pile,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=228" target="_blank">00:03:48.440</a></span> | <span class="t">Big Science Data, Wikipedia, and Reddit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=231" target="_blank">00:03:51.840</a></span> | <span class="t">I have listed these here not really to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=234" target="_blank">00:03:54.520</a></span> | <span class="t">encourage you to go off and train your own large language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=237" target="_blank">00:03:57.680</a></span> | <span class="t">but rather to think about auditing these datasets as a way of more deeply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=242" target="_blank">00:04:02.360</a></span> | <span class="t">understanding the artifacts that we do have and coming to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=246" target="_blank">00:04:06.040</a></span> | <span class="t">an understanding of where they're likely to be successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=248" target="_blank">00:04:08.840</a></span> | <span class="t">and where they might be actually very problematic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=251" target="_blank">00:04:11.720</a></span> | <span class="t">A lot of that is going to trace to the nature of the input data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=256" target="_blank">00:04:16.480</a></span> | <span class="t">Then finally, let's look ahead to the future,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=259" target="_blank">00:04:19.080</a></span> | <span class="t">some current trends to the best of my estimation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=262" target="_blank">00:04:22.920</a></span> | <span class="t">This is likely the situation we're in and what we're going to see going forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=267" target="_blank">00:04:27.160</a></span> | <span class="t">First, it seems like autoregressive architectures have taken over.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=271" target="_blank">00:04:31.720</a></span> | <span class="t">That's the rise of GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=274" target="_blank">00:04:34.000</a></span> | <span class="t">But this may be simply because the field is so focused on generation right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=280" target="_blank">00:04:40.200</a></span> | <span class="t">I would still maintain that if you simply want to represent examples for the sake of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=285" target="_blank">00:04:45.840</a></span> | <span class="t">having a sentence embedding or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=287" target="_blank">00:04:47.800</a></span> | <span class="t">understanding how different representations compare to each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=291" target="_blank">00:04:51.080</a></span> | <span class="t">it seems to me that bi-directional models like BERT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=293" target="_blank">00:04:53.880</a></span> | <span class="t">might still have the edge over models like GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=297" target="_blank">00:04:57.800</a></span> | <span class="t">Sequence-to-sequence models are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=300" target="_blank">00:05:00.520</a></span> | <span class="t">a dominant choice for tasks that have that structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=303" target="_blank">00:05:03.520</a></span> | <span class="t">It seems like they might have an edge in terms of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=305" target="_blank">00:05:05.800</a></span> | <span class="t">an architectural bias that helps them understand the tasks themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=310" target="_blank">00:05:10.280</a></span> | <span class="t">Although item 1 here is important as we get these really large pure language models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=315" target="_blank">00:05:15.960</a></span> | <span class="t">we might find ourselves moving more toward autoregressive formulations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=320" target="_blank">00:05:20.320</a></span> | <span class="t">even of tasks that have a sequence-to-sequence structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=323" target="_blank">00:05:23.360</a></span> | <span class="t">We shall see. Then finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=326" target="_blank">00:05:26.180</a></span> | <span class="t">and maybe this is the most interesting point of all,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=328" target="_blank">00:05:28.960</a></span> | <span class="t">people are still obsessed with scaling up to ever larger language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=333" target="_blank">00:05:33.540</a></span> | <span class="t">But happily, we are seeing a counter-movement towards smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=338" target="_blank">00:05:38.120</a></span> | <span class="t">I've put smaller in quotes here because we're still talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=341" target="_blank">00:05:41.400</a></span> | <span class="t">artifacts that have on the order of 10 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=345" target="_blank">00:05:45.040</a></span> | <span class="t">but that is substantially smaller than these really massive language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=350" target="_blank">00:05:50.240</a></span> | <span class="t">There are a lot of incentives that are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=353" target="_blank">00:05:53.640</a></span> | <span class="t">encourage these smaller models to become very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=356" target="_blank">00:05:56.760</a></span> | <span class="t">We can deploy them in more places,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=358" target="_blank">00:05:58.880</a></span> | <span class="t">we can train them more efficiently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=360" target="_blank">00:06:00.760</a></span> | <span class="t">we can train more of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=362" target="_blank">00:06:02.320</a></span> | <span class="t">and we might have more control of them in the end for the things that we want to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=367" target="_blank">00:06:07.280</a></span> | <span class="t">All the incentives are there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=368" target="_blank">00:06:08.760</a></span> | <span class="t">This is a moment of intense innovation and a lot of change in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=373" target="_blank">00:06:13.260</a></span> | <span class="t">I have no idea what these small models are going to be able to do a year from now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=377" target="_blank">00:06:17.840</a></span> | <span class="t">but I would exhort all of you to think about how you might participate in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=382" target="_blank">00:06:22.360</a></span> | <span class="t">this exciting moment and help us reach the point where relatively small and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=386" target="_blank">00:06:26.680</a></span> | <span class="t">inefficient models are nonetheless incredibly performant and useful to us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ni3T4vStzBI&t=393" target="_blank">00:06:33.080</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>