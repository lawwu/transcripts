<html><head><title>Stanford CS25: V3 I Generalist Agents in Open-Ended Worlds</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V3 I Generalist Agents in Open-Ended Worlds</h2><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU"><img src="https://i.ytimg.com/vi/wwQ1LQA3RCU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./wwQ1LQA3RCU.html">Whisper Transcript</a> | <a href="./transcript_wwQ1LQA3RCU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">So today we're honored to have Jim Ban from NVIDIA, who will be talking about generalist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=14" target="_blank">00:00:14.200</a></span> | <span class="t">agents in open-ended worlds, and he's a senior AI research scientist at NVIDIA, where his</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=21" target="_blank">00:00:21.040</a></span> | <span class="t">mission is to build generally capable AI agents with applications to gaming, robotics, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=26" target="_blank">00:00:26.920</a></span> | <span class="t">software automation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=27" target="_blank">00:00:27.920</a></span> | <span class="t">His research spans foundation models, multi-modal AI, reinforcement learning, and open-ended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=34" target="_blank">00:00:34.060</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=35" target="_blank">00:00:35.060</a></span> | <span class="t">Jim obtained his PhD degree in computer science from here, Stanford, advised by Professor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=41" target="_blank">00:00:41.320</a></span> | <span class="t">Pei-Pei Li.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=42" target="_blank">00:00:42.320</a></span> | <span class="t">And previously, he did research internships at OpenAI, Google AI, as well as Mila Quebec</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=49" target="_blank">00:00:49.080</a></span> | <span class="t">AI Institute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=50" target="_blank">00:00:50.080</a></span> | <span class="t">So yeah, give it up for Jim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=54" target="_blank">00:00:54.520</a></span> | <span class="t">Yeah, thanks for having me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=62" target="_blank">00:01:02.400</a></span> | <span class="t">So I want to start with a story of two kittens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=68" target="_blank">00:01:08.040</a></span> | <span class="t">It's a story that gave me a lot of inspiration over my career, so I want to share this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=73" target="_blank">00:01:13.720</a></span> | <span class="t">first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=74" target="_blank">00:01:14.720</a></span> | <span class="t">Back in 1963, there were two scientists from MIT, Held and Hein.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=79" target="_blank">00:01:19.480</a></span> | <span class="t">They did this ingenious experiment where they put two newborn kittens in this device, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=85" target="_blank">00:01:25.700</a></span> | <span class="t">the kittens have not seen the visual world yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=88" target="_blank">00:01:28.120</a></span> | <span class="t">So it's kind of like a merry-go-round, where the two kittens are linked by a rigid mechanical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=93" target="_blank">00:01:33.080</a></span> | <span class="t">bar, so their movements are exactly mirrored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=97" target="_blank">00:01:37.000</a></span> | <span class="t">And there's an active kitten on the right-hand side, and that's the only one able to move</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=101" target="_blank">00:01:41.000</a></span> | <span class="t">freely and then transmit the motion over this link to the passive kitten, which is confined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=107" target="_blank">00:01:47.280</a></span> | <span class="t">to the basket and cannot really control its own movements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=111" target="_blank">00:01:51.760</a></span> | <span class="t">And then after a couple of days, Held and Hein kind of take the kittens out of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=116" target="_blank">00:01:56.240</a></span> | <span class="t">merry-go-round and then did visual testing on them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=119" target="_blank">00:01:59.440</a></span> | <span class="t">And they found that only the active kitten was able to develop a healthy visual motor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=123" target="_blank">00:02:03.560</a></span> | <span class="t">loop, like responding correctly to approaching objects or visual cliffs, but a passive kitten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=130" target="_blank">00:02:10.200</a></span> | <span class="t">did not have a healthy visual system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=133" target="_blank">00:02:13.180</a></span> | <span class="t">So I find this experiment fascinating because it shows the importance of having this embodied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=139" target="_blank">00:02:19.940</a></span> | <span class="t">active experience to really ground a system of intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=146" target="_blank">00:02:26.360</a></span> | <span class="t">And let's put this experiment in today's AI context, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=150" target="_blank">00:02:30.400</a></span> | <span class="t">We actually have a very powerful passive kitten, and that is ChagGBT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=155" target="_blank">00:02:35.800</a></span> | <span class="t">It passively observes and rehearses the text on the internet, and it doesn't have any embodiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=161" target="_blank">00:02:41.860</a></span> | <span class="t">And because of this, its knowledge is kind of abstract and ungrounded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=165" target="_blank">00:02:45.660</a></span> | <span class="t">And that partially contributes to the fact that ChagGBT hallucinates things that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=170" target="_blank">00:02:50.540</a></span> | <span class="t">just incompatible with our common sense and our physical experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=175" target="_blank">00:02:55.460</a></span> | <span class="t">And I believe the future belongs to active kittens, which translates to journalist agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=181" target="_blank">00:03:01.800</a></span> | <span class="t">They are the decision-makers in a constant feedback loop, and they're embodied in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=186" target="_blank">00:03:06.580</a></span> | <span class="t">fully immersive world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=188" target="_blank">00:03:08.100</a></span> | <span class="t">They're also not mutually exclusive with the passive kitten.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=192" target="_blank">00:03:12.240</a></span> | <span class="t">And in fact, I see the active embodiment part as a layer on top of the passive pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=198" target="_blank">00:03:18.300</a></span> | <span class="t">from lots and lots of internet data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=202" target="_blank">00:03:22.520</a></span> | <span class="t">So are we there yet?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=204" target="_blank">00:03:24.980</a></span> | <span class="t">Have we achieved journalist agent?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=206" target="_blank">00:03:26.660</a></span> | <span class="t">You know, back in 2016, I remember it was like spring of 2016.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=211" target="_blank">00:03:31.740</a></span> | <span class="t">I was sitting in an undergraduate class at Columbia University, but I wasn't paying attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=215" target="_blank">00:03:35.420</a></span> | <span class="t">to the lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=216" target="_blank">00:03:36.660</a></span> | <span class="t">I was watching a board game tournament on my laptop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=220" target="_blank">00:03:40.660</a></span> | <span class="t">And this screenshot was the moment when Arthur Goh versus Lisa Dahl, and Arthur Goh won three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=227" target="_blank">00:03:47.640</a></span> | <span class="t">matches out of five, and became the first ever to beat a human champion at a game of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=231" target="_blank">00:03:51.580</a></span> | <span class="t">Goh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=232" target="_blank">00:03:52.580</a></span> | <span class="t">You know, I remember the adrenaline that day, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=234" target="_blank">00:03:54.620</a></span> | <span class="t">I've seen history unfold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=236" target="_blank">00:03:56.020</a></span> | <span class="t">Oh my God, we're finally getting to HCI, and everyone's like so excited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=240" target="_blank">00:04:00.980</a></span> | <span class="t">And I think that was the moment when AI agents entered the mainstream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=245" target="_blank">00:04:05.540</a></span> | <span class="t">And you know, like when the excitement fades, I felt that even though Arthur Goh was so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=252" target="_blank">00:04:12.340</a></span> | <span class="t">mighty and so great, it could only do one thing and one thing alone, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=258" target="_blank">00:04:18.620</a></span> | <span class="t">And afterwards, you know, in 2019, there were more impressive achievements like OpenAI 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=264" target="_blank">00:04:24.820</a></span> | <span class="t">beating the human champions at a game of Dota, and Arthur Stark from DeepMind beat StarCraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=270" target="_blank">00:04:30.900</a></span> | <span class="t">But all of these, with Arthur Goh, they all have a single kind of theme, and that is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=276" target="_blank">00:04:36.940</a></span> | <span class="t">beat the opponent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=278" target="_blank">00:04:38.420</a></span> | <span class="t">There is this one objective that the agent needs to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=282" target="_blank">00:04:42.540</a></span> | <span class="t">And the models trained on Dota or Goh cannot generalize to any other tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=288" target="_blank">00:04:48.720</a></span> | <span class="t">It cannot even play other games like Super Mario or Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=292" target="_blank">00:04:52.700</a></span> | <span class="t">And the world is fixed and have very little room for like open-ended creativity and exploration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=299" target="_blank">00:04:59.840</a></span> | <span class="t">So I argue that a journalist agent should have the following essential properties.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=304" target="_blank">00:05:04.620</a></span> | <span class="t">First, it should be able to pursue very complex, semantically rich and open world objectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=311" target="_blank">00:05:11.100</a></span> | <span class="t">Basically you explain what you want in natural language, and the agent should perform the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=314" target="_blank">00:05:14.740</a></span> | <span class="t">actions for you in a dynamic world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=317" target="_blank">00:05:17.140</a></span> | <span class="t">And second, the agent should have a large amount of pre-trained knowledge instead of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=322" target="_blank">00:05:22.060</a></span> | <span class="t">knowing only a few concepts that's extremely specific to the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=327" target="_blank">00:05:27.580</a></span> | <span class="t">And third, massively multitask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=329" target="_blank">00:05:29.900</a></span> | <span class="t">A journalist agent, as the name implies, needs to do more than just a couple of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=335" target="_blank">00:05:35.660</a></span> | <span class="t">It should be, in the best case, infinitely multitask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=340" target="_blank">00:05:40.060</a></span> | <span class="t">As expressive as human language can dictate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=344" target="_blank">00:05:44.460</a></span> | <span class="t">So what does it take?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=346" target="_blank">00:05:46.420</a></span> | <span class="t">Correspondingly, we need three main ingredients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=349" target="_blank">00:05:49.980</a></span> | <span class="t">First is the environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=351" target="_blank">00:05:51.980</a></span> | <span class="t">The environment needs to be open-ended enough because the agent's capability is upper bounded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=358" target="_blank">00:05:58.300</a></span> | <span class="t">by the environment complexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=361" target="_blank">00:06:01.220</a></span> | <span class="t">And I'd argue that Earth is actually a perfect example because it's so open-ended, this world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=365" target="_blank">00:06:05.700</a></span> | <span class="t">we live in, that it allows an algorithm called natural evolution to produce all the diverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=370" target="_blank">00:06:10.940</a></span> | <span class="t">forms and behaviors of life on this planet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=374" target="_blank">00:06:14.400</a></span> | <span class="t">So can we have a simulator that is essentially a lo-fi Earth, but we can still run it on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=379" target="_blank">00:06:19.740</a></span> | <span class="t">the lab clusters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=383" target="_blank">00:06:23.340</a></span> | <span class="t">And second, we need to provide the agent with massive pre-training data because exploration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=387" target="_blank">00:06:27.740</a></span> | <span class="t">in an open-ended world from scratch is just intractable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=391" target="_blank">00:06:31.780</a></span> | <span class="t">And the data will serve at least two purposes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=394" target="_blank">00:06:34.260</a></span> | <span class="t">One as a reference manual on how to do things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=397" target="_blank">00:06:37.500</a></span> | <span class="t">And second, as a guidance on what are the interesting things worth pursuing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=402" target="_blank">00:06:42.660</a></span> | <span class="t">And GPT is only, at least up to GPT-4, it only learns from pure text on the web.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=408" target="_blank">00:06:48.780</a></span> | <span class="t">But can we provide the agent with much richer data, such as video walkthrough or multimedia,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=415" target="_blank">00:06:55.540</a></span> | <span class="t">wiki documents and other media forms?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=420" target="_blank">00:07:00.580</a></span> | <span class="t">And finally, once we have the environment and the database, we are ready to train foundation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=425" target="_blank">00:07:05.980</a></span> | <span class="t">models for the agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=428" target="_blank">00:07:08.300</a></span> | <span class="t">And it should be flexible enough to pursue the open-ended tasks without any task-specific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=433" target="_blank">00:07:13.340</a></span> | <span class="t">assumptions, and also scalable enough to compress all of the multi-modal data that I just described.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=440" target="_blank">00:07:20.940</a></span> | <span class="t">And here language, I argue, will play at least two key roles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=444" target="_blank">00:07:24.860</a></span> | <span class="t">One is as a simple and intuitive interface to communicate a task, to communicate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=450" target="_blank">00:07:30.020</a></span> | <span class="t">human intentions to the agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=452" target="_blank">00:07:32.300</a></span> | <span class="t">And second, as a bridge to ground all of the multi-modal concepts and signals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=458" target="_blank">00:07:38.040</a></span> | <span class="t">And that train of thought landed us in Minecraft, the best-selling video game of all time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=465" target="_blank">00:07:45.480</a></span> | <span class="t">And for those who are unfamiliar, Minecraft is a procedurally generated 3D voxel world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=471" target="_blank">00:07:51.640</a></span> | <span class="t">And in the game, you can basically do whatever your heart desires.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=475" target="_blank">00:07:55.460</a></span> | <span class="t">And what's so special about the game is that unlike AlphaGo, StarCraft, or Dota, Minecraft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=481" target="_blank">00:08:01.680</a></span> | <span class="t">defines no particular objective to maximize, no particular opponent to beat, and doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=487" target="_blank">00:08:07.880</a></span> | <span class="t">even have a fixed storyline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=489" target="_blank">00:08:09.960</a></span> | <span class="t">And that makes it very well suited as a truly open-ended AI playground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=493" target="_blank">00:08:13.760</a></span> | <span class="t">And here, we see people doing extremely impressive things in Minecraft, like this is a YouTube</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=499" target="_blank">00:08:19.800</a></span> | <span class="t">video where a gamer built the entire Hogwarts castle block by block, by hand, in the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=508" target="_blank">00:08:28.200</a></span> | <span class="t">And here's another example of someone just digging a big hole in the ground and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=512" target="_blank">00:08:32.120</a></span> | <span class="t">making this beautiful underground temple with a river nearby.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=516" target="_blank">00:08:36.700</a></span> | <span class="t">It's all crafted by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=520" target="_blank">00:08:40.120</a></span> | <span class="t">And one more, this is someone building a functioning CPU circuit inside the game, because there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=526" target="_blank">00:08:46.180</a></span> | <span class="t">are something called redstone in Minecraft that you can build circuits out of it, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=531" target="_blank">00:08:51.920</a></span> | <span class="t">logical gates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=532" target="_blank">00:08:52.920</a></span> | <span class="t">And actually, the game is Turing-complete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=535" target="_blank">00:08:55.360</a></span> | <span class="t">You can simulate a computer inside a game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=538" target="_blank">00:08:58.400</a></span> | <span class="t">Just think about how crazy that is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=540" target="_blank">00:09:00.600</a></span> | <span class="t">And here, I want to highlight a number that is 140 million active players.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=546" target="_blank">00:09:06.380</a></span> | <span class="t">And just to put this number in perspective, this is more than twice the population of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=551" target="_blank">00:09:11.320</a></span> | <span class="t">the UK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=553" target="_blank">00:09:13.240</a></span> | <span class="t">And that is the amount of people playing Minecraft on a daily basis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=556" target="_blank">00:09:16.840</a></span> | <span class="t">And it just so happens that gamers are generally happier than PhDs, so they love to stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=564" target="_blank">00:09:24.680</a></span> | <span class="t">and share what they're doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=566" target="_blank">00:09:26.640</a></span> | <span class="t">And that produces a huge amount of data every day online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=570" target="_blank">00:09:30.620</a></span> | <span class="t">And there's this treasure trove of learning materials that we can tap into for training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=575" target="_blank">00:09:35.700</a></span> | <span class="t">journalist agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=576" target="_blank">00:09:36.700</a></span> | <span class="t">You know, remember that data is the key for foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=581" target="_blank">00:09:41.640</a></span> | <span class="t">So we introduce MindDojo, a new open framework to help the community develop generally capable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=588" target="_blank">00:09:48.520</a></span> | <span class="t">agents using Minecraft as a kind of primordial soup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=595" target="_blank">00:09:55.400</a></span> | <span class="t">MindDojo features three major parts, an open-ended environment, an international knowledge base,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=601" target="_blank">00:10:01.300</a></span> | <span class="t">and then a journalist agent developed with a simulator and massive data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=606" target="_blank">00:10:06.500</a></span> | <span class="t">So let's zoom in the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=609" target="_blank">00:10:09.060</a></span> | <span class="t">Here's a sample gallery of the interesting things that you can do with MindDojo's API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=615" target="_blank">00:10:15.220</a></span> | <span class="t">We feature a massive benchmarking suite of more than 3,000 tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=619" target="_blank">00:10:19.380</a></span> | <span class="t">And this is by far the largest open source agent benchmark to our knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=625" target="_blank">00:10:25.540</a></span> | <span class="t">And we implement a very versatile API that unlocks the full potential of the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=629" target="_blank">00:10:29.700</a></span> | <span class="t">Like for example, MindDojo supports multi-modal observation and a full action space, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=636" target="_blank">00:10:36.500</a></span> | <span class="t">moving or attack or inventory management.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=639" target="_blank">00:10:39.780</a></span> | <span class="t">And that can be customized at every detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=642" target="_blank">00:10:42.260</a></span> | <span class="t">Like you can tweak the terrains, the weather, plot placement, monster spawning, and just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=649" target="_blank">00:10:49.060</a></span> | <span class="t">anything you want to customize in the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=652" target="_blank">00:10:52.640</a></span> | <span class="t">And given the simulator, we introduce around 1,500 programmatic tasks, which are tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=658" target="_blank">00:10:58.880</a></span> | <span class="t">that have ground-true success conditions defined in Python code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=662" target="_blank">00:11:02.660</a></span> | <span class="t">And you can also explicitly write down like sparse or best reward functions using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=667" target="_blank">00:11:07.060</a></span> | <span class="t">API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=668" target="_blank">00:11:08.060</a></span> | <span class="t">And some examples are like harvesting different resources, unlocking the tech tree, or fighting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=673" target="_blank">00:11:13.860</a></span> | <span class="t">various monsters and getting reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=676" target="_blank">00:11:16.060</a></span> | <span class="t">And all these tasks come with language prompts that are templated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=680" target="_blank">00:11:20.700</a></span> | <span class="t">Next, we also introduce 1,500 creative tasks that are freeform and open-ended.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=686" target="_blank">00:11:26.580</a></span> | <span class="t">And that is in contrast to the programmatic tasks I just mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=690" target="_blank">00:11:30.660</a></span> | <span class="t">So for example, let's say we want the agent to build a house.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=695" target="_blank">00:11:35.300</a></span> | <span class="t">But what makes a house a house, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=697" target="_blank">00:11:37.920</a></span> | <span class="t">It is ill-defined.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=699" target="_blank">00:11:39.140</a></span> | <span class="t">And just like image generation, you don't know if it generates a cat correctly or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=705" target="_blank">00:11:45.240</a></span> | <span class="t">So it's very difficult to use simple Python programs to give these kinds of tasks reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=709" target="_blank">00:11:49.360</a></span> | <span class="t">functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=711" target="_blank">00:11:51.020</a></span> | <span class="t">And the best way is to use foundation models trained on internet skill knowledge so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=716" target="_blank">00:11:56.660</a></span> | <span class="t">the model itself understands abstract concepts like the concept of a house.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=724" target="_blank">00:12:04.460</a></span> | <span class="t">And finally, there's one task that holds a very special status called playsuit, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=728" target="_blank">00:12:08.380</a></span> | <span class="t">is to beat the final boss of Minecraft, the Ender Dragon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=732" target="_blank">00:12:12.580</a></span> | <span class="t">So Minecraft doesn't force you to do this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=734" target="_blank">00:12:14.620</a></span> | <span class="t">As we said, it doesn't have a fixed storyline, but it's still considered a really big milestone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=739" target="_blank">00:12:19.580</a></span> | <span class="t">for any kind of beginner human players.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=743" target="_blank">00:12:23.100</a></span> | <span class="t">I want to highlight it is an extremely difficult task that requires very complex preparation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=748" target="_blank">00:12:28.380</a></span> | <span class="t">exploration, and also martial skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=751" target="_blank">00:12:31.020</a></span> | <span class="t">And for an average human, it will take many hours or even days to solve, easily over like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=757" target="_blank">00:12:37.540</a></span> | <span class="t">1 million action steps in a single episode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=760" target="_blank">00:12:40.540</a></span> | <span class="t">And that would be the longest benchmarking task for policy learning ever created here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=765" target="_blank">00:12:45.660</a></span> | <span class="t">So I admit, I am personally a below average human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=769" target="_blank">00:12:49.300</a></span> | <span class="t">I was never able to beat Ender Dragon and my friends laugh at me and I'm like, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=774" target="_blank">00:12:54.780</a></span> | <span class="t">one day my AI will avenge my poor skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=779" target="_blank">00:12:59.300</a></span> | <span class="t">That was one of the motivations for this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=783" target="_blank">00:13:03.380</a></span> | <span class="t">Now let's move on to the second ingredient, the internet skill knowledge base part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=787" target="_blank">00:13:07.660</a></span> | <span class="t">MindDojo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=789" target="_blank">00:13:09.420</a></span> | <span class="t">We offer three datasets here, the YouTube, Wiki, and Reddit, and combined they are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=794" target="_blank">00:13:14.580</a></span> | <span class="t">largest open-ended agent behavior database ever compiled to our knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=800" target="_blank">00:13:20.900</a></span> | <span class="t">The first is YouTube, and we already said Minecraft is one of the most streamed games</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=806" target="_blank">00:13:26.500</a></span> | <span class="t">on YouTube, and the gamers love to narrate what they are doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=810" target="_blank">00:13:30.740</a></span> | <span class="t">So we collected more than 700,000 videos with 2 billion words in the corresponding transcripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=817" target="_blank">00:13:37.820</a></span> | <span class="t">And these transcripts will help the agent learn about human strategies and creativities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=823" target="_blank">00:13:43.140</a></span> | <span class="t">without us manually labeling things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=826" target="_blank">00:13:46.780</a></span> | <span class="t">And second, the Minecraft player base is so crazy that they have compiled a huge Minecraft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=834" target="_blank">00:13:54.860</a></span> | <span class="t">specific Wikipedia that basically explains everything you ever need to know in every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=841" target="_blank">00:14:01.060</a></span> | <span class="t">version of the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=842" target="_blank">00:14:02.460</a></span> | <span class="t">It's crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=843" target="_blank">00:14:03.500</a></span> | <span class="t">And we scraped 7,000 Wikipedias with interleaving multi-modal data, like images, tables, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=849" target="_blank">00:14:09.620</a></span> | <span class="t">diagrams, and here are some screenshots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=852" target="_blank">00:14:12.620</a></span> | <span class="t">Like this is a gallery of all of the monsters and their corresponding behaviors, like spawn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=858" target="_blank">00:14:18.140</a></span> | <span class="t">and attack patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=860" target="_blank">00:14:20.460</a></span> | <span class="t">And also like the thousands of crafting recipes are all present on the Wiki, and we scraped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=865" target="_blank">00:14:25.100</a></span> | <span class="t">all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=866" target="_blank">00:14:26.500</a></span> | <span class="t">And more like complex diagrams and tables and embedded figures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=869" target="_blank">00:14:29.740</a></span> | <span class="t">Now we have something like GPT-4V, it may be able to understand many of these diagrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=876" target="_blank">00:14:36.980</a></span> | <span class="t">And finally, the Minecraft sub-Reddit is one of the most active forums across the entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=882" target="_blank">00:14:42.820</a></span> | <span class="t">Reddit, and players showcase their creations and also ask questions for help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=887" target="_blank">00:14:47.860</a></span> | <span class="t">So we scraped more than 300,000 posts from Minecraft Reddit, and here are some examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=893" target="_blank">00:14:53.520</a></span> | <span class="t">of how people use the Reddit as a kind of stack overflow for Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=899" target="_blank">00:14:59.540</a></span> | <span class="t">And we can see that some of the top voted answers are actually quite good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=903" target="_blank">00:15:03.020</a></span> | <span class="t">Like someone is asking, "Oh, why doesn't my wheat farm grow?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=906" target="_blank">00:15:06.540</a></span> | <span class="t">And the answer says, "You need to light up the room with more torches, you don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=909" target="_blank">00:15:09.540</a></span> | <span class="t">enough lighting."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=910" target="_blank">00:15:10.540</a></span> | <span class="t">Now, given the massive task suite and internet data, we have the essential components to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=917" target="_blank">00:15:17.940</a></span> | <span class="t">build journalist agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=921" target="_blank">00:15:21.580</a></span> | <span class="t">So in the first Minecraft Dojo paper, we introduced a foundation model called Minecraft Clip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=926" target="_blank">00:15:26.220</a></span> | <span class="t">And the idea is very simple, I can explain in three slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=930" target="_blank">00:15:30.620</a></span> | <span class="t">Basically for our YouTube database, we have time-aligned videos and transcripts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=935" target="_blank">00:15:35.900</a></span> | <span class="t">And these are actually the real tutorial videos from our data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=939" target="_blank">00:15:39.900</a></span> | <span class="t">You see on the third clip, as I raise my axe in front of this pig, there's only one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=945" target="_blank">00:15:45.900</a></span> | <span class="t">that you know is going to happen, is actually someone said this, a big YouTuber of Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=953" target="_blank">00:15:53.180</a></span> | <span class="t">And then, given this data, we train Minecraft in the same spirit as OpenAI Clip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=959" target="_blank">00:15:59.040</a></span> | <span class="t">So for those who are unfamiliar, OpenAI Clip is a contrastive model that learns the association</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=964" target="_blank">00:16:04.580</a></span> | <span class="t">between an image and its caption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=966" target="_blank">00:16:06.940</a></span> | <span class="t">And here, it's a very similar idea, but this time it is a video text contrastive model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=972" target="_blank">00:16:12.700</a></span> | <span class="t">And we associate the text with a video snippet that runs about eight to 16 seconds each.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=982" target="_blank">00:16:22.780</a></span> | <span class="t">And intuitively, Minecraft learns the association between the video and the transcript that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=987" target="_blank">00:16:27.660</a></span> | <span class="t">describes the activity in the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=990" target="_blank">00:16:30.980</a></span> | <span class="t">And Minecraft outputs a score between 0 and 1, where 1 means a perfect correlation between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=995" target="_blank">00:16:35.940</a></span> | <span class="t">the text and the video, and 0 means the text is irrelevant to the activity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1001" target="_blank">00:16:41.420</a></span> | <span class="t">So you see, this is effectively a language-prompted foundation reward model that knows the nuances</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1008" target="_blank">00:16:48.500</a></span> | <span class="t">of things like forests, animal behaviors, and architectures in Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1015" target="_blank">00:16:55.060</a></span> | <span class="t">So how do we use Minecraft in action?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1017" target="_blank">00:16:57.580</a></span> | <span class="t">Here's an example of our agent interacting with the simulator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1021" target="_blank">00:17:01.500</a></span> | <span class="t">And here, the task is "shear sheep to obtain wool."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1025" target="_blank">00:17:05.620</a></span> | <span class="t">And as the agent explores in the simulator, it generates a video snippet as a moving window,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1032" target="_blank">00:17:12.620</a></span> | <span class="t">which can be encoded and fed into Minecraft, along with an encoding of the text prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1037" target="_blank">00:17:17.580</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1039" target="_blank">00:17:19.500</a></span> | <span class="t">And Minecraft computes the association.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1042" target="_blank">00:17:22.260</a></span> | <span class="t">The higher the association is, the more the agent's behavior in this video aligns with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1047" target="_blank">00:17:27.540</a></span> | <span class="t">the language, which is a task you want it to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1050" target="_blank">00:17:30.980</a></span> | <span class="t">And that becomes a reward function to any reinforcement learning algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1055" target="_blank">00:17:35.700</a></span> | <span class="t">So this looks very familiar, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1058" target="_blank">00:17:38.480</a></span> | <span class="t">Because it's essentially RL from human feedback, or RLHF in Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1066" target="_blank">00:17:46.300</a></span> | <span class="t">And RLHF was the cornerstone algorithm that made ChagGBT possible, and I believe it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1071" target="_blank">00:17:51.500</a></span> | <span class="t">play a critical role in journalists' agents as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1075" target="_blank">00:17:55.940</a></span> | <span class="t">I'll quickly gloss over some quantitative results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1078" target="_blank">00:17:58.700</a></span> | <span class="t">I promise there won't be, like, many tables of numbers here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1082" target="_blank">00:18:02.820</a></span> | <span class="t">For these eight tasks, we show the percentage success rate over 200 test episodes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1088" target="_blank">00:18:08.180</a></span> | <span class="t">And here, in the green circle, is two variants of our Minecraft method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1092" target="_blank">00:18:12.580</a></span> | <span class="t">And in the orange circles are the baselines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1095" target="_blank">00:18:15.920</a></span> | <span class="t">So I'll highlight one baseline, which is that we construct a dense reward function manually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1101" target="_blank">00:18:21.700</a></span> | <span class="t">for each task using the MindDojo API, it's a Python API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1106" target="_blank">00:18:26.340</a></span> | <span class="t">And you can consider this column as a kind of oracle, the upper bound of the performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1111" target="_blank">00:18:31.180</a></span> | <span class="t">because we put a lot of human efforts into designing these reward functions just for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1115" target="_blank">00:18:35.020</a></span> | <span class="t">the tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1117" target="_blank">00:18:37.400</a></span> | <span class="t">And we can see that Mineclip is able to match the quality of many of these, not all of them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1122" target="_blank">00:18:42.940</a></span> | <span class="t">but many of these manually generated rewards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1125" target="_blank">00:18:45.980</a></span> | <span class="t">It is important to highlight that Mineclip is open vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1129" target="_blank">00:18:49.660</a></span> | <span class="t">So we use a single model for all of these tasks instead of one model for each.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1133" target="_blank">00:18:53.660</a></span> | <span class="t">And we simply prompt the reward model with different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1138" target="_blank">00:18:58.500</a></span> | <span class="t">And that's the only variation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1143" target="_blank">00:19:03.420</a></span> | <span class="t">One major feature of the foundation model is strong generalization out of the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1147" target="_blank">00:19:07.220</a></span> | <span class="t">So can our agent generalize to dramatic changes in the visual appearance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1152" target="_blank">00:19:12.620</a></span> | <span class="t">So we did this experiment where during training, we only train our agents on a default terrain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1159" target="_blank">00:19:19.780</a></span> | <span class="t">at noon on a sunny day.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1161" target="_blank">00:19:21.820</a></span> | <span class="t">But we tested zero shot in a diverse range of terrains, weathers, and day/night cycles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1167" target="_blank">00:19:27.000</a></span> | <span class="t">And you can customize everything in MindDojo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1169" target="_blank">00:19:29.500</a></span> | <span class="t">And in our paper, we have numbers showing that Mineclip significantly beats an off-the-shelf</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1174" target="_blank">00:19:34.060</a></span> | <span class="t">visual encoder when facing these kind of distribution shifts out of the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1178" target="_blank">00:19:38.980</a></span> | <span class="t">And this is no surprise, right, because Mineclip was trained on hundreds of thousands of clips</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1183" target="_blank">00:19:43.900</a></span> | <span class="t">from Minecraft videos on YouTube, which have a very good coverage of all the scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1191" target="_blank">00:19:51.660</a></span> | <span class="t">And I think that is just a testament to the big advantage of using international data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1197" target="_blank">00:19:57.980</a></span> | <span class="t">because you get robustness out of the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1201" target="_blank">00:20:01.780</a></span> | <span class="t">And here are some demos of our learned agent behaviors on various tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1206" target="_blank">00:20:06.140</a></span> | <span class="t">So you may notice that these tasks are relatively short, around like 100 to 500 time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1212" target="_blank">00:20:12.440</a></span> | <span class="t">And that is because Mineclip is not able to plan over very long time horizons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1218" target="_blank">00:20:18.700</a></span> | <span class="t">It is an inherent limitation in the training pipeline, because we could only use 8 to 16</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1224" target="_blank">00:20:24.220</a></span> | <span class="t">seconds of the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1225" target="_blank">00:20:25.560</a></span> | <span class="t">So it's constrained to short actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1228" target="_blank">00:20:28.700</a></span> | <span class="t">But our hope is to build an agent that can explore and make new discoveries autonomously,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1233" target="_blank">00:20:33.900</a></span> | <span class="t">just all by itself, and it keeps going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1236" target="_blank">00:20:36.380</a></span> | <span class="t">And in 2022, this goal seems quite out of reach for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1240" target="_blank">00:20:40.100</a></span> | <span class="t">Mine Dojo was June 2022.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1243" target="_blank">00:20:43.380</a></span> | <span class="t">And this year, something happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1244" target="_blank">00:20:44.820</a></span> | <span class="t">And that is GP4, a language model that is so good at coding and long-horizon planning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1251" target="_blank">00:20:51.780</a></span> | <span class="t">so we just cannot sit still, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1254" target="_blank">00:20:54.220</a></span> | <span class="t">We built Voyager, the first large-language-model-powered lifelong learning agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1260" target="_blank">00:21:00.540</a></span> | <span class="t">And when we set Voyager loose in Minecraft, we see that it just keeps going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1264" target="_blank">00:21:04.980</a></span> | <span class="t">And by the way, all these video snippets are from a single episode of Voyager.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1269" target="_blank">00:21:09.980</a></span> | <span class="t">It's not from different episodes, it's a single one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1273" target="_blank">00:21:13.860</a></span> | <span class="t">And we see that Voyager is just able to keep exploring the terrains, mine all kinds of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1278" target="_blank">00:21:18.820</a></span> | <span class="t">materials, fight monsters, craft hundreds of recipes, and unlock an ever-expanding tree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1284" target="_blank">00:21:24.620</a></span> | <span class="t">of diverse skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1287" target="_blank">00:21:27.060</a></span> | <span class="t">So how do we do this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1290" target="_blank">00:21:30.180</a></span> | <span class="t">If we want to use the full power of GP4, a central question is how to stringify things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1295" target="_blank">00:21:35.700</a></span> | <span class="t">converting this 3D world into a textual representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1300" target="_blank">00:21:40.380</a></span> | <span class="t">We need a magic box here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1302" target="_blank">00:21:42.700</a></span> | <span class="t">And thankfully, again, the crazy Minecraft community already built one for us, and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1307" target="_blank">00:21:47.500</a></span> | <span class="t">been around for many years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1309" target="_blank">00:21:49.740</a></span> | <span class="t">It's called Mineflayer, a high-level JavaScript API that's actively maintained to work with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1314" target="_blank">00:21:54.840</a></span> | <span class="t">any Minecraft version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1317" target="_blank">00:21:57.300</a></span> | <span class="t">And the beauty of Mineflayer is it has access to the game states surrounding the agent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1322" target="_blank">00:22:02.660</a></span> | <span class="t">like the nearby blocks, animals, and enemies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1325" target="_blank">00:22:05.940</a></span> | <span class="t">So we effectively have a ground-truth perception module as textual input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1330" target="_blank">00:22:10.340</a></span> | <span class="t">At the same time, Mineflayer also supports action APIs that we can compose skills.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1338" target="_blank">00:22:18.500</a></span> | <span class="t">And now that we can convert everything to text, we are ready to construct an agent on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1342" target="_blank">00:22:22.620</a></span> | <span class="t">top of GP4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1344" target="_blank">00:22:24.580</a></span> | <span class="t">So on a high level, there are three components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1346" target="_blank">00:22:26.780</a></span> | <span class="t">One is a coding module that writes JavaScript code to control the game bot, and it's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1353" target="_blank">00:22:33.180</a></span> | <span class="t">main module that generates the executable actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1356" target="_blank">00:22:36.340</a></span> | <span class="t">And second, we have a code base to store the correctly written code and look it up in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1361" target="_blank">00:22:41.740</a></span> | <span class="t">future if the agent needs to recall the skill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1364" target="_blank">00:22:44.820</a></span> | <span class="t">And in this way, we don't duplicate efforts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1366" target="_blank">00:22:46.980</a></span> | <span class="t">And whenever facing similar situations in the future, the agent knows what to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1371" target="_blank">00:22:51.240</a></span> | <span class="t">And third, we have a curriculum that proposes what to do next, given the agent's current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1377" target="_blank">00:22:57.320</a></span> | <span class="t">capabilities and also situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1380" target="_blank">00:23:00.320</a></span> | <span class="t">And when you wire these components up together, you get a loop that drives the agent indefinitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1386" target="_blank">00:23:06.800</a></span> | <span class="t">and achieve something like lifelong learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1390" target="_blank">00:23:10.060</a></span> | <span class="t">So let's zoom in the center module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1393" target="_blank">00:23:13.520</a></span> | <span class="t">We prompt GT4 with documentations and examples on how to use a subset of the Mineflayer API.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1400" target="_blank">00:23:20.400</a></span> | <span class="t">And GT4 writes code to take actions given the current assigned task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1405" target="_blank">00:23:25.080</a></span> | <span class="t">And because JavaScript runs a code interpreter, GT4 is able to define functions on the fly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1409" target="_blank">00:23:29.840</a></span> | <span class="t">and run it interactively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1412" target="_blank">00:23:32.400</a></span> | <span class="t">But the code that GT4 writes isn't always correct, just like human engineers, you can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1415" target="_blank">00:23:35.880</a></span> | <span class="t">get everything correct on the first try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1418" target="_blank">00:23:38.440</a></span> | <span class="t">So we develop an iterative prompting mechanism to refine the program.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1423" target="_blank">00:23:43.320</a></span> | <span class="t">And there are three types of feedback here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1425" target="_blank">00:23:45.760</a></span> | <span class="t">The environment feedback, like, you know, what are the new materials you've got after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1429" target="_blank">00:23:49.400</a></span> | <span class="t">taking an action, or, you know, some enemies nearby.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1433" target="_blank">00:23:53.240</a></span> | <span class="t">And the execution error from the JavaScript interpreter, if you wrote some buggy code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1437" target="_blank">00:23:57.680</a></span> | <span class="t">like undefined variable, for example, if it hallucinates something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1442" target="_blank">00:24:02.120</a></span> | <span class="t">And another GT4 that provides critique through self reflection from the agent state and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1448" target="_blank">00:24:08.520</a></span> | <span class="t">world state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1450" target="_blank">00:24:10.020</a></span> | <span class="t">And that also helps refine the program effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1453" target="_blank">00:24:13.680</a></span> | <span class="t">So I want to show some quick example of how the critic provides feedback on the task completion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1458" target="_blank">00:24:18.760</a></span> | <span class="t">progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1460" target="_blank">00:24:20.000</a></span> | <span class="t">So let's say in the first example, the task is to craft a spyglass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1464" target="_blank">00:24:24.200</a></span> | <span class="t">And GT4 looks at the agent's inventory, and decides that it has enough copper, but not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1469" target="_blank">00:24:29.120</a></span> | <span class="t">enough Amherst as material.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1472" target="_blank">00:24:32.720</a></span> | <span class="t">And the second task is to kill three sheep to collect food.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1476" target="_blank">00:24:36.020</a></span> | <span class="t">And each sheep drops one unit of wool, but there are only two units in inventory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1480" target="_blank">00:24:40.560</a></span> | <span class="t">So GT4 reasons and says that, okay, you have one more sheep to go, and likewise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1487" target="_blank">00:24:47.180</a></span> | <span class="t">Now moving on to the second part, once Voyager implements a skill correctly, we save it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1493" target="_blank">00:24:53.520</a></span> | <span class="t">our persistent storage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1495" target="_blank">00:24:55.720</a></span> | <span class="t">And you can think of the skill library as a code repository written entirely by a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1500" target="_blank">00:25:00.880</a></span> | <span class="t">model through interaction with a 3D world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1504" target="_blank">00:25:04.740</a></span> | <span class="t">And the agent can record new skills, and also retrieve skills from the library facing similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1509" target="_blank">00:25:09.840</a></span> | <span class="t">situations in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1511" target="_blank">00:25:11.960</a></span> | <span class="t">So it doesn't have to go through this whole program refinement that we just saw, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1515" target="_blank">00:25:15.720</a></span> | <span class="t">is quite inefficient, but you do it once you save it to disk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1520" target="_blank">00:25:20.360</a></span> | <span class="t">And in this way, Voyager kind of bootstraps its own capabilities recursively as it explores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1526" target="_blank">00:25:26.380</a></span> | <span class="t">and experiments in the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1529" target="_blank">00:25:29.880</a></span> | <span class="t">And let's dive a little bit deeper into how the skill library is implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1533" target="_blank">00:25:33.840</a></span> | <span class="t">So this is how we insert a new skill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1536" target="_blank">00:25:36.480</a></span> | <span class="t">First we use GPT 3.5 to summarize the program into plain English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1541" target="_blank">00:25:41.000</a></span> | <span class="t">And summarization is very easy, and GPT 4 is expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1543" target="_blank">00:25:43.960</a></span> | <span class="t">So we just go for a cheaper tier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1547" target="_blank">00:25:47.600</a></span> | <span class="t">And then we embed this summary as the key, and we save the program, which is a bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1552" target="_blank">00:25:52.880</a></span> | <span class="t">of code, as the value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1555" target="_blank">00:25:55.000</a></span> | <span class="t">And we find out doing this makes retrieval better, because the summary is more semantic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1560" target="_blank">00:26:00.080</a></span> | <span class="t">and the code is a bit more discreet, and you insert it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1566" target="_blank">00:26:06.940</a></span> | <span class="t">And now for the retrieval process, when Voyager is faced with a new task, let's say craft</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1571" target="_blank">00:26:11.800</a></span> | <span class="t">iron pickaxe, we again use GPT 3.5 to generate a hint on how to solve the task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1578" target="_blank">00:26:18.120</a></span> | <span class="t">And that is something like a natural language paragraph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1580" target="_blank">00:26:20.780</a></span> | <span class="t">And then we embed that and use that as the query into the vector database, and we retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1587" target="_blank">00:26:27.200</a></span> | <span class="t">the skill from the library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1590" target="_blank">00:26:30.920</a></span> | <span class="t">So you can think of it as a kind of in-context replay buffer in the reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1595" target="_blank">00:26:35.480</a></span> | <span class="t">literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1597" target="_blank">00:26:37.920</a></span> | <span class="t">And now moving on to the third part, we have another GPT 4 that proposes what task to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1604" target="_blank">00:26:44.240</a></span> | <span class="t">given its own capabilities at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1607" target="_blank">00:26:47.960</a></span> | <span class="t">And here we give GPT 4 a very high-level unsupervised objective, that is to obtain as many unique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1614" target="_blank">00:26:54.480</a></span> | <span class="t">items as possible, that is our high-level directive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1618" target="_blank">00:26:58.080</a></span> | <span class="t">And then GPT 4 takes this directive and implements a curriculum of progressively harder challenges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1624" target="_blank">00:27:04.600</a></span> | <span class="t">and more novel challenges to solve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1627" target="_blank">00:27:07.200</a></span> | <span class="t">So it's kind of like curiosity exploration, where it is not novel research in a prior</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1633" target="_blank">00:27:13.400</a></span> | <span class="t">literature, but implemented purely in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1636" target="_blank">00:27:16.360</a></span> | <span class="t">If you're listening to Zoom, the next example is fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1642" target="_blank">00:27:22.240</a></span> | <span class="t">Let's go through this example together, just to show you how Voyager works, the whole complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1648" target="_blank">00:27:28.640</a></span> | <span class="t">data flow that I just showed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1650" target="_blank">00:27:30.800</a></span> | <span class="t">So the agent finds itself hungry, and only has one out of 20 hunger bar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1655" target="_blank">00:27:35.720</a></span> | <span class="t">So GPT 4 knows that it needs to find food ASAP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1659" target="_blank">00:27:39.480</a></span> | <span class="t">And then it senses there are four entities nearby, a cat, a villager, a pig, and some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1665" target="_blank">00:27:45.160</a></span> | <span class="t">wheat seeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1666" target="_blank">00:27:46.600</a></span> | <span class="t">And now GPT 4 starts a self-reflection, like, do I kill the cat and villager to get some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1671" target="_blank">00:27:51.160</a></span> | <span class="t">meat?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1672" target="_blank">00:27:52.280</a></span> | <span class="t">That sounds horrible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1673" target="_blank">00:27:53.280</a></span> | <span class="t">How about the wheat seeds?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1675" target="_blank">00:27:55.600</a></span> | <span class="t">I can use the seeds to grow a farm, but that's going to take a very long time until I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1680" target="_blank">00:28:00.080</a></span> | <span class="t">generate some food.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1681" target="_blank">00:28:01.560</a></span> | <span class="t">So sorry, piggy, you are the one being chosen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1685" target="_blank">00:28:05.360</a></span> | <span class="t">So GPT 4 looks at the inventory, which is the agent state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1689" target="_blank">00:28:09.600</a></span> | <span class="t">There's a piece of iron in inventory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1692" target="_blank">00:28:12.120</a></span> | <span class="t">So Voyager recalls a skill from the library, that is to craft an iron sword, and then use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1698" target="_blank">00:28:18.360</a></span> | <span class="t">that skill to start learning a new skill, and that is hunt pig.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1703" target="_blank">00:28:23.880</a></span> | <span class="t">And once the hunt pig routine is successful, GPT 4 saves it to the skill library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1710" target="_blank">00:28:30.560</a></span> | <span class="t">That's roughly how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1713" target="_blank">00:28:33.640</a></span> | <span class="t">And putting all these together, we have this iterative prompting mechanism, the skill library,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1718" target="_blank">00:28:38.560</a></span> | <span class="t">and an automatic curriculum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1720" target="_blank">00:28:40.680</a></span> | <span class="t">And all of these combined is Voyager's no-gradient architecture, where we don't train any new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1726" target="_blank">00:28:46.680</a></span> | <span class="t">models or fine-tune any parameters, and allows Voyager to self-bootstrap on top of GPT 4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1734" target="_blank">00:28:54.360</a></span> | <span class="t">even though we are treating the underlying language model as a black box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1739" target="_blank">00:28:59.200</a></span> | <span class="t">It looks like my example work, and they started to listen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1749" target="_blank">00:29:09.000</a></span> | <span class="t">So yeah, these are the tasks that Voyager picked up along the way, and we didn't pre-program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1753" target="_blank">00:29:13.840</a></span> | <span class="t">any of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1754" target="_blank">00:29:14.840</a></span> | <span class="t">We just saw Voyager's idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1756" target="_blank">00:29:16.680</a></span> | <span class="t">The agent is kind of forever curious, and also forever pursuing new adventures just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1761" target="_blank">00:29:21.200</a></span> | <span class="t">by itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1764" target="_blank">00:29:24.160</a></span> | <span class="t">So to quickly show some quantitative results, here we have a learning curve, where the x-axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1771" target="_blank">00:29:31.440</a></span> | <span class="t">is the number of prompting iterations, and the y-axis is the number of unique items that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1776" target="_blank">00:29:36.360</a></span> | <span class="t">Voyager discovered as it's exploring an environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1780" target="_blank">00:29:40.760</a></span> | <span class="t">And these two curves are baselines, React and Reflexion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1787" target="_blank">00:29:47.840</a></span> | <span class="t">And this is AutoGPT, which is like a popular software repo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1790" target="_blank">00:29:50.720</a></span> | <span class="t">Basically, you can think of it as combining React and a task planner that decomposes an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1795" target="_blank">00:29:55.880</a></span> | <span class="t">objective into sub-goals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1798" target="_blank">00:29:58.520</a></span> | <span class="t">And this is Voyager.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1800" target="_blank">00:30:00.280</a></span> | <span class="t">We're able to obtain three times more novel items than the prior methods, and also unlock</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1805" target="_blank">00:30:05.600</a></span> | <span class="t">the entire tech tree significantly faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1809" target="_blank">00:30:09.320</a></span> | <span class="t">And if you take away the skill library, you see that Voyager really suffers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1813" target="_blank">00:30:13.640</a></span> | <span class="t">The performance takes a hit, because every time it needs to kind of repeat and relearn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1818" target="_blank">00:30:18.480</a></span> | <span class="t">every skill from scratch, and it starts to make a lot more mistakes, and that really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1823" target="_blank">00:30:23.240</a></span> | <span class="t">degrades the exploration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1825" target="_blank">00:30:25.920</a></span> | <span class="t">Here, these two are the bird-eye views of the Minecraft map, and these circles are what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1833" target="_blank">00:30:33.160</a></span> | <span class="t">the prior methods are able to explore, given the same prompting iteration budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1839" target="_blank">00:30:39.360</a></span> | <span class="t">And we see that they tend to get stuck in local areas and kind of fail to explore more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1845" target="_blank">00:30:45.120</a></span> | <span class="t">But Voyager is able to navigate distances at least two times as much as the prior works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1852" target="_blank">00:30:52.760</a></span> | <span class="t">So it's able to visit a lot more places, because to satisfy this high-level directive of obtaining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1859" target="_blank">00:30:59.000</a></span> | <span class="t">as many unique items as possible, you've got to travel, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1862" target="_blank">00:31:02.480</a></span> | <span class="t">If you stay at one place, you will quickly exhaust interesting things to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1866" target="_blank">00:31:06.760</a></span> | <span class="t">And Voyager travels a lot, so that's how we came up with the name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1871" target="_blank">00:31:11.960</a></span> | <span class="t">So finally, one limitation is that Voyager does not currently support visual perception,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1878" target="_blank">00:31:18.080</a></span> | <span class="t">because the GPT-4 that we used back then was text-only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1881" target="_blank">00:31:21.720</a></span> | <span class="t">But there's nothing stopping Voyager from adopting, like, multi-modal language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1886" target="_blank">00:31:26.320</a></span> | <span class="t">in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1887" target="_blank">00:31:27.360</a></span> | <span class="t">So here we have a little proof-of-concept demo, where we ask a human to basically function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1892" target="_blank">00:31:32.160</a></span> | <span class="t">as the image captioner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1894" target="_blank">00:31:34.040</a></span> | <span class="t">And the human will tell Voyager that, as you're building these houses, what are the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1898" target="_blank">00:31:38.960</a></span> | <span class="t">that are missing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1899" target="_blank">00:31:39.960</a></span> | <span class="t">Like, you placed a door incorrectly, like, the roof is also not done correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1905" target="_blank">00:31:45.200</a></span> | <span class="t">So the human is acting as a critic module of the Voyager stack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1909" target="_blank">00:31:49.560</a></span> | <span class="t">And we see that with some of that help, Voyager is able to build a farmhouse and a nether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1915" target="_blank">00:31:55.320</a></span> | <span class="t">portal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1916" target="_blank">00:31:56.320</a></span> | <span class="t">So it's not a hard time understanding, you know, 3D spatial coordinates just by itself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1920" target="_blank">00:32:00.360</a></span> | <span class="t">in a textual domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1923" target="_blank">00:32:03.680</a></span> | <span class="t">Now, after doing Voyager, we're considering, like, where else can we apply this idea, right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1931" target="_blank">00:32:11.440</a></span> | <span class="t">of coding in an embodied environment, observe the feedback, and iteratively refine the program.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1938" target="_blank">00:32:18.920</a></span> | <span class="t">So we came to realize that physics simulations themselves are also just Python code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1944" target="_blank">00:32:24.740</a></span> | <span class="t">So why not apply some of the principles from Voyager and do something in another domain?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1951" target="_blank">00:32:31.560</a></span> | <span class="t">What if you apply Voyager in the space of this physics simulator API?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1955" target="_blank">00:32:35.420</a></span> | <span class="t">And this is Eureka, which my team announced just, like, three days ago, fresh out of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1960" target="_blank">00:32:40.840</a></span> | <span class="t">oven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1961" target="_blank">00:32:41.840</a></span> | <span class="t">It is an open-ended agent that designs reward functions for robot dexterity at superhuman</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1968" target="_blank">00:32:48.480</a></span> | <span class="t">level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1969" target="_blank">00:32:49.700</a></span> | <span class="t">And it turns out that GPT-4 plus reinforcement learning can spin a pen much better than I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1975" target="_blank">00:32:55.280</a></span> | <span class="t">do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1976" target="_blank">00:32:56.280</a></span> | <span class="t">I gave up on this task a long time ago from childhood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1979" target="_blank">00:32:59.640</a></span> | <span class="t">It's so hard for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1983" target="_blank">00:33:03.640</a></span> | <span class="t">So Eureka's idea is very simple and intuitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1987" target="_blank">00:33:07.040</a></span> | <span class="t">GPT-4 generates a bunch of possible reward function candidates implemented in Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1993" target="_blank">00:33:13.040</a></span> | <span class="t">And then you just do a full reinforcement learning training loop for each candidate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=1998" target="_blank">00:33:18.640</a></span> | <span class="t">in a GPU-accelerated simulator, and you get a performance metric, and you take the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2004" target="_blank">00:33:24.600</a></span> | <span class="t">candidates and feedback to GPT-4, and it samples the next proposals of candidates and keeps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2011" target="_blank">00:33:31.020</a></span> | <span class="t">improving the whole population of the reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2014" target="_blank">00:33:34.600</a></span> | <span class="t">That's the whole idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2015" target="_blank">00:33:35.600</a></span> | <span class="t">It's kind of like an in-context evolutionary search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2020" target="_blank">00:33:40.360</a></span> | <span class="t">So here's the initial reward generation, where Eureka takes as context the environment code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2026" target="_blank">00:33:46.100</a></span> | <span class="t">of NVIDIA's iSOC sim and a task description, and samples the initial reward function implementation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2034" target="_blank">00:33:54.540</a></span> | <span class="t">So we found that the simulator code itself is actually a very good reference manual,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2038" target="_blank">00:33:58.840</a></span> | <span class="t">because it tells Eureka what are the variables you can use, like the hand positions, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2044" target="_blank">00:34:04.140</a></span> | <span class="t">here, the fingertip position, the fingertip safe, the rotation, angular velocity, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2049" target="_blank">00:34:09.780</a></span> | <span class="t">So you know all of these variables from the simulator code, and you know how they interact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2055" target="_blank">00:34:15.260</a></span> | <span class="t">with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2056" target="_blank">00:34:16.520</a></span> | <span class="t">So that serves as a very good in-context instruction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2061" target="_blank">00:34:21.640</a></span> | <span class="t">So Eureka doesn't need to reference any human-written reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2066" target="_blank">00:34:26.760</a></span> | <span class="t">And then once you have the generated reward, you plug it into any reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2070" target="_blank">00:34:30.920</a></span> | <span class="t">algorithm and just train it to completion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2073" target="_blank">00:34:33.860</a></span> | <span class="t">So this step is typically very costly and very slow, because reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2078" target="_blank">00:34:38.920</a></span> | <span class="t">itself is slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2080" target="_blank">00:34:40.220</a></span> | <span class="t">And we were only able to scale up Eureka because of NVIDIA's iSOC sim, which runs 1,000 simulated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2087" target="_blank">00:34:47.740</a></span> | <span class="t">environment copies on a single GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2090" target="_blank">00:34:50.360</a></span> | <span class="t">So basically, you can think of it as speeding up reality by 1,000x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2097" target="_blank">00:34:57.360</a></span> | <span class="t">And then after training, you will get the performance metrics back on each reward component.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2102" target="_blank">00:35:02.640</a></span> | <span class="t">And as we saw from Voyager, GPT-4 is very good at self-reflection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2107" target="_blank">00:35:07.280</a></span> | <span class="t">So we leveraged that capability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2110" target="_blank">00:35:10.080</a></span> | <span class="t">And there's a software trial reminding you to activate a license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2117" target="_blank">00:35:17.920</a></span> | <span class="t">Yeah, so Voyager reflects on it and then proposes mutations on the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2126" target="_blank">00:35:26.800</a></span> | <span class="t">So here, the mutations, we found, can be very diverse, ranging from something as simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2131" target="_blank">00:35:31.400</a></span> | <span class="t">as just changing a hyperparameter in the reward function weighting to all the way to adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2136" target="_blank">00:35:36.600</a></span> | <span class="t">completely novel components to the reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2141" target="_blank">00:35:41.280</a></span> | <span class="t">And in our experiments, Eureka turns out to be a superhuman reward engineer, actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2147" target="_blank">00:35:47.720</a></span> | <span class="t">outperforming some of the functions implemented by the expert human engineers on NVIDIA's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2153" target="_blank">00:35:53.400</a></span> | <span class="t">iSOC sim team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2157" target="_blank">00:35:57.100</a></span> | <span class="t">So here are some more demos of how Eureka is able to write very complex rewards that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2162" target="_blank">00:36:02.480</a></span> | <span class="t">lead to these extremely dexterous behaviors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2165" target="_blank">00:36:05.940</a></span> | <span class="t">And we can actually train the robot hand to rotate pens, not just in one direction, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2171" target="_blank">00:36:11.320</a></span> | <span class="t">in different directions along different 3D axes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2175" target="_blank">00:36:15.480</a></span> | <span class="t">I think one major contribution of Eureka, different from Voyager, is to bridge the gap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2180" target="_blank">00:36:20.640</a></span> | <span class="t">between high-level reasoning and low-level model controls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2185" target="_blank">00:36:25.060</a></span> | <span class="t">So Eureka introduces a new paradigm that I'm calling hybrid gradient architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2190" target="_blank">00:36:30.500</a></span> | <span class="t">So recall Voyager is a no-gradient architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2192" target="_blank">00:36:32.800</a></span> | <span class="t">We don't touch anything, and we don't train anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2195" target="_blank">00:36:35.960</a></span> | <span class="t">But Eureka is a hybrid gradient, where a black box inference-only language model instructs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2201" target="_blank">00:36:41.880</a></span> | <span class="t">a white box learnable neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2205" target="_blank">00:36:45.280</a></span> | <span class="t">So you can think of it as two loops, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2207" target="_blank">00:36:47.760</a></span> | <span class="t">The outer loop is gradient-free, and it's driven by GP4, kind of selecting the reward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2213" target="_blank">00:36:53.760</a></span> | <span class="t">functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2214" target="_blank">00:36:54.760</a></span> | <span class="t">And the inner loop is gradient-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2216" target="_blank">00:36:56.400</a></span> | <span class="t">You train a full reinforcement learning episode from it to achieve extreme dexterity using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2222" target="_blank">00:37:02.580</a></span> | <span class="t">a specialized, like training, by training a special neural network controller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2226" target="_blank">00:37:06.640</a></span> | <span class="t">And you must have both loops to succeed, to deliver this kind of dexterity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2231" target="_blank">00:37:11.820</a></span> | <span class="t">And I think it will be a very useful paradigm for training robot agents in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2239" target="_blank">00:37:19.100</a></span> | <span class="t">So you know, these days when I go on Twitter or X, I see AI conquering new lands like every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2246" target="_blank">00:37:26.720</a></span> | <span class="t">week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2247" target="_blank">00:37:27.720</a></span> | <span class="t">You know, chat, image generation, and music, they're all very well within reach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2253" target="_blank">00:37:33.260</a></span> | <span class="t">But MindDojo, Voyager, and Eureka, these are just scratching the surface of open-ended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2258" target="_blank">00:37:38.220</a></span> | <span class="t">journalist agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2260" target="_blank">00:37:40.720</a></span> | <span class="t">And looking forward, I want to share two key research directions that I personally find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2265" target="_blank">00:37:45.760</a></span> | <span class="t">extremely promising, and I'm also working on it myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2270" target="_blank">00:37:50.300</a></span> | <span class="t">The first is a continuation of MindClip, basically how to develop methods that learn from internet-skilled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2276" target="_blank">00:37:56.520</a></span> | <span class="t">videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2277" target="_blank">00:37:57.680</a></span> | <span class="t">And the second is multimodal foundation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2280" target="_blank">00:38:00.720</a></span> | <span class="t">Not that GPT-4VE is coming, but it is just the beginning of an era.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2285" target="_blank">00:38:05.680</a></span> | <span class="t">And I think it's important to have all of the modalities in a single foundation model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2292" target="_blank">00:38:12.780</a></span> | <span class="t">So first, about videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2295" target="_blank">00:38:15.080</a></span> | <span class="t">We all know that videos are abundant, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2297" target="_blank">00:38:17.400</a></span> | <span class="t">Like so many data on YouTube, way too many for our limited GPUs to process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2304" target="_blank">00:38:24.600</a></span> | <span class="t">They're extremely useful to train models that not only have dynamic perception and intuitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2310" target="_blank">00:38:30.400</a></span> | <span class="t">physics, but also capture the complexity of human creativity and human behaviors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2316" target="_blank">00:38:36.520</a></span> | <span class="t">It's all good, except that when you are using video to pre-training body agents, there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2322" target="_blank">00:38:42.960</a></span> | <span class="t">huge distribution shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2324" target="_blank">00:38:44.600</a></span> | <span class="t">You also don't get action labels, and you don't get any of the groundings because you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2328" target="_blank">00:38:48.280</a></span> | <span class="t">are a passive observer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2331" target="_blank">00:38:51.160</a></span> | <span class="t">So I think here's a demonstration of why learning from video is hard, even for natural intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2337" target="_blank">00:38:57.280</a></span> | <span class="t">So a little cat is seeing boxers shaking their head, and it thinks maybe shaking head is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2342" target="_blank">00:39:02.840</a></span> | <span class="t">the best way to do fighting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2348" target="_blank">00:39:08.720</a></span> | <span class="t">This is why learning from video is hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2353" target="_blank">00:39:13.720</a></span> | <span class="t">You have no idea, like, why...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2358" target="_blank">00:39:18.280</a></span> | <span class="t">This is too good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2359" target="_blank">00:39:19.280</a></span> | <span class="t">Let's play this again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2360" target="_blank">00:39:20.280</a></span> | <span class="t">You have no idea why Tyson is doing this, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2362" target="_blank">00:39:22.720</a></span> | <span class="t">Like, the cat has no idea, and then it associates this with just the wrong kind of policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2370" target="_blank">00:39:30.880</a></span> | <span class="t">But for sure, it doesn't help the fighting, but it definitely boosts the cat's confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2379" target="_blank">00:39:39.280</a></span> | <span class="t">That's why learning from video is hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2381" target="_blank">00:39:41.280</a></span> | <span class="t">Now, I want to point out a few kind of latest research in how to leverage so much video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2387" target="_blank">00:39:47.000</a></span> | <span class="t">for journalists' agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2389" target="_blank">00:39:49.320</a></span> | <span class="t">There are a couple of approaches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2391" target="_blank">00:39:51.480</a></span> | <span class="t">The first is the simplest, just learn kind of a visual feature extractor from the videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2397" target="_blank">00:39:57.080</a></span> | <span class="t">So this is R3M from Chelsea's group at Stanford.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2401" target="_blank">00:40:01.360</a></span> | <span class="t">And this model is still an image-level representation, just that it uses a video-level loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2406" target="_blank">00:40:06.200</a></span> | <span class="t">to train, more specifically, time-contrasted learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2410" target="_blank">00:40:10.280</a></span> | <span class="t">And after that, you can use this as an image backbone for any agent, but you still need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2415" target="_blank">00:40:15.560</a></span> | <span class="t">to kind of fine-tune using domain-specific data for the agent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2421" target="_blank">00:40:21.040</a></span> | <span class="t">The second path is to learn reward functions from video, and MindClip is one model under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2427" target="_blank">00:40:27.920</a></span> | <span class="t">this category.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2428" target="_blank">00:40:28.920</a></span> | <span class="t">It uses a contrastive objective between the transcript and video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2433" target="_blank">00:40:33.160</a></span> | <span class="t">And here, this work, VIP, is another way to learn a similarity-based reward for goal-conditioned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2439" target="_blank">00:40:39.600</a></span> | <span class="t">tasks in the image space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2441" target="_blank">00:40:41.920</a></span> | <span class="t">So this work, VIP is led by, who's also the first author of Eureka, and Eureka is his</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2447" target="_blank">00:40:47.160</a></span> | <span class="t">internship project with me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2450" target="_blank">00:40:50.560</a></span> | <span class="t">And the third idea is very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2452" target="_blank">00:40:52.680</a></span> | <span class="t">Can we directly do imitation learning from video, but better than the cat that we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2459" target="_blank">00:40:59.000</a></span> | <span class="t">saw?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2460" target="_blank">00:41:00.040</a></span> | <span class="t">So we just said, you know, the videos don't have the actions, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2464" target="_blank">00:41:04.640</a></span> | <span class="t">We need to find some ways to pseudo-label the actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2467" target="_blank">00:41:07.860</a></span> | <span class="t">And this is video training of VPT from OpenAI last year, to solve long-range tasks in Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2475" target="_blank">00:41:15.000</a></span> | <span class="t">And here, the pipeline works like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2478" target="_blank">00:41:18.400</a></span> | <span class="t">Basically, you use a keyboard and a mouse action space, so you can align this action</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2483" target="_blank">00:41:23.880</a></span> | <span class="t">space with the human actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2486" target="_blank">00:41:26.320</a></span> | <span class="t">And OpenAI hires a bunch of Minecraft players and actually collects data in-house.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2491" target="_blank">00:41:31.920</a></span> | <span class="t">So they record the episodes done by those gamers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2495" target="_blank">00:41:35.240</a></span> | <span class="t">And now you have a dataset of video and action pairs, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2500" target="_blank">00:41:40.160</a></span> | <span class="t">And you train something called an inverse dynamics model, which is to take the observation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2505" target="_blank">00:41:45.440</a></span> | <span class="t">and then predict the actions that caused the observation to change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2509" target="_blank">00:41:49.280</a></span> | <span class="t">So that's the inverse dynamics model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2511" target="_blank">00:41:51.240</a></span> | <span class="t">And that becomes a labeler that you can apply to in-the-wild YouTube videos that don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2518" target="_blank">00:41:58.120</a></span> | <span class="t">the actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2519" target="_blank">00:41:59.400</a></span> | <span class="t">So you apply IDM to like 70K hours of in-the-wild YouTube videos, and you will get these pseudo-labeled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2525" target="_blank">00:42:05.120</a></span> | <span class="t">pseudo-actions that are not always correct, but also way better than random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2530" target="_blank">00:42:10.320</a></span> | <span class="t">And then you're trying imitation learning on top of this augmented dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2534" target="_blank">00:42:14.520</a></span> | <span class="t">And in this way, OpenAI is able to greatly expand the data because the original data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2541" target="_blank">00:42:21.160</a></span> | <span class="t">collected from the humans are high quality, but they're extremely expensive, while in-the-wild</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2545" target="_blank">00:42:25.920</a></span> | <span class="t">YouTube videos are very cheap, but you don't have the actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2549" target="_blank">00:42:29.020</a></span> | <span class="t">So they kind of solved and got the best of both worlds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2552" target="_blank">00:42:32.920</a></span> | <span class="t">But still, you know, it's really expensive to hire these humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2557" target="_blank">00:42:37.800</a></span> | <span class="t">Now what's beyond the videos, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2560" target="_blank">00:42:40.480</a></span> | <span class="t">I'm a firm believer that multimodal models will be the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2564" target="_blank">00:42:44.560</a></span> | <span class="t">And I see text as a very lousy kind of 1D projection of our physical world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2569" target="_blank">00:42:49.320</a></span> | <span class="t">So it's essential to include the other sensory modalities to provide a full embodied experience.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2575" target="_blank">00:42:55.480</a></span> | <span class="t">And in the context of embodied agents, I think the input will be a mixture of text, images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2581" target="_blank">00:43:01.040</a></span> | <span class="t">videos, and even audio in the future, and the output will be actions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2586" target="_blank">00:43:06.780</a></span> | <span class="t">So here's a very early example of a multimodal language model for robot learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2592" target="_blank">00:43:12.960</a></span> | <span class="t">So let's imagine a household robot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2595" target="_blank">00:43:15.400</a></span> | <span class="t">We can ask the robot to bring us a cup of tea from the kitchen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2599" target="_blank">00:43:19.360</a></span> | <span class="t">But if we want to be more specific, I want this particular cup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2602" target="_blank">00:43:22.880</a></span> | <span class="t">That is my favorite cup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2604" target="_blank">00:43:24.040</a></span> | <span class="t">So show me this image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2606" target="_blank">00:43:26.620</a></span> | <span class="t">And we also provide a video demo of how we want to mop the floor and ask the robot to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2612" target="_blank">00:43:32.640</a></span> | <span class="t">imitate the similar motion in context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2616" target="_blank">00:43:36.480</a></span> | <span class="t">And when the robot sees an unfamiliar object, like a sweeper, we can explain it by providing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2620" target="_blank">00:43:40.480</a></span> | <span class="t">an image and showing this is a sweeper, now, you know, go ahead and do something with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2625" target="_blank">00:43:45.400</a></span> | <span class="t">tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2626" target="_blank">00:43:46.400</a></span> | <span class="t">And finally, to ensure safety, we can say, take a picture of that room and just do not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2629" target="_blank">00:43:49.440</a></span> | <span class="t">enter that room.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2631" target="_blank">00:43:51.720</a></span> | <span class="t">To achieve this, back last year, we proposed a model called VIMA, which stands for Visual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2638" target="_blank">00:43:58.360</a></span> | <span class="t">Motor Attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2639" target="_blank">00:43:59.360</a></span> | <span class="t">And in this work, we introduced a concept called multimodal prompting, where the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2644" target="_blank">00:44:04.580</a></span> | <span class="t">can be a mixture of text, image, and videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2648" target="_blank">00:44:08.400</a></span> | <span class="t">And this provides a very expressive API that just unifies a bunch of different robot tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2654" target="_blank">00:44:14.060</a></span> | <span class="t">that otherwise would require a very different pipeline or specialized models to solve in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2658" target="_blank">00:44:18.920</a></span> | <span class="t">prior literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2660" target="_blank">00:44:20.960</a></span> | <span class="t">And VIMA simply tokenizes everything, converting image and text into sequences of tokens and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2668" target="_blank">00:44:28.340</a></span> | <span class="t">training a transformer on top to output the robot arm actions autoregressively, one step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2674" target="_blank">00:44:34.440</a></span> | <span class="t">at a time during inference time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2677" target="_blank">00:44:37.600</a></span> | <span class="t">So just to look at some of the examples here, like this prompt, rearrange objects to match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2682" target="_blank">00:44:42.840</a></span> | <span class="t">the scene.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2683" target="_blank">00:44:43.840</a></span> | <span class="t">It is a classical task called visual goal reaching that has a big body of prior works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2687" target="_blank">00:44:47.480</a></span> | <span class="t">on it, and that's how our robot does it, given this prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2694" target="_blank">00:44:54.440</a></span> | <span class="t">And we can also give it novel concepts in context, like this is a blicker, this is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2699" target="_blank">00:44:59.040</a></span> | <span class="t">work, now put a work into a blicker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2701" target="_blank">00:45:01.720</a></span> | <span class="t">And both words are nonsensical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2703" target="_blank">00:45:03.760</a></span> | <span class="t">So it's not in the training data, but VIMA is able to generalize your shot and follow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2708" target="_blank">00:45:08.320</a></span> | <span class="t">the motion to manipulate this object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2711" target="_blank">00:45:11.560</a></span> | <span class="t">So the bot understands what we want and then follow this trajectory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2715" target="_blank">00:45:15.600</a></span> | <span class="t">And finally, we can give it more complex prompt, like these are the safety constraints, sweep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2720" target="_blank">00:45:20.960</a></span> | <span class="t">the box into this, but without exceeding that line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2724" target="_blank">00:45:24.000</a></span> | <span class="t">And we do this using the interleaving image and text tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2730" target="_blank">00:45:30.720</a></span> | <span class="t">And recently, Google Brain Robotics followed up after VIMA with RT1 and RT2, Robot Transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2737" target="_blank">00:45:37.640</a></span> | <span class="t">1 and 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2739" target="_blank">00:45:39.200</a></span> | <span class="t">And RT2 is using a similar recipe as I described, where they first kind of pre-train on internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2745" target="_blank">00:45:45.520</a></span> | <span class="t">scale data and then fine tune with some human collected demonstrations on the Google robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2751" target="_blank">00:45:51.760</a></span> | <span class="t">And RoboCAD from DeepMind is another interesting work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2754" target="_blank">00:45:54.880</a></span> | <span class="t">They train a single unified policy that works not just on a single robot, but actually across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2762" target="_blank">00:46:02.120</a></span> | <span class="t">different embodiments, different robot forms, and even generalize to a new hardware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2767" target="_blank">00:46:07.540</a></span> | <span class="t">So I think this is like a higher form of multimodal agent with a physical form factor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2772" target="_blank">00:46:12.560</a></span> | <span class="t">The morphology of the agent itself is another modality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2777" target="_blank">00:46:17.620</a></span> | <span class="t">So that concludes our looking forward section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2782" target="_blank">00:46:22.220</a></span> | <span class="t">And lastly, I want to kind of put all the links together of the works I described.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2787" target="_blank">00:46:27.340</a></span> | <span class="t">So this is minddojo.org.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2789" target="_blank">00:46:29.140</a></span> | <span class="t">We have open source everything, well, for all the projects where big fans are open source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2794" target="_blank">00:46:34.880</a></span> | <span class="t">we open source as much as we can, including like the model code, checkpoints, simulator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2801" target="_blank">00:46:41.080</a></span> | <span class="t">code, and training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2804" target="_blank">00:46:44.820</a></span> | <span class="t">And this is voyager.minddojo.org.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2807" target="_blank">00:46:47.860</a></span> | <span class="t">This is Eureka.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2809" target="_blank">00:46:49.980</a></span> | <span class="t">And this is Vyma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2813" target="_blank">00:46:53.540</a></span> | <span class="t">And one more thing, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2815" target="_blank">00:46:55.140</a></span> | <span class="t">If you just want an excuse to play Minecraft at work, then MindDojo is perfect for you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2820" target="_blank">00:47:00.080</a></span> | <span class="t">because you are collecting human demonstration to train generalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2824" target="_blank">00:47:04.220</a></span> | <span class="t">And if there's one thing that you take away from this talk, it should be this slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2830" target="_blank">00:47:10.180</a></span> | <span class="t">And lastly, I just want to remind all of us, despite all the progress I've shown, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2834" target="_blank">00:47:14.780</a></span> | <span class="t">we can do is still very far from human ingenuity as embodied agents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2841" target="_blank">00:47:21.380</a></span> | <span class="t">These are the videos from our dataset of people doing like decorating a winter wonderland</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2847" target="_blank">00:47:27.100</a></span> | <span class="t">or building the functioning CPU circuit within Minecraft.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2850" target="_blank">00:47:30.780</a></span> | <span class="t">And we're very far from that as AI research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2853" target="_blank">00:47:33.940</a></span> | <span class="t">So here's a call to the community.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2855" target="_blank">00:47:35.780</a></span> | <span class="t">If human can do these mind-blowing tasks, then why not our AI, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2860" target="_blank">00:47:40.740</a></span> | <span class="t">Let's find out together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2861" target="_blank">00:47:41.540</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=wwQ1LQA3RCU&t=2863" target="_blank">00:47:43.540</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>