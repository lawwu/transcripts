<html><head><title>Stanford CS25: V4 I Transformers that Transform Well Enough to Support Near-Shallow Architectures</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V4 I Transformers that Transform Well Enough to Support Near-Shallow Architectures</h2><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY"><img src="https://i.ytimg.com/vi/zL9B3eXq0gY/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./zL9B3eXq0gY.html">Whisper Transcript</a> | <a href="./transcript_zL9B3eXq0gY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">>> Today, for our talk, we have Professor Jake Williams from Drexel University.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=9" target="_blank">00:00:09.480</a></span> | <span class="t">He is an Associate Professor at Information Science at Drexel University's College of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=15" target="_blank">00:00:15.200</a></span> | <span class="t">Computing and Informatics in Philadelphia, Pennsylvania.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=18" target="_blank">00:00:18.920</a></span> | <span class="t">Dr. Williams has a background in physics and math with degrees from the University of Vermont,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=25" target="_blank">00:00:25.000</a></span> | <span class="t">and his research leverages a quantitative linguistic perspective that applies math and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=31" target="_blank">00:00:31.120</a></span> | <span class="t">statistical methodologies to analyze and improve linguistic learning systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=36" target="_blank">00:00:36.800</a></span> | <span class="t">Following a one-year postdoc appointment at the University of Berkeley, studying large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=41" target="_blank">00:00:41.760</a></span> | <span class="t">language, large-scale machine learning in 2015, Dr. Williams became a data science faculty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=48" target="_blank">00:00:48.440</a></span> | <span class="t">at Drexel, where he drove the foundation of a DSMS program and develops and instructs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=54" target="_blank">00:00:54.000</a></span> | <span class="t">data science coursework, including natural language processing with deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=60" target="_blank">00:01:00.560</a></span> | <span class="t">So, welcome, and thank you for coming today for your talk, and you could do a quick introduction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=66" target="_blank">00:01:06.320</a></span> | <span class="t">of yourself before you start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=68" target="_blank">00:01:08.320</a></span> | <span class="t">>> Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=69" target="_blank">00:01:09.320</a></span> | <span class="t">Thanks so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=70" target="_blank">00:01:10.320</a></span> | <span class="t">I got the mic here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=71" target="_blank">00:01:11.320</a></span> | <span class="t">Nice to see you all here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=72" target="_blank">00:01:12.600</a></span> | <span class="t">Thanks for coming out, and also for showing up online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=76" target="_blank">00:01:16.080</a></span> | <span class="t">It's a pleasure to be here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=79" target="_blank">00:01:19.760</a></span> | <span class="t">As was mentioned, my name is Jake, and my background's in math and physics, so the perspective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=85" target="_blank">00:01:25.240</a></span> | <span class="t">that I'm coming from towards this work might be a little bit different than the standard,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=89" target="_blank">00:01:29.400</a></span> | <span class="t">and that'll be a theme throughout the discussion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=92" target="_blank">00:01:32.800</a></span> | <span class="t">The purpose of this discussion is to go through a relatively long-term development, a project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=100" target="_blank">00:01:40.840</a></span> | <span class="t">that I've been working on, and as mentioned, my background is in quantitative linguistics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=106" target="_blank">00:01:46.840</a></span> | <span class="t">which means my history of focus on language has primarily been to develop general theories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=116" target="_blank">00:01:56.600</a></span> | <span class="t">and descriptions of phenomena that you observe with regards to linguistic units, whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=121" target="_blank">00:02:01.560</a></span> | <span class="t">those might be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=124" target="_blank">00:02:04.120</a></span> | <span class="t">It's a statistical approach based on theories of language generation that are statistical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=131" target="_blank">00:02:11.640</a></span> | <span class="t">in basis, and over the course of my time as a researcher, I've explored and ventured into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=141" target="_blank">00:02:21.560</a></span> | <span class="t">language modeling itself and ultimately into neural networks as they approach language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=146" target="_blank">00:02:26.080</a></span> | <span class="t">modeling themselves, and that's what brought me here through quite a bit of other work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=154" target="_blank">00:02:34.200</a></span> | <span class="t">so if you look into my profile, you'll see a lot of different subjects in either applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=158" target="_blank">00:02:38.800</a></span> | <span class="t">NLP, like I said, quantitative linguistics, and neural networks is a natural transition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=164" target="_blank">00:02:44.520</a></span> | <span class="t">for me into inferential work, so let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=172" target="_blank">00:02:52.160</a></span> | <span class="t">So well, this is how we'll start the conversation today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=176" target="_blank">00:02:56.320</a></span> | <span class="t">It's not exactly how we got here in my lab.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=178" target="_blank">00:02:58.920</a></span> | <span class="t">We came at this subject from a different approach, trying to think about layer initializations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=187" target="_blank">00:03:07.600</a></span> | <span class="t">in neural networks, and this subject that we're discussing as a front for this talk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=196" target="_blank">00:03:16.440</a></span> | <span class="t">is specifically focused on transformer architecture components, the self-attention component that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=201" target="_blank">00:03:21.480</a></span> | <span class="t">pivotal to the success of the transformer architecture, and it focuses on the fact that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=208" target="_blank">00:03:28.120</a></span> | <span class="t">self-attention requires a quadratic comparison of vectors in order to produce the feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=212" target="_blank">00:03:32.200</a></span> | <span class="t">weights of those vectors needed to model long-range dependencies in text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=218" target="_blank">00:03:38.320</a></span> | <span class="t">Commonly, parameters for self-attention are based on a transformation matrix, two, usually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=223" target="_blank">00:03:43.640</a></span> | <span class="t">queries and keys, that are responsible for dimensionalizing input vectors, and I describe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=229" target="_blank">00:03:49.800</a></span> | <span class="t">it this way because generally speaking, when you're at the point of a self-attention layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=234" target="_blank">00:03:54.640</a></span> | <span class="t">you already have low-dimensional vectors, but the parameters in a standard self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=240" target="_blank">00:04:00.080</a></span> | <span class="t">layer are changing the dimensionalities and the structure of that dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=244" target="_blank">00:04:04.520</a></span> | <span class="t">They are like an embedding layer, which is factorizing the embedding dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=250" target="_blank">00:04:10.560</a></span> | <span class="t">This redimensionalization is the primary means by which self-attention creates feature weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=255" target="_blank">00:04:15.520</a></span> | <span class="t">It really just computes similarity in that shared space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=261" target="_blank">00:04:21.400</a></span> | <span class="t">Large and similar inner products really just result in strongly weighted features, so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=265" target="_blank">00:04:25.360</a></span> | <span class="t">up to that dimensionalization to produce good similarities for whatever purpose your prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=271" target="_blank">00:04:31.520</a></span> | <span class="t">requires.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=272" target="_blank">00:04:32.520</a></span> | <span class="t">However, an alternative strategy for feature weights might ask, given a basis, so in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=278" target="_blank">00:04:38.000</a></span> | <span class="t">words, you're stuck with your low-dimensional vectors, what is the optimal way to convert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=283" target="_blank">00:04:43.320</a></span> | <span class="t">those comparisons of the vectors you're looking at by a matrix transformation to modify the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=289" target="_blank">00:04:49.320</a></span> | <span class="t">vector similarities that you are stuck with that correspond to the best weights for features?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=295" target="_blank">00:04:55.600</a></span> | <span class="t">In other words, treat this as a feed-forward layer to produce self-attention weights as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=300" target="_blank">00:05:00.240</a></span> | <span class="t">opposed to try and transform to some basis that produces good feature weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=306" target="_blank">00:05:06.320</a></span> | <span class="t">The use of this modified self-attention mechanism will be part and parcel the substance of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=310" target="_blank">00:05:10.920</a></span> | <span class="t">talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=315" target="_blank">00:05:15.960</a></span> | <span class="t">It's worth noting that this alternative mechanism is entirely compatible with the traditional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=320" target="_blank">00:05:20.160</a></span> | <span class="t">dimensionalizing version of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=322" target="_blank">00:05:22.120</a></span> | <span class="t">In other words, you could still change the dimension and compute similarities and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=329" target="_blank">00:05:29.400</a></span> | <span class="t">convert that with a second feed-forward layer to produce optimal feature weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=334" target="_blank">00:05:34.360</a></span> | <span class="t">This is not exclusive in any way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=336" target="_blank">00:05:36.600</a></span> | <span class="t">This is exploring how useful that alternative prediction of feature weights can function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=342" target="_blank">00:05:42.960</a></span> | <span class="t">However, we'll avoid the standard mechanism for two reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=347" target="_blank">00:05:47.200</a></span> | <span class="t">First, we have no solution to the standard parameters for self-attention as an initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=354" target="_blank">00:05:54.040</a></span> | <span class="t">And this will be discussed at length in slides to come.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=358" target="_blank">00:05:58.280</a></span> | <span class="t">Likewise, it would create an additional model complexity that would muddle the effects of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=363" target="_blank">00:06:03.160</a></span> | <span class="t">the modified form of self-attention that we wish to study.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=365" target="_blank">00:06:05.640</a></span> | <span class="t">So having that dimensionalization as a way to produce good feature weights would confuse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=372" target="_blank">00:06:12.120</a></span> | <span class="t">whether or not the feed-forward computation of feature weights is functioning well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=377" target="_blank">00:06:17.440</a></span> | <span class="t">There's a catch to this, however, which is that these vectors that we use for such a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=384" target="_blank">00:06:24.200</a></span> | <span class="t">self-attention layer better be good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=386" target="_blank">00:06:26.240</a></span> | <span class="t">In other words, their comparisons must be consistent and meaningful in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=394" target="_blank">00:06:34.080</a></span> | <span class="t">So to get it out of the way, here's an architectural diagram for the relatively simple near-shallow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=399" target="_blank">00:06:39.880</a></span> | <span class="t">architecture pattern that we're using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=403" target="_blank">00:06:43.400</a></span> | <span class="t">It doesn't seem like there are many neurons in a network of this type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=406" target="_blank">00:06:46.320</a></span> | <span class="t">And that's because all of the activations are softmax, which means despite the fact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=411" target="_blank">00:06:51.080</a></span> | <span class="t">that the U matrix, for example, is an entire layer, it's really just going through a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=416" target="_blank">00:06:56.920</a></span> | <span class="t">prediction non-linearity, the softmax function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=420" target="_blank">00:07:00.200</a></span> | <span class="t">So you can think about this as essentially a three-layer network that might be creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=424" target="_blank">00:07:04.000</a></span> | <span class="t">an encoder-decoder kind of design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=427" target="_blank">00:07:07.080</a></span> | <span class="t">Likewise, the difference in presentation here over self-attention, which is parameterized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=432" target="_blank">00:07:12.240</a></span> | <span class="t">by the matrix W here, is intending to show how a-- whether you consider it the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=441" target="_blank">00:07:21.560</a></span> | <span class="t">or the key-- one vector is the pivot for the comparison that will produce the feature weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=449" target="_blank">00:07:29.360</a></span> | <span class="t">which is then fed forward in this model through W.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=453" target="_blank">00:07:33.600</a></span> | <span class="t">This is the case for standard self-attention, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=455" target="_blank">00:07:35.960</a></span> | <span class="t">In other words, you can reduce it to a by-prediction diagram in this way, where a gray vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=462" target="_blank">00:07:42.360</a></span> | <span class="t">such as is depicted here, is that pivot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=467" target="_blank">00:07:47.160</a></span> | <span class="t">The attention distribution coming out of the W matrix and the softmax function is indicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=471" target="_blank">00:07:51.780</a></span> | <span class="t">by the vertical red bar there, which weights the block of vectors in black.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=477" target="_blank">00:07:57.840</a></span> | <span class="t">That includes the pivot vector in gray, which is then passed through a feed-forward layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=483" target="_blank">00:08:03.720</a></span> | <span class="t">often called the values of a standard self-attention matrix, U.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=488" target="_blank">00:08:08.760</a></span> | <span class="t">We then-- since we use U as a way to reduce the dimensionality of the prediction that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=494" target="_blank">00:08:14.720</a></span> | <span class="t">we're trying to make, we then feed that forward through another layer and then to output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=502" target="_blank">00:08:22.200</a></span> | <span class="t">And that's essentially the relative shallowness that we're talking about here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=507" target="_blank">00:08:27.140</a></span> | <span class="t">U is a self-attention matrix, which means there's really only two layers in effect here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=514" target="_blank">00:08:34.420</a></span> | <span class="t">And the activation functions are strange.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=516" target="_blank">00:08:36.660</a></span> | <span class="t">And you might wonder, for example, why we're using a different activation function, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=520" target="_blank">00:08:40.260</a></span> | <span class="t">softmax, instead of any of the dimensionally independent activation functions, like a logistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=525" target="_blank">00:08:45.780</a></span> | <span class="t">function or anything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=528" target="_blank">00:08:48.360</a></span> | <span class="t">And that's because we have additional insight into the softmax function and the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=533" target="_blank">00:08:53.340</a></span> | <span class="t">that it optimizes, which is very useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=538" target="_blank">00:08:58.580</a></span> | <span class="t">So let's talk about those vectors first, though, before we get to layer initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=547" target="_blank">00:09:07.700</a></span> | <span class="t">Optimizing the keys and queries of standard self-attention bears substantial similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=551" target="_blank">00:09:11.540</a></span> | <span class="t">to token and word embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=554" target="_blank">00:09:14.460</a></span> | <span class="t">This is because the key and query matrices have a common dimension that they project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=559" target="_blank">00:09:19.700</a></span> | <span class="t">to, much like you'd see with the factorization of an embedding layer on its own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=564" target="_blank">00:09:24.740</a></span> | <span class="t">Think Word2Vec, something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=569" target="_blank">00:09:29.500</a></span> | <span class="t">These normally-- there might be multiple self-attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=573" target="_blank">00:09:33.620</a></span> | <span class="t">And because of the indeterminacy in creating a different dimensional space-- in other words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=578" target="_blank">00:09:38.780</a></span> | <span class="t">there are multiple equivalent reshufflings of those different dimensions which will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=583" target="_blank">00:09:43.580</a></span> | <span class="t">the same output-- that indeterminacy is something that we hypothesize has bearing on what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=591" target="_blank">00:09:51.420</a></span> | <span class="t">now referred to as the lottery ticket hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=593" target="_blank">00:09:53.540</a></span> | <span class="t">In other words, that multiple-- or this is the way that I would state it-- but that multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=599" target="_blank">00:09:59.980</a></span> | <span class="t">different embeddings which produce different vector spaces can be leveraged in parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=604" target="_blank">00:10:04.340</a></span> | <span class="t">to create further robustness for the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=607" target="_blank">00:10:07.340</a></span> | <span class="t">Or in the way that it's implemented, that if a random initialization doesn't do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=613" target="_blank">00:10:13.500</a></span> | <span class="t">well, you can eliminate it from the network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=616" target="_blank">00:10:16.180</a></span> | <span class="t">And that sub-network will do just as well, even after it's totally trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=621" target="_blank">00:10:21.700</a></span> | <span class="t">In other words, having multiple clones, self-attention heads, which have no difference in the outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=627" target="_blank">00:10:27.140</a></span> | <span class="t">that they're trying to predict, is at the root of the lottery ticket hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=632" target="_blank">00:10:32.380</a></span> | <span class="t">And ultimately, that invocation of the lottery ticket hypothesis is really a justification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=637" target="_blank">00:10:37.100</a></span> | <span class="t">for eliminating parameters whose substantial cost of training are essentially wasted as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=641" target="_blank">00:10:41.780</a></span> | <span class="t">a result of random parameter initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=645" target="_blank">00:10:45.700</a></span> | <span class="t">You might ask questions like, well, what is a good initialization?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=649" target="_blank">00:10:49.100</a></span> | <span class="t">What is a good set of word embeddings to use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=655" target="_blank">00:10:55.780</a></span> | <span class="t">So how can lottery ticket hypothesis interactive effects of randomly initialized embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=662" target="_blank">00:11:02.020</a></span> | <span class="t">layers be avoided when constructing language models is another question that is embedded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=667" target="_blank">00:11:07.820</a></span> | <span class="t">in this discussion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=672" target="_blank">00:11:12.860</a></span> | <span class="t">But we shouldn't say that dimensionality reduction isn't needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=675" target="_blank">00:11:15.980</a></span> | <span class="t">It's incredibly necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=679" target="_blank">00:11:19.280</a></span> | <span class="t">For language modeling, you absolutely have to work with reduced dimension unless you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=683" target="_blank">00:11:23.660</a></span> | <span class="t">in a very small vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=685" target="_blank">00:11:25.500</a></span> | <span class="t">For example, like 26 Latin characters or something like that, like a wave to Vec.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=693" target="_blank">00:11:33.860</a></span> | <span class="t">The inherent input dimension of a large vocabulary model presents many computational intractabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=699" target="_blank">00:11:39.940</a></span> | <span class="t">when designing NLP systems, something that you're probably all very aware of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=703" target="_blank">00:11:43.660</a></span> | <span class="t">Likewise, though, the distance from embedding layers to learning information, the loss at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=710" target="_blank">00:11:50.500</a></span> | <span class="t">outputs, puts them in a challenging position to train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=713" target="_blank">00:11:53.980</a></span> | <span class="t">It's really hard to learn embedding layers because of the indeterminacy in the space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=720" target="_blank">00:12:00.260</a></span> | <span class="t">that you're trying to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=721" target="_blank">00:12:01.460</a></span> | <span class="t">You could swap dimensions, and it's equivalent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=727" target="_blank">00:12:07.060</a></span> | <span class="t">But the distance means that they receive learning information last.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=733" target="_blank">00:12:13.760</a></span> | <span class="t">This is a real challenge, and it's present in the history of NLP and deep learning, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=741" target="_blank">00:12:21.620</a></span> | <span class="t">Vanishing gradient stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=744" target="_blank">00:12:24.380</a></span> | <span class="t">And this is exacerbated in the way that we have to actually learn embedding layers in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=748" target="_blank">00:12:28.140</a></span> | <span class="t">standard models where we might modify learning rates to be lower all the way back at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=753" target="_blank">00:12:33.300</a></span> | <span class="t">bottom of a network to be gentle with those embedding layers and help them learn effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=760" target="_blank">00:12:40.620</a></span> | <span class="t">But this is really trouble because if we had a good embedding layer at the start, those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=766" target="_blank">00:12:46.380</a></span> | <span class="t">subsequent layers could be much easier to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=773" target="_blank">00:12:53.420</a></span> | <span class="t">So ultimately, in order to approach this challenge, we came along with a discernibility hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=783" target="_blank">00:13:03.060</a></span> | <span class="t">In other words, this boiled down to the theory that low-dimensional vectors, more than anything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=789" target="_blank">00:13:09.260</a></span> | <span class="t">needed to be able to discern features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=791" target="_blank">00:13:11.900</a></span> | <span class="t">And that doesn't sound like a very strong assertion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=797" target="_blank">00:13:17.020</a></span> | <span class="t">And we started with a really, really, really low bar and assumed that the most common features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=805" target="_blank">00:13:25.340</a></span> | <span class="t">needed to be the most discernible features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=807" target="_blank">00:13:27.480</a></span> | <span class="t">So if we're stuck with a lower dimension and we can't give everything a one-hot vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=811" target="_blank">00:13:31.140</a></span> | <span class="t">to be told apart very well, then we might want to give the more clear vectors, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=816" target="_blank">00:13:36.980</a></span> | <span class="t">have more dimensional independencies, to those features which appear most frequently and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=824" target="_blank">00:13:44.060</a></span> | <span class="t">could stand to confuse models the most.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=828" target="_blank">00:13:48.980</a></span> | <span class="t">This hypothesis led us directly to develop the bit cipher algorithm, which is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=834" target="_blank">00:13:54.500</a></span> | <span class="t">just a scheme for assigning vectors of zeros and ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=839" target="_blank">00:13:59.300</a></span> | <span class="t">Nothing too crazy in terms of what we're attempting to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=842" target="_blank">00:14:02.940</a></span> | <span class="t">In the figure at right here, the order of vector assignment is by row from top to bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=848" target="_blank">00:14:08.220</a></span> | <span class="t">And this is on a five-dimension, five-bit vector system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=854" target="_blank">00:14:14.140</a></span> | <span class="t">The first five from bottom are those one-hot vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=858" target="_blank">00:14:18.380</a></span> | <span class="t">Past that point, you'll see two-hot vectors, but they're a little bit less darkly shaded,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=864" target="_blank">00:14:24.700</a></span> | <span class="t">indicating the way that we actually utilize the system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=866" target="_blank">00:14:26.740</a></span> | <span class="t">In other words, we normalize them to have unit sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=874" target="_blank">00:14:34.020</a></span> | <span class="t">What I hope you can see from this is that the bit cipher algorithm generalizes one-hot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=879" target="_blank">00:14:39.940</a></span> | <span class="t">vectors to low dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=883" target="_blank">00:14:43.060</a></span> | <span class="t">And as a result, we can work from a very sparse feature set and explore dimensionalities as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=890" target="_blank">00:14:50.220</a></span> | <span class="t">a controlled phenomenon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=894" target="_blank">00:14:54.020</a></span> | <span class="t">And this assignment is incredibly naive, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=897" target="_blank">00:14:57.180</a></span> | <span class="t">That's the other thing that I want you to see as well, that this discernibility hypothesis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=901" target="_blank">00:15:01.740</a></span> | <span class="t">does not create any meaningful correlations between tokens that behave similarly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=905" target="_blank">00:15:05.980</a></span> | <span class="t">So if you've got the upper and lower case of a word, their vectors aren't going to capture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=911" target="_blank">00:15:11.620</a></span> | <span class="t">those similarities according to the bit cipher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=914" target="_blank">00:15:14.460</a></span> | <span class="t">It's really just gonna try and make sure that those features are distinguishable in a low-dimensional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=919" target="_blank">00:15:19.020</a></span> | <span class="t">space and that the most distinguishable features are those which appear most commonly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=925" target="_blank">00:15:25.020</a></span> | <span class="t">This was enough to do a surprising amount of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=931" target="_blank">00:15:31.960</a></span> | <span class="t">So with some scheme for a deterministic low-dimensionalization procedure, we were then able to utilize this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=941" target="_blank">00:15:41.960</a></span> | <span class="t">solution that we had actually developed previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=945" target="_blank">00:15:45.500</a></span> | <span class="t">So this was actually the real motivator for a lot of the work that you're seeing today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=949" target="_blank">00:15:49.940</a></span> | <span class="t">although it might seem like it's just a checkpoint in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=956" target="_blank">00:15:56.020</a></span> | <span class="t">Provided bit cipher produces decent embeddings, we can ask, can other layers be non-randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=960" target="_blank">00:16:00.980</a></span> | <span class="t">initialized?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=961" target="_blank">00:16:01.980</a></span> | <span class="t">In other words, without gradient descent or backpropagation or other gradient-based iterative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=965" target="_blank">00:16:05.180</a></span> | <span class="t">algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=967" target="_blank">00:16:07.740</a></span> | <span class="t">This equation came about from analysis of Word2Vec with the original Softmax activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=973" target="_blank">00:16:13.880</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=976" target="_blank">00:16:16.300</a></span> | <span class="t">And much like other articulations of the Word2Vec family of embeddings, came up with differential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=987" target="_blank">00:16:27.220</a></span> | <span class="t">solutions that depended on co-occurrence matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=991" target="_blank">00:16:31.020</a></span> | <span class="t">We formalized this as a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=992" target="_blank">00:16:32.820</a></span> | <span class="t">Is there a way to take a co-occurrence matrix, F, in this equation here, and convert it with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1001" target="_blank">00:16:41.300</a></span> | <span class="t">some weights, some denominators by row, into something that warms up a single-layer feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1011" target="_blank">00:16:51.020</a></span> | <span class="t">in a neural network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1013" target="_blank">00:16:53.380</a></span> | <span class="t">And ultimately, this k minus 1 over k term here, and this sum, is really just expressing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1020" target="_blank">00:17:00.520</a></span> | <span class="t">something like conditional probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1023" target="_blank">00:17:03.700</a></span> | <span class="t">Like conditional probability, because k minus 1 over k is a wrinkle that says that as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1031" target="_blank">00:17:11.860</a></span> | <span class="t">number of features increases, in other words, the context window increases in a block transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1039" target="_blank">00:17:19.500</a></span> | <span class="t">then the warm start that we could apply to start off a neural network without a randomness,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1046" target="_blank">00:17:26.540</a></span> | <span class="t">entirely determined by the vectors underneath, nearing whatever direction it's going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1054" target="_blank">00:17:34.540</a></span> | <span class="t">All we have to do is compute some co-occurrences between inputs and outputs, and I don't mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1059" target="_blank">00:17:39.340</a></span> | <span class="t">necessarily standard co-occurrences that you might have learned about a long time ago which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1063" target="_blank">00:17:43.820</a></span> | <span class="t">depend on a radius.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1064" target="_blank">00:17:44.860</a></span> | <span class="t">I mean, whatever your inputs are, whatever your outputs are, you take their sum of outer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1072" target="_blank">00:17:52.240</a></span> | <span class="t">products and you get a co-occurrence matrix of inputs and outputs, and that can then be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1078" target="_blank">00:17:58.220</a></span> | <span class="t">utilized to initialize your layer in that neural network to be vastly more performant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1086" target="_blank">00:18:06.100</a></span> | <span class="t">than what you'd get by a random initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1090" target="_blank">00:18:10.380</a></span> | <span class="t">This was a strong motivator for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1093" target="_blank">00:18:13.900</a></span> | <span class="t">This was just for a single-layer model, but it depended on the softmax function for activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1102" target="_blank">00:18:22.060</a></span> | <span class="t">And the softmax function as an activation function, we knew, is also necessary for self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1108" target="_blank">00:18:28.860</a></span> | <span class="t">features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1110" target="_blank">00:18:30.240</a></span> | <span class="t">And this meant that if we could put self-attention into some kind of a standard form with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1115" target="_blank">00:18:35.700</a></span> | <span class="t">equation just like a single layer, then we could apply the same solution with one catch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1123" target="_blank">00:18:43.940</a></span> | <span class="t">That catch is specifically that we don't know what the targets are for self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1129" target="_blank">00:18:49.100</a></span> | <span class="t">There's no target vector y, the thing that you're trying to predict, which position is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1135" target="_blank">00:18:55.340</a></span> | <span class="t">the one that you want to weight most strongly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1138" target="_blank">00:18:58.700</a></span> | <span class="t">And so in order to apply this solution for a self-attention model, we had to do some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1143" target="_blank">00:19:03.260</a></span> | <span class="t">more analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1145" target="_blank">00:19:05.140</a></span> | <span class="t">And that's in the reference number one, which is all the way back up in the first slide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1149" target="_blank">00:19:09.340</a></span> | <span class="t">if you want to see it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1151" target="_blank">00:19:11.120</a></span> | <span class="t">But that derives a differential criterion, an analog for the single-layer solution that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1157" target="_blank">00:19:17.980</a></span> | <span class="t">tells us what the targets of that kind of self-attention actually are, the hidden targets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1163" target="_blank">00:19:23.460</a></span> | <span class="t">the weights that you're trying to create, which really are just about making sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1168" target="_blank">00:19:28.700</a></span> | <span class="t">the layer above self-attention has some unsurprising things coming towards it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1176" target="_blank">00:19:36.300</a></span> | <span class="t">The self-attention layer is really just trying to massage the vectors so that way they look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1180" target="_blank">00:19:40.140</a></span> | <span class="t">like something that the next layer above expects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1184" target="_blank">00:19:44.540</a></span> | <span class="t">Aside from that, though, it's a much more in-depth conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1188" target="_blank">00:19:48.620</a></span> | <span class="t">The point, though, is that for the model in this picture here, we can now start off with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1200" target="_blank">00:20:00.100</a></span> | <span class="t">vectors x that are not random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1204" target="_blank">00:20:04.640</a></span> | <span class="t">We can use those vectors x to initialize non-randomly the parameters in W, the self-attention matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1213" target="_blank">00:20:13.780</a></span> | <span class="t">and then use that, going up the network, to initialize the parameters in U, since it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1219" target="_blank">00:20:19.460</a></span> | <span class="t">just a feed-forward layer with whatever self-attention is giving it as weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1224" target="_blank">00:20:24.620</a></span> | <span class="t">And then whatever that produces, the hidden state, H, we can use that with the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1229" target="_blank">00:20:29.700</a></span> | <span class="t">targets after the output layer to warm up the matrix O.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1237" target="_blank">00:20:37.460</a></span> | <span class="t">And you might say, "Okay, well, how did you figure out what those hidden targets are?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1243" target="_blank">00:20:43.660</a></span> | <span class="t">You had to have an output for the U matrix to try and hit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1249" target="_blank">00:20:49.100</a></span> | <span class="t">That too is something that the bit cipher can provide in the form of label embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1257" target="_blank">00:20:57.300</a></span> | <span class="t">In other words, low-dimensional targets of the thing that is downstream that you're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1261" target="_blank">00:21:01.420</a></span> | <span class="t">to hit, the language model's output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1264" target="_blank">00:21:04.660</a></span> | <span class="t">So similarly, we can warm start the U matrix in terms of those bit cipher label embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1276" target="_blank">00:21:16.260</a></span> | <span class="t">So in this view, the aim is to show how simple and general a single-layer softmax activated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1280" target="_blank">00:21:20.880</a></span> | <span class="t">solution is to apply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1282" target="_blank">00:21:22.880</a></span> | <span class="t">It's really just no more challenging than computing conditional probability given inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1287" target="_blank">00:21:27.660</a></span> | <span class="t">and outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1290" target="_blank">00:21:30.420</a></span> | <span class="t">It's fast, it's something that you can distribute in terms of processing, and it's very, very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1297" target="_blank">00:21:37.460</a></span> | <span class="t">general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1299" target="_blank">00:21:39.960</a></span> | <span class="t">So this is essentially the process that we're using in order to warm up the W and U matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1311" target="_blank">00:21:51.060</a></span> | <span class="t">There's the U matrix there, starts out as zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1314" target="_blank">00:21:54.980</a></span> | <span class="t">In other words, nothing, no random values, no weights anywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1320" target="_blank">00:22:00.460</a></span> | <span class="t">Over the data, which is just borrowing the dimension of this gigantic Y matrix that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1326" target="_blank">00:22:06.940</a></span> | <span class="t">all of the targets in it for the entire data set, we simply just take the outer products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1334" target="_blank">00:22:14.060</a></span> | <span class="t">of whatever the hidden state, the input to that layer is, assuming that the lower layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1338" target="_blank">00:22:18.740</a></span> | <span class="t">beneath it are also warmed up with whatever the targets for that layer are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1345" target="_blank">00:22:25.740</a></span> | <span class="t">Following that, it's really just about normalization and a logarithmic transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1351" target="_blank">00:22:31.380</a></span> | <span class="t">And that logarithm really just emerges as a result of being an inverse to the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1356" target="_blank">00:22:36.860</a></span> | <span class="t">function, which is a part of softmax, pretty much all of softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1363" target="_blank">00:22:43.540</a></span> | <span class="t">And that's really what brought us here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1368" target="_blank">00:22:48.140</a></span> | <span class="t">So what does warm starting a network do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1370" target="_blank">00:22:50.700</a></span> | <span class="t">This is going back to before we had the bit cipher algorithm for dimensionality reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1378" target="_blank">00:22:58.980</a></span> | <span class="t">And we started out by just saying, OK, if we take a simple, simple language model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1385" target="_blank">00:23:05.700</a></span> | <span class="t">only looks at a radius of traditional co-occurrences as features, we can concatenate those vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1393" target="_blank">00:23:13.140</a></span> | <span class="t">and feed them forward for a language model's output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1397" target="_blank">00:23:17.380</a></span> | <span class="t">A completely random start, a cold start to a language model, is really just the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1404" target="_blank">00:23:24.620</a></span> | <span class="t">of the vocabulary in perplexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1407" target="_blank">00:23:27.860</a></span> | <span class="t">And those three lines here for a few different radii are demonstrating that point with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1413" target="_blank">00:23:33.980</a></span> | <span class="t">point all the way at the top left-hand corner of this figure, cold starts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1421" target="_blank">00:23:41.380</a></span> | <span class="t">In any of those cases, when the warm start is applied, the perplexity is immediately</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1425" target="_blank">00:23:45.460</a></span> | <span class="t">automatically lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1429" target="_blank">00:23:49.100</a></span> | <span class="t">And furthermore, the trajectories that the updates follow continue in the same learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1437" target="_blank">00:23:57.460</a></span> | <span class="t">rate and the same time to perform better than models that were started cold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1445" target="_blank">00:24:05.700</a></span> | <span class="t">If you have an early stopping criterion, similarly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1448" target="_blank">00:24:08.820</a></span> | <span class="t">Early stopping, well, more than just generally, engage first and with a higher perplexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1459" target="_blank">00:24:19.300</a></span> | <span class="t">So this was the first indication that we had figured out something that's very useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1464" target="_blank">00:24:24.820</a></span> | <span class="t">There are some folks on Slido saying they're a bit confused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1471" target="_blank">00:24:31.780</a></span> | <span class="t">They're asking, are we talking about an alternative approach to self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1476" target="_blank">00:24:36.060</a></span> | <span class="t">We are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1477" target="_blank">00:24:37.060</a></span> | <span class="t">So we're all the way back at slide one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1481" target="_blank">00:24:41.660</a></span> | <span class="t">And it is the premise of this whole conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1485" target="_blank">00:24:45.980</a></span> | <span class="t">So here, in this modified version of self-attention, you might normally expect to do a comparison</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1493" target="_blank">00:24:53.020</a></span> | <span class="t">of your inputs, the matrix X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1495" target="_blank">00:24:55.820</a></span> | <span class="t">Whatever your inputs are, they might be a whole block of vectors, or they might be--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1499" target="_blank">00:24:59.660</a></span> | <span class="t">this is self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1500" target="_blank">00:25:00.660</a></span> | <span class="t">It's not cross-attention, where you have different vectors that you're trying to attend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1506" target="_blank">00:25:06.500</a></span> | <span class="t">And forgetting about the values, which for us is the U matrix, the keys and queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1515" target="_blank">00:25:15.700</a></span> | <span class="t">which are the parameters for self-attention, are in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1518" target="_blank">00:25:18.340</a></span> | <span class="t">They're in between the two copies of the inputs, X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1525" target="_blank">00:25:25.580</a></span> | <span class="t">Each of those you can view as some kind of a projection down to a dimension where they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1529" target="_blank">00:25:29.860</a></span> | <span class="t">can interact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1530" target="_blank">00:25:30.860</a></span> | <span class="t">And this is necessary for something like cross-attention, where you might have different dimensionalities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1535" target="_blank">00:25:35.540</a></span> | <span class="t">like X1 and X2 in two separate groups of vectors if you're doing something like machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1541" target="_blank">00:25:41.820</a></span> | <span class="t">That's not necessary to think about when you're just looking to do a standard language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1548" target="_blank">00:25:48.660</a></span> | <span class="t">that has to predict the next output according to the inputs, which are also outputs from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1553" target="_blank">00:25:53.700</a></span> | <span class="t">previous iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1558" target="_blank">00:25:58.940</a></span> | <span class="t">Two insights here-- one, that multiplying the key and query matrices, WK and WQ, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1568" target="_blank">00:26:08.740</a></span> | <span class="t">just another parameter matrix that's implied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1571" target="_blank">00:26:11.860</a></span> | <span class="t">There aren't two parameter matrices there in the middle for self-attention in any effective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1577" target="_blank">00:26:17.380</a></span> | <span class="t">way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1579" target="_blank">00:26:19.060</a></span> | <span class="t">There is a common dimension of comparison, and that kind of just moves stuff around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1584" target="_blank">00:26:24.580</a></span> | <span class="t">It creates degrees of freedom so that optimization can figure out what's the best weighting from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1591" target="_blank">00:26:31.300</a></span> | <span class="t">comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1592" target="_blank">00:26:32.300</a></span> | <span class="t">But the softmax function is strictly operating on similarities of that comparison space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1600" target="_blank">00:26:40.500</a></span> | <span class="t">It's not doing anything with those similarities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1603" target="_blank">00:26:43.620</a></span> | <span class="t">It's just softmaxing them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1604" target="_blank">00:26:44.860</a></span> | <span class="t">It's just activating them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1606" target="_blank">00:26:46.460</a></span> | <span class="t">So if it was a big similarity, it's a big attention value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1611" target="_blank">00:26:51.060</a></span> | <span class="t">In this equation, there's no transformation happening before those vectors are multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1616" target="_blank">00:26:56.580</a></span> | <span class="t">together, inner products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1618" target="_blank">00:26:58.980</a></span> | <span class="t">So those vectors better be good vectors that you're starting with-- x and x transpose,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1623" target="_blank">00:27:03.740</a></span> | <span class="t">the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1626" target="_blank">00:27:06.020</a></span> | <span class="t">They better be vectors that are comparable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1628" target="_blank">00:27:08.580</a></span> | <span class="t">They can't be vectors from cross-attention, where you're trying to translate from one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1631" target="_blank">00:27:11.940</a></span> | <span class="t">language to another, and they just don't inner product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1633" target="_blank">00:27:13.940</a></span> | <span class="t">They're different dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1635" target="_blank">00:27:15.740</a></span> | <span class="t">You could force it through if they were two differently trained embedding layers, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1639" target="_blank">00:27:19.740</a></span> | <span class="t">they had the same dimension with this mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1643" target="_blank">00:27:23.140</a></span> | <span class="t">And if you didn't, you could put those key and query matrices back in between the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1647" target="_blank">00:27:27.500</a></span> | <span class="t">x vectors, x blocks of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1653" target="_blank">00:27:33.580</a></span> | <span class="t">But a lot of what's going on here in this talk is trying to simplify and make more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1662" target="_blank">00:27:42.860</a></span> | <span class="t">the architectures that we need and the mechanisms that they utilize, given what we know about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1669" target="_blank">00:27:49.940</a></span> | <span class="t">how language functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1672" target="_blank">00:27:52.300</a></span> | <span class="t">And that's a critical piece there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1673" target="_blank">00:27:53.640</a></span> | <span class="t">We have assumptions that we can make.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1675" target="_blank">00:27:55.520</a></span> | <span class="t">If all we're doing is autoregression, we don't need cross-attention dimensionalization in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1680" target="_blank">00:28:00.260</a></span> | <span class="t">between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1681" target="_blank">00:28:01.260</a></span> | <span class="t">That'll be the theme, in other words, that can we use knowledge that we have about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1688" target="_blank">00:28:08.980</a></span> | <span class="t">way language functions to design better versions of architectures that meet the needs of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1696" target="_blank">00:28:16.300</a></span> | <span class="t">instead of being simply general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1698" target="_blank">00:28:18.460</a></span> | <span class="t">Is this good?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1702" target="_blank">00:28:22.780</a></span> | <span class="t">This is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1703" target="_blank">00:28:23.780</a></span> | <span class="t">So if there are any questions here, it's a good time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1710" target="_blank">00:28:30.980</a></span> | <span class="t">We are there and there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1716" target="_blank">00:28:36.980</a></span> | <span class="t">So we just talked briefly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1718" target="_blank">00:28:38.920</a></span> | <span class="t">This was for language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1719" target="_blank">00:28:39.920</a></span> | <span class="t">The thing about language models is it's a really simple language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1723" target="_blank">00:28:43.100</a></span> | <span class="t">There's no self-attention here yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1726" target="_blank">00:28:46.420</a></span> | <span class="t">This is really just evaluating that a warm start in either the blue, green, or purple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1731" target="_blank">00:28:51.320</a></span> | <span class="t">case does better than its partner, which is a cold start of the same architecture, same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1737" target="_blank">00:28:57.660</a></span> | <span class="t">hyperparameters, orange, reddish, and brown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1744" target="_blank">00:29:04.740</a></span> | <span class="t">So three different models, regardless of how long your context is in each case here, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1750" target="_blank">00:29:10.160</a></span> | <span class="t">see that a model which has a nonrandom initialization by the equation presented two slides back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1756" target="_blank">00:29:16.520</a></span> | <span class="t">from here starts a network off with a much lower perplexity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1766" target="_blank">00:29:26.920</a></span> | <span class="t">The requirements to apply this solution to a feedforward layer of parameters is simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1773" target="_blank">00:29:33.180</a></span> | <span class="t">that your inputs should not have negative values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1781" target="_blank">00:29:41.300</a></span> | <span class="t">That's really all we have to worry about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1784" target="_blank">00:29:44.420</a></span> | <span class="t">So it becomes really easy to ask questions like, well, what happens when you apply this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1790" target="_blank">00:29:50.080</a></span> | <span class="t">to other data with non-negative values?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1792" target="_blank">00:29:52.940</a></span> | <span class="t">Well, there's one little catch that we had to think about here in this case, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1797" target="_blank">00:29:57.820</a></span> | <span class="t">is with the bit cipher or one-hot vectors, we're controlling the norms of the inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1804" target="_blank">00:30:04.260</a></span> | <span class="t">With standard embeddings, with MNIST, for example, when you're trying to predict the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1810" target="_blank">00:30:10.420</a></span> | <span class="t">handwritten digits, 0 through 9 value, you don't get to assume necessarily that all inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1818" target="_blank">00:30:18.660</a></span> | <span class="t">have the same norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1821" target="_blank">00:30:21.580</a></span> | <span class="t">You can normalize the inputs, but it doesn't necessarily make sense to normalize them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1826" target="_blank">00:30:26.540</a></span> | <span class="t">one when you're looking at images, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1829" target="_blank">00:30:29.340</a></span> | <span class="t">They're non-negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1830" target="_blank">00:30:30.340</a></span> | <span class="t">They have 0 through 255, for example, in MNIST.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1835" target="_blank">00:30:35.440</a></span> | <span class="t">And as a result, we can put these data through that same warm start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1842" target="_blank">00:30:42.100</a></span> | <span class="t">Now one little caveat here I've alluded to about the norms of vectors is that we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1851" target="_blank">00:30:51.020</a></span> | <span class="t">know what that value of k is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1853" target="_blank">00:30:53.060</a></span> | <span class="t">In other words, let me go back, you could look at it here or here, that's the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1862" target="_blank">00:31:02.660</a></span> | <span class="t">of features per prediction, which if you're looking at unit-normed word vectors is however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1870" target="_blank">00:31:10.540</a></span> | <span class="t">big your context window is, k, because they all have unit norm and there's k of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1877" target="_blank">00:31:17.860</a></span> | <span class="t">But if you're looking at just an image, it's not clear if it's a composition of multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1883" target="_blank">00:31:23.060</a></span> | <span class="t">vectors, if it's one vector, and how many it is, if it is a composition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1888" target="_blank">00:31:28.340</a></span> | <span class="t">It just has a norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1892" target="_blank">00:31:32.940</a></span> | <span class="t">In application to data like that, that is what k becomes, the average norm of an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1901" target="_blank">00:31:41.900</a></span> | <span class="t">And I'm regretting not putting a graph in this, but the paper that discusses this shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1905" target="_blank">00:31:45.820</a></span> | <span class="t">that in the MNIST dataset, the exact optimal value of k is the average norm of the inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1914" target="_blank">00:31:54.300</a></span> | <span class="t">however you've pre-processed them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1918" target="_blank">00:31:58.300</a></span> | <span class="t">And that's how we generally apply this rule when we're warm starting systems and we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1922" target="_blank">00:32:02.420</a></span> | <span class="t">have unit-normed vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1924" target="_blank">00:32:04.940</a></span> | <span class="t">And it was learned from studying this model's application, this solution's application to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1931" target="_blank">00:32:11.340</a></span> | <span class="t">non-linguistic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1933" target="_blank">00:32:13.180</a></span> | <span class="t">But as mentioned, the purpose was always towards language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1941" target="_blank">00:32:21.620</a></span> | <span class="t">So longer context windows in principle should provide models with more information than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1946" target="_blank">00:32:26.820</a></span> | <span class="t">shorter context windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1950" target="_blank">00:32:30.140</a></span> | <span class="t">This means one should expect that models perform better when context window length is longer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1957" target="_blank">00:32:37.940</a></span> | <span class="t">theoretically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1960" target="_blank">00:32:40.420</a></span> | <span class="t">And this is essentially the reason for why self-attention was initially developed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1965" target="_blank">00:32:45.180</a></span> | <span class="t">Researchers wanted to improve language models and context windows, providing more information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1969" target="_blank">00:32:49.620</a></span> | <span class="t">were seen as the key to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1971" target="_blank">00:32:51.380</a></span> | <span class="t">In other words, the more features, the more information, the more flexibility a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1977" target="_blank">00:32:57.380</a></span> | <span class="t">can have and expressivity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1980" target="_blank">00:33:00.900</a></span> | <span class="t">However, without feature weights, models didn't simply get better with long context windows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1986" target="_blank">00:33:06.300</a></span> | <span class="t">and feature weights and self-attention were hypothesized to be needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1991" target="_blank">00:33:11.540</a></span> | <span class="t">And this was proven back in 2017 with the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=1998" target="_blank">00:33:18.740</a></span> | <span class="t">In moving towards self-attention and transformer though, the primacy of the transformer architecture's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2004" target="_blank">00:33:24.700</a></span> | <span class="t">block context model casts a shadow over the use of other context models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2012" target="_blank">00:33:32.260</a></span> | <span class="t">So for example, if I were to ask here, is it clear to everyone that the standard self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2021" target="_blank">00:33:41.020</a></span> | <span class="t">block model of context is different than the traditional notion of co-occurrences, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2026" target="_blank">00:33:46.660</a></span> | <span class="t">use a radius that is not positionally anchored?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2030" target="_blank">00:33:50.620</a></span> | <span class="t">It is the context model, the positional anchoring of the block context model, that gives it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2036" target="_blank">00:33:56.980</a></span> | <span class="t">its information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2039" target="_blank">00:33:59.540</a></span> | <span class="t">It is not, in all likelihood, anything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2046" target="_blank">00:34:06.540</a></span> | <span class="t">Now what you do with that context model matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2050" target="_blank">00:34:10.540</a></span> | <span class="t">You can't just take those vectors in a block, add them together, and expect a feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2054" target="_blank">00:34:14.460</a></span> | <span class="t">to do well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2055" target="_blank">00:34:15.460</a></span> | <span class="t">That's where self-attention is needed in order to figure out which vector needs the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2059" target="_blank">00:34:19.300</a></span> | <span class="t">weight, most weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2063" target="_blank">00:34:23.540</a></span> | <span class="t">So what you'll also see in the architectures that are based on what I've already presented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2069" target="_blank">00:34:29.260</a></span> | <span class="t">is that we're interested to explore how different models of context for language models can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2074" target="_blank">00:34:34.740</a></span> | <span class="t">be integrated in general because they each provide different information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2081" target="_blank">00:34:41.440</a></span> | <span class="t">And we all know that the standard transformer's block model of context requires a ridiculous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2086" target="_blank">00:34:46.300</a></span> | <span class="t">amount of information and data in order to become effectively trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2093" target="_blank">00:34:53.080</a></span> | <span class="t">So the current state of contexts that we use, top there might be the standard transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2101" target="_blank">00:35:01.900</a></span> | <span class="t">context that has a fixed positional block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2104" target="_blank">00:35:04.860</a></span> | <span class="t">And it takes the first 10 tokens, for example, the second 10 tokens, and the third 10 tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2110" target="_blank">00:35:10.860</a></span> | <span class="t">each in different blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2113" target="_blank">00:35:13.080</a></span> | <span class="t">Each of those is a group of contextualizing vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2118" target="_blank">00:35:18.220</a></span> | <span class="t">The second one there that you see with the r as a subscript is a radial model because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2123" target="_blank">00:35:23.620</a></span> | <span class="t">those do different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2124" target="_blank">00:35:24.980</a></span> | <span class="t">In other words, rather than assume you're looking at the first 10 or the nth 10 features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2130" target="_blank">00:35:30.820</a></span> | <span class="t">you pick a radius and you say, what are the last r features, the last r vectors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2136" target="_blank">00:35:36.980</a></span> | <span class="t">That can also have an attention distribution, a self-attention distribution, according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2141" target="_blank">00:35:41.140</a></span> | <span class="t">the exact same model that's being presented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2145" target="_blank">00:35:45.140</a></span> | <span class="t">It produces an entirely separate context in the state, whatever you want to call it, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2151" target="_blank">00:35:51.900</a></span> | <span class="t">can be conjoined with the block model to articulate features and be given to an output layer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2160" target="_blank">00:36:00.780</a></span> | <span class="t">knows what to do with them when each has different values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2166" target="_blank">00:36:06.940</a></span> | <span class="t">The concatenation of those different context models keeps the information separate so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2171" target="_blank">00:36:11.500</a></span> | <span class="t">output layer can decide which portion of the context is useful for the prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2178" target="_blank">00:36:18.820</a></span> | <span class="t">This last one is getting really traditional at the bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2182" target="_blank">00:36:22.540</a></span> | <span class="t">It's what I refer to as a document model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2186" target="_blank">00:36:26.300</a></span> | <span class="t">If you've ever implemented something like a Naive Bayes classifier or a term frequency</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2193" target="_blank">00:36:33.660</a></span> | <span class="t">inverse document frequency model, that's essentially what a document model is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2198" target="_blank">00:36:38.780</a></span> | <span class="t">Set up your vectors, you get something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2203" target="_blank">00:36:43.140</a></span> | <span class="t">Is it going to be the best for predicting the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2206" target="_blank">00:36:46.260</a></span> | <span class="t">Absolutely not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2207" target="_blank">00:36:47.260</a></span> | <span class="t">However, it's always different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2209" target="_blank">00:36:49.700</a></span> | <span class="t">What that means is that even if you wrap to the next block between the radial and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2215" target="_blank">00:36:55.060</a></span> | <span class="t">document models, you have a unique context vector, even if you're looking at the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2219" target="_blank">00:36:59.860</a></span> | <span class="t">same block, because the document has grown and the radius just says, what are the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2224" target="_blank">00:37:04.660</a></span> | <span class="t">three?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2225" target="_blank">00:37:05.660</a></span> | <span class="t">What are the last 10?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2227" target="_blank">00:37:07.780</a></span> | <span class="t">As a result, when you incorporate different models of context, you don't really have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2232" target="_blank">00:37:12.380</a></span> | <span class="t">say that there's a finite context window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2234" target="_blank">00:37:14.300</a></span> | <span class="t">It might not be very good to make predictions past the first block, but that might be about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2239" target="_blank">00:37:19.260</a></span> | <span class="t">how much data you've used, and it might be about the hyperparameters for each one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2244" target="_blank">00:37:24.340</a></span> | <span class="t">those models that you're applying, in other words, radius, the block size, like usual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2253" target="_blank">00:37:33.220</a></span> | <span class="t">So far, the only embeddings that I've suggested are from this BitCypher algorithm, and as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2258" target="_blank">00:37:38.540</a></span> | <span class="t">I've expressed, they don't capture any useful similarities between similar tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2265" target="_blank">00:37:45.140</a></span> | <span class="t">The BitCypher algorithm, it doesn't care if you're looking at the uppercase or the lowercase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2269" target="_blank">00:37:49.860</a></span> | <span class="t">version of a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2270" target="_blank">00:37:50.860</a></span> | <span class="t">It doesn't see them as bearing any similarity, even though they might be used very similarly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2277" target="_blank">00:37:57.300</a></span> | <span class="t">So how can you utilize the BitCypher to create vectors for tokens that have meaningful similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2287" target="_blank">00:38:07.660</a></span> | <span class="t">between words that are used similarly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2292" target="_blank">00:38:12.060</a></span> | <span class="t">And this is just backing off to the traditional methods once again, taking co-occurrences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2299" target="_blank">00:38:19.460</a></span> | <span class="t">of BitCypher vectors with whatever's there at the middle or center of a co-occurrence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2305" target="_blank">00:38:25.780</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2307" target="_blank">00:38:27.980</a></span> | <span class="t">Normally, if you think about one-hot vectors, a co-occurrence matrix is really just the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2314" target="_blank">00:38:34.380</a></span> | <span class="t">same thing, except now we just have smaller vectors with different dimensions on, so to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2321" target="_blank">00:38:41.420</a></span> | <span class="t">speak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2324" target="_blank">00:38:44.580</a></span> | <span class="t">And we normalize after concatenating these blocks of different radii from the BitCypher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2332" target="_blank">00:38:52.660</a></span> | <span class="t">to match the original input requirements that we discovered for the warm start solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2340" target="_blank">00:39:00.620</a></span> | <span class="t">And that enables us to use these just like we would the original BitCypher vectors, except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2346" target="_blank">00:39:06.900</a></span> | <span class="t">now, just from the usual co-occurrence statistics, you'll see that capital word and lowercase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2354" target="_blank">00:39:14.580</a></span> | <span class="t">word have a lot of common usage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2357" target="_blank">00:39:17.980</a></span> | <span class="t">And you know this works because you've seen co-occurrences for a very long time, and while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2363" target="_blank">00:39:23.740</a></span> | <span class="t">they might not normally be useful in our applications these days with deep learning, they can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2369" target="_blank">00:39:29.740</a></span> | <span class="t">imparted through the BitCypher algorithm to prescribed vectors as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2381" target="_blank">00:39:41.920</a></span> | <span class="t">So here's where things start paying out in terms of speed and efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2391" target="_blank">00:39:51.380</a></span> | <span class="t">If you only have one layer of self-attention, then that means that you don't need to worry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2397" target="_blank">00:39:57.780</a></span> | <span class="t">about whatever weird expressive stuff is happening that, you know, similar inputs might have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2403" target="_blank">00:40:03.540</a></span> | <span class="t">slightly different hidden states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2407" target="_blank">00:40:07.620</a></span> | <span class="t">Since that first layer is just a set of static word embeddings, the self-attention layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2414" target="_blank">00:40:14.580</a></span> | <span class="t">is working off of static word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2418" target="_blank">00:40:18.500</a></span> | <span class="t">And that means each pair of words have a fixed comparison given static word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2426" target="_blank">00:40:26.220</a></span> | <span class="t">And that means if you want to compute the quadratic features of self-attention, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2431" target="_blank">00:40:31.460</a></span> | <span class="t">can just pre-compute them and pull them from memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2436" target="_blank">00:40:36.540</a></span> | <span class="t">This caching of vector comparisons is essentially reducing the self-attention layer's cost from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2443" target="_blank">00:40:43.260</a></span> | <span class="t">quadratic to linear, since those values that we're using to weight the vectors for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2449" target="_blank">00:40:49.980</a></span> | <span class="t">feedforward layer no longer require comparison across the block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2455" target="_blank">00:40:55.660</a></span> | <span class="t">They're already compared.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2458" target="_blank">00:40:58.220</a></span> | <span class="t">So when our vectors are static, which is at inference time, and if we're not learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2465" target="_blank">00:41:05.980</a></span> | <span class="t">the embedding layer's parameters with iterative differential updates, then not only do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2474" target="_blank">00:41:14.060</a></span> | <span class="t">have to not track gradients for the embedding layer, but we don't even have to compute the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2479" target="_blank">00:41:19.140</a></span> | <span class="t">vector comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2480" target="_blank">00:41:20.140</a></span> | <span class="t">We can pre-compute them and just load them, which is much, much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2491" target="_blank">00:41:31.900</a></span> | <span class="t">So we can reduce a lot of, all the inference and training costs, not all the training costs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2498" target="_blank">00:41:38.620</a></span> | <span class="t">some of the training costs, because if we want to update those vectors, then we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2502" target="_blank">00:41:42.340</a></span> | <span class="t">assume cache comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2505" target="_blank">00:41:45.540</a></span> | <span class="t">But it's a huge cost savings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2508" target="_blank">00:41:48.460</a></span> | <span class="t">This means that we can train these self-attentive feedforward unit models very quickly and with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2514" target="_blank">00:41:54.540</a></span> | <span class="t">good initializations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2517" target="_blank">00:41:57.460</a></span> | <span class="t">But there are some other things that we immediately observed while developing these models, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2521" target="_blank">00:42:01.820</a></span> | <span class="t">that is the lack of randomization produced models which were quite effective even on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2529" target="_blank">00:42:09.300</a></span> | <span class="t">small data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2530" target="_blank">00:42:10.300</a></span> | <span class="t">Now, it doesn't mean that training on small data will let you generalize to everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2534" target="_blank">00:42:14.260</a></span> | <span class="t">else that's out there in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2535" target="_blank">00:42:15.500</a></span> | <span class="t">In other words, training on a small data set might produce a model which has a surprisingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2540" target="_blank">00:42:20.100</a></span> | <span class="t">low perplexity on its training set, but it doesn't mean that you're going to be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2544" target="_blank">00:42:24.820</a></span> | <span class="t">to generalize and have a language model that's talking well from just hearing a couple of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2548" target="_blank">00:42:28.020</a></span> | <span class="t">thousand tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2549" target="_blank">00:42:29.940</a></span> | <span class="t">It does mean it will know that couple of thousand tokens very well, very quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2558" target="_blank">00:42:38.700</a></span> | <span class="t">But there's a challenge with using self-attention still, and that is the fact that the block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2565" target="_blank">00:42:45.220</a></span> | <span class="t">model of context often is not fully utilized, since many documents are shorter than long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2575" target="_blank">00:42:55.220</a></span> | <span class="t">context models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2576" target="_blank">00:42:56.220</a></span> | <span class="t">There are long context windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2579" target="_blank">00:42:59.620</a></span> | <span class="t">And these days, there are exceptionally long context windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2583" target="_blank">00:43:03.140</a></span> | <span class="t">I'm not even talking about those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2585" target="_blank">00:43:05.500</a></span> | <span class="t">Many of the language modeling benchmarks simply don't even go out to a thousand words when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2589" target="_blank">00:43:09.060</a></span> | <span class="t">it comes to context, and you're looking at a document to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2595" target="_blank">00:43:15.020</a></span> | <span class="t">So this has been a problem for a while, and it means that if you're going to pad your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2602" target="_blank">00:43:22.680</a></span> | <span class="t">short documents, you're going to waste a lot of prediction on those paddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2607" target="_blank">00:43:27.740</a></span> | <span class="t">A lot of computation gets lost just for null information, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2615" target="_blank">00:43:35.300</a></span> | <span class="t">And the way that this is often relieved in some groups, and to great effect, is by packing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2622" target="_blank">00:43:42.740</a></span> | <span class="t">long contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2624" target="_blank">00:43:44.740</a></span> | <span class="t">So for example, if you've got a hundred thousand token context window, most documents will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2629" target="_blank">00:43:49.220</a></span> | <span class="t">not be a hundred thousand tokens long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2631" target="_blank">00:43:51.660</a></span> | <span class="t">What do you do with the rest of that long context if you want to use a thousand tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2635" target="_blank">00:43:55.740</a></span> | <span class="t">of good training data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2638" target="_blank">00:43:58.020</a></span> | <span class="t">You fill out the other ninety-nine thousand tokens with a bunch of other random documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2642" target="_blank">00:44:02.100</a></span> | <span class="t">that don't belong anywhere near the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2644" target="_blank">00:44:04.400</a></span> | <span class="t">That's called packing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2647" target="_blank">00:44:07.640</a></span> | <span class="t">Packing can be utilized without impacting different documents with each other, without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2654" target="_blank">00:44:14.880</a></span> | <span class="t">contaminating the information between documents, and that takes a lot of work, but it can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2659" target="_blank">00:44:19.640</a></span> | <span class="t">done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2662" target="_blank">00:44:22.160</a></span> | <span class="t">However, there are different strategies that we could employ, different engineering tricks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2668" target="_blank">00:44:28.760</a></span> | <span class="t">that we could employ, to make our operation of self-attention more effective at any length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2676" target="_blank">00:44:36.760</a></span> | <span class="t">of document without having to deal with this packing problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2681" target="_blank">00:44:41.040</a></span> | <span class="t">And that comes about by dynamically changing the context length from some maximum value,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2689" target="_blank">00:44:49.640</a></span> | <span class="t">that's what you would normally set, just use the context that you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2695" target="_blank">00:44:55.280</a></span> | <span class="t">But you still have to create batches if you want to train models quickly, and what that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2698" target="_blank">00:44:58.840</a></span> | <span class="t">means is that there's still some padding if you use this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2703" target="_blank">00:45:03.040</a></span> | <span class="t">But you can pad those short documents to set lengths, batch short documents together, batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2713" target="_blank">00:45:13.680</a></span> | <span class="t">long documents together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2717" target="_blank">00:45:17.200</a></span> | <span class="t">This means that we don't need to pack documents together to make use of a long context window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2726" target="_blank">00:45:26.440</a></span> | <span class="t">When a document is long, you can let its context be long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2728" target="_blank">00:45:28.960</a></span> | <span class="t">When a document is short, you can put it with other short documents and just use a subset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2732" target="_blank">00:45:32.880</a></span> | <span class="t">of those self-attention parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2736" target="_blank">00:45:36.560</a></span> | <span class="t">And with traditional self-attention parameters, keys and queries, it would never be a subset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2740" target="_blank">00:45:40.480</a></span> | <span class="t">because it's a low dimensionalization that that matrix provides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2744" target="_blank">00:45:44.800</a></span> | <span class="t">With this modified self-attention, though, there's a different shape to the weight matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2749" target="_blank">00:45:49.120</a></span> | <span class="t">and that's why it's a subset of those parameters that we have to utilize, and that might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2753" target="_blank">00:45:53.600</a></span> | <span class="t">something worth discussing afterwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2755" target="_blank">00:45:55.320</a></span> | <span class="t">In other words, how does the difference in shapes of dimensionalities between this and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2760" target="_blank">00:46:00.360</a></span> | <span class="t">the standard self-attention weights shake out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2768" target="_blank">00:46:08.000</a></span> | <span class="t">But we want to get to a different point for the sake of this conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2775" target="_blank">00:46:15.320</a></span> | <span class="t">What is a model like this useful for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2777" target="_blank">00:46:17.720</a></span> | <span class="t">That should be a question that you're asking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2779" target="_blank">00:46:19.400</a></span> | <span class="t">It's a question that we've been asking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2786" target="_blank">00:46:26.040</a></span> | <span class="t">We're not entirely certain yet how an extremely large model like this will function on trillions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2794" target="_blank">00:46:34.920</a></span> | <span class="t">of tokens, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2795" target="_blank">00:46:35.920</a></span> | <span class="t">In other words, can you expect the same kinds of outcomes, like a chat GPT kind of thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2804" target="_blank">00:46:44.120</a></span> | <span class="t">from some of these models, human interaction and RLHF and all the rest of that, though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2810" target="_blank">00:46:50.720</a></span> | <span class="t">it's something that we're considering, but also at different scales, too, since those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2817" target="_blank">00:46:57.760</a></span> | <span class="t">are performant on their own as well, but for what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2825" target="_blank">00:47:05.760</a></span> | <span class="t">So the point is, is that from what we've stress tested into the billions, models can be trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2833" target="_blank">00:47:13.120</a></span> | <span class="t">very quickly on a relatively small GPU, in ways that we expect when we cache vector comparisons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2840" target="_blank">00:47:20.240</a></span> | <span class="t">we see really big speedups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2842" target="_blank">00:47:22.160</a></span> | <span class="t">When we don't cache those comparisons, you see all of the growth in computation time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2849" target="_blank">00:47:29.280</a></span> | <span class="t">that you would expect from longer context windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2855" target="_blank">00:47:35.820</a></span> | <span class="t">This one here, though, we're trying to make it really, really, really small, the one called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2860" target="_blank">00:47:40.240</a></span> | <span class="t">potato.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2862" target="_blank">00:47:42.060</a></span> | <span class="t">That's because we want to see if we can train a model from scratch, since on very little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2867" target="_blank">00:47:47.280</a></span> | <span class="t">data, these models can fit effectively with the initializations that we've developed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2875" target="_blank">00:47:55.880</a></span> | <span class="t">And with the purpose of starting from scratch, starting with no data, we're thinking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2881" target="_blank">00:48:01.040</a></span> | <span class="t">edge computing cases where we could deploy a language model with a microphone so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2885" target="_blank">00:48:05.880</a></span> | <span class="t">a person can talk to it and just train it from their own data, train it from their own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2890" target="_blank">00:48:10.720</a></span> | <span class="t">speech, to understand their speech.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2899" target="_blank">00:48:19.480</a></span> | <span class="t">So between these, we've explored a lot of different configurations, trying to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2905" target="_blank">00:48:25.040</a></span> | <span class="t">similarities to what some standard configurations might look like, a couple thousand tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2909" target="_blank">00:48:29.240</a></span> | <span class="t">in a context window, for example, to look something like a GPT-2 style model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2914" target="_blank">00:48:34.660</a></span> | <span class="t">Thinking about bit cipher embeddings that are 500 dimensional or 1,000 dimensional to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2919" target="_blank">00:48:39.560</a></span> | <span class="t">be something like a GPT-2, that's, again, pointing towards the big/large category of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2926" target="_blank">00:48:46.200</a></span> | <span class="t">models that we've experimented with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2929" target="_blank">00:48:49.400</a></span> | <span class="t">Beyond that, we haven't really touched those scales, because our first objective is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2935" target="_blank">00:48:55.440</a></span> | <span class="t">to make big, big language models and train chatbots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2939" target="_blank">00:48:59.020</a></span> | <span class="t">We want to know, what can we do with a small model, since this is a relatively unique capability?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2949" target="_blank">00:49:09.740</a></span> | <span class="t">So what does training look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2952" target="_blank">00:49:12.140</a></span> | <span class="t">To the best of our ability so far, it's kind of hard to see, but the first step is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2960" target="_blank">00:49:20.220</a></span> | <span class="t">warm start, where you train the bit cipher, and you take a couple of splits of data, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2967" target="_blank">00:49:27.940</a></span> | <span class="t">you compute that warm start for the self-attention layer and the feedforward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2974" target="_blank">00:49:34.140</a></span> | <span class="t">In this case, which is really just using a 100 million token data set from the baby language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2980" target="_blank">00:49:40.180</a></span> | <span class="t">model challenge, which has as an objective to see what language models can do on a relatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2989" target="_blank">00:49:49.800</a></span> | <span class="t">human scale of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2991" target="_blank">00:49:51.220</a></span> | <span class="t">In other words, 100 million tokens is something that a person might hear in 10 years of their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2996" target="_blank">00:49:56.380</a></span> | <span class="t">life.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=2998" target="_blank">00:49:58.100</a></span> | <span class="t">In 10 years of life, people become pretty proficient speakers, and can a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3004" target="_blank">00:50:04.100</a></span> | <span class="t">be trained at that scale?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3007" target="_blank">00:50:07.900</a></span> | <span class="t">The second stage, after the warm start happens, is where the majority of training time occurs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3014" target="_blank">00:50:14.620</a></span> | <span class="t">and yet is also where training operates the most quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3022" target="_blank">00:50:22.260</a></span> | <span class="t">At this stage, we find that freezing vectors is important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3026" target="_blank">00:50:26.060</a></span> | <span class="t">One, because it means that we can train much quicker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3029" target="_blank">00:50:29.120</a></span> | <span class="t">So we can have the subsequent layers optimized beyond their warm starts very, very fast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3035" target="_blank">00:50:35.620</a></span> | <span class="t">using that vector caching, the vector comparison caching, to avoid the quadratic costs of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3043" target="_blank">00:50:43.260</a></span> | <span class="t">This articulates the parameters in the middle layers of the model for taking 100 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3050" target="_blank">00:50:50.180</a></span> | <span class="t">tokens and making five passes over the data here a lot quicker than any of the other stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3057" target="_blank">00:50:57.980</a></span> | <span class="t">The comparison that you'd make to this is the training time once those embedding layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3063" target="_blank">00:51:03.460</a></span> | <span class="t">are unfrozen, where everything slows down to the normal speeds, where you have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3068" target="_blank">00:51:08.580</a></span> | <span class="t">all of your vector comparisons on the fly, since you can't assume that the same comparisons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3074" target="_blank">00:51:14.060</a></span> | <span class="t">will always result in the same numbers, since model parameters might be updated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3082" target="_blank">00:51:22.380</a></span> | <span class="t">This is the best procedure that we've figured out so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3085" target="_blank">00:51:25.160</a></span> | <span class="t">And in order to make those vectors update, we find that learning rates have to be adjusted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3090" target="_blank">00:51:30.160</a></span> | <span class="t">dynamically inside of the network, like normal, and that the embedding layers are really tough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3096" target="_blank">00:51:36.140</a></span> | <span class="t">to make progress on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3100" target="_blank">00:51:40.380</a></span> | <span class="t">And you'll notice here in this picture that the slowness and the lack of stability, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3105" target="_blank">00:51:45.660</a></span> | <span class="t">example, in learning the embedding layer once it had been prescribed earlier, makes it really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3111" target="_blank">00:51:51.220</a></span> | <span class="t">hard to train over the entire data set compared to five passes, for example, in the middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3117" target="_blank">00:51:57.460</a></span> | <span class="t">phase when the middle and upper parameters are being updated, still with backpropagation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3123" target="_blank">00:52:03.940</a></span> | <span class="t">And the other thing that I would highlight before leaving this slide is, in phase one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3131" target="_blank">00:52:11.060</a></span> | <span class="t">how the warm start saturates pretty quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3135" target="_blank">00:52:15.540</a></span> | <span class="t">So if you have 100 million tokens, you really only need to apply the warm start to something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3139" target="_blank">00:52:19.820</a></span> | <span class="t">like maybe 10 million tokens, not that much more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3143" target="_blank">00:52:23.060</a></span> | <span class="t">You don't see that much gain from that much more data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3147" target="_blank">00:52:27.820</a></span> | <span class="t">That's not a bad thing, because it means that we don't have to apply that process for any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3152" target="_blank">00:52:32.660</a></span> | <span class="t">longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3154" target="_blank">00:52:34.500</a></span> | <span class="t">It would be great if it gave us all of the optimization that we could hope for, but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3158" target="_blank">00:52:38.820</a></span> | <span class="t">not something that we could necessarily expect, since it's just an approximation of where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3163" target="_blank">00:52:43.340</a></span> | <span class="t">the parameters are headed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3168" target="_blank">00:52:48.960</a></span> | <span class="t">So on the back of an envelope, thinking about how the systems that example was trained on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3174" target="_blank">00:52:54.860</a></span> | <span class="t">as compared to other examples that are out there, and thinking about models that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3180" target="_blank">00:53:00.900</a></span> | <span class="t">kind of sort of similar size, we're talking about a 12 gigabyte GPU, a relatively small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3188" target="_blank">00:53:08.180</a></span> | <span class="t">single chip, specifically when referring to these training times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3194" target="_blank">00:53:14.740</a></span> | <span class="t">So that's a 12 gigabyte GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3200" target="_blank">00:53:20.020</a></span> | <span class="t">Just working off of eight chips, each having roughly four times the scale, and comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3206" target="_blank">00:53:26.060</a></span> | <span class="t">to this time that it took to train something with maybe an additional order of magnitude,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3211" target="_blank">00:53:31.820</a></span> | <span class="t">although we have trained models up to around 50 million parameters, too, which is getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3215" target="_blank">00:53:35.940</a></span> | <span class="t">towards GPT-2 scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3219" target="_blank">00:53:39.820</a></span> | <span class="t">We see training times that, if we scaled up to the relatively large systems that present</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3225" target="_blank">00:53:45.300</a></span> | <span class="t">us with how much work we should expect to have to do for a model that large, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3230" target="_blank">00:53:50.220</a></span> | <span class="t">expect to be able to train much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3233" target="_blank">00:53:53.500</a></span> | <span class="t">But as mentioned, the initial objective here is not to simply figure out how well we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3239" target="_blank">00:53:59.500</a></span> | <span class="t">do something that's being done well already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3242" target="_blank">00:54:02.180</a></span> | <span class="t">It's to figure out what these alternative strategies are useful for, since they give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3246" target="_blank">00:54:06.900</a></span> | <span class="t">us access to different regimes of model scale as effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3255" target="_blank">00:54:15.860</a></span> | <span class="t">So as mentioned, we've gone to relatively large amounts of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3262" target="_blank">00:54:22.380</a></span> | <span class="t">I wouldn't really call them big data at this time, even though just a couple of years ago</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3266" target="_blank">00:54:26.500</a></span> | <span class="t">a billion tokens would be a relatively large amount of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3270" target="_blank">00:54:30.860</a></span> | <span class="t">It's really just a stress test at this point, gives us something like, do we continue to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3275" target="_blank">00:54:35.860</a></span> | <span class="t">see models getting better as we continue to give them more data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3279" target="_blank">00:54:39.460</a></span> | <span class="t">Do we continue to see models getting better as we continue to give them longer context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3283" target="_blank">00:54:43.780</a></span> | <span class="t">windows?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3284" target="_blank">00:54:44.780</a></span> | <span class="t">And the answer to both of those questions is absolutely yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3287" target="_blank">00:54:47.540</a></span> | <span class="t">So nothing is telling us that we can't train bigger models with these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3291" target="_blank">00:54:51.540</a></span> | <span class="t">But will those bigger models be as good as a standard self-attention model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3295" target="_blank">00:54:55.300</a></span> | <span class="t">I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3296" target="_blank">00:54:56.300</a></span> | <span class="t">It's a different self-attention parameter matrix than what you see in a standard self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3300" target="_blank">00:55:00.100</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3301" target="_blank">00:55:01.420</a></span> | <span class="t">You could integrate the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3303" target="_blank">00:55:03.300</a></span> | <span class="t">And in theory, that should be overkill, because you'd have more parameters and more power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3308" target="_blank">00:55:08.700</a></span> | <span class="t">through them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3310" target="_blank">00:55:10.220</a></span> | <span class="t">And we can see from this work that the alternative self-attention parameters are reasonably effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3318" target="_blank">00:55:18.460</a></span> | <span class="t">We're getting close to time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3321" target="_blank">00:55:21.180</a></span> | <span class="t">So I'll go quick through these, since this is the work that we're approaching right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3327" target="_blank">00:55:27.740</a></span> | <span class="t">And this is the idea that we're seeing as a use case for such a model like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3334" target="_blank">00:55:34.860</a></span> | <span class="t">In other words, no pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3338" target="_blank">00:55:38.540</a></span> | <span class="t">Just training on the target data, whatever the data of interaction are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3343" target="_blank">00:55:43.420</a></span> | <span class="t">And in this example, you'll see that this relatively smaller precision language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3347" target="_blank">00:55:47.900</a></span> | <span class="t">just needs to predict whether or not a light should go on or off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3351" target="_blank">00:55:51.860</a></span> | <span class="t">A lamp that listens with a microphone and a switch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3356" target="_blank">00:55:56.000</a></span> | <span class="t">And you can use that switch to train the lamp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3362" target="_blank">00:56:02.460</a></span> | <span class="t">So that's the goal here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3364" target="_blank">00:56:04.800</a></span> | <span class="t">Can pre-training be eliminated?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3367" target="_blank">00:56:07.180</a></span> | <span class="t">And we want to anticipate whether or not you're going to flip the light on or off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3372" target="_blank">00:56:12.520</a></span> | <span class="t">That's the task that we're going to try and approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3376" target="_blank">00:56:16.420</a></span> | <span class="t">Or that, rather, we're currently approaching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3379" target="_blank">00:56:19.340</a></span> | <span class="t">There's a few different processes that integrate into this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3383" target="_blank">00:56:23.820</a></span> | <span class="t">There has to be a microphone that's listening to you, recording audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3388" target="_blank">00:56:28.020</a></span> | <span class="t">There has to be a transcription algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3389" target="_blank">00:56:29.700</a></span> | <span class="t">And we use Wave2Vec at this point, because there's a very small version of it that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3393" target="_blank">00:56:33.340</a></span> | <span class="t">character-based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3394" target="_blank">00:56:34.340</a></span> | <span class="t">And as a result, it doesn't even require you to use consistent-- or it does require to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3398" target="_blank">00:56:38.540</a></span> | <span class="t">use consistent language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3400" target="_blank">00:56:40.260</a></span> | <span class="t">But it doesn't even require you to use words, since it's strictly phonetic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3405" target="_blank">00:56:45.740</a></span> | <span class="t">There has to be-- and this is the bread and butter of what's going on here-- a process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3411" target="_blank">00:56:51.300</a></span> | <span class="t">which anticipates what you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3414" target="_blank">00:56:54.620</a></span> | <span class="t">And that process is responsible for creating good training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3418" target="_blank">00:56:58.100</a></span> | <span class="t">So this is a smart data collection algorithm that figures out, when you flip the switch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3424" target="_blank">00:57:04.820</a></span> | <span class="t">is that the target for something that you just said?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3428" target="_blank">00:57:08.460</a></span> | <span class="t">Is that the transfer learning objectives from text that was transcribed, that it anticipates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3437" target="_blank">00:57:17.700</a></span> | <span class="t">you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3440" target="_blank">00:57:20.340</a></span> | <span class="t">Following this, there's also two other processes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3443" target="_blank">00:57:23.060</a></span> | <span class="t">One which operates on a different time cycle, and that's training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3448" target="_blank">00:57:28.180</a></span> | <span class="t">So always train a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3449" target="_blank">00:57:29.940</a></span> | <span class="t">Always be training a model, whenever there's new data, is essentially what that fourth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3455" target="_blank">00:57:35.900</a></span> | <span class="t">process says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3457" target="_blank">00:57:37.620</a></span> | <span class="t">And the last one is operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3459" target="_blank">00:57:39.300</a></span> | <span class="t">In other words, if you flip the switch, there has to be a process which operates the light</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3463" target="_blank">00:57:43.100</a></span> | <span class="t">bulb.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3464" target="_blank">00:57:44.100</a></span> | <span class="t">It always has to be a lamp in order to be useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3465" target="_blank">00:57:45.940</a></span> | <span class="t">It always has to be able to just be a switch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3471" target="_blank">00:57:51.380</a></span> | <span class="t">However, that operation process likewise needs to see a directive from the anticipator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3477" target="_blank">00:57:57.780</a></span> | <span class="t">If the language model predicts that you just said a thing, that means you want there to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3481" target="_blank">00:58:01.900</a></span> | <span class="t">be light, that operator then needs to receive the signal from the anticipator and execute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3490" target="_blank">00:58:10.180</a></span> | <span class="t">the directive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3491" target="_blank">00:58:11.660</a></span> | <span class="t">If the user then, though, within some time scale, changes the switch back after the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3498" target="_blank">00:58:18.980</a></span> | <span class="t">created a prediction that was bad, the operator is also responsible for issuing a correction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3504" target="_blank">00:58:24.920</a></span> | <span class="t">to the anticipator to correct the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3510" target="_blank">00:58:30.220</a></span> | <span class="t">What this looks like as a process is in this diagram here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3514" target="_blank">00:58:34.900</a></span> | <span class="t">And you can see the flow here from stage one, a verbal command maybe gets recorded, transcribed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3523" target="_blank">00:58:43.740</a></span> | <span class="t">turned into text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3525" target="_blank">00:58:45.240</a></span> | <span class="t">And if there's no model that's yet trained, that text is just stored as data along with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3530" target="_blank">00:58:50.060</a></span> | <span class="t">any directives given by the user in the form of a light switch going on or off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3537" target="_blank">00:58:57.180</a></span> | <span class="t">Once there's any data, the learning process says, okay, time to train a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3543" target="_blank">00:59:03.940</a></span> | <span class="t">and integrate it with these targets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3547" target="_blank">00:59:07.180</a></span> | <span class="t">Once a model is done training, it's sent over to the anticipator who is responsible for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3552" target="_blank">00:59:12.940</a></span> | <span class="t">using the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3555" target="_blank">00:59:15.620</a></span> | <span class="t">That small language model then is now empowered to make predictions every single time it receives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3562" target="_blank">00:59:22.700</a></span> | <span class="t">a text command.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3565" target="_blank">00:59:25.380</a></span> | <span class="t">And those predictions are sent to the operator, which then does whatever it's told.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3571" target="_blank">00:59:31.580</a></span> | <span class="t">And the last thing that can happen, step six, is if the wrong prediction was made and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3576" target="_blank">00:59:36.360</a></span> | <span class="t">user fixes it by turning off the light because they didn't want the light on, that corrects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3583" target="_blank">00:59:43.460</a></span> | <span class="t">the data that was transcribed and the next model which is trained will be able to avoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3587" target="_blank">00:59:47.000</a></span> | <span class="t">that problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3588" target="_blank">00:59:48.820</a></span> | <span class="t">And there's some dialing this in in terms of the time scales that you want based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3594" target="_blank">00:59:54.020</a></span> | <span class="t">the way humans interact with the light switch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3595" target="_blank">00:59:55.740</a></span> | <span class="t">So there's a lot of development that goes into figuring out the right way to set this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3599" target="_blank">00:59:59.580</a></span> | <span class="t">up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3601" target="_blank">01:00:01.340</a></span> | <span class="t">The data that you collect from a process like this, how do we organize it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3606" target="_blank">01:00:06.940</a></span> | <span class="t">This actually is not transfer learning, so I kind of lied there a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3611" target="_blank">01:00:11.040</a></span> | <span class="t">This is strictly language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3614" target="_blank">01:00:14.140</a></span> | <span class="t">It's a conversation between the human and the lamp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3617" target="_blank">01:00:17.100</a></span> | <span class="t">You say something, the lamp says, here's what you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3623" target="_blank">01:00:23.060</a></span> | <span class="t">And it's just an extending context window, like you'd see with a decoder-only kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3629" target="_blank">01:00:29.100</a></span> | <span class="t">architecture these days, a chatbot kind of thing, a human personal assistant, human assistant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3636" target="_blank">01:00:36.420</a></span> | <span class="t">dialogue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3639" target="_blank">01:00:39.300</a></span> | <span class="t">And you might also suspect then that, well, couldn't you let the lamp talk?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3643" target="_blank">01:00:43.700</a></span> | <span class="t">Yes, you could absolutely let it use other tokens, and that is something which is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3647" target="_blank">01:00:47.540</a></span> | <span class="t">the horizon for us, in other words, how to determine once the model is learned enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3652" target="_blank">01:00:52.900</a></span> | <span class="t">and knows when you want to hear it talk and knows what you want to hear it say, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3658" target="_blank">01:00:58.980</a></span> | <span class="t">requires other smart data collection currently in development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3664" target="_blank">01:01:04.780</a></span> | <span class="t">And there's three tags here if you don't see it, although what they really are are tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3668" target="_blank">01:01:08.900</a></span> | <span class="t">since they're integrated within the language model's vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3673" target="_blank">01:01:13.540</a></span> | <span class="t">I want the lamp lit, I want the lamp dark, or nothing, if no switch is applied during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3679" target="_blank">01:01:19.220</a></span> | <span class="t">transcription.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3683" target="_blank">01:01:23.700</a></span> | <span class="t">So what do the models look like that go into a lamp?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3687" target="_blank">01:01:27.580</a></span> | <span class="t">They're a little bit smaller than that micro model in terms of having a long context window,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3692" target="_blank">01:01:32.860</a></span> | <span class="t">B, the block size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3694" target="_blank">01:01:34.780</a></span> | <span class="t">They still use these other features, like a radius, which help them to do well with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3700" target="_blank">01:01:40.940</a></span> | <span class="t">only little data, those other context models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3706" target="_blank">01:01:46.020</a></span> | <span class="t">And the embedding size is around 50 or 100 and something, and this is small enough to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3713" target="_blank">01:01:53.380</a></span> | <span class="t">fit on a microprocessor, on a CPU of a microprocessor, including training, no GPU whatsoever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3724" target="_blank">01:02:04.220</a></span> | <span class="t">And the first time we ever got the interaction right, the right timescales, from no data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3729" target="_blank">01:02:09.860</a></span> | <span class="t">whatsoever, creating this data, and 20 minutes of it, was enough, and you can see there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3738" target="_blank">01:02:18.340</a></span> | <span class="t">loads of misspellings here because the transcription is not required to produce known words, known</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3743" target="_blank">01:02:23.500</a></span> | <span class="t">tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3745" target="_blank">01:02:25.080</a></span> | <span class="t">It's strictly character-based, so you can say whatever you want to say, you can whistle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3748" target="_blank">01:02:28.820</a></span> | <span class="t">and as long as wav2vec thinks that's tokens, it'll figure out what to transcribe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3759" target="_blank">01:02:39.820</a></span> | <span class="t">That's enough, 20 minutes of talking to it, to have it know pretty well when you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3765" target="_blank">01:02:45.420</a></span> | <span class="t">the light on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3767" target="_blank">01:02:47.740</a></span> | <span class="t">This is what the numbers look like for that prediction, and you see lots of zeros there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3771" target="_blank">01:02:51.100</a></span> | <span class="t">That's because there's no positive instances yet in the data, until you flip the switch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3776" target="_blank">01:02:56.780</a></span> | <span class="t">there's nothing to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3779" target="_blank">01:02:59.620</a></span> | <span class="t">Once there is enough to predict, we see an immediate jump in the model's ability to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3785" target="_blank">01:03:05.000</a></span> | <span class="t">out whether LAMP should say on, off, or nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3791" target="_blank">01:03:11.540</a></span> | <span class="t">And while we trained this first model, for example, in 20 minutes on LePotato, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3799" target="_blank">01:03:19.940</a></span> | <span class="t">is a really, really, really small microprocessor, it's incredibly frustrating to utilize because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3806" target="_blank">01:03:26.620</a></span> | <span class="t">the processing time is a couple seconds, and it feels like it's going somewhere, even though</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3811" target="_blank">01:03:31.420</a></span> | <span class="t">the data is entirely localized, there's no Wi-Fi, there's no internet connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3816" target="_blank">01:03:36.140</a></span> | <span class="t">It just takes the model on this tiny chip a minute, not really a minute, like a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3820" target="_blank">01:03:40.900</a></span> | <span class="t">seconds, to flip the switch on, because it has to transcribe it, interpret it, issue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3826" target="_blank">01:03:46.100</a></span> | <span class="t">the directive, ask the operator to operate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3829" target="_blank">01:03:49.060</a></span> | <span class="t">And so part of what we're doing is figuring out at what scale of microprocessing do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3833" target="_blank">01:03:53.220</a></span> | <span class="t">models that we're developing really make a good real-time system that a user can make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3837" target="_blank">01:03:57.980</a></span> | <span class="t">use of well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3840" target="_blank">01:04:00.560</a></span> | <span class="t">And as you can see, the larger the model in terms of hyperparameters and so forth, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3847" target="_blank">01:04:07.020</a></span> | <span class="t">more performant it gets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3851" target="_blank">01:04:11.800</a></span> | <span class="t">So we see these as potentially useful in edge scenarios, but not just for operation, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3857" target="_blank">01:04:17.940</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3859" target="_blank">01:04:19.060</a></span> | <span class="t">So go to Home Depot, buy a light switch installed in your house, start talking to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3868" target="_blank">01:04:28.040</a></span> | <span class="t">But this isn't really the stopping point that we want to get to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3874" target="_blank">01:04:34.820</a></span> | <span class="t">We want to eventually get to the point of talkback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3876" target="_blank">01:04:36.900</a></span> | <span class="t">We want to treat these as language models that essentially have a bit of you inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3880" target="_blank">01:04:40.580</a></span> | <span class="t">of them that you can converse with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3884" target="_blank">01:04:44.380</a></span> | <span class="t">And that's important to know when the model is aware of what you want to hear said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3890" target="_blank">01:04:50.900</a></span> | <span class="t">In other words, it needs to know what is a good thing to say back to what you just said.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3895" target="_blank">01:04:55.700</a></span> | <span class="t">And the lamp has never heard a lamp talk before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3898" target="_blank">01:04:58.860</a></span> | <span class="t">So there are challenges to figuring out the lamp's role in conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3904" target="_blank">01:05:04.940</a></span> | <span class="t">And choosing a lamp, though, is arbitrary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3908" target="_blank">01:05:08.660</a></span> | <span class="t">We don't have to make it be a light bulb which goes on and off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3910" target="_blank">01:05:10.900</a></span> | <span class="t">This could be a controller for anything which is a binary switch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3915" target="_blank">01:05:15.660</a></span> | <span class="t">And you could imagine, like others are looking at right now, there's a lot of opportunities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3920" target="_blank">01:05:20.820</a></span> | <span class="t">with predicting the action on your phone that you want to take, which thing you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3924" target="_blank">01:05:24.940</a></span> | <span class="t">push.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3926" target="_blank">01:05:26.860</a></span> | <span class="t">And with a system like this, microsizing on to your cell phone, for example, assumes better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3933" target="_blank">01:05:33.220</a></span> | <span class="t">hardware than what we're already using, but would be entirely localized, including training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3944" target="_blank">01:05:44.140</a></span> | <span class="t">But this is also really just getting to the point of feasibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3949" target="_blank">01:05:49.580</a></span> | <span class="t">It's not getting to the point of a well-optimized system, which we're still developing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3954" target="_blank">01:05:54.460</a></span> | <span class="t">There are, in principle, different modifications that we could make to the self-attention layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3959" target="_blank">01:05:59.100</a></span> | <span class="t">which include traditional self-attention parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3962" target="_blank">01:06:02.220</a></span> | <span class="t">That's just one example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3964" target="_blank">01:06:04.820</a></span> | <span class="t">Then there are updates to the very naive scheme that we have for BitCipher, the vectors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3970" target="_blank">01:06:10.020</a></span> | <span class="t">we're using to initialize our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3973" target="_blank">01:06:13.260</a></span> | <span class="t">And a lot of other minutia that need to be approached.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3979" target="_blank">01:06:19.500</a></span> | <span class="t">So this isn't really work that's done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3981" target="_blank">01:06:21.740</a></span> | <span class="t">It's a work in progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3983" target="_blank">01:06:23.300</a></span> | <span class="t">And in addition to what I just described, we're moving towards larger models and evaluations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3989" target="_blank">01:06:29.600</a></span> | <span class="t">that compare better to modern systems, which will eventually come online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=3994" target="_blank">01:06:34.980</a></span> | <span class="t">We'll most likely participate in this year's baby language model challenge, although that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4000" target="_blank">01:06:40.780</a></span> | <span class="t">challenge assumes you're working with a standard architecture, which is already developed for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4005" target="_blank">01:06:45.660</a></span> | <span class="t">all of the evaluative needs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4008" target="_blank">01:06:48.140</a></span> | <span class="t">So there's a lot of work to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4011" target="_blank">01:06:51.020</a></span> | <span class="t">But that's really all I have prepared for you to discuss today in this conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4014" target="_blank">01:06:54.300</a></span> | <span class="t">I've gone over a lot of details, and if you'd like to talk about any of these, I'm certainly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4018" target="_blank">01:06:58.820</a></span> | <span class="t">happy to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4020" target="_blank">01:07:00.560</a></span> | <span class="t">Questions that you might have as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4021" target="_blank">01:07:01.900</a></span> | <span class="t">And if you have access to the slides, there's some links to the different papers I've referenced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4029" target="_blank">01:07:09.580</a></span> | <span class="t">That's all for today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4030" target="_blank">01:07:10.580</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4031" target="_blank">01:07:11.580</a></span> | <span class="t">[APPLAUSE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4032" target="_blank">01:07:12.580</a></span> | <span class="t">Hey, so thanks, Jake, for the great talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4039" target="_blank">01:07:19.380</a></span> | <span class="t">And now we'll have some time for questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4041" target="_blank">01:07:21.660</a></span> | <span class="t">So if anybody here has any questions, feel free to raise your hand and ask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4045" target="_blank">01:07:25.420</a></span> | <span class="t">Otherwise, we'll go to some questions on Slido.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4056" target="_blank">01:07:36.180</a></span> | <span class="t">Some folks are asking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4057" target="_blank">01:07:37.180</a></span> | <span class="t">So we'll be posting the slides later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4060" target="_blank">01:07:40.100</a></span> | <span class="t">But I've also pasted these references in the Zoom chat, as well as Discord, in case anybody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4066" target="_blank">01:07:46.420</a></span> | <span class="t">wants to see them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4068" target="_blank">01:07:48.340</a></span> | <span class="t">I was wondering, in the plots that you showed for warm start versus cold start, does the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4079" target="_blank">01:07:59.100</a></span> | <span class="t">cold start use the modified self-attention or the standard self-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4085" target="_blank">01:08:05.780</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4087" target="_blank">01:08:07.180</a></span> | <span class="t">So the question was, in this picture, comparing warm starts to cold starts, what self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4094" target="_blank">01:08:14.340</a></span> | <span class="t">was used here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4095" target="_blank">01:08:15.340</a></span> | <span class="t">None.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4096" target="_blank">01:08:16.340</a></span> | <span class="t">This is strictly a feed-forward experiment, where we take a single layer, and all we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4100" target="_blank">01:08:20.660</a></span> | <span class="t">is feed forward with one-hot vectors from some context window and concatenate them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4108" target="_blank">01:08:28.420</a></span> | <span class="t">And the general property that you'll see is, by concatenating vectors, there's very little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4113" target="_blank">01:08:33.980</a></span> | <span class="t">for attention to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4116" target="_blank">01:08:36.460</a></span> | <span class="t">Simply with a block, you're adding the vectors together, and that superposition of the dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4121" target="_blank">01:08:41.860</a></span> | <span class="t">smears them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4123" target="_blank">01:08:43.500</a></span> | <span class="t">And that's why self-attention is needed, in order to weight that superposition so just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4127" target="_blank">01:08:47.940</a></span> | <span class="t">the right ones stick out and it's not muddled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4131" target="_blank">01:08:51.320</a></span> | <span class="t">If those vectors are instead concatenated, a weighting of those is really just appealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4136" target="_blank">01:08:56.060</a></span> | <span class="t">to the sensibilities of the matrix above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4141" target="_blank">01:09:01.020</a></span> | <span class="t">When they're superimposed, there's a lot to work on, since you're smearing separate information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4146" target="_blank">01:09:06.260</a></span> | <span class="t">together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4147" target="_blank">01:09:07.500</a></span> | <span class="t">When the information is already separated, there's not that much re-weighting can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4154" target="_blank">01:09:14.020</a></span> | <span class="t">And in this case, there's absolutely no re-weighting going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4158" target="_blank">01:09:18.180</a></span> | <span class="t">And what I've described to you is really just something that's become very clear from a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4164" target="_blank">01:09:24.060</a></span> | <span class="t">lot of small-scale experiments in between the models that we've developed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4169" target="_blank">01:09:29.420</a></span> | <span class="t">And moving towards self-attention took additional time, and we didn't have a solution for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4175" target="_blank">01:09:35.380</a></span> | <span class="t">layer yet when this work was done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4178" target="_blank">01:09:38.180</a></span> | <span class="t">I had a question in regards to-- so you're doing this with on-edge controllers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4187" target="_blank">01:09:47.500</a></span> | <span class="t">What?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4188" target="_blank">01:09:48.500</a></span> | <span class="t">You're doing this with on-edge controllers, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4189" target="_blank">01:09:49.500</a></span> | <span class="t">You're doing training for on-edge controllers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4190" target="_blank">01:09:50.500</a></span> | <span class="t">So this could be for IoT devices, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4191" target="_blank">01:09:51.500</a></span> | <span class="t">Could be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4192" target="_blank">01:09:52.500</a></span> | <span class="t">And you talked about how this also could work for image data, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4193" target="_blank">01:09:53.500</a></span> | <span class="t">Oh, I saw that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4194" target="_blank">01:09:54.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4195" target="_blank">01:09:55.500</a></span> | <span class="t">Have you conducted any tests with image data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4196" target="_blank">01:09:56.500</a></span> | <span class="t">Like, with these small-scale models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4197" target="_blank">01:09:57.500</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4198" target="_blank">01:09:58.500</a></span> | <span class="t">So image data works best on not just feed-forward architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4216" target="_blank">01:10:16.020</a></span> | <span class="t">They have, for example, convolutional bits and pieces that are useful to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4221" target="_blank">01:10:21.140</a></span> | <span class="t">And that means if we want to apply some kind of a warm start for, for example, a convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4227" target="_blank">01:10:27.220</a></span> | <span class="t">layer to create a performant image classifier or something that's working with images, we'd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4231" target="_blank">01:10:31.700</a></span> | <span class="t">want to develop an initialization for that layer, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4235" target="_blank">01:10:35.660</a></span> | <span class="t">It has weirder activation functions, which means we need to branch out from softmax as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4239" target="_blank">01:10:39.660</a></span> | <span class="t">an activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4241" target="_blank">01:10:41.740</a></span> | <span class="t">But surprisingly similar convolution is to a radial model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4247" target="_blank">01:10:47.260</a></span> | <span class="t">It's really just saying what's near where I'm trying to create a feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4251" target="_blank">01:10:51.980</a></span> | <span class="t">So I would say, yes, it seems like it's something that we could do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4255" target="_blank">01:10:55.540</a></span> | <span class="t">But currently, it's in the phase of future work that it fits in one bullet here at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4268" target="_blank">01:11:08.260</a></span> | <span class="t">bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4269" target="_blank">01:11:09.260</a></span> | <span class="t">Different layer types need formal derivation for warm starts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4273" target="_blank">01:11:13.420</a></span> | <span class="t">So if we wanted to do this kind of a thing with performant architecture, we would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4278" target="_blank">01:11:18.100</a></span> | <span class="t">probably uniforming or randomly initializing some of those parameters that we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4282" target="_blank">01:11:22.140</a></span> | <span class="t">warm starts for yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4283" target="_blank">01:11:23.980</a></span> | <span class="t">And as a result, we would receive a lot of just sort of like noise in where things are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4288" target="_blank">01:11:28.340</a></span> | <span class="t">going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4289" target="_blank">01:11:29.540</a></span> | <span class="t">And if we started to utilize the activation functions, whether it's even just logistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4294" target="_blank">01:11:34.300</a></span> | <span class="t">activation, a logistic activation is not really fundamentally different than a softmax activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4299" target="_blank">01:11:39.420</a></span> | <span class="t">So you might say, for example, well, why can't you just apply that to logistic function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4303" target="_blank">01:11:43.220</a></span> | <span class="t">like a two-dimensional softmax?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4306" target="_blank">01:11:46.660</a></span> | <span class="t">And the reason is, is because if we treat it like a standard logistic, then each dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4310" target="_blank">01:11:50.140</a></span> | <span class="t">is independent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4311" target="_blank">01:11:51.900</a></span> | <span class="t">Each dimension is trying to predict the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4314" target="_blank">01:11:54.800</a></span> | <span class="t">And there's a lot more questions about how you can get different information out of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4318" target="_blank">01:11:58.440</a></span> | <span class="t">dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4319" target="_blank">01:11:59.860</a></span> | <span class="t">So it's a question that's really worth spending time on, in my opinion, separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4325" target="_blank">01:12:05.820</a></span> | <span class="t">And it's not the first question that makes a lot of what we've developed practical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4330" target="_blank">01:12:10.860</a></span> | <span class="t">On one of the slides, you had a dialogue with your user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4341" target="_blank">01:12:21.500</a></span> | <span class="t">I'm wondering, does that imply there is a speech-to-text system inside the microprocessor?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4348" target="_blank">01:12:28.540</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4349" target="_blank">01:12:29.540</a></span> | <span class="t">So audio goes in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4351" target="_blank">01:12:31.300</a></span> | <span class="t">And there's a process here which accepts that audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4355" target="_blank">01:12:35.340</a></span> | <span class="t">And it utilizes a pre-trained wave to vet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4359" target="_blank">01:12:39.100</a></span> | <span class="t">It's really just fitting a need with a pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4362" target="_blank">01:12:42.020</a></span> | <span class="t">That's what we're doing right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4364" target="_blank">01:12:44.420</a></span> | <span class="t">Although transcription is something that we would like to move into in our future work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4369" target="_blank">01:12:49.100</a></span> | <span class="t">for the purposes of training from scratch, because one of the real benefits of a system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4373" target="_blank">01:12:53.700</a></span> | <span class="t">like this is that it doesn't come with any biases from other people's data, aside from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4379" target="_blank">01:12:59.420</a></span> | <span class="t">the fact that there's a pre-trained transcription system, which means that it's pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4384" target="_blank">01:13:04.060</a></span> | <span class="t">towards whatever phonetics were within the language that was there for pre-training in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4389" target="_blank">01:13:09.340</a></span> | <span class="t">the wave-to-vec algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4391" target="_blank">01:13:11.460</a></span> | <span class="t">So there is external utility here coming from a pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4397" target="_blank">01:13:17.600</a></span> | <span class="t">But the text itself and the language model that we're presenting is only working from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4403" target="_blank">01:13:23.280</a></span> | <span class="t">what gets transcribed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4404" target="_blank">01:13:24.280</a></span> | <span class="t">I have a follow-up on my previous question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4415" target="_blank">01:13:35.320</a></span> | <span class="t">You said that the feed-forward worm start is independent of the choice of self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4422" target="_blank">01:13:42.040</a></span> | <span class="t">Does that mean that the worm start strategy can be used for any network that uses a feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4429" target="_blank">01:13:49.160</a></span> | <span class="t">layer, not just PLMs, but any LLM or any other network?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4434" target="_blank">01:13:54.600</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4435" target="_blank">01:13:55.600</a></span> | <span class="t">So that's going back to the worm start solution here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4441" target="_blank">01:14:01.880</a></span> | <span class="t">And what it says is that in terms of any layer beneath, if you assume that those layers'</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4447" target="_blank">01:14:07.320</a></span> | <span class="t">parameters are what they are, you're not going to update them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4452" target="_blank">01:14:12.680</a></span> | <span class="t">And assuming that you know what the targets for that layer are, which for middle layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4456" target="_blank">01:14:16.560</a></span> | <span class="t">there's some questions to be answered, then this initialization will do better than random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4463" target="_blank">01:14:23.940</a></span> | <span class="t">for a softmax output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4466" target="_blank">01:14:26.600</a></span> | <span class="t">That's really important at this stage, that there's a softmax as a part of the activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4472" target="_blank">01:14:32.080</a></span> | <span class="t">If there's not, then more math, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4479" target="_blank">01:14:39.000</a></span> | <span class="t">But the point at which it becomes clear should that whatever type of prediction scenario</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4488" target="_blank">01:14:48.040</a></span> | <span class="t">you're in, as long as you have non-negative features and a softmax for activation, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4496" target="_blank">01:14:56.720</a></span> | <span class="t">in this case with a single layer, or even two softmax layers, whatever that's doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4502" target="_blank">01:15:02.220</a></span> | <span class="t">on MNIST, you can get a really good initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4507" target="_blank">01:15:07.120</a></span> | <span class="t">Doesn't have to be linguistic data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4513" target="_blank">01:15:13.080</a></span> | <span class="t">This can be mixed data, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4514" target="_blank">01:15:14.480</a></span> | <span class="t">You could do an image caption generation system that has both features from images and text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4520" target="_blank">01:15:20.120</a></span> | <span class="t">and warms them up with the same solution with entirely different data in two places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4525" target="_blank">01:15:25.760</a></span> | <span class="t">Could you point out which part of the process requires the values to be non-negative?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4533" target="_blank">01:15:33.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4534" target="_blank">01:15:34.720</a></span> | <span class="t">What happens when you put a negative in a logarithm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4542" target="_blank">01:15:42.740</a></span> | <span class="t">Not saying you can't, but it's not going to start making probabilities for you at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4546" target="_blank">01:15:46.840</a></span> | <span class="t">other end of the softmax any time fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4550" target="_blank">01:15:50.400</a></span> | <span class="t">So you have to start with a different premise, essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4555" target="_blank">01:15:55.140</a></span> | <span class="t">And that premise is something that requires more derivation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4560" target="_blank">01:16:00.920</a></span> | <span class="t">You'd want to assure, if you're going to use a logarithm anywhere, or assume that inverse,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4565" target="_blank">01:16:05.600</a></span> | <span class="t">that you're able to probably modify every parameter independently, instead of full rows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4573" target="_blank">01:16:13.640</a></span> | <span class="t">of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4576" target="_blank">01:16:16.480</a></span> | <span class="t">I think we should get to a couple of questions on Slido that folks asked.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4584" target="_blank">01:16:24.880</a></span> | <span class="t">The first is, what's the difference in performance between naive assignment and optimized or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4590" target="_blank">01:16:30.280</a></span> | <span class="t">omniscient assignment for packing tokens into bit vectors, and any experimental results?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4596" target="_blank">01:16:36.860</a></span> | <span class="t">What's the difference in performance between naive assignment and optimized assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4604" target="_blank">01:16:44.560</a></span> | <span class="t">for packing tokens into bit vectors?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4613" target="_blank">01:16:53.740</a></span> | <span class="t">The performance differences are going to be in speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4616" target="_blank">01:16:56.720</a></span> | <span class="t">The systems which utilize packing for contexts have, at great length, gone to make sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4622" target="_blank">01:17:02.400</a></span> | <span class="t">information from different portions of the context that have nothing to do with each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4626" target="_blank">01:17:06.640</a></span> | <span class="t">other don't bleed information, if you're going to pack them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4631" target="_blank">01:17:11.760</a></span> | <span class="t">That creates a lot of logistical challenges in terms of defining models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4636" target="_blank">01:17:16.920</a></span> | <span class="t">And it's still just doing the regular self-attention thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4639" target="_blank">01:17:19.520</a></span> | <span class="t">So it's quadratic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4640" target="_blank">01:17:20.520</a></span> | <span class="t">So if you have the same length of context window, it's going to be the same computational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4643" target="_blank">01:17:23.920</a></span> | <span class="t">cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4645" target="_blank">01:17:25.440</a></span> | <span class="t">However, if you pack all of your small documents together, they don't need the whole context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4654" target="_blank">01:17:34.040</a></span> | <span class="t">window worth of quadratic comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4658" target="_blank">01:17:38.960</a></span> | <span class="t">And that's why you pack something into the empty end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4661" target="_blank">01:17:41.720</a></span> | <span class="t">I guess it should be over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4666" target="_blank">01:17:46.040</a></span> | <span class="t">But document packing isn't exactly, even though it's well known as a mechanism to make training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4675" target="_blank">01:17:55.040</a></span> | <span class="t">much more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4676" target="_blank">01:17:56.040</a></span> | <span class="t">In other words, you only need fewer batches if more documents are packed together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4680" target="_blank">01:18:00.920</a></span> | <span class="t">It's not something which is, for example, entirely accepted as a published accepted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4688" target="_blank">01:18:08.440</a></span> | <span class="t">form of preprocessing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4691" target="_blank">01:18:11.140</a></span> | <span class="t">So what I would say is just document packing is not a correct model of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4696" target="_blank">01:18:16.440</a></span> | <span class="t">It is an efficiency, but requires the same level of quadratic comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4701" target="_blank">01:18:21.920</a></span> | <span class="t">Whereas dynamically batching and utilizing a block size that is dynamic preserves the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4708" target="_blank">01:18:28.520</a></span> | <span class="t">model of context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4710" target="_blank">01:18:30.240</a></span> | <span class="t">It does something that is true to the objective and unwavering in that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4715" target="_blank">01:18:35.040</a></span> | <span class="t">And it reduces the complexity for smaller documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4719" target="_blank">01:18:39.360</a></span> | <span class="t">But a direct comparison of the two is something I have not done, because it would require</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4724" target="_blank">01:18:44.160</a></span> | <span class="t">having that oracle and utilizing those algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4726" target="_blank">01:18:46.720</a></span> | <span class="t">And where are they used?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4728" target="_blank">01:18:48.040</a></span> | <span class="t">They're used with insanely big models, which means we would likewise have to compare two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4732" target="_blank">01:18:52.880</a></span> | <span class="t">insanely big models to create the same level of expectation that people have from packing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4738" target="_blank">01:18:58.120</a></span> | <span class="t">So that's in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4740" target="_blank">01:19:00.400</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4741" target="_blank">01:19:01.400</a></span> | <span class="t">Thanks for your detailed response.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4742" target="_blank">01:19:02.400</a></span> | <span class="t">We have a question quickly that's asking, are there any implementations of SAFU available</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4747" target="_blank">01:19:07.920</a></span> | <span class="t">that one could experiment with?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4750" target="_blank">01:19:10.600</a></span> | <span class="t">Well, once we publish, there will be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4755" target="_blank">01:19:15.400</a></span> | <span class="t">But that requires a lot of work on developing systems for evaluation, since the evaluation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4760" target="_blank">01:19:20.420</a></span> | <span class="t">systems rely upon standardized functions within the architectures that you're all very familiar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4766" target="_blank">01:19:26.280</a></span> | <span class="t">with, like GPT-2, that are easily taken for granted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4769" target="_blank">01:19:29.680</a></span> | <span class="t">Even though you do lots of work in training them, you have to do a lot of work in creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4774" target="_blank">01:19:34.200</a></span> | <span class="t">those functions that meet the needs of the separate prediction tasks and fine tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4778" target="_blank">01:19:38.040</a></span> | <span class="t">that evaluations perform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4779" target="_blank">01:19:39.840</a></span> | <span class="t">All right, great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4782" target="_blank">01:19:42.160</a></span> | <span class="t">Makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4783" target="_blank">01:19:43.160</a></span> | <span class="t">I think we're pretty much out of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4785" target="_blank">01:19:45.320</a></span> | <span class="t">So thanks, Jake, for the great talk, and thanks for coming to another lecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4790" target="_blank">01:19:50.560</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4791" target="_blank">01:19:51.560</a></span> | <span class="t">[END]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zL9B3eXq0gY&t=4791" target="_blank">01:19:51.560</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>