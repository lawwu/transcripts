<html><head><title>Stanford XCS224U: NLU I Information Retrieval, Part 4: Neural IR I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Information Retrieval, Part 4: Neural IR I Spring 2023</h2><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q"><img src="https://i.ytimg.com/vi/EDVqG86AT0Q/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGFkgZChlMA8=&rs=AOn4CLDL8YOzk7JyHofqXzihNteTMsHGlQ" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=21">0:21</a> Cross-encoders<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=293">4:53</a> Shared loss function The negative log-likelihood of the positive passage<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=518">8:38</a> Soft alignment with ColBERT<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=601">10:1</a> ColBERT as a reranker<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=738">12:18</a> Beyond reranking for CoIBERT<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=801">13:21</a> Centroid-based ranking<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=869">14:29</a> ColBERT latency analysis<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=964">16:4</a> Additional ColBERT optimizations<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1019">16:59</a> SPLADE<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1191">19:51</a> Additional recent developments<br><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1229">20:29</a> Multidimensional benchmarking<br><br><div style="text-align: left;"><a href="./EDVqG86AT0Q.html">Whisper Transcript</a> | <a href="./transcript_EDVqG86AT0Q.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=6" target="_blank">00:00:06.160</a></span> | <span class="t">This is part 4 in our series on information retrieval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=9" target="_blank">00:00:09.240</a></span> | <span class="t">We come to the heart of it, neural information retrieval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=11" target="_blank">00:00:11.840</a></span> | <span class="t">This is the class of models that has done so much to bring NLP and IR</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=16" target="_blank">00:00:16.200</a></span> | <span class="t">back together again and open new doors for both of those fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=20" target="_blank">00:00:20.480</a></span> | <span class="t">In the background throughout the screencast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=23" target="_blank">00:00:23.040</a></span> | <span class="t">I think you should imagine that the name of the game is to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=25" target="_blank">00:00:25.800</a></span> | <span class="t">a pre-trained BERT model and fine-tune it for information retrieval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=31" target="_blank">00:00:31.320</a></span> | <span class="t">In that context, cross-encoders are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=34" target="_blank">00:00:34.240</a></span> | <span class="t">conceptually the simplest approach that you could take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=37" target="_blank">00:00:37.400</a></span> | <span class="t">For cross-encoders, we're going to concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=40" target="_blank">00:00:40.720</a></span> | <span class="t">the query text and the document text together into one single text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=44" target="_blank">00:00:44.920</a></span> | <span class="t">process that text with BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=47" target="_blank">00:00:47.020</a></span> | <span class="t">and then use representations in that BERT model as the basis for IR fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=52" target="_blank">00:00:52.720</a></span> | <span class="t">In a bit more detail, we'd process the query in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=55" target="_blank">00:00:55.040</a></span> | <span class="t">the document and then probably take the final output state above the class token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=60" target="_blank">00:01:00.080</a></span> | <span class="t">add some task specific parameters on top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=63" target="_blank">00:01:03.120</a></span> | <span class="t">and fine-tune the model against our information retrieval objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=67" target="_blank">00:01:07.440</a></span> | <span class="t">That will be incredibly semantically expressive because we have all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=71" target="_blank">00:01:11.080</a></span> | <span class="t">these interesting interactions between query and document in this mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=75" target="_blank">00:01:15.680</a></span> | <span class="t">In a bit more detail, in the background here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=77" target="_blank">00:01:17.760</a></span> | <span class="t">I'm imagining that we have a dataset of triples where we have a query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=81" target="_blank">00:01:21.800</a></span> | <span class="t">one positive document for that query, and some number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=85" target="_blank">00:01:25.280</a></span> | <span class="t">one or more negative documents for that query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=88" target="_blank">00:01:28.920</a></span> | <span class="t">The basis for scoring is as I described it before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=92" target="_blank">00:01:32.360</a></span> | <span class="t">we're going to take our BERT encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=94" target="_blank">00:01:34.480</a></span> | <span class="t">concatenate the query in the document and process that text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=97" target="_blank">00:01:37.680</a></span> | <span class="t">and then retrieve the final output state above the class token that's given here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=102" target="_blank">00:01:42.400</a></span> | <span class="t">and that's fed through a dense layer that is used for scoring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=106" target="_blank">00:01:46.760</a></span> | <span class="t">Then the loss function for the model is typically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=109" target="_blank">00:01:49.820</a></span> | <span class="t">the negative log likelihood of the positive passage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=112" target="_blank">00:01:52.840</a></span> | <span class="t">In the numerator here, we have our score for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=115" target="_blank">00:01:55.400</a></span> | <span class="t">the positive passage according to our scoring function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=118" target="_blank">00:01:58.360</a></span> | <span class="t">and the denominator is that positive passage score again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=121" target="_blank">00:02:01.440</a></span> | <span class="t">sum together with the total for all the negative passages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=125" target="_blank">00:02:05.760</a></span> | <span class="t">Let's step back. This will be incredibly semantically rich,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=129" target="_blank">00:02:09.560</a></span> | <span class="t">but it simply won't scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=131" target="_blank">00:02:11.400</a></span> | <span class="t">The richness comes from us using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=133" target="_blank">00:02:13.360</a></span> | <span class="t">the BERT model to jointly encode the query and the documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=136" target="_blank">00:02:16.760</a></span> | <span class="t">We have all these rich token level interactions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=139" target="_blank">00:02:19.700</a></span> | <span class="t">but that is the model's downfall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=141" target="_blank">00:02:21.760</a></span> | <span class="t">This won't scale because we need to encode every document at query time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=146" target="_blank">00:02:26.860</a></span> | <span class="t">In principle, this means that if we have a billion documents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=149" target="_blank">00:02:29.780</a></span> | <span class="t">we need to do a billion forward passes with the BERT model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=154" target="_blank">00:02:34.100</a></span> | <span class="t">one for every document with respect to our query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=157" target="_blank">00:02:37.100</a></span> | <span class="t">get all those scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=158" target="_blank">00:02:38.300</a></span> | <span class="t">and then make decisions on that basis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=160" target="_blank">00:02:40.220</a></span> | <span class="t">and that will be simply infeasible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=162" target="_blank">00:02:42.940</a></span> | <span class="t">Although there's something conceptually right about this approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=166" target="_blank">00:02:46.180</a></span> | <span class="t">it's simply intractable for modern search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=170" target="_blank">00:02:50.380</a></span> | <span class="t">DPR can be seen as a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=172" target="_blank">00:02:52.820</a></span> | <span class="t">that's at the other end of the spectrum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=174" target="_blank">00:02:54.700</a></span> | <span class="t">This stands for dense passage retriever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=177" target="_blank">00:02:57.080</a></span> | <span class="t">In this mode, we're going to separately encode queries and documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=181" target="_blank">00:03:01.340</a></span> | <span class="t">On the left here, I've got our query encoded with a BERT-like model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=184" target="_blank">00:03:04.940</a></span> | <span class="t">and I've grayed out all of the states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=187" target="_blank">00:03:07.160</a></span> | <span class="t">except the final output state above the class token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=189" target="_blank">00:03:09.940</a></span> | <span class="t">That's the only one that we'll really need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=192" target="_blank">00:03:12.100</a></span> | <span class="t">I separately encode the document, and again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=194" target="_blank">00:03:14.500</a></span> | <span class="t">we just need that final output state above the class token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=198" target="_blank">00:03:18.260</a></span> | <span class="t">Then we're going to do scoring as a dot product of those two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=202" target="_blank">00:03:22.780</a></span> | <span class="t">In a bit more detail, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=204" target="_blank">00:03:24.220</a></span> | <span class="t">we have a dataset consisting of those triples for our query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=207" target="_blank">00:03:27.620</a></span> | <span class="t">one positive document, and one or more negative documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=211" target="_blank">00:03:31.500</a></span> | <span class="t">Now, our core comparison function is what I've called sim here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=215" target="_blank">00:03:35.820</a></span> | <span class="t">The basis for sim is that we encode our query using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=218" target="_blank">00:03:38.700</a></span> | <span class="t">our query encoder and get the final output state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=221" target="_blank">00:03:41.780</a></span> | <span class="t">and we get the dot product of that with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=223" target="_blank">00:03:43.900</a></span> | <span class="t">the encoding for our document again focused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=226" target="_blank">00:03:46.460</a></span> | <span class="t">on the output state above the class token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=229" target="_blank">00:03:49.220</a></span> | <span class="t">The loss is as before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=230" target="_blank">00:03:50.980</a></span> | <span class="t">this is the negative log likelihood of the positive passage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=234" target="_blank">00:03:54.100</a></span> | <span class="t">The positive score up here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=235" target="_blank">00:03:55.960</a></span> | <span class="t">and then again, use down here sum together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=238" target="_blank">00:03:58.420</a></span> | <span class="t">with the sum for all of the negative passages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=241" target="_blank">00:04:01.540</a></span> | <span class="t">This will be highly scalable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=244" target="_blank">00:04:04.220</a></span> | <span class="t">but it's very limited in terms of its query document interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=248" target="_blank">00:04:08.100</a></span> | <span class="t">Let's unpack that a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=249" target="_blank">00:04:09.460</a></span> | <span class="t">The core of the scalability is that we can now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=252" target="_blank">00:04:12.580</a></span> | <span class="t">encode all of our documents offline ahead of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=255" target="_blank">00:04:15.460</a></span> | <span class="t">and indeed, we only need to store</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=258" target="_blank">00:04:18.460</a></span> | <span class="t">one single vector associated with each one of those documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=262" target="_blank">00:04:22.180</a></span> | <span class="t">Then at query time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=263" target="_blank">00:04:23.460</a></span> | <span class="t">we just encode the query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=265" target="_blank">00:04:25.020</a></span> | <span class="t">get that one representation above the class token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=268" target="_blank">00:04:28.180</a></span> | <span class="t">and do a fast dot product with all of our documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=271" target="_blank">00:04:31.100</a></span> | <span class="t">It's highly scalable in that sense too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=273" target="_blank">00:04:33.460</a></span> | <span class="t">But at the same time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=274" target="_blank">00:04:34.860</a></span> | <span class="t">we have lost all of those token level interactions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=277" target="_blank">00:04:37.660</a></span> | <span class="t">we had with the cross encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=279" target="_blank">00:04:39.980</a></span> | <span class="t">Now, we have to hope that all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=281" target="_blank">00:04:41.580</a></span> | <span class="t">the information about the query and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=283" target="_blank">00:04:43.180</a></span> | <span class="t">the document is summarized in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=285" target="_blank">00:04:45.340</a></span> | <span class="t">those single vector representations and we might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=288" target="_blank">00:04:48.460</a></span> | <span class="t">worry that that results in a loss of expressivity for the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=293" target="_blank">00:04:53.100</a></span> | <span class="t">Before moving on to some additional models in this space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=296" target="_blank">00:04:56.340</a></span> | <span class="t">I thought I would just pause here and point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=298" target="_blank">00:04:58.540</a></span> | <span class="t">out that we have a little bit of modularity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=301" target="_blank">00:05:01.300</a></span> | <span class="t">The loss function for both of the models that I presented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=304" target="_blank">00:05:04.540</a></span> | <span class="t">is the negative log likelihood of the positive passage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=307" target="_blank">00:05:07.780</a></span> | <span class="t">Here's how I presented it for the cross encoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=310" target="_blank">00:05:10.100</a></span> | <span class="t">and the core of that is this rep function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=312" target="_blank">00:05:12.500</a></span> | <span class="t">Here's how I presented it for DPR,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=315" target="_blank">00:05:15.020</a></span> | <span class="t">where the core of it is the sim function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=317" target="_blank">00:05:17.300</a></span> | <span class="t">You can now see that there's a general form of this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=320" target="_blank">00:05:20.260</a></span> | <span class="t">where we just have some comparison function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=322" target="_blank">00:05:22.460</a></span> | <span class="t">and everything else remains the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=324" target="_blank">00:05:24.620</a></span> | <span class="t">This is freeing because if you developed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=327" target="_blank">00:05:27.420</a></span> | <span class="t">variants of DPR or cross encoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=330" target="_blank">00:05:30.500</a></span> | <span class="t">the way that might play out is that you've simply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=332" target="_blank">00:05:32.740</a></span> | <span class="t">adjusted the comparison function here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=335" target="_blank">00:05:35.260</a></span> | <span class="t">and everything else about how you're setting up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=337" target="_blank">00:05:37.340</a></span> | <span class="t">models and optimizing them could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=338" target="_blank">00:05:38.900</a></span> | <span class="t">potentially stay the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=341" target="_blank">00:05:41.460</a></span> | <span class="t">Let's move to Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=344" target="_blank">00:05:44.420</a></span> | <span class="t">This model is near and dear to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=346" target="_blank">00:05:46.340</a></span> | <span class="t">Colbert was developed by Omar Khattab,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=348" target="_blank">00:05:48.300</a></span> | <span class="t">who is my student, along with Matei Zaharia,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=351" target="_blank">00:05:51.100</a></span> | <span class="t">who's my longtime collaborator and co-advises Omar with me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=355" target="_blank">00:05:55.340</a></span> | <span class="t">Omar would want me to point out for you that Colbert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=358" target="_blank">00:05:58.700</a></span> | <span class="t">stands for contextualized late interaction with BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=362" target="_blank">00:06:02.300</a></span> | <span class="t">That's an homage to the late night talk show host,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=365" target="_blank">00:06:05.100</a></span> | <span class="t">Stephen Colbert, who has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=366" target="_blank">00:06:06.860</a></span> | <span class="t">late night contextual interactions with his guests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=370" target="_blank">00:06:10.420</a></span> | <span class="t">But you are also free to pronounce this Colbert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=373" target="_blank">00:06:13.420</a></span> | <span class="t">because obviously the BERT in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=375" target="_blank">00:06:15.220</a></span> | <span class="t">that name is the famous BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=378" target="_blank">00:06:18.300</a></span> | <span class="t">Here's how Colbert works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=380" target="_blank">00:06:20.220</a></span> | <span class="t">First, we encode queries using BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=382" target="_blank">00:06:22.540</a></span> | <span class="t">I've drawn this on its side for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=384" target="_blank">00:06:24.140</a></span> | <span class="t">reasons that will become clear when I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=385" target="_blank">00:06:25.540</a></span> | <span class="t">show you my full diagram,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=386" target="_blank">00:06:26.780</a></span> | <span class="t">but it's just a BERT encoding of the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=389" target="_blank">00:06:29.140</a></span> | <span class="t">I've grayed out all the states except the final ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=391" target="_blank">00:06:31.980</a></span> | <span class="t">because the only states we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=394" target="_blank">00:06:34.060</a></span> | <span class="t">are the output states from this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=397" target="_blank">00:06:37.020</a></span> | <span class="t">Similarly, we encode the document again with BERT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=400" target="_blank">00:06:40.660</a></span> | <span class="t">and here again, the only states we need are the output states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=404" target="_blank">00:06:44.860</a></span> | <span class="t">Then the basis for Colbert scoring is a matrix of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=408" target="_blank">00:06:48.380</a></span> | <span class="t">similarity scores between query tokens and document tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=412" target="_blank">00:06:52.580</a></span> | <span class="t">again, as represented by these final output layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=415" target="_blank">00:06:55.540</a></span> | <span class="t">We get scores, and in fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=417" target="_blank">00:06:57.140</a></span> | <span class="t">we get a full grid of these scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=419" target="_blank">00:06:59.740</a></span> | <span class="t">Then the basis for scoring is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=421" target="_blank">00:07:01.940</a></span> | <span class="t">a maxim comparison for every query token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=425" target="_blank">00:07:05.820</a></span> | <span class="t">We get the value of the maximum similarity for document tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=430" target="_blank">00:07:10.260</a></span> | <span class="t">and we sum those together to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=432" target="_blank">00:07:12.260</a></span> | <span class="t">the maxim value that is the basis for the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=435" target="_blank">00:07:15.500</a></span> | <span class="t">In a bit more detail, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=437" target="_blank">00:07:17.260</a></span> | <span class="t">we have a dataset consisting of those triples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=440" target="_blank">00:07:20.220</a></span> | <span class="t">The loss is the negative log likelihood of the positive passage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=443" target="_blank">00:07:23.860</a></span> | <span class="t">but now maxim is the basis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=445" target="_blank">00:07:25.620</a></span> | <span class="t">and here is the maxim scoring function in full detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=449" target="_blank">00:07:29.460</a></span> | <span class="t">But again, the essence of this is that for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=451" target="_blank">00:07:31.380</a></span> | <span class="t">each query token, we get the maxim for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=454" target="_blank">00:07:34.500</a></span> | <span class="t">some document token and sum all those maxim values together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=459" target="_blank">00:07:39.180</a></span> | <span class="t">This will be highly scalable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=461" target="_blank">00:07:41.780</a></span> | <span class="t">but it has late contextual interactions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=464" target="_blank">00:07:44.380</a></span> | <span class="t">between tokens in the query and tokens in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=467" target="_blank">00:07:47.460</a></span> | <span class="t">Let me unpack that. It's highly scalable because as with DPR,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=471" target="_blank">00:07:51.420</a></span> | <span class="t">we can store all of our documents ahead of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=474" target="_blank">00:07:54.660</a></span> | <span class="t">We just need to score this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=477" target="_blank">00:07:57.580</a></span> | <span class="t">of output vectors here to represent documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=480" target="_blank">00:08:00.660</a></span> | <span class="t">At query time, we encode the query and get the output states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=484" target="_blank">00:08:04.100</a></span> | <span class="t">and then perform a bunch of very fast maxim comparisons for scoring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=489" target="_blank">00:08:09.020</a></span> | <span class="t">But it's also semantically rich.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=491" target="_blank">00:08:11.180</a></span> | <span class="t">We have retained some of the advantages of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=493" target="_blank">00:08:13.340</a></span> | <span class="t">the cross encoder because we do have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=495" target="_blank">00:08:15.780</a></span> | <span class="t">token level interactions between query and document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=498" target="_blank">00:08:18.780</a></span> | <span class="t">It's now, it's just that they happen only on the output states,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=502" target="_blank">00:08:22.340</a></span> | <span class="t">whereas the cross encoder allowed them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=504" target="_blank">00:08:24.020</a></span> | <span class="t">happen at every layer in the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=506" target="_blank">00:08:26.820</a></span> | <span class="t">That was too expensive and this looks like a nice compromise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=510" target="_blank">00:08:30.700</a></span> | <span class="t">Colbert has indeed proven to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=512" target="_blank">00:08:32.900</a></span> | <span class="t">an extremely powerful and effective IR mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=517" target="_blank">00:08:37.700</a></span> | <span class="t">One thing I really like about Colbert is that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=521" target="_blank">00:08:41.420</a></span> | <span class="t">brings in an older insight from IR,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=524" target="_blank">00:08:44.180</a></span> | <span class="t">which is that essentially we want to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=525" target="_blank">00:08:45.860</a></span> | <span class="t">some level of term matching between queries and documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=529" target="_blank">00:08:49.300</a></span> | <span class="t">Except now, since this is a neural model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=531" target="_blank">00:08:51.620</a></span> | <span class="t">we get to do that in a semantically very rich space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=535" target="_blank">00:08:55.060</a></span> | <span class="t">Let me show you that by way of an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=536" target="_blank">00:08:56.940</a></span> | <span class="t">Here I have the query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=538" target="_blank">00:08:58.100</a></span> | <span class="t">when did the Transformers cartoon series come out?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=541" target="_blank">00:09:01.260</a></span> | <span class="t">We have the document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=542" target="_blank">00:09:02.500</a></span> | <span class="t">the animated Transformers was released in August 1986.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=546" target="_blank">00:09:06.740</a></span> | <span class="t">I'm going to show you some maxim values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=549" target="_blank">00:09:09.660</a></span> | <span class="t">The largest score is between Transformers in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=552" target="_blank">00:09:12.380</a></span> | <span class="t">the query and Transformers in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=554" target="_blank">00:09:14.940</a></span> | <span class="t">That makes good sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=556" target="_blank">00:09:16.660</a></span> | <span class="t">But we also have a very strong maxim match between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=559" target="_blank">00:09:19.860</a></span> | <span class="t">cartoon in the query and animated in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=563" target="_blank">00:09:23.300</a></span> | <span class="t">That's a very semantic connection that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=565" target="_blank">00:09:25.460</a></span> | <span class="t">only neural models like Colbert can make without extra effort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=568" target="_blank">00:09:28.900</a></span> | <span class="t">Similarly, for come out in the context of the query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=572" target="_blank">00:09:32.860</a></span> | <span class="t">we have a strong match to released in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=576" target="_blank">00:09:36.020</a></span> | <span class="t">Then for when in the query that matches to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=578" target="_blank">00:09:38.420</a></span> | <span class="t">the two parts of the date expression, August 1986.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=581" target="_blank">00:09:41.620</a></span> | <span class="t">Here I've shown the top two maxim values to show that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=584" target="_blank">00:09:44.780</a></span> | <span class="t">really getting a semantic connection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=586" target="_blank">00:09:46.420</a></span> | <span class="t">to that full unit in the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=588" target="_blank">00:09:48.820</a></span> | <span class="t">This thing makes the model highly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=591" target="_blank">00:09:51.620</a></span> | <span class="t">interpretable and also reveals to us why this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=594" target="_blank">00:09:54.300</a></span> | <span class="t">such an effective retrieval mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=597" target="_blank">00:09:57.340</a></span> | <span class="t">because it can make all of these deep associations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=601" target="_blank">00:10:01.100</a></span> | <span class="t">Before moving on to SPLADE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=603" target="_blank">00:10:03.660</a></span> | <span class="t">the final model that I wanted to talk about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=605" target="_blank">00:10:05.580</a></span> | <span class="t">I thought I would pause here and just talk a little bit with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=608" target="_blank">00:10:08.420</a></span> | <span class="t">you about how you take Colbert or any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=611" target="_blank">00:10:11.420</a></span> | <span class="t">these neural models and then turn them into something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=614" target="_blank">00:10:14.220</a></span> | <span class="t">could be effective as a deployed search technology.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=618" target="_blank">00:10:18.540</a></span> | <span class="t">Because in the background here is that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=621" target="_blank">00:10:21.060</a></span> | <span class="t">semantic expressiveness but it comes at a price,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=624" target="_blank">00:10:24.300</a></span> | <span class="t">we need to do forward inference in BERT models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=627" target="_blank">00:10:27.300</a></span> | <span class="t">and that can be very expensive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=629" target="_blank">00:10:29.700</a></span> | <span class="t">prohibitively so if we have very tight latency restrictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=633" target="_blank">00:10:33.860</a></span> | <span class="t">The question for us is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=635" target="_blank">00:10:35.420</a></span> | <span class="t">can we overcome those limitations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=637" target="_blank">00:10:37.780</a></span> | <span class="t">and make this a practical solution?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=640" target="_blank">00:10:40.580</a></span> | <span class="t">One easy thing to do to make this practical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=644" target="_blank">00:10:44.180</a></span> | <span class="t">is to employ these models as re-rankers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=646" target="_blank">00:10:46.860</a></span> | <span class="t">Here this is how this would play out for Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=649" target="_blank">00:10:49.780</a></span> | <span class="t">For Colbert, remember we have an index that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=652" target="_blank">00:10:52.060</a></span> | <span class="t">essentially consists of token level representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=655" target="_blank">00:10:55.620</a></span> | <span class="t">Those are each associated with documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=658" target="_blank">00:10:58.260</a></span> | <span class="t">Given an index structure like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=660" target="_blank">00:11:00.380</a></span> | <span class="t">a simple thing to do would be to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=661" target="_blank">00:11:01.940</a></span> | <span class="t">our query and code it as a bunch of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=664" target="_blank">00:11:04.780</a></span> | <span class="t">get the top K documents for that query using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=668" target="_blank">00:11:08.020</a></span> | <span class="t">a fast term-based model like BM25,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=671" target="_blank">00:11:11.180</a></span> | <span class="t">and then use Colbert only at stage 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=673" target="_blank">00:11:13.900</a></span> | <span class="t">to re-rank the top K documents there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=677" target="_blank">00:11:17.060</a></span> | <span class="t">We use BM25 for the expensive first phase where we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=681" target="_blank">00:11:21.420</a></span> | <span class="t">brute force search over our entire index of documents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=685" target="_blank">00:11:25.100</a></span> | <span class="t">and the model like Colbert comes in only at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=687" target="_blank">00:11:27.500</a></span> | <span class="t">phase 2 to do re-ranking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=690" target="_blank">00:11:30.140</a></span> | <span class="t">It sounds like a small thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=691" target="_blank">00:11:31.740</a></span> | <span class="t">but in fact the re-ranking that happens in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=693" target="_blank">00:11:33.700</a></span> | <span class="t">that second phase can be incredibly powerful and add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=696" target="_blank">00:11:36.780</a></span> | <span class="t">a lot of value as a result of the fact that Colbert and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=699" target="_blank">00:11:39.860</a></span> | <span class="t">models like it are so good at doing retrieval in this context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=704" target="_blank">00:11:44.700</a></span> | <span class="t">but they're expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=706" target="_blank">00:11:46.140</a></span> | <span class="t">One nice thing about this though is that we can control</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=708" target="_blank">00:11:48.940</a></span> | <span class="t">our costs because if we set K very low,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=711" target="_blank">00:11:51.700</a></span> | <span class="t">we'll do very little processing with Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=714" target="_blank">00:11:54.180</a></span> | <span class="t">If we set K high,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=715" target="_blank">00:11:55.420</a></span> | <span class="t">we'll use Colbert more often and we can calibrate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=718" target="_blank">00:11:58.220</a></span> | <span class="t">that against other constraints that we're operating under.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=722" target="_blank">00:12:02.100</a></span> | <span class="t">This is a perfectly reasonable solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=724" target="_blank">00:12:04.940</a></span> | <span class="t">The one concern you might have maybe as a purist,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=728" target="_blank">00:12:08.060</a></span> | <span class="t">is that you now have two retrieval mechanisms in play,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=730" target="_blank">00:12:10.780</a></span> | <span class="t">BM25 which does a lot of the work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=733" target="_blank">00:12:13.220</a></span> | <span class="t">and Colbert which performs the re-ranking function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=736" target="_blank">00:12:16.220</a></span> | <span class="t">We might hope for a more integrated solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=739" target="_blank">00:12:19.140</a></span> | <span class="t">Could we get beyond re-ranking for Colbert?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=742" target="_blank">00:12:22.100</a></span> | <span class="t">I think the answer is yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=743" target="_blank">00:12:23.820</a></span> | <span class="t">We're going to make a slight adjustment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=745" target="_blank">00:12:25.740</a></span> | <span class="t">to how we set up the index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=747" target="_blank">00:12:27.060</a></span> | <span class="t">Now, the primary thing will be that we'll have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=749" target="_blank">00:12:29.220</a></span> | <span class="t">these token level vectors which of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=752" target="_blank">00:12:32.180</a></span> | <span class="t">as before, associate with documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=755" target="_blank">00:12:35.220</a></span> | <span class="t">Now, when a query comes in,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=757" target="_blank">00:12:37.300</a></span> | <span class="t">we encode that into a sequence of vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=760" target="_blank">00:12:40.420</a></span> | <span class="t">and then for each vector in that query representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=764" target="_blank">00:12:44.020</a></span> | <span class="t">we retrieve the P most similar token vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=767" target="_blank">00:12:47.420</a></span> | <span class="t">and then travel through them to their associated documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=770" target="_blank">00:12:50.860</a></span> | <span class="t">Then the only Colbert work that we do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=773" target="_blank">00:12:53.940</a></span> | <span class="t">scoring this potentially small set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=775" target="_blank">00:12:55.780</a></span> | <span class="t">of documents that we end up in phase 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=777" target="_blank">00:12:57.860</a></span> | <span class="t">Because in phase 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=779" target="_blank">00:12:59.340</a></span> | <span class="t">all we're doing is a bunch of similarity calculations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=782" target="_blank">00:13:02.420</a></span> | <span class="t">between vector representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=784" target="_blank">00:13:04.860</a></span> | <span class="t">Again, we have a lot of control over how much we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=788" target="_blank">00:13:08.100</a></span> | <span class="t">actually use the full Colbert model at step 2 here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=791" target="_blank">00:13:11.220</a></span> | <span class="t">and therefore we can calibrate against</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=793" target="_blank">00:13:13.220</a></span> | <span class="t">other constraints that we're operating under.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=796" target="_blank">00:13:16.180</a></span> | <span class="t">This is certainly workable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=798" target="_blank">00:13:18.220</a></span> | <span class="t">but we can probably do even better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=800" target="_blank">00:13:20.860</a></span> | <span class="t">The way we can do even better is with centroid-based ranking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=804" target="_blank">00:13:24.780</a></span> | <span class="t">This begins from the insight that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=806" target="_blank">00:13:26.580</a></span> | <span class="t">this index that we've constructed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=808" target="_blank">00:13:28.340</a></span> | <span class="t">here will have a lot of semantic structure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=811" target="_blank">00:13:31.260</a></span> | <span class="t">and we can capture that by clustering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=813" target="_blank">00:13:33.620</a></span> | <span class="t">the token level vectors that represent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=815" target="_blank">00:13:35.660</a></span> | <span class="t">our documents into clusters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=818" target="_blank">00:13:38.460</a></span> | <span class="t">and then taking their centroids to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=820" target="_blank">00:13:40.540</a></span> | <span class="t">representative summaries of those clusters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=823" target="_blank">00:13:43.460</a></span> | <span class="t">We can use those as the basis for search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=827" target="_blank">00:13:47.060</a></span> | <span class="t">Now, given a query that we encode again as a sequence of vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=831" target="_blank">00:13:51.220</a></span> | <span class="t">for each one of those vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=832" target="_blank">00:13:52.780</a></span> | <span class="t">we retrieve the closest centroids,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=835" target="_blank">00:13:55.460</a></span> | <span class="t">and then travel from them to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=837" target="_blank">00:13:57.540</a></span> | <span class="t">similar document tokens and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=839" target="_blank">00:13:59.860</a></span> | <span class="t">then from them to similar documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=842" target="_blank">00:14:02.100</a></span> | <span class="t">Then again, we use Colbert,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=843" target="_blank">00:14:03.620</a></span> | <span class="t">the full model only at step 3 here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=846" target="_blank">00:14:06.340</a></span> | <span class="t">All these other comparisons are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=847" target="_blank">00:14:07.900</a></span> | <span class="t">just fast similarity comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=850" target="_blank">00:14:10.940</a></span> | <span class="t">This gives us huge gains because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=852" target="_blank">00:14:12.980</a></span> | <span class="t">instead of having to search over this entire index,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=855" target="_blank">00:14:15.580</a></span> | <span class="t">we search over a potentially very small number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=858" target="_blank">00:14:18.940</a></span> | <span class="t">centroid representations and use those as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=861" target="_blank">00:14:21.780</a></span> | <span class="t">the basis for getting down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=863" target="_blank">00:14:23.740</a></span> | <span class="t">a small set of documents that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=865" target="_blank">00:14:25.340</a></span> | <span class="t">going to score completely with Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=868" target="_blank">00:14:28.420</a></span> | <span class="t">That's a bunch of the work that we've done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=872" target="_blank">00:14:32.260</a></span> | <span class="t">I thought I would just mention a little bit of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=874" target="_blank">00:14:34.540</a></span> | <span class="t">the work that we've done specifically to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=876" target="_blank">00:14:36.380</a></span> | <span class="t">address latency concerns for Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=878" target="_blank">00:14:38.660</a></span> | <span class="t">This comes from the paper that we called Plaid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=881" target="_blank">00:14:41.460</a></span> | <span class="t">It begins from the observation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=883" target="_blank">00:14:43.580</a></span> | <span class="t">despite all the hard work that I just described for you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=886" target="_blank">00:14:46.260</a></span> | <span class="t">the latency for the Colbert model was still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=889" target="_blank">00:14:49.020</a></span> | <span class="t">prohibitively high at 287 milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=892" target="_blank">00:14:52.780</a></span> | <span class="t">Whereas you might hope you could get this down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=894" target="_blank">00:14:54.700</a></span> | <span class="t">around 50 milliseconds for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=896" target="_blank">00:14:56.300</a></span> | <span class="t">a feasible deployable solution at a minimum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=900" target="_blank">00:15:00.380</a></span> | <span class="t">This chart here is showing you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=902" target="_blank">00:15:02.420</a></span> | <span class="t">where the work actually happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=904" target="_blank">00:15:04.100</a></span> | <span class="t">One surprising thing for Colbert is that only a small part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=908" target="_blank">00:15:08.300</a></span> | <span class="t">the overall time there is actually spent on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=910" target="_blank">00:15:10.860</a></span> | <span class="t">the core modeling steps of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=912" target="_blank">00:15:12.380</a></span> | <span class="t">representing examples and doing scoring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=915" target="_blank">00:15:15.860</a></span> | <span class="t">In fact, only a small part is even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=918" target="_blank">00:15:18.660</a></span> | <span class="t">used with the centroids that I described before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=921" target="_blank">00:15:21.180</a></span> | <span class="t">The bulk of the work is being done when we have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=924" target="_blank">00:15:24.180</a></span> | <span class="t">look things up in this giant index,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=926" target="_blank">00:15:26.820</a></span> | <span class="t">and also when we do decompression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=929" target="_blank">00:15:29.140</a></span> | <span class="t">That's a point that I haven't mentioned before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=931" target="_blank">00:15:31.060</a></span> | <span class="t">but the essence of this is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=933" target="_blank">00:15:33.140</a></span> | <span class="t">the Colbert index can get very large because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=936" target="_blank">00:15:36.220</a></span> | <span class="t">need to store token level representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=939" target="_blank">00:15:39.180</a></span> | <span class="t">But we find that we can make them relatively low resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=943" target="_blank">00:15:43.140</a></span> | <span class="t">for or even two-bit representations because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=946" target="_blank">00:15:46.060</a></span> | <span class="t">all they need to do is represent individual tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=949" target="_blank">00:15:49.580</a></span> | <span class="t">But that does mean that we would like to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=951" target="_blank">00:15:51.460</a></span> | <span class="t">decompress them at some point to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=953" target="_blank">00:15:53.300</a></span> | <span class="t">get back to their full semantic richness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=955" target="_blank">00:15:55.700</a></span> | <span class="t">We found that that step of unpacking them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=958" target="_blank">00:15:58.900</a></span> | <span class="t">was also expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=961" target="_blank">00:16:01.020</a></span> | <span class="t">What the team did is do a lot of work to reduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=964" target="_blank">00:16:04.380</a></span> | <span class="t">the amount of heavy-duty lookup and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=966" target="_blank">00:16:06.580</a></span> | <span class="t">decompression that the Colbert model was doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=969" target="_blank">00:16:09.260</a></span> | <span class="t">They trade that a little bit off against using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=971" target="_blank">00:16:11.740</a></span> | <span class="t">more centroids as part of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=973" target="_blank">00:16:13.260</a></span> | <span class="t">that initial search phase that I described.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=976" target="_blank">00:16:16.100</a></span> | <span class="t">But they did successfully remove almost all the overhead that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=979" target="_blank">00:16:19.740</a></span> | <span class="t">coming from these large data structures and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=981" target="_blank">00:16:21.940</a></span> | <span class="t">the corresponding decompression that we had to do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=984" target="_blank">00:16:24.900</a></span> | <span class="t">and they got the latency all the way down to 58 milliseconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=988" target="_blank">00:16:28.860</a></span> | <span class="t">I regard this as absolutely an amazing achievement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=992" target="_blank">00:16:32.740</a></span> | <span class="t">I think it shows you how much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=994" target="_blank">00:16:34.540</a></span> | <span class="t">innovative work can happen in this space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=996" target="_blank">00:16:36.300</a></span> | <span class="t">not focused on hill climbing on accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=999" target="_blank">00:16:39.420</a></span> | <span class="t">but rather thinking about issues like latency and how they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1002" target="_blank">00:16:42.380</a></span> | <span class="t">impact the deployability of systems like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1006" target="_blank">00:16:46.180</a></span> | <span class="t">There's lots more room for innovation in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1009" target="_blank">00:16:49.540</a></span> | <span class="t">I would exhort you-all to think about how you could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1011" target="_blank">00:16:51.740</a></span> | <span class="t">contribute to making systems not only more accurate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1014" target="_blank">00:16:54.700</a></span> | <span class="t">but also more efficient along this and other dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1019" target="_blank">00:16:59.020</a></span> | <span class="t">There's one more model that I wanted to mention because I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1022" target="_blank">00:17:02.740</a></span> | <span class="t">this is incredibly powerful and competitive and also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1025" target="_blank">00:17:05.980</a></span> | <span class="t">offers yet again another perspective on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1028" target="_blank">00:17:08.900</a></span> | <span class="t">how to use neural representations in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1031" target="_blank">00:17:11.700</a></span> | <span class="t">This model is SPLADE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1034" target="_blank">00:17:14.100</a></span> | <span class="t">Here's how SPLADE works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1035" target="_blank">00:17:15.820</a></span> | <span class="t">I've got at the bottom here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1037" target="_blank">00:17:17.180</a></span> | <span class="t">our encoding mechanism for sequences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1039" target="_blank">00:17:19.940</a></span> | <span class="t">and I'm trying to be agnostic about whether this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1042" target="_blank">00:17:22.220</a></span> | <span class="t">a query sequence or a document sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1044" target="_blank">00:17:24.540</a></span> | <span class="t">because we do both of those with the same kind of calculations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1049" target="_blank">00:17:29.500</a></span> | <span class="t">Just imagine we're processing some text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1052" target="_blank">00:17:32.100</a></span> | <span class="t">The core shift in perspective here is that now we're going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1055" target="_blank">00:17:35.820</a></span> | <span class="t">scoring with respect not to some other text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1058" target="_blank">00:17:38.860</a></span> | <span class="t">but rather with respect to our entire vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1062" target="_blank">00:17:42.460</a></span> | <span class="t">Here I have a small vocabulary of just seven items,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1065" target="_blank">00:17:45.780</a></span> | <span class="t">but of course you could have tens of thousands of items.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1068" target="_blank">00:17:48.660</a></span> | <span class="t">That's important for SPLADE because we're going to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1071" target="_blank">00:17:51.060</a></span> | <span class="t">very sparse representations by comparison</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1073" target="_blank">00:17:53.820</a></span> | <span class="t">with cross-encoders, DPR and Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1077" target="_blank">00:17:57.300</a></span> | <span class="t">Here's how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1078" target="_blank">00:17:58.980</a></span> | <span class="t">We're going to form like with Colbert,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1080" target="_blank">00:18:00.940</a></span> | <span class="t">a matrix of scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1082" target="_blank">00:18:02.220</a></span> | <span class="t">but now the scoring is with respect to tokens in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1085" target="_blank">00:18:05.060</a></span> | <span class="t">the sequence that we're processing and all of our vocabulary items.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1089" target="_blank">00:18:09.300</a></span> | <span class="t">The scoring function for that is detailed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1091" target="_blank">00:18:11.860</a></span> | <span class="t">I've depicted it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1092" target="_blank">00:18:12.980</a></span> | <span class="t">You should think of it as a bunch of neural layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1096" target="_blank">00:18:16.060</a></span> | <span class="t">that help you represent all of these comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1099" target="_blank">00:18:19.420</a></span> | <span class="t">You do all of that work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1101" target="_blank">00:18:21.020</a></span> | <span class="t">and then the SPLADE scoring function is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1103" target="_blank">00:18:23.260</a></span> | <span class="t">the sparsification of the scores that we get out of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1107" target="_blank">00:18:27.060</a></span> | <span class="t">That's depicted here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1108" target="_blank">00:18:28.420</a></span> | <span class="t">The essential insight is that with this SPLADE function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1112" target="_blank">00:18:32.060</a></span> | <span class="t">we're going to get a score for every vocabulary item</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1115" target="_blank">00:18:35.100</a></span> | <span class="t">with respect to the sequence that we have processed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1117" target="_blank">00:18:37.940</a></span> | <span class="t">That's what's depicted in orange here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1120" target="_blank">00:18:40.260</a></span> | <span class="t">You should think of this orange thing as a vector with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1123" target="_blank">00:18:43.980</a></span> | <span class="t">the same dimensionality as our vocabulary giving what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1127" target="_blank">00:18:47.540</a></span> | <span class="t">probably very sparse scores for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1130" target="_blank">00:18:50.220</a></span> | <span class="t">our sequence with respect to everything in that vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1134" target="_blank">00:18:54.020</a></span> | <span class="t">Again, we do that for queries and for documents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1136" target="_blank">00:18:56.940</a></span> | <span class="t">and then the similarity function that's at the heart of all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1139" target="_blank">00:18:59.660</a></span> | <span class="t">these models is now SIMSPLADE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1142" target="_blank">00:19:02.420</a></span> | <span class="t">which is a dot product between the SPLADE representation for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1145" target="_blank">00:19:05.900</a></span> | <span class="t">the query and the SPLADE representation for the document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1149" target="_blank">00:19:09.820</a></span> | <span class="t">These are big long sparse vectors and we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1152" target="_blank">00:19:12.340</a></span> | <span class="t">the dot product of them for scoring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1154" target="_blank">00:19:14.900</a></span> | <span class="t">The loss is our usual negative log likelihood plus importantly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1159" target="_blank">00:19:19.780</a></span> | <span class="t">a regularization term that leads to sparse balance scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1163" target="_blank">00:19:23.860</a></span> | <span class="t">which I think is an important modification given how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1166" target="_blank">00:19:26.580</a></span> | <span class="t">different the SPLADE representations are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1168" target="_blank">00:19:28.820</a></span> | <span class="t">compared to the others we've discussed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1171" target="_blank">00:19:31.660</a></span> | <span class="t">But this is an incredibly powerful model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1173" target="_blank">00:19:33.900</a></span> | <span class="t">and I love this perspective where we're now even further back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1177" target="_blank">00:19:37.980</a></span> | <span class="t">to original IR insights about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1180" target="_blank">00:19:40.540</a></span> | <span class="t">the vocabulary and term matching is so important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1183" target="_blank">00:19:43.220</a></span> | <span class="t">But again, it's happening in this very rich neural space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1187" target="_blank">00:19:47.140</a></span> | <span class="t">it's defined by this grid of scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1190" target="_blank">00:19:50.420</a></span> | <span class="t">I'm not going to go through this slide in detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1193" target="_blank">00:19:53.740</a></span> | <span class="t">but I couldn't resist mentioning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1195" target="_blank">00:19:55.260</a></span> | <span class="t">a bunch of other recent developments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1197" target="_blank">00:19:57.580</a></span> | <span class="t">They are biased toward Colbert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1199" target="_blank">00:19:59.660</a></span> | <span class="t">because I'm biased toward Colbert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1201" target="_blank">00:20:01.580</a></span> | <span class="t">But I think the list does point to a general set of directions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1206" target="_blank">00:20:06.100</a></span> | <span class="t">around making systems more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1208" target="_blank">00:20:08.260</a></span> | <span class="t">and also making them more multilingual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1210" target="_blank">00:20:10.580</a></span> | <span class="t">That can happen with things like distillation and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1213" target="_blank">00:20:13.740</a></span> | <span class="t">also innovative ways of training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1215" target="_blank">00:20:15.580</a></span> | <span class="t">the models and setting up new objectives for them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1218" target="_blank">00:20:18.460</a></span> | <span class="t">while balancing lots of considerations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1220" target="_blank">00:20:20.820</a></span> | <span class="t">not just accuracy, but also efficiency for these systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1225" target="_blank">00:20:25.100</a></span> | <span class="t">Tremendously exciting and active area of research for the field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1229" target="_blank">00:20:29.260</a></span> | <span class="t">To round out that point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1231" target="_blank">00:20:31.340</a></span> | <span class="t">I thought I would return to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1233" target="_blank">00:20:33.460</a></span> | <span class="t">the thing that I emphasized so much when we talked about IR metrics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1237" target="_blank">00:20:37.420</a></span> | <span class="t">which is that there is more at stake here than just accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1241" target="_blank">00:20:41.980</a></span> | <span class="t">This is from a series of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1243" target="_blank">00:20:43.860</a></span> | <span class="t">controlled experiments that we did in this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1246" target="_blank">00:20:46.540</a></span> | <span class="t">trying to get a sense for the system requirements,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1249" target="_blank">00:20:49.740</a></span> | <span class="t">latency, costs, and accuracy for a variety of systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1254" target="_blank">00:20:54.500</a></span> | <span class="t">There's no simple way to navigate this table,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1256" target="_blank">00:20:56.780</a></span> | <span class="t">so let me just highlight a few things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1258" target="_blank">00:20:58.860</a></span> | <span class="t">First, BM25 is the only model that we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1262" target="_blank">00:21:02.660</a></span> | <span class="t">even get to run with this tiny little compute budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1266" target="_blank">00:21:06.140</a></span> | <span class="t">If you are absolutely compute constrained or cost constrained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1270" target="_blank">00:21:10.380</a></span> | <span class="t">you might be forced to choose BM25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1273" target="_blank">00:21:13.140</a></span> | <span class="t">It's a reasonably effective model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1275" target="_blank">00:21:15.700</a></span> | <span class="t">But assuming you can have more heavy-duty hardware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1278" target="_blank">00:21:18.820</a></span> | <span class="t">you might think about trade-offs within the space of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1281" target="_blank">00:21:21.180</a></span> | <span class="t">possible Colbert setups and this is illuminating because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1285" target="_blank">00:21:25.140</a></span> | <span class="t">for example, these two models are pretty close in accuracy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1288" target="_blank">00:21:28.740</a></span> | <span class="t">but very far apart in terms of cost and latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1292" target="_blank">00:21:32.340</a></span> | <span class="t">You might think I can sacrifice this amount of accuracy here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1298" target="_blank">00:21:38.420</a></span> | <span class="t">to do this much in terms of reduced latency and cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1302" target="_blank">00:21:42.140</a></span> | <span class="t">Here's another such comparisons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1303" target="_blank">00:21:43.980</a></span> | <span class="t">Colbert small has latency of 206,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1307" target="_blank">00:21:47.460</a></span> | <span class="t">BT-Splayed large has latency of 46,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1310" target="_blank">00:21:50.580</a></span> | <span class="t">and costs a fraction of what the Colbert model costs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1314" target="_blank">00:21:54.020</a></span> | <span class="t">Now, the Colbert model is much more accurate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1316" target="_blank">00:21:56.740</a></span> | <span class="t">but maybe this is an affordable drop here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1319" target="_blank">00:21:59.500</a></span> | <span class="t">given the other considerations that are in play.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1322" target="_blank">00:22:02.660</a></span> | <span class="t">Here's another comparison between two BT-Splayed large models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1326" target="_blank">00:22:06.580</a></span> | <span class="t">For a modest reduction in latency that comes from running on a GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1331" target="_blank">00:22:11.820</a></span> | <span class="t">I have to pay a whole lot more money for the same accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1336" target="_blank">00:22:16.860</a></span> | <span class="t">For example, you start to see that it's very unlikely that you'd be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1340" target="_blank">00:22:20.580</a></span> | <span class="t">justify using a GPU with BT-Splayed large when it's only a modest latency reduction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1347" target="_blank">00:22:27.100</a></span> | <span class="t">but a huge ballooning in the overall cost that you pay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1350" target="_blank">00:22:30.980</a></span> | <span class="t">I think there are lots of other comparisons like this that we can make,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1354" target="_blank">00:22:34.700</a></span> | <span class="t">and we're going to talk later in the course about how we might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1357" target="_blank">00:22:37.860</a></span> | <span class="t">systemize some of these observations into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1361" target="_blank">00:22:41.140</a></span> | <span class="t">a leaderboard that takes account of all of these different pressures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1364" target="_blank">00:22:44.940</a></span> | <span class="t">IR is a wonderful playground for thinking about such trade-offs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=EDVqG86AT0Q&t=1370" target="_blank">00:22:50.060</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>