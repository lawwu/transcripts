<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 2: Transformer I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 2: Transformer I Spring 2023</h2><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0"><img src="https://i.ytimg.com/vi/yqV_YfBBtK0/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGGUgZShlMA8=&rs=AOn4CLDURqRufQl82FmGx_eTeEJxt2IWcg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./yqV_YfBBtK0.html">Whisper Transcript</a> | <a href="./transcript_yqV_YfBBtK0.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=5" target="_blank">00:00:05.840</a></span> | <span class="t">This is part two in our series on contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=8" target="_blank">00:00:08.960</a></span> | <span class="t">We've come to the heart of it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=10" target="_blank">00:00:10.160</a></span> | <span class="t">the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=11" target="_blank">00:00:11.960</a></span> | <span class="t">While we're still feeling fresh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=13" target="_blank">00:00:13.840</a></span> | <span class="t">I propose that we just dive into the core model structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">I'm going to introduce that by way of a simple example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=20" target="_blank">00:00:20.200</a></span> | <span class="t">I've got that at the bottom of the slide here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=22" target="_blank">00:00:22.040</a></span> | <span class="t">Our sentence is the rock rules,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=24" target="_blank">00:00:24.220</a></span> | <span class="t">and I've paired each one of those tokens with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=26" target="_blank">00:00:26.440</a></span> | <span class="t">a token representing its position in the string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=30" target="_blank">00:00:30.040</a></span> | <span class="t">The first thing that we do in this model is look up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=32" target="_blank">00:00:32.820</a></span> | <span class="t">each one of those tokens in its own embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=36" target="_blank">00:00:36.820</a></span> | <span class="t">For word embeddings, we look those up and get things like x47,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=40" target="_blank">00:00:40.360</a></span> | <span class="t">which is a vector corresponding to the word the.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=43" target="_blank">00:00:43.380</a></span> | <span class="t">That representation is a static word representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=47" target="_blank">00:00:47.040</a></span> | <span class="t">that's very similar conceptually to what we had in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=49" target="_blank">00:00:49.600</a></span> | <span class="t">the previous era with models like word2vec and GloVe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=53" target="_blank">00:00:53.420</a></span> | <span class="t">We do something similar for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=55" target="_blank">00:00:55.220</a></span> | <span class="t">these positional tokens here and get their vector representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=59" target="_blank">00:00:59.300</a></span> | <span class="t">Then to combine them, we simply add them together dimension-wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=63" target="_blank">00:01:03.460</a></span> | <span class="t">to get the representations that I have in green here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=66" target="_blank">00:01:06.360</a></span> | <span class="t">which you could think of as the first contextual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=69" target="_blank">00:01:09.220</a></span> | <span class="t">representations that we have in this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=72" target="_blank">00:01:12.380</a></span> | <span class="t">On the right here, I've depicted that calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=75" target="_blank">00:01:15.380</a></span> | <span class="t">for the C input part of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=78" target="_blank">00:01:18.860</a></span> | <span class="t">That's a pattern that I'm going to continue all the way up as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=81" target="_blank">00:01:21.900</a></span> | <span class="t">build this transformer block just showing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=84" target="_blank">00:01:24.420</a></span> | <span class="t">the calculations for the C dimension because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=87" target="_blank">00:01:27.140</a></span> | <span class="t">the calculations are entirely parallel for A and for B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=91" target="_blank">00:01:31.340</a></span> | <span class="t">To get C input, we simply add together x34 with P3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=95" target="_blank">00:01:35.540</a></span> | <span class="t">and that gives us C input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=98" target="_blank">00:01:38.260</a></span> | <span class="t">The next layer is the attention layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=101" target="_blank">00:01:41.360</a></span> | <span class="t">This is the part of the model that gives rise to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=103" target="_blank">00:01:43.420</a></span> | <span class="t">that famous paper title, attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=106" target="_blank">00:01:46.860</a></span> | <span class="t">The reason the paper has the title attention is all you need is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=110" target="_blank">00:01:50.060</a></span> | <span class="t">that the author saw what was happening in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=112" target="_blank">00:01:52.860</a></span> | <span class="t">the previous era with recurrent neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=115" target="_blank">00:01:55.500</a></span> | <span class="t">where people had recurrent mechanisms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=117" target="_blank">00:01:57.580</a></span> | <span class="t">and then they added a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=119" target="_blank">00:01:59.260</a></span> | <span class="t">attention mechanisms on top of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=121" target="_blank">00:02:01.280</a></span> | <span class="t">those recurrences to further connect everything to everything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=124" target="_blank">00:02:04.980</a></span> | <span class="t">What the paper title is saying is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=126" target="_blank">00:02:06.940</a></span> | <span class="t">you can get rid of those recurrent connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=129" target="_blank">00:02:09.620</a></span> | <span class="t">and rely entirely on attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=132" target="_blank">00:02:12.180</a></span> | <span class="t">Hence, attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=133" target="_blank">00:02:13.940</a></span> | <span class="t">That's an important historical note because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=135" target="_blank">00:02:15.720</a></span> | <span class="t">the transformer has many other pieces as you'll see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=139" target="_blank">00:02:19.060</a></span> | <span class="t">but they were saying in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=140" target="_blank">00:02:20.860</a></span> | <span class="t">I believe, that you could drop the recurrent mechanisms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=144" target="_blank">00:02:24.780</a></span> | <span class="t">The attention mechanism that the transformer uses is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=148" target="_blank">00:02:28.420</a></span> | <span class="t">essentially the same one that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=150" target="_blank">00:02:30.220</a></span> | <span class="t">introduced in the previous lecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=152" target="_blank">00:02:32.200</a></span> | <span class="t">coming from the pre-transformer era.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=154" target="_blank">00:02:34.440</a></span> | <span class="t">It is a dot product-based approach to attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=158" target="_blank">00:02:38.300</a></span> | <span class="t">I've summarized that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=159" target="_blank">00:02:39.740</a></span> | <span class="t">You can see in the numerator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=160" target="_blank">00:02:40.940</a></span> | <span class="t">we have C input dot product with A input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=163" target="_blank">00:02:43.640</a></span> | <span class="t">and C input dot product with B input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=166" target="_blank">00:02:46.340</a></span> | <span class="t">Let me show you what those look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=168" target="_blank">00:02:48.260</a></span> | <span class="t">Here, I've got depicted each dot product is a dot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=173" target="_blank">00:02:53.280</a></span> | <span class="t">and the arrows going into it correspond to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=175" target="_blank">00:02:55.720</a></span> | <span class="t">the components that feed into that calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=178" target="_blank">00:02:58.500</a></span> | <span class="t">This dot here corresponds to A input combined with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=181" target="_blank">00:03:01.880</a></span> | <span class="t">C input and this one, A input with B input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=185" target="_blank">00:03:05.420</a></span> | <span class="t">We do that same thing for the B step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=188" target="_blank">00:03:08.020</a></span> | <span class="t">and then we do the same thing for the C step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=190" target="_blank">00:03:10.500</a></span> | <span class="t">The two dots that are depicted here correspond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=193" target="_blank">00:03:13.580</a></span> | <span class="t">to the two dot products that are in this numerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=196" target="_blank">00:03:16.780</a></span> | <span class="t">One new thing that they did in the transformer paper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=199" target="_blank">00:03:19.780</a></span> | <span class="t">normalize those dot products by the square root of DK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=204" target="_blank">00:03:24.300</a></span> | <span class="t">DK is the dimensionality of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=206" target="_blank">00:03:26.860</a></span> | <span class="t">It is the dimensionality of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=208" target="_blank">00:03:28.220</a></span> | <span class="t">all the representations that we have talked about so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=211" target="_blank">00:03:31.380</a></span> | <span class="t">That's a really important element of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=214" target="_blank">00:03:34.540</a></span> | <span class="t">We're going to do a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=215" target="_blank">00:03:35.660</a></span> | <span class="t">additive combinations in this model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=217" target="_blank">00:03:37.780</a></span> | <span class="t">and that means that essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=219" target="_blank">00:03:39.660</a></span> | <span class="t">every representation has to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=221" target="_blank">00:03:41.880</a></span> | <span class="t">the same dimensionality and that is DK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=224" target="_blank">00:03:44.220</a></span> | <span class="t">There is one exception to that which I will return to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=227" target="_blank">00:03:47.160</a></span> | <span class="t">but all the states that I depict on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=229" target="_blank">00:03:49.220</a></span> | <span class="t">this slide need to have dimensionality DK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=232" target="_blank">00:03:52.220</a></span> | <span class="t">What the transformer authors found is that they got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=235" target="_blank">00:03:55.740</a></span> | <span class="t">better scaling for the dot products when they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=237" target="_blank">00:03:57.860</a></span> | <span class="t">normalized by the square root of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=240" target="_blank">00:04:00.140</a></span> | <span class="t">that model dimensionality as a heuristic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=243" target="_blank">00:04:03.500</a></span> | <span class="t">Those normalized dot products give us a new vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=246" target="_blank">00:04:06.780</a></span> | <span class="t">alpha with a tilde on top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=248" target="_blank">00:04:08.720</a></span> | <span class="t">We softmax normalize that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=250" target="_blank">00:04:10.820</a></span> | <span class="t">and that gives us alpha,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=252" target="_blank">00:04:12.340</a></span> | <span class="t">which you could think of as attention scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=255" target="_blank">00:04:15.700</a></span> | <span class="t">To get the actual attention representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=258" target="_blank">00:04:18.580</a></span> | <span class="t">corresponding to this block here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=260" target="_blank">00:04:20.640</a></span> | <span class="t">we take each component of this vector alpha and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=264" target="_blank">00:04:24.440</a></span> | <span class="t">multiply it by each one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=266" target="_blank">00:04:26.260</a></span> | <span class="t">of the representations that we're attending to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=268" target="_blank">00:04:28.580</a></span> | <span class="t">Alpha 1 by A input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=270" target="_blank">00:04:30.500</a></span> | <span class="t">alpha 2 by B input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=272" target="_blank">00:04:32.300</a></span> | <span class="t">and then we sum those values together to get C attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=276" target="_blank">00:04:36.660</a></span> | <span class="t">As a reminder, we have all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=279" target="_blank">00:04:39.100</a></span> | <span class="t">dense connections for all of these different states.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=281" target="_blank">00:04:41.460</a></span> | <span class="t">I'm just showing you the calculations for C attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=285" target="_blank">00:04:45.500</a></span> | <span class="t">That's important because all those lines that are now on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=288" target="_blank">00:04:48.600</a></span> | <span class="t">the slide are really the only place at which we knit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=291" target="_blank">00:04:51.980</a></span> | <span class="t">together all of these columns which will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=294" target="_blank">00:04:54.380</a></span> | <span class="t">otherwise be operating independently of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=297" target="_blank">00:04:57.320</a></span> | <span class="t">This really gives us all the dense connections that we think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=300" target="_blank">00:05:00.480</a></span> | <span class="t">are so powerful for the transformer learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=303" target="_blank">00:05:03.360</a></span> | <span class="t">what sequences are like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=305" target="_blank">00:05:05.360</a></span> | <span class="t">Now, I do think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=307" target="_blank">00:05:07.140</a></span> | <span class="t">the representations that I have in orange are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=309" target="_blank">00:05:09.620</a></span> | <span class="t">attention representations but they're raw materials</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=312" target="_blank">00:05:12.940</a></span> | <span class="t">because they're really just recording the similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=316" target="_blank">00:05:16.260</a></span> | <span class="t">between our target representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=318" target="_blank">00:05:18.400</a></span> | <span class="t">and the representations around it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=320" target="_blank">00:05:20.580</a></span> | <span class="t">To get an actual attention representation in the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=323" target="_blank">00:05:23.900</a></span> | <span class="t">what we do is add together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=326" target="_blank">00:05:26.340</a></span> | <span class="t">these contextual representations down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=328" target="_blank">00:05:28.420</a></span> | <span class="t">here with these attention values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=330" target="_blank">00:05:30.940</a></span> | <span class="t">and that gives us the representations in yellow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=333" target="_blank">00:05:33.500</a></span> | <span class="t">see a layer, and those are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=336" target="_blank">00:05:36.180</a></span> | <span class="t">full-fledged attention-based representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=340" target="_blank">00:05:40.020</a></span> | <span class="t">I've depicted the calculation over here and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=342" target="_blank">00:05:42.260</a></span> | <span class="t">includes a nice reminder that we actually apply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=344" target="_blank">00:05:44.300</a></span> | <span class="t">dropout to the sum of the orange and the green.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=348" target="_blank">00:05:48.980</a></span> | <span class="t">Dropout is a simple regularization technique that will help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=352" target="_blank">00:05:52.220</a></span> | <span class="t">the model to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=353" target="_blank">00:05:53.340</a></span> | <span class="t">diverse representations as part of its training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=356" target="_blank">00:05:56.700</a></span> | <span class="t">The next step is layer normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=359" target="_blank">00:05:59.700</a></span> | <span class="t">and this is simply going to help us with scaling the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=362" target="_blank">00:06:02.440</a></span> | <span class="t">We're going to adjust them so that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=364" target="_blank">00:06:04.300</a></span> | <span class="t">zero mean and a nice normal distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=366" target="_blank">00:06:06.900</a></span> | <span class="t">falling off of that zero mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=368" target="_blank">00:06:08.740</a></span> | <span class="t">and that's just a happy place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=370" target="_blank">00:06:10.640</a></span> | <span class="t">for machine learning models in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=373" target="_blank">00:06:13.700</a></span> | <span class="t">The next step is really crucially important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=377" target="_blank">00:06:17.260</a></span> | <span class="t">These are the feedforward components in the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=380" target="_blank">00:06:20.820</a></span> | <span class="t">I have depicted them as a single representation in blue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=384" target="_blank">00:06:24.220</a></span> | <span class="t">but it's really important to see that this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=386" target="_blank">00:06:26.620</a></span> | <span class="t">hiding two feedforward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=389" target="_blank">00:06:29.480</a></span> | <span class="t">We take CA norm in purple here as the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=392" target="_blank">00:06:32.920</a></span> | <span class="t">and we feed that through a dense layer with parameters W1 and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=396" target="_blank">00:06:36.980</a></span> | <span class="t">B1 and we apply a ReLU activation to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=401" target="_blank">00:06:41.100</a></span> | <span class="t">That is fed into a second dense layer with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=404" target="_blank">00:06:44.180</a></span> | <span class="t">parameters W2 and bias term B2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=407" target="_blank">00:06:47.280</a></span> | <span class="t">and that gives us CFF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=409" target="_blank">00:06:49.860</a></span> | <span class="t">This is important because many of the parameters for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=413" target="_blank">00:06:53.300</a></span> | <span class="t">the transformer are actually hidden</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=415" target="_blank">00:06:55.020</a></span> | <span class="t">away in these feedforward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=417" target="_blank">00:06:57.260</a></span> | <span class="t">In fact, this is the one place where we could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=420" target="_blank">00:07:00.340</a></span> | <span class="t">depart from this dimensionality decay</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=423" target="_blank">00:07:03.280</a></span> | <span class="t">because CA norm here has dimensionality decay by design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=428" target="_blank">00:07:08.760</a></span> | <span class="t">But since we have two feedforward layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=430" target="_blank">00:07:10.920</a></span> | <span class="t">we have the opportunity to expand out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=433" target="_blank">00:07:13.660</a></span> | <span class="t">some larger dimensionality if we want as long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=436" target="_blank">00:07:16.620</a></span> | <span class="t">as the output of that goes back down to decay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=440" target="_blank">00:07:20.360</a></span> | <span class="t">As we'll see for some of these very large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=442" target="_blank">00:07:22.520</a></span> | <span class="t">deployed transformer architectures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=444" target="_blank">00:07:24.820</a></span> | <span class="t">people have seized this opportunity to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=447" target="_blank">00:07:27.420</a></span> | <span class="t">really wide internal layers in this feedforward step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=451" target="_blank">00:07:31.620</a></span> | <span class="t">Then of course, you have to collapse back down,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=453" target="_blank">00:07:33.840</a></span> | <span class="t">and that might be giving these models a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=456" target="_blank">00:07:36.100</a></span> | <span class="t">of their representational power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=458" target="_blank">00:07:38.640</a></span> | <span class="t">But we collapse back down to decay for CFF here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=462" target="_blank">00:07:42.620</a></span> | <span class="t">Then we have another addition of CA norm with CFF,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=468" target="_blank">00:07:48.300</a></span> | <span class="t">to get CFF layer here in yellow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=470" target="_blank">00:07:50.500</a></span> | <span class="t">and we have dropout applied to CFF,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=472" target="_blank">00:07:52.620</a></span> | <span class="t">that's that regularization step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=474" target="_blank">00:07:54.660</a></span> | <span class="t">Then finally, we have a layer normalization step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=477" target="_blank">00:07:57.660</a></span> | <span class="t">just as we had down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=478" target="_blank">00:07:58.900</a></span> | <span class="t">which will help us with rescaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=481" target="_blank">00:08:01.000</a></span> | <span class="t">the values that we've produced thus far,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=483" target="_blank">00:08:03.120</a></span> | <span class="t">and therefore help the model learn more effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=486" target="_blank">00:08:06.780</a></span> | <span class="t">That is the essence of the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=490" target="_blank">00:08:10.780</a></span> | <span class="t">There are few more details to add on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=493" target="_blank">00:08:13.140</a></span> | <span class="t">but I feel like this gives you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=494" target="_blank">00:08:14.460</a></span> | <span class="t">a good conceptual understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=496" target="_blank">00:08:16.380</a></span> | <span class="t">We began with position-sensitive versions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=499" target="_blank">00:08:19.660</a></span> | <span class="t">of static word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=501" target="_blank">00:08:21.820</a></span> | <span class="t">We had these attention layers down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=504" target="_blank">00:08:24.780</a></span> | <span class="t">and then we have the feedforward layers up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=507" target="_blank">00:08:27.880</a></span> | <span class="t">In between, we have some regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=510" target="_blank">00:08:30.620</a></span> | <span class="t">and some normalization of the values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=512" target="_blank">00:08:32.600</a></span> | <span class="t">but the essence of it is position sensitivity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=515" target="_blank">00:08:35.840</a></span> | <span class="t">attention, feedforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=517" target="_blank">00:08:37.900</a></span> | <span class="t">We are going to stack these blocks on top of each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=521" target="_blank">00:08:41.060</a></span> | <span class="t">and that's going to lead to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=522" target="_blank">00:08:42.100</a></span> | <span class="t">lots more representational power,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=523" target="_blank">00:08:43.740</a></span> | <span class="t">but all the blocks will follow that same rhythm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=527" target="_blank">00:08:47.420</a></span> | <span class="t">Since attention is so important for these models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=530" target="_blank">00:08:50.420</a></span> | <span class="t">I thought I would linger a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=532" target="_blank">00:08:52.020</a></span> | <span class="t">over the attention calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=534" target="_blank">00:08:54.740</a></span> | <span class="t">What I've shown you so far is the calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=537" target="_blank">00:08:57.520</a></span> | <span class="t">that I've given at the top of the slide here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=539" target="_blank">00:08:59.500</a></span> | <span class="t">which shows piecewise how all of these dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=542" target="_blank">00:09:02.040</a></span> | <span class="t">come together and get rescaled and added in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=545" target="_blank">00:09:05.020</a></span> | <span class="t">to form C-attention in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=547" target="_blank">00:09:07.940</a></span> | <span class="t">In the attention is all you need paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=549" target="_blank">00:09:09.860</a></span> | <span class="t">and in a lot of the subsequent literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=551" target="_blank">00:09:11.980</a></span> | <span class="t">that calculation is presented in this matrix format here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=556" target="_blank">00:09:16.040</a></span> | <span class="t">And if you're like me, you might not immediately see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=558" target="_blank">00:09:18.720</a></span> | <span class="t">how these two calculations correspond to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=562" target="_blank">00:09:22.460</a></span> | <span class="t">And so what I've done is just offer you some simple code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=565" target="_blank">00:09:25.660</a></span> | <span class="t">that you could get hands-on with to convince yourself</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=568" target="_blank">00:09:28.620</a></span> | <span class="t">that those two calculations are the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=571" target="_blank">00:09:31.220</a></span> | <span class="t">And that might help you bootstrap an understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=574" target="_blank">00:09:34.180</a></span> | <span class="t">of what you typically see in the literature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=576" target="_blank">00:09:36.100</a></span> | <span class="t">and then you can go forth with that more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=578" target="_blank">00:09:38.900</a></span> | <span class="t">matrix version of the calculation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=580" target="_blank">00:09:40.980</a></span> | <span class="t">secure in the knowledge that it corresponds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=584" target="_blank">00:09:44.040</a></span> | <span class="t">to the more piecewise thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=585" target="_blank">00:09:45.680</a></span> | <span class="t">that I've been depicting thus far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=588" target="_blank">00:09:48.440</a></span> | <span class="t">The other major piece that I have so far not introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=592" target="_blank">00:09:52.540</a></span> | <span class="t">is multi-headed attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=594" target="_blank">00:09:54.140</a></span> | <span class="t">So far, I have showed you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=596" target="_blank">00:09:56.060</a></span> | <span class="t">effectively single-headed attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=598" target="_blank">00:09:58.960</a></span> | <span class="t">So let's dive into what it means to be multi-headed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=601" target="_blank">00:10:01.540</a></span> | <span class="t">I'm gonna show you a worked example with three heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=605" target="_blank">00:10:05.180</a></span> | <span class="t">The idea is actually very simple,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=606" target="_blank">00:10:06.900</a></span> | <span class="t">but there are a lot of moving pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=608" target="_blank">00:10:08.980</a></span> | <span class="t">So let's try to do this by way of a simple example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=612" target="_blank">00:10:12.260</a></span> | <span class="t">I've got our usual sequence at the bottom here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=614" target="_blank">00:10:14.740</a></span> | <span class="t">the rock rules, and I've got our usual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=617" target="_blank">00:10:17.220</a></span> | <span class="t">three contextual representations given in green.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=621" target="_blank">00:10:21.100</a></span> | <span class="t">We are gonna do three parallel calculations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=625" target="_blank">00:10:25.080</a></span> | <span class="t">corresponding to our three heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=627" target="_blank">00:10:27.080</a></span> | <span class="t">Here's the first head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=628" target="_blank">00:10:28.860</a></span> | <span class="t">We do our same dot products as before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=632" target="_blank">00:10:32.000</a></span> | <span class="t">and it is effectively the same calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=634" target="_blank">00:10:34.400</a></span> | <span class="t">that leads to them with the small twist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=636" target="_blank">00:10:36.940</a></span> | <span class="t">that we have introduced a bunch of new parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=639" target="_blank">00:10:39.800</a></span> | <span class="t">into the calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=641" target="_blank">00:10:41.040</a></span> | <span class="t">Those are WQ1 for queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=645" target="_blank">00:10:45.400</a></span> | <span class="t">WK1 for keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=647" target="_blank">00:10:47.840</a></span> | <span class="t">and WV1 for values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=649" target="_blank">00:10:49.960</a></span> | <span class="t">Those are depicted in orange in this calculation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=652" target="_blank">00:10:52.160</a></span> | <span class="t">and I put them in orange to try to make it easy to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=655" target="_blank">00:10:55.120</a></span> | <span class="t">that if we simply remove all of those learned parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=658" target="_blank">00:10:58.280</a></span> | <span class="t">we get back to the dot product calculation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=661" target="_blank">00:11:01.280</a></span> | <span class="t">that I was showing you before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=662" target="_blank">00:11:02.980</a></span> | <span class="t">We've introduced these new matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=665" target="_blank">00:11:05.600</a></span> | <span class="t">to provide more representational power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=668" target="_blank">00:11:08.080</a></span> | <span class="t">inside this attention block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=671" target="_blank">00:11:11.080</a></span> | <span class="t">And the subscripts one indicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=672" target="_blank">00:11:12.900</a></span> | <span class="t">that we are dealing with parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=674" target="_blank">00:11:14.280</a></span> | <span class="t">for the first attention head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=676" target="_blank">00:11:16.100</a></span> | <span class="t">We do the same thing for our second attention head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=680" target="_blank">00:11:20.400</a></span> | <span class="t">all of those dot products,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=681" target="_blank">00:11:21.740</a></span> | <span class="t">but now augmented with those new learned parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=684" target="_blank">00:11:24.840</a></span> | <span class="t">Same thing, queries, keys, and values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=687" target="_blank">00:11:27.760</a></span> | <span class="t">but now two for the second attention head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=691" target="_blank">00:11:31.680</a></span> | <span class="t">And we repeat exactly the same thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=693" target="_blank">00:11:33.600</a></span> | <span class="t">for the third attention head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=695" target="_blank">00:11:35.400</a></span> | <span class="t">again with parameters corresponding to that third head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=699" target="_blank">00:11:39.680</a></span> | <span class="t">And then to actually get back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=701" target="_blank">00:11:41.320</a></span> | <span class="t">to the attention representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=702" target="_blank">00:11:42.920</a></span> | <span class="t">that I was showing you before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=704" target="_blank">00:11:44.560</a></span> | <span class="t">we kind of reassemble the pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=706" target="_blank">00:11:46.760</a></span> | <span class="t">So here is the attention representation for A,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=710" target="_blank">00:11:50.640</a></span> | <span class="t">here it is for B, and here it is for C.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=713" target="_blank">00:11:53.900</a></span> | <span class="t">We've pieced together from all the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=715" target="_blank">00:11:55.820</a></span> | <span class="t">that we did down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=717" target="_blank">00:11:57.240</a></span> | <span class="t">these three separate representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=719" target="_blank">00:11:59.840</a></span> | <span class="t">And those are what was depicted in orange</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=722" target="_blank">00:12:02.400</a></span> | <span class="t">on the previous slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=723" target="_blank">00:12:03.800</a></span> | <span class="t">But now you can see that implicitly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=726" target="_blank">00:12:06.120</a></span> | <span class="t">that was probably a multi-headed attention process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=729" target="_blank">00:12:09.740</a></span> | <span class="t">So now I think we can summarize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=733" target="_blank">00:12:13.680</a></span> | <span class="t">Maybe the one big idea that's worth repeating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=736" target="_blank">00:12:16.720</a></span> | <span class="t">is that we typically stack transformer blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=739" target="_blank">00:12:19.440</a></span> | <span class="t">on top of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=740" target="_blank">00:12:20.640</a></span> | <span class="t">So this is the first block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=742" target="_blank">00:12:22.340</a></span> | <span class="t">I've got C input coming in and C out here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=745" target="_blank">00:12:25.040</a></span> | <span class="t">but C out could be the basis for a second transformer block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=749" target="_blank">00:12:29.600</a></span> | <span class="t">where those were the inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=750" target="_blank">00:12:30.920</a></span> | <span class="t">And then of course we could repeat that process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=753" target="_blank">00:12:33.200</a></span> | <span class="t">And that is very typical to have 12, 24,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=756" target="_blank">00:12:36.720</a></span> | <span class="t">maybe even hundreds of transformer blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=759" target="_blank">00:12:39.220</a></span> | <span class="t">stacked on top of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=761" target="_blank">00:12:41.640</a></span> | <span class="t">And the other thing that's worth reminding yourself of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=763" target="_blank">00:12:43.880</a></span> | <span class="t">is that these representations in orange here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=767" target="_blank">00:12:47.020</a></span> | <span class="t">are probably not single-headed attention representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=769" target="_blank">00:12:49.920</a></span> | <span class="t">but rather multi-headed ones</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=772" target="_blank">00:12:52.000</a></span> | <span class="t">where we piece together a bunch of component pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=775" target="_blank">00:12:55.360</a></span> | <span class="t">that themselves correspond to a lot of learned parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=780" target="_blank">00:13:00.040</a></span> | <span class="t">And that is again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=781" target="_blank">00:13:01.240</a></span> | <span class="t">why this attention layer is so much a part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=784" target="_blank">00:13:04.400</a></span> | <span class="t">of the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=786" target="_blank">00:13:06.060</a></span> | <span class="t">In addition to the fact that that's the one place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=788" target="_blank">00:13:08.760</a></span> | <span class="t">where all of these columns of representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=791" target="_blank">00:13:11.320</a></span> | <span class="t">interact with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=792" target="_blank">00:13:12.800</a></span> | <span class="t">So that probably further emphasizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=795" target="_blank">00:13:15.080</a></span> | <span class="t">why the attention layer is so important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=797" target="_blank">00:13:17.080</a></span> | <span class="t">and why it's good to have lots of heads in there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=799" target="_blank">00:13:19.440</a></span> | <span class="t">offering lots of diversity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=800" target="_blank">00:13:20.820</a></span> | <span class="t">for this crucial interactional layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=803" target="_blank">00:13:23.680</a></span> | <span class="t">across the different parts of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=809" target="_blank">00:13:29.260</a></span> | <span class="t">So that is the essence of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=811" target="_blank">00:13:31.420</a></span> | <span class="t">And I hope that you are now in a position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=814" target="_blank">00:13:34.160</a></span> | <span class="t">to better understand the famous transformer diagram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=818" target="_blank">00:13:38.500</a></span> | <span class="t">that appears in the attention is all you need paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=821" target="_blank">00:13:41.380</a></span> | <span class="t">I will confess to you that I myself on first reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=823" target="_blank">00:13:43.780</a></span> | <span class="t">did not understand this diagram,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=825" target="_blank">00:13:45.780</a></span> | <span class="t">but now I feel that I do understand it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=828" target="_blank">00:13:48.820</a></span> | <span class="t">Reminder that in that paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=830" target="_blank">00:13:50.620</a></span> | <span class="t">they are dealing mainly with sequence to sequence problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=833" target="_blank">00:13:53.460</a></span> | <span class="t">so that they have an encoder and a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=836" target="_blank">00:13:56.980</a></span> | <span class="t">And so now we can see that on the encoder side here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=839" target="_blank">00:13:59.820</a></span> | <span class="t">what they've depicted is repeated for every step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=844" target="_blank">00:14:04.340</a></span> | <span class="t">in that encoder thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=845" target="_blank">00:14:05.540</a></span> | <span class="t">So every step in the sequence that we're processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=848" target="_blank">00:14:08.740</a></span> | <span class="t">And once you see that, you can see, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=850" target="_blank">00:14:10.540</a></span> | <span class="t">they've used the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=851" target="_blank">00:14:11.620</a></span> | <span class="t">I use the same colors that they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=853" target="_blank">00:14:13.220</a></span> | <span class="t">So red for the embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=855" target="_blank">00:14:15.780</a></span> | <span class="t">we have multi-headed attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=857" target="_blank">00:14:17.540</a></span> | <span class="t">additive and layer norm steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=860" target="_blank">00:14:20.220</a></span> | <span class="t">Then we have the feed forward part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=862" target="_blank">00:14:22.260</a></span> | <span class="t">more normalization and kind of adding together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=865" target="_blank">00:14:25.180</a></span> | <span class="t">of different representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=867" target="_blank">00:14:27.020</a></span> | <span class="t">That's that same rhythm that I pointed out before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=870" target="_blank">00:14:30.060</a></span> | <span class="t">That's on the encoder side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=871" target="_blank">00:14:31.820</a></span> | <span class="t">On the decoder side, things get a little more complicated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=874" target="_blank">00:14:34.620</a></span> | <span class="t">We're gonna return to some of these details,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=876" target="_blank">00:14:36.900</a></span> | <span class="t">but the important thing is that now we need to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=878" target="_blank">00:14:38.940</a></span> | <span class="t">masked attention because as we think about decoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=882" target="_blank">00:14:42.180</a></span> | <span class="t">we need to be sure that our attention layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=884" target="_blank">00:14:44.860</a></span> | <span class="t">doesn't look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=886" target="_blank">00:14:46.460</a></span> | <span class="t">We need to mask out future states</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=888" target="_blank">00:14:48.740</a></span> | <span class="t">and look only into the past when we do those dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=892" target="_blank">00:14:52.300</a></span> | <span class="t">So that's the masking down here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=893" target="_blank">00:14:53.740</a></span> | <span class="t">but otherwise the decoder has the same exact structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=897" target="_blank">00:14:57.100</a></span> | <span class="t">as the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=898" target="_blank">00:14:58.780</a></span> | <span class="t">They do have additional parameters on top here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=900" target="_blank">00:15:00.900</a></span> | <span class="t">corresponding to output probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=903" target="_blank">00:15:03.300</a></span> | <span class="t">If we're doing something like machine translation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=905" target="_blank">00:15:05.540</a></span> | <span class="t">or language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=906" target="_blank">00:15:06.540</a></span> | <span class="t">we'll have those heads on every single state in the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=910" target="_blank">00:15:10.700</a></span> | <span class="t">But if we're doing something like classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=912" target="_blank">00:15:12.900</a></span> | <span class="t">we might have those task specific parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=915" target="_blank">00:15:15.860</a></span> | <span class="t">only on one of the output states, maybe the final one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=919" target="_blank">00:15:19.260</a></span> | <span class="t">But other than that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=921" target="_blank">00:15:21.460</a></span> | <span class="t">you can see the same pieces that I've discussed before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=924" target="_blank">00:15:24.340</a></span> | <span class="t">just presented in this encoder decoder phase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=927" target="_blank">00:15:27.740</a></span> | <span class="t">So I hope that helps a little bit with the famous diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=931" target="_blank">00:15:31.540</a></span> | <span class="t">The final thing I wanted to say under this heading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=934" target="_blank">00:15:34.060</a></span> | <span class="t">is just that you can get an even deeper feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=936" target="_blank">00:15:36.460</a></span> | <span class="t">for how these models work by downloading them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=939" target="_blank">00:15:39.220</a></span> | <span class="t">and using hugging face code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=940" target="_blank">00:15:40.820</a></span> | <span class="t">to kind of inspect their structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=943" target="_blank">00:15:43.100</a></span> | <span class="t">I've done that on this slide with BERT base,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=946" target="_blank">00:15:46.300</a></span> | <span class="t">and this is really illuminating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=947" target="_blank">00:15:47.700</a></span> | <span class="t">You see a lot of the pieces that we've already discussed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=950" target="_blank">00:15:50.180</a></span> | <span class="t">This is the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=952" target="_blank">00:15:52.020</a></span> | <span class="t">It's got an embedding layer, which has word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=955" target="_blank">00:15:55.060</a></span> | <span class="t">And you can see that there are about 30,000 items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=957" target="_blank">00:15:57.540</a></span> | <span class="t">in the embedding space, each one dimensionality 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=961" target="_blank">00:16:01.180</a></span> | <span class="t">That's DK that I emphasize so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=964" target="_blank">00:16:04.580</a></span> | <span class="t">The positional embeddings, we have 512 positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=968" target="_blank">00:16:08.100</a></span> | <span class="t">So that will be our maximum sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=970" target="_blank">00:16:10.660</a></span> | <span class="t">And those by definition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=972" target="_blank">00:16:12.100</a></span> | <span class="t">have to have dimensionality 768 as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=975" target="_blank">00:16:15.460</a></span> | <span class="t">We'll return to these token type embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=977" target="_blank">00:16:17.500</a></span> | <span class="t">when we talk about BERT in particular,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=980" target="_blank">00:16:20.340</a></span> | <span class="t">but that's kind of like a positional embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=983" target="_blank">00:16:23.140</a></span> | <span class="t">Then we have layer norm and dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=984" target="_blank">00:16:24.740</a></span> | <span class="t">So that's kind of regularization of these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=987" target="_blank">00:16:27.460</a></span> | <span class="t">And then we have the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=989" target="_blank">00:16:29.420</a></span> | <span class="t">And what you can see on this slide is just the first layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=991" target="_blank">00:16:31.860</a></span> | <span class="t">It's the same structure repeated for all subsequent layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=996" target="_blank">00:16:36.060</a></span> | <span class="t">Down here, we have the attention layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=997" target="_blank">00:16:37.660</a></span> | <span class="t">You see 768 all over the place because that's DK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1001" target="_blank">00:16:41.740</a></span> | <span class="t">And the model pretty much defines for us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1003" target="_blank">00:16:43.580</a></span> | <span class="t">that we need to have that same dimensionality everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1006" target="_blank">00:16:46.660</a></span> | <span class="t">The one exception is that when we get down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1008" target="_blank">00:16:48.620</a></span> | <span class="t">into the feed forward layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1010" target="_blank">00:16:50.660</a></span> | <span class="t">we go from 768 out to 3072.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1014" target="_blank">00:16:54.500</a></span> | <span class="t">That's that intermediate part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1016" target="_blank">00:16:56.460</a></span> | <span class="t">But then we have to go from 3072 back to 768 for the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1020" target="_blank">00:17:00.900</a></span> | <span class="t">so that we can stack these components on top of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1023" target="_blank">00:17:03.940</a></span> | <span class="t">But you can see that opportunity there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1026" target="_blank">00:17:06.100</a></span> | <span class="t">to add a lot more parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1027" target="_blank">00:17:07.900</a></span> | <span class="t">and therefore a lot more representational power.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1031" target="_blank">00:17:11.980</a></span> | <span class="t">And as I said, this would continue for all the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1034" target="_blank">00:17:14.820</a></span> | <span class="t">And that's pretty much a summary of the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1037" target="_blank">00:17:17.860</a></span> | <span class="t">And you can do this for lots of different models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1040" target="_blank">00:17:20.020</a></span> | <span class="t">with Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1040" target="_blank">00:17:20.860</a></span> | <span class="t">You can check out GPT and BERT and Roberta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1043" target="_blank">00:17:23.420</a></span> | <span class="t">and all the other models we talk about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1045" target="_blank">00:17:25.260</a></span> | <span class="t">They'll differ subtly in their kind of graphs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1048" target="_blank">00:17:28.180</a></span> | <span class="t">but I expect that you'll see a lot of the core pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1051" target="_blank">00:17:31.220</a></span> | <span class="t">repeated in various flavors as you look at those models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1055" target="_blank">00:17:35.300</a></span> | <span class="t">(upbeat music)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=yqV_YfBBtK0&t=1057" target="_blank">00:17:37.900</a></span> | <span class="t">you</span></div></div></body></html>