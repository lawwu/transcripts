<html><head><title>The Fastest Way to AGI: LLMs + Tree Search – Demis Hassabis (Google DeepMind CEO)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>The Fastest Way to AGI: LLMs + Tree Search – Demis Hassabis (Google DeepMind CEO)</h2><a href="https://www.youtube.com/watch?v=eqXfhejDeqA" target="_blank"><img src="https://i.ytimg.com/vi_webp/eqXfhejDeqA/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>I obviously DeepMind is at the frontier and has been for many years with systems like AlphaZero and so forth of having these agents who can think through different steps to get to an end outcome. Will this just be is a path for LLMs to have this sort of tree search kind of thing on top of them?</p><p>Basically making them more and more accurate predictors of the world. So in effect making them more and more reliable world models. That's clearly a necessary but I would say probably not sufficient component of an AGI system. And then on top of that we're working on things like AlphaZero like planning mechanisms on top that make use of that model in order to make concrete plans to achieve certain goals in the world and perhaps sort of chain thought together or lines of reasoning together and maybe use search to kind of explore massive spaces of possibility.</p><p>I think that's kind of missing from our current large models. Is there any potential for the AGI to eventually come from just a pure RL approach? Like the way we're talking about it, it sounds like the LLM will form right prior completely out of the blue. There's no reason why you couldn't go full AlphaZero like on it.</p><p>And there are some people here at Google DeepMind and in the RL community who work on that. Fully assuming no priors, no data, and just build all knowledge from scratch. And I think that's valuable because of course those ideas and those algorithms should also work when you have some knowledge too.</p><p>But having said that, I think by far probably my betting would be the quickest way to get to AGI and the most likely plausible way is to use all the knowledge that's existing in the world right now on things like the web and that we've collected and we have these scalable algorithms like transformers that are capable of ingesting all of that information.</p><p>You can't start with a model as a kind of prior or to build on and to make predictions that helps bootstrap your learning. I just think it doesn't make sense not to make use of that. So my betting would be is that the final AGI system will have these large multimodals models as part of the overall solution, but probably won't be enough on their own.</p><p>So that's my planning search on top. How do you get past the sort of immense amount of compute that these approaches tend to require? So even the off-the-go system was a pretty expensive system because you had to do this sort of running an LLM on each node of the tree.</p><p>How do you anticipate that will get made more efficient? And also just looking at more efficient ways. I mean, the better your world model is, the more efficient your search can be. So one example I always give with AlphaZero, our system to play Go and chess and any game is that it's stronger than world champion level, human world champion level at all these games.</p><p>And it uses a lot less search than a brute force method like Deep Blue, say, to play chess. Traditional Stockfish or Deep Blue systems would maybe look at millions of possible moves for every decision it's going to make. AlphaZero and AlphaGo looked at around tens of thousands of possible positions in order to make a decision about what to move next.</p><p>But a human grandmaster, a human world champion, probably only looks at a few hundreds of moves, even the top ones, in order to make their very good decision about what to play next. So that suggests that obviously the brute force systems don't have any real model other than the heuristics about the game.</p><p>AlphaZero has quite a decent model, but the top human players have a much richer, much more accurate model then of Go or chess. So that allows them to make world class decisions on a very small amount of search. So I think there's a sort of tradeoff there. If you improve the models, then I think your search can be more efficient, and therefore you can get further with your search.</p><p>I have two questions based on that. The first being, with AlphaGo, you had a very concrete win condition of, at the end of the day, do I win this game of Go or not? And you can reinforce on that. When you're just thinking of an LLM putting out thought, do you think there will be this kind of ability to discriminate in the end, whether that was a good thing to reward or not?</p><p>Well, of course, that's why we pioneered and DeepMind's sort of famous for using games as a proving ground, partly because, obviously, it's efficient to research in that domain. But the other reason is, obviously, it's extremely easy to specify a reward function, winning the game or improving the score, something like that sort of built into most games.</p><p>So that is one of the challenges of real world systems, is how does one define the right objective function, the right reward function, and the right goals, and specify them in a general way, but that's specific enough and actually points the system in the right direction.</p></div></div></body></html>