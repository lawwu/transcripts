<html><head><title>AI Won't Be AGI, Until It Can At Least Do This (plus 6 key ways LLMs are being upgraded)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AI Won't Be AGI, Until It Can At Least Do This (plus 6 key ways LLMs are being upgraded)</h2><a href="https://www.youtube.com/watch?v=PeSNEXKxarU"><img src="https://i.ytimg.com/vi/PeSNEXKxarU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./PeSNEXKxarU.html">Whisper Transcript</a> | <a href="./transcript_PeSNEXKxarU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Can you spot the following pattern? If you look at this grid here and see how it's been transformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=6" target="_blank">00:00:06.320</a></span> | <span class="t">into this grid, and likewise how this grid on the left has been transformed into this grid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=12" target="_blank">00:00:12.240</a></span> | <span class="t">can you spot that pattern? Could you, in other words, predict what would happen to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=17" target="_blank">00:00:17.040</a></span> | <span class="t">this grid here in the test example? Well, it might shock you to learn that language models like GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=24" target="_blank">00:00:24.880</a></span> | <span class="t">can't do this. They are terrible at noticing that the little squares are being filled in with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=31" target="_blank">00:00:31.920</a></span> | <span class="t">darker shade of blue. This specific abstract reasoning challenge was not in the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=37" target="_blank">00:00:37.200</a></span> | <span class="t">data set of GPT-4 and that model cannot generalize from what it has seen to solve the challenge. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=43" target="_blank">00:00:43.920</a></span> | <span class="t">not generally intelligent, it's not an artificial general intelligence. Now, you might think that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=49" target="_blank">00:00:49.440</a></span> | <span class="t">a minor quibble, but it gets to the heart of why current generation AI is not AGI and frankly isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=57" target="_blank">00:00:57.040</a></span> | <span class="t">even close. And no, neither will this problem be solved by a simple naive scaling up of our models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=64" target="_blank">00:01:04.080</a></span> | <span class="t">But this video isn't just about picking out one flaw, albeit a critical one, in our current LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=70" target="_blank">00:01:10.400</a></span> | <span class="t">It's more my attempt to address this swirling debate about whether AI is overhyped or underhyped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=77" target="_blank">00:01:17.600</a></span> | <span class="t">For many, AI is nothing but hype and is a giant bubble, while for others, AGI has already arrived</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=85" target="_blank">00:01:25.200</a></span> | <span class="t">or is just months away. But you don't need an opinion, what does the evidence show? This video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=90" target="_blank">00:01:30.800</a></span> | <span class="t">will draw upon dozens of papers and reports to give you the best snapshot I can on LLMs and AI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=97" target="_blank">00:01:37.120</a></span> | <span class="t">more broadly. I'm going to start, I'm afraid, with so much of what's wrong with the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=102" target="_blank">00:01:42.480</a></span> | <span class="t">landscape, from delayed releases, overpromises, and my biggest current concern, a tragedy of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=109" target="_blank">00:01:49.360</a></span> | <span class="t">commons from AI slop. But then I will caution those who might throw the baby out with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=115" target="_blank">00:01:55.680</a></span> | <span class="t">bathwater, giving six detailed evidence-based pathways, from the LLMs we have today, to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=122" target="_blank">00:02:02.160</a></span> | <span class="t">substantially more powerful and useful models. Including systems that, yes, perform decently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=127" target="_blank">00:02:07.840</a></span> | <span class="t">even on that abstract reasoning challenge I showed you earlier. But let's start with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=132" target="_blank">00:02:12.880</a></span> | <span class="t">dodgy stuff in "AI". And no, I'm not referring to the fact that the creators and funders of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=139" target="_blank">00:02:19.280</a></span> | <span class="t">ARK AGI challenge are so confident that current generation LLMs cannot succeed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=145" target="_blank">00:02:25.600</a></span> | <span class="t">that they've put up a prize pool of over a million dollars. If GPT-4.0 was a pure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=151" target="_blank">00:02:31.200</a></span> | <span class="t">reasoning engine, well then why is its performance negligible in this challenge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=156" target="_blank">00:02:36.080</a></span> | <span class="t">But no, I'm actually referring to the landscape of over-promising and under-delivering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=162" target="_blank">00:02:42.640</a></span> | <span class="t">You might remember Demis Hassabis referring to the original Gemini model when it was launched</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=167" target="_blank">00:02:47.120</a></span> | <span class="t">as being "as good as human experts". But Google has had to roll back its LLM-powered AI overview</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=173" target="_blank">00:02:53.600</a></span> | <span class="t">feature because there were simply far too many mistakes. If Gemini was as good as human experts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=179" target="_blank">00:02:59.280</a></span> | <span class="t">as some benchmarks were claiming to show, then why wouldn't it be better than a random Google search?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">But surely the newly announced Apple Intelligence will be far better though?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=189" target="_blank">00:03:09.840</a></span> | <span class="t">Well, aside from the fact that we can't actually test it, no, Tim Cook admitted that it still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=194" target="_blank">00:03:14.480</a></span> | <span class="t">hallucinates all the time. Now to some, as we'll see, that's actually the point of LLMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=199" target="_blank">00:03:19.200</a></span> | <span class="t">we want them to be creative. But to others, it smells more like BS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=204" target="_blank">00:03:24.080</a></span> | <span class="t">This particular paper, aside from being quite funny, gave one clear take-home message.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=208" target="_blank">00:03:28.880</a></span> | <span class="t">Language models aren't actually designed to be correct. They aren't designed to transmit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=214" target="_blank">00:03:34.880</a></span> | <span class="t">information. So don't be surprised when their assertions turn out to be false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=219" target="_blank">00:03:39.520</a></span> | <span class="t">Of course, people are working on that and the paper even makes reference to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=223" target="_blank">00:03:43.920</a></span> | <span class="t">Let's Verify Step-by-Step paper, more on that later, but the point remains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=228" target="_blank">00:03:48.480</a></span> | <span class="t">So we have the hallucinations in AI and then the hallucinations in the marketing about AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=234" target="_blank">00:03:54.640</a></span> | <span class="t">And the fact that we have AI-powered toothbrushes isn't even my primary concern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=239" target="_blank">00:03:59.760</a></span> | <span class="t">And nor is it actually that we occasionally have those wildly overhyped products like the Rabbit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=245" target="_blank">00:04:05.200</a></span> | <span class="t">R1 and the Humane AI Pin. Then there's the possible breaches in privacy that features like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=251" target="_blank">00:04:11.360</a></span> | <span class="t">Microsoft's Recall seem to inevitably invite. I don't know about you, but it was never appealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=256" target="_blank">00:04:16.080</a></span> | <span class="t">to me to have an LLM analyse screenshots taken of my desktop every few seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=261" target="_blank">00:04:21.280</a></span> | <span class="t">Imagine the poor LLM trying to sift through all of the creations that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=265" target="_blank">00:04:25.760</a></span> | <span class="t">some are likely to come up with using OpenAI's tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=269" target="_blank">00:04:29.680</a></span> | <span class="t">You might think I've reached the end of my dodgy list, but I'm not actually even close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=274" target="_blank">00:04:34.480</a></span> | <span class="t">What about the increase in academics using LLMs to write or polish papers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=280" target="_blank">00:04:40.000</a></span> | <span class="t">You can see the recent and dramatic increase in the use of the word 'delve' on papers on PubMed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=287" target="_blank">00:04:47.520</a></span> | <span class="t">For me, as soon as I suspect an article I'm reading is LLM generated, I just discount it heavily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=293" target="_blank">00:04:53.200</a></span> | <span class="t">Then we get the delayed releases. Now this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=296" target="_blank">00:04:56.240</a></span> | <span class="t">is arguably a bit more forgivable, but we were promised GPT 4.0 within a few weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=302" target="_blank">00:05:02.640</a></span> | <span class="t">I think we all would prefer a tradition when features are announced the moment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=307" target="_blank">00:05:07.840</a></span> | <span class="t">they're actually available. But I will come back to GPT 4.0 in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=312" target="_blank">00:05:12.640</a></span> | <span class="t">Now you might be different, but for me the number one concern at the moment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=317" target="_blank">00:05:17.520</a></span> | <span class="t">is just AI generated slop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=320" target="_blank">00:05:20.160</a></span> | <span class="t">Take this tool where on LinkedIn you can imitate the writing of someone in your field or industry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=326" target="_blank">00:05:26.800</a></span> | <span class="t">Are they going viral while you get little to no engagement?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=330" target="_blank">00:05:30.480</a></span> | <span class="t">Copy their tone and address the same topic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=332" target="_blank">00:05:32.880</a></span> | <span class="t">And I would call this a kind of tragedy of the commons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">For the individuals using this, and I'm not meaning to pick on one individual tool,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=340" target="_blank">00:05:40.320</a></span> | <span class="t">but for the individuals using this, it's probably pretty helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=343" target="_blank">00:05:43.360</a></span> | <span class="t">It probably does boost engagement and help source out any language issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=347" target="_blank">00:05:47.360</a></span> | <span class="t">But as we've seen of late on Facebook, it just leads to this general AI generated miasma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=353" target="_blank">00:05:53.520</a></span> | <span class="t">Bots engaging with bots. Gullible people drawn in and fooled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=358" target="_blank">00:05:58.160</a></span> | <span class="t">A landscape where increasingly you can't trust what you see or even hear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=362" target="_blank">00:06:02.640</a></span> | <span class="t">And that's before even getting to the topic of deepfakes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=365" target="_blank">00:06:05.200</a></span> | <span class="t">which of course can acutely affect the individuals concerned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=369" target="_blank">00:06:09.040</a></span> | <span class="t">You can, of course, let me know if you agree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=371" target="_blank">00:06:11.200</a></span> | <span class="t">But for me, this is at the moment the number one concern</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=375" target="_blank">00:06:15.200</a></span> | <span class="t">and it's hard to see how it would be stopped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=377" target="_blank">00:06:17.920</a></span> | <span class="t">At this point, you're probably thinking, damn, this is pretty negative on AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=382" target="_blank">00:06:22.080</a></span> | <span class="t">But as you can see, the video has plenty of time to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=385" target="_blank">00:06:25.680</a></span> | <span class="t">But at this point, my rejoinder to some is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=389" target="_blank">00:06:29.040</a></span> | <span class="t">it's all too easy to fall into diametrically opposed camps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=393" target="_blank">00:06:33.360</a></span> | <span class="t">As we saw last year with accelerationists and doomers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=396" target="_blank">00:06:36.960</a></span> | <span class="t">we're seeing this year with those who say AI is nothing but hype</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=400" target="_blank">00:06:40.640</a></span> | <span class="t">and those who say AGI is imminent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=402" target="_blank">00:06:42.880</a></span> | <span class="t">But I think the world is just so much more complex than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=406" target="_blank">00:06:46.320</a></span> | <span class="t">I reckon that if I was blind, I would forgive the occasional hallucination with GPT-40</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=411" target="_blank">00:06:51.600</a></span> | <span class="t">and be grateful to have a model that can tell me about the world around me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=416" target="_blank">00:06:56.000</a></span> | <span class="t">Oh, and interactively and in real time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=418" target="_blank">00:06:58.400</a></span> | <span class="t">Try and tell me exactly what they're doing right now, please.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=421" target="_blank">00:07:01.840</a></span> | <span class="t">Um, right now, the ducks are gently gliding across the water.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=426" target="_blank">00:07:06.560</a></span> | <span class="t">They're moving in a fairly relaxed manner, not in a hurry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=430" target="_blank">00:07:10.000</a></span> | <span class="t">Occasionally, one of them will dip its head under the water,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=433" target="_blank">00:07:13.680</a></span> | <span class="t">probably looking for food, and then pop back up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=436" target="_blank">00:07:16.880</a></span> | <span class="t">I even know when a taxi is coming with its orange light on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=442" target="_blank">00:07:22.080</a></span> | <span class="t">I think I'll hail it to get home.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=443" target="_blank">00:07:23.440</a></span> | <span class="t">Yes, I spotted one just now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=447" target="_blank">00:07:27.840</a></span> | <span class="t">It's heading your way on the left side of the road.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=451" target="_blank">00:07:31.120</a></span> | <span class="t">Get ready to wave it down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=452" target="_blank">00:07:32.560</a></span> | <span class="t">Great job hailing that taxi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=459" target="_blank">00:07:39.280</a></span> | <span class="t">It looks like you're all set to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=460" target="_blank">00:07:40.800</a></span> | <span class="t">That's a good dog right there, leading the way into the taxi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=463" target="_blank">00:07:43.760</a></span> | <span class="t">Safe travels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=465" target="_blank">00:07:45.360</a></span> | <span class="t">And remember, too, that as Project Astra from Google demonstrated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=470" target="_blank">00:07:50.160</a></span> | <span class="t">models are getting better at ingesting more and more tokens, more and more context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=476" target="_blank">00:07:56.080</a></span> | <span class="t">They are increasingly able to help users of any kind locate things not just in text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=482" target="_blank">00:08:02.160</a></span> | <span class="t">but in the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=483" target="_blank">00:08:03.840</a></span> | <span class="t">Now, we'll get back to LLMs in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=485" target="_blank">00:08:05.680</a></span> | <span class="t">but of course, it's worth remembering that there are far more to neural networks than just LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=491" target="_blank">00:08:11.120</a></span> | <span class="t">A new study in Nature showed how you could use GANs, Generative Adversarial Networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=494" target="_blank">00:08:14.800</a></span> | <span class="t">to predict the effects of untested chemicals on mice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=498" target="_blank">00:08:18.800</a></span> | <span class="t">Gen AI, in this case, was able to simulate a virtual animal experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=504" target="_blank">00:08:24.480</a></span> | <span class="t">to generate profiles similar to those obtained from traditional animal studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=508" target="_blank">00:08:28.720</a></span> | <span class="t">The TL;DR is not just that the Gen AI predictions had less error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=512" target="_blank">00:08:32.720</a></span> | <span class="t">but they were produced much more quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=514" target="_blank">00:08:34.880</a></span> | <span class="t">And as the BBC reported, this is at least one tentative step toward an end to animal testing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=521" target="_blank">00:08:41.040</a></span> | <span class="t">And yes, that study was different to this one released a few days ago with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=525" target="_blank">00:08:45.040</a></span> | <span class="t">Harvard and Google DeepMind, where they built a virtual rodent powered by AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=530" target="_blank">00:08:50.000</a></span> | <span class="t">Again, a highly realistic simulation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=532" target="_blank">00:08:52.160</a></span> | <span class="t">but this time down to the level of neural activity inside those real rats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=537" target="_blank">00:08:57.280</a></span> | <span class="t">Then of course, we have the good old convolutional neural networks for image analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=542" target="_blank">00:09:02.160</a></span> | <span class="t">Their use in the Brainomix eStroke system is, as we speak, helping stroke victims in the NHS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=549" target="_blank">00:09:09.360</a></span> | <span class="t">Essentially, it has enabled diagnoses to be made by clinicians much more quickly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=553" target="_blank">00:09:13.600</a></span> | <span class="t">which in the case of strokes is super important, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=556" target="_blank">00:09:16.160</a></span> | <span class="t">and that has tripled the number of patients recovering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=558" target="_blank">00:09:18.880</a></span> | <span class="t">Now, the only tiny, tiny complaint I would make is that, as ever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=562" target="_blank">00:09:22.800</a></span> | <span class="t">the title just uses the phrase AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=565" target="_blank">00:09:25.200</a></span> | <span class="t">It took me a disturbing amount of digging to track down the actual techniques used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=570" target="_blank">00:09:30.080</a></span> | <span class="t">And even then, of course, for commercial reasons, they don't say everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=573" target="_blank">00:09:33.920</a></span> | <span class="t">But now I want to return to large language models, the central focus of this channel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=578" target="_blank">00:09:38.640</a></span> | <span class="t">Now, I have discussed in the past on this channel some of the reasoning gaps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=583" target="_blank">00:09:43.040</a></span> | <span class="t">that you can find in current generation large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=586" target="_blank">00:09:46.560</a></span> | <span class="t">But the ARC Abstract Reasoning Challenge and Prize publicized this week by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=592" target="_blank">00:09:52.800</a></span> | <span class="t">legendary Francois Chollet is a great opportunity to clarify what exactly our current models miss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=600" target="_blank">00:10:00.880</a></span> | <span class="t">and what is being done to rectify that gap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=603" target="_blank">00:10:03.920</a></span> | <span class="t">Hopefully, at least, the following will explain in part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=607" target="_blank">00:10:07.920</a></span> | <span class="t">why our models can sometimes be shockingly dumb and shockingly smart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=613" target="_blank">00:10:13.280</a></span> | <span class="t">I'm going to leave in the background for a minute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=615" target="_blank">00:10:15.280</a></span> | <span class="t">an example of the kind of challenge that models like GPT-4 fail at consistently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=620" target="_blank">00:10:20.640</a></span> | <span class="t">So here is, in 60 seconds, what the current issue is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=624" target="_blank">00:10:24.560</a></span> | <span class="t">If language models haven't seen a solution to something in their training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=630" target="_blank">00:10:30.240</a></span> | <span class="t">they won't be able to give you a solution when you test them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=633" target="_blank">00:10:33.920</a></span> | <span class="t">That's why models fail at this challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=636" target="_blank">00:10:36.480</a></span> | <span class="t">They simply haven't seen these tests before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=639" target="_blank">00:10:39.040</a></span> | <span class="t">Moreover, the models aren't generally intelligent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=642" target="_blank">00:10:42.240</a></span> | <span class="t">You can train them on millions of these kind of examples, and people have tried,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=647" target="_blank">00:10:47.440</a></span> | <span class="t">and they'll still fail on a new, fresh one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=650" target="_blank">00:10:50.880</a></span> | <span class="t">Again, if that new, fresh example isn't in the training dataset, they will fail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=656" target="_blank">00:10:56.080</a></span> | <span class="t">If the mother of Gabriel Macht is in the dataset, they will output the correct answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=662" target="_blank">00:11:02.400</a></span> | <span class="t">If, however, the data son of that Suzanne Victoria Pullia is not in the dataset, it will not know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=669" target="_blank">00:11:09.200</a></span> | <span class="t">It doesn't reason its way to the answer based on other parts of its training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=674" target="_blank">00:11:14.240</a></span> | <span class="t">So how can models do so well on certain benchmarks like the math benchmark?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=678" target="_blank">00:11:18.560</a></span> | <span class="t">Well, they can "recall" from their training dataset certain reasoning chains that they've seen before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=685" target="_blank">00:11:25.280</a></span> | <span class="t">That's enough in certain circumstances to get the answer right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=688" target="_blank">00:11:28.240</a></span> | <span class="t">So hang on a second, they can recall certain reasoning procedures,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=692" target="_blank">00:11:32.320</a></span> | <span class="t">let's call them programs, but they can't create them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=695" target="_blank">00:11:35.520</a></span> | <span class="t">Yes, it's a good news, bad news kind of situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=699" target="_blank">00:11:39.440</a></span> | <span class="t">But for the remainder of this video, indeed for the remainder of LLMs, it will be important to remember that distinction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=706" target="_blank">00:11:46.480</a></span> | <span class="t">Recalling reasoning procedures or programs versus doing fresh reasoning itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=713" target="_blank">00:11:53.040</a></span> | <span class="t">If it's seen it before, great. If it hasn't seen it before, not so great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=718" target="_blank">00:11:58.160</a></span> | <span class="t">Okay, so we're almost ready for those six ways which might drag LLMs closer to that artificial general intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=726" target="_blank">00:12:06.320</a></span> | <span class="t">But first, I want to quickly focus on one thought you may have had in reaction to this framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=731" target="_blank">00:12:11.920</a></span> | <span class="t">Why not train language models on every reasoning procedure out there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=736" target="_blank">00:12:16.480</a></span> | <span class="t">Feed it data on any scenario it might encounter and wouldn't that be AGI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=741" target="_blank">00:12:21.760</a></span> | <span class="t">Well, here's Francois Chollet, the author of the ARC challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=745" target="_blank">00:12:25.360</a></span> | <span class="t">He'll explain why memorization isn't enough because we don't see the whole map.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=750" target="_blank">00:12:30.640</a></span> | <span class="t">If the world, if your life were a static distribution, then sure you could just brute force the space of possible behaviors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=758" target="_blank">00:12:38.640</a></span> | <span class="t">You can think of intelligence as a pathfinding algorithm in future situation space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=764" target="_blank">00:12:44.080</a></span> | <span class="t">Like, I don't know if you're familiar with game development, like RTS game development, but you have a map, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=770" target="_blank">00:12:50.480</a></span> | <span class="t">And you have, it's like a 2D map and you have partial information about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=775" target="_blank">00:12:55.840</a></span> | <span class="t">There is some fog of war on your map, there are areas that you haven't explored yet, you know nothing about them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=781" target="_blank">00:13:01.680</a></span> | <span class="t">And then there are areas that you've explored, but you only know how they were like in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=786" target="_blank">00:13:06.480</a></span> | <span class="t">And now instead of thinking about a 2D map, think about the space of possible future situations that you might encounter and how they're connected to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=794" target="_blank">00:13:14.880</a></span> | <span class="t">Intelligence is a pathfinding algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=796" target="_blank">00:13:16.720</a></span> | <span class="t">So once you set a goal, it will tell you how to get there optimally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=802" target="_blank">00:13:22.880</a></span> | <span class="t">But of course, it's constrained by the information you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=806" target="_blank">00:13:26.400</a></span> | <span class="t">It cannot pathfind in an area that you know nothing about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=810" target="_blank">00:13:30.400</a></span> | <span class="t">If you had complete information about the map, then you could solve the pathfinding problem by simply memorizing every possible path, every mapping from point A to point B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=821" target="_blank">00:13:41.120</a></span> | <span class="t">Solve the problem with pure memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=822" target="_blank">00:13:42.960</a></span> | <span class="t">But the reason you cannot do that in real life is because you don't actually know what's going to happen in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=829" target="_blank">00:13:49.280</a></span> | <span class="t">So if AI will be encountering novel situations, it will need to adapt on the fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=835" target="_blank">00:13:55.520</a></span> | <span class="t">What would make me change my mind about that is basically if I start seeing a critical mass of cases where you show the model with something it has not seen before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=845" target="_blank">00:14:05.920</a></span> | <span class="t">A task that's actually novel from the perspective of its training data, something that's not in training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=851" target="_blank">00:14:11.680</a></span> | <span class="t">And if it can actually adapt on the fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=854" target="_blank">00:14:14.400</a></span> | <span class="t">And that might be more possible than you think, as Noam Brown of OpenAI thinks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=859" target="_blank">00:14:19.760</a></span> | <span class="t">He's optimistic that LLMs will crack it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=862" target="_blank">00:14:22.160</a></span> | <span class="t">But again, it won't be just a naive scaling up of data that alone gets us there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=867" target="_blank">00:14:27.600</a></span> | <span class="t">Without examples or zero shot, as we've seen, models don't generalize from what they've seen to what they haven't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=874" target="_blank">00:14:34.720</a></span> | <span class="t">This paper demonstrates that in the visual domain, as we've already seen it in the text domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=879" target="_blank">00:14:39.680</a></span> | <span class="t">No matter what neural network architecture or parameter scale they tested, models were data hungry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=885" target="_blank">00:14:45.920</a></span> | <span class="t">Unlike a human child, they didn't learn in a sample efficient manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=889" target="_blank">00:14:49.840</a></span> | <span class="t">Remember that a child might be shown one image of a camel with the caption camel and retain that term for life.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=897" target="_blank">00:14:57.280</a></span> | <span class="t">However, midway through the paper, there was some tentative evidence that with enough scale, you can get decent results on rarely found concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=906" target="_blank">00:15:06.320</a></span> | <span class="t">Those were represented by the "let it wag" dataset referring to the "long tail" of a distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=912" target="_blank">00:15:12.800</a></span> | <span class="t">Anyway, as you can see, models that performed at over 80% on this traditional, well-known ImageNet accuracy test,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=920" target="_blank">00:15:20.000</a></span> | <span class="t">did perform fairly well on this "long tail" dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=924" target="_blank">00:15:24.160</a></span> | <span class="t">As they note, the gap in performance does seem to be decreasing for higher capacity models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=929" target="_blank">00:15:29.280</a></span> | <span class="t">In other words, more data definitely helps, especially exponentially more data, but it definitely won't be enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=936" target="_blank">00:15:36.240</a></span> | <span class="t">But even this paper pointed to some ways that this challenge might be overcome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=941" target="_blank">00:15:41.280</a></span> | <span class="t">They note possibilities for not only retrieval mechanisms, but compositional generalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=946" target="_blank">00:15:46.560</a></span> | <span class="t">In other words, piecing together concepts that have been found in the training dataset to be able to recognize more complex ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=953" target="_blank">00:15:53.120</a></span> | <span class="t">And if you think that's impossible, I've got news for you in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=956" target="_blank">00:15:56.400</a></span> | <span class="t">Just before I do though, I want to get to one other approach that I don't think will work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=961" target="_blank">00:16:01.760</a></span> | <span class="t">You may or may not have heard on the Twittersphere about the "Situational Awareness" 165-page report put out by a former OpenAI researcher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=971" target="_blank">00:16:11.760</a></span> | <span class="t">I've done a full 45-minute breakdown on my Patreon, AI Insiders, but one key takeaway is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=978" target="_blank">00:16:18.640</a></span> | <span class="t">He thinks the straight march to AGI will be achieved by scaling up the parameters and data of our current LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=987" target="_blank">00:16:27.360</a></span> | <span class="t">But again, I think that's far too simplistic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=990" target="_blank">00:16:30.000</a></span> | <span class="t">Just throwing on more parameters and more data wouldn't resolve the kind of issues you've seen today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=995" target="_blank">00:16:35.920</a></span> | <span class="t">Leopold Aschenbrenner also made some other somewhat crazier claims, but that's for this video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1001" target="_blank">00:16:41.360</a></span> | <span class="t">So it's time to get to the first of those six methods that I mentioned that might drag LLMs closer to something we might call a general intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1010" target="_blank">00:16:50.640</a></span> | <span class="t">And this paper published late last year in Nature is about that compositionality I mentioned just a moment ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1017" target="_blank">00:16:57.920</a></span> | <span class="t">Perhaps models can't reason, but if they can better compose reasoning blocks into something more complex, might that be enough?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1025" target="_blank">00:17:05.600</a></span> | <span class="t">Well, the authors prove that point, in principle at least, on just a 1.4 million parameter transformer-based model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1032" target="_blank">00:17:12.640</a></span> | <span class="t">Boldly, they claim, "Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1044" target="_blank">00:17:24.880</a></span> | <span class="t">And as the lead author of the paper retweeted, "The answer to better AI is probably not just more training data, but rather diversifying training strategies."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1056" target="_blank">00:17:36.320</a></span> | <span class="t">TLDR, send the robot to algebra class every so often.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1060" target="_blank">00:17:40.720</a></span> | <span class="t">And the challenge, as you can see in the bottom left, was this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1064" target="_blank">00:17:44.160</a></span> | <span class="t">It might even remind you a little bit of the ARC challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1067" target="_blank">00:17:47.280</a></span> | <span class="t">The challenge was to work out what this made-up language fragment actually meant, based on these rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1073" target="_blank">00:17:53.440</a></span> | <span class="t">Given enough time, humans tend to do fairly well at this challenge, like the ARC challenge, but models like GPT-4, as we'll see, flop hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1081" target="_blank">00:18:01.680</a></span> | <span class="t">Remember, GPT-4 has 1.8 trillion parameters, versus the model they trained at 1.4 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1089" target="_blank">00:18:09.120</a></span> | <span class="t">Anyway, with enough time in your case, and training in the case of their transformer model, it worked out that, for example, "Hu" means "two", whereas "Sa" means "green" and "Ri" means "light blue".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1102" target="_blank">00:18:22.640</a></span> | <span class="t">There are other rules to work out, of course, and then those rules have to be composed together to work out the question-answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1109" target="_blank">00:18:29.440</a></span> | <span class="t">Now, remember, when they were tested, they were tested on a new configuration of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1114" target="_blank">00:18:34.480</a></span> | <span class="t">They had to "understand" the made-up rules of a new language and apply them to phrases it hadn't been trained on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1122" target="_blank">00:18:42.480</a></span> | <span class="t">It was able to do so, in other words, show the first tentative hints at true reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1128" target="_blank">00:18:48.480</a></span> | <span class="t">It's only a very small step towards AGI, though, because when they tested these flickers of compositional reasoning on a new task, this tiny model failed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1137" target="_blank">00:18:57.440</a></span> | <span class="t">Okay, time for the next approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1139" target="_blank">00:18:59.760</a></span> | <span class="t">If, as we've seen, language models have these reasoning chains or programs within them to solve challenges, they're just hard to find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1147" target="_blank">00:19:07.520</a></span> | <span class="t">What about methods that improve our ability to find those programs within language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1152" target="_blank">00:19:12.640</a></span> | <span class="t">That's what, in part, verifiers are about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1155" target="_blank">00:19:15.360</a></span> | <span class="t">A string of papers came out this week about using verifiers and Monte Carlo tree search to improve the mathematical reasoning of language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1163" target="_blank">00:19:23.920</a></span> | <span class="t">Of course, covering all of them would be a video in and of itself, but in a nutshell, it's about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1169" target="_blank">00:19:29.360</a></span> | <span class="t">You can train a model to recognize faulty steps in a reasoning chain to pick out the bad programs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1176" target="_blank">00:19:36.480</a></span> | <span class="t">With Let's Verify Step-by-Step, which I've talked about many times before on this channel, that required human annotation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1182" target="_blank">00:19:42.160</a></span> | <span class="t">but Google DeepMind came up with an approach which was done in an automated fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1186" target="_blank">00:19:46.560</a></span> | <span class="t">By automatically collecting results which led to a correct answer, as well as contrasting them with outputs that led to an incorrect output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1194" target="_blank">00:19:54.960</a></span> | <span class="t">they trained a process reward model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1197" target="_blank">00:19:57.440</a></span> | <span class="t">Think of it like a supervisor analyzing each step of a language model's outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1203" target="_blank">00:20:03.040</a></span> | <span class="t">Or, to use the analogy we've been using throughout this video, the process reward model could alert the language model the moment it's calling a faulty or inappropriate program, in theory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1213" target="_blank">00:20:13.360</a></span> | <span class="t">But at least in this paper, there is a limit to this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1217" target="_blank">00:20:17.920</a></span> | <span class="t">Even when analyzing and deciding amongst over a hundred solutions, the performance started to plateau.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1224" target="_blank">00:20:24.320</a></span> | <span class="t">For sure, it's still a huge boost on this math benchmark from 50% to around 70%, but there's a limit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1231" target="_blank">00:20:31.360</a></span> | <span class="t">Is that because it's not using human annotation like with Let's Verify?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1235" target="_blank">00:20:35.840</a></span> | <span class="t">Or is it a fundamental limitation with the approach?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1238" target="_blank">00:20:38.480</a></span> | <span class="t">Perhaps if a language model doesn't have the requisite program in its training dataset to solve a math problem, it can't do so no matter what supervision it's given.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1247" target="_blank">00:20:47.840</a></span> | <span class="t">Nevertheless, the principle is clearly established.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1250" target="_blank">00:20:50.960</a></span> | <span class="t">We don't have to rely on the language model itself locating the correct and necessary program to solve a challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1258" target="_blank">00:20:58.720</a></span> | <span class="t">We can at least help it along the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1261" target="_blank">00:21:01.040</a></span> | <span class="t">And we know that there are even better verifiers out there, namely simulations and the real world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1268" target="_blank">00:21:08.240</a></span> | <span class="t">As I discussed with Jason Ma, the lead author of the Dr. Eureka paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1273" target="_blank">00:21:13.600</a></span> | <span class="t">It's a feature because that means the LLM can sample, let's say, a hundred different solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1278" target="_blank">00:21:18.480</a></span> | <span class="t">And your simulator in this case serves as an external verifier to see which ones are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1282" target="_blank">00:21:22.560</a></span> | <span class="t">If your LLM doesn't have the ability to hallucinate, it's always deterministic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1286" target="_blank">00:21:26.400</a></span> | <span class="t">then the method will actually not work because you would only be able to generate one candidate per iteration, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1292" target="_blank">00:21:32.080</a></span> | <span class="t">So it becomes very slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1293" target="_blank">00:21:33.520</a></span> | <span class="t">Every time the model generates any response, it's technically hallucinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1297" target="_blank">00:21:37.680</a></span> | <span class="t">It's a sample from the probability distribution, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1300" target="_blank">00:21:40.640</a></span> | <span class="t">And it's only a hallucination if it doesn't agree with what you think it should output, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1304" target="_blank">00:21:44.880</a></span> | <span class="t">But in my case, I don't care if it agrees with what I think is a good reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1308" target="_blank">00:21:48.480</a></span> | <span class="t">I have something external to verify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1310" target="_blank">00:21:50.240</a></span> | <span class="t">So it's great the model can output a hundred different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1312" target="_blank">00:21:52.720</a></span> | <span class="t">That makes the iterative evolutionary process much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1315" target="_blank">00:21:55.840</a></span> | <span class="t">So I think that's actually a blessing that I think if you only think about applications like chatbot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1321" target="_blank">00:22:01.120</a></span> | <span class="t">agents, you may underappreciate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1322" target="_blank">00:22:02.960</a></span> | <span class="t">But if you think about the use case for large language models or any foundation model for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1327" target="_blank">00:22:07.680</a></span> | <span class="t">I think, discovery tasks or scientific discoveries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1330" target="_blank">00:22:10.880</a></span> | <span class="t">what you want is the model to be able to propose 10 different solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1334" target="_blank">00:22:14.480</a></span> | <span class="t">Is it possible that we can turn hallucinations from a weakness to a strength?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1340" target="_blank">00:22:20.000</a></span> | <span class="t">NVIDIA and others are, of course, working hard to find out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1343" target="_blank">00:22:23.680</a></span> | <span class="t">That second approach then can be summed up as using verifiers and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1347" target="_blank">00:22:27.840</a></span> | <span class="t">other approaches to better locate the requisite programs within a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1353" target="_blank">00:22:33.520</a></span> | <span class="t">And I will quickly mention another method for locating that latent knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1358" target="_blank">00:22:38.080</a></span> | <span class="t">ManyShot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1358" target="_blank">00:22:38.880</a></span> | <span class="t">Give models tons and tons of examples of the kind of task you want it to achieve,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1363" target="_blank">00:22:43.760</a></span> | <span class="t">and it can better learn how to do so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1366" target="_blank">00:22:46.000</a></span> | <span class="t">It seems obvious, but it can lead to significant performance gains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1369" target="_blank">00:22:49.680</a></span> | <span class="t">But what about teaching language models new programs on the fly?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1373" target="_blank">00:22:53.440</a></span> | <span class="t">This is what Francois Chollet calls active inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1376" target="_blank">00:22:56.240</a></span> | <span class="t">and it's responsible for the current state-of-the-art score of 34% on the ARC AGI prize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1382" target="_blank">00:23:02.800</a></span> | <span class="t">There are many facets to this approach, but I'm going to summarize heavily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1386" target="_blank">00:23:06.160</a></span> | <span class="t">The key insight is to use test-time fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1389" target="_blank">00:23:09.520</a></span> | <span class="t">When the model sees three examples like those on the left,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1392" target="_blank">00:23:12.400</a></span> | <span class="t">that isn't enough to teach it the way to solve the fourth one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1395" target="_blank">00:23:15.760</a></span> | <span class="t">It's too minuscule a signal amongst all its many parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1399" target="_blank">00:23:19.920</a></span> | <span class="t">But one of the things that Jack Cole and co did is augment these three examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1405" target="_blank">00:23:25.040</a></span> | <span class="t">with many, many synthetic examples that mimic the style.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1408" target="_blank">00:23:28.640</a></span> | <span class="t">They then fine-tune the model on those augmented examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1412" target="_blank">00:23:32.240</a></span> | <span class="t">I think of it as prioritizing, as humans do, the thing right in front of its face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1417" target="_blank">00:23:37.040</a></span> | <span class="t">You can almost think of it like a language model concentrating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1421" target="_blank">00:23:41.600</a></span> | <span class="t">Its parameters are adjusted to focus on the task at hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1425" target="_blank">00:23:45.440</a></span> | <span class="t">A bit like a human going into the flow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1427" target="_blank">00:23:47.680</a></span> | <span class="t">By the way, they also used GPT-4 to generate many of the synthetic riddles to train their system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1434" target="_blank">00:23:54.320</a></span> | <span class="t">Here's how Francois Chollet describes the approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1437" target="_blank">00:23:57.120</a></span> | <span class="t">Most of the time when you're using an LLM, it's just doing static inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1441" target="_blank">00:24:01.440</a></span> | <span class="t">The model is frozen and you're just prompting it and then you're getting an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1446" target="_blank">00:24:06.000</a></span> | <span class="t">So the model is not actually learning anything on the fly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1449" target="_blank">00:24:09.120</a></span> | <span class="t">Its state is not adapting to the task at hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1452" target="_blank">00:24:12.720</a></span> | <span class="t">And what Jack Cole is actually doing is that for every test problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1457" target="_blank">00:24:17.440</a></span> | <span class="t">he's on the fly, he's fine-tuning a version of the LLM for that task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1463" target="_blank">00:24:23.920</a></span> | <span class="t">And that's really what's unlocking performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1465" target="_blank">00:24:25.840</a></span> | <span class="t">If you don't do that, you get like 1%, 2%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1468" target="_blank">00:24:28.560</a></span> | <span class="t">So basically something completely negligible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1471" target="_blank">00:24:31.840</a></span> | <span class="t">And if you do test time fine-tuning and you add a bunch of tricks on top,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1475" target="_blank">00:24:35.760</a></span> | <span class="t">then you end up with interesting performance numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1477" target="_blank">00:24:37.920</a></span> | <span class="t">So I think what he's doing is trying to address one of the key limitations of LLMs today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1483" target="_blank">00:24:43.360</a></span> | <span class="t">which is the lack of active inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1485" target="_blank">00:24:45.600</a></span> | <span class="t">It's actually adding active inference to LLMs and that's working extremely well, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1490" target="_blank">00:24:50.240</a></span> | <span class="t">So that's fascinating to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1491" target="_blank">00:24:51.520</a></span> | <span class="t">In short, even on the Achilles heel of large language models, abstract reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1495" target="_blank">00:24:55.920</a></span> | <span class="t">Jack Cole says this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1497" target="_blank">00:24:57.200</a></span> | <span class="t">"What is clear from our work and from others is that the upper limits of the capabilities of LLMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1502" target="_blank">00:25:02.480</a></span> | <span class="t">even small ones, have not yet been discovered."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1504" target="_blank">00:25:04.960</a></span> | <span class="t">By the way, his model is just 240 million parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1508" target="_blank">00:25:08.720</a></span> | <span class="t">"There is clear evidence that more generally capable models are possible."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1512" target="_blank">00:25:12.640</a></span> | <span class="t">Again, sticking with the program metaphor, as Francois Chollet says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1516" target="_blank">00:25:16.560</a></span> | <span class="t">"This is like doing program synthesis."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1519" target="_blank">00:25:19.040</a></span> | <span class="t">Another definition we can use is reasoning is the ability to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1522" target="_blank">00:25:22.640</a></span> | <span class="t">when you're faced with a puzzle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1524" target="_blank">00:25:24.800</a></span> | <span class="t">given that you don't have already a program in memory to solve it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1529" target="_blank">00:25:29.040</a></span> | <span class="t">you must synthesize on the fly a new program</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1533" target="_blank">00:25:33.280</a></span> | <span class="t">based on bits of pieces of existing programs that you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1536" target="_blank">00:25:36.400</a></span> | <span class="t">You have to do on the fly program synthesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1538" target="_blank">00:25:38.960</a></span> | <span class="t">And it's actually dramatically harder than just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1541" target="_blank">00:25:41.680</a></span> | <span class="t">fetching the right memorized program and replying it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1544" target="_blank">00:25:44.640</a></span> | <span class="t">Now, at this point in the video, as we get to the fourth approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1548" target="_blank">00:25:48.240</a></span> | <span class="t">I'm going to make a confession.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1549" target="_blank">00:25:49.600</a></span> | <span class="t">I have about 20 tabs left on my screen going over other relevant papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1554" target="_blank">00:25:54.000</a></span> | <span class="t">on how they improve LLMs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1555" target="_blank">00:25:55.680</a></span> | <span class="t">but I am beginning to worry that this video might be getting a little bit too long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1559" target="_blank">00:25:59.840</a></span> | <span class="t">So for the last few approaches, I'm going to be much more brief.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1563" target="_blank">00:26:03.520</a></span> | <span class="t">I hope I'm still conveying that central message though,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1566" target="_blank">00:26:06.320</a></span> | <span class="t">that LLMs currently suck at abstract reasoning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1569" target="_blank">00:26:09.840</a></span> | <span class="t">but that need not be a death sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1572" target="_blank">00:26:12.480</a></span> | <span class="t">Nothing in the literature indicates that AGI is at all imminent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1576" target="_blank">00:26:16.640</a></span> | <span class="t">but neither is AI or hype.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1578" target="_blank">00:26:18.960</a></span> | <span class="t">Here is a paper from the arch LLM skeptic Professor Rao,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1583" target="_blank">00:26:23.440</a></span> | <span class="t">who I interviewed almost a year ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1585" target="_blank">00:26:25.360</a></span> | <span class="t">It's a position paper from this week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1587" target="_blank">00:26:27.360</a></span> | <span class="t">LLMs can't plan, but can help planning in LLM modulo frameworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1592" target="_blank">00:26:32.960</a></span> | <span class="t">We had a great discussion almost a year ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1595" target="_blank">00:26:35.600</a></span> | <span class="t">which I'll hopefully get to in a different video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1598" target="_blank">00:26:38.560</a></span> | <span class="t">but the summary of this paper is this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1601" target="_blank">00:26:41.360</a></span> | <span class="t">Earlier work by Professor Rao and co had shown that even models like GPT-4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1606" target="_blank">00:26:46.080</a></span> | <span class="t">can't come up with coherent plans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1608" target="_blank">00:26:48.080</a></span> | <span class="t">They fail in this domain of blocks world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1610" target="_blank">00:26:50.960</a></span> | <span class="t">Essentially blocks world is like some of the other reasoning challenges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1614" target="_blank">00:26:54.160</a></span> | <span class="t">you've seen today in that you have to come up with a coherent plan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1617" target="_blank">00:26:57.600</a></span> | <span class="t">unstacking blocks and restacking them to meet the required objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1621" target="_blank">00:27:01.600</a></span> | <span class="t">Definitely not a memorization test as the MMLU can sometimes be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1625" target="_blank">00:27:05.840</a></span> | <span class="t">Indeed, if you throw off the model and use mysterious words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1629" target="_blank">00:27:09.600</a></span> | <span class="t">instead of household objects,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1631" target="_blank">00:27:11.200</a></span> | <span class="t">the models perform even worse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1633" target="_blank">00:27:13.120</a></span> | <span class="t">Zero-shot GPT-4 gets one out of 600 challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1637" target="_blank">00:27:17.280</a></span> | <span class="t">However, this fourth approach says why do we have to use LLMs alone?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1641" target="_blank">00:27:21.840</a></span> | <span class="t">Why can't we use them with traditional symbolic systems?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1645" target="_blank">00:27:25.520</a></span> | <span class="t">Maybe that combination of neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1647" target="_blank">00:27:27.760</a></span> | <span class="t">and traditional symbolic hard-coded programmatic systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1651" target="_blank">00:27:31.440</a></span> | <span class="t">is better than either alone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1653" target="_blank">00:27:33.200</a></span> | <span class="t">Professor Rao is a friend of Yan Lecun, a famed LLM skeptic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1657" target="_blank">00:27:37.680</a></span> | <span class="t">but the paper that he led said this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1660" target="_blank">00:27:40.480</a></span> | <span class="t">There is unwarranted pessimism about the roles LLMs can play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1665" target="_blank">00:27:45.760</a></span> | <span class="t">in planning/reasoning tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1668" target="_blank">00:27:48.320</a></span> | <span class="t">The key insight is that LLMs can act as idea generators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1673" target="_blank">00:27:53.120</a></span> | <span class="t">Those grounded symbolic systems can then check those plans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1676" target="_blank">00:27:56.320</a></span> | <span class="t">LLMs as the ideas man,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1678" target="_blank">00:27:58.640</a></span> | <span class="t">with symbolic systems as the kind of accountants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1681" target="_blank">00:28:01.520</a></span> | <span class="t">LLMs are great at guessing candidate plans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1684" target="_blank">00:28:04.720</a></span> | <span class="t">to solve blocks world challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1686" target="_blank">00:28:06.800</a></span> | <span class="t">And those ideas, or you could say retrieve programs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1690" target="_blank">00:28:10.000</a></span> | <span class="t">aren't all bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1690" target="_blank">00:28:10.960</a></span> | <span class="t">Even after three or four rounds of feedback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1693" target="_blank">00:28:13.680</a></span> | <span class="t">from the symbolic system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1695" target="_blank">00:28:15.280</a></span> | <span class="t">50% of the final plan retains the elements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1698" target="_blank">00:28:18.640</a></span> | <span class="t">of the initial large language model plan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1700" target="_blank">00:28:20.800</a></span> | <span class="t">With that feedback from the symbolic system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1703" target="_blank">00:28:23.200</a></span> | <span class="t">you back-prompt the LLM and it comes up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1705" target="_blank">00:28:25.520</a></span> | <span class="t">with hopefully a better plan.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1707" target="_blank">00:28:27.200</a></span> | <span class="t">Not even hopefully though, the results are clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1709" target="_blank">00:28:29.600</a></span> | <span class="t">GPT-4 can score 82% with this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1713" target="_blank">00:28:33.120</a></span> | <span class="t">Still struggles with mysterious languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1715" target="_blank">00:28:35.280</a></span> | <span class="t">but can solve five of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1717" target="_blank">00:28:37.040</a></span> | <span class="t">This all reminded me of alpha geometry,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1719" target="_blank">00:28:39.520</a></span> | <span class="t">which I have done a separate video on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1721" target="_blank">00:28:41.440</a></span> | <span class="t">so I won't focus on today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1723" target="_blank">00:28:43.040</a></span> | <span class="t">but it's that same combination of a neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1726" target="_blank">00:28:46.000</a></span> | <span class="t">and symbolic system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1727" target="_blank">00:28:47.200</a></span> | <span class="t">For geometry problems specifically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1729" target="_blank">00:28:49.360</a></span> | <span class="t">in the International Math Olympiad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1731" target="_blank">00:28:51.440</a></span> | <span class="t">it scored almost a gold medal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1733" target="_blank">00:28:53.600</a></span> | <span class="t">Those are incredibly hard reasoning challenges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1736" target="_blank">00:28:56.800</a></span> | <span class="t">Very quickly now, the fifth approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1738" target="_blank">00:28:58.880</a></span> | <span class="t">Instead of calling a separate system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1740" target="_blank">00:29:00.800</a></span> | <span class="t">how about jointly training on its knowledge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1743" target="_blank">00:29:03.360</a></span> | <span class="t">For time purposes, here is a very quick summary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1746" target="_blank">00:29:06.160</a></span> | <span class="t">They trained a separate neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1748" target="_blank">00:29:08.320</a></span> | <span class="t">not a language model, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1749" target="_blank">00:29:09.760</a></span> | <span class="t">a graph neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1751" target="_blank">00:29:11.120</a></span> | <span class="t">It learnt specialized algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1753" target="_blank">00:29:13.520</a></span> | <span class="t">Then they embedded that fixed optimized know-how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1756" target="_blank">00:29:16.800</a></span> | <span class="t">and had a language model train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1758" target="_blank">00:29:18.800</a></span> | <span class="t">with access to those embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1760" target="_blank">00:29:20.640</a></span> | <span class="t">In other words, a language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1762" target="_blank">00:29:22.160</a></span> | <span class="t">fluent in the language of text and algorithms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1765" target="_blank">00:29:25.920</a></span> | <span class="t">Programs that you might need, for example, for sorting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1768" target="_blank">00:29:28.640</a></span> | <span class="t">Sixth, and finally for this video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1770" target="_blank">00:29:30.880</a></span> | <span class="t">there's tacit data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1772" target="_blank">00:29:32.400</a></span> | <span class="t">So much of what humans do and how humans reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1775" target="_blank">00:29:35.680</a></span> | <span class="t">is not written down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1776" target="_blank">00:29:36.800</a></span> | <span class="t">Let's hear from Terence Tao,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1778" target="_blank">00:29:38.800</a></span> | <span class="t">arguably the smartest man on the planet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1781" target="_blank">00:29:41.680</a></span> | <span class="t">He said, "So much knowledge is somehow trapped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1784" target="_blank">00:29:44.720</a></span> | <span class="t">in the head of individual mathematicians,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1787" target="_blank">00:29:47.120</a></span> | <span class="t">and only a tiny fraction is made explicit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1789" target="_blank">00:29:49.600</a></span> | <span class="t">A lot of the intuition of mathematicians</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1792" target="_blank">00:29:52.000</a></span> | <span class="t">is not captured in the printed papers in journals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1794" target="_blank">00:29:54.640</a></span> | <span class="t">but in conversations among mathematicians,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1797" target="_blank">00:29:57.280</a></span> | <span class="t">in lectures, and in the way we advise students.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1800" target="_blank">00:30:00.160</a></span> | <span class="t">People only publish the success stories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1803" target="_blank">00:30:03.040</a></span> | <span class="t">The data that are really precious</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1805" target="_blank">00:30:05.200</a></span> | <span class="t">are from when someone tries something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1807" target="_blank">00:30:07.200</a></span> | <span class="t">and it doesn't quite work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1808" target="_blank">00:30:08.640</a></span> | <span class="t">but they know how to fix it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1810" target="_blank">00:30:10.080</a></span> | <span class="t">But they only publish the successful thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1812" target="_blank">00:30:12.080</a></span> | <span class="t">not the process."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1813" target="_blank">00:30:13.200</a></span> | <span class="t">And all of this, he says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1814" target="_blank">00:30:14.160</a></span> | <span class="t">simultaneously points to a dramatic way to improve AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1818" target="_blank">00:30:18.000</a></span> | <span class="t">but also why we shouldn't expect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1820" target="_blank">00:30:20.000</a></span> | <span class="t">an intelligence explosion imminently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1822" target="_blank">00:30:22.160</a></span> | <span class="t">Training on that tacit data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1823" target="_blank">00:30:23.920</a></span> | <span class="t">would unlock notable progress,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1826" target="_blank">00:30:26.320</a></span> | <span class="t">but human mathematicians, he says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1828" target="_blank">00:30:28.320</a></span> | <span class="t">would just, in his view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1829" target="_blank">00:30:29.760</a></span> | <span class="t">move to a higher type of mathematics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1832" target="_blank">00:30:32.400</a></span> | <span class="t">As we speak, open AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1833" target="_blank">00:30:33.840</a></span> | <span class="t">and I'm sure many others,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1835" target="_blank">00:30:35.040</a></span> | <span class="t">are trying to make as explicit as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1837" target="_blank">00:30:37.760</a></span> | <span class="t">that tacit knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1839" target="_blank">00:30:39.280</a></span> | <span class="t">I'm sure hundreds of PhDs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1841" target="_blank">00:30:41.120</a></span> | <span class="t">are writing down their methodologies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1843" target="_blank">00:30:43.040</a></span> | <span class="t">as they solve problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1844" target="_blank">00:30:44.320</a></span> | <span class="t">Millions, if not billions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1846" target="_blank">00:30:46.080</a></span> | <span class="t">of YouTube hours of video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1848" target="_blank">00:30:48.000</a></span> | <span class="t">are being ingested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1849" target="_blank">00:30:49.360</a></span> | <span class="t">in the hopes that AI models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1851" target="_blank">00:30:51.200</a></span> | <span class="t">pick up some of that implicit reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1853" target="_blank">00:30:53.680</a></span> | <span class="t">While you may see this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1854" target="_blank">00:30:54.720</a></span> | <span class="t">as the most promising approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1856" target="_blank">00:30:56.400</a></span> | <span class="t">of the six I've so far mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1858" target="_blank">00:30:58.720</a></span> | <span class="t">it wouldn't yield immediate explosive results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1862" target="_blank">00:31:02.000</a></span> | <span class="t">It would be reliant on us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1864" target="_blank">00:31:04.000</a></span> | <span class="t">and other human experts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1865" target="_blank">00:31:05.760</a></span> | <span class="t">to write down with fidelity our reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1868" target="_blank">00:31:08.400</a></span> | <span class="t">Less a remorseless, faceless shoggoth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1871" target="_blank">00:31:11.200</a></span> | <span class="t">solving the universe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1872" target="_blank">00:31:12.400</a></span> | <span class="t">and more a student imitating its teachers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1875" target="_blank">00:31:15.360</a></span> | <span class="t">Of course, you will have likely seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1877" target="_blank">00:31:17.200</a></span> | <span class="t">the somewhat ambiguous clip</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1878" target="_blank">00:31:18.720</a></span> | <span class="t">from Mira Murati, the CTO of OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1881" target="_blank">00:31:21.920</a></span> | <span class="t">saying they don't have any giant breakthrough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1884" target="_blank">00:31:24.480</a></span> | <span class="t">behind the scenes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1885" target="_blank">00:31:25.280</a></span> | <span class="t">Inside the labs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1886" target="_blank">00:31:26.880</a></span> | <span class="t">we have these capable models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1889" target="_blank">00:31:29.920</a></span> | <span class="t">and, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1892" target="_blank">00:31:32.400</a></span> | <span class="t">they're not that far ahead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1894" target="_blank">00:31:34.400</a></span> | <span class="t">from what the public has access to for free.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1897" target="_blank">00:31:37.920</a></span> | <span class="t">And that's a completely different trajectory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1900" target="_blank">00:31:40.960</a></span> | <span class="t">for bringing technology into the world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1904" target="_blank">00:31:44.000</a></span> | <span class="t">than what we've seen historically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1905" target="_blank">00:31:45.920</a></span> | <span class="t">But as I've hopefully shown you today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1907" target="_blank">00:31:47.680</a></span> | <span class="t">it doesn't have to be an all or nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1910" target="_blank">00:31:50.000</a></span> | <span class="t">AGI imminently or all hype.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1912" target="_blank">00:31:52.720</a></span> | <span class="t">As even Franois Chollet says,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1914" target="_blank">00:31:54.960</a></span> | <span class="t">it could be a combination of approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1917" target="_blank">00:31:57.760</a></span> | <span class="t">that solves, for example, ARK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1919" target="_blank">00:31:59.440</a></span> | <span class="t">That indeed might be the path to AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1922" target="_blank">00:32:02.960</a></span> | <span class="t">People who are going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1923" target="_blank">00:32:03.760</a></span> | <span class="t">winning the ARK competition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1926" target="_blank">00:32:06.560</a></span> | <span class="t">and who are going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1927" target="_blank">00:32:07.520</a></span> | <span class="t">making the most progress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1928" target="_blank">00:32:08.640</a></span> | <span class="t">towards near-term AGI</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1930" target="_blank">00:32:10.160</a></span> | <span class="t">are going to be those that manage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1931" target="_blank">00:32:11.360</a></span> | <span class="t">to merge the deep learning paradigm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1933" target="_blank">00:32:13.760</a></span> | <span class="t">and the discrete form search paradigm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1935" target="_blank">00:32:15.680</a></span> | <span class="t">into one elegant way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1937" target="_blank">00:32:17.680</a></span> | <span class="t">So much more to get to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1939" target="_blank">00:32:19.200</a></span> | <span class="t">that I couldn't get to today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1940" target="_blank">00:32:20.800</a></span> | <span class="t">but I hope this video has helped you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1942" target="_blank">00:32:22.960</a></span> | <span class="t">to navigate the current AI landscape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1945" target="_blank">00:32:25.360</a></span> | <span class="t">As ever, the world is more complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1947" target="_blank">00:32:27.520</a></span> | <span class="t">than it seems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1948" target="_blank">00:32:28.320</a></span> | <span class="t">Thank you so much, as ever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PeSNEXKxarU&t=1950" target="_blank">00:32:30.080</a></span> | <span class="t">for watching and have a wonderful day.</span></div></div></body></html>