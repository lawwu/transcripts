<html><head><title>Llama 405b: Full 92 page Analysis, and Uncontaminated SIMPLE Benchmark Results</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Llama 405b: Full 92 page Analysis, and Uncontaminated SIMPLE Benchmark Results</h2><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE"><img src="https://i.ytimg.com/vi_webp/Tf1nooXtUHE/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./Tf1nooXtUHE.html">Whisper Transcript</a> | <a href="./transcript_Tf1nooXtUHE.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">The llama herd has left the building and is roaming the streets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=5" target="_blank">00:00:05.200</a></span> | <span class="t">More specifically, the Llama 3.1 405 billion parameter language model is out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=11" target="_blank">00:00:11.280</a></span> | <span class="t">but I thought the former expression would be a bit more dramatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=15" target="_blank">00:00:15.360</a></span> | <span class="t">The 92-page paper that came with the model was released less than 24 hours ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=21" target="_blank">00:00:21.680</a></span> | <span class="t">and yes, I've read it in full and have benchmarked the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=25" target="_blank">00:00:25.360</a></span> | <span class="t">comparing it to four competitors on over 100 private questions with 74 notes to touch on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=33" target="_blank">00:00:33.440</a></span> | <span class="t">The model is impressive and the paper is revealing, so let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=39" target="_blank">00:00:39.920</a></span> | <span class="t">There are three sizes of text-only Llama 3 models, but this video will focus almost entirely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=46" target="_blank">00:00:46.640</a></span> | <span class="t">on the biggest and best, the 405 billion parameter model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=50" target="_blank">00:00:50.400</a></span> | <span class="t">And no, Meta weren't exaggerating when they say that it delivers comparable quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=55" target="_blank">00:00:55.840</a></span> | <span class="t">to leading language models such as GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=58" target="_blank">00:00:58.720</a></span> | <span class="t">And in case you're new to the channel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=60" target="_blank">00:01:00.320</a></span> | <span class="t">I'm not just relying on those traditional benchmarks to assess that comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=65" target="_blank">00:01:05.360</a></span> | <span class="t">Meta's innovations in a nutshell were higher quality data that's filtered for quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=71" target="_blank">00:01:11.680</a></span> | <span class="t">and simply more compute, bigger scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=74" target="_blank">00:01:14.560</a></span> | <span class="t">Indeed, the sheer scale of compute, way more than 10 to the 25 floating point operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=80" target="_blank">00:01:20.480</a></span> | <span class="t">was so big that at one point the EU classified that as presenting a systemic risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=86" target="_blank">00:01:26.960</a></span> | <span class="t">So whether that scares you or hypes you, let's see the results of all of those flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=92" target="_blank">00:01:32.960</a></span> | <span class="t">Here is just a quick snapshot of a comparison on traditional benchmarks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=97" target="_blank">00:01:37.440</a></span> | <span class="t">between Llama 3.1 405B and GPT-4, GPT-4.0, Claude 3.5 Sonnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=103" target="_blank">00:01:43.440</a></span> | <span class="t">As I'll try to persuade you in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=105" target="_blank">00:01:45.200</a></span> | <span class="t">I don't think these benchmarks quite capture the nuanced differences between the models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=109" target="_blank">00:01:49.920</a></span> | <span class="t">but it certainly shows you that this new "open source" model from Meta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=114" target="_blank">00:01:54.880</a></span> | <span class="t">is on a par, if not better than GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=117" target="_blank">00:01:57.840</a></span> | <span class="t">Of course, it doesn't yet have all the fancy speech in and speech out that GPT-4 Omni does,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=124" target="_blank">00:02:04.080</a></span> | <span class="t">but technically we don't have access to that yet either for that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=128" target="_blank">00:02:08.160</a></span> | <span class="t">I do think it's worth noting though, just for 10 seconds, that we have a downloadable model now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=134" target="_blank">00:02:14.320</a></span> | <span class="t">that is as good or better than the GPT-4 that caused such waves early last year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=140" target="_blank">00:02:20.560</a></span> | <span class="t">At the time, people thought that day might take 2 years or even 5 years, but no, it's here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=146" target="_blank">00:02:26.320</a></span> | <span class="t">And yes, Meta are still arguing that this series of models charts a "responsible path"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=153" target="_blank">00:02:33.680</a></span> | <span class="t">towards the development of Artificial General Intelligence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=157" target="_blank">00:02:37.360</a></span> | <span class="t">I'll at least have a few comments on that when it comes to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=160" target="_blank">00:02:40.640</a></span> | <span class="t">my private General Intelligence benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=163" target="_blank">00:02:43.120</a></span> | <span class="t">Just quickly, why do I keep saying "open source"?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=165" target="_blank">00:02:45.600</a></span> | <span class="t">Well, according to the semi-official Open Source Initiative,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=169" target="_blank">00:02:49.520</a></span> | <span class="t">the definition of Open Source AI includes the training data provenance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=174" target="_blank">00:02:54.480</a></span> | <span class="t">where it comes from, how it was obtained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=176" target="_blank">00:02:56.880</a></span> | <span class="t">The paper on page 4 simply says "from a variety of data sources".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=182" target="_blank">00:03:02.560</a></span> | <span class="t">So even if you had the budget, you couldn't recreate LLAMA 3.1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=186" target="_blank">00:03:06.400</a></span> | <span class="t">because you simply wouldn't know what data they used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=188" target="_blank">00:03:08.880</a></span> | <span class="t">Indeed, I did a video on this for my new Coursera course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=192" target="_blank">00:03:12.080</a></span> | <span class="t">but just remember that anytime you hear that Meta is committed to Open Source AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=197" target="_blank">00:03:17.520</a></span> | <span class="t">I mean, just look how many times in this one paragraph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=200" target="_blank">00:03:20.480</a></span> | <span class="t">Mark Zuckerberg used the phrase "open source".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=202" target="_blank">00:03:22.960</a></span> | <span class="t">So why be shy about the data they're using?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=205" target="_blank">00:03:25.600</a></span> | <span class="t">Well, as the New York Times recently reported,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=208" target="_blank">00:03:28.480</a></span> | <span class="t">the data is getting harder and harder to find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=211" target="_blank">00:03:31.200</a></span> | <span class="t">Companies like Reddit and Twitter are charging for their data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=214" target="_blank">00:03:34.800</a></span> | <span class="t">and Meta may not have had permission for all of that data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=218" target="_blank">00:03:38.560</a></span> | <span class="t">And one theme you'll see throughout the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=220" target="_blank">00:03:40.960</a></span> | <span class="t">is using language models to improve the performance of language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=225" target="_blank">00:03:45.600</a></span> | <span class="t">Using LLAMA 2, for example, to filter the data used to train LLAMA 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=231" target="_blank">00:03:51.600</a></span> | <span class="t">That's just one example, there are literally dozens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=234" target="_blank">00:03:54.640</a></span> | <span class="t">So you can bet that LLAMA 3.1 is being used to help train LLAMA 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=240" target="_blank">00:04:00.640</a></span> | <span class="t">Before you predict, though, that this is setting off some form of intelligence explosion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=244" target="_blank">00:04:04.560</a></span> | <span class="t">remember that it was just yesterday that Zuckerberg admitted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=247" target="_blank">00:04:07.920</a></span> | <span class="t">that the LLAMA models are hemorrhaging Meta money.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=250" target="_blank">00:04:10.880</a></span> | <span class="t">It's hard to know in advance when something is good enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=254" target="_blank">00:04:14.240</a></span> | <span class="t">that you're going to have a product that billions of people use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=256" target="_blank">00:04:16.720</a></span> | <span class="t">and then when it's ready to kind of be a large business.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=259" target="_blank">00:04:19.760</a></span> | <span class="t">And I mean, look, we're all spending, you know, a lot of capital</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=262" target="_blank">00:04:22.560</a></span> | <span class="t">and on basically training these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=265" target="_blank">00:04:25.200</a></span> | <span class="t">So I think that people are going to be probably losing money for quite a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=268" target="_blank">00:04:28.800</a></span> | <span class="t">But I don't know, maybe that'll all happen quicker.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=271" target="_blank">00:04:31.040</a></span> | <span class="t">It's hard to know exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=273" target="_blank">00:04:33.040</a></span> | <span class="t">And even OpenAI might be losing $5 billion this year alone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=279" target="_blank">00:04:39.040</a></span> | <span class="t">That's at least according to a report released by The Information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=282" target="_blank">00:04:42.320</a></span> | <span class="t">while I was filming the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=284" target="_blank">00:04:44.320</a></span> | <span class="t">But we do know that LLAMA 4 is coming, and probably before the end of the year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=289" target="_blank">00:04:49.520</a></span> | <span class="t">How do you define AGI, and do you get there first?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=293" target="_blank">00:04:53.280</a></span> | <span class="t">Well, it's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=294" target="_blank">00:04:54.080</a></span> | <span class="t">We're basically already starting to work on LLAMA 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=298" target="_blank">00:04:58.080</a></span> | <span class="t">And our goal is to completely close the gap with all the others on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=302" target="_blank">00:05:02.400</a></span> | <span class="t">So I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=304" target="_blank">00:05:04.000</a></span> | <span class="t">I mean, do we get to AGI first?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=305" target="_blank">00:05:05.680</a></span> | <span class="t">I mean, I think that there will probably be some breakthroughs between now and then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=308" target="_blank">00:05:08.000</a></span> | <span class="t">It's hard to just predict in a straight line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=310" target="_blank">00:05:10.080</a></span> | <span class="t">Then you get to the more complicated question, which is like, what is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=313" target="_blank">00:05:13.040</a></span> | <span class="t">I don't know that there's one specific definition for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=316" target="_blank">00:05:16.720</a></span> | <span class="t">Throughout the paper, they give away their recipe for doing what they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=320" target="_blank">00:05:20.960</a></span> | <span class="t">Having read both the original LLAMA paper and the LLAMA 2 paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=324" target="_blank">00:05:24.240</a></span> | <span class="t">this is quite different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=325" target="_blank">00:05:25.520</a></span> | <span class="t">It almost feels like they're much more confident</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=328" target="_blank">00:05:28.240</a></span> | <span class="t">giving away the secrets of large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=331" target="_blank">00:05:31.280</a></span> | <span class="t">They almost don't believe that there's much of a secret source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=335" target="_blank">00:05:35.120</a></span> | <span class="t">and they're not scared of China.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=337" target="_blank">00:05:37.120</a></span> | <span class="t">And Claude 3.5 Sonnet aside, they've almost proven that with this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=342" target="_blank">00:05:42.080</a></span> | <span class="t">I must say that there was one part of the paper that I found especially sensational.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=346" target="_blank">00:05:46.800</a></span> | <span class="t">They developed scaling laws, not just for next token prediction loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=351" target="_blank">00:05:51.040</a></span> | <span class="t">but for benchmark performance, or to somewhat translate that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=354" target="_blank">00:05:54.800</a></span> | <span class="t">how long to run the GPUs to get the benchmark performance that they wanted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=360" target="_blank">00:06:00.240</a></span> | <span class="t">Given their flop budget, they predicted how the model would perform and got it right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=366" target="_blank">00:06:06.000</a></span> | <span class="t">only just slightly underestimating final performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=369" target="_blank">00:06:09.600</a></span> | <span class="t">Or in their words, this approach enables us to predict downstream task performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=374" target="_blank">00:06:14.880</a></span> | <span class="t">given a specific number of training flops for compute optimal models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=379" target="_blank">00:06:19.200</a></span> | <span class="t">They set themselves a compute budget and got the benchmark performance that they expected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=385" target="_blank">00:06:25.040</a></span> | <span class="t">It's almost a bit like you can imagine a benchmark performance dial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=389" target="_blank">00:06:29.680</a></span> | <span class="t">in Mark Zuckerberg's office that he can move clockwise at will,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=394" target="_blank">00:06:34.240</a></span> | <span class="t">while the money lasts, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=396" target="_blank">00:06:36.080</a></span> | <span class="t">These benchmark scaling laws, by the way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=398" target="_blank">00:06:38.160</a></span> | <span class="t">extrapolate across four orders of magnitude, so are pretty reliable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=402" target="_blank">00:06:42.960</a></span> | <span class="t">In case you're wondering, that's where they got the quirky 405 billion parameter number from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=408" target="_blank">00:06:48.000</a></span> | <span class="t">They had the compute budget, looked at those benchmark scaling laws,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=411" target="_blank">00:06:51.760</a></span> | <span class="t">and assigned that number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=414" target="_blank">00:06:54.560</a></span> | <span class="t">On the right is the sigmoidal scaling law curve that they anticipated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=419" target="_blank">00:06:59.760</a></span> | <span class="t">and that followed on the ARK challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=422" target="_blank">00:07:02.640</a></span> | <span class="t">That's not, by the way, the ARK AGI challenge that I've talked about on this channel recently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=427" target="_blank">00:07:07.280</a></span> | <span class="t">but it is legit questions like this that you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=430" target="_blank">00:07:10.400</a></span> | <span class="t">General knowledge and what they call a reasoning challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=434" target="_blank">00:07:14.160</a></span> | <span class="t">Now, just how many benchmarks that scaling law holds for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=437" target="_blank">00:07:17.920</a></span> | <span class="t">is a question that I, at least, am immensely curious about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=441" target="_blank">00:07:21.760</a></span> | <span class="t">I'll come back to benchmarks, but the amount of detail they went into,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=445" target="_blank">00:07:25.760</a></span> | <span class="t">down to the exact hardware issues they had, is quite incredible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=449" target="_blank">00:07:29.920</a></span> | <span class="t">They even note at one point that temperature fluctuations during the day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=454" target="_blank">00:07:34.880</a></span> | <span class="t">impacted GPU dynamic voltage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=457" target="_blank">00:07:37.360</a></span> | <span class="t">And slightly more concerningly, the fluctuations of power consumption</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=461" target="_blank">00:07:41.760</a></span> | <span class="t">across the data center stretched the limits of the power grid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=466" target="_blank">00:07:46.720</a></span> | <span class="t">It does make me, at least, wonder what the kind of issues they'll have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=470" target="_blank">00:07:50.480</a></span> | <span class="t">when they scale up another 50x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=472" target="_blank">00:07:52.880</a></span> | <span class="t">Now, clearly, because it is a 92-page paper, I am skipping over a lot,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=477" target="_blank">00:07:57.200</a></span> | <span class="t">but I do want to bring you the most interesting highlights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=480" target="_blank">00:08:00.080</a></span> | <span class="t">For example, there was this detail about how they obsessively cleaned the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=484" target="_blank">00:08:04.720</a></span> | <span class="t">They found an annoying problem that was too common in their data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=488" target="_blank">00:08:08.640</a></span> | <span class="t">Overly apologetic tonal issues. Phrases such as "I'm sorry" or "I apologize".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=494" target="_blank">00:08:14.320</a></span> | <span class="t">They didn't want that, nor did they want excessive emojis or exclamation points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=499" target="_blank">00:08:19.360</a></span> | <span class="t">Back to that theme, though, of AI improving AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=502" target="_blank">00:08:22.480</a></span> | <span class="t">they trained a code expert model to help them find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=506" target="_blank">00:08:26.480</a></span> | <span class="t">the highest quality human annotations for code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=509" target="_blank">00:08:29.280</a></span> | <span class="t">Five pages on in the paper, they say that they trained a multilingual expert model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=514" target="_blank">00:08:34.640</a></span> | <span class="t">to collect higher quality annotations in non-English languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=518" target="_blank">00:08:38.480</a></span> | <span class="t">And it seems appropriate at this point to mention that Meta, for the first time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=522" target="_blank">00:08:42.560</a></span> | <span class="t">allow you to use this frontier model to generate synthetic data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=526" target="_blank">00:08:46.880</a></span> | <span class="t">to improve and train your smaller model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=529" target="_blank">00:08:49.760</a></span> | <span class="t">They didn't allow that before, and nor did companies like OpenAI, to the best of my knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=534" target="_blank">00:08:54.480</a></span> | <span class="t">So that flywheel of models improving models is now technically open to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=539" target="_blank">00:08:59.440</a></span> | <span class="t">Now, you do have to be slightly sophisticated about it, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=543" target="_blank">00:09:03.120</a></span> | <span class="t">When they trained LLAMA3-405B on its own generated data in programming,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=548" target="_blank">00:09:08.560</a></span> | <span class="t">they found it wasn't helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=550" target="_blank">00:09:10.560</a></span> | <span class="t">Notice that is different from those last two examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=552" target="_blank">00:09:12.800</a></span> | <span class="t">This is the same model training on its own generated data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=556" target="_blank">00:09:16.560</a></span> | <span class="t">But when they introduced execution feedback,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=559" target="_blank">00:09:19.280</a></span> | <span class="t">which I've talked about quite a lot on this channel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=561" target="_blank">00:09:21.840</a></span> | <span class="t">it did enable the model to learn from its own mistakes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=565" target="_blank">00:09:25.600</a></span> | <span class="t">And anyone who has been following this channel knows that I talk often about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=569" target="_blank">00:09:29.440</a></span> | <span class="t">verifier models, and LLAMA3 indeed incorporated that approach during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=575" target="_blank">00:09:35.280</a></span> | <span class="t">In coding, for example, only generations that pass syntax checking and unit tests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=580" target="_blank">00:09:40.640</a></span> | <span class="t">were used for fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=582" target="_blank">00:09:42.400</a></span> | <span class="t">But for maths and reasoning, the story is even more interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=586" target="_blank">00:09:46.560</a></span> | <span class="t">First, they give a curious definition of reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=589" target="_blank">00:09:49.840</a></span> | <span class="t">We define reasoning as the ability to perform multi-step computations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=593" target="_blank">00:09:53.760</a></span> | <span class="t">and arrive at the correct final answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=596" target="_blank">00:09:56.000</a></span> | <span class="t">I'm definitely going to leave a question mark on that one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=598" target="_blank">00:09:58.720</a></span> | <span class="t">because under that definition, wouldn't a calculator be doing reasoning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=602" target="_blank">00:10:02.880</a></span> | <span class="t">But the interesting bit is how they say that training data on the web</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=607" target="_blank">00:10:07.280</a></span> | <span class="t">shows a shortage of ground-truth correct chains of thought for reasoning and math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=613" target="_blank">00:10:13.280</a></span> | <span class="t">But those are essential for guiding the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=616" target="_blank">00:10:16.080</a></span> | <span class="t">how to break down the problem step-by-step and reach the final answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=619" target="_blank">00:10:19.680</a></span> | <span class="t">In other words, most online text contains results and analysis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=623" target="_blank">00:10:23.840</a></span> | <span class="t">not the chains of thought involved in coming up with those results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=628" target="_blank">00:10:28.080</a></span> | <span class="t">Then they quote directly from the Let's Verify Step-by-Step paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=632" target="_blank">00:10:32.080</a></span> | <span class="t">that I've talked about many times on this channel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=634" target="_blank">00:10:34.240</a></span> | <span class="t">And they go on to say the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">"They identified mathematical skills where the model underperforms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=641" target="_blank">00:10:41.040</a></span> | <span class="t">and actively source prompts from humans to teach the models such skills."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=645" target="_blank">00:10:45.280</a></span> | <span class="t">And then they use the model, LLAMA3, to check the reasoning steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=650" target="_blank">00:10:50.160</a></span> | <span class="t">behind a step-by-step solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=652" target="_blank">00:10:52.400</a></span> | <span class="t">In other words, training a model to recognize good steps in a reasoning chain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=658" target="_blank">00:10:58.160</a></span> | <span class="t">They could then filter the training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=660" target="_blank">00:11:00.560</a></span> | <span class="t">where those intermediate reasoning steps were incorrect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=663" target="_blank">00:11:03.760</a></span> | <span class="t">So not just the final results, the reasons used to get those final results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=668" target="_blank">00:11:08.480</a></span> | <span class="t">They wanted to eliminate invalid reasoning traces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=672" target="_blank">00:11:12.080</a></span> | <span class="t">And for the hardest prompts, they even used Monte Carlo Tree Search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=675" target="_blank">00:11:15.920</a></span> | <span class="t">a bit like AlphaGo, with those process-based reward models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=680" target="_blank">00:11:20.000</a></span> | <span class="t">to generate valid reasoning traces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=682" target="_blank">00:11:22.320</a></span> | <span class="t">Translated, they searched as hard as they could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=685" target="_blank">00:11:25.280</a></span> | <span class="t">to find the best reasoning steps to teach the model reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=689" target="_blank">00:11:29.200</a></span> | <span class="t">And at this point, I can hold off no longer from talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=692" target="_blank">00:11:32.720</a></span> | <span class="t">my own private benchmark, what I call SimpleBench,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=696" target="_blank">00:11:36.320</a></span> | <span class="t">to test general intelligence reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=698" target="_blank">00:11:38.800</a></span> | <span class="t">And there are a few things I love about this benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=702" target="_blank">00:11:42.400</a></span> | <span class="t">Obviously, I am ridiculously biased, so take this with a pinch of salt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=706" target="_blank">00:11:46.160</a></span> | <span class="t">But this is actually the benchmark I rely on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=708" target="_blank">00:11:48.640</a></span> | <span class="t">to test the real reasoning intelligence of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=711" target="_blank">00:11:51.680</a></span> | <span class="t">First, it's fully private, so it hasn't been contaminated at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=715" target="_blank">00:11:55.280</a></span> | <span class="t">Second, it is rigorously vetted, not just by me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=718" target="_blank">00:11:58.560</a></span> | <span class="t">but by outside experts with more to come.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=721" target="_blank">00:12:01.280</a></span> | <span class="t">If even one mistake makes it into the final 100 or 200 questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=725" target="_blank">00:12:05.840</a></span> | <span class="t">I'll be pretty pissed off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=727" target="_blank">00:12:07.280</a></span> | <span class="t">But third, and I think most interestingly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=729" target="_blank">00:12:09.440</a></span> | <span class="t">even the best models, as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=732" target="_blank">00:12:12.000</a></span> | <span class="t">fall well, well, well behind the performance of humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=736" target="_blank">00:12:16.400</a></span> | <span class="t">as I have anecdotally tested them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=738" target="_blank">00:12:18.400</a></span> | <span class="t">I'll show you one example in a moment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=740" target="_blank">00:12:20.080</a></span> | <span class="t">which of course won't make it into the final benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=743" target="_blank">00:12:23.040</a></span> | <span class="t">But for me, it has been the most reliable vibe test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=746" target="_blank">00:12:26.720</a></span> | <span class="t">that I've seen so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=748" target="_blank">00:12:28.080</a></span> | <span class="t">Now, I will be testing the models again using self-consistency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=751" target="_blank">00:12:31.600</a></span> | <span class="t">But for now, we have Claude 3.5 Sonnet way ahead at 32%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=756" target="_blank">00:12:36.960</a></span> | <span class="t">Lama 405B at 18%, well ahead of both versions of GPT-4 and Gemini 1.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=764" target="_blank">00:12:44.800</a></span> | <span class="t">Smaller models, by the way, in case you're curious,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=766" target="_blank">00:12:46.800</a></span> | <span class="t">like GPT-40 Mini score 0%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=769" target="_blank">00:12:49.120</a></span> | <span class="t">And here is one example that the new Lama model actually usually gets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=774" target="_blank">00:12:54.720</a></span> | <span class="t">but GPT-40 basically never gets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=778" target="_blank">00:12:58.320</a></span> | <span class="t">It comes from the spatial intelligence section of the benchmark</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=781" target="_blank">00:13:01.760</a></span> | <span class="t">and involves placing four whole ice cubes into a fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=786" target="_blank">00:13:06.160</a></span> | <span class="t">Then some more ice cubes into the fire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=788" target="_blank">00:13:08.320</a></span> | <span class="t">And then the question ends with how many whole ice cubes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=792" target="_blank">00:13:12.240</a></span> | <span class="t">can be found in the fire at the end of the third minute?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=795" target="_blank">00:13:15.840</a></span> | <span class="t">I even add in pick the most realistic answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=798" target="_blank">00:13:18.880</a></span> | <span class="t">And no, the model doesn't pick zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=801" target="_blank">00:13:21.040</a></span> | <span class="t">reflecting that none of the ice cubes will be whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=804" target="_blank">00:13:24.000</a></span> | <span class="t">or even still there after the third minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=807" target="_blank">00:13:27.040</a></span> | <span class="t">Most models, of course, go down a rabbit hole of calculations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=810" target="_blank">00:13:30.720</a></span> | <span class="t">Now, admittedly, this was one of the easier questions on the benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=814" target="_blank">00:13:34.960</a></span> | <span class="t">And if you add things like think about this carefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=818" target="_blank">00:13:38.080</a></span> | <span class="t">or this is a trick question, the models can sometimes get it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=821" target="_blank">00:13:41.840</a></span> | <span class="t">But I know the models well enough now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=824" target="_blank">00:13:44.000</a></span> | <span class="t">that I can create genuine spatial, temporal, linguistic or social questions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=829" target="_blank">00:13:49.280</a></span> | <span class="t">that no amount of warnings allow the models to get right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=832" target="_blank">00:13:52.720</a></span> | <span class="t">And yes, that's still with humans scoring near perfectly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=836" target="_blank">00:13:56.080</a></span> | <span class="t">How so?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=836" target="_blank">00:13:56.800</a></span> | <span class="t">Well, it's because, of course, the models are modelling language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=840" target="_blank">00:14:00.720</a></span> | <span class="t">They're language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=842" target="_blank">00:14:02.000</a></span> | <span class="t">They're not reality simulators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=844" target="_blank">00:14:04.160</a></span> | <span class="t">They don't actually visualise things in their head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=847" target="_blank">00:14:07.200</a></span> | <span class="t">or think about problems in the same way that we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=850" target="_blank">00:14:10.400</a></span> | <span class="t">So how would a model like Lama 3 ever get a question like this right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=855" target="_blank">00:14:15.360</a></span> | <span class="t">Well, it's because I can leave, let's say, linguistic clues,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=859" target="_blank">00:14:19.040</a></span> | <span class="t">crumbs to allow them to infer the answer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=862" target="_blank">00:14:22.480</a></span> | <span class="t">even if they can't simulate the situation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=865" target="_blank">00:14:25.040</a></span> | <span class="t">Testing, if you will, their ability to pick up faint signal amidst the noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=870" target="_blank">00:14:30.080</a></span> | <span class="t">If I remove all signal, models score zero with humans still scoring almost perfectly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=876" target="_blank">00:14:36.080</a></span> | <span class="t">But with just faint signals,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=878" target="_blank">00:14:38.320</a></span> | <span class="t">I can separate the smart models from the less smart models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=882" target="_blank">00:14:42.400</a></span> | <span class="t">I'll be totally honest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=883" target="_blank">00:14:43.280</a></span> | <span class="t">I wish I could go through all the hundred plus questions with you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=886" target="_blank">00:14:46.800</a></span> | <span class="t">because they're pretty fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=888" target="_blank">00:14:48.240</a></span> | <span class="t">But then, of course, it would leak into the training data inevitably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=892" target="_blank">00:14:52.000</a></span> | <span class="t">and contaminate the test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=893" target="_blank">00:14:53.600</a></span> | <span class="t">Now, I have made the benchmark functional so I can change the numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=897" target="_blank">00:14:57.600</a></span> | <span class="t">But still, I want to avoid that if possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=899" target="_blank">00:14:59.920</a></span> | <span class="t">Now, I get it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=900" target="_blank">00:15:00.560</a></span> | <span class="t">Many of you are thinking that was a very long way of saying that Lama 405B is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=907" target="_blank">00:15:07.360</a></span> | <span class="t">Not quite as good as Claude 3.5 Sonnet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=910" target="_blank">00:15:10.720</a></span> | <span class="t">but better, I think, in text at least than GPT-40.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=914" target="_blank">00:15:14.560</a></span> | <span class="t">Now, you could say that part of this benchmark is somewhat adversarial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=918" target="_blank">00:15:18.880</a></span> | <span class="t">and Meta on page 33 talk about how adversarial tests</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=923" target="_blank">00:15:23.920</a></span> | <span class="t">cause significantly worse performance than non-adversarial ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=927" target="_blank">00:15:27.840</a></span> | <span class="t">What they mean by that is that in some of the benchmarks that they used,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=931" target="_blank">00:15:31.760</a></span> | <span class="t">even a single distracting sentence at the end of a question</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=935" target="_blank">00:15:35.680</a></span> | <span class="t">caused significantly worse performance than simply asking the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=939" target="_blank">00:15:39.760</a></span> | <span class="t">If the model was actually thinking about the question, that shouldn't happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=943" target="_blank">00:15:43.760</a></span> | <span class="t">And the paper highlights this without actually suggesting a solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=947" target="_blank">00:15:47.440</a></span> | <span class="t">For mathematical reasoning and question answering, however,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=950" target="_blank">00:15:50.480</a></span> | <span class="t">the adversarial performances are substantially lower</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=954" target="_blank">00:15:54.240</a></span> | <span class="t">than the non-adversarial performances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=956" target="_blank">00:15:56.640</a></span> | <span class="t">This pattern is similar for pre-trained and post-trained models, full stop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=961" target="_blank">00:16:01.600</a></span> | <span class="t">So much to cover, so I'm going to move swiftly on to contamination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=965" target="_blank">00:16:05.120</a></span> | <span class="t">Through fascinating word matching or n-gram checks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=968" target="_blank">00:16:08.560</a></span> | <span class="t">they found that contamination was rife in traditional benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=973" target="_blank">00:16:13.040</a></span> | <span class="t">And these contamination scores in this column actually underestimate the problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=977" target="_blank">00:16:17.920</a></span> | <span class="t">They excluded benchmarks from this chart when the clean set had too few examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=983" target="_blank">00:16:23.840</a></span> | <span class="t">or because the observed performance gain when they cleaned the data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=988" target="_blank">00:16:28.000</a></span> | <span class="t">showed extremely erratic behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=990" target="_blank">00:16:30.160</a></span> | <span class="t">And they go on to describe the MMLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=992" target="_blank">00:16:32.560</a></span> | <span class="t">Even when they allowed for a higher threshold of 8-word overlap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=996" target="_blank">00:16:36.640</a></span> | <span class="t">between the training data and the test,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=998" target="_blank">00:16:38.960</a></span> | <span class="t">it gave such high contamination scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1001" target="_blank">00:16:41.440</a></span> | <span class="t">that it was impossible to get a good performance gain estimate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1004" target="_blank">00:16:44.720</a></span> | <span class="t">So they couldn't even really estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1006" target="_blank">00:16:46.800</a></span> | <span class="t">how much contamination was affecting the MMLU scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1010" target="_blank">00:16:50.240</a></span> | <span class="t">It seems like private benchmarks such as those from Scale.ai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1013" target="_blank">00:16:53.840</a></span> | <span class="t">and indeed mine will be more common in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1016" target="_blank">00:16:56.800</a></span> | <span class="t">Here was the ranking for example in math by Scale.ai</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1020" target="_blank">00:17:00.560</a></span> | <span class="t">with Claude 3.5 Sonnet in number one position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1023" target="_blank">00:17:03.520</a></span> | <span class="t">At a glance, human comparisons leading to leaderboards like those from Elemsis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1028" target="_blank">00:17:08.160</a></span> | <span class="t">seem to be a bit more problematic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1030" target="_blank">00:17:10.000</a></span> | <span class="t">Even though Sam Altman said that we now have GPT-40 Mini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1034" target="_blank">00:17:14.160</a></span> | <span class="t">matching GPT-40's performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1036" target="_blank">00:17:16.480</a></span> | <span class="t">In my own experiments, and let me know what you think in the comments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1039" target="_blank">00:17:19.760</a></span> | <span class="t">it's not even close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1041" target="_blank">00:17:21.280</a></span> | <span class="t">Having Mini beating Claude 3.5 Sonnet just seems shocking to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1045" target="_blank">00:17:25.440</a></span> | <span class="t">Now Elemsis have addressed that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1047" target="_blank">00:17:27.120</a></span> | <span class="t">and said that they're going to release a random 20% subset of those battles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1051" target="_blank">00:17:31.760</a></span> | <span class="t">So I will look at that with interest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1053" target="_blank">00:17:33.920</a></span> | <span class="t">Back to the paper though, and here's another way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1056" target="_blank">00:17:36.080</a></span> | <span class="t">that Llama405B does seem to be better than its rivals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1060" target="_blank">00:17:40.400</a></span> | <span class="t">It has a long context of 128k tokens or around 100,000 words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1065" target="_blank">00:17:45.760</a></span> | <span class="t">Now other models of course have more than that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1067" target="_blank">00:17:47.920</a></span> | <span class="t">but that's not why it's better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1069" target="_blank">00:17:49.520</a></span> | <span class="t">It's when it's asked questions that rely on scouring through that long context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1074" target="_blank">00:17:54.160</a></span> | <span class="t">that it performs better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1075" target="_blank">00:17:55.360</a></span> | <span class="t">And annoyingly, they didn't compare it to Gemini 1.5 Pro,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1078" target="_blank">00:17:58.960</a></span> | <span class="t">but here it beats GPT-4, GPT-40 and Claude 3.5 Sonnet significantly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1084" target="_blank">00:18:04.640</a></span> | <span class="t">What is this infinite bench in QA?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1087" target="_blank">00:18:07.440</a></span> | <span class="t">Well, as you'd expect, I tracked down that paper and read it in full.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1091" target="_blank">00:18:11.360</a></span> | <span class="t">And a typical question from that infinity bench was this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1095" target="_blank">00:18:15.040</a></span> | <span class="t">With details strewn throughout a story the length of a novel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1098" target="_blank">00:18:18.400</a></span> | <span class="t">they asked what colour dress did person A wear when A met B for the second time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1104" target="_blank">00:18:24.800</a></span> | <span class="t">So the model would obviously have to track when A met B for the first time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1109" target="_blank">00:18:29.040</a></span> | <span class="t">then the second time and what colour dress they were wearing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">On that, Llama 3.1 crushes Claude 3.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1115" target="_blank">00:18:35.920</a></span> | <span class="t">Also, when there are multiple needles in a haystack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1119" target="_blank">00:18:39.120</a></span> | <span class="t">A bit like if there's four passwords strewn throughout a long document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1123" target="_blank">00:18:43.360</a></span> | <span class="t">Can't do this quite as well as GPT-4 apparently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1126" target="_blank">00:18:46.640</a></span> | <span class="t">Or even Llama 3 A billion parameters randomly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1130" target="_blank">00:18:50.320</a></span> | <span class="t">but does far better than Claude 3.5 Sonnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1133" target="_blank">00:18:53.040</a></span> | <span class="t">It does seem a bit random to me to not compare it to Gemini 1.5 Pro</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1137" target="_blank">00:18:57.360</a></span> | <span class="t">when that's its specialty, long context, but anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1141" target="_blank">00:19:01.040</a></span> | <span class="t">Now, I will give some more credit to Meta for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1144" target="_blank">00:19:04.640</a></span> | <span class="t">They gave plenty of win-loss human comparisons with GPT-4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1148" target="_blank">00:19:08.960</a></span> | <span class="t">not only in the paper, but also on the website of the Llama 3 release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1153" target="_blank">00:19:13.520</a></span> | <span class="t">And most of those comparisons were actually unfavourable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1156" target="_blank">00:19:16.960</a></span> | <span class="t">That's commendable honesty to include charts which make your model seem less good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1162" target="_blank">00:19:22.320</a></span> | <span class="t">In the middle, you can see Llama 3 losing out to GPT-4.0 most of the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1168" target="_blank">00:19:28.160</a></span> | <span class="t">No, actually, it's all of these comparisons across English reasoning, coding, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1172" target="_blank">00:19:32.880</a></span> | <span class="t">Now again, as we've seen, human at a glance evaluation can't always be trusted though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1178" target="_blank">00:19:38.320</a></span> | <span class="t">Now though, for a word on safety,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1180" target="_blank">00:19:40.560</a></span> | <span class="t">and they claim that the violation rate has dropped significantly for Llama 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1186" target="_blank">00:19:46.160</a></span> | <span class="t">compared to its competitors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1187" target="_blank">00:19:47.680</a></span> | <span class="t">Now, normally a lower violation rate for safety would lead to an increased false refusal rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1193" target="_blank">00:19:53.680</a></span> | <span class="t">when they refuse to answer simple, innocent questions, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1196" target="_blank">00:19:56.800</a></span> | <span class="t">But actually, it still has a pretty low false refusal rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1200" target="_blank">00:20:00.640</a></span> | <span class="t">And they make this point that it is critical to consider false refusal as a countermetric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1206" target="_blank">00:20:06.080</a></span> | <span class="t">because a model that always refuses is maximally safe, cough, cough, Claude 3.5 sonnet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1211" target="_blank">00:20:11.440</a></span> | <span class="t">but not always helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1212" target="_blank">00:20:12.800</a></span> | <span class="t">The reference I'm making there is that Claude very frequently compared to other models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1217" target="_blank">00:20:17.360</a></span> | <span class="t">seems to refuse my innocent questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1219" target="_blank">00:20:19.680</a></span> | <span class="t">Anyway, so false refusals are definitely a thing and I'm glad Meta are aware of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1224" target="_blank">00:20:24.800</a></span> | <span class="t">And again, commendable honesty,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1226" target="_blank">00:20:26.320</a></span> | <span class="t">they admit that Llama 3 is on average more susceptible to prompt injection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1231" target="_blank">00:20:31.280</a></span> | <span class="t">compared at least to GPT-4 or Gemini Pro, but it's better apparently than mixed trial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1236" target="_blank">00:20:36.160</a></span> | <span class="t">But there's a wider point on safety that I do want to note.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1239" target="_blank">00:20:39.440</a></span> | <span class="t">It was only around a year ago that Mark Zuckerberg was receiving a letter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1244" target="_blank">00:20:44.000</a></span> | <span class="t">from two senators in America concerned about the leak of Llama 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1249" target="_blank">00:20:49.280</a></span> | <span class="t">talking about its potential for spam, fraud, malware, privacy violations, and harassment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1254" target="_blank">00:20:54.400</a></span> | <span class="t">Now, clearly that letter went nowhere because they subsequently released</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1258" target="_blank">00:20:58.560</a></span> | <span class="t">not only Llama 2, but Llama 3 open weights and downloadable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1263" target="_blank">00:21:03.440</a></span> | <span class="t">And again, on the safety point, Leopold Aschenbrenner will be having a fit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1268" target="_blank">00:21:08.160</a></span> | <span class="t">because he says there's no point keeping models closed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1271" target="_blank">00:21:11.360</a></span> | <span class="t">because adversaries like China will simply steal the models anyway on a thumb drive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1276" target="_blank">00:21:16.000</a></span> | <span class="t">So when I see letters like this from a couple of days ago to Sam Altman,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1280" target="_blank">00:21:20.080</a></span> | <span class="t">signed by around six senators asking him if he has indeed committed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1284" target="_blank">00:21:24.000</a></span> | <span class="t">20% of their compute budget to safety,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1286" target="_blank">00:21:26.480</a></span> | <span class="t">I just have a slight suspicion that OpenAI might completely ignore this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1291" target="_blank">00:21:31.200</a></span> | <span class="t">and completely get away with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1293" target="_blank">00:21:33.440</a></span> | <span class="t">I also want to commend Meta for being much more rigorous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1296" target="_blank">00:21:36.240</a></span> | <span class="t">in how they pre-check models before release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1298" target="_blank">00:21:38.560</a></span> | <span class="t">They got a set of volunteers and saw if there was any uplift in their ability to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1303" target="_blank">00:21:43.360</a></span> | <span class="t">or at least ideate about chemical and biological weapons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1306" target="_blank">00:21:46.960</a></span> | <span class="t">Basically when they had access to Llama 3 versus having no access.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1310" target="_blank">00:21:50.880</a></span> | <span class="t">Both groups did have the internet at least.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1313" target="_blank">00:21:53.360</a></span> | <span class="t">And the analysis of these results showed no significant uplift in performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1317" target="_blank">00:21:57.520</a></span> | <span class="t">related to usage of Llama 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1319" target="_blank">00:21:59.520</a></span> | <span class="t">And honestly, that doesn't surprise me too much given how much data filtering went on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1324" target="_blank">00:22:04.480</a></span> | <span class="t">Count me at least as being surprised if biological or chemical weapon data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1329" target="_blank">00:22:09.440</a></span> | <span class="t">still made it into the final model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1331" target="_blank">00:22:11.280</a></span> | <span class="t">I would hope not at least.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1332" target="_blank">00:22:12.720</a></span> | <span class="t">To their credit, OpenAI did a similar study almost six months ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1336" target="_blank">00:22:16.880</a></span> | <span class="t">which I talked about on my Patreon AI Insiders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1340" target="_blank">00:22:20.080</a></span> | <span class="t">Now the vision, speech and video parts of Llama 3.1 aren't yet available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1345" target="_blank">00:22:25.360</a></span> | <span class="t">Zuckerberg described some sort of mess up but didn't go into much more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1349" target="_blank">00:22:29.520</a></span> | <span class="t">But they did have one interesting conjecture in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1352" target="_blank">00:22:32.800</a></span> | <span class="t">You might remember how Gemini 1.5 Pro and GPT 4.0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1356" target="_blank">00:22:36.960</a></span> | <span class="t">are trained from the ground up to be multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1359" target="_blank">00:22:39.760</a></span> | <span class="t">That has advantages but Meta contends that a compositional approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1364" target="_blank">00:22:44.880</a></span> | <span class="t">as in separate models, is actually in some ways more advantageous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1370" target="_blank">00:22:50.000</a></span> | <span class="t">Apparently, it's more efficient during inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1372" target="_blank">00:22:52.640</a></span> | <span class="t">Obviously, we can all judge this when it comes out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1374" target="_blank">00:22:54.880</a></span> | <span class="t">But I do note that Noam Brown said that GPT 4.0 didn't turn out as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1379" target="_blank">00:22:59.200</a></span> | <span class="t">as they hoped with multimodal reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1381" target="_blank">00:23:01.840</a></span> | <span class="t">Here were the final results though in a benchmark I do pay attention to, the MMMU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1387" target="_blank">00:23:07.600</a></span> | <span class="t">You can see that Llama 3 with vision scores 64.5% versus Claw 3.5 68.3%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1395" target="_blank">00:23:15.920</a></span> | <span class="t">GPT 4.0 is better at 69.1% and I can believe that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1400" target="_blank">00:23:20.240</a></span> | <span class="t">And very quickly on the video data that Meta used for training Llama 3v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1405" target="_blank">00:23:25.120</a></span> | <span class="t">Well, they don't say it but they strongly imply that they're using Instagram Reels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1410" target="_blank">00:23:30.480</a></span> | <span class="t">Now anyone who knows can correct me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1412" target="_blank">00:23:32.720</a></span> | <span class="t">but the duration and resolution of the videos does seem to hint at that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1417" target="_blank">00:23:37.120</a></span> | <span class="t">If that's true, well then like Google they can of course flex those muscles</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1421" target="_blank">00:23:41.520</a></span> | <span class="t">of the masses of data that they have that people like OpenAI wouldn't necessarily have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1426" target="_blank">00:23:46.560</a></span> | <span class="t">Yes, by the way, they are working on speech generation as well as speech understanding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1431" target="_blank">00:23:51.520</a></span> | <span class="t">so you should be able to talk eventually to Llama 3.1 just like we were promised with GPT 4.0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1438" target="_blank">00:23:58.320</a></span> | <span class="t">They even claim that their speech recognition is better than Whisper v2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1443" target="_blank">00:24:03.120</a></span> | <span class="t">and for multilingual scenarios Whisper v3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1446" target="_blank">00:24:06.320</a></span> | <span class="t">Now admittedly, this experiment was using Whisper v3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1449" target="_blank">00:24:09.360</a></span> | <span class="t">but just look at the speed, in this case using Grok, that these smaller Llama 3 models can act at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1455" target="_blank">00:24:15.280</a></span> | <span class="t">Located, can you tabularize it for me?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1457" target="_blank">00:24:17.280</a></span> | <span class="t">Can you add a duration column?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1462" target="_blank">00:24:22.320</a></span> | <span class="t">Can you remove the end times from the time column?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1467" target="_blank">00:24:27.120</a></span> | <span class="t">Can you make the duration in minutes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1471" target="_blank">00:24:31.200</a></span> | <span class="t">And can you move the duration to between the time and stop column?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1477" target="_blank">00:24:37.680</a></span> | <span class="t">Can you add lunch and dinner at a nice restaurant?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1485" target="_blank">00:24:45.120</a></span> | <span class="t">You know what, I changed my mind. Make it Vancouver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1490" target="_blank">00:24:50.320</a></span> | <span class="t">Of course, for time reasons, I've got to draw this video to an end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1496" target="_blank">00:24:56.960</a></span> | <span class="t">but there were countless more experiments with training models revealed throughout the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1501" target="_blank">00:25:01.920</a></span> | <span class="t">And speaking of tracking experiments, you may already know that AI labs, including OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1508" target="_blank">00:25:08.240</a></span> | <span class="t">have used Weights & Biases, this video's sponsor, to track frontier machine learning experiments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1514" target="_blank">00:25:14.400</a></span> | <span class="t">as well as visualize, iterate on, optimize, and share them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1518" target="_blank">00:25:18.240</a></span> | <span class="t">But you might not know that Weights & Biases now have Weave,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1522" target="_blank">00:25:22.160</a></span> | <span class="t">a lightweight toolkit to confidently iterate on LLM applications,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1526" target="_blank">00:25:26.800</a></span> | <span class="t">and that they produce free prompting and LLM agent courses on their website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1531" target="_blank">00:25:31.360</a></span> | <span class="t">And if you didn't know that, do let them know that you came from this video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1535" target="_blank">00:25:35.920</a></span> | <span class="t">the link is in the description.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1537" target="_blank">00:25:37.520</a></span> | <span class="t">And so let's conclude with Meta's conclusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1540" target="_blank">00:25:40.640</a></span> | <span class="t">They say, and I agree, that in many ways,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1542" target="_blank">00:25:42.800</a></span> | <span class="t">the development of high-quality foundation models is still in its infancy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1546" target="_blank">00:25:46.720</a></span> | <span class="t">Our experience in developing LLAMA 3 suggests that substantial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1550" target="_blank">00:25:50.240</a></span> | <span class="t">further improvements of these models are on the horizon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1553" target="_blank">00:25:53.360</a></span> | <span class="t">They go on to admit that they did explore more complex model architectures and training recipes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1558" target="_blank">00:25:58.000</a></span> | <span class="t">but did not find the benefits of such approaches to outweigh the additional complexity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1562" target="_blank">00:26:02.960</a></span> | <span class="t">that they introduce into model development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1565" target="_blank">00:26:05.120</a></span> | <span class="t">Like you, I can't wait, of course, to compare LLAMA 3.1 with Gemini 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1570" target="_blank">00:26:10.080</a></span> | <span class="t">and GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1571" target="_blank">00:26:11.360</a></span> | <span class="t">And they had the right plan to ensure that LLAMA 3 was not accidentally overfitted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1576" target="_blank">00:26:16.080</a></span> | <span class="t">on commonly used benchmarks, and that their pre-training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1579" target="_blank">00:26:19.760</a></span> | <span class="t">was not only procured, but processed by a separate team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1583" target="_blank">00:26:23.600</a></span> | <span class="t">That was, they say, strongly incentivized to prevent contamination of that pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1589" target="_blank">00:26:29.760</a></span> | <span class="t">The model's performance on my simple bench does imply that their benchmark results aren't fluky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1595" target="_blank">00:26:35.120</a></span> | <span class="t">And they end with this, we hope that the release of LLAMA 3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1598" target="_blank">00:26:38.080</a></span> | <span class="t">encourages the industry to embrace the open, in quotes, "responsible" development of AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Tf1nooXtUHE&t=1604" target="_blank">00:26:44.000</a></span> | <span class="t">Let me know what you think in the comments, and as always, have a wonderful day.</span></div></div></body></html>