<html><head><title>Stanford CS25: V5 I Transformers in Diffusion Models for Image Generation and Beyond</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V5 I Transformers in Diffusion Models for Image Generation and Beyond</h2><a href="https://www.youtube.com/watch?v=vXtapCFctTI"><img src="https://i.ytimg.com/vi/vXtapCFctTI/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./vXtapCFctTI.html">Whisper Transcript</a> | <a href="./transcript_vXtapCFctTI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">All right, thank you so much for joining us today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4" target="_blank">00:00:04.560</a></span> | <span class="t">Today, I'm very honored to welcome Syek Paul from Hugging Face,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=9" target="_blank">00:00:09.720</a></span> | <span class="t">who works a lot on diffusion models, image generation, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=15" target="_blank">00:00:15.760</a></span> | <span class="t">So his day-to-day includes contributing to diffusers, diffusers library,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=21" target="_blank">00:00:21.560</a></span> | <span class="t">training and babysitting diffusion models, and working on applied ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=26" target="_blank">00:00:26.560</a></span> | <span class="t">He's interested in subject-driven generation, preference alignment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=30" target="_blank">00:00:30.560</a></span> | <span class="t">and evaluation of diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=33" target="_blank">00:00:33.040</a></span> | <span class="t">And when he's not working, he can be found playing the guitar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=36" target="_blank">00:00:36.800</a></span> | <span class="t">and binge-watching ICML tutorials and suits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=40" target="_blank">00:00:40.560</a></span> | <span class="t">So without further ado, I'll hand it off to Syek.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=43" target="_blank">00:00:43.920</a></span> | <span class="t">I guess I'm here to depart and deviate from the usual theme that's followed at CS25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=50" target="_blank">00:00:50.040</a></span> | <span class="t">It's not a lot about visual modality, especially the generative aspects of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=55" target="_blank">00:00:55.280</a></span> | <span class="t">So I guess I'm happy in a way that I'm here to depart and deviate from that theme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=59" target="_blank">00:00:59.040</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=61" target="_blank">00:01:01.760</a></span> | <span class="t">I wanted to start with a couple of disclaimers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=65" target="_blank">00:01:05.760</a></span> | <span class="t">And this talk is definitely not going to be an exhaustive overview of all the possible methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=71" target="_blank">00:01:11.760</a></span> | <span class="t">So it might be the case that I didn't cover your work, which you thought to be very seminal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=78" target="_blank">00:01:18.800</a></span> | <span class="t">So I apologize in advance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=80" target="_blank">00:01:20.880</a></span> | <span class="t">And then I'm not going to cover what is diffusion or flow matching in details,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=85" target="_blank">00:01:25.280</a></span> | <span class="t">but I'll give a very quick overview just to set the context and tone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=89" target="_blank">00:01:29.840</a></span> | <span class="t">And then the architectures I'll discuss today will be fairly agnostic to diffusion or flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=97" target="_blank">00:01:37.680</a></span> | <span class="t">And since I work on image and videos primarily, I will take my examples with images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=102" target="_blank">00:01:42.800</a></span> | <span class="t">But just know that these architectures are fairly well known to generalize to other continuous modalities such as audios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=111" target="_blank">00:01:51.600</a></span> | <span class="t">And then, of course, I'll share my slide after the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=117" target="_blank">00:01:57.600</a></span> | <span class="t">And here's how I want to approach this talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=120" target="_blank">00:02:00.560</a></span> | <span class="t">This is the rough overview of all the things that I want to cover.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">As I mentioned, I'll give you a brief introduction to diffusion models or flow matching as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=130" target="_blank">00:02:10.720</a></span> | <span class="t">And then I'll try to set the context by discussing the early architectures for diffusion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=137" target="_blank">00:02:17.040</a></span> | <span class="t">the early architectures for image generation in this field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=140" target="_blank">00:02:20.160</a></span> | <span class="t">And then we'll head straight to DITS and, as I like to call, their friends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=146" target="_blank">00:02:26.000</a></span> | <span class="t">And I'll conclude with some thoughts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=148" target="_blank">00:02:28.320</a></span> | <span class="t">And in those thoughts, I'll discuss some of the promising directions that I've become lately interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=154" target="_blank">00:02:34.160</a></span> | <span class="t">And then I'm sure there will be time for Q&A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=156" target="_blank">00:02:36.320</a></span> | <span class="t">I really wanted to kind of fascinate you with all the cool examples in the text-to-image arena.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=165" target="_blank">00:02:45.920</a></span> | <span class="t">But I guess at this point in time, we all know these examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=169" target="_blank">00:02:49.680</a></span> | <span class="t">Because I think, like, these days we are becoming more and more interested in native multimodal generation, not just images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=178" target="_blank">00:02:58.720</a></span> | <span class="t">But nonetheless, I just wanted to start off by giving you a couple of examples from the text-to-image arena.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=185" target="_blank">00:03:05.520</a></span> | <span class="t">My favorite is the last one, "tiny astronaut hatching from an egg on the moon."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=190" target="_blank">00:03:10.000</a></span> | <span class="t">I think human imagination really took off there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=194" target="_blank">00:03:14.560</a></span> | <span class="t">But yeah, you can see, apart from DALI-3, all these are open models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=198" target="_blank">00:03:18.880</a></span> | <span class="t">And the photorealism aspects of these models are quite impressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=203" target="_blank">00:03:23.840</a></span> | <span class="t">So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=204" target="_blank">00:03:24.640</a></span> | <span class="t">And then I want to also give you an infographic of how I like to think about diffusion models in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=214" target="_blank">00:03:34.880</a></span> | <span class="t">So, I like to think about diffusion models as the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=217" target="_blank">00:03:37.680</a></span> | <span class="t">So, when you take a random noise vector and what happens as you denoise it over a period of time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=224" target="_blank">00:03:44.160</a></span> | <span class="t">so that it becomes a photorealistic image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=226" target="_blank">00:03:46.640</a></span> | <span class="t">And if you take a look closely, you will notice that it's an iterative process, unlike GANs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=233" target="_blank">00:03:53.680</a></span> | <span class="t">GANs are one-shot in nature, but diffusion models are iterative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=236" target="_blank">00:03:56.960</a></span> | <span class="t">It's sequential in nature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=238" target="_blank">00:03:58.560</a></span> | <span class="t">So, in a way, we are essentially denoising the random noise that we had started off with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=244" target="_blank">00:04:04.000</a></span> | <span class="t">until and unless we are done with the kind of image that we are looking for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=249" target="_blank">00:04:09.680</a></span> | <span class="t">So, just know that it's iterative in nature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=251" target="_blank">00:04:11.440</a></span> | <span class="t">That's the main takeaway from this slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=253" target="_blank">00:04:13.520</a></span> | <span class="t">And when you condition, when you start conditioning the denoising process with text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=259" target="_blank">00:04:19.520</a></span> | <span class="t">for example, you can condition in lots of ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=261" target="_blank">00:04:21.760</a></span> | <span class="t">But let's say, I think, text is one of the more liberating conditions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=266" target="_blank">00:04:26.320</a></span> | <span class="t">As you start conditioning the denoising process with text, you get abstract creatures like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=272" target="_blank">00:04:32.080</a></span> | <span class="t">But yeah, you will start feeling very liberated with text images, I like to believe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=281" target="_blank">00:04:41.280</a></span> | <span class="t">And then, let's take a step back and start developing a mental model of what it takes to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=287" target="_blank">00:04:47.440</a></span> | <span class="t">a fairly state-of-the-art text-to-image model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=291" target="_blank">00:04:51.440</a></span> | <span class="t">Like, let's say I want to get from this text prompt a cat looking like a tiger and from all the way up to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=296" target="_blank">00:04:56.960</a></span> | <span class="t">image on the top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=299" target="_blank">00:04:59.040</a></span> | <span class="t">What does it take?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=300" target="_blank">00:05:00.560</a></span> | <span class="t">What are the components that we are looking for?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=304" target="_blank">00:05:04.160</a></span> | <span class="t">How should they be connected?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=305" target="_blank">00:05:05.440</a></span> | <span class="t">So, I want to kind of give you a connected graph of how the different components should be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=312" target="_blank">00:05:12.800</a></span> | <span class="t">connected so that we can backtrack them and start developing more intuition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=318" target="_blank">00:05:18.560</a></span> | <span class="t">So, of course, when you have text, you need to have some sort of embedding so that you can sort of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=323" target="_blank">00:05:23.840</a></span> | <span class="t">with those embeddings and we have text encoders for them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=326" target="_blank">00:05:26.320</a></span> | <span class="t">And like state-of-the-art diffusion models, they usually rely on more than one text encoders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=331" target="_blank">00:05:31.680</a></span> | <span class="t">For example, stable diffusion three, it relies on three text encoders, not one, not two, but three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=337" target="_blank">00:05:37.120</a></span> | <span class="t">So, you have your text prompt, you pass it off to text encoders, and you then have the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=343" target="_blank">00:05:43.440</a></span> | <span class="t">And as we saw in the earlier slide, we are starting off with some random noise drawn from a Gaussian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=349" target="_blank">00:05:49.040</a></span> | <span class="t">distribution. So, we are starting with some noisy latents and then you have your text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=355" target="_blank">00:05:55.200</a></span> | <span class="t">And then you have some time step which you do the math with the scheduler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=362" target="_blank">00:06:02.560</a></span> | <span class="t">I'm going to come to the scheduler component in a minute, but you have your conditions at this point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=367" target="_blank">00:06:07.520</a></span> | <span class="t">in time. You have your text embeddings, you have your noisy latents, and you have some time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">And then you have your core diffusion network, and you pass all these inputs to your core diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=382" target="_blank">00:06:22.240</a></span> | <span class="t">network. And it's a sequential process. You run the diffusion network over a period of time as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=389" target="_blank">00:06:29.200</a></span> | <span class="t">saw in the earlier slide. And then it's going to give you some refined latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=393" target="_blank">00:06:33.200</a></span> | <span class="t">And then you give it to some decoder model, and you have your image out. Now, I would like to call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=400" target="_blank">00:06:40.080</a></span> | <span class="t">out two broad classes of diffusion models. This overview resembles the pixels, the latent space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=406" target="_blank">00:06:46.400</a></span> | <span class="t">diffusion models. But there's another class of diffusion models, which is called the pixel space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=410" target="_blank">00:06:50.080</a></span> | <span class="t">diffusion model. But the recent or the state of the diffusion models, they are all latent space based.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=415" target="_blank">00:06:55.440</a></span> | <span class="t">Because pixel space is quite prohibitive and computationally intensive in nature. That's why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=421" target="_blank">00:07:01.200</a></span> | <span class="t">it's more common to see latent space diffusion models. So that's why you see noisy latents there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=428" target="_blank">00:07:08.320</a></span> | <span class="t">and not raw pixels. And if you were to sort of generalize this to other continuous modalities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=434" target="_blank">00:07:14.560</a></span> | <span class="t">the basic idea here is how do you represent the raw modality data points, and how do you compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=441" target="_blank">00:07:21.840</a></span> | <span class="t">intermediate representations of them? So if you were to do this on the audio space, you can think of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=447" target="_blank">00:07:27.200</a></span> | <span class="t">some similar analogies. But long story cut short, for a fairly well-performing text-to-image system,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=454" target="_blank">00:07:34.240</a></span> | <span class="t">these are roughly the components that you need. So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=457" target="_blank">00:07:37.120</a></span> | <span class="t">And then, let's now start developing some notations, so that we can start discussing about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=466" target="_blank">00:07:46.080</a></span> | <span class="t">how these models are trained, and finally how you should perform inference with these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=470" target="_blank">00:07:50.720</a></span> | <span class="t">So again, I'm giving these examples with images, but these are fairly agnostic, modality agnostic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=480" target="_blank">00:08:00.720</a></span> | <span class="t">Apart from text, of course, these won't work on discrete tokens. So let's say I have some original image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=486" target="_blank">00:08:06.800</a></span> | <span class="t">and I'm drawing some noise from a standard Gaussian distribution. And then, let's say I'm also drawing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=492" target="_blank">00:08:12.400</a></span> | <span class="t">some time step from a uniform distribution, and then I have a particular noise schedule, and also some terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=499" target="_blank">00:08:19.360</a></span> | <span class="t">that are controlling the noise schedule. And then, let's say I have some conditioning vector, and by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=504" target="_blank">00:08:24.240</a></span> | <span class="t">conditioning vector, I essentially mean some text embeddings or some other form of structural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=509" target="_blank">00:08:29.680</a></span> | <span class="t">controls, like let's say depth maps, segmentation maps, and so on. And then, you have your diffusion model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=516" target="_blank">00:08:36.400</a></span> | <span class="t">that you will learn. And during training, what we basically do is, we compute some intermediate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=524" target="_blank">00:08:44.240</a></span> | <span class="t">representations of the images, or let's say if you are working on the pixel space, you add small amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=531" target="_blank">00:08:51.120</a></span> | <span class="t">noise to your clean images, and you make the model predict what was the amount of noise that was added. This is one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=538" target="_blank">00:08:58.720</a></span> | <span class="t">very popular and widely adopted parameterization of the diffusion network training. It's called the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=543" target="_blank">00:09:03.520</a></span> | <span class="t">epsilon objective, if you will. And that's basically it. So, we basically make the model learn what was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=550" target="_blank">00:09:10.320</a></span> | <span class="t">the amount of noise that was added. So, that's training. And during sampling, we repeat the noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=558" target="_blank">00:09:18.240</a></span> | <span class="t">prediction part in a sequential manner until and unless we arrive at an image that we feel good about. So, we start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=566" target="_blank">00:09:26.240</a></span> | <span class="t">from a random noise, and we denoise it over a period of time with some condition, which can be text embeddings, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=574" target="_blank">00:09:34.960</a></span> | <span class="t">And then we sort of repeat the denoising process over a period of time until and unless we arrive at a data point that we feel good about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=583" target="_blank">00:09:43.040</a></span> | <span class="t">So, yeah. And for flow matching, which is becoming more and more common these days,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=588" target="_blank">00:09:48.000</a></span> | <span class="t">for example, flux, if you have heard of the model flux or stable diffusion three, these are all flow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=592" target="_blank">00:09:52.960</a></span> | <span class="t">matching based. In flow matching, the paths become more straight. We try to connect noise and clean data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=602" target="_blank">00:10:02.480</a></span> | <span class="t">through a straight path. That's why you see the linear interpolation equation in the first point. And we try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=608" target="_blank">00:10:08.800</a></span> | <span class="t">predict the target velocity field with a neural network. So, the key takeaway here is we try to connect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=616" target="_blank">00:10:16.960</a></span> | <span class="t">noise and clean data through a straight line. But in diffusion models, the path is not assumed to be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=623" target="_blank">00:10:23.120</a></span> | <span class="t">straight line. So, that's why a lot of simplification you will see in flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=630" target="_blank">00:10:30.240</a></span> | <span class="t">But long story cut short, in this talk, we are more interested in the network component of things. We are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=636" target="_blank">00:10:36.320</a></span> | <span class="t">not interested in how you should add the noise, what particular noise schedules you should follow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=641" target="_blank">00:10:41.760</a></span> | <span class="t">and so on. We are more focused on the parameterization side of things in this talk. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=652" target="_blank">00:10:52.400</a></span> | <span class="t">And let's now start discussing the components we might need. The components that are kind of expected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=661" target="_blank">00:11:01.520</a></span> | <span class="t">in a diffusion model. What are the core requirements that we want to see in a diffusion model to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=668" target="_blank">00:11:08.560</a></span> | <span class="t">so that it performs within some bound of expectations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=675" target="_blank">00:11:15.120</a></span> | <span class="t">So, we have to figure a way out to deal with the noisy inputs. As we saw noise, we can't really get rid of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=682" target="_blank">00:11:22.640</a></span> | <span class="t">the noise because that kind of gives us the foundation. And usually, in pixel space, your shapes will look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=690" target="_blank">00:11:30.880</a></span> | <span class="t">something like this if your channels first. And if you are working on the latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=698" target="_blank">00:11:38.800</a></span> | <span class="t">it will, of course, be compressed a bit. And we usually always deal with the latent space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=703" target="_blank">00:11:43.680</a></span> | <span class="t">diffusion model. So, it helps to have the kind of shapes that you would expect to see in your models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=710" target="_blank">00:11:50.880</a></span> | <span class="t">that will flow through. And then you will have to deal with the conditions, right? Because all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=716" target="_blank">00:11:56.880</a></span> | <span class="t">text image models that you see, text is your condition. So, you'll have to deal with your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=722" target="_blank">00:12:02.000</a></span> | <span class="t">conditions. And, of course, time step. So, the amount of noise that gets added to your latents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=728" target="_blank">00:12:08.000</a></span> | <span class="t">it should depend on the time step. Like, let's say, if your time step is 10, the amount of noise that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=733" target="_blank">00:12:13.600</a></span> | <span class="t">will get added to your data points, it will be different from the time step if the time step were to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=740" target="_blank">00:12:20.480</a></span> | <span class="t">somewhere like 1,000 or 100. So, that's why time step is also a very crucial condition. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=747" target="_blank">00:12:27.600</a></span> | <span class="t">it basically tells the model that which point in the trajectory it's in. Should it denoise less? Should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=756" target="_blank">00:12:36.400</a></span> | <span class="t">it denoise more aggressively? And so on. And then you have got other forms of conditions such as class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=763" target="_blank">00:12:43.440</a></span> | <span class="t">maybe tiger, bird, or just like natural language. And then we have to also model the dependencies quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=771" target="_blank">00:12:51.680</a></span> | <span class="t">a bit. How should the noisy inputs interact with your conditions? Of course, you might want to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=778" target="_blank">00:12:58.480</a></span> | <span class="t">about these things through the lens of cross-attention and so on. But we'll get to that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=785" target="_blank">00:13:05.360</a></span> | <span class="t">And then how should the final outputs be produced? Should we just flat out decode everything if we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=791" target="_blank">00:13:11.920</a></span> | <span class="t">to deal with transformer-based architectures? Should we be up-sampling if we were to deal with pure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=797" target="_blank">00:13:17.280</a></span> | <span class="t">convolutional architectures? And so on. And usually, for the diffusion network, the input shape exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=805" target="_blank">00:13:25.840</a></span> | <span class="t">equals the output shape until and unless you are dealing with a separate parameterization of diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=811" target="_blank">00:13:31.520</a></span> | <span class="t">There are other parameterizations, but it's way more common to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=814" target="_blank">00:13:34.720</a></span> | <span class="t">the inputs that are flowing through the diffusion model will equal the outputs that the diffusion model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=821" target="_blank">00:13:41.360</a></span> | <span class="t">is supposed to produce. And then, yes, here's a bit of a history. So, DDPM, all the early work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=830" target="_blank">00:13:50.720</a></span> | <span class="t">in this area, DDPM latent diffusion models, they all used unit-based architectures. And I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=838" target="_blank">00:13:58.720</a></span> | <span class="t">historically speaking, units have kind of dominated this area for quite a bit, like StyleGAN, they all had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=846" target="_blank">00:14:06.240</a></span> | <span class="t">a unit-based architecture, right? And then, until SDXL, which is a fairly recent, I would like to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=852" target="_blank">00:14:12.560</a></span> | <span class="t">recent because it's like from 2023. And if you are trying to develop a chronology of all the architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=858" target="_blank">00:14:18.800</a></span> | <span class="t">that have come around, I would like to argue that SDXL is fairly decent. And also, based on its usage,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=865" target="_blank">00:14:25.200</a></span> | <span class="t">I think it's decent. So, until SDXL, all the works relied on unit. So, I think it makes sense to sort of discuss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=875" target="_blank">00:14:35.360</a></span> | <span class="t">the unit-based architecture in this paradigm and then start seeing why we really need to transition to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=884" target="_blank">00:14:44.720</a></span> | <span class="t">transformers. So, yeah. And, of course, the unit for diffusion, it's giant. It's also one of the reasons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=896" target="_blank">00:14:56.000</a></span> | <span class="t">why you would want to probably get rid of it, but that's not the end of the story. But let's try to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=902" target="_blank">00:15:02.160</a></span> | <span class="t">what are the different components that are involved in our giant unit. So, you, of course, need to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=908" target="_blank">00:15:08.320</a></span> | <span class="t">an input convolutional stem that directly operates at the inputs that are coming at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=914" target="_blank">00:15:14.720</a></span> | <span class="t">And then, you have a bunch of down blocks, which is basically comprised of custom residual blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=921" target="_blank">00:15:21.840</a></span> | <span class="t">made of normalization layers and convolutional layers. And then, you have got custom transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=928" target="_blank">00:15:28.480</a></span> | <span class="t">blocks. Again, normalization projections and regular transformer blocks. And then, you have got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=934" target="_blank">00:15:34.960</a></span> | <span class="t">convolutional layers for upsampling. So, basically, you areâ€¦ So, when you are operating on the latent space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=942" target="_blank">00:15:42.320</a></span> | <span class="t">you would want to upsample to a higher resolution. And then, you would have some middle block like you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=950" target="_blank">00:15:50.000</a></span> | <span class="t">would have in a standard unit architecture where you would not have any resolution changes. And then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=957" target="_blank">00:15:57.440</a></span> | <span class="t">you would have a series of up blocks, which will basically downsample the upsampled, you know, outputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=964" target="_blank">00:16:04.640</a></span> | <span class="t">that you had in your down blocks. And finally, you will produce the output that will have the same shape</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=970" target="_blank">00:16:10.480</a></span> | <span class="t">as your input. And it basically resembles the same kind of blocks that you would have in your down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=978" target="_blank">00:16:18.160</a></span> | <span class="t">block counterpart. But instead of doing upsampling, it will have downsampling layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=984" target="_blank">00:16:24.160</a></span> | <span class="t">So, yeah. And there are some miscellaneous things that you should worry about. For example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=989" target="_blank">00:16:29.680</a></span> | <span class="t">time step embeddings, how you should embed your time steps, and additional embeddings. For example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=995" target="_blank">00:16:35.040</a></span> | <span class="t">if it's a class conditional model, the way to embed them would be different. If it's a text image model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1002" target="_blank">00:16:42.400</a></span> | <span class="t">the way to embed the text embeddings would be different. Basically, the way you modulate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1006" target="_blank">00:16:46.640</a></span> | <span class="t">all your conditional embeddings, that will change depending on the kind of conditions that you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1011" target="_blank">00:16:51.280</a></span> | <span class="t">dealing with. And here's basically how a down block of the unit architecture looks like. I mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1018" target="_blank">00:16:58.800</a></span> | <span class="t">it's so, so giant that I had to transpose it. So, it's kind of very painful to even think in the head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1024" target="_blank">00:17:04.960</a></span> | <span class="t">So, if you are someone that works with architectures quite a bit, I think we'll have to solemnly agree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1030" target="_blank">00:17:10.320</a></span> | <span class="t">that this is bad. This is bad news already. I mean, I can't even imagine it in my head. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1036" target="_blank">00:17:16.000</a></span> | <span class="t">and here's a bit of a zoomed-in look of what goes into that down block component of the unit. You have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1045" target="_blank">00:17:25.120</a></span> | <span class="t">got a bunch of custom ResNet blocks, as I was discussing. I've got a bunch of custom transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1050" target="_blank">00:17:30.720</a></span> | <span class="t">blocks, as I was discussing, but nothing too brutal. Nothing that you haven't seen already. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1055" target="_blank">00:17:35.760</a></span> | <span class="t">it's just a bunch of composition of those blocks. And I have also tried mentioning the resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1061" target="_blank">00:17:41.440</a></span> | <span class="t">changes in each of the stages. So, yeah. And putting it all together, it looks something like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1069" target="_blank">00:17:49.200</a></span> | <span class="t">this. I mean, I have tried shortening it quite a bit. But I mean, if I had to imagine it in my head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1075" target="_blank">00:17:55.200</a></span> | <span class="t">it's going to be extremely painful. And the blocks are just, oh, damn. It's very prohibitive in nature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1081" target="_blank">00:18:01.200</a></span> | <span class="t">just for the task of image generation. So, yeah. That's like the complete perspective. I know it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1089" target="_blank">00:18:09.040</a></span> | <span class="t">not as complete as you might expect. But it's also hard to kind of fit all the unit in a single screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1095" target="_blank">00:18:15.760</a></span> | <span class="t">So, yeah. And then of course, the natural next thing that one would have tried would be to try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1105" target="_blank">00:18:25.680</a></span> | <span class="t">replace the convolutional layers with MLP blocks to try to simplify quite a bit. And yeah, folks from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1110" target="_blank">00:18:30.880</a></span> | <span class="t">Google tried it in the UWIT architecture. So, that's there. That was the precursor to the pure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1120" target="_blank">00:18:40.720</a></span> | <span class="t">transformer-based architectures, but not quite. It still had its fair share of complicacies and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1127" target="_blank">00:18:47.200</a></span> | <span class="t">architectural painfulness, as you saw in the unit-based design. So, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1133" target="_blank">00:18:53.360</a></span> | <span class="t">Now, I'm going to now try to motivate why we really need a pure transformer-based architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1141" target="_blank">00:19:01.920</a></span> | <span class="t">Now, the first point is probably very obvious. We would want to benefit from the advancements that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1147" target="_blank">00:19:07.600</a></span> | <span class="t">are happening in the transformer-based architectures, like all the divine benevolence, as Noam Shazir likes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1153" target="_blank">00:19:13.200</a></span> | <span class="t">to call it. You would want to have Swigloo. You would want to have QK normalization. You would want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1158" target="_blank">00:19:18.240</a></span> | <span class="t">have parallel MLP layers and so on. So, that's one. Of course, good scaling properties and so on. And then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1166" target="_blank">00:19:26.000</a></span> | <span class="t">let's say you want to connect the pure transformer-based diffusion architecture or the backbone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1173" target="_blank">00:19:33.760</a></span> | <span class="t">with some other backbones, let's say a pure LLM-based backbone, the integration becomes very easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1180" target="_blank">00:19:40.000</a></span> | <span class="t">And then, it allows you to get rid of the giant unit, which I guess is my main motivation. But I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1187" target="_blank">00:19:47.440</a></span> | <span class="t">hope I was able to convince you fairly strongly that why we need a change, like a paradigm shift in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1196" target="_blank">00:19:56.320</a></span> | <span class="t">architectures that are primarily inspired from the unit design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1200" target="_blank">00:20:00.800</a></span> | <span class="t">And also, this is side to the sore eyes already. I mean, this is not uncommon. We all know that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1210" target="_blank">00:20:10.320</a></span> | <span class="t">is the standard forward pass in a vision transformer network. They should feel very familiar at this point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1216" target="_blank">00:20:16.320</a></span> | <span class="t">in time. Now, the point I am trying to make here is this doesn't have to change a whole lot. If we were to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1224" target="_blank">00:20:24.160</a></span> | <span class="t">sort of extrapolate this to image generation. And I think you will agree with me that this is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1230" target="_blank">00:20:30.640</a></span> | <span class="t">changing a lot. Like all the core components are there. The patchification is there. The positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1238" target="_blank">00:20:38.320</a></span> | <span class="t">embeddings are there. Of course, the way to class embed things, that's different. The Y embedder bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1245" target="_blank">00:20:45.360</a></span> | <span class="t">you would still need to have a component to embed your time steps. That's different. But the rest of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1250" target="_blank">00:20:50.240</a></span> | <span class="t">components, it's still there. Like you are still iterating through the blocks. You have your final</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1255" target="_blank">00:20:55.840</a></span> | <span class="t">layer to finally decode your outputs and so on. And then non-patchification layer. So like this is still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1261" target="_blank">00:21:01.920</a></span> | <span class="t">very similar to how you would do it in a standard bit. But of course, you have to, you know, account for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1269" target="_blank">00:21:09.680</a></span> | <span class="t">generation head also at the same time. So my point is, you are not changing a whole lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1277" target="_blank">00:21:17.840</a></span> | <span class="t">in the standard bit forward pass. Now taking a closer look, I think this should also kind of feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1286" target="_blank">00:21:26.320</a></span> | <span class="t">very familiar apart from a few mods here and there. Like we have got some scale and shift parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1292" target="_blank">00:21:32.560</a></span> | <span class="t">which I'm going to discuss in a bit. But rest of the other blocks, like you have got the same layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1298" target="_blank">00:21:38.560</a></span> | <span class="t">you have got the patchification layer, you have got some embeddings and so on. And then you have got a linear and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1303" target="_blank">00:21:43.600</a></span> | <span class="t">reshape operation. So most of it should feel familiar, but the ones that are apparently a little foreign, I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1312" target="_blank">00:21:52.080</a></span> | <span class="t">discuss them now. So yeah. Let's start with the time step bit. Like how do we actually embed time steps?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1319" target="_blank">00:21:59.680</a></span> | <span class="t">I'm, I've been talking about time steps for quite a bit now. But let's now see how do we actually embed the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1326" target="_blank">00:22:06.960</a></span> | <span class="t">steps. And time steps are really important. And I'm also going to show you the expected shapes, the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1334" target="_blank">00:22:14.000</a></span> | <span class="t">shapes. And in this case, this is batch size comma the hidden dimension of the transform blocks that you are expecting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1341" target="_blank">00:22:21.760</a></span> | <span class="t">So how do we embed time steps? And time steps can range in between zero to thousand, where zero meaning like no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1348" target="_blank">00:22:28.240</a></span> | <span class="t">noise and thousand should mean it's fully noised.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1351" target="_blank">00:22:31.280</a></span> | <span class="t">So each T is embedded into sinusoidal frequency to make them phase aware. Like at any point in time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1361" target="_blank">00:22:41.200</a></span> | <span class="t">the network is seeing extremely low frequencies and extremely higher frequencies. And it must be aware of the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1367" target="_blank">00:22:47.120</a></span> | <span class="t">of phase it should be, it should be modulating into. So that's why sinusoidal frequencies are really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1374" target="_blank">00:22:54.080</a></span> | <span class="t">helpful. And then after that, how the network should weigh these different frequencies. And in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1381" target="_blank">00:23:01.120</a></span> | <span class="t">model those weights, we basically pass it through a very shallow MLP. And then how do you embed class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1388" target="_blank">00:23:08.960</a></span> | <span class="t">labels? You just take an nn.embedding layer as simple as that. And it's the standard patchification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1395" target="_blank">00:23:15.520</a></span> | <span class="t">You do it with a convolutional stem. And for positional encodings, they use the standard sine cosine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1401" target="_blank">00:23:21.760</a></span> | <span class="t">scheme. And the final conditioning is you first embed the time steps and then you embed the class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1411" target="_blank">00:23:31.040</a></span> | <span class="t">labels and you basically sum them up. And then you have your final condition that goes into the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1417" target="_blank">00:23:37.520</a></span> | <span class="t">blocks. And this is very important to note that C remains fixed across all the blocks. So that's very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1423" target="_blank">00:23:43.920</a></span> | <span class="t">important to note. And you would probably think in order to model the conditioning with the actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1436" target="_blank">00:23:56.240</a></span> | <span class="t">inputs, the noisy latency, you would probably want to use cross-attention, but that's not the case, as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1443" target="_blank">00:24:03.200</a></span> | <span class="t">will see in a few slides later. So let's take a step back and try to think of how we can inject the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1454" target="_blank">00:24:14.640</a></span> | <span class="t">conditioning bit into the transformer blocks. So we have something called adaptive layer norm, which is very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1463" target="_blank">00:24:23.200</a></span> | <span class="t">important in order to be able to model the stylistic aspects that you are getting out of your images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1471" target="_blank">00:24:31.440</a></span> | <span class="t">And it's basically this. So you have your standard layer norm and then you have an additional set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1476" target="_blank">00:24:36.880</a></span> | <span class="t">parameters, which we call modulation parameters, which basically operate on the condition space. And remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1484" target="_blank">00:24:44.960</a></span> | <span class="t">the condition is basically a summation of the time step embeddings as well as the class embeddings. So that's your condition right there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1495" target="_blank">00:24:55.440</a></span> | <span class="t">And skipping the regular transformer bits, like the QKV, the multi-head self-attention and the MLP layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1502" target="_blank">00:25:02.640</a></span> | <span class="t">we know the equations that govern the computations that take place within a standard transformer encoder block, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1509" target="_blank">00:25:09.920</a></span> | <span class="t">So this is fairly well known. Now the part that's not known at this point in time, hopefully,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1515" target="_blank">00:25:15.840</a></span> | <span class="t">is how do we modulate the conditioning bit? How do we actually inject the conditioning in the transformer blocks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1523" target="_blank">00:25:23.760</a></span> | <span class="t">So this is how we do it. Instead of doing any cross-attention, which would have been a fairly natural choice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1529" target="_blank">00:25:29.760</a></span> | <span class="t">I guess, we actually do not do cross-attention. We are still doing self-attention and then we are basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1536" target="_blank">00:25:36.720</a></span> | <span class="t">modulating the conditioning along with self-attention, as you can see in the bottom half of your equations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1544" target="_blank">00:25:44.320</a></span> | <span class="t">And these modulation parameters are learned from the modality that you are training these things on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1549" target="_blank">00:25:49.520</a></span> | <span class="t">And then in order to get your final outputs, you basically have a single-layer decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1558" target="_blank">00:25:58.960</a></span> | <span class="t">and then you basically unpatchify it to get the same shape as your inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1568" target="_blank">00:26:08.560</a></span> | <span class="t">Now, a note on init, because the initialization is fairly important. It's all standard vision transformer init,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1577" target="_blank">00:26:17.280</a></span> | <span class="t">but with two key modifications. Each transformer block is initialized as identity block, taking inspiration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1586" target="_blank">00:26:26.000</a></span> | <span class="t">from the early works in ImageNet training, wherein if you have a bunch of ResNet blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1591" target="_blank">00:26:31.760</a></span> | <span class="t">you usually initialize the beta parameter in the batch normalization layer as zero. It helps with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1596" target="_blank">00:26:36.880</a></span> | <span class="t">training stability and stabilization, and turns out that's the case here as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1600" target="_blank">00:26:40.240</a></span> | <span class="t">Now, coming to the adaptive layer normalization thing, it's very important and it's also more compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1608" target="_blank">00:26:48.880</a></span> | <span class="t">efficient. And as I was mentioning, cross-attention would have been the natural choice in order to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1616" target="_blank">00:26:56.960</a></span> | <span class="t">kind of model the dependency between the conditions and the noise inputs, but that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1622" target="_blank">00:27:02.160</a></span> | <span class="t">not at all the case, if you take a look at the graph. In fact, adaptive layer norm performs the best and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1628" target="_blank">00:27:08.000</a></span> | <span class="t">beats cross-attention big time. And it's also not because the conditions are fairly simple to model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1634" target="_blank">00:27:14.240</a></span> | <span class="t">but it's because when you are operating with continuous modalities like images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1639" target="_blank">00:27:19.120</a></span> | <span class="t">it's not that trivial to model dependencies with cross-attention. And when you are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1646" target="_blank">00:27:26.000</a></span> | <span class="t">conditions are simple like class embeddings, it doesn't make sense to model them with cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1652" target="_blank">00:27:32.720</a></span> | <span class="t">It's also because it's a waste of compute, and the graph kind of confirms it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1660" target="_blank">00:27:40.720</a></span> | <span class="t">And later works have also explored like a more compute efficient variant of adaptive layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1665" target="_blank">00:27:45.920</a></span> | <span class="t">which we are going to get to in a couple slides later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1668" target="_blank">00:27:48.320</a></span> | <span class="t">And how you are modulating these conditions, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1671" target="_blank">00:27:51.920</a></span> | <span class="t">in this case, we are basically operating on a summation of the time step embeddings and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1678" target="_blank">00:27:58.240</a></span> | <span class="t">class embeddings. Now, how you are modulating it across and throughout your different transformer blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1684" target="_blank">00:28:04.960</a></span> | <span class="t">that becomes very important, as we will see in a couple blocks later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1690" target="_blank">00:28:10.560</a></span> | <span class="t">And as expected, it scales fairly graciously with more compute and turns out that you can basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1697" target="_blank">00:28:17.360</a></span> | <span class="t">apply all the unit-based training techniques to a diffusion transformer. So, that's pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1705" target="_blank">00:28:25.920</a></span> | <span class="t">And it scales pretty graciously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1707" target="_blank">00:28:27.760</a></span> | <span class="t">It performs also well when compared to other equi-sized unit counterparts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1714" target="_blank">00:28:34.720</a></span> | <span class="t">And of course, at this point in time, you must be thinking no one really does class conditional in the space of image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1726" target="_blank">00:28:46.160</a></span> | <span class="t">So, yeah, that's where we are headed next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1728" target="_blank">00:28:48.880</a></span> | <span class="t">Now, I want to try to motivate what it would take to enable text to image generation in the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1741" target="_blank">00:29:01.200</a></span> | <span class="t">diffusion transformer architecture, because I think it makes sense to approach the problem in that sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1748" target="_blank">00:29:08.240</a></span> | <span class="t">like, what are the components that are missing in a standard diffusion transformer so that it becomes a tool for text to image generation as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1756" target="_blank">00:29:16.160</a></span> | <span class="t">And I think PixArt Alpha is one of the early works that explored it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1760" target="_blank">00:29:20.960</a></span> | <span class="t">So, yeah, we are going to definitely see it in details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1765" target="_blank">00:29:25.520</a></span> | <span class="t">Now, one natural question would be how to embed the input natural language text prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1772" target="_blank">00:29:32.480</a></span> | <span class="t">And the answer would be simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1774" target="_blank">00:29:34.000</a></span> | <span class="t">You would need a text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1776" target="_blank">00:29:36.400</a></span> | <span class="t">And that's exactly what the PixArt Alpha work does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1779" target="_blank">00:29:39.840</a></span> | <span class="t">And then how to learn your contextual dependencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1785" target="_blank">00:29:45.200</a></span> | <span class="t">Now we have, instead of classes, we have got natural language text on top of time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1794" target="_blank">00:29:54.160</a></span> | <span class="t">We could do self-attention on noisy latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1797" target="_blank">00:29:57.120</a></span> | <span class="t">And then we could do cross-attention in between your noisy latents and text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1801" target="_blank">00:30:01.360</a></span> | <span class="t">And mind you, this text is not just class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1804" target="_blank">00:30:04.320</a></span> | <span class="t">This text is some natural language description, like a baby astronaut hatching out of an egg on the moon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1811" target="_blank">00:30:11.440</a></span> | <span class="t">So this is natural language description we are talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1813" target="_blank">00:30:13.920</a></span> | <span class="t">This is not simply class labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1816" target="_blank">00:30:16.000</a></span> | <span class="t">So that could make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1818" target="_blank">00:30:18.720</a></span> | <span class="t">It could make sense to have self-attention on noisy latents,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1823" target="_blank">00:30:23.040</a></span> | <span class="t">to model the local dependencies within the patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1826" target="_blank">00:30:26.160</a></span> | <span class="t">And then also to cross-attention in between the noisy patches and the text embeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1832" target="_blank">00:30:32.320</a></span> | <span class="t">And then we'll have to figure a way out to modulate the time steps as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1837" target="_blank">00:30:37.520</a></span> | <span class="t">And as we saw in the DIT work, that it's important to modulate the time step embeddings throughout your transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1847" target="_blank">00:30:47.440</a></span> | <span class="t">And then, if you have got access to a class conditional diffusion transformer, and if it's compatible, it might also make sense to kind of initialize some blocks from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1858" target="_blank">00:30:58.400</a></span> | <span class="t">Because why waste compute, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1861" target="_blank">00:31:01.680</a></span> | <span class="t">It might help with training stabilization, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1865" target="_blank">00:31:05.200</a></span> | <span class="t">Now, that's exactly what Pixart Alpha does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1872" target="_blank">00:31:12.000</a></span> | <span class="t">So if you were paying attention at this point in time, you would have realized that implementing these things is not extremely challenging, but it helps to know that that's exactly what Pixart Alpha does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1889" target="_blank">00:31:29.120</a></span> | <span class="t">So it uses a text encoder to embed the prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1892" target="_blank">00:31:32.240</a></span> | <span class="t">It uses self-attention on noisy latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1895" target="_blank">00:31:35.040</a></span> | <span class="t">It uses cross-attention to model the dependency between the noisy latents and the text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1902" target="_blank">00:31:42.000</a></span> | <span class="t">And it also initializes from a class conditional DIT model in order to accelerate training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1906" target="_blank">00:31:46.960</a></span> | <span class="t">So it kind of helps to know that you can still think about these things and see it getting implemented in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1914" target="_blank">00:31:54.960</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1915" target="_blank">00:31:55.360</a></span> | <span class="t">And here's some discussion around the use of text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1921" target="_blank">00:32:01.840</a></span> | <span class="t">So Pixart Alpha used Flan T5XXL in order to really get that text rendering ability, and some of the concurrent works like Imagine, they showed that if you scale the text encoder, and if you ever wanted to render text in your images, having a better text encoder actually helps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1942" target="_blank">00:32:22.880</a></span> | <span class="t">And also then there's this problem of long prompt following ability, because models like SDXL, they rely on clip for embedding text, and clip has a very short context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1955" target="_blank">00:32:35.280</a></span> | <span class="t">I think it's 77, but with the T5XXL, you get a way longer context length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1961" target="_blank">00:32:41.040</a></span> | <span class="t">So you basically get to have longer prompts, you get to describe your prompt in a bit more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1967" target="_blank">00:32:47.680</a></span> | <span class="t">So that's there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1970" target="_blank">00:32:50.080</a></span> | <span class="t">And also exploring the space of text encoder, it's still a kind of good research problem to take a look at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1976" target="_blank">00:32:56.720</a></span> | <span class="t">And if you were wondering, why not just use a standard language model in place of T5?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1982" target="_blank">00:33:02.320</a></span> | <span class="t">Well, it's not that difficult to actually use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1984" target="_blank">00:33:04.720</a></span> | <span class="t">At many works like Lumina, they explore it quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1988" target="_blank">00:33:08.560</a></span> | <span class="t">And if you were wondering, diffusion models are already compute bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1992" target="_blank">00:33:12.480</a></span> | <span class="t">So why add the baggage of adding another heavy model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=1996" target="_blank">00:33:16.720</a></span> | <span class="t">Well, I think it's okay to use that because computing prompt embedding, it's a one-step process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2002" target="_blank">00:33:22.320</a></span> | <span class="t">So it's okay to have a memory bound model like a large language model in the mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2007" target="_blank">00:33:27.680</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2011" target="_blank">00:33:31.360</a></span> | <span class="t">And here we will again see the return of adaptive layer norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2016" target="_blank">00:33:36.720</a></span> | <span class="t">And remember, if you forgot, remember that for each diffusion transformer block, we were operating on a summation of timestep embeddings and class embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2030" target="_blank">00:33:50.000</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2030" target="_blank">00:33:50.320</a></span> | <span class="t">And for Pixart Alpha, we basically, we already have a way to compute our text embeddings, which we do not want to touch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2041" target="_blank">00:34:01.360</a></span> | <span class="t">Because those embeddings are already computed with a dedicated rich text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2047" target="_blank">00:34:07.200</a></span> | <span class="t">So we maybe do not want to touch those embeddings and modulate them, unlike diffusion transformers, where we modulated the class embeddings as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2056" target="_blank">00:34:16.080</a></span> | <span class="t">And instead of having adaptive layer normalization blocks in every diffusion transformer encoder block, we basically maintain tables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2067" target="_blank">00:34:27.360</a></span> | <span class="t">We basically maintain embedding tables and we sum them up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2071" target="_blank">00:34:31.360</a></span> | <span class="t">So instead of doing another matmul for each and every transformer encoder block, we basically get away with addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2081" target="_blank">00:34:41.040</a></span> | <span class="t">Addition with another embedding table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2084" target="_blank">00:34:44.640</a></span> | <span class="t">And it helps us reduce the compute quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2087" target="_blank">00:34:47.360</a></span> | <span class="t">It helps us reduce 27% of the original diffusion transformer computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2092" target="_blank">00:34:52.560</a></span> | <span class="t">And I think the idea is fairly elegant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2094" target="_blank">00:34:54.880</a></span> | <span class="t">It also gives another perspective to think about how you can reduce parameters and still sort of maintain performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2103" target="_blank">00:35:03.680</a></span> | <span class="t">This was good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2105" target="_blank">00:35:05.040</a></span> | <span class="t">Cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2107" target="_blank">00:35:07.840</a></span> | <span class="t">And I must say, this is impressive performance for a fairly compact model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2111" target="_blank">00:35:11.920</a></span> | <span class="t">Like it's only 0.6 billion parameter model and it's already like breaking all the charts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2117" target="_blank">00:35:17.840</a></span> | <span class="t">Like when it came around in 2023, it was fairly, fairly good, fairly good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2122" target="_blank">00:35:22.880</a></span> | <span class="t">I think I have the general scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2127" target="_blank">00:35:27.520</a></span> | <span class="t">No, we don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2128" target="_blank">00:35:28.720</a></span> | <span class="t">But the last chart here, it basically shows you the human preference rating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2133" target="_blank">00:35:33.440</a></span> | <span class="t">Quality wise, it's overall image quality and alignment wise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2138" target="_blank">00:35:38.560</a></span> | <span class="t">It's the alignment between the text and the generated images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2142" target="_blank">00:35:42.480</a></span> | <span class="t">And in both of those aspects, PixArt Alpha performed fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2151" target="_blank">00:35:51.280</a></span> | <span class="t">Now, before I jump to the time and memory complexity of these models, because we are still using the quadratic vanilla attention thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2164" target="_blank">00:36:04.480</a></span> | <span class="t">I want to see if there are any questions at this point in time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2168" target="_blank">00:36:08.720</a></span> | <span class="t">So, I'm going to open the floor for questions, if that's okay, Steven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2171" target="_blank">00:36:11.920</a></span> | <span class="t">That's okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2173" target="_blank">00:36:13.120</a></span> | <span class="t">Yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2177" target="_blank">00:36:17.200</a></span> | <span class="t">You mentioned that the state-of-the-art diffusion models require more than one text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2185" target="_blank">00:36:25.440</a></span> | <span class="t">Why is that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2186" target="_blank">00:36:26.160</a></span> | <span class="t">What's the benefit of having multiple different types of encodings for the same text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2192" target="_blank">00:36:32.880</a></span> | <span class="t">Oh, yeah, for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2194" target="_blank">00:36:34.080</a></span> | <span class="t">So, the question is, why would you want to have multiple text encoders?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2198" target="_blank">00:36:38.320</a></span> | <span class="t">And why does it help improve the image generation performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2203" target="_blank">00:36:43.040</a></span> | <span class="t">So, you saw in class conditional dates, you modulate not just the time step embeddings, but also the class embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2213" target="_blank">00:36:53.120</a></span> | <span class="t">So, the conditional embeddings that we were modulating along with self-attention were a summation of class embeddings and time step embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2220" target="_blank">00:37:00.640</a></span> | <span class="t">Now, for text image models, that is not as trivial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2225" target="_blank">00:37:05.680</a></span> | <span class="t">So, apart from your text embeddings, you also kind of have your time step embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2231" target="_blank">00:37:11.120</a></span> | <span class="t">But what about modulating the other condition that you computed from your text input?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2239" target="_blank">00:37:19.200</a></span> | <span class="t">So, turns out that the richer the representations are, the diverse the representations are, the better it is for the generation backbone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2247" target="_blank">00:37:27.440</a></span> | <span class="t">Now, when I said many models do use more than one text encoders, they usually have a combination of Clip and T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2256" target="_blank">00:37:36.320</a></span> | <span class="t">Now, Clip is an entirely different model and T5 is an entirely different model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2259" target="_blank">00:37:39.840</a></span> | <span class="t">With Clip, you embed some kind of contrastive nature in the text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2264" target="_blank">00:37:44.880</a></span> | <span class="t">And with T5, you have a completely different nature in your text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2270" target="_blank">00:37:50.800</a></span> | <span class="t">So, the more the diverse are these embeddings, the better it is for the generative performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2276" target="_blank">00:37:56.800</a></span> | <span class="t">But there's no systematic study of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2279" target="_blank">00:37:59.840</a></span> | <span class="t">That's what the general belief is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2283" target="_blank">00:38:03.520</a></span> | <span class="t">Yeah, but there are works that get away just by using a single language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2289" target="_blank">00:38:09.440</a></span> | <span class="t">So, maybe the language model, maybe the language models inherit both as a virtue of their good pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2299" target="_blank">00:38:19.840</a></span> | <span class="t">I think most of them are using diffusion transformers because of their efficiencies and also they are easy to adapt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2321" target="_blank">00:38:41.200</a></span> | <span class="t">Like, if you wanted to embed another form of control, like let's say you wanted to additionally prompt the model with some stylistic reference from images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2331" target="_blank">00:38:51.760</a></span> | <span class="t">it's easier to do that on a diffusion transformer as we will see in a couple slides later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2337" target="_blank">00:38:57.120</a></span> | <span class="t">So, in the original diffusion transformer block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2356" target="_blank">00:39:16.000</a></span> | <span class="t">you initialize all your modulation parameters as basically an affine layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2360" target="_blank">00:39:20.800</a></span> | <span class="t">So, you can do that, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2363" target="_blank">00:39:23.040</a></span> | <span class="t">I mean, you can chunk it and then you have your different modulation parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2367" target="_blank">00:39:27.040</a></span> | <span class="t">And this is where your matrix multiplication lies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2374" target="_blank">00:39:34.400</a></span> | <span class="t">And notice how I am computing the modulation parameters, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2378" target="_blank">00:39:38.320</a></span> | <span class="t">But here I am only computing it through an embedding table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2383" target="_blank">00:39:43.040</a></span> | <span class="t">There's no affine transformation happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2385" target="_blank">00:39:45.920</a></span> | <span class="t">And I end up adding it to my timestep embeddings, that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2392" target="_blank">00:39:52.080</a></span> | <span class="t">So, that's how I avoid the matmul.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2394" target="_blank">00:39:54.560</a></span> | <span class="t">And that's largely why I am able to sort of reduce the computation by 27%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2401" target="_blank">00:40:01.840</a></span> | <span class="t">I'll take one last question before I jump to the next section.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2407" target="_blank">00:40:07.200</a></span> | <span class="t">If there's any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2408" target="_blank">00:40:08.560</a></span> | <span class="t">I've got some over Zoom as well as online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2412" target="_blank">00:40:12.240</a></span> | <span class="t">I'll ask some, let me see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2416" target="_blank">00:40:16.160</a></span> | <span class="t">Someone's wondering, is there still any point of GANs for image and video generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2420" target="_blank">00:40:20.800</a></span> | <span class="t">Or have they been fully replaced basically with diffusion models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2424" target="_blank">00:40:24.560</a></span> | <span class="t">I won't say they have been fully replaced because you, as we saw, diffusion models are still sequential in nature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2431" target="_blank">00:40:31.120</a></span> | <span class="t">So, if you're looking for really ultra real-time generation, I think GANs are still the way to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2436" target="_blank">00:40:36.560</a></span> | <span class="t">And many companies are, in fact, using them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2438" target="_blank">00:40:38.880</a></span> | <span class="t">So, if you're looking for really cool one-shot generation, I think GANs is the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2442" target="_blank">00:40:42.800</a></span> | <span class="t">And even if, so there's this literature around timestep distillation that basically looks at reducing the number of timesteps or inference steps that you need in order to produce a good quality image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2454" target="_blank">00:40:54.480</a></span> | <span class="t">So, there you need a GAN loss, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2456" target="_blank">00:40:56.160</a></span> | <span class="t">So, GANs are not going to be completely replaced yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2459" target="_blank">00:40:59.920</a></span> | <span class="t">Okay, because I thought, I thought GANs had a lot of issues like mode collapse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2463" target="_blank">00:41:03.520</a></span> | <span class="t">Oh yeah, they have, but if you have got specific use cases and if you have got a fairly good data set, you can still train a good GAN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2474" target="_blank">00:41:14.800</a></span> | <span class="t">I can ask one more if there's time from online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2476" target="_blank">00:41:16.960</a></span> | <span class="t">Sure, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2477" target="_blank">00:41:17.280</a></span> | <span class="t">Image generation generally requires a lot of data to train, especially for diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2484" target="_blank">00:41:24.560</a></span> | <span class="t">Are there techniques or architectural choices for low data regimes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2488" target="_blank">00:41:28.240</a></span> | <span class="t">That's a good one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2491" target="_blank">00:41:31.840</a></span> | <span class="t">The short answer is no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2493" target="_blank">00:41:33.360</a></span> | <span class="t">You need, I mean, I think it's kind of correlated with your use case a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2501" target="_blank">00:41:41.120</a></span> | <span class="t">Like for medical imaging, probably you do not need a whole lot of diversity, but if you need a fairly well and generative model, I think you need to train it on a lot of data to inherit all the biases that you are looking for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2515" target="_blank">00:41:55.840</a></span> | <span class="t">Like if you want to train on the distribution of natural images and you want the model to always produce realistic images, I think you need to have a lot of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2526" target="_blank">00:42:06.880</a></span> | <span class="t">At least diverse data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2528" target="_blank">00:42:08.880</a></span> | <span class="t">Because nowadays for like LLMs, you know, there's like zero shot, few shot learning after you preach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2536" target="_blank">00:42:16.160</a></span> | <span class="t">Oh, I'm going to talk a bit about in context learning for diffusion models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2540" target="_blank">00:42:20.000</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2540" target="_blank">00:42:20.320</a></span> | <span class="t">But, but you still, you are assuming you have access to a pre-trained model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2546" target="_blank">00:42:26.240</a></span> | <span class="t">Right, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2546" target="_blank">00:42:26.960</a></span> | <span class="t">So there you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2550" target="_blank">00:42:30.640</a></span> | <span class="t">Alright, I'm going to, I'm going to start this leg of the talk by discussing the quadratic time and memory complexity argument a bit, because now we are in the image generation territory and when you are trying to generate like really high resolution images, like 4K images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2574" target="_blank">00:42:54.640</a></span> | <span class="t">So, vanilla attention becomes extremely prohibitive even if you were to operate on the latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2581" target="_blank">00:43:01.920</a></span> | <span class="t">Like take a look at the dimensions of the latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2583" target="_blank">00:43:03.920</a></span> | <span class="t">Uh, let's say your, uh, number of latent channels is 16 and your latent dimensions are 512, 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2591" target="_blank">00:43:11.920</a></span> | <span class="t">Like it's, it's still way too large, way larger than what we are used to seeing in the VLM space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2599" target="_blank">00:43:19.200</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2600" target="_blank">00:43:20.080</a></span> | <span class="t">And I have some dummy computations here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2602" target="_blank">00:43:22.720</a></span> | <span class="t">This is of course not using flash attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2604" target="_blank">00:43:24.640</a></span> | <span class="t">If I were to give you some dummy estimates, it would be like 190 GBs in floating point 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2614" target="_blank">00:43:34.400</a></span> | <span class="t">And this is all like reasonable defaults.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2616" target="_blank">00:43:36.800</a></span> | <span class="t">I mean, I have 24 attention heads, I've got batch size one, the sequence length is flattened out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2625" target="_blank">00:43:45.040</a></span> | <span class="t">I would need 190 GB.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2627" target="_blank">00:43:47.920</a></span> | <span class="t">This is of course not using flash attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2630" target="_blank">00:43:50.480</a></span> | <span class="t">But you kind of get an idea of the prohibitive computation space of the quadratic attention that we usually use in diffusion worlds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2639" target="_blank">00:43:59.440</a></span> | <span class="t">If we were to deal with really, uh, high quality and high resolution, uh, images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2645" target="_blank">00:44:05.520</a></span> | <span class="t">So what could we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2647" target="_blank">00:44:07.680</a></span> | <span class="t">Like two simple things that we could do is operate on an even more compressed space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2652" target="_blank">00:44:12.800</a></span> | <span class="t">Like this, this 1, 16, 512, 512, this space is already compressed enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2659" target="_blank">00:44:19.040</a></span> | <span class="t">But could we even increase the compression ratio further?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2662" target="_blank">00:44:22.080</a></span> | <span class="t">Right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2663" target="_blank">00:44:23.520</a></span> | <span class="t">And then the second, second obvious thing could be to use some form of linear attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2668" target="_blank">00:44:28.560</a></span> | <span class="t">that doesn't do the n cross n, uh, multiplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2671" target="_blank">00:44:31.440</a></span> | <span class="t">So that brings me to my next architecture, which is the SANA architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2678" target="_blank">00:44:38.160</a></span> | <span class="t">And it uses both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2679" target="_blank">00:44:39.360</a></span> | <span class="t">It uses an, an even more compressed latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2683" target="_blank">00:44:43.680</a></span> | <span class="t">And it also uses a linear variant, uh, of the, of the, of the attention mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2689" target="_blank">00:44:49.200</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2690" target="_blank">00:44:50.640</a></span> | <span class="t">So a linear attention is on the other hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2695" target="_blank">00:44:55.680</a></span> | <span class="t">And of course you might expect some performance loss with, with the linear complexity of the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2702" target="_blank">00:45:02.400</a></span> | <span class="t">mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2703" target="_blank">00:45:03.440</a></span> | <span class="t">And to compensate for the performance lost, we use mix FF.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2707" target="_blank">00:45:07.440</a></span> | <span class="t">And I'm gonna, I'm gonna get into the details in a bit, but just wanted to give you a quick overview.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2713" target="_blank">00:45:13.120</a></span> | <span class="t">So SANA does self, self-attention, self-linear attention as you will.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2717" target="_blank">00:45:17.840</a></span> | <span class="t">But it still does cross-attention to model the dependencies between the noisy latents, as well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2723" target="_blank">00:45:23.680</a></span> | <span class="t">as, uh, the text prompts that you are providing it to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2726" target="_blank">00:45:26.640</a></span> | <span class="t">And then there's no n-square computation happening in the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2731" target="_blank">00:45:31.360</a></span> | <span class="t">And the equation makes that clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2733" target="_blank">00:45:33.600</a></span> | <span class="t">So we have got shared terms, shared terms computed from the KV projections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2738" target="_blank">00:45:38.080</a></span> | <span class="t">And these are reused for all the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2740" target="_blank">00:45:40.640</a></span> | <span class="t">And this way we are basically and effectively, uh, not doing the n cross n multiplications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2747" target="_blank">00:45:47.120</a></span> | <span class="t">And all of the multiplications are upper bound to n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2749" target="_blank">00:45:49.840</a></span> | <span class="t">That's why we are able to reduce from order of n-squared to order of n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2755" target="_blank">00:45:55.360</a></span> | <span class="t">And then, as I was mentioning, there has to be some form of accountability as we are not using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2762" target="_blank">00:46:02.640</a></span> | <span class="t">quadratic attention mechanism, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2765" target="_blank">00:46:05.200</a></span> | <span class="t">So we use mix FFN blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2767" target="_blank">00:46:07.280</a></span> | <span class="t">It's basically a bunch of inverted residual blocks and point-first convolutions in order to model the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2772" target="_blank">00:46:12.800</a></span> | <span class="t">local dependencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2774" target="_blank">00:46:14.160</a></span> | <span class="t">Because it turns out that when you take the softmax out of, out of the picture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2778" target="_blank">00:46:18.240</a></span> | <span class="t">you lose all the notion of locality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2780" target="_blank">00:46:20.960</a></span> | <span class="t">Let's not say all, majority of the locality notions are taken out also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2786" target="_blank">00:46:26.240</a></span> | <span class="t">So that's why you, you need some, some components to also model the locality aspect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2792" target="_blank">00:46:32.320</a></span> | <span class="t">And mix FFN blocks are used for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2794" target="_blank">00:46:34.240</a></span> | <span class="t">And for the first time, Sana got rid of the positional embedding bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2800" target="_blank">00:46:40.160</a></span> | <span class="t">So, I mean, it's very, it's very funny to say it nope.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2805" target="_blank">00:46:45.520</a></span> | <span class="t">So no positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2807" target="_blank">00:46:47.440</a></span> | <span class="t">And mix FFN blocks actually helps because you have a bunch of convolutional blocks in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2812" target="_blank">00:46:52.640</a></span> | <span class="t">So we are still returning to the convolutional argument, but not so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2816" target="_blank">00:46:56.880</a></span> | <span class="t">Like it's, it's not fully convolutional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2819" target="_blank">00:46:59.360</a></span> | <span class="t">You just have a few convolutional layers thrown in there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2822" target="_blank">00:47:02.720</a></span> | <span class="t">So it's largely did based, but instead of using like a linear MLP,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2827" target="_blank">00:47:07.680</a></span> | <span class="t">you have two convolutional layers to, to, to account for the local interactions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2834" target="_blank">00:47:14.240</a></span> | <span class="t">And it performs fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2839" target="_blank">00:47:19.680</a></span> | <span class="t">For its compact size, it performs fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2841" target="_blank">00:47:21.920</a></span> | <span class="t">I must say that we shouldn't take, take into account the kernel fusion and the fancy, fancy DPM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2849" target="_blank">00:47:29.840</a></span> | <span class="t">problem solver that they're using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2850" target="_blank">00:47:30.880</a></span> | <span class="t">But up until the fourth row, I think it's still performing fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2856" target="_blank">00:47:36.480</a></span> | <span class="t">Like it's giving very decent general performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2860" target="_blank">00:47:40.000</a></span> | <span class="t">It's giving very decent DPG performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2863" target="_blank">00:47:43.600</a></span> | <span class="t">And these are all metrics that assess, that assess a given image in terms of compositionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2870" target="_blank">00:47:50.800</a></span> | <span class="t">in terms of their fidelity, in terms of their overall quality, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2876" target="_blank">00:47:56.240</a></span> | <span class="t">And these metrics are fairly well grounded in terms of reality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2880" target="_blank">00:48:00.400</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2881" target="_blank">00:48:01.920</a></span> | <span class="t">And, and this is, this is probably going to be the final flavor of attention that I'm going to discuss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2892" target="_blank">00:48:12.080</a></span> | <span class="t">before I move on to other topics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2894" target="_blank">00:48:14.720</a></span> | <span class="t">And this is, I think, a bit new.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2898" target="_blank">00:48:18.560</a></span> | <span class="t">And also new in the sense that no one in the VLM space really does it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2902" target="_blank">00:48:22.640</a></span> | <span class="t">So what happens if we were to kind of, you know, model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2906" target="_blank">00:48:26.720</a></span> | <span class="t">dependencies of the different modalities in separate spaces?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2910" target="_blank">00:48:30.400</a></span> | <span class="t">So let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2911" target="_blank">00:48:31.600</a></span> | <span class="t">So, and the, and, and one motivation behind this could be text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2918" target="_blank">00:48:38.240</a></span> | <span class="t">Let's say you are computing text embeddings with a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2922" target="_blank">00:48:42.160</a></span> | <span class="t">You will end up inheriting a lot of bias from it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2925" target="_blank">00:48:45.840</a></span> | <span class="t">Like the unidirectionality bias will be there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2929" target="_blank">00:48:49.440</a></span> | <span class="t">If you're using a standard autoregressive large language model, you will end up inheriting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2934" target="_blank">00:48:54.640</a></span> | <span class="t">unidirectionality bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2936" target="_blank">00:48:56.320</a></span> | <span class="t">So, so you will have bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2939" target="_blank">00:48:59.360</a></span> | <span class="t">And they might, you know, creep into your, your generative model in all sorts of different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2945" target="_blank">00:49:05.280</a></span> | <span class="t">So, so one idea could be to do QKV projections, but, but separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2952" target="_blank">00:49:12.960</a></span> | <span class="t">Like you do QKV projections on the text embeddings and you also, you also maintain another set of QKV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2959" target="_blank">00:49:19.680</a></span> | <span class="t">projection parameters for your noisy latins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2963" target="_blank">00:49:23.360</a></span> | <span class="t">And then you concatenate them before you compute the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2966" target="_blank">00:49:26.720</a></span> | <span class="t">So you operate on a concatenate, concatenated representation before you actually do the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2973" target="_blank">00:49:33.440</a></span> | <span class="t">So you project your QKV stuff on your image latins separately from the text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2984" target="_blank">00:49:44.960</a></span> | <span class="t">You concatenate them before you actually compute attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2989" target="_blank">00:49:49.040</a></span> | <span class="t">So this is basically it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2991" target="_blank">00:49:51.360</a></span> | <span class="t">So this is MMDIT that was introduced in the Stable Diffusion 3 paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=2995" target="_blank">00:49:55.120</a></span> | <span class="t">And their motivation was to get rid of the different biases that might be there in the text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3002" target="_blank">00:50:02.640</a></span> | <span class="t">And they also showed how qualitatively different text embeddings can be from the image embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3008" target="_blank">00:50:08.640</a></span> | <span class="t">So I guess that kind of gives you a hint about why we might need different text embeddings to not end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3016" target="_blank">00:50:16.240</a></span> | <span class="t">inherit each other's biases and have more diversity in the mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3019" target="_blank">00:50:19.600</a></span> | <span class="t">So it basically looks like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3024" target="_blank">00:50:24.960</a></span> | <span class="t">So it might feel a little complicated, but the reason why it's so big is because we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3031" target="_blank">00:50:31.200</a></span> | <span class="t">separate projection matrices for the different modalities that we are modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3035" target="_blank">00:50:35.440</a></span> | <span class="t">So on the left-hand side, we have got the captions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3040" target="_blank">00:50:40.560</a></span> | <span class="t">And on the right-hand side, we have got the noisy latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3043" target="_blank">00:50:43.280</a></span> | <span class="t">And we have got separate QKV projections and adaptive layer non-matrices for the separate modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3049" target="_blank">00:50:49.840</a></span> | <span class="t">So that's why it feels big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3051" target="_blank">00:50:51.840</a></span> | <span class="t">But conceptually, it's basically this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3054" target="_blank">00:50:54.560</a></span> | <span class="t">So you have got separate adaptive layer non-matrices for the separate modalities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3062" target="_blank">00:51:02.720</a></span> | <span class="t">You have got separate QKV projection matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3065" target="_blank">00:51:05.040</a></span> | <span class="t">You have got separate output projection matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3068" target="_blank">00:51:08.400</a></span> | <span class="t">And then you have got separate, everything separate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3071" target="_blank">00:51:11.920</a></span> | <span class="t">Everything is separate from the two modalities that we are interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3076" target="_blank">00:51:16.800</a></span> | <span class="t">So in a way, it kind of gives a way to co-evolve the two embeddings from the two different modalities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3088" target="_blank">00:51:28.160</a></span> | <span class="t">that we are working with for the given task, which is image generation in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3091" target="_blank">00:51:31.760</a></span> | <span class="t">And also, if we are very used to cross-attention, you might want to ask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3098" target="_blank">00:51:38.800</a></span> | <span class="t">if we were to compute attention in this way, how do we do masking in the first place?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3103" target="_blank">00:51:43.360</a></span> | <span class="t">And that's actually an active area of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3105" target="_blank">00:51:45.440</a></span> | <span class="t">We do not know how to do masks holistically if we were to do mmdit variant of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3112" target="_blank">00:51:52.400</a></span> | <span class="t">And then modulation happens with both time steps and the conditional embedding that you are operating with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3122" target="_blank">00:52:02.240</a></span> | <span class="t">So Stable Diffusion 3 uses a different set of pulled text embeddings that are usually computed from CLIP and not the T5-based text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3132" target="_blank">00:52:12.000</a></span> | <span class="t">So again, you need some form of diversity to not end up inheriting the bias from the other text encoder that you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3141" target="_blank">00:52:21.840</a></span> | <span class="t">And Stable Diffusion 3 uses three, but they can be mixed and matched during inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3148" target="_blank">00:52:28.000</a></span> | <span class="t">They showed that you need the T5 if you need to have really good text rendering capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3154" target="_blank">00:52:34.480</a></span> | <span class="t">But other than that, you can still do a lot with the two clips that they use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3161" target="_blank">00:52:41.360</a></span> | <span class="t">And you can drop the T5 if you are not solely focused on text rendering tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3166" target="_blank">00:52:46.080</a></span> | <span class="t">And of course, it matters quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3172" target="_blank">00:52:52.960</a></span> | <span class="t">You might want to ask, does mmdit matter at all?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3177" target="_blank">00:52:57.040</a></span> | <span class="t">And it turns out that it does matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3179" target="_blank">00:52:59.200</a></span> | <span class="t">Like they tried all forms of different attention variants.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3183" target="_blank">00:53:03.440</a></span> | <span class="t">They tried cross-dit, they tried uvid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3186" target="_blank">00:53:06.240</a></span> | <span class="t">And mmdit seems to be the variant that gives you the lowest validation loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3190" target="_blank">00:53:10.800</a></span> | <span class="t">And they also show that validation loss is fairly well correlated with the kind of image generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3196" target="_blank">00:53:16.960</a></span> | <span class="t">matrix that we care about, such as GenieVal and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3200" target="_blank">00:53:20.640</a></span> | <span class="t">And it scales fairly well, but it needs QKNORM relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3205" target="_blank">00:53:25.760</a></span> | <span class="t">And thanks to the concurrent set of works, they didn't have to reinvent QKNORM from the scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3212" target="_blank">00:53:32.240</a></span> | <span class="t">So thanks to the developments in the regular transformer literature, they were able to just use QKNORM to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3220" target="_blank">00:53:40.240</a></span> | <span class="t">solve the training and stability issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3222" target="_blank">00:53:42.320</a></span> | <span class="t">And I think Stability Fusion 3 is basically incomplete without this picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3228" target="_blank">00:53:48.400</a></span> | <span class="t">And I'm not showing this picture just for fun, but also because it shows how complex is this prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3238" target="_blank">00:53:58.080</a></span> | <span class="t">and how well Stability Fusion 3 was able to interpret this prompt and get us a creature like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3245" target="_blank">00:54:05.360</a></span> | <span class="t">So I must give credits to the authors of Stability Fusion 3 who came up with these kinds of prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3251" target="_blank">00:54:11.520</a></span> | <span class="t">But it does fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3253" target="_blank">00:54:13.840</a></span> | <span class="t">So it was among the first kinds of models that shows impressive prompt following ability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3260" target="_blank">00:54:20.960</a></span> | <span class="t">and also while also preserving the details that we care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3267" target="_blank">00:54:27.040</a></span> | <span class="t">So that was quick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3268" target="_blank">00:54:28.080</a></span> | <span class="t">And mmdit didn't stop here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3274" target="_blank">00:54:34.000</a></span> | <span class="t">There are different flavors of mmdits that I wanted to discuss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3278" target="_blank">00:54:38.320</a></span> | <span class="t">Like in Stability Fusion 3, all the transformer blocks followed the mmdit flavor of attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3287" target="_blank">00:54:47.440</a></span> | <span class="t">And it's computationally demanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3289" target="_blank">00:54:49.520</a></span> | <span class="t">I mean, if you have worked with transformer encoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3292" target="_blank">00:54:52.720</a></span> | <span class="t">you would appreciate the computational intensity of mmdits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3299" target="_blank">00:54:59.440</a></span> | <span class="t">because you are having to kind of maintain separate projection matrices for the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3305" target="_blank">00:55:05.120</a></span> | <span class="t">modalities that you are working with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3306" target="_blank">00:55:06.960</a></span> | <span class="t">So it's computationally extremely demanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3308" target="_blank">00:55:08.880</a></span> | <span class="t">So maybe we could combine mmdit blocks and regular dit blocks to be able to better utilize the flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3317" target="_blank">00:55:17.120</a></span> | <span class="t">So conceptually, this becomes this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3321" target="_blank">00:55:21.280</a></span> | <span class="t">So you have some number of mmdit blocks, you concatenate the final representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3328" target="_blank">00:55:28.080</a></span> | <span class="t">and then you operate on the concatenate space and basically do vanilla dit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3333" target="_blank">00:55:33.680</a></span> | <span class="t">You do not do mmdit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3335" target="_blank">00:55:35.200</a></span> | <span class="t">And you end up utilizing the flops a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3337" target="_blank">00:55:37.840</a></span> | <span class="t">And that's exactly what recent models like Flux from Black Forest Labs, they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3343" target="_blank">00:55:43.360</a></span> | <span class="t">So another twisty one could be you have modality A, you compute output A by passing it through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3354" target="_blank">00:55:54.560</a></span> | <span class="t">a bunch of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3356" target="_blank">00:55:56.480</a></span> | <span class="t">And then you have got modality B, let's say text embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3359" target="_blank">00:55:59.600</a></span> | <span class="t">You pass it through another set of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3362" target="_blank">00:56:02.560</a></span> | <span class="t">You concatenate these outputs, and then you pass them through another set of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3369" target="_blank">00:56:09.280</a></span> | <span class="t">And then you basically have your final output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3371" target="_blank">00:56:11.920</a></span> | <span class="t">And in this way, you can configure all the different blocks in their own manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3379" target="_blank">00:56:19.680</a></span> | <span class="t">like the transformer blocks for modality A could be different from the transformer blocks that you would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3387" target="_blank">00:56:27.040</a></span> | <span class="t">use for modality B. So this way, you have a greater level of flexibility and control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3392" target="_blank">00:56:32.960</a></span> | <span class="t">And that's what the Lumina 2 work did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3396" target="_blank">00:56:36.080</a></span> | <span class="t">And as we can see, for conditions, they pass it through a separate set of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3402" target="_blank">00:56:42.400</a></span> | <span class="t">For noisy latents, they pass it through a separate set of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3406" target="_blank">00:56:46.640</a></span> | <span class="t">And they end up concatenating them and then again, they have another set of transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3412" target="_blank">00:56:52.400</a></span> | <span class="t">So transformer blocks on the left-hand side, in the RHS big block, can be different from the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3421" target="_blank">00:57:01.760</a></span> | <span class="t">blocks used to model the noisy latents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3425" target="_blank">00:57:05.360</a></span> | <span class="t">And I want to give you a sense of how we can think of simplifying all of this design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3438" target="_blank">00:57:18.400</a></span> | <span class="t">Because it might feel complicated at this point in time, but I don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3444" target="_blank">00:57:24.080</a></span> | <span class="t">Maybe we can simplify it quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3446" target="_blank">00:57:26.560</a></span> | <span class="t">But I want to quickly see if there are any questions at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3450" target="_blank">00:57:30.880</a></span> | <span class="t">Do we have any questions over Zoom?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3453" target="_blank">00:57:33.120</a></span> | <span class="t">Yeah, we've got some online questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3455" target="_blank">00:57:35.200</a></span> | <span class="t">Let me see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3458" target="_blank">00:57:38.240</a></span> | <span class="t">Somebody just asked, is GPT-4.0 a completely different architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3461" target="_blank">00:57:41.600</a></span> | <span class="t">How does that compare with the diffusion transformer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3464" target="_blank">00:57:44.880</a></span> | <span class="t">That's a good one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3467" target="_blank">00:57:47.120</a></span> | <span class="t">I think it uses a hybrid architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3470" target="_blank">00:57:50.720</a></span> | <span class="t">It definitely has an LLM component to it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3473" target="_blank">00:57:53.440</a></span> | <span class="t">which was evolved to generate images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3476" target="_blank">00:57:56.720</a></span> | <span class="t">I'm going to come to this sort of hybrid architectures in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3480" target="_blank">00:58:00.240</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3480" target="_blank">00:58:00.560</a></span> | <span class="t">And then someone's wondering for evaluation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3483" target="_blank">00:58:03.440</a></span> | <span class="t">are there good automatic metrics for image and video generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3486" target="_blank">00:58:06.720</a></span> | <span class="t">Or is it mainly human-based subjective evaluation, similar to creative writing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3491" target="_blank">00:58:11.520</a></span> | <span class="t">I think it's an ensemble of different metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3494" target="_blank">00:58:14.800</a></span> | <span class="t">Like you can't evaluate an image on a single metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3498" target="_blank">00:58:18.880</a></span> | <span class="t">And also it depends on what you are exactly looking for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3502" target="_blank">00:58:22.320</a></span> | <span class="t">Like if you are more interested in compositionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3505" target="_blank">00:58:25.600</a></span> | <span class="t">the metrics will change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3506" target="_blank">00:58:26.960</a></span> | <span class="t">You are more, if you are more interested in aesthetics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3509" target="_blank">00:58:29.440</a></span> | <span class="t">the metrics will change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3510" target="_blank">00:58:30.560</a></span> | <span class="t">So that depends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3512" target="_blank">00:58:32.240</a></span> | <span class="t">Let me see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3515" target="_blank">00:58:35.440</a></span> | <span class="t">What are your thoughts on the vision models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3517" target="_blank">00:58:37.840</a></span> | <span class="t">Someone noted, there's a common perspective about like image generation models failing to generate things like fingers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3526" target="_blank">00:58:46.240</a></span> | <span class="t">I also see that, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3530" target="_blank">00:58:50.400</a></span> | <span class="t">do they still suffer from things like counting and like spatial consistency?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3534" target="_blank">00:58:54.400</a></span> | <span class="t">Or are those basic things pretty much like solved?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3537" target="_blank">00:58:57.520</a></span> | <span class="t">I think models like flags definitely, they do a whole lot better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3541" target="_blank">00:59:01.600</a></span> | <span class="t">And there's a question of like, what are the major additional challenges of video generation compared to image generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3556" target="_blank">00:59:16.800</a></span> | <span class="t">And how ways to overcome them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3558" target="_blank">00:59:18.000</a></span> | <span class="t">Well, the first problem is the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3561" target="_blank">00:59:21.280</a></span> | <span class="t">Because image generation, diffusion-based image generation models are already compute intensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3566" target="_blank">00:59:26.720</a></span> | <span class="t">And then with video generation models, you have got another dimension of temporality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3572" target="_blank">00:59:32.080</a></span> | <span class="t">So they just become more compute intensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3575" target="_blank">00:59:35.440</a></span> | <span class="t">And then you have got more cars of dimensionality to basically address.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3581" target="_blank">00:59:41.360</a></span> | <span class="t">And then if you are generating more and more frames, they just become more and more compute intensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3587" target="_blank">00:59:47.840</a></span> | <span class="t">So how do you, how do you kind of make them more efficient is definitely the need of the art.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3592" target="_blank">00:59:52.800</a></span> | <span class="t">It also adds things like temporal dependencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3595" target="_blank">00:59:55.280</a></span> | <span class="t">Yeah, yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3596" target="_blank">00:59:56.560</a></span> | <span class="t">Yeah, another access of dependency to model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3599" target="_blank">00:59:59.520</a></span> | <span class="t">Right, right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3600" target="_blank">01:00:00.080</a></span> | <span class="t">And you are basically doing full 3D attention, which is extremely prohibitive to even think about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3615" target="_blank">01:00:15.440</a></span> | <span class="t">In the video, you don't have to generate frame by frame from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3618" target="_blank">01:00:18.560</a></span> | <span class="t">You already have something more like figuring out and then just do a small change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3623" target="_blank">01:00:23.120</a></span> | <span class="t">Well, I mean, if you, if you take, if you take a 2D image generation model, and if you try to conflate it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3631" target="_blank">01:00:31.920</a></span> | <span class="t">such that it can also do spatio-temporal generation, well, it doesn't turn out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3637" target="_blank">01:00:37.440</a></span> | <span class="t">It will work well, at least in the realistic setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3640" target="_blank">01:00:40.400</a></span> | <span class="t">Like if you wanted to generate cinematic frames, it won't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3644" target="_blank">01:00:44.080</a></span> | <span class="t">Because the spatio-temporal consistency, it just gets lost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3647" target="_blank">01:00:47.840</a></span> | <span class="t">Oh, but, but, but, but there has to be a limit, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3655" target="_blank">01:00:55.040</a></span> | <span class="t">Like if you are operating with that many frames as your previous input, you're, you're like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3661" target="_blank">01:01:01.040</a></span> | <span class="t">the queue also becomes extremely prohibitive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3663" target="_blank">01:01:03.840</a></span> | <span class="t">So you have to figure out how, how, how you can compress the temporal dimension effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3669" target="_blank">01:01:09.920</a></span> | <span class="t">Like, let's say you have a variable number of frames.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3672" target="_blank">01:01:12.640</a></span> | <span class="t">Now, how do you map it on the latent temporal level so that it still has some meaning, uh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3678" target="_blank">01:01:18.560</a></span> | <span class="t">to it while being efficient at the same time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3680" target="_blank">01:01:20.880</a></span> | <span class="t">So when you are doing videos, you no longer just have the spatial compression, but also you have temporal compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3689" target="_blank">01:01:29.440</a></span> | <span class="t">And how, how do you model the two as you, as you, as you make progress?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3693" target="_blank">01:01:33.200</a></span> | <span class="t">So, are there any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3698" target="_blank">01:01:38.160</a></span> | <span class="t">I'll just take one maybe?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3703" target="_blank">01:01:43.920</a></span> | <span class="t">Come on, Zoom asks, can you give some intuition about adaptive layer normalization and why it works so well?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3710" target="_blank">01:01:50.080</a></span> | <span class="t">Oh yeah. So, so for, for layer norm, so you, you are basically using layer norm for stabilizing training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3717" target="_blank">01:01:57.760</a></span> | <span class="t">I would say like you have more stable representations across the, across the, across the blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3724" target="_blank">01:02:04.800</a></span> | <span class="t">But for images, there are certain kind of characteristics that you would want to model beyond the standard representations that you are computing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3732" target="_blank">01:02:12.320</a></span> | <span class="t">And in, and you will have to let those kind of, let those features flow freely into your transform blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3740" target="_blank">01:02:20.320</a></span> | <span class="t">And that's why you need the modulation parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3742" target="_blank">01:02:22.480</a></span> | <span class="t">Otherwise, the interaction in between the normalization parameters and your regular attention features or whatever MLP features that you are computing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3752" target="_blank">01:02:32.160</a></span> | <span class="t">they don't get to interact, uh, in a way that will benefit the generation performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3757" target="_blank">01:02:37.200</a></span> | <span class="t">If you were to deal with just understanding or maybe just discriminative performance, it wouldn't have mattered that much, much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3764" target="_blank">01:02:44.560</a></span> | <span class="t">But if you care about stylistic aspects and fidelity, you need to kind of modulate, uh, the additional features that you are getting from the, uh, visual, uh, cues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3774" target="_blank">01:02:54.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3775" target="_blank">01:02:55.440</a></span> | <span class="t">Any, any, any, any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3779" target="_blank">01:02:59.520</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3780" target="_blank">01:03:00.320</a></span> | <span class="t">So you mentioned, we don't know yet how to do masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3783" target="_blank">01:03:03.440</a></span> | <span class="t">For, for mmdit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3785" target="_blank">01:03:05.200</a></span> | <span class="t">Yeah, mmdit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3786" target="_blank">01:03:06.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3786" target="_blank">01:03:06.640</a></span> | <span class="t">What's the next best thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3789" target="_blank">01:03:09.200</a></span> | <span class="t">Is there an alternative to masking?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3791" target="_blank">01:03:11.200</a></span> | <span class="t">Maybe that's a hunch, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3792" target="_blank">01:03:12.320</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3793" target="_blank">01:03:13.840</a></span> | <span class="t">So may, why would you need masking?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3796" target="_blank">01:03:16.640</a></span> | <span class="t">That's a good reason to ask for mmdit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3798" target="_blank">01:03:18.960</a></span> | <span class="t">Because if you, if you are, so mmdit, mmdit was done in order to benefit, like, in order to get away with the unidirectionality bias that you may have in your text embedding representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3811" target="_blank">01:03:31.760</a></span> | <span class="t">So that was one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3813" target="_blank">01:03:33.680</a></span> | <span class="t">Second, if you have really long prompts, you do not need to compute masks in the first place, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3818" target="_blank">01:03:38.400</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3818" target="_blank">01:03:38.560</a></span> | <span class="t">And it also turns out that it's always better to have long prompts, like descriptive prompts, rather than having to have short prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3826" target="_blank">01:03:46.880</a></span> | <span class="t">So that is kind of a hand wavy way to answer this question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3831" target="_blank">01:03:51.120</a></span> | <span class="t">But long story cut short, it's really non-trivial to add masks when you are doing mmdit attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3837" target="_blank">01:03:57.680</a></span> | <span class="t">So, uh, there you go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3839" target="_blank">01:03:59.200</a></span> | <span class="t">So that, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3840" target="_blank">01:04:00.160</a></span> | <span class="t">It was about doing separately QKV on both text and image, how about on the image side?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3846" target="_blank">01:04:06.320</a></span> | <span class="t">Yeah, you, you, so when you are doing the image, image interaction, you could in theory do masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3852" target="_blank">01:04:12.960</a></span> | <span class="t">But again, how would you frame the problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3856" target="_blank">01:04:16.480</a></span> | <span class="t">where would you end up adding the masks if you are just doing text to image?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3860" target="_blank">01:04:20.560</a></span> | <span class="t">Maybe, maybe if you have another form of condition, conditional control, maybe you would want to mask some of the interactions there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3868" target="_blank">01:04:28.640</a></span> | <span class="t">But if you are just restricted to text to image, where would you add the masks in the first place?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3873" target="_blank">01:04:33.200</a></span> | <span class="t">Yeah, but what I'm asking to avoid masking, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3876" target="_blank">01:04:36.240</a></span> | <span class="t">Oh, yeah, yeah, yeah, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3877" target="_blank">01:04:37.680</a></span> | <span class="t">I mean, probably that's why they didn't do masking in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3880" target="_blank">01:04:40.400</a></span> | <span class="t">Maybe their motivations, yeah, yeah, exactly, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3883" target="_blank">01:04:43.760</a></span> | <span class="t">I'll maybe take one final question if there's any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3887" target="_blank">01:04:47.520</a></span> | <span class="t">Okay, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3890" target="_blank">01:04:50.400</a></span> | <span class="t">So I, I also want to leave you with some ideas for simplification, simplifying the design a bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3897" target="_blank">01:04:57.520</a></span> | <span class="t">if these things felt a little complicated, because they do not have to be complicated at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3903" target="_blank">01:05:03.360</a></span> | <span class="t">We can simplify it quite a bit, and let's see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3906" target="_blank">01:05:06.880</a></span> | <span class="t">So how much is parameter sharing is useful?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3912" target="_blank">01:05:12.000</a></span> | <span class="t">Like things like Adiln that we saw and the, and the, and the work on Pixar Alpha that reduce the computation by 27%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3920" target="_blank">01:05:20.560</a></span> | <span class="t">How much is that kind of parameter sharing is useful?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3923" target="_blank">01:05:23.680</a></span> | <span class="t">And we saw Adiln already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3926" target="_blank">01:05:26.240</a></span> | <span class="t">Can we also share QKVO and the MLP, like Albert?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3931" target="_blank">01:05:31.280</a></span> | <span class="t">This is from back in the days, Albert, where we shared all the projection matrices for, for a couple of layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3938" target="_blank">01:05:38.320</a></span> | <span class="t">We also shared the MLP matrices and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3940" target="_blank">01:05:40.960</a></span> | <span class="t">And do we really need, like self-attention and then cross-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3946" target="_blank">01:05:46.560</a></span> | <span class="t">Or do we really need MMD?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3948" target="_blank">01:05:48.080</a></span> | <span class="t">I think this will probably answer your question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3950" target="_blank">01:05:50.400</a></span> | <span class="t">So look forward, I look forward to that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3952" target="_blank">01:05:52.160</a></span> | <span class="t">So do, do we really need these things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3954" target="_blank">01:05:54.640</a></span> | <span class="t">Because this kinds, this in a sense complicates the design space quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3959" target="_blank">01:05:59.120</a></span> | <span class="t">Can we simplify it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3962" target="_blank">01:06:02.160</a></span> | <span class="t">So can we basically do self-attention on this concatenated space?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3966" target="_blank">01:06:06.320</a></span> | <span class="t">We do not have to do self-attention on noisy latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3969" target="_blank">01:06:09.200</a></span> | <span class="t">noisy latency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3970" target="_blank">01:06:10.080</a></span> | <span class="t">We do not have to do cross-attention on noisy latency and text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3973" target="_blank">01:06:13.680</a></span> | <span class="t">We do not have to do MMDit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3975" target="_blank">01:06:15.360</a></span> | <span class="t">Can we basically concatenate all the image tokens and the text tokens and compute self-attention on it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3981" target="_blank">01:06:21.840</a></span> | <span class="t">Can we do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3984" target="_blank">01:06:24.880</a></span> | <span class="t">And text encoders, like what's the secret sauce?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3987" target="_blank">01:06:27.440</a></span> | <span class="t">We are using three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3988" target="_blank">01:06:28.560</a></span> | <span class="t">We are using two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3989" target="_blank">01:06:29.520</a></span> | <span class="t">What helps?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3990" target="_blank">01:06:30.720</a></span> | <span class="t">What helps the cause of text to image generation at the end of the day?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3995" target="_blank">01:06:35.040</a></span> | <span class="t">It turns out you can simplify things a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=3998" target="_blank">01:06:38.160</a></span> | <span class="t">So one thing that you can do is you can parameter share quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4002" target="_blank">01:06:42.800</a></span> | <span class="t">You can parameter share QKVO if you are looking for efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4006" target="_blank">01:06:46.800</a></span> | <span class="t">You can definitely parameter share the adaptive layer norm parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4012" target="_blank">01:06:52.080</a></span> | <span class="t">And you can basically do self-attention on a concatenated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4014" target="_blank">01:06:54.960</a></span> | <span class="t">representation space of image tokens and text tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4019" target="_blank">01:06:59.040</a></span> | <span class="t">So this is, I think, good news, but not so much because Apple didn't open source this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4024" target="_blank">01:07:04.880</a></span> | <span class="t">So we'll have to do our own.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4026" target="_blank">01:07:06.560</a></span> | <span class="t">But this is good to know that you can simplify the design quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4030" target="_blank">01:07:10.960</a></span> | <span class="t">So the extreme right hand side is the simplified design that I was talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4035" target="_blank">01:07:15.120</a></span> | <span class="t">So you are basically operating on a concatenated space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4040" target="_blank">01:07:20.400</a></span> | <span class="t">You are reducing the adaptive layer norm parameters quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4044" target="_blank">01:07:24.800</a></span> | <span class="t">And then your parameter sharing the QKV projections as well as the MLP layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4049" target="_blank">01:07:29.920</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4050" target="_blank">01:07:30.800</a></span> | <span class="t">And it turns out to be working well, fairly well, I must say, in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4058" target="_blank">01:07:38.160</a></span> | <span class="t">So the green one is the simplified design, and it always gets the lowest amount of loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4065" target="_blank">01:07:45.280</a></span> | <span class="t">So yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4066" target="_blank">01:07:46.880</a></span> | <span class="t">And as I was mentioning, other than sharing the adaptive layer norm parameters, you can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4074" target="_blank">01:07:54.880</a></span> | <span class="t">share the QKV parameters as well as the MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4079" target="_blank">01:07:59.280</a></span> | <span class="t">if you can compromise some of the quality a bit, and if you are only targeting efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4084" target="_blank">01:08:04.160</a></span> | <span class="t">And for text encoders, the Apple folks found out that bi-directional clip and text-only LLM is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4092" target="_blank">01:08:12.880</a></span> | <span class="t">So instead of using a combination of clip and T5, if you use clip and a regular large language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4099" target="_blank">01:08:19.680</a></span> | <span class="t">it's better for text presentations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4101" target="_blank">01:08:21.280</a></span> | <span class="t">So the performance is pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4105" target="_blank">01:08:25.200</a></span> | <span class="t">The final row is Apple's work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4108" target="_blank">01:08:28.240</a></span> | <span class="t">It's called DIT year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4109" target="_blank">01:08:29.120</a></span> | <span class="t">So it turns out fairly well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4112" target="_blank">01:08:32.080</a></span> | <span class="t">All the bold numbers from DIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4116" target="_blank">01:08:36.480</a></span> | <span class="t">So it turns out pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4118" target="_blank">01:08:38.240</a></span> | <span class="t">So long story cut short, you can simplify the design quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4121" target="_blank">01:08:41.360</a></span> | <span class="t">So you do not need mmDIT as it turns out, but we will see, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4124" target="_blank">01:08:44.800</a></span> | <span class="t">Now, one burning question you must be having at this point in time is, text-to-image is liberating,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4133" target="_blank">01:08:53.280</a></span> | <span class="t">but how do I inject more control, more sources of control?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4138" target="_blank">01:08:58.800</a></span> | <span class="t">Like if I were to do more structural inputs to my text-to-image model, how do I do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4145" target="_blank">01:09:05.760</a></span> | <span class="t">Maybe I want the model to follow a particular pose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4149" target="_blank">01:09:09.200</a></span> | <span class="t">Maybe I want the model to follow a particular segmentation map and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4153" target="_blank">01:09:13.600</a></span> | <span class="t">And how do I do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4154" target="_blank">01:09:14.960</a></span> | <span class="t">And can I combine multiple structural signals at the same time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4158" target="_blank">01:09:18.720</a></span> | <span class="t">So one could be to learn maybe an auxiliary network that gives you a way to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4168" target="_blank">01:09:28.960</a></span> | <span class="t">salient representations from your structural image signals, and maybe you then figure a way out to inject</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4175" target="_blank">01:09:35.040</a></span> | <span class="t">those structural signals into your base diffusion model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4180" target="_blank">01:09:40.480</a></span> | <span class="t">So that could be one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4181" target="_blank">01:09:41.600</a></span> | <span class="t">And control net, these lines of work, they basically follow this philosophy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4189" target="_blank">01:09:49.680</a></span> | <span class="t">And then maybe you could also change the base diffusion transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4194" target="_blank">01:09:54.800</a></span> | <span class="t">You basically increase your input channels to accept, you know, more controls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4200" target="_blank">01:10:00.640</a></span> | <span class="t">And that's what the flux control framework does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4205" target="_blank">01:10:05.840</a></span> | <span class="t">And maybe you could also learn a small adapter network to model the dependencies between all your conditions and your noisy latent tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4217" target="_blank">01:10:17.920</a></span> | <span class="t">And also one important call-out here is structural control will always have spatial correspondence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4225" target="_blank">01:10:25.200</a></span> | <span class="t">but other tasks like subject-driven generation or image edits, they may not have direct spatial correspondence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4232" target="_blank">01:10:32.240</a></span> | <span class="t">So what do we do in those cases?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4234" target="_blank">01:10:34.160</a></span> | <span class="t">And this is, again, an active area of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4238" target="_blank">01:10:38.240</a></span> | <span class="t">And for videos, well, rope works fairly well for positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4245" target="_blank">01:10:45.520</a></span> | <span class="t">And as I was mentioning, the attention computation space becomes way more prohibitive because it's full 3D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4252" target="_blank">01:10:52.800</a></span> | <span class="t">And if you were thinking about some form of factorized attention, it doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4256" target="_blank">01:10:56.720</a></span> | <span class="t">I have been very explicit about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4259" target="_blank">01:10:59.280</a></span> | <span class="t">And for efficiency, if you are particularly interested in efficiency-related literature around video models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4267" target="_blank">01:11:07.200</a></span> | <span class="t">I highly suggest checking out the LTX video work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4270" target="_blank">01:11:10.480</a></span> | <span class="t">And for performance, like for fidelity, photorealism, and so on, I think one has to be my favorite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4277" target="_blank">01:11:17.040</a></span> | <span class="t">It's pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4279" target="_blank">01:11:19.040</a></span> | <span class="t">I have a little demo here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4281" target="_blank">01:11:21.520</a></span> | <span class="t">It's not that bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4283" target="_blank">01:11:23.600</a></span> | <span class="t">So it's still short-form video, but the realism aspect and the fidelity aspect of these videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4290" target="_blank">01:11:30.640</a></span> | <span class="t">I think they have improved quite a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4292" target="_blank">01:11:32.160</a></span> | <span class="t">And some next-generation architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4297" target="_blank">01:11:37.360</a></span> | <span class="t">Now I'm coming towards the end of my talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4299" target="_blank">01:11:39.280</a></span> | <span class="t">I wanted to also give you a flavor and a sense of next-generation architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4305" target="_blank">01:11:45.200</a></span> | <span class="t">You probably have this question, how do we enable in-context learning in diffusion models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4309" target="_blank">01:11:49.360</a></span> | <span class="t">Like in language model, it's very common, zero-shot learning and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4313" target="_blank">01:11:53.040</a></span> | <span class="t">How do we enable in-context learning in diffusion models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4316" target="_blank">01:11:56.400</a></span> | <span class="t">Current architectures are clearly not sufficient to enable it in a reasonable time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4321" target="_blank">01:12:01.760</a></span> | <span class="t">So the basic idea is you take an LLM model, you add some components so that it also becomes adept at generating images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4329" target="_blank">01:12:09.440</a></span> | <span class="t">Like recent works like Bagel, then there are works like Lada, Mada, they basically follow this area of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4336" target="_blank">01:12:16.720</a></span> | <span class="t">They are not necessarily diffusion-based, but they share a similar philosophy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4342" target="_blank">01:12:22.000</a></span> | <span class="t">So you basically start from a pre-trained LLM, you add components to it so that it also becomes adept at generating images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4349" target="_blank">01:12:29.120</a></span> | <span class="t">Like you could do autoregression on discrete tokens, for example, and you could also do at the same time diffusion on the continuous tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4356" target="_blank">01:12:36.960</a></span> | <span class="t">So that's one line of work that's called Transfusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4359" target="_blank">01:12:39.520</a></span> | <span class="t">So as I was mentioning, this Playground v3, this Fusedit, this Transfusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4365" target="_blank">01:12:45.920</a></span> | <span class="t">Fusedit is from our group.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4367" target="_blank">01:12:47.440</a></span> | <span class="t">It's the only open source work that you will probably find that tackles this problem of in-context learning and how do you sort of explore these architectures in a holistic manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4381" target="_blank">01:13:01.520</a></span> | <span class="t">And if you are already feeling inspired, I hope you are feeling inspired to explore these architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4388" target="_blank">01:13:08.480</a></span> | <span class="t">I highly welcome you to check out the library that I work on at Hugging Face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4392" target="_blank">01:13:12.240</a></span> | <span class="t">It's called Diffusers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4393" target="_blank">01:13:13.120</a></span> | <span class="t">We have got reasonably clean implementations of all the models that I discussed today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4398" target="_blank">01:13:18.800</a></span> | <span class="t">which should probably inspire you to hack into these things and tweak them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4406" target="_blank">01:13:26.160</a></span> | <span class="t">And there's a bunch of things I didn't get to cover, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4409" target="_blank">01:13:29.200</a></span> | <span class="t">I am a little bit over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4411" target="_blank">01:13:31.120</a></span> | <span class="t">I think one minute over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4412" target="_blank">01:13:32.320</a></span> | <span class="t">I'm probably going to finish it in the next one minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4415" target="_blank">01:13:35.600</a></span> | <span class="t">So I apologize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4417" target="_blank">01:13:37.120</a></span> | <span class="t">I didn't cover MOEs, hot topic again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4420" target="_blank">01:13:40.720</a></span> | <span class="t">But MOEs are making their ways in the diffusion community.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4425" target="_blank">01:13:45.120</a></span> | <span class="t">It's called HiDream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4426" target="_blank">01:13:46.080</a></span> | <span class="t">I didn't cover training at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4428" target="_blank">01:13:48.320</a></span> | <span class="t">There are all kinds of shenanigans there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4431" target="_blank">01:13:51.040</a></span> | <span class="t">How do you do data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4432" target="_blank">01:13:52.080</a></span> | <span class="t">How do you do alignment?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4433" target="_blank">01:13:53.200</a></span> | <span class="t">How do you do post-training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4434" target="_blank">01:13:54.400</a></span> | <span class="t">How do you do safety mitigation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4436" target="_blank">01:13:56.240</a></span> | <span class="t">How do you do memorization mitigation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4437" target="_blank">01:13:57.920</a></span> | <span class="t">And so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4438" target="_blank">01:13:58.480</a></span> | <span class="t">And of course, these architectures go well beyond image and video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4443" target="_blank">01:14:03.600</a></span> | <span class="t">They find their application in all sorts of stuff, like robotics, gene synthesis, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4450" target="_blank">01:14:10.880</a></span> | <span class="t">And if you are into mechanistic interpretability, interpretation of these beasts are not trivial at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4457" target="_blank">01:14:17.680</a></span> | <span class="t">So they interest me quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4459" target="_blank">01:14:19.440</a></span> | <span class="t">And yeah, that's about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4463" target="_blank">01:14:23.440</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4463" target="_blank">01:14:23.840</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4464" target="_blank">01:14:24.400</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4464" target="_blank">01:14:24.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4466" target="_blank">01:14:26.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4468" target="_blank">01:14:28.720</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vXtapCFctTI&t=4468" target="_blank">01:14:28.720</a></span> | <span class="t">Thank you.</span></div></div></body></html>