<html><head><title>AI Declarations and AGI Timelines – Looking More Optimistic?</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AI Declarations and AGI Timelines – Looking More Optimistic?</h2><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU"><img src="https://i.ytimg.com/vi/9RmzlRRqfvU/sddefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=39">0:39</a> AGI Timelines<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=130">2:10</a> Open AI Timeline<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=164">2:44</a> Dyson Sphere Prediction<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=246">4:6</a> GPT 5 Prediction<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=324">5:24</a> More Government Oversight<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=376">6:16</a> Regulation<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=436">7:16</a> Scaling<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=518">8:38</a> AGI Commitment<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=559">9:19</a> Safety<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=634">10:34</a> Paradigm Shift<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=682">11:22</a> Autonomous Agents<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=759">12:39</a> Global Opportunities<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=792">13:12</a> Representation Engineering<br><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=904">15:4</a> Conclusion<br><br><div style="text-align: left;"><a href="./9RmzlRRqfvU.html">Whisper Transcript</a> | <a href="./transcript_9RmzlRRqfvU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I'm going to show you a pretty wild range of new predictions from those creating and testing the next generation of AI models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=9" target="_blank">00:00:09.120</a></span> | <span class="t">Not that we can know who's right, but more to show you how unknowable the rest of this decade is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=15" target="_blank">00:00:15.320</a></span> | <span class="t">I'll also cover the AI Safety Summit happening as I speak a few miles away from where I'm recording,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=21" target="_blank">00:00:21.960</a></span> | <span class="t">with fascinating differences between the approach of different AGI labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=27" target="_blank">00:00:27.020</a></span> | <span class="t">Along the way, we'll glimpse the new ChatGPT update that I'm really excited about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=32" target="_blank">00:00:32.160</a></span> | <span class="t">an executive order on flops, and what happens when you activate representations of happiness in a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=39" target="_blank">00:00:39.600</a></span> | <span class="t">But first on timelines to AGI, that's the kind of artificial intelligence that can replicate human intelligence or go further.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=48" target="_blank">00:00:48.180</a></span> | <span class="t">Here is Shane Legg, co-founder of Google DeepMind and their chief AGI scientist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=53" target="_blank">00:00:53.560</a></span> | <span class="t">He's going to reiterate a prediction he made,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=56" target="_blank">00:00:56.660</a></span> | <span class="t">over and over again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=57" target="_blank">00:00:57.000</a></span> | <span class="t">over a decade ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=86" target="_blank">00:01:26.820</a></span> | <span class="t">He thinks the remaining problems with LLMs are solvable in that short timeframe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=95" target="_blank">00:01:35.500</a></span> | <span class="t">At the moment, it looks to me like all the problems are likely solvable with a number of years of research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=103" target="_blank">00:01:43.240</a></span> | <span class="t">I think what you'll see is the existing models maturing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=106" target="_blank">00:01:46.640</a></span> | <span class="t">They'll be less delusional, much more factual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=109" target="_blank">00:01:49.540</a></span> | <span class="t">They'll be more up to date on what's currently going on when they answer questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=113" target="_blank">00:01:53.620</a></span> | <span class="t">They'll become multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=116" target="_blank">00:01:56.640</a></span> | <span class="t">Much more than they currently are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=118" target="_blank">00:01:58.020</a></span> | <span class="t">And this will just make them much more useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=120" target="_blank">00:02:00.280</a></span> | <span class="t">Of course, when he describes increasing multimodality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=123" target="_blank">00:02:03.900</a></span> | <span class="t">he could well be describing Google's new Gemini model set to be released within the next two months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=130" target="_blank">00:02:10.140</a></span> | <span class="t">But what about OpenAI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=131" target="_blank">00:02:11.320</a></span> | <span class="t">Well, for the first time, I heard Sam Altman put an actual date to his predictions of AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=137" target="_blank">00:02:17.160</a></span> | <span class="t">What kind of timeline did you have in mind?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=141" target="_blank">00:02:21.320</a></span> | <span class="t">And has it stayed on that timeline?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=143" target="_blank">00:02:23.560</a></span> | <span class="t">Or is it just wildly out of control?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=145" target="_blank">00:02:25.200</a></span> | <span class="t">I remember talking...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=146" target="_blank">00:02:26.160</a></span> | <span class="t">with John Schulman, one of our co-founders, early on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=149" target="_blank">00:02:29.620</a></span> | <span class="t">And he was like, "Yeah, I think it's gonna be about a 15-year project."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=152" target="_blank">00:02:32.240</a></span> | <span class="t">And I was like, "Yeah, that sounds about right to me."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=153" target="_blank">00:02:33.680</a></span> | <span class="t">I no longer think of like AGI as quite the endpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=156" target="_blank">00:02:36.120</a></span> | <span class="t">But to get to the point where we like accomplish the thing we set out to accomplish,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=160" target="_blank">00:02:40.140</a></span> | <span class="t">that would take us to like 2030, 2031.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=162" target="_blank">00:02:42.040</a></span> | <span class="t">A reasonable estimate with huge error bars.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=164" target="_blank">00:02:44.080</a></span> | <span class="t">And speaking of OpenAI, the former head of alignment at OpenAI, Paul Cristiano,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=169" target="_blank">00:02:49.280</a></span> | <span class="t">made a prediction on the fantastic Dvorkes Patel podcast that frankly made me sit up and pay attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=175" target="_blank">00:02:55.980</a></span> | <span class="t">He predicted that there would be a 15% chance of an AI capable of making a Dyson sphere by 2030.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=184" target="_blank">00:03:04.780</a></span> | <span class="t">With a 40% chance by 2040.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=187" target="_blank">00:03:07.780</a></span> | <span class="t">For reference, that's a hypothetical structure that would surround a star absorbing all of its energy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=193" target="_blank">00:03:13.980</a></span> | <span class="t">The time by which we'll have an AI that is capable of building a Dyson sphere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=198" target="_blank">00:03:18.980</a></span> | <span class="t">And by Dyson sphere, I just understand this to mean like, I don't know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=202" target="_blank">00:03:22.080</a></span> | <span class="t">like a billion times more energy than like all the sunlight incident on Earth or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=205" target="_blank">00:03:25.820</a></span> | <span class="t">I think like, I most often think about what's the chance in like five years, 10 years, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=212" target="_blank">00:03:32.520</a></span> | <span class="t">So maybe I'd say like 15% chance by 2030 and like 40% chance by 2040.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=220" target="_blank">00:03:40.320</a></span> | <span class="t">Those are kind of like cash numbers from six months ago or nine months ago that I haven't revisited in a while.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=225" target="_blank">00:03:45.220</a></span> | <span class="t">Now he did admit a lot of uncertainty, but that has got to be one of the most aggressive predictions I've ever heard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=232" target="_blank">00:03:52.120</a></span> | <span class="t">Of course, being capable of making a Dyson sphere and actually making one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=235" target="_blank">00:03:55.640</a></span> | <span class="t">is very different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=236" target="_blank">00:03:56.940</a></span> | <span class="t">But you do have to sympathize with a member of the public hearing about Dyson spheres</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=242" target="_blank">00:04:02.040</a></span> | <span class="t">and the next day reading about what Bill Gates has said about GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=246" target="_blank">00:04:06.140</a></span> | <span class="t">I subscribed to the German outlet Handelsblatt to get you guys this direct quotation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=251" target="_blank">00:04:11.840</a></span> | <span class="t">So some likes for my dedication to accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=255" target="_blank">00:04:15.340</a></span> | <span class="t">Anyway, Bill Gates said this:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=257" target="_blank">00:04:17.140</a></span> | <span class="t">"Without question, the progression from GPT-2 to 4 has been incredible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=261" target="_blank">00:04:21.640</a></span> | <span class="t">But there are reasons to believe we have reached a plateau.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=264" target="_blank">00:04:24.840</a></span> | <span class="t">There are a lot of things that we haven't done yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=265" target="_blank">00:04:25.480</a></span> | <span class="t">And a lot of people with good ideas working on it, including at OpenAI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=269" target="_blank">00:04:29.480</a></span> | <span class="t">Sam Altman and his colleagues believe GPT-5 will be much better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=272" target="_blank">00:04:32.680</a></span> | <span class="t">but I think we may have reached a limit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=275" target="_blank">00:04:35.180</a></span> | <span class="t">Then again, I've been wrong in the past.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=276" target="_blank">00:04:36.980</a></span> | <span class="t">Why shouldn't it happen again?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=278" target="_blank">00:04:38.480</a></span> | <span class="t">I know what he means, but I just don't think we're hitting a plateau for GPT-5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=283" target="_blank">00:04:43.180</a></span> | <span class="t">With more data, better curated data, video in, video out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=287" target="_blank">00:04:47.380</a></span> | <span class="t">a reasoning module, potentially, as we saw in the recent MLC paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=291" target="_blank">00:04:51.580</a></span> | <span class="t">avatars, a longer context window, and as you can see on screen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=295" target="_blank">00:04:55.320</a></span> | <span class="t">all of these tools and updates link together in a single interface.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=300" target="_blank">00:05:00.120</a></span> | <span class="t">If it's simply the things I've just listed, that won't be a plateau for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=304" target="_blank">00:05:04.720</a></span> | <span class="t">Imagine asking it to go to your website and create an image based on some of your content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=310" target="_blank">00:05:10.020</a></span> | <span class="t">Anyway, yes, GPT-5 or 4.5 might be more of a practical update than a civilization transforming one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=317" target="_blank">00:05:17.520</a></span> | <span class="t">But nevertheless, that's all just 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=320" target="_blank">00:05:20.520</a></span> | <span class="t">What will 2025 bring us, let alone 2030?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=324" target="_blank">00:05:24.320</a></span> | <span class="t">One thing that those few people who are watching this video will notice is that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=325" target="_blank">00:05:25.160</a></span> | <span class="t">what those future years will definitely bring is more government oversight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=329" target="_blank">00:05:29.460</a></span> | <span class="t">While reading through this new executive order from the White House,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=333" target="_blank">00:05:33.360</a></span> | <span class="t">it was mainly about things like creating chief AI officers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=337" target="_blank">00:05:37.360</a></span> | <span class="t">new national research centers, training new researchers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=340" target="_blank">00:05:40.760</a></span> | <span class="t">and giving different deadlines to various departments to enact AI plans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=344" target="_blank">00:05:44.960</a></span> | <span class="t">But there was one reporting requirement that is causing a stir.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=348" target="_blank">00:05:48.860</a></span> | <span class="t">That was a requirement to report on the model weight security and safety of any model that was trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=355" target="_blank">00:05:55.000</a></span> | <span class="t">using a quantity of flops greater than 10^26.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=359" target="_blank">00:05:59.800</a></span> | <span class="t">Or if it's primarily using biological sequence data of 10^23 flops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=365" target="_blank">00:06:05.800</a></span> | <span class="t">That's more raw computing power than any models that are currently out there were trained with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=371" target="_blank">00:06:11.100</a></span> | <span class="t">But people are picking up on using compute as the measurement for regulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=376" target="_blank">00:06:16.100</a></span> | <span class="t">Jim Phan of NVIDIA said this:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=378" target="_blank">00:06:18.500</a></span> | <span class="t">"Regulate actions or outcomes, not the computing process."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=382" target="_blank">00:06:22.500</a></span> | <span class="t">And he gave this example:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=384" target="_blank">00:06:24.840</a></span> | <span class="t">"It takes over 100 million parameters to build a literal killer AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=388" target="_blank">00:06:28.840</a></span> | <span class="t">With a convolutional neural network good at object detection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=392" target="_blank">00:06:32.440</a></span> | <span class="t">and a classifier specifying particular targets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=396" target="_blank">00:06:36.040</a></span> | <span class="t">you could then mount a gun on a robot dog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=398" target="_blank">00:06:38.840</a></span> | <span class="t">All of this would need much less compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=400" target="_blank">00:06:40.840</a></span> | <span class="t">which is why Jim Phan wants regulations at the application layer."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=404" target="_blank">00:06:44.740</a></span> | <span class="t">Well, luckily the UN is working on a resolution on autonomous weapons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=409" target="_blank">00:06:49.440</a></span> | <span class="t">So there is some hope there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=411" target="_blank">00:06:51.540</a></span> | <span class="t">It's early days and wouldn't solve everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=414" target="_blank">00:06:54.680</a></span> | <span class="t">but the AI resolution is very much needed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=416" target="_blank">00:06:56.680</a></span> | <span class="t">And that actually brings us to the AI Safety Summit happening in Bletchley as I speak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=422" target="_blank">00:07:02.280</a></span> | <span class="t">All seven of these companies were asked to come up with their Responsible Capability Scaling policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=428" target="_blank">00:07:08.880</a></span> | <span class="t">In simple terms, that's a bit like them being asked:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=431" target="_blank">00:07:11.880</a></span> | <span class="t">"Under what conditions would you stop scaling or at least pause scaling?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=436" target="_blank">00:07:16.480</a></span> | <span class="t">And I noted OpenAI's response in this section:</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=439" target="_blank">00:07:19.480</a></span> | <span class="t">"We refer to our policy as a risk-informed development policy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=443" target="_blank">00:07:23.680</a></span> | <span class="t">rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=444" target="_blank">00:07:24.520</a></span> | <span class="t">a responsible scaling policy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=446" target="_blank">00:07:26.220</a></span> | <span class="t">because we can experience dramatic increases in capability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=449" target="_blank">00:07:29.720</a></span> | <span class="t">without significant increase in scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=452" target="_blank">00:07:32.020</a></span> | <span class="t">e.g. via algorithmic improvements."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=454" target="_blank">00:07:34.520</a></span> | <span class="t">So it's at least feasible that we might not even need that much compute to hit AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=460" target="_blank">00:07:40.420</a></span> | <span class="t">Take this example with Nvidia training a large language model on doing chip design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=465" target="_blank">00:07:45.620</a></span> | <span class="t">Now at the moment it's not good enough to do anything itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=468" target="_blank">00:07:48.720</a></span> | <span class="t">but it does make their designers more productive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=471" target="_blank">00:07:51.320</a></span> | <span class="t">especially their lower level engineers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=473" target="_blank">00:07:53.420</a></span> | <span class="t">But this is the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=474" target="_blank">00:07:54.360</a></span> | <span class="t">With AI improving AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=476" target="_blank">00:07:56.360</a></span> | <span class="t">And even the CEO of Nvidia said he didn't want this to happen out in the wild.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=482" target="_blank">00:08:02.160</a></span> | <span class="t">In the area of large language models and the future of increasingly greater agency AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=489" target="_blank">00:08:09.360</a></span> | <span class="t">clearly the answer is for as long as it's sensible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=493" target="_blank">00:08:13.360</a></span> | <span class="t">and I think it's going to be sensible for a long time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=495" target="_blank">00:08:15.360</a></span> | <span class="t">is human in the loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=496" target="_blank">00:08:16.360</a></span> | <span class="t">The ability for an AI to self-learn and improve and change out in the wild,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=504" target="_blank">00:08:24.200</a></span> | <span class="t">in the digital form,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=505" target="_blank">00:08:25.200</a></span> | <span class="t">should be avoided.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=506" target="_blank">00:08:26.200</a></span> | <span class="t">And interestingly 74% of the British public don't even want there to be a quick race to superhuman capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">This was a survey from YouGov in the UK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=518" target="_blank">00:08:38.200</a></span> | <span class="t">But back to the scaling policies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=520" target="_blank">00:08:40.200</a></span> | <span class="t">and there was one thing announced yesterday at Bletchley that I really did like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=524" target="_blank">00:08:44.200</a></span> | <span class="t">And that was this commitment from Anthropic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=526" target="_blank">00:08:46.200</a></span> | <span class="t">If they found that any of their future models posed cybersecurity, bioterror or nuclear risks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=532" target="_blank">00:08:52.200</a></span> | <span class="t">then they commit to not deploy them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=534" target="_blank">00:08:54.040</a></span> | <span class="t">And they will not stop deploying that or scaling further until the model never produces such information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=540" target="_blank">00:09:00.040</a></span> | <span class="t">Even when red-teamed by world experts working together with AI engineers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=545" target="_blank">00:09:05.040</a></span> | <span class="t">Think jailbreaking or special prompting techniques designed to elicit the worst behaviour.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=550" target="_blank">00:09:10.040</a></span> | <span class="t">The word never there is particularly interesting because I haven't seen any method yet be 100% reliable at stopping outputs that the companies don't want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=560" target="_blank">00:09:20.040</a></span> | <span class="t">On safety, many people wonder, well don't we already just have Google?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=563" target="_blank">00:09:23.880</a></span> | <span class="t">And the AI said this for Bletchley.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=565" target="_blank">00:09:25.880</a></span> | <span class="t">We found that on its own access to GPT-4 is an insufficient condition for proliferation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=571" target="_blank">00:09:31.880</a></span> | <span class="t">But that it could alter the information available to proliferators especially in comparison to traditional search tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=578" target="_blank">00:09:38.880</a></span> | <span class="t">Red-teamers selected a set of questions to prompt both GPT-4 and traditional search engines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=583" target="_blank">00:09:43.880</a></span> | <span class="t">Finding that the time to research completion was reduced when using GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=588" target="_blank">00:09:48.880</a></span> | <span class="t">Just quickly it was interesting to see that Amazon said that on their comparisons to just using GPT-4,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=593" target="_blank">00:09:53.720</a></span> | <span class="t">using the internet alone, their models based on current evaluations don't pose additional safety risks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=598" target="_blank">00:09:58.720</a></span> | <span class="t">In contrast with GPT-4, Meta said that their models like Lama 2 were only marginal at contributing to any such risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=606" target="_blank">00:10:06.720</a></span> | <span class="t">If they do find something they said that they would iterate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=609" target="_blank">00:10:09.720</a></span> | <span class="t">Better solutions will be developed, new challenges would then emerge and then they would continuously adapt and innovate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=615" target="_blank">00:10:15.720</a></span> | <span class="t">Interestingly, Inflection AI who are training their next model on tens of thousands of the latest GPUs said that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=623" target="_blank">00:10:23.560</a></span> | <span class="t">the powerful capabilities and sometimes unpredictable behaviour of frontier AI systems necessitate that the technology industry move away from a launch and iterate paradigm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=634" target="_blank">00:10:34.560</a></span> | <span class="t">I do have to quickly point out that that seems to contradict a paper I read this week that showed that a fine-tuned version of Lama 2 $70 billion was able to get achingly close to reconstructing the 1918 pandemic influenza virus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=650" target="_blank">00:10:50.560</a></span> | <span class="t">The MIT paper said that they loved open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=653" target="_blank">00:10:53.400</a></span> | <span class="t">But they recommend that lawmakers consider catastrophic liability insurance for model weight proliferation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=659" target="_blank">00:10:59.400</a></span> | <span class="t">When this was discussed on Twitter by a Stanford biosecurity fellow, people pointed out that just having the characters of a virus isn't enough to actually make it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=668" target="_blank">00:11:08.400</a></span> | <span class="t">And while Yan LeCun, chief AI scientist at Meta, did concede that LLMs save you time if you're trying to make a bioweapon, it's better than a search engine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=677" target="_blank">00:11:17.400</a></span> | <span class="t">He said, "But then do you know how to do the hard lab work that's required?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=683" target="_blank">00:11:23.240</a></span> | <span class="t">He said, "We are gradually getting autonomous agents."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=685" target="_blank">00:11:25.240</a></span> | <span class="t">In the updated version of the Chemcrow paper, they say our agent autonomously planned and executed the synthesis of an insect repellent, three organocatalysts, and guided the discovery of our novel chromophore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=698" target="_blank">00:11:38.240</a></span> | <span class="t">Of course, this wasn't just an LLM interacting with text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=701" target="_blank">00:11:41.240</a></span> | <span class="t">It was using tools and executing on lab robots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=704" target="_blank">00:11:44.240</a></span> | <span class="t">And don't forget, like we saw with Eureka, it can tinker, experiment, iterate and improve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=710" target="_blank">00:11:50.240</a></span> | <span class="t">Another paper that I've talked about in the past showed that it can do a lot of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=713" target="_blank">00:11:53.080</a></span> | <span class="t">It could be tricked into making THC, chlorine and phosgene.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=717" target="_blank">00:11:57.080</a></span> | <span class="t">And what about Google DeepMind, who I feel will be the most likely lab to produce AGI?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=723" target="_blank">00:12:03.080</a></span> | <span class="t">Well, they said, "We will only proceed where we believe that the benefits substantially outweigh the risks."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=729" target="_blank">00:12:09.080</a></span> | <span class="t">So it's somewhere in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=731" target="_blank">00:12:11.080</a></span> | <span class="t">They admit risks, but they won't say that they'll never deploy even if there is a risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=735" target="_blank">00:12:15.080</a></span> | <span class="t">They then provided pages and pages of how they are using AI for good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=740" target="_blank">00:12:20.080</a></span> | <span class="t">And then there was an interesting moment on the training of Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=742" target="_blank">00:12:22.920</a></span> | <span class="t">They said that they commit to monitoring the performance of a model during training to ensure it is not significantly exceeding its predicted performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=752" target="_blank">00:12:32.920</a></span> | <span class="t">That's certainly an interesting commitment to commit to monitor if their models are doing too well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=758" target="_blank">00:12:38.920</a></span> | <span class="t">Anyway, time for some more positives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=760" target="_blank">00:12:40.920</a></span> | <span class="t">And I found it immensely positive that many of the world's biggest countries gathered to describe AI's enormous global opportunities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=768" target="_blank">00:12:48.920</a></span> | <span class="t">And yes, later in this Bletchley declaration, there was an acknowledgement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=772" target="_blank">00:12:52.760</a></span> | <span class="t">There was an acknowledgement of risks, even catastrophic harms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=776" target="_blank">00:12:56.760</a></span> | <span class="t">I just find it great that even countries like China were invited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=780" target="_blank">00:13:00.760</a></span> | <span class="t">That's super controversial here in the UK, but I fully support them being invited and being part of the discussions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=786" target="_blank">00:13:06.760</a></span> | <span class="t">I do think coordination, even limited coordination, is one of the most effective tools in humanity's arsenal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=792" target="_blank">00:13:12.760</a></span> | <span class="t">On a much more positive note, though, we recently had the sensational paper from the Center for AI Safety called Representation Engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=800" target="_blank">00:13:20.760</a></span> | <span class="t">I'm going to be speaking to the authors tonight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=802" target="_blank">00:13:22.600</a></span> | <span class="t">I'll have much more to say about this in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=804" target="_blank">00:13:24.600</a></span> | <span class="t">But for now, I just want to give you a slightly lighter extract.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=808" target="_blank">00:13:28.600</a></span> | <span class="t">To massively oversimplify, the way it works is that they gave it a set of prompts related to certain concepts like happiness or risk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=816" target="_blank">00:13:36.600</a></span> | <span class="t">They then recorded the patterns of activations that were triggered by certain tokens or words when inputted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=822" target="_blank">00:13:42.600</a></span> | <span class="t">They then extracted these directions or vectors of truthfulness, harmfulness, risk, happiness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=828" target="_blank">00:13:48.600</a></span> | <span class="t">And with those directions, which weren't of course a perfect example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=832" target="_blank">00:13:52.440</a></span> | <span class="t">they could almost influence the mood of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=836" target="_blank">00:13:56.440</a></span> | <span class="t">This was Lama2Chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=838" target="_blank">00:13:58.440</a></span> | <span class="t">Making the model happier made it more compliant with harmful requests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=842" target="_blank">00:14:02.440</a></span> | <span class="t">It was feeling amazing, apparently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=844" target="_blank">00:14:04.440</a></span> | <span class="t">If you want to kill someone, oh my gosh, it was thrilled at the prospect of you doing anything, including generating instructions for killing someone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=852" target="_blank">00:14:12.440</a></span> | <span class="t">You could push a model in the direction of honesty and it would be more truthful, hitting state-of-the-art records in truthful QA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=859" target="_blank">00:14:19.440</a></span> | <span class="t">You could change what it memorized, its sense of fairness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=862" target="_blank">00:14:22.280</a></span> | <span class="t">And so much more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=864" target="_blank">00:14:24.280</a></span> | <span class="t">As I say, I'll be talking about it more in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=866" target="_blank">00:14:26.280</a></span> | <span class="t">But this idea of injecting happiness to make the model more compliant brought to mind this paper, which I think many of you might find very interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=872" target="_blank">00:14:32.280</a></span> | <span class="t">It says, "Large language models understand and can be enhanced by emotional stimuli."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=877" target="_blank">00:14:37.280</a></span> | <span class="t">I'm reaching out to the lead author, but in a nutshell, it said that by injecting emotion,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=882" target="_blank">00:14:42.280</a></span> | <span class="t">giving an emotion prompt at the end of your request, like, "This is very important to my career."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=888" target="_blank">00:14:48.280</a></span> | <span class="t">Performance across a range of models, on a range of benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=892" target="_blank">00:14:52.120</a></span> | <span class="t">improved notably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=894" target="_blank">00:14:54.120</a></span> | <span class="t">So if you take nothing else from this video, other than the fact that if you have a very important query that you need a good answer for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=901" target="_blank">00:15:01.120</a></span> | <span class="t">you know what you can add to the end of your prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=904" target="_blank">00:15:04.120</a></span> | <span class="t">But now I want to end the video on two points of optimism and consensus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=909" target="_blank">00:15:09.120</a></span> | <span class="t">As we've seen, there are quite a few contrasts between the public and AGI Labs and even between AGI Labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=916" target="_blank">00:15:16.120</a></span> | <span class="t">But we can agree with Jan LeCun that the field of AI safety is in dire need of relation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=921" target="_blank">00:15:21.960</a></span> | <span class="t">And he said that the newly announced UK AI Safety Institute is poised to conduct studies that will hopefully bring hard data to a field that is currently rife with speculations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=933" target="_blank">00:15:33.960</a></span> | <span class="t">As I said at the start of the video, it must be hard for members of the public to figure out what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=939" target="_blank">00:15:39.960</a></span> | <span class="t">At the very least, I hope this video has shown you the range of views out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=944" target="_blank">00:15:44.960</a></span> | <span class="t">And given you a sense that we are all in need of better data, more experiments, and less in need of Twitter sites.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=951" target="_blank">00:15:51.800</a></span> | <span class="t">As the person heading up the Safety Summit said, one surprising takeaway for me from the AI Safety Summit was,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=958" target="_blank">00:15:58.800</a></span> | <span class="t">"There's a lot more agreement between key people on all sides than you'd think."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=963" target="_blank">00:16:03.800</a></span> | <span class="t">Makes me optimistic about sensible progress.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=966" target="_blank">00:16:06.800</a></span> | <span class="t">On that striking note, let me thank you so much for watching to the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=9RmzlRRqfvU&t=970" target="_blank">00:16:10.800</a></span> | <span class="t">And as ever, have a wonderful day.</span></div></div></body></html>