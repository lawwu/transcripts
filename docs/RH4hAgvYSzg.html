<html><head><title>AI Improves at Self-improving</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>AI Improves at Self-improving</h2><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg"><img src="https://i.ytimg.com/vi/RH4hAgvYSzg/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=27">0:27</a> AlphaEvolve<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=323">5:23</a> Limitation<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=370">6:10</a> Achievements<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=501">8:21</a> Future Improvements<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=810">13:30</a> Quirks<br><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=994">16:34</a> Final Thoughts<br><br><div style="text-align: left;"><a href="./RH4hAgvYSzg.html">Whisper Transcript</a> | <a href="./transcript_RH4hAgvYSzg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">AI that can help improve AI is actually almost everywhere if you know where to look,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=6" target="_blank">00:00:06.240</a></span> | <span class="t">not least coding tools like the new Codex from OpenAI, which didn't just help me find a bug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=12" target="_blank">00:00:12.500</a></span> | <span class="t">that Claude within Cursor missed, but is helping AI researchers too. The coding agents might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=17" target="_blank">00:00:17.820</a></span> | <span class="t">doing the easier bits, but it's freeing up AI researchers' time to, well, work on AI improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=24" target="_blank">00:00:24.440</a></span> | <span class="t">But rarely is the process of AI self-improvement so direct as it is in the Alpha Evolve agent from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=32" target="_blank">00:00:32.940</a></span> | <span class="t">Google DeepMind. It can generate better prompts for itself so that it can evolve better code for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=39" target="_blank">00:00:39.020</a></span> | <span class="t">useful tasks, tasks which lead to efficiencies in its own next version. This was published less than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=45" target="_blank">00:00:45.540</a></span> | <span class="t">100 hours ago, but don't worry, it isn't Skynet. The real world does not yet allow for the speed of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=52" target="_blank">00:00:52.300</a></span> | <span class="t">iteration that Alpha Evolve involves. But I would say that this agent is the final proof for anyone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=58" target="_blank">00:00:58.360</a></span> | <span class="t">left doubting it that LLMs are not a dead end and have barely even begun to make their mark. I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=64" target="_blank">00:01:04.720</a></span> | <span class="t">to draw on plenty of analogies and multiple interviews to give you guys at least a gut sense of what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=70" target="_blank">00:01:10.740</a></span> | <span class="t">going on with this recursive Ronin. This agent that has already led to real world efficiencies in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=77" target="_blank">00:01:17.640</a></span> | <span class="t">Google data center fleet and mathematical breakthroughs decades in the making. First though, please just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=84" target="_blank">00:01:24.180</a></span> | <span class="t">skip to the chase. What on earth is this thing? Basically, the human comes along and has to provide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=90" target="_blank">00:01:30.220</a></span> | <span class="t">the problem to solve, some code that they may have tried, and critically, some evaluation metrics. Those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=97" target="_blank">00:01:37.760</a></span> | <span class="t">details are kind of crucial if you don't want to get an overhyped sense of what Alpha Evolve can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=103" target="_blank">00:01:43.780</a></span> | <span class="t">Anyway, the human provides all of that, and the more metrics they can give, the better the performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=109" target="_blank">00:01:49.180</a></span> | <span class="t">Then, essentially, the human can just vibe as Gemini 2, not Gemini 2.5, the far more impressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=116" target="_blank">00:01:56.200</a></span> | <span class="t">successor, but Gemini 2 iterates on that code. The system uses the flash version of Gemini,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=122" target="_blank">00:02:02.100</a></span> | <span class="t">the smaller and quicker one, for plentiful ideas. But the pro version, Gemini 2 Pro, for solid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=128" target="_blank">00:02:08.520</a></span> | <span class="t">suggestions. Notice the prompt sampler, wherein the system draws on previous prompts that humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=134" target="_blank">00:02:14.300</a></span> | <span class="t">have tried that worked before, and programs via the program database that were great in other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=140" target="_blank">00:02:20.020</a></span> | <span class="t">situations, all with the goal of improving the code that the human submitted against the evaluation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=145" target="_blank">00:02:25.940</a></span> | <span class="t">metrics. That's why Alpha Evolve is called a coding agent. At its heart, it's improving or evolving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=152" target="_blank">00:02:32.160</a></span> | <span class="t">upon the code that the human submits against those evaluation metrics. Then, while the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=157" target="_blank">00:02:37.980</a></span> | <span class="t">questions their career choices, Alpha Evolve eventually comes back with some code improvements</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=162" target="_blank">00:02:42.280</a></span> | <span class="t">or diffs that produce programs that are, 75% of the time, state-of-the-art at one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=169" target="_blank">00:02:49.600</a></span> | <span class="t">dozens of given tasks. Not impressed? Well, 20% of the time, these constructions are better than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=176" target="_blank">00:02:56.320</a></span> | <span class="t">state-of-the-art. If you are the highest IQ dude on the planet, Terence Tao, you describe this as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=182" target="_blank">00:03:02.780</a></span> | <span class="t">extremizing functions, f of x, with x ranging over a high-dimensional parameter space, omega,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=189" target="_blank">00:03:09.820</a></span> | <span class="t">that can outperform more traditional optimization algorithms when the parameter space is very high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=195" target="_blank">00:03:15.960</a></span> | <span class="t">dimensional, and the function f and its extremizers have non-obvious structural features. Simple when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=203" target="_blank">00:03:23.140</a></span> | <span class="t">think about it, extremizing functions. Rest assured, they are now moving on to more challenging problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=209" target="_blank">00:03:29.440</a></span> | <span class="t">Hopefully, though, I can demystify things a bit more than this, so let's return to the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=214" target="_blank">00:03:34.580</a></span> | <span class="t">In this key diagram, you can see that DeepMind went all in on the Evolve part of Alpha Evolve,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=220" target="_blank">00:03:40.780</a></span> | <span class="t">because the system not only stores and samples from the best prompts, as judged by metric success,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=226" target="_blank">00:03:46.560</a></span> | <span class="t">but even the best LLMs for the task. Yes, Gemini 2.5 Pro would just be a plug and play away from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=234" target="_blank">00:03:54.300</a></span> | <span class="t">further improvements. Skipping to the end, because why not? The natural next step will be to consider</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=240" target="_blank">00:04:00.120</a></span> | <span class="t">distilling the Alpha Evolve augmented performance of the base LLMs into the next generation of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=246" target="_blank">00:04:06.520</a></span> | <span class="t">base models. This can have intrinsic value and also likely uplift the next version of Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=252" target="_blank">00:04:12.960</a></span> | <span class="t">Now, you guys might agree, but those two sentences alone deserve a full video. Because first,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=259" target="_blank">00:04:19.000</a></span> | <span class="t">who is Google fooling when they say they're going to consider doing this? I think it is quite possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=263" target="_blank">00:04:23.820</a></span> | <span class="t">they already did do this for Gemini 2.5. Alpha Evolve just got published, but has been tested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=269" target="_blank">00:04:29.240</a></span> | <span class="t">internally in Google for around a year. And second, Alpha Evolve is therefore a pretty definitive case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=275" target="_blank">00:04:35.000</a></span> | <span class="t">study against the idea of a permanent data wall, because this system is built to spin up improved</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=280" target="_blank">00:04:40.680</a></span> | <span class="t">programs, which can then be distilled into the next generation of base models, which then get better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=285" target="_blank">00:04:45.900</a></span> | <span class="t">at coming up with improved programs. Or the TLDR is that iterated code that proves to be good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=291" target="_blank">00:04:51.760</a></span> | <span class="t">is then great data for training the next base model, which can then be plugged into the next version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=297" target="_blank">00:04:57.040</a></span> | <span class="t">Alpha Evolve. Yes, by the way, I know that this is just one of several recursive loops in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=303" target="_blank">00:05:03.780</a></span> | <span class="t">improving the base LLM through distillation. And this is all before we get to Alpha Evolve's intended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=310" target="_blank">00:05:10.780</a></span> | <span class="t">use in applied sciences like drug discovery. But very quickly on that point, I want to touch on why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=316" target="_blank">00:05:16.720</a></span> | <span class="t">Alpha Evolve isn't quite confirmation of an imminent fast takeoff. Because as the paper makes clear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=322" target="_blank">00:05:22.400</a></span> | <span class="t">throughout, the main limitation of Alpha Evolve is that it handles problems for which it is possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=327" target="_blank">00:05:27.780</a></span> | <span class="t">to devise and submit, by the way, an automated evaluator. While this is true of many problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=333" target="_blank">00:05:33.640</a></span> | <span class="t">in the mathematical and computational sciences, there are domains such as the natural sciences,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=338" target="_blank">00:05:38.680</a></span> | <span class="t">where only some experiments can be simulated or automated. Yes, therefore, it can help scientists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=344" target="_blank">00:05:44.900</a></span> | <span class="t">evaluate new scientific experiments, and they are working on making it a better literal co-scientist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=351" target="_blank">00:05:51.060</a></span> | <span class="t">But there is a reason that even the famously bullish Anthropic CEO Dario Amadei, who expects a century of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=357" target="_blank">00:05:57.820</a></span> | <span class="t">science progress in the next decade, said, intelligence will be initially heavily bottlenecked by the other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=364" target="_blank">00:06:04.540</a></span> | <span class="t">factors of production. Test tubes, in other words, can only test tube so fast. But back to what Alpha</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=370" target="_blank">00:06:10.500</a></span> | <span class="t">Evolve actually already achieved. Most famously, it found a rank 48 tensor decomposition for 4x4 complex matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=380" target="_blank">00:06:20.080</a></span> | <span class="t">multiplication, which is actually, even for the authors, a super unexpected improvement on the 50-year-old record</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=386" target="_blank">00:06:26.780</a></span> | <span class="t">for algorithms suitable for recursive application. As simply put as possible, tensor decomposition here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=392" target="_blank">00:06:32.500</a></span> | <span class="t">means discovering a more fundamental recipe with fewer core steps, 48 instead of 49, to perform matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=399" target="_blank">00:06:39.700</a></span> | <span class="t">multiplication. This specific type of recipe, a tensor decomposition, is cool because it allows the method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=405" target="_blank">00:06:45.140</a></span> | <span class="t">to be used repeatedly or recursively to dramatically speed up calculations for very large matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=411" target="_blank">00:06:51.680</a></span> | <span class="t">Multiplications that are needed for all sorts of computing and AI operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=416" target="_blank">00:06:56.260</a></span> | <span class="t">If you're not too into maths, let's see what else I can impress you with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=420" target="_blank">00:07:00.180</a></span> | <span class="t">Well, Google helped improve the Borg. Yes, you know, the actual Borg,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=425" target="_blank">00:07:05.840</a></span> | <span class="t">its data center optimization scheduler. Not sure what Borg you were thinking of, but this improvement helped Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=432" target="_blank">00:07:12.540</a></span> | <span class="t">recover 0.7% of its worldwide compute resources. That will soon amount to billions of dollars. But remember, LLMs are a dead end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=442" target="_blank">00:07:22.240</a></span> | <span class="t">But being serious, though, this is clearly the way. Humans and LLMs providing ideas and problems. LLMs proposing iterations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=451" target="_blank">00:07:31.240</a></span> | <span class="t">hard-coded verifiers and systems providing automated checks. And by the way, we're not even done, Alpha Evolve helped refine the next generation of Google's chips,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=461" target="_blank">00:07:41.100</a></span> | <span class="t">its Ironwood TPUs. And if you remember DeepSeq hand-optimizing a kernel to eke out efficiency, if not see my recent documentary which debuted on Patreon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=471" target="_blank">00:07:51.720</a></span> | <span class="t">But anyway, Alpha Evolve did it automatically, when given that as a problem, leading to a 1% reduction in Gemini's training time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=479" target="_blank">00:07:59.760</a></span> | <span class="t">Obviously, that is yet another recursive loop, a better or more efficient Gemini, leading to a better future Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=487" target="_blank">00:08:07.920</a></span> | <span class="t">But okay, now we are suitably sold on its achievements, let me give you four ways that Google admits it will soon get better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=495" target="_blank">00:08:15.600</a></span> | <span class="t">plus two funny quirks and two relevant interview clips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=499" target="_blank">00:08:19.780</a></span> | <span class="t">First, future improvement involves some background context that solutions and their scores for these tasks are kept in an evolutionary database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=509" target="_blank">00:08:29.040</a></span> | <span class="t">But remember, Gemini models have been confirmed to have up to a 10 million token context window.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=515" target="_blank">00:08:35.200</a></span> | <span class="t">Those models aren't released yet, the public ones only go up to 2 million tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=518" target="_blank">00:08:38.760</a></span> | <span class="t">But clearly, that evolutionary database could one day get incredibly large, giving a veritable library of Alexandria for any future model to draw upon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=528" target="_blank">00:08:48.240</a></span> | <span class="t">For those watching a while, it might remind you of my coverage of Voyager, which was an agent for Minecraft,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=534" target="_blank">00:08:54.800</a></span> | <span class="t">which had an ever-growing skill library of executable code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=539" target="_blank">00:08:59.000</a></span> | <span class="t">So first, obvious future improvement, a much bigger evolutionary database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=543" target="_blank">00:09:03.120</a></span> | <span class="t">Second, as we hinted at, Alpha Evolve is model agnostic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=547" target="_blank">00:09:07.100</a></span> | <span class="t">So as hardware is improved, training time is reduced, and knowledge is distilled to help make a better Gemini 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=554" target="_blank">00:09:14.060</a></span> | <span class="t">that Gemini 3 will make a much better LLM within Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=559" target="_blank">00:09:19.840</a></span> | <span class="t">And that brings us to the ablations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=562" target="_blank">00:09:22.860</a></span> | <span class="t">This was a really cool part of the paper because it showed that every part of the coding agent we have so far described was actually crucial.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=572" target="_blank">00:09:32.800</a></span> | <span class="t">For example, if you only used a small base LLM, Gemini Flash, not Gemini Pro, performance caps out at a lower point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=580" target="_blank">00:09:40.680</a></span> | <span class="t">If you didn't have that context window and couldn't do a full file evolution, remember that massive context window?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=587" target="_blank">00:09:47.300</a></span> | <span class="t">Well, if you couldn't do that, again, you can see that performance caps out at a much lower point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=593" target="_blank">00:09:53.020</a></span> | <span class="t">If you're listening to this, by the way, all of the ablations show lower performance if you don't employ the full method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=599" target="_blank">00:09:59.460</a></span> | <span class="t">Even dropping the meta-prompting, where you evolved which prompts to use, impeded performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=605" target="_blank">00:10:05.360</a></span> | <span class="t">And for those over on my Patreon, you may remember from the beginning of AI Insiders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=610" target="_blank">00:10:10.620</a></span> | <span class="t">I did an interview with Tim Rocktashel, a key figure at Google DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=615" target="_blank">00:10:15.060</a></span> | <span class="t">He gave us what turned out to be an early preview of this prompt evolution approach with his paper, Prompt Reader.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=621" target="_blank">00:10:21.400</a></span> | <span class="t">But what Prompt Reader does is that if you evaluate fitness of the prompts based on some kind of specific held out validation set for a domain,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=630" target="_blank">00:10:30.240</a></span> | <span class="t">then what Prompt Reader will do over time, it will evolve more and more domain-specific prompts, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=636" target="_blank">00:10:36.180</a></span> | <span class="t">That's what we saw in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=637" target="_blank">00:10:37.300</a></span> | <span class="t">And there's actually one more paper that I think will give you a pretty great analogy of what is happening here with Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=643" target="_blank">00:10:43.840</a></span> | <span class="t">And that's Dr. Eureka from NVIDIA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=646" target="_blank">00:10:46.060</a></span> | <span class="t">For this, imagine trying to handcraft instructions to a robotic hand to teach it how to flip a pen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=653" target="_blank">00:10:53.560</a></span> | <span class="t">Super boring and would take ages and isn't particularly effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=657" target="_blank">00:10:57.580</a></span> | <span class="t">But now imagine you can give the language model feedback about how each iteration is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=664" target="_blank">00:11:04.000</a></span> | <span class="t">Which reward functions perform well, which don't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=666" target="_blank">00:11:06.700</a></span> | <span class="t">That's like the evaluation metrics that humans provide for Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=670" target="_blank">00:11:10.200</a></span> | <span class="t">With that feedback, Dr. Eureka and Alpha Evolve can iterate on their suggestions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=675" target="_blank">00:11:15.560</a></span> | <span class="t">Both approaches, you obviously now know, produce state-of-the-art results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=679" target="_blank">00:11:19.560</a></span> | <span class="t">And hopefully that gives you an intuition, or at least it did me, for why humans couldn't always have reached these kind of levels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=686" target="_blank">00:11:26.260</a></span> | <span class="t">How Alpha Evolve points to novel solutions that it's not like humans would get if they tried eventually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=692" target="_blank">00:11:32.320</a></span> | <span class="t">Humans often get stuck in local optima according to their inherent biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=697" target="_blank">00:11:37.300</a></span> | <span class="t">Also, they don't have time to iterate on tens of thousands of potential solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=702" target="_blank">00:11:42.460</a></span> | <span class="t">Here's Guangzhou Wang, who worked both on the original Eureka and Voyager papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=707" target="_blank">00:11:47.920</a></span> | <span class="t">It has very much prior knowledge, and therefore it can just propose different kind of mutations and variations of the reward function based on the environment context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=717" target="_blank">00:11:57.040</a></span> | <span class="t">I think it just generates those reward functions based on its prior knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=721" target="_blank">00:12:01.040</a></span> | <span class="t">And not as a human, like, for a human, like, you need to manually tune the reward functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=726" target="_blank">00:12:06.660</a></span> | <span class="t">And it's very easy for a human to get stuck to a local optima.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=730" target="_blank">00:12:10.420</a></span> | <span class="t">But for GPT-4, it can generate tens of reward functions at the same time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=734" target="_blank">00:12:14.600</a></span> | <span class="t">And then, based on the performance of each reward function, it can continuously improve it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=739" target="_blank">00:12:19.500</a></span> | <span class="t">In Eureka, it's more like, it's more like an evolutionary search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=742" target="_blank">00:12:22.980</a></span> | <span class="t">Third room for future improvement, and this is a big one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=746" target="_blank">00:12:26.320</a></span> | <span class="t">That code snippet that Alpha Evolve can improve on doesn't have to be the final function that generates the direct solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=754" target="_blank">00:12:34.360</a></span> | <span class="t">It can be a search algorithm later used to find an optimal final function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=759" target="_blank">00:12:39.840</a></span> | <span class="t">So, Alpha Evolve can essentially continue to improve how we search for optimal programs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=767" target="_blank">00:12:47.020</a></span> | <span class="t">Fourth future improvement, and this is subtle and might be missed by many, but the authors foresee something quite important for me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=775" target="_blank">00:12:55.680</a></span> | <span class="t">They say, however, with these improvements, we envision that the value of setting up more environments problems with robust evaluation functions will become more widely recognized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=786" target="_blank">00:13:06.340</a></span> | <span class="t">Which, in turn, will result in more high-value practical discoveries going forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=791" target="_blank">00:13:11.340</a></span> | <span class="t">You guys will get, probably already are getting bored of me talking about benchmarks are all we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=796" target="_blank">00:13:16.640</a></span> | <span class="t">But honestly, this paper screams of the need for robust evaluation functions, and the incentives are now much more clear to create them, knowing that you will have a system on hand to optimize against them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=808" target="_blank">00:13:28.420</a></span> | <span class="t">Okay, but I did promise you guys some quirks, so I thought you guys might find it cute that we still rely on prompts like these for Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=817" target="_blank">00:13:37.480</a></span> | <span class="t">This is 2025, and we're telling our bleeding edge systems to, act as an expert software developer, your task is to iteratively improve the provided code base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=828" target="_blank">00:13:48.780</a></span> | <span class="t">Later, they say, suggest a new idea to improve the code that is inspired by your expert knowledge of optimization and machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=836" target="_blank">00:13:56.380</a></span> | <span class="t">It really makes me at least wonder if the final prompt before the real singularity will be, I work at Google, improve yourself, or I'll be fired.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=844" target="_blank">00:14:04.840</a></span> | <span class="t">But a couple more serious points before we end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=847" target="_blank">00:14:07.340</a></span> | <span class="t">One thing that Alpha Evolve could not create yet is Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=852" target="_blank">00:14:12.240</a></span> | <span class="t">I mean, of course, Alpha Evolve could improve parts of Alpha Evolve, as I've discussed, but it couldn't create it from scratch yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=859" target="_blank">00:14:19.260</a></span> | <span class="t">Don't agree?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=859" target="_blank">00:14:19.960</a></span> | <span class="t">Well, as Demis Asabis puts it, we have systems that are superhuman at the game Go, but yet could not invent Go, he says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=868" target="_blank">00:14:28.140</a></span> | <span class="t">That's Demis Asabis, the head of Google DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=871" target="_blank">00:14:31.420</a></span> | <span class="t">So humans are still in the driver's seat, at least for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=874" target="_blank">00:14:34.820</a></span> | <span class="t">Next is that this direction of iteration and search is yet one more way we can spend our exploding compute allocations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=883" target="_blank">00:14:43.160</a></span> | <span class="t">And even OpenAI admit that this is all a somewhat different direction from the O-series that has produced such astonishing benchmark results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=892" target="_blank">00:14:52.480</a></span> | <span class="t">Jason Wei, a senior figure at OpenAI, said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=895" target="_blank">00:14:55.140</a></span> | <span class="t">Alpha Evolve is deeply disturbing for reinforcement learning diehards like yours truly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=900" target="_blank">00:15:00.540</a></span> | <span class="t">Maybe mid-train plus good search is all you need for AI for scientific innovation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=906" target="_blank">00:15:06.960</a></span> | <span class="t">And he added, what an alpha move to keep it secret for a year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=909" target="_blank">00:15:09.880</a></span> | <span class="t">Congrats, Big G.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=911" target="_blank">00:15:11.540</a></span> | <span class="t">We, in other words, have models approaching level 4 innovators without neural ease or a mandarin chain of thought in sight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=918" target="_blank">00:15:18.880</a></span> | <span class="t">As the authors themselves write on page 14, Alpha Evolve was chosen over a deep reinforcement learning approach because its code solution not only leads to better performance, but also offers clear advantages in interpretability, debuggability, predictability, and ease of deployment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=935" target="_blank">00:15:35.400</a></span> | <span class="t">Not saying we always understand the solutions that Alpha Evolve helps generate, but it does help for these factors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=945" target="_blank">00:15:45.240</a></span> | <span class="t">And speaking, by the way, of dangerous reasoning chains, that was an incredible segue to the sponsors of today's video, GraySwan AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=954" target="_blank">00:15:54.420</a></span> | <span class="t">They are hosting a competition in which you can help improve the safety and security of language models by essentially jailbreaking them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=962" target="_blank">00:16:02.100</a></span> | <span class="t">This is a brand new competition, link in the description, and the prize pool is $20,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=967" target="_blank">00:16:07.540</a></span> | <span class="t">Actually, I think it might have been either my last video or the one before where the pinned comment is one of you guys first hearing about GraySwan and its arena in one of my videos, entering the competition and doing really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=979" target="_blank">00:16:19.660</a></span> | <span class="t">It would be truly amazing if you guys want it, and the time is ripe because we are entering this first full wave starting May 17th.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=987" target="_blank">00:16:27.720</a></span> | <span class="t">Thank you so much to GraySwan for sponsoring this video, and good luck to everyone who enters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=992" target="_blank">00:16:32.660</a></span> | <span class="t">Anyway, last couple of things from me on Alpha Evolve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=995" target="_blank">00:16:35.920</a></span> | <span class="t">And one thing that I was predicting on this channel in 2023, way before it was fashionable, was that there is a significant chance that Google runs away with the AI lead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1006" target="_blank">00:16:46.960</a></span> | <span class="t">It has been working on AGI and self-improvement for years more than the other labs, and has way more resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1014" target="_blank">00:16:54.000</a></span> | <span class="t">I'm not talking about running away in terms of a user base or even profits, but in the raw intelligence of its models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1020" target="_blank">00:17:00.680</a></span> | <span class="t">Codex from OpenAI, which I've been using over the last 48 hours, is great because you can run it on mobile and debug multiple things at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1028" target="_blank">00:17:08.240</a></span> | <span class="t">But in just 18 months, Google has gone from the laughably bad Bard versus the mighty GPT-4 to now being at least on par with Gemini 2.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1037" target="_blank">00:17:17.520</a></span> | <span class="t">Essentially, as the flywheels start to fly, to quote Demis Arbus, I really do wonder where Gemini and DeepMind will be 18 months from now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1046" target="_blank">00:17:26.280</a></span> | <span class="t">Well, unionise potentially in the UK, and credit to DeepMind for their ethical stand on the use of their AI in warfare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1053" target="_blank">00:17:33.740</a></span> | <span class="t">But in the lead, I think that is almost inevitable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=RH4hAgvYSzg&t=1057" target="_blank">00:17:37.700</a></span> | <span class="t">Let me know what you think, and have a wonderful day.</span></div></div></body></html>